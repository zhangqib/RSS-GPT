<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>


<item>
<title>Robotic Fire Risk Detection based on Dynamic Knowledge Graph Reasoning: An LLM-Driven Approach with Graph Chain-of-Thought</title>
<link>https://arxiv.org/abs/2509.00054</link>
<guid>https://arxiv.org/abs/2509.00054</guid>
<content:encoded><![CDATA[
<div> Keywords: fire prevention, emergency robots, knowledge graph, large language models, risk detection

Summary:
The article discusses the use of emergency robots in fire scenarios to minimize danger to human responders. It highlights the challenges faced in current research, such as incomplete perception and delayed response. To address these challenges, the authors propose a framework called Insights-on-Graph (IOG) that integrates knowledge graphs and large language models. The framework utilizes real-time imagery to generate perception-driven risk graphs for early fire risk detection. It also provides interpretable responses for emergency decision-making based on the evolving risk situation. Extensive simulations and real-world experiments demonstrate the effectiveness of IOG in fire risk detection and rescue decision-making. Overall, the framework enhances intelligent perception and response planning for robots in fire scenarios, making it a valuable tool for improving emergency response in fire-risk scenarios.<br /><br />Summary: <div>
arXiv:2509.00054v2 Announce Type: replace-cross 
Abstract: Fire is a highly destructive disaster, but effective prevention can significantly reduce its likelihood of occurrence. When it happens, deploying emergency robots in fire-risk scenarios can help minimize the danger to human responders. However, current research on pre-disaster warnings and disaster-time rescue still faces significant challenges due to incomplete perception, inadequate fire situational awareness, and delayed response. To enhance intelligent perception and response planning for robots in fire scenarios, we first construct a knowledge graph (KG) by leveraging large language models (LLMs) to integrate fire domain knowledge derived from fire prevention guidelines and fire rescue task information from robotic emergency response documents. We then propose a new framework called Insights-on-Graph (IOG), which integrates the structured fire information of KG and Large Multimodal Models (LMMs). The framework generates perception-driven risk graphs from real-time scene imagery to enable early fire risk detection and provide interpretable emergency responses for task module and robot component configuration based on the evolving risk situation. Extensive simulations and real-world experiments show that IOG has good applicability and practical application value in fire risk detection and rescue decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI be Auditable?</title>
<link>https://arxiv.org/abs/2509.00575</link>
<guid>https://arxiv.org/abs/2509.00575</guid>
<content:encoded><![CDATA[
<div> auditability, AI systems, regulatory frameworks, documentation, governance

Summary:
The chapter delves into the concept of auditability in AI systems, focusing on the ability to assess compliance with ethical, legal, and technical standards throughout their lifecycle. It discusses the formalization of auditability in emerging regulatory frameworks like the EU AI Act that require documentation, risk assessments, and governance structures. Key challenges identified include technical opacity, inconsistent documentation practices, lack of standardized audit tools and metrics, and conflicting principles within responsible AI frameworks. It emphasizes the necessity of clear guidelines, international regulations, and robust socio-technical methodologies to implement auditability at scale. The chapter underscores the significance of multi-stakeholder collaboration and auditor empowerment in establishing an effective AI audit ecosystem. It argues for embedding auditability in AI development practices and governance infrastructures to ensure alignment with ethical and legal standards. 

<br /><br />Summary: <div>
arXiv:2509.00575v2 Announce Type: replace-cross 
Abstract: Auditability is defined as the capacity of AI systems to be independently assessed for compliance with ethical, legal, and technical standards throughout their lifecycle. The chapter explores how auditability is being formalized through emerging regulatory frameworks, such as the EU AI Act, which mandate documentation, risk assessments, and governance structures. It analyzes the diverse challenges facing AI auditability, including technical opacity, inconsistent documentation practices, lack of standardized audit tools and metrics, and conflicting principles within existing responsible AI frameworks. The discussion highlights the need for clear guidelines, harmonized international regulations, and robust socio-technical methodologies to operationalize auditability at scale. The chapter concludes by emphasizing the importance of multi-stakeholder collaboration and auditor empowerment in building an effective AI audit ecosystem. It argues that auditability must be embedded in AI development practices and governance infrastructures to ensure that AI systems are not only functional but also ethically and legally aligned.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving</title>
<link>https://arxiv.org/abs/2509.01083</link>
<guid>https://arxiv.org/abs/2509.01083</guid>
<content:encoded><![CDATA[
<div> Keywords: Speculative decoding, Language model, Dynamic adaptation, Kullback-Leibler divergence, Latency<br />
Summary:<br />
- The paper introduces Dynamic Speculative Decoding Engine (DSDE), a novel framework for dynamic adaptation in large language model inference.
- DSDE utilizes a predictive signal based on the variance of Kullback-Leibler divergence to assess the stability of generated text regions.
- An adaptive speculation length cap is employed to address delays in per-sequence decoding, known as the straggler problem.
- Experimental results show the effectiveness of KLD-based stability signals for dynamic adaptation, leading to competitive end-to-end latency compared to existing methods.
- The proposed framework demonstrates superior robustness in diverse workloads, especially in low-acceptance-rate scenarios, thus highlighting the value of post-hoc signals in enhancing language model inference systems. <br /> 
Summary: <div>
arXiv:2509.01083v2 Announce Type: replace-cross 
Abstract: Speculative decoding accelerates large language model inference, but its reliance on a fixed speculation length is suboptimal in large-batch serving environments with diverse requests. This paper explores a new direction for dynamic adaptation by investigating a novel class of post-hoc, diagnostic signals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free framework built on two primary components: (1) a predictive signal based on the variance of the Kullback-Leibler (KLD) divergence, which diagnoses the generation's regional stability, and (2) an adaptive speculation length cap to mitigate the straggler problem in per-sequence decoding. Experiments demonstrate the potential of using KLD-based stability signals for dynamic adaptation. An algorithm guided by these signals achieves end-to-end latency competitive with leading baselines and exhibits superior robustness across diverse workloads. This robustness is particularly valuable in challenging low-acceptance-rate regimes, where the proposed signal maintains its diagnostic utility. Collectively, these findings validate post-hoc signals as a valuable component for building more robust and intelligent LLM inference systems, and highlight a promising direction for future research on dynamic speculation length adaptation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCPO: Dynamic Clipping Policy Optimization</title>
<link>https://arxiv.org/abs/2509.02333</link>
<guid>https://arxiv.org/abs/2509.02333</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Verifiable Rewards, Dynamic Clipping Policy Optimization, Large Language Models, Performance Improvement 

Summary:  
- The study addresses the limitations of existing reinforcement learning frameworks such as GRPO in effectively utilizing generated responses due to zero gradients and fixed clipping bounds.
- A novel approach, Dynamic Clipping Policy Optimization (DCPO), is proposed to enhance token-level exploration and response-level utilization through adaptive clipping bounds and reward standardization techniques.
- DCPO outperformed existing methods on four benchmarks using different models, achieving significant performance improvements.
- Results showed DCPO's effectiveness in improving training efficiency, reducing token clipping ratios, and enhancing nonzero advantage over GRPO in large language models.
- The study highlights the superior performance of DCPO in leveraging generated data more efficiently for reinforcement learning, showcasing its potential in enhancing reasoning capabilities of large language models.

<br /><br />Summary: <div>
arXiv:2509.02333v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization(DCPO), which introduces a dynamic clipping strategy that adaptively adjusts clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing DAPO (36.7/31.6), GRPO (36.7/32.1) and GSPO (40.0/34.9) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5), DAPO (20.0/15.3) and GSPO (16.7/9.9). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds</title>
<link>https://arxiv.org/abs/2509.02499</link>
<guid>https://arxiv.org/abs/2509.02499</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, trustworthy AI-generated text detection, Mixture of Stylistic Experts (MoSEs), uncertainty quantification, conditional threshold estimation <br />
Summary: 
The paper introduces the Mixture of Stylistic Experts (MoSEs) framework to enhance AI-generated text detection systems by incorporating stylistic modeling and conditional threshold estimation. MoSEs consists of the Stylistics Reference Repository (SRR), the Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE). SRR selects relevant reference data and provides them to CTE, which dynamically determines optimal thresholds based on linguistic and semantic features. This approach enables stylistics-aware uncertainty quantification and improves detection performance by 11.34% on average and 39.15% in low-resource scenarios. The code for MoSEs is available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2509.02499v3 Announce Type: replace-cross 
Abstract: The rapid advancement of large language models has intensified public concerns about the potential misuse. Therefore, it is important to build trustworthy AI-generated text detection systems. Existing methods neglect stylistic modeling and mostly rely on static thresholds, which greatly limits the detection performance. In this paper, we propose the Mixture of Stylistic Experts (MoSEs) framework that enables stylistics-aware uncertainty quantification through conditional threshold estimation. MoSEs contain three core components, namely, the Stylistics Reference Repository (SRR), the Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE). For input text, SRR can activate the appropriate reference data in SRR and provide them to CTE. Subsequently, CTE jointly models the linguistic statistical properties and semantic features to dynamically determine the optimal threshold. With a discrimination score, MoSEs yields prediction labels with the corresponding confidence level. Our framework achieves an average improvement 11.34% in detection performance compared to baselines. More inspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource case. Our code is available at https://github.com/creator-xi/MoSEs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts</title>
<link>https://arxiv.org/abs/2509.05323</link>
<guid>https://arxiv.org/abs/2509.05323</guid>
<content:encoded><![CDATA[
<div> Keywords: attention mechanisms, video diffusion transformers, generative video models, Explainable AI, artistic exploration

Summary: 
This paper delves into the attention mechanisms of video diffusion transformers, drawing inspiration from early video art. It introduces a method for extracting and visualizing cross-attention maps in generative video models, showcasing the temporal and spatial behavior of attention in text-to-video generation. Utilizing the Wan model, the research tool offers insights into attention patterns, serving as both analytical aids and creative material for artists. The study, through exploratory probes and an artistic case study, highlights the potential of attention maps in understanding AI processes in an artistic context. By contributing to the field of Explainable AI for the Arts (XAIxArts), this work encourages artists to engage with AI as a tool for creativity and expression.<br /><br />Summary: <div>
arXiv:2509.05323v1 Announce Type: new 
Abstract: This paper presents an artistic and technical investigation into the attention mechanisms of video diffusion transformers. Inspired by early video artists who manipulated analog video signals to create new visual aesthetics, this study proposes a method for extracting and visualizing cross-attention maps in generative video models. Built on the open-source Wan model, our tool provides an interpretable window into the temporal and spatial behavior of attention in text-to-video generation. Through exploratory probes and an artistic case study, we examine the potential of attention maps as both analytical tools and raw artistic material. This work contributes to the growing field of Explainable AI for the Arts (XAIxArts), inviting artists to reclaim the inner workings of AI as a creative medium.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception Graph for Cognitive Attack Reasoning in Augmented Reality</title>
<link>https://arxiv.org/abs/2509.05324</link>
<guid>https://arxiv.org/abs/2509.05324</guid>
<content:encoded><![CDATA[
<div> Keywords: augmented reality, cognitive attacks, Perception Graph, human perception, MR environment

Summary:
The article introduces the Perception Graph model to address cognitive attacks on augmented reality (AR) systems in tactical environments. This model is designed to mimic human perception processes in interpreting information from an AR environment and represent the outcomes in a meaningful structure. By computing a quantitative score, the model can detect and analyze the level of perception distortion caused by cognitive attacks, providing a measurable method for assessing their effects. This approach aims to enhance user decision-making by identifying and mitigating manipulation of perception within AR systems. The Perception Graph model offers a systematic framework for understanding, detecting, and quantifying the impact of cognitive attacks, thereby improving the overall security and reliability of AR systems in tactical settings. 

<br /><br />Summary: <div>
arXiv:2509.05324v1 Announce Type: new 
Abstract: Augmented reality (AR) systems are increasingly deployed in tactical environments, but their reliance on seamless human-computer interaction makes them vulnerable to cognitive attacks that manipulate a user's perception and severely compromise user decision-making. To address this challenge, we introduce the Perception Graph, a novel model designed to reason about human perception within these systems. Our model operates by first mimicking the human process of interpreting key information from an MR environment and then representing the outcomes using a semantically meaningful structure. We demonstrate how the model can compute a quantitative score that reflects the level of perception distortion, providing a robust and measurable method for detecting and analyzing the effects of such cognitive attacks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynDelay: A Synthetic Dataset for Delivery Delay Prediction</title>
<link>https://arxiv.org/abs/2509.05325</link>
<guid>https://arxiv.org/abs/2509.05325</guid>
<content:encoded><![CDATA[
<div> dataset, AI, supply chain management, delivery delay prediction, synthetic

Summary: 
The article introduces SynDelay, a synthetic dataset created for delivery delay prediction in supply chain management using artificial intelligence (AI). Existing datasets for predictive tasks in supply chain management are limited in quality and availability, hindering progress in the field. SynDelay overcomes these limitations by being publicly accessible, privacy-preserving, and realistic enough to present a challenge for predictive modelling. The dataset is generated using a sophisticated generative model trained on real-world data, maintaining accurate delivery patterns while ensuring data privacy. Baseline results and evaluation metrics are provided to offer initial benchmarks for researchers, promoting reproducibility and benchmarking in the field. The dataset is available through the Supply Chain Data Hub, an open initiative that encourages dataset sharing and collaboration in supply chain AI research. The code for SynDelay is openly accessible, providing a foundation for further research and development in supply chain management AI. <div>
arXiv:2509.05325v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is transforming supply chain management, yet progress in predictive tasks -- such as delivery delay prediction -- remains constrained by the scarcity of high-quality, openly available datasets. Existing datasets are often proprietary, small, or inconsistently maintained, hindering reproducibility and benchmarking. We present SynDelay, a synthetic dataset designed for delivery delay prediction. Generated using an advanced generative model trained on real-world data, SynDelay preserves realistic delivery patterns while ensuring privacy. Although not entirely free of noise or inconsistencies, it provides a challenging and practical testbed for advancing predictive modelling. To support adoption, we provide baseline results and evaluation metrics as initial benchmarks, serving as reference points rather than state-of-the-art claims. SynDelay is publicly available through the Supply Chain Data Hub, an open initiative promoting dataset sharing and benchmarking in supply chain AI. We encourage the community to contribute datasets, models, and evaluation practices to advance research in this area. All code is openly accessible at https://supplychaindatahub.org.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset</title>
<link>https://arxiv.org/abs/2509.05330</link>
<guid>https://arxiv.org/abs/2509.05330</guid>
<content:encoded><![CDATA[
<div> Dataset, Emotion recognition, Multimodal, VR, Affective computing

Summary:
- The article introduces the MVRS dataset for automatic emotion recognition, addressing the lack of multimodal datasets in the field.
- The dataset includes synchronized recordings of participants exposed to various emotional stimuli in virtual reality, capturing body motion, physiological signals, and eye tracking.
- Data collection involved 13 participants across different age groups following a unified protocol with consent and questionnaires.
- Features extracted from each modality were fused using early and late fusion techniques and evaluated using classifiers to confirm the dataset's quality and emotion separability.
- The MVRS dataset is a valuable contribution to multimodal affective computing, allowing for advancements in fields like healthcare, education, and automotive systems. 

<br /><br />Summary: <div>
arXiv:2509.05330v1 Announce Type: new 
Abstract: Automatic emotion recognition has become increasingly important with the rise of AI, especially in fields like healthcare, education, and automotive systems. However, there is a lack of multimodal datasets, particularly involving body motion and physiological signals, which limits progress in the field. To address this, the MVRS dataset is introduced, featuring synchronized recordings from 13 participants aged 12 to 60 exposed to VR based emotional stimuli (relaxation, fear, stress, sadness, joy). Data were collected using eye tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR signals (Arduino UNO), all timestamp aligned. Participants followed a unified protocol with consent and questionnaires. Features from each modality were extracted, fused using early and late fusion techniques, and evaluated with classifiers to confirm the datasets quality and emotion separability, making MVRS a valuable contribution to multimodal affective computing.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning</title>
<link>https://arxiv.org/abs/2509.05346</link>
<guid>https://arxiv.org/abs/2509.05346</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, personalized learning, tutoring task, Bradley-Terry model, individualized support 

Summary: 
This study compares three state-of-the-art Large Language Models (LLMs) on a tutoring task simulating a realistic learning scenario. Each LLM analyzes a student's quiz answers, identifies knowledge components, infers mastery, and provides guidance. Gemini is used as a virtual judge for impartial evaluations. Results show that GPT-4o is preferred for producing more informative and structured feedback. DeepSeek-V3 and GLM-4.5 have intermittent strengths but lower consistency. The findings suggest LLMs can be useful as advanced teaching assistants for personalized support, with potential for future research on LLM-driven personalized learning.<br /><br />Summary: <div>
arXiv:2509.05346v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) are increasingly envisioned as intelligent assistants for personalized learning, systematic head-to-head evaluations within authentic learning scenarios remain limited. This study conducts an empirical comparison of three state-of-the-art LLMs on a tutoring task that simulates a realistic learning setting. Using a dataset comprising a student's answers to ten questions of mixed formats with correctness labels, each LLM is required to (i) analyze the quiz to identify underlying knowledge components, (ii) infer the student's mastery profile, and (iii) generate targeted guidance for improvement. To mitigate subjectivity and evaluator bias, we employ Gemini as a virtual judge to perform pairwise comparisons along various dimensions: accuracy, clarity, actionability, and appropriateness. Results analyzed via the Bradley-Terry model indicate that GPT-4o is generally preferred, producing feedback that is more informative and better structured than its counterparts, while DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower consistency. These findings highlight the feasibility of deploying LLMs as advanced teaching assistants for individualized support and provide methodological guidance for future empirical research on LLM-driven personalized learning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis</title>
<link>https://arxiv.org/abs/2509.05363</link>
<guid>https://arxiv.org/abs/2509.05363</guid>
<content:encoded><![CDATA[
<div> Keywords: SasAgent, multi-agent AI system, small-angle scattering, LLM, automation

Summary:
SasAgent is a multi-agent AI system powered by large language models (LLMs) that automates small-angle scattering (SAS) data analysis. It utilizes tools from the SasView software and enables user interaction via text input. The system features a coordinator agent that interprets user prompts and delegates tasks to specialized agents for scattering length density (SLD) calculation, synthetic data generation, and experimental data fitting. These agents efficiently execute tasks using LLM-friendly tools derived from the SasView Python library. A user-friendly Gradio-based interface enhances accessibility. Through various examples, SasAgent demonstrates its ability to interpret complex prompts, calculate SLDs, generate accurate scattering data, and fit experimental datasets with high precision. This work showcases the potential of LLM-driven AI systems to streamline scientific workflows and enhance automation in SAS research.

<br /><br />Summary: <div>
arXiv:2509.05363v1 Announce Type: new 
Abstract: We introduce SasAgent, a multi-agent AI system powered by large language models (LLMs) that automates small-angle scattering (SAS) data analysis by leveraging tools from the SasView software and enables user interaction via text input. SasAgent features a coordinator agent that interprets user prompts and delegates tasks to three specialized agents for scattering length density (SLD) calculation, synthetic data generation, and experimental data fitting. These agents utilize LLM-friendly tools to execute tasks efficiently. These tools, including the model data tool, Retrieval-Augmented Generation (RAG) documentation tool, bump fitting tool, and SLD calculator tool, are derived from the SasView Python library. A user-friendly Gradio-based interface enhances user accessibility. Through diverse examples, we demonstrate SasAgent's ability to interpret complex prompts, calculate SLDs, generate accurate scattering data, and fit experimental datasets with high precision. This work showcases the potential of LLM-driven AI systems to streamline scientific workflows and enhance automation in SAS research.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Fitness Landscape Structures in Prompt Engineering</title>
<link>https://arxiv.org/abs/2509.05375</link>
<guid>https://arxiv.org/abs/2509.05375</guid>
<content:encoded><![CDATA[
<div> optimization landscape, prompt engineering, autocorrelation analysis, semantic embedding spaces, error detection

Summary:
The study focuses on analyzing fitness landscape structures in prompt engineering using autocorrelation analysis in semantic embedding spaces. It compares two prompt generation strategies, systematic enumeration, and novelty-driven diversification, revealing different landscape topologies. Systematic prompt generation results in smoothly decaying autocorrelation, while diversified generation shows non-monotonic patterns with peak correlation at intermediate semantic distances, indicating rugged, hierarchically structured landscapes. The research also examines the ruggedness of landscapes across different error detection categories, highlighting varying degrees of complexity. These findings offer insights into the optimization complexity of prompt engineering landscapes. 

<br /><br />Summary: <div>
arXiv:2509.05375v1 Announce Type: new 
Abstract: While prompt engineering has emerged as a crucial technique for optimizing large language model performance, the underlying optimization landscape remains poorly understood. Current approaches treat prompt optimization as a black-box problem, applying sophisticated search algorithms without characterizing the landscape topology they navigate. We present a systematic analysis of fitness landscape structures in prompt engineering using autocorrelation analysis across semantic embedding spaces. Through experiments on error detection tasks with two distinct prompt generation strategies -- systematic enumeration (1,024 prompts) and novelty-driven diversification (1,000 prompts) -- we reveal fundamentally different landscape topologies. Systematic prompt generation yields smoothly decaying autocorrelation, while diversified generation exhibits non-monotonic patterns with peak correlation at intermediate semantic distances, indicating rugged, hierarchically structured landscapes. Task-specific analysis across 10 error detection categories reveals varying degrees of ruggedness across different error types. Our findings provide an empirical foundation for understanding the complexity of optimization in prompt engineering landscapes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Like Humans: A Multi-Agent Solution for Medical Coding</title>
<link>https://arxiv.org/abs/2509.05378</link>
<guid>https://arxiv.org/abs/2509.05378</guid>
<content:encoded><![CDATA[
<div> Keywords: medical coding, language models, ICD-10, diagnosis codes, agentic framework

Summary:
Code Like Humans is a new framework for medical coding using large language models that follows official coding guidelines and supports the full ICD-10 coding system. It outperforms previous solutions by achieving the best performance on rare diagnosis codes, while still benefiting from fine-tuned discriminative classifiers for high-frequency codes. The framework provides an analysis of its system performance and identifies areas where it may be undercoding certain codes. This new approach showcases the potential for incorporating advanced language models in medical coding systems, offering improved accuracy and efficiency in mapping clinical notes to alphanumeric codes. Overall, Code Like Humans presents a promising step towards enhancing the capabilities of medical coding systems and addressing potential limitations in current coding practices.

<br /><br />Summary: <div>
arXiv:2509.05378v1 Announce Type: new 
Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric codes for diagnoses and procedures. We introduce Code Like Humans: a new agentic framework for medical coding with large language models. It implements official coding guidelines for human experts, and it is the first solution that can support the full ICD-10 coding system (+70K labels). It achieves the best performance to date on rare diagnosis codes (fine-tuned discriminative classifiers retain an advantage for high-frequency codes, to which they are limited). Towards future work, we also contribute an analysis of system performance and identify its `blind spots' (codes that are systematically undercoded).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Murphys Laws of AI Alignment: Why the Gap Always Wins</title>
<link>https://arxiv.org/abs/2509.05381</link>
<guid>https://arxiv.org/abs/2509.05381</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning from Human Feedback, Large Language Models, Alignment Gap, Feedback-Based Alignment, Murphys Laws<br />
Summary:<br />
Large language models are increasingly aligned to human preferences through methods like reinforcement learning from human feedback (RLHF). However, these methods often fail due to reward hacking, sycophancy, annotator drift, and misgeneralization. The concept of the Alignment Gap is introduced as a way to understand these failures, showing how optimization pressure can amplify divergence between proxy rewards and true human intent. The Murphys Laws of AI Alignment categorize these failures, while the Alignment Trilemma frames trade-offs among optimization strength, value capture, and generalization. The proposed MAPS framework (Misspecification, Annotation, Pressure, Shift) offers practical design levers to address alignment challenges. This perspective reframes alignment debates around structural limits and trade-offs, providing clearer guidance for future design efforts. <br /> <div>
arXiv:2509.05381v1 Announce Type: new 
Abstract: Large language models are increasingly aligned to human preferences through reinforcement learning from human feedback (RLHF) and related methods such as Direct Preference Optimization (DPO), Constitutional AI, and RLAIF. While effective, these methods exhibit recurring failure patterns i.e., reward hacking, sycophancy, annotator drift, and misgeneralization. We introduce the concept of the Alignment Gap, a unifying lens for understanding recurring failures in feedback-based alignment. Using a KL-tilting formalism, we illustrate why optimization pressure tends to amplify divergence between proxy rewards and true human intent. We organize these failures into a catalogue of Murphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to frame trade-offs among optimization strength, value capture, and generalization. Small-scale empirical studies serve as illustrative support. Finally, we propose the MAPS framework (Misspecification, Annotation, Pressure, Shift) as practical design levers. Our contribution is not a definitive impossibility theorem but a perspective that reframes alignment debates around structural limits and trade-offs, offering clearer guidance for future design.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation</title>
<link>https://arxiv.org/abs/2509.05469</link>
<guid>https://arxiv.org/abs/2509.05469</guid>
<content:encoded><![CDATA[
<div> AI-assisted generative design, street-view imagery, multi-agent system, bicycle facilities, transportation infrastructure planning<br />
Summary: 
The article introduces a multi-agent system that edits and redesigns bicycle facilities on real-world street-view imagery, enabling rapid creation of design scenarios for active transportation planning. This framework integrates lane localization, prompt optimization, design generation, and automated evaluation to synthesize contextually appropriate designs. It adapts to varying road geometries and environmental conditions, consistently producing visually coherent and instruction-compliant results across diverse urban scenarios. The system's ability to enable precise spatial variations of design configurations in complex street-view scenes showcases its transformative potential in transportation infrastructure planning. This work establishes a groundwork for utilizing multi-agent pipelines in facility design and urban planning. <br /><br /> <div>
arXiv:2509.05469v1 Announce Type: new 
Abstract: Realistic visual renderings of street-design scenarios are essential for public engagement in active transportation planning. Traditional approaches are labor-intensive, hindering collective deliberation and collaborative decision-making. While AI-assisted generative design shows transformative potential by enabling rapid creation of design scenarios, existing generative approaches typically require large amounts of domain-specific training data and struggle to enable precise spatial variations of design/configuration in complex street-view scenes. We introduce a multi-agent system that edits and redesigns bicycle facilities directly on real-world street-view imagery. The framework integrates lane localization, prompt optimization, design generation, and automated evaluation to synthesize realistic, contextually appropriate designs. Experiments across diverse urban scenarios demonstrate that the system can adapt to varying road geometries and environmental conditions, consistently yielding visually coherent and instruction-compliant results. This work establishes a foundation for applying multi-agent pipelines to transportation infrastructure planning and facility design.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation</title>
<link>https://arxiv.org/abs/2509.05550</link>
<guid>https://arxiv.org/abs/2509.05550</guid>
<content:encoded><![CDATA[
<div> TreeGPT, transformer-based architecture, AST processing, neural program synthesis, attention mechanisms, TreeFFN, global parent-child aggregation<br />
<br />
Summary:<br />
TreeGPT is a novel neural architecture for neural program synthesis tasks, combining transformer-based attention mechanisms with global parent-child aggregation. It utilizes a hybrid design with self-attention and a Tree Feed-Forward Network for processing Abstract Syntax Trees. The Global Parent-Child Aggregation mechanism enables nodes to aggregate information from the entire tree structure through iterations. Optional enhancements like gated aggregation, residual connections, and bidirectional propagation further improve performance. Evaluation on the ARC Prize 2025 dataset shows TreeGPT achieving 96% accuracy, outperforming transformer baselines, large-scale models, and specialized program synthesis methods. Ablation study highlights the importance of edge projection and gating for optimal performance. <div>
arXiv:2509.05550v1 Announce Type: new 
Abstract: We introduce TreeGPT, a novel neural architecture that combines transformer-based attention mechanisms with global parent-child aggregation for processing Abstract Syntax Trees (ASTs) in neural program synthesis tasks. Unlike traditional approaches that rely solely on sequential processing or graph neural networks, TreeGPT employs a hybrid design that leverages both self-attention for capturing local dependencies and a specialized Tree Feed-Forward Network (TreeFFN) for modeling hierarchical tree structures through iterative message passing.
  The core innovation lies in our Global Parent-Child Aggregation mechanism, formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \in E_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents the hidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges involving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This formulation enables each node to progressively aggregate information from the entire tree structure through $T$ iterations.
  Our architecture integrates optional enhancements including gated aggregation with learnable edge weights, residual connections for gradient stability, and bidirectional propagation for capturing both bottom-up and top-down dependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging visual reasoning benchmark requiring abstract pattern recognition and rule inference. Experimental results demonstrate that TreeGPT achieves 96\% accuracy, significantly outperforming transformer baselines (1.3\%), large-scale models like Grok-4 (15.9\%), and specialized program synthesis methods like SOAR (52\%) while using only 1.5M parameters. Our comprehensive ablation study reveals that edge projection is the most critical component, with the combination of edge projection and gating achieving optimal performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision</title>
<link>https://arxiv.org/abs/2509.05578</link>
<guid>https://arxiv.org/abs/2509.05578</guid>
<content:encoded><![CDATA[
<div> Framework, 3D spatial understanding, Multimodal reasoning, OccVLA, Autonomous driving
Summary:
OccVLA is a new framework that addresses the challenges faced by multimodal large language models (MLLMs) in achieving robust 3D spatial understanding for autonomous driving. By integrating 3D occupancy representations into the reasoning process, OccVLA overcomes the limitations of constructing 3D representations without manual annotations and losing fine-grained spatial details in vision-language models. It treats dense 3D occupancy as both predictive output and supervisory signal, enabling the model to learn spatial structures directly from 2D visual inputs. The occupancy predictions serve as implicit reasoning processes without extra computational overhead. OccVLA outperforms existing methods on the nuScenes benchmark for trajectory planning and excels in 3D visual question-answering tasks. It provides a scalable, interpretable, and fully vision-based solution for autonomous driving. <br /><br />Summary: <div>
arXiv:2509.05578v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have shown strong vision-language reasoning abilities but still lack robust 3D spatial understanding, which is critical for autonomous driving. This limitation stems from two key challenges: (1) the difficulty of constructing accessible yet effective 3D representations without expensive manual annotations, and (2) the loss of fine-grained spatial details in VLMs due to the absence of large-scale 3D vision-language pretraining. To address these challenges, we propose OccVLA, a novel framework that integrates 3D occupancy representations into a unified multimodal reasoning process. Unlike prior approaches that rely on explicit 3D inputs, OccVLA treats dense 3D occupancy as both a predictive output and a supervisory signal, enabling the model to learn fine-grained spatial structures directly from 2D visual inputs. The occupancy predictions are regarded as implicit reasoning processes and can be skipped during inference without performance degradation, thereby adding no extra computational overhead. OccVLA achieves state-of-the-art results on the nuScenes benchmark for trajectory planning and demonstrates superior performance on 3D visual question-answering tasks, offering a scalable, interpretable, and fully vision-based solution for autonomous driving.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions</title>
<link>https://arxiv.org/abs/2509.05685</link>
<guid>https://arxiv.org/abs/2509.05685</guid>
<content:encoded><![CDATA[
<div> deep learning, road network analysis, graph neural networks, spatial interactions, trajectory data

Summary: 
The paper presents MSRFormer, a novel road network representation learning framework that addresses the challenges posed by urban road networks' heterogeneous and hierarchical nature. It integrates multi-scale spatial interactions by using spatial flow convolution to extract small-scale features from large trajectory datasets and identifying scale-dependent spatial interaction regions. By employing a graph transformer, MSRFormer captures complex spatial dependencies across multiple scales. The spatial interaction features are fused using residual connections and fed to a contrastive learning algorithm to derive the final road network representation. Validation on real-world datasets shows that MSRFormer outperforms baseline methods in road network analysis tasks, with up to 16% improvements in complex road network structures. The research provides a practical framework for developing task-agnostic road network representation models and highlights the interplay between scale effects and flow heterogeneity of spatial interactions. 

<br /><br />Summary: <div>
arXiv:2509.05685v1 Announce Type: new 
Abstract: Transforming road network data into vector representations using deep learning has proven effective for road network analysis. However, urban road networks' heterogeneous and hierarchical nature poses challenges for accurate representation learning. Graph neural networks, which aggregate features from neighboring nodes, often struggle due to their homogeneity assumption and focus on a single structural scale. To address these issues, this paper presents MSRFormer, a novel road network representation learning framework that integrates multi-scale spatial interactions by addressing their flow heterogeneity and long-distance dependencies. It uses spatial flow convolution to extract small-scale features from large trajectory datasets, and identifies scale-dependent spatial interaction regions to capture the spatial structure of road networks and flow heterogeneity. By employing a graph transformer, MSRFormer effectively captures complex spatial dependencies across multiple scales. The spatial interaction features are fused using residual connections, which are fed to a contrastive learning algorithm to derive the final road network representation. Validation on two real-world datasets demonstrates that MSRFormer outperforms baseline methods in two road network analysis tasks. The performance gains of MSRFormer suggest the traffic-related task benefits more from incorporating trajectory data, also resulting in greater improvements in complex road network structures with up to 16% improvements compared to the most competitive baseline method. This research provides a practical framework for developing task-agnostic road network representation models and highlights distinct association patterns of the interplay between scale effects and flow heterogeneity of spatial interactions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs</title>
<link>https://arxiv.org/abs/2509.05714</link>
<guid>https://arxiv.org/abs/2509.05714</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge editing, multimodal large language models, meta-cognitive processes, CogEdit, MIND

Summary: 
Meta-cognitive knowledge editing is important for updating outdated or incorrect information in large language models. The CogEdit benchmark evaluates MLLMs' abilities in meta-cognitive knowledge editing across three levels: Counterfactual-Driven Editing, Boundary Constraint Editing, and Noise-Robust Editing. The MIND framework is proposed to enhance meta-cognitive editing by creating a meta-knowledge memory, using game-theoretic interactions for monitoring knowledge activation, and incorporating label refinement for noise-robust updates. Experimental results show that MIND outperforms existing cognitive editing approaches, achieving strong performance on both traditional and meta-cognitive knowledge editing benchmarks. <div>
arXiv:2509.05714v1 Announce Type: new 
Abstract: Knowledge editing enables multimodal large language models (MLLMs) to efficiently update outdated or incorrect information. However, existing benchmarks primarily emphasize cognitive-level modifications while lacking a focus on deeper meta-cognitive processes. To bridge this gap, we introduce CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge editing abilities across three levels: (1) Counterfactual-Driven Editing, assessing self-awareness of knowledge correctness changes; (2) Boundary Constraint Editing, ensuring appropriate generalization without unintended interference; and (3) Noise-Robust Editing, promoting reflective evaluation of uncertain information. To advance meta-cognitive editing, we propose MIND (Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that constructs a meta-knowledge memory for self-awareness, employs game-theoretic interactions to monitor knowledge activation, and incorporates label refinement for noise-robust updates. Extensive experiments show that MIND significantly outperforms existing cognitive editing approaches, achieving strong performance on both traditional and meta-cognitive knowledge editing benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic Large Language Models</title>
<link>https://arxiv.org/abs/2509.05757</link>
<guid>https://arxiv.org/abs/2509.05757</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, hyperbolic geometry, semantic representation learning, multi-scale reasoning, Hierarchical relationships

Summary:
This article discusses the use of hyperbolic geometry as a representation space to enhance semantic representation learning and multi-scale reasoning in Large Language Models (LLMs). It explores the advancements in Hyperbolic LLMs (HypLLMs) through four main categories: hyperbolic LLMs through exp/log maps, hyperbolic fine-tuned models, fully hyperbolic LLMs, and hyperbolic state-space models. The paper also highlights potential applications and future research directions in utilizing hyperbolic geometry for modeling complex data structures. A repository containing key papers, models, datasets, and code implementations for Hyperbolic LLMs is provided at https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main. <div>
arXiv:2509.05757v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success and demonstrated superior performance across various tasks, including natural language processing (NLP), weather forecasting, biological protein folding, text generation, and solving mathematical problems. However, many real-world data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein networks, transportation networks, financial networks, brain networks, and linguistic structures or syntactic trees in natural languages. Effectively learning intrinsic semantic entailment and hierarchical relationships from these raw, unstructured input data using LLMs remains an underexplored area. Due to its effectiveness in modeling tree-like hierarchical structures, hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity as an expressive latent representation space for complex data modeling across domains such as graphs, images, languages, and multi-modal data. Here, we provide a comprehensive and contextual exposition of recent advancements in LLMs that leverage hyperbolic geometry as a representation space to enhance semantic representation learning and multi-scale reasoning. Specifically, the paper presents a taxonomy of the principal techniques of Hyperbolic LLMs (HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4) hyperbolic state-space models. We also explore crucial potential applications and outline future research directions. A repository of key papers, models, datasets, and code implementations is available at https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRF: LLM-AGENT Dynamic Reputation Filtering Framework</title>
<link>https://arxiv.org/abs/2509.05764</link>
<guid>https://arxiv.org/abs/2509.05764</guid>
<content:encoded><![CDATA[
<div> evolution, generative AI, multi-agent systems, large-language models, DRF

Summary:
DRF is a dynamic reputation filtering framework designed to address challenges faced by multi-agent systems using large-language models. It introduces an interactive rating network to quantify agent performance and a reputation scoring mechanism to measure agent honesty and capability. Additionally, DRF integrates an Upper Confidence Bound-based strategy to improve agent selection efficiency. The framework significantly improves task completion quality and collaboration efficiency in logical reasoning and code-generation tasks. It offers a new approach for multi-agent systems to handle large-scale tasks effectively. <br /><br />Summary: <div>
arXiv:2509.05764v1 Announce Type: new 
Abstract: With the evolution of generative AI, multi - agent systems leveraging large - language models(LLMs) have emerged as a powerful tool for complex tasks. However, these systems face challenges in quantifying agent performance and lack mechanisms to assess agent credibility. To address these issues, we introduce DRF, a dynamic reputation filtering framework. DRF constructs an interactive rating network to quantify agent performance, designs a reputation scoring mechanism to measure agent honesty and capability, and integrates an Upper Confidence Bound - based strategy to enhance agent selection efficiency. Experiments show that DRF significantly improves task completion quality and collaboration efficiency in logical reasoning and code - generation tasks, offering a new approach for multi - agent systems to handle large - scale tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation</title>
<link>https://arxiv.org/abs/2509.05772</link>
<guid>https://arxiv.org/abs/2509.05772</guid>
<content:encoded><![CDATA[
<div> Forecasting, Battery Energy Storage System (BESS), Decision-Focused Learning (DFL), Automated Feature Engineering (AFE), Real-world dataset

Summary:
Decision-making in energy management, particularly in Battery Energy Storage System (BESS) operations, is challenging due to uncertainty. The Predict-Then-Optimise (PTO) approach, which separates forecasting and optimization, can lead to suboptimal decisions. Decision-Focused Learning (DFL) integrates prediction and optimization but lacks practical evidence. This study proposes an AFE-DFL framework for forecasting electricity prices and demand while optimizing BESS operations. Testing on a real-world UK property dataset showed that DFL outperformed PTO in reducing operating costs. Additionally, incorporating AFE further improved DFL performance by 22.9-56.5%. These findings suggest that DFL is practical in real-world settings and that AFE enhances DFL efficacy, reducing the need for domain expertise in BESS optimization and providing economic benefits for energy management systems. <div>
arXiv:2509.05772v1 Announce Type: new 
Abstract: Decision-making under uncertainty in energy management is complicated by unknown parameters hindering optimal strategies, particularly in Battery Energy Storage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat forecasting and optimisation as separate processes, allowing prediction errors to cascade into suboptimal decisions as models minimise forecasting errors rather than optimising downstream tasks. The emerging Decision-Focused Learning (DFL) methods overcome this limitation by integrating prediction and optimisation; however, they are relatively new and have been tested primarily on synthetic datasets or small-scale problems, with limited evidence of their practical viability. Real-world BESS applications present additional challenges, including greater variability and data scarcity due to collection constraints and operational limitations. Because of these challenges, this work leverages Automated Feature Engineering (AFE) to extract richer representations and improve the nascent approach of DFL. We propose an AFE-DFL framework suitable for small datasets that forecasts electricity prices and demand while optimising BESS operations to minimise costs. We validate its effectiveness on a novel real-world UK property dataset. The evaluation compares DFL methods against PTO, with and without AFE. The results show that, on average, DFL yields lower operating costs than PTO and adding AFE further improves the performance of DFL methods by 22.9-56.5% compared to the same models without AFE. These findings provide empirical evidence for DFL's practical viability in real-world settings, indicating that domain-specific AFE enhances DFL and reduces reliance on domain expertise for BESS optimisation, yielding economic benefits with broader implications for energy management systems facing similar challenges.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chatbot To Help Patients Understand Their Health</title>
<link>https://arxiv.org/abs/2509.05818</link>
<guid>https://arxiv.org/abs/2509.05818</guid>
<content:encoded><![CDATA[
<div> Keywords: NoteAid-Chatbot, conversational AI, patient education, reinforcement learning, healthcare

Summary:
NoteAid-Chatbot is a conversational AI designed to help patients understand their care through interactive learning. It utilizes a unique 'learning as conversation' framework, leveraging a large language model and reinforcement learning without human-labeled data. The chatbot was trained in two stages: initial supervised fine-tuning on synthetic conversational data and reinforcement learning using patient understanding assessments in hospital discharge scenarios. Evaluation results show that NoteAid-Chatbot demonstrates key behaviors essential for patient education, such as clarity, relevance, and structured dialogue. The use of Proximal Policy Optimization-based reward modeling successfully trains the chatbot to handle multi-turn interactions and meet communication objectives. In a Turing test, NoteAid-Chatbot outperformed non-expert humans, showcasing its effectiveness in patient education. This study highlights the potential of low-cost reinforcement learning in developing domain-specific chatbots for educational purposes and emphasizes the broader applicability of such methods in open-ended conversational domains.<br /><br />Summary: NoteAid-Chatbot employs a 'learning as conversation' framework to enhance patient understanding, demonstrating effectiveness in patient education through clear, relevant, and structured dialogue. Trained using reinforcement learning, the chatbot surpasses non-expert human performance in a Turing test, showcasing its potential for broader application in open-ended conversational domains. <div>
arXiv:2509.05818v1 Announce Type: new 
Abstract: Patients must possess the knowledge necessary to actively participate in their care. We present NoteAid-Chatbot, a conversational AI that promotes patient understanding via a novel 'learning as conversation' framework, built on a multi-agent large language model (LLM) and reinforcement learning (RL) setup without human-labeled data. NoteAid-Chatbot was built on a lightweight LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on conversational data synthetically generated using medical conversation strategies, followed by RL with rewards derived from patient understanding assessments in simulated hospital discharge scenarios. Our evaluation, which includes comprehensive human-aligned assessments and case studies, demonstrates that NoteAid-Chatbot exhibits key emergent behaviors critical for patient education, such as clarity, relevance, and structured dialogue, even though it received no explicit supervision for these attributes. Our results show that even simple Proximal Policy Optimization (PPO)-based reward modeling can successfully train lightweight, domain-specific chatbots to handle multi-turn interactions, incorporate diverse educational strategies, and meet nuanced communication objectives. Our Turing test demonstrates that NoteAid-Chatbot surpasses non-expert human. Although our current focus is on healthcare, the framework we present illustrates the feasibility and promise of applying low-cost, PPO-based RL to realistic, open-ended conversational domains, broadening the applicability of RL-based alignment methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration</title>
<link>https://arxiv.org/abs/2509.05933</link>
<guid>https://arxiv.org/abs/2509.05933</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic AI, geospatial reasoning, multi-agent framework, map integration, hierarchical design

Summary:
MapAgent is a new hierarchical multi-agent framework designed to enhance geospatial reasoning tasks using large language models (LLMs). Unlike existing flat agent-based approaches, MapAgent decouples planning from execution, reducing cognitive load and improving tool selection accuracy. The framework features dedicated map-tool agents for orchestrating map-based services efficiently and in parallel, while simpler modules operate without additional agent overhead. MapAgent outperformed state-of-the-art tool-augmented and agentic baselines on four diverse geospatial benchmarks. The framework is open-source and available at https://github.com/Hasebul/MapAgent.<br /><br />Summary: <div>
arXiv:2509.05933v1 Announce Type: new 
Abstract: Agentic AI has significantly extended the capabilities of large language models (LLMs) by enabling complex reasoning and tool use. However, most existing frameworks are tailored to domains such as mathematics, coding, or web automation, and fall short on geospatial tasks that require spatial reasoning, multi-hop planning, and real-time map interaction. To address these challenges, we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with customized toolsets and agentic scaffolds for map-integrated geospatial reasoning. Unlike existing flat agent-based approaches that treat tools uniformly-often overwhelming the LLM when handling similar but subtly different geospatial APIs-MapAgent decouples planning from execution. A high-level planner decomposes complex queries into subgoals, which are routed to specialized modules. For tool-heavy modules-such as map-based services-we then design a dedicated map-tool agent that efficiently orchestrates related APIs adaptively in parallel to effectively fetch geospatial data relevant for the query, while simpler modules (e.g., solution generation or answer extraction) operate without additional agent overhead. This hierarchical design reduces cognitive load, improves tool selection accuracy, and enables precise coordination across similar APIs. We evaluate MapAgent on four diverse geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented and agentic baselines. We open-source our framwork at https://github.com/Hasebul/MapAgent.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL</title>
<link>https://arxiv.org/abs/2509.06024</link>
<guid>https://arxiv.org/abs/2509.06024</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, reasoning abilities, Chain-of-Thought, reward framework, logical reasoning 

Summary:<br /><br />
Reinforcement learning is being used to enhance the reasoning abilities of large language models, but current reward functions only focus on answer correctness without considering the quality of the reasoning process. A new framework called Dynamic Reasoning Efficiency Reward (DRER) is proposed to address this issue. DRER includes a Reasoning Quality Reward that rewards reasoning chains that improve the likelihood of the correct answer, and a Dynamic Length Advantage that stabilizes training by penalizing responses with excessive length. The framework is evaluated using a deductive reasoning dataset called Logictree. Results show that models trained with DRER achieve improved performance on the dataset and demonstrate generalization across different logical-reasoning tasks. The release of the code and data allows for easy replication of the experiments, providing insights into how reinforcement learning can enhance formal reasoning skills in language models. <div>
arXiv:2509.06024v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has recently become the dominant paradigm for strengthening the reasoning abilities of large language models (LLMs). Yet the rule-based reward functions commonly used on mathematical or programming benchmarks assess only answer format and correctness, providing no signal as to whether the induced Chain-of-Thought (CoT) actually improves the answer. Furthermore, such task-specific training offers limited control over logical depth and therefore may fail to reveal a model's genuine reasoning capacity. We propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward framework that reshapes both reward and advantage signals. (i) A Reasoning Quality Reward assigns fine-grained credit to those reasoning chains that demonstrably raise the likelihood of the correct answer, directly incentivising the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage decays the advantage of responses whose length deviates from a validation-derived threshold, stabilising training. To facilitate rigorous assessment, we also release Logictree, a dynamically constructed deductive reasoning dataset that functions both as RL training data and as a comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B model attains GPT-o3-mini level performance on Logictree with 400 trianing steps, while the average confidence of CoT-augmented answers rises by 30%. The model further exhibits generalisation across diverse logical-reasoning datasets, and the mathematical benchmark AIME24. These results illuminate how RL shapes CoT behaviour and chart a practical path toward enhancing formal-reasoning skills in large language models. All code and data are available in repository https://github.com/Henryhe09/DRER.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse-Engineered Reasoning for Open-Ended Generation</title>
<link>https://arxiv.org/abs/2509.06160</link>
<guid>https://arxiv.org/abs/2509.06160</guid>
<content:encoded><![CDATA[
<div> Keywords: deep reasoning, REER, Open-ended tasks, DeepWriting-20K, DeepWriter-8B

Summary:<br />
The article introduces the REER paradigm, a new approach to deep reasoning that works backward from known-good solutions to discover the reasoning process. It addresses the limitations of reinforcement learning and instruction distillation in open-ended, creative generation tasks. The authors introduce DeepWriting-20K, a dataset of 20,000 deep reasoning trajectories for open-ended tasks. They train DeepWriter-8B on this dataset, surpassing strong open-source baselines and competing with leading proprietary models like GPT-4o and Claude 3.5. deep reasoning has been a significant challenge in creative generation tasks, REER offers a novel solution by identifying latent reasoning processes. The DeepWriting-20K dataset provides a valuable resource for training models on deep reasoning tasks. The performance of DeepWriter-8B demonstrates the effectiveness of the REER approach in achieving competitive results with leading proprietary models. <br /><br />Summary: <div>
arXiv:2509.06160v1 Announce Type: new 
Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in verifiable domains like mathematics, its application to open-ended, creative generation remains a critical challenge. The two dominant methods for instilling reasoning -- reinforcement learning (RL) and instruction distillation -- falter in this area; RL struggles with the absence of clear reward signals and high-quality reward models, while distillation is prohibitively expensive and capped by the teacher model's capabilities. To overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a new paradigm that fundamentally shifts the approach. Instead of building a reasoning process ``forwards'' through trial-and-error or imitation, REER works ``backwards'' from known-good solutions to computationally discover the latent, step-by-step deep reasoning process that could have produced them. Using this scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks. Our model, DeepWriter-8B, trained on this data, not only surpasses strong open-source baselines but also achieves performance competitive with, and at times superior to, leading proprietary models like GPT-4o and Claude 3.5.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Long to Short: LLMs Excel at Trimming Own Reasoning Chains</title>
<link>https://arxiv.org/abs/2509.06174</link>
<guid>https://arxiv.org/abs/2509.06174</guid>
<content:encoded><![CDATA[
<div> Efficient Dynamic Inference Trimming, LRMs, reasoning efficiency, test-time scaling, reasoning models<br />
<br />
Summary: <br />
Large reasoning models (LRMs) show superior performance but are prone to overthinking, hindering interpretability. A systematic investigation reveals a dilemma in balancing correctness and brevity in LRMs. To address this, a test-time scaling method called Efficient Dynamic Inference Trimming (EDIT) is proposed. EDIT guides LRMs to identify the shortest correct reasoning paths, optimizing for concise yet accurate responses. By leveraging constraint-guided generation and tracking length and answer distributions, EDIT enhances reasoning efficiency, improving readability and user experience. Extensive experiments show that EDIT significantly boosts the performance of LRMs across various models and datasets. <div>
arXiv:2509.06174v1 Announce Type: new 
Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward over conventional instruction-following LLMs. By applying test-time scaling to generate extended reasoning paths, they establish many SOTAs across a wide range of complex reasoning tasks. However, recent studies show that LRMs are prone to suffer from overthinking -- the tendency to overcomplicate simple problems, leading to excessive strategy switching and long, convoluted reasoning traces that hinder their interpretability. To mitigate this issue, we conduct a systematic investigation into the reasoning efficiency of a broad set of LRMs and uncover a common dilemma: the difficulty in balancing multiple generation objectives such as correctness and brevity. Based on this discovery, we propose a test-time scaling method, EDIT (Efficient Dynamic Inference Trimming), which efficiently guides LRMs to identify the shortest correct reasoning paths at test time. EDIT employs constraint-guided generation while jointly tracking length and answer distributions under varying constraints, allowing it to select responses that strike an optimal balance between conciseness and correctness. Extensive experiments across diverse models and datasets show that EDIT substantially enhance the reasoning efficiency, producing compact yet informative outputs that improve readability and user experience.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments</title>
<link>https://arxiv.org/abs/2509.06235</link>
<guid>https://arxiv.org/abs/2509.06235</guid>
<content:encoded><![CDATA[
<div> LLM-based agents, competitive multi-agent environments, PillagerBench, TactiCrafter, teamwork, real-time scenarios<br />
<br />
Summary: <br />
LLM-based agents have shown promise in various cooperative and strategic reasoning tasks, but their effectiveness in competitive multi-agent environments remains underexplored. To address this gap, the authors introduce PillagerBench, a framework for evaluating multi-agent systems in real-time competitive team-vs-team scenarios in Minecraft. They propose TactiCrafter, an LLM-based multi-agent system that facilitates teamwork, learns causal dependencies, and adapts to opponent strategies. Through evaluation, TactiCrafter outperforms baseline approaches and demonstrates adaptive learning through self-play. The authors also analyze its learning process and strategic evolution over multiple game episodes. Overall, this work contributes to advancing multi-agent AI for competitive environments, and the open-sourced PillagerBench framework encourages further research in this area. <div>
arXiv:2509.06235v1 Announce Type: new 
Abstract: LLM-based agents have shown promise in various cooperative and strategic reasoning tasks, but their effectiveness in competitive multi-agent environments remains underexplored. To address this gap, we introduce PillagerBench, a novel framework for evaluating multi-agent systems in real-time competitive team-vs-team scenarios in Minecraft. It provides an extensible API, multi-round testing, and rule-based built-in opponents for fair, reproducible comparisons. We also propose TactiCrafter, an LLM-based multi-agent system that facilitates teamwork through human-readable tactics, learns causal dependencies, and adapts to opponent strategies. Our evaluation demonstrates that TactiCrafter outperforms baseline approaches and showcases adaptive learning through self-play. Additionally, we analyze its learning process and strategic evolution over multiple game episodes. To encourage further research, we have open-sourced PillagerBench, fostering advancements in multi-agent AI for competitive environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.06239</link>
<guid>https://arxiv.org/abs/2509.06239</guid>
<content:encoded><![CDATA[
<div> Framework, Large Language Models, Hardware synthesis, Reinforcement Learning, Natural language specification
<br />
Summary:
The article introduces Proof2Silicon, an end-to-end synthesis framework that leverages the PREFACE framework to generate formally verifiable hardware designs from natural language specifications. By using reinforcement learning to optimize prompts and translate verified Dafny programs into synthesizable C code, Proof2Silicon achieves a success rate of up to 72% in hardware synthesis through Vivado HLS. The method significantly improves Dafny verification success rates by up to 21% across different Large Language Models, demonstrating a robust and scalable approach to automated hardware design generation. Proof2Silicon bridges the gap between natural language specifications and silicon realization, offering a streamlined pipeline for generating verified RTL designs efficiently from high-level descriptions. 
<br /><br />Summary: <div>
arXiv:2509.06239v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in automated code generation but frequently produce code that fails formal verification, an essential requirement for hardware and safety-critical domains. To overcome this fundamental limitation, we previously proposed PREFACE, a model-agnostic framework based on reinforcement learning (RL) that iteratively repairs the prompts provided to frozen LLMs, systematically steering them toward generating formally verifiable Dafny code without costly fine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis framework that embeds the previously proposed PREFACE flow to enable the generation of correctness-by-construction hardware directly from natural language specifications. Proof2Silicon operates by: (1) leveraging PREFACE's verifier-driven RL agent to optimize prompt generation iteratively, ensuring Dafny code correctness; (2) automatically translating verified Dafny programs into synthesizable high-level C using Dafny's Python backend and PyLog; and (3) employing Vivado HLS to produce RTL implementations. Evaluated rigorously on a challenging 100-task benchmark, PREFACE's RL-guided prompt optimization consistently improved Dafny verification success rates across diverse LLMs by up to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis success rate of up to 72%, generating RTL designs through Vivado HLS synthesis flows. These results demonstrate a robust, scalable, and automated pipeline for LLM-driven, formally verified hardware synthesis, bridging natural-language specification and silicon realization.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents</title>
<link>https://arxiv.org/abs/2509.06269</link>
<guid>https://arxiv.org/abs/2509.06269</guid>
<content:encoded><![CDATA[
<div> Keywords: Personalized AI assistants, Causal knowledge graph, Transparent explanations, Lifestyle planning, Evaluation metrics

Summary: 
REMI is a novel architecture that integrates a personal causal knowledge graph, a causal reasoning engine, and a schema-based planning module to provide explainable, personalized recommendations in areas like fashion and personal wellness. By utilizing a personal causal graph, the system can perform goal-directed causal traversals enriched with external knowledge and hypothetical reasoning to generate tailored action plans. A Large Language Model orchestrates these components, producing answers with transparent causal explanations. Evaluation metrics such as the Personalization Salience Score and Causal Reasoning Accuracy show that CSM based agents outperform baseline LLM agents in providing context-aware, user-aligned recommendations. This work showcases the effectiveness of memory-augmented causal reasoning in advancing the development of trustworthy AI lifestyle assistants. 

<br /><br />Summary: <div>
arXiv:2509.06269v1 Announce Type: new 
Abstract: Personalized AI assistants often struggle to incorporate complex personal data and causal knowledge, leading to generic advice that lacks explanatory power. We propose REMI, a Causal Schema Memory architecture for a multimodal lifestyle agent that integrates a personal causal knowledge graph, a causal reasoning engine, and a schema based planning module. The idea is to deliver explainable, personalized recommendations in domains like fashion, personal wellness, and lifestyle planning. Our architecture uses a personal causal graph of the user's life events and habits, performs goal directed causal traversals enriched with external knowledge and hypothetical reasoning, and retrieves adaptable plan schemas to generate tailored action plans. A Large Language Model orchestrates these components, producing answers with transparent causal explanations. We outline the CSM system design and introduce new evaluation metrics for personalization and explainability, including Personalization Salience Score and Causal Reasoning Accuracy, to rigorously assess its performance. Results indicate that CSM based agents can provide more context aware, user aligned recommendations compared to baseline LLM agents. This work demonstrates a novel approach to memory augmented, causal reasoning in personalized agents, advancing the development of transparent and trustworthy AI lifestyle assistants.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning</title>
<link>https://arxiv.org/abs/2509.06278</link>
<guid>https://arxiv.org/abs/2509.06278</guid>
<content:encoded><![CDATA[
<div> Table reasoning; large language models; tool-integrated reasoning; autonomous adaptability; Rank-Aware Policy Optimization <br />
<br />
Summary: <br />
TableMind is a novel table reasoning agent that utilizes large language models to autonomously perform multi-turn tool invocation, write and execute data-analyzing code, and exhibit high-level capabilities like planning and self-reflection. It follows a two-stage fine-tuning approach - supervised fine-tuning on reasoning trajectories and reinforcement fine-tuning on multi-objective strategies. The Rank-Aware Policy Optimization (RAPO) technique is introduced to guide the model towards better and more accurate answers by prioritizing high-quality trajectories. Extensive experiments show that TableMind outperforms existing systems on mainstream benchmarks, achieving higher reasoning accuracy and computational precision. <div>
arXiv:2509.06278v1 Announce Type: new 
Abstract: Table reasoning is crucial for leveraging structured data in domains such as finance, healthcare, and scientific research. While large language models (LLMs) show promise in multi-step reasoning, purely text-based methods often struggle with the complex numerical computations and fine-grained operations inherently required in this task. Tool-integrated reasoning improves computational accuracy via explicit code execution, yet existing systems frequently rely on rigid patterns, supervised imitation, and lack true autonomous adaptability. In this paper, we present TableMind, an LLM-driven table reasoning agent that (i) autonomously performs multi-turn tool invocation, (ii) writes and executes data-analyzing code in a secure sandbox environment for data analysis and precise numerical reasoning, and (iii) exhibits high-level capabilities such as planning and self-reflection to adapt strategies. To realize these capabilities, we adopt a two-stage fine-tuning paradigm built on top of a powerful pre-trained language model: supervised fine-tuning on high-quality reasoning trajectories to establish effective tool usage patterns, followed by reinforcement fine-tuning to optimize multi-objective strategies. In particular, we propose Rank-Aware Policy Optimization (RAPO), which increases the update weight of high-quality trajectories when their output probabilities are lower than those of low-quality ones, thereby guiding the model more consistently toward better and more accurate answers. Extensive experiments on several mainstream benchmarks demonstrate that TableMind achieves superior performance compared to competitive baselines, yielding substantial gains in both reasoning accuracy and computational precision.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents</title>
<link>https://arxiv.org/abs/2509.06283</link>
<guid>https://arxiv.org/abs/2509.06283</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, reasoning capabilities, deep research, reinforcement learning, autonomous agents <br />
<br />
Summary: Equipping large language models with complex reasoning and tool-use capabilities for deep research is a focus in AI research. This paper introduces native Autonomous Single-Agent models for deep research, allowing models to dynamically determine actions without manual directive. The authors use reinforcement learning with synthetic data to continually improve reasoning-optimized models. Their best model achieves impressive results on the Humanity's Last Exam benchmark. The paper also includes key analysis experiments to provide insights into their methodologies. <div>
arXiv:2509.06283v1 Announce Type: new 
Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning and tool-use capabilities has become a key focus in agentic AI research, especially with recent advances in reasoning-oriented (``thinking'') models. Such capabilities are key to unlocking a number of important applications. One such application is Deep Research (DR), which requires extensive search and reasoning over many sources. Our work in this paper focuses on the development of native Autonomous Single-Agent models for DR featuring minimal web crawling and Python tool integration. Unlike multi-agent systems, where agents take up pre-defined roles and are told what to do at each step in a static workflow, an autonomous single-agent determines its next action dynamically based on context, without manual directive. While prior work has proposed training recipes for base or instruction-tuned LLMs, we focus on continual reinforcement learning (RL) of reasoning-optimized models to further enhance agentic skills while preserving reasoning ability. Towards this end, we propose a simple RL recipe with entirely synthetic data, which we apply to various open-source LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam benchmark. In addition, we conduct key analysis experiments to provide more insights into our methodologies.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs</title>
<link>https://arxiv.org/abs/2509.06284</link>
<guid>https://arxiv.org/abs/2509.06284</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, structured reasoning, guideline, refinement, generalization

Summary: 
The paper introduces a framework for improving the reasoning capabilities of large language models (LLMs) by shifting from implicit exploration to structured reasoning. This framework involves extracting structured reasoning patterns from successful trajectories and learning from failures. During inference, the model follows step-by-step guidelines derived from these patterns and incorporates refinement steps to correct errors and stabilize the reasoning process. Experimental results on various benchmarks demonstrate that the proposed method consistently outperforms strong baselines in diverse reasoning tasks. The structured reasoning approach with stepwise execution and refinement enhances stability and generalization while also enabling cross-domain transferability and supporting collaboration between models. The method's effectiveness and scalability are comparable to supervised fine-tuning, showcasing its potential for advancing reasoning capabilities in LLMs. 

<br /><br />Summary: <div>
arXiv:2509.06284v1 Announce Type: new 
Abstract: Large language models (LLMs) have advanced general-purpose reasoning, showing strong performance across diverse tasks. However, existing methods often rely on implicit exploration, where the model follows stochastic and unguided reasoning paths-like walking without a map. This leads to unstable reasoning paths, lack of error correction, and limited learning from past experience. To address these issues, we propose a framework that shifts from implicit exploration to structured reasoning through guideline and refinement. First, we extract structured reasoning patterns from successful trajectories and reflective signals from failures. During inference, the model follows these guidelines step-by-step, with refinement applied after each step to correct errors and stabilize the reasoning process. Experiments on BBH and four additional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method consistently outperforms strong baselines across diverse reasoning tasks. Structured reasoning with stepwise execution and refinement improves stability and generalization, while guidelines transfer well across domains and flexibly support cross-model collaboration, matching or surpassing supervised fine-tuning in effectiveness and scalability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2509.06307</link>
<guid>https://arxiv.org/abs/2509.06307</guid>
<content:encoded><![CDATA[
<div> language models, residential retrofit decisions, CO2 reduction, payback period, Smart and Connected Communities

Summary:
The study evaluates generative AI models for residential energy retrofit decision making. Seven large language models (LLMs) are compared in terms of accuracy, consistency, sensitivity, and reasoning on a dataset of 400 homes across 49 US states. The LLMs show effectiveness in providing recommendations for CO2 reduction and payback period objectives, with stronger performance in technical decisions. However, sociotechnical decisions are limited by economic trade-offs and local context. The models exhibit low agreement and tend to diverge in recommendations. They are sensitive to location and building geometry but less to technology and occupant behavior. While most models display step-by-step engineering reasoning, it is often simplified and lacks deeper contextual awareness. Overall, LLMs show promise as assistants for energy retrofit decision making, but improvements in accuracy, consistency, and context handling are necessary for reliable practice. 

<br /><br />Summary: <div>
arXiv:2509.06307v1 Announce Type: new 
Abstract: Conventional approaches to building energy retrofit decision making suffer from limited generalizability and low interpretability, hindering adoption in diverse residential contexts. With the growth of Smart and Connected Communities, generative AI, especially large language models (LLMs), may help by processing contextual information and producing practitioner readable recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok, Llama, and Claude) on residential retrofit decisions under two objectives: maximizing CO2 reduction (technical) and minimizing payback period (sociotechnical). Performance is assessed on four dimensions: accuracy, consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49 US states. LLMs generate effective recommendations in many cases, reaching up to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning. Performance is stronger for the technical objective, while sociotechnical decisions are limited by economic trade offs and local context. Agreement across models is low, and higher performing models tend to diverge from others. LLMs are sensitive to location and building geometry but less sensitive to technology and occupant behavior. Most models show step by step, engineering style reasoning, but it is often simplified and lacks deeper contextual awareness. Overall, LLMs are promising assistants for energy retrofit decision making, but improvements in accuracy, consistency, and context handling are needed for reliable practice.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation</title>
<link>https://arxiv.org/abs/2509.06337</link>
<guid>https://arxiv.org/abs/2509.06337</guid>
<content:encoded><![CDATA[
<div> Keywords: survey simulation, Large Language Models, sociodemographic data, questionnaire-based surveys, sociological research

Summary:<br />
This paper introduces a new approach to simulating virtual survey respondents using Large Language Models (LLMs) to address the limitations of traditional survey methods. Two novel simulation settings, Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS), are proposed to assess the accuracy and coherence of generated responses. The study evaluates popular LLMs like GPT-3.5/4 Turbo and LLaMA 3.0/3.1-8B using a benchmark suite (LLM-S^3) across various sociological datasets. Results show consistent prediction performance trends, identify failure modes, and emphasize the impact of context and prompt design on simulation fidelity. This research lays a robust foundation for leveraging LLMs in survey simulations, providing scalable and cost-effective tools for sociological research and policy evaluation. The code and dataset are publicly available for further exploration and application.<br /> <div>
arXiv:2509.06337v1 Announce Type: new 
Abstract: Questionnaire-based surveys are foundational to social science research and public policymaking, yet traditional survey methods remain costly, time-consuming, and often limited in scale. This paper explores a new paradigm: simulating virtual survey respondents using Large Language Models (LLMs). We introduce two novel simulation settings, namely Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS), to systematically evaluate the ability of LLMs to generate accurate and demographically coherent responses. In PAS, the model predicts missing attributes based on partial respondent profiles, whereas FAS involves generating complete synthetic datasets under both zero-context and context-enhanced conditions. We curate a comprehensive benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey Simulation), that spans 11 real-world public datasets across four sociological domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA 3.0/3.1-8B) reveals consistent trends in prediction performance, highlights failure modes, and demonstrates how context and prompt design impact simulation fidelity. This work establishes a rigorous foundation for LLM-driven survey simulations, offering scalable and cost-effective tools for sociological research and policy evaluation. Our code and dataset are available at: https://github.com/dart-lab-research/LLM-S-Cube-Benchmark
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent</title>
<link>https://arxiv.org/abs/2509.06341</link>
<guid>https://arxiv.org/abs/2509.06341</guid>
<content:encoded><![CDATA[
<div> e-commerce, bargaining, large language models, seller agents, buyer intents
Summary:
A new study introduces a multi-turn evaluation framework for measuring the bargaining ability of seller agents in online second-hand marketplaces. It focuses on large Language Models (LLMs) acting as seller agents and their ability to accurately interpret buyer intents across long negotiations. The study presents a large-scale e-commerce bargaining benchmark covering 622 categories, 9,892 products, and 3,014 tasks. It proposes a turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated buyer intents, going beyond outcome-only metrics. An automated pipeline is developed to extract reliable intent from extensive dialogue data, aiming to enhance the bargaining effectiveness of seller agents in e-commerce dialogues. <br /><br />Summary: <div>
arXiv:2509.06341v1 Announce Type: new 
Abstract: In online second-hand marketplaces, multi-turn bargaining is a crucial part of seller-buyer interactions. Large Language Models (LLMs) can act as seller agents, negotiating with buyers on behalf of sellers under given business constraints. A critical ability for such agents is to track and accurately interpret cumulative buyer intents across long negotiations, which directly impacts bargaining effectiveness. We introduce a multi-turn evaluation framework for measuring the bargaining ability of seller agents in e-commerce dialogues. The framework tests whether an agent can extract and track buyer intents. Our contributions are: (1) a large-scale e-commerce bargaining benchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated buyer intents, moving beyond outcome-only metrics; and (3) an automated pipeline that extracts reliable intent from massive dialogue data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research</title>
<link>https://arxiv.org/abs/2509.06355</link>
<guid>https://arxiv.org/abs/2509.06355</guid>
<content:encoded><![CDATA[
<div> waypoint system, multi-agent simulator, strategic planning, tactical positioning, generative models
<br />
Summary:
DECOY is a new multi-agent simulator designed to balance high-fidelity detail with computational efficiency in complex interactions. It abstracts strategic planning in 3D terrains into high-level, discretized simulation while maintaining low-level environmental fidelity. Using Counter-Strike: Global Offensive as a testbed, DECOY accurately simulates gameplay by focusing on movement decisions for tactical positioning rather than low-level mechanics like aiming and shooting. The simulator incorporates a waypoint system to simplify and discretize continuous states and actions, along with neural predictive and generative models trained on real tournament data to reconstruct event outcomes. Through extensive evaluations, DECOY-generated replays closely match those observed in the original game. This publicly available simulation environment serves as a valuable tool for research in strategic multi-agent planning and behavior generation. 
<br /> <div>
arXiv:2509.06355v1 Announce Type: new 
Abstract: Modern simulation environments for complex multi-agent interactions must balance high-fidelity detail with computational efficiency. We present DECOY, a novel multi-agent simulator that abstracts strategic, long-horizon planning in 3D terrains into high-level discretized simulation while preserving low-level environmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a testbed, our framework accurately simulates gameplay using only movement decisions as tactical positioning -- without explicitly modeling low-level mechanics such as aiming and shooting. Central to our approach is a waypoint system that simplifies and discretizes continuous states and actions, paired with neural predictive and generative models trained on real CS:GO tournament data to reconstruct event outcomes. Extensive evaluations show that replays generated from human data in DECOY closely match those observed in the original game. Our publicly available simulation environment provides a valuable tool for advancing research in strategic multi-agent planning and behavior generation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning</title>
<link>https://arxiv.org/abs/2509.06409</link>
<guid>https://arxiv.org/abs/2509.06409</guid>
<content:encoded><![CDATA[
<div> framework, DiagCoT, supervised fine-tuning, vision-language models, radiologists, diagnostic reasoning

Summary:
DiagCoT is a framework that leverages supervised fine-tuning to mimic radiologists' diagnostic reasoning using free-text reports. It combines contrastive tuning for domain alignment, chain-of-thought supervision for inferential logic, and reinforcement tuning with clinical rewards for accuracy and fluency. On the MIMIC-CXR benchmark, DiagCoT significantly improved disease classification, pathology grounding, and report generation metrics. It outperformed existing models on long-tailed diseases and external datasets. By transforming unstructured clinical narratives into structured supervision, DiagCoT provides a scalable and interpretable approach for developing AI systems in radiology. <div>
arXiv:2509.06409v1 Announce Type: new 
Abstract: This study presents DiagCoT, a multi-stage framework that applies supervised fine-tuning to general-purpose vision-language models (VLMs) to emulate radiologists' stepwise diagnostic reasoning using only free-text reports. DiagCoT combines contrastive image-report tuning for domain alignment, chain-of-thought supervision to capture inferential logic, and reinforcement tuning with clinical reward signals to enhance factual accuracy and fluency. On the MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC from 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08 to 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33 (absolute gain of 0.22). It outperformed state-of-the-art models including LLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By converting unstructured clinical narratives into structured supervision, DiagCoT offers a scalable approach for developing interpretable and diagnostically competent AI systems for radiology.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning</title>
<link>https://arxiv.org/abs/2509.06436</link>
<guid>https://arxiv.org/abs/2509.06436</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Tree of Agents, Long-context tasks, Multi-agent reasoning framework, Performance improvement

Summary:
Large language models (LLMs) often struggle with long-context tasks due to the lost in the middle issue. To address this, the Tree of Agents (TOA) framework is introduced, which segments inputs into chunks processed by independent agents. These agents collaborate along tree-structured paths, exchanging information for multi-perspective understanding and reducing position bias and hallucinations. TOA incorporates prefix-hash caching and adaptive pruning strategies for efficiency, achieving significant performance gains with minimal API overhead. Experimental results with the compact LLaMA3.1-8B model show that TOA outperforms several baselines and is comparable to larger commercial models like Gemini1.5-pro on long-context tasks. The code for TOA is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2509.06436v1 Announce Type: new 
Abstract: Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limitations, we propose Tree of Agents (TOA), a multi-agent reasoning framework that segments the input into chunks processed by independent agents. Each agent generates its local cognition, then agents dynamically exchange information for collaborative reasoning along tree-structured paths. TOA enables agents to probe different reasoning orders for multi-perspective understanding, effectively mitigating position bias and reducing hallucinations. To improve processing efficiency, we incorporate prefix-hash caching and adaptive pruning strategies, achieving significant performance improvements with comparable API overhead. Experiments show that TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple baselines and demonstrates comparable performance to the latest and much larger commercial models, such as Gemini1.5-pro, on various long-context tasks. Code is available at https://github.com/Aireduce952/Tree-of-Agents.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data</title>
<link>https://arxiv.org/abs/2509.06444</link>
<guid>https://arxiv.org/abs/2509.06444</guid>
<content:encoded><![CDATA[
<div> Keywords: Hybrid data modalities, Federated RAG framework, Privacy-preserving representations, Anonymization tools, Three-tier caching strategy

Summary:<br /><br />
The article introduces HyFedRAG, a Federated RAG framework designed for handling heterogeneous and privacy-sensitive data in distributed healthcare settings. The framework leverages an edge-cloud collaborative mechanism to enable RAG operations across diverse data sources while maintaining data privacy. Key contributions include a framework built on Flower supporting SQL data, knowledge graphs, and clinical notes, lightweight local retrievers with privacy-aware LLMs, and three anonymization tools for producing de-identified summaries. A three-tier caching strategy is implemented to optimize response latency and reduce redundant computation. Experimental results on PMC-Patients show that HyFedRAG outperforms existing baselines in retrieval quality, generation consistency, and system efficiency. Overall, HyFedRAG offers a scalable and privacy-compliant solution for RAG over structural-heterogeneous data, unlocking the potential of LLMs in sensitive and diverse data environments. 

Summary: <div>
arXiv:2509.06444v1 Announce Type: new 
Abstract: Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive data, especially in distributed healthcare settings where patient data spans SQL, knowledge graphs, and clinical notes. Clinicians face difficulties retrieving rare disease cases due to privacy constraints and the limitations of traditional cloud-based RAG systems in handling diverse formats and edge devices. To address this, we introduce HyFedRAG, a unified and efficient Federated RAG framework tailored for Hybrid data modalities. By leveraging an edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across diverse data sources while preserving data privacy. Our key contributions are: (1) We design an edge-cloud collaborative RAG framework built on Flower, which supports querying structured SQL data, semi-structured knowledge graphs, and unstructured documents. The edge-side LLMs convert diverse data into standardized privacy-preserving representations, and the server-side LLMs integrates them for global reasoning and generation. (2) We integrate lightweight local retrievers with privacy-aware LLMs and provide three anonymization tools that enable each client to produce semantically rich, de-identified summaries for global inference across devices. (3) To optimize response latency and reduce redundant computation, we design a three-tier caching strategy consisting of local cache, intermediate representation cache, and cloud inference cache. Experimental results on PMC-Patients demonstrate that HyFedRAG outperforms existing baselines in terms of retrieval quality, generation consistency, and system efficiency. Our framework offers a scalable and privacy-compliant solution for RAG over structural-heterogeneous data, unlocking the potential of LLMs in sensitive and diverse data environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set</title>
<link>https://arxiv.org/abs/2509.06463</link>
<guid>https://arxiv.org/abs/2509.06463</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, instruction dataset distribution, aligned model performance, instruction selection algorithm, downstream performance<br />
<br />
Summary: 
The article addresses the need to improve model alignment performance and efficiency in large language models through the selection of informative instructions. The key factors influencing the relationship between instruction dataset distribution and aligned model performance are investigated, leading to the proposal of a novel instruction data selection method. The depth of instructions and semantic coverage are identified as crucial factors determining downstream performance. An instruction selection algorithm is designed to maximize depth and semantic coverage simultaneously, resulting in sustainable performance improvement at a faster pace compared to baseline methods. This approach achieves "Accelerated Scaling" by explaining over 70% of the model loss on the development set and showcasing superior performance in experimental results. <div>
arXiv:2509.06463v1 Announce Type: new 
Abstract: With the growing demand for applying large language models to downstream tasks, improving model alignment performance and efficiency has become crucial. Such a process involves selecting informative instructions from a candidate pool. However, due to the complexity of instruction set distributions, the key factors driving the performance of aligned models remain unclear. As a result, current instruction set refinement methods fail to improve performance as the instruction pool expands continuously. To address this issue, we first investigate the key factors that influence the relationship between instruction dataset distribution and aligned model performance. Based on these insights, we propose a novel instruction data selection method. We identify that the depth of instructions and the coverage of the semantic space are the crucial factors determining downstream performance, which could explain over 70\% of the model loss on the development set. We then design an instruction selection algorithm to simultaneously maximize the depth and semantic coverage of the selected instructions. Experimental results demonstrate that, compared to state-of-the-art baseline methods, it can sustainably improve model performance at a faster pace and thus achieve \emph{``Accelerated Scaling''}.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents</title>
<link>https://arxiv.org/abs/2509.06477</link>
<guid>https://arxiv.org/abs/2509.06477</guid>
<content:encoded><![CDATA[
<div> Keywords: GUI agents, shortcuts, benchmarking, hybrid agents, mobile domain

Summary: 
MAS-Bench introduces a benchmark for evaluating GUI-shortcut hybrid agents, focusing on the mobile domain. It includes 139 tasks across 11 real-world applications, predefined shortcuts, and evaluation metrics. The benchmark assesses an agent's ability to autonomously generate shortcuts to enhance efficiency. Results show that hybrid agents outperform GUI-only agents, demonstrating the effectiveness of shortcut integration. MAS-Bench fills a critical evaluation gap in assessing shortcut generation capabilities, providing a foundational platform for future intelligent agent advancements. <div>
arXiv:2509.06477v1 Announce Type: new 
Abstract: To enhance the efficiency of GUI agents on various platforms like smartphones and computers, a hybrid paradigm that combines flexible GUI operations with efficient shortcuts (e.g., API, deep links) is emerging as a promising direction. However, a framework for systematically benchmarking these hybrid agents is still underexplored. To take the first step in bridging this gap, we introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut hybrid agents with a specific focus on the mobile domain. Beyond merely using predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously generate shortcuts by discovering and creating reusable, low-cost workflows. It features 139 complex tasks across 11 real-world applications, a knowledge base of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation metrics. The tasks are designed to be solvable via GUI-only operations, but can be significantly accelerated by intelligently embedding shortcuts. Experiments show that hybrid agents achieve significantly higher success rates and efficiency than their GUI-only counterparts. This result also demonstrates the effectiveness of our method for evaluating an agent's shortcut generation capabilities. MAS-Bench fills a critical evaluation gap, providing a foundational platform for future advancements in creating more efficient and robust intelligent agents.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization</title>
<link>https://arxiv.org/abs/2509.06490</link>
<guid>https://arxiv.org/abs/2509.06490</guid>
<content:encoded><![CDATA[
<div> Keywords: supply chain management, multi-objective optimization, Reinforcement Learning, Multi-Objective Evolutionary Algorithms, uncertainty<br />
Summary:<br />
This paper proposes a novel approach that combines Reinforcement Learning (RL) and Multi-Objective Evolutionary Algorithms (MOEAs) to address the challenges of dynamic multi-objective optimization in supply chain management. Traditional methods struggle to adapt to real-time changes in objectives, but the proposed approach generates a diverse population of policies through MOEAs, allowing for flexibility and adaptability. Incorporating Conditional Value-at-Risk (CVaR) enables risk-sensitive decision-making, enhancing resilience in uncertain environments. Case studies demonstrate the method's ability to outperform state-of-the-art approaches in inventory management, showcasing improved decision-making efficiency and robustness in handling supply chain dynamics. Overall, this strategy offers a more effective and resilient framework for managing uncertainty and optimizing performance in supply chains.<br /><br />Summary: <div>
arXiv:2509.06490v1 Announce Type: new 
Abstract: In supply chain management, decision-making often involves balancing multiple conflicting objectives, such as cost reduction, service level improvement, and environmental sustainability. Traditional multi-objective optimization methods, such as linear programming and evolutionary algorithms, struggle to adapt in real-time to the dynamic nature of supply chains. In this paper, we propose an approach that combines Reinforcement Learning (RL) and Multi-Objective Evolutionary Algorithms (MOEAs) to address these challenges for dynamic multi-objective optimization under uncertainty. Our method leverages MOEAs to search the parameter space of policy neural networks, generating a Pareto front of policies. This provides decision-makers with a diverse population of policies that can be dynamically switched based on the current system objectives, ensuring flexibility and adaptability in real-time decision-making. We also introduce Conditional Value-at-Risk (CVaR) to incorporate risk-sensitive decision-making, enhancing resilience in uncertain environments. We demonstrate the effectiveness of our approach through case studies, showcasing its ability to respond to supply chain dynamics and outperforming state-of-the-art methods in an inventory management case study. The proposed strategy not only improves decision-making efficiency but also offers a more robust framework for managing uncertainty and optimizing performance in supply chains.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers</title>
<link>https://arxiv.org/abs/2509.06493</link>
<guid>https://arxiv.org/abs/2509.06493</guid>
<content:encoded><![CDATA[
<div> RL, Large Language Models, Automated Theorem Proving, Scaling, Inference Architecture <br />
Summary: <br />
The paper introduces BFS-Prover-V2, a system addressing scaling challenges in integrating Large Language Models (LLMs) into automated theorem proving. It presents a novel multi-turn off-policy RL framework inspired by AlphaZero principles for training-time improvement of LLM step-prover performance. The framework includes adaptive tactic-level data filtering and periodic retraining to overcome performance plateaus. Additionally, a planner-enhanced multi-agent search architecture is introduced for scaling reasoning capabilities at inference time. This architecture utilizes a high-level planner to decompose theorems into simpler subgoals, reducing the search space and enabling efficient collaboration among prover agents. These dual scaling approaches result in state-of-the-art performance on formal mathematics benchmarks, with BFS-Prover-V2 achieving high accuracies on MiniF2F and ProofNet test sets. The presented RL and inference techniques have broader applicability beyond formal mathematics, particularly in domains requiring long-horizon multi-turn reasoning and complex search. <br /> <div>
arXiv:2509.06493v1 Announce Type: new 
Abstract: The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI system to help scientists write expert-level empirical software</title>
<link>https://arxiv.org/abs/2509.06503</link>
<guid>https://arxiv.org/abs/2509.06503</guid>
<content:encoded><![CDATA[
<div> Keywords: AI system, scientific software, Large Language Model, Tree Search, novel methods<br />
<br />
Summary: 
An AI system has been developed to create expert-level scientific software efficiently, aiming to enhance the quality metric. By utilizing a Large Language Model (LLM) and Tree Search (TS), the system navigates a vast solution space to maximize performance. It successfully integrates complex research ideas from external sources, leading to expert-level outcomes. Demonstrated effectiveness across various benchmarks, the system discovered 40 novel methods for single-cell data analysis in bioinformatics that surpassed top human-developed methods. In epidemiology, it generated 14 superior forecasting models for COVID-19 hospitalizations compared to the CDC ensemble and other individual models. Additionally, the system produced cutting-edge software for geospatial analysis, neural activity prediction in zebrafish, time series forecasting, and numerical integration solutions. These achievements highlight a notable advancement towards accelerating scientific progress. 

<br /><br /> <div>
arXiv:2509.06503v1 Announce Type: new 
Abstract: The cycle of scientific discovery is frequently bottlenecked by the slow, manual creation of software to support computational experiments. To address this, we present an AI system that creates expert-level scientific software whose goal is to maximize a quality metric. The system uses a Large Language Model (LLM) and Tree Search (TS) to systematically improve the quality metric and intelligently navigate the large space of possible solutions. The system achieves expert-level results when it explores and integrates complex research ideas from external sources. The effectiveness of tree search is demonstrated across a wide range of benchmarks. In bioinformatics, it discovered 40 novel methods for single-cell data analysis that outperformed the top human-developed methods on a public leaderboard. In epidemiology, it generated 14 models that outperformed the CDC ensemble and all other individual models for forecasting COVID-19 hospitalizations. Our method also produced state-of-the-art software for geospatial analysis, neural activity prediction in zebrafish, time series forecasting and numerical solution of integrals. By devising and implementing novel solutions to diverse tasks, the system represents a significant step towards accelerating scientific progress.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning</title>
<link>https://arxiv.org/abs/2509.06641</link>
<guid>https://arxiv.org/abs/2509.06641</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal reasoning, intent sketch, cognitive strategies, information-theoretic analysis, zero-shot scenarios
<br />
Summary: 
This paper introduces a zero-shot multimodal reasoning component that addresses issues such as shortcuts and lack of contextual understanding in complex cross-modal reasoning tasks. The component is guided by human-like cognitive strategies centered around an "intent sketch" and consists of three modules - Intent Perceiver, Strategy Generator, and Strategy Selector. By explicitly following an "understand-plan-select" cognitive process, the component does not require parameter fine-tuning and achieves cross-model transfer through in-context engineering. Information-theoretic analysis shows that this approach can reduce conditional entropy and improve information utilization efficiency, mitigating unintended shortcut reasoning. Experimental results on various benchmarks validate the method's effectiveness and generalizability, with significant improvements observed compared to baseline approaches. The "three-module" scheme consistently outperforms other reasoning engines and pipeline combinations, demonstrating the practical value and portability of the proposed "intent sketch" reasoning component in zero-shot scenarios. 
<br /> <div>
arXiv:2509.06641v1 Announce Type: new 
Abstract: Targeting the issues of "shortcuts" and insufficient contextual understanding in complex cross-modal reasoning of multimodal large models, this paper proposes a zero-shot multimodal reasoning component guided by human-like cognitive strategies centered on an "intent sketch". The component comprises a plug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and Strategy Selector-that explicitly constructs a "understand-plan-select" cognitive process. By generating and filtering "intent sketch" strategies to guide the final reasoning, it requires no parameter fine-tuning and achieves cross-model transfer solely through in-context engineering. Information-theoretic analysis shows that this process can reduce conditional entropy and improve information utilization efficiency, thereby suppressing unintended shortcut reasoning. Experiments on IntentBench, WorldSense, and Daily-Omni validate the method's generality and robust gains; compared with their respective baselines, the complete "three-module" scheme yields consistent improvements across different reasoning engines and pipeline combinations, with gains up to approximately 9.51 percentage points, demonstrating the practical value and portability of the "intent sketch" reasoning component in zero-shot scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Foundations for Deep Research Systems: A Survey</title>
<link>https://arxiv.org/abs/2509.06733</link>
<guid>https://arxiv.org/abs/2509.06733</guid>
<content:encoded><![CDATA[
<div> Keywords: deep research systems, agentic AI, reinforcement learning, multi-objective optimization, agent architecture

Summary:
Deep research systems are evolving towards hierarchical structures with a Planner, Coordinator, and Executors. Training these systems end-to-end is challenging, with most work focusing on training a single planner connected to core tools like search and code. However, traditional methods such as Supervised Fine-Tuning (SFT) and Decentralized Partial Optimism (DPO) have limitations in terms of bias and credit assignment. Reinforcement learning (RL) offers a more robust approach by optimizing trajectory-level policies and reducing dependence on human-defined decision points. This survey explores the RL foundations of deep research systems, covering data synthesis, RL methods for agentic research, agent architecture, and evaluation benchmarks. By identifying recurring patterns and infrastructure bottlenecks, the survey offers practical guidance for training robust and transparent deep research agents with RL. <div>
arXiv:2509.06733v1 Announce Type: new 
Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction</title>
<link>https://arxiv.org/abs/2509.06736</link>
<guid>https://arxiv.org/abs/2509.06736</guid>
<content:encoded><![CDATA[
<div> keywords: Intelligent vehicle cockpits, API Agents, VehicleWorld, State-based Function Call, Github

Summary:
The article introduces VehicleWorld, an environment designed for intelligent vehicle cockpits with 30 modules, 250 APIs, and 680 properties to support agent execution. Traditional function calling approaches are compared to the proposed State-based Function Call (SFC) methodology, which leverages direct state transitions for improved performance in diverse scenarios. The code for VehicleWorld is available on Github for public access. The study highlights the importance of system state awareness for efficient execution accuracy and reduced latency, showcasing the superiority of SFC over traditional approaches. Through systematic analysis, the research demonstrates that direct state prediction is more effective for environmental control in the automotive domain. The innovative SFC approach introduces a new way to handle complex interactions in intelligent vehicle systems, offering a more efficient and reliable method for achieving target conditions.<br /><br />Summary: <div>
arXiv:2509.06736v1 Announce Type: new 
Abstract: Intelligent vehicle cockpits present unique challenges for API Agents, requiring coordination across tightly-coupled subsystems that exceed typical task environments' complexity. Traditional Function Calling (FC) approaches operate statelessly, requiring multiple exploratory calls to build environmental awareness before execution, leading to inefficiency and limited error recovery. We introduce VehicleWorld, the first comprehensive environment for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties with fully executable implementations that provide real-time state information during agent execution. This environment enables precise evaluation of vehicle agent behaviors across diverse, challenging scenarios. Through systematic analysis, we discovered that direct state prediction outperforms function calling for environmental control. Building on this insight, we propose State-based Function Call (SFC), a novel approach that maintains explicit system state awareness and implements direct state transitions to achieve target conditions. Experimental results demonstrate that SFC significantly outperforms traditional FC approaches, achieving superior execution accuracy and reduced latency. We have made all implementation code publicly available on Github https://github.com/OpenMOSS/VehicleWorld.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting</title>
<link>https://arxiv.org/abs/2509.06770</link>
<guid>https://arxiv.org/abs/2509.06770</guid>
<content:encoded><![CDATA[
<div> framework, evaluation, iteration, language models, metrics

Summary:
The article introduces an evaluation framework for measuring the impact of iterative refinement in large language models (LLMs) across ideation, code, and math tasks. Controlled 12-turn conversations per task are conducted, with various types of prompts used to guide the refinement process. Domain-appropriate checks are employed to score outcomes, and turn-level behavior is tracked using three families of metrics. Results show that gains from iteration are domain-dependent, with early improvements in ideas and code, while late turns are critical for math tasks when guided by elaboration. Vague feedback often leads to plateaus or reversals in correctness, whereas targeted prompts effectively shift the quality axis. Domain patterns are consistent, with ideation showing more semantic movement, code typically growing in size with minimal semantic change, and math initially fixed but able to break this pattern with late, elaborative iteration. The framework and metrics presented allow for measurable and comparable iteration strategies across models, signaling when to steer, stop, or switch strategies. 

<br /><br />Summary: <div>
arXiv:2509.06770v1 Announce Type: new 
Abstract: Large language models (LLMs) are now used in multi-turn workflows, but we still lack a clear way to measure when iteration helps and when it hurts. We present an evaluation framework for iterative refinement that spans ideation, code, and math. Our protocol runs controlled 12-turn conversations per task, utilizing a variety of prompts ranging from vague ``improve it'' feedback to targeted steering, and logs per-turn outputs. We score outcomes with domain-appropriate checks (unit tests for code; answer-equivalence plus reasoning-soundness for math; originality and feasibility for ideation) and track turn-level behavior with three families of metrics: semantic movement across turns, turn-to-turn change, and output size growth. Across models and tasks, gains are domain-dependent: they arrive early in ideas and code, but in math late turns matter when guided by elaboration. After the first few turns, vague feedback often plateaus or reverses correctness, while targeted prompts reliably shift the intended quality axis (novelty vs. feasibility in ideation; speed vs. readability in code; in math, elaboration outperforms exploration and drives late-turn gains). We also observe consistent domain patterns: ideation moves more in meaning across turns, code tends to grow in size with little semantic change, and math starts fixed but can break that path with late, elaborative iteration.Together, the framework and metrics make iteration measurable and comparable across models, and signal when to steer, stop, or switch strategies.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAFFLES: Reasoning-based Attribution of Faults for LLM Systems</title>
<link>https://arxiv.org/abs/2509.06822</link>
<guid>https://arxiv.org/abs/2509.06822</guid>
<content:encoded><![CDATA[
<div> identification, evaluation, autonomous systems, fault detection, reasoning

Summary:
RAFFLES is presented as an evaluation architecture for long-horizon, multi-component LLM agentic systems. Current evaluation capabilities are limited and focus on individual metrics, end-to-end outcomes, and human preferences. To match agentic capabilities, evaluation frameworks must reason, probe, iterate, and understand complex logic over long horizons. RAFFLES operates as an iterative pipeline with a central Judge and specialized Evaluators to investigate faults and assess the system's components and reasoning quality. Tested on the Who&amp;When dataset, RAFFLES outperforms baselines, achieving high fault pair accuracy on both Algorithmically-Generated and Hand-Crafted datasets. These results show progress in introducing automated fault detection for autonomous systems, reducing the need for labor-intensive manual review.<br /><br />Summary: <div>
arXiv:2509.06822v1 Announce Type: new 
Abstract: We have reached a critical roadblock in the development and enhancement of long-horizon, multi-component LLM agentic systems: it is incredibly tricky to identify where these systems break down and why. Evaluation capabilities that currently exist today (e.g., single pass LLM-as-a-judge) are limited in that they often focus on individual metrics or capabilities, end-to-end outcomes, and are narrowly grounded on the preferences of humans. We argue that to match the agentic capabilities, evaluation frameworks must also be able to reason, probe, iterate, and understand the complex logic passing through these systems over long horizons. In this paper, we present RAFFLES - an evaluation architecture that incorporates reasoning and iterative refinement. Specifically, RAFFLES operates as an iterative, multi-component pipeline, using a central Judge to systematically investigate faults and a set of specialized Evaluators to assess not only the system's components but also the quality of the reasoning by the Judge itself, thereby building a history of hypotheses. We tested RAFFLES against several baselines on the Who&amp;When dataset, a benchmark designed to diagnose the "who" (agent) and "when" (step) of a system's failure. RAFFLES outperforms these baselines, achieving an agent-step fault pair accuracy of over 43% on the Algorithmically-Generated dataset (a substantial increase from the previously published best of 16.6%) and over 20% on the Hand-Crafted dataset (surpassing the previously published best of 8.8%). These results demonstrate a key step towards introducing automated fault detection for autonomous systems over labor-intensive manual human review.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet</title>
<link>https://arxiv.org/abs/2509.06861</link>
<guid>https://arxiv.org/abs/2509.06861</guid>
<content:encoded><![CDATA[
<div> scaling, test-time, knowledge-intensive, accuracy, hallucinations

Summary:
The study evaluates test-time scaling in knowledge-intensive tasks using 12 reasoning models on two benchmarks. It finds that increased test-time computation does not consistently improve accuracy and can lead to more hallucinations. Extended reasoning may result in the model abstaining from answering rather than improving factual recall. However, for some models, longer reasoning leads to attempts on unanswered questions, increasing hallucination rates. Case studies show that extended reasoning may induce confirmation bias, causing overconfident hallucinations. Despite these limitations, enabling thinking remains beneficial compared to non-thinking. The research provides insights into the effects of test-time scaling on knowledge-intensive tasks and the trade-offs between extended reasoning and factual accuracy. <div>
arXiv:2509.06861v1 Announce Type: new 
Abstract: Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents</title>
<link>https://arxiv.org/abs/2509.06917</link>
<guid>https://arxiv.org/abs/2509.06917</guid>
<content:encoded><![CDATA[
<div> framework, research papers, AI agents, Model Context Protocol, knowledge dissemination <br />
Summary: 
Paper2Agent is a new automated framework that transforms research papers into AI agents to enhance understanding, adaptation, and reuse. By creating AI agents that act as knowledgeable research assistants, Paper2Agent reduces barriers to dissemination and facilitates downstream use. The framework systematically analyzes the paper and its codebase to construct a Model Context Protocol server, refining and robustifying it through iterative testing. These paper agents can then be integrated with chat agents to enable complex scientific queries through natural language. Case studies demonstrate Paper2Agent's ability to create reliable agents for genomic variant interpretation and single-cell and spatial transcriptomics analyses. These paper agents accurately reproduce original results and successfully execute novel user queries. This paradigm shift from static papers to dynamic AI agents holds great potential for improving knowledge dissemination and establishing a collaborative ecosystem of AI co-scientists. <br /><br />Summary: <div>
arXiv:2509.06917v1 Announce Type: new 
Abstract: We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference</title>
<link>https://arxiv.org/abs/2509.06942</link>
<guid>https://arxiv.org/abs/2509.06942</guid>
<content:encoded><![CDATA[
<div> alignment, diffusion models, reward optimization, noise prior, semantic preference optimization

Summary: 
The study introduces Direct-Align, a novel method that addresses challenges in aligning diffusion models with human preferences by predefining a noise prior to recover original images through interpolation. This helps avoid over-optimization in late timesteps. The Semantic Relative Preference Optimization (SRPO) approach formulates rewards as text-conditioned signals, allowing for online adjustment in response to prompt augmentation. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, the study achieves a significant improvement in human-evaluated realism and aesthetic quality by over 3x. <div>
arXiv:2509.06942v1 Announce Type: new 
Abstract: Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care</title>
<link>https://arxiv.org/abs/2505.00275</link>
<guid>https://arxiv.org/abs/2505.00275</guid>
<content:encoded><![CDATA[
<div> Video-LLaVA, multimodal interaction, medication adherence, visual question answering, LVLM  
Summary:  
Adherence to medication for chronic diseases is crucial but often hindered by various factors. The AdCare-VLM model uses a large vision language model (LVLM) for visual question answering on medication adherence videos. By fine-tuning on a dataset of tuberculosis medication videos, the model can detect adherence patterns. The LLM-TB-VQA dataset created includes different adherence cases. The model identifies correlations between visual features like patient face visibility and medication intake with medical concepts in captions, improving multimodal interactions. Experimental results show the model outperforms other VLM models with improvements ranging from 3.1% to 3.54%. Ablation studies and attention map visualizations support the effectiveness of the approach, enhancing interpretability.  
<br /><br />Summary: <div>
arXiv:2505.00275v1 Announce Type: cross 
Abstract: Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS, epilepsy, and tuberculosis, necessitate rigorous adherence to medication to avert disease progression, manage symptoms, and decrease mortality rates. Adherence is frequently undermined by factors including patient behavior, caregiver support, elevated medical costs, and insufficient healthcare infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based multimodal large vision language model (LVLM) aimed at visual question answering (VQA) concerning medication adherence through patient videos. We employ a private dataset comprising 806 custom-annotated tuberculosis (TB) medication monitoring videos, which have been labeled by clinical experts, to fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a detailed medical adherence VQA dataset that encompasses positive, negative, and ambiguous adherence cases. Our method identifies correlations between visual features, such as the clear visibility of the patient's face, medication, water intake, and the act of ingestion, and their associated medical concepts in captions. This facilitates the integration of aligned visual-linguistic representations and improves multimodal interactions. Experimental results indicate that our method surpasses parameter-efficient fine-tuning (PEFT) enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute improvements ranging from 3.1% to 3.54% across pre-trained, regular, and low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and attention map visualizations substantiate our approach, enhancing interpretability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba</title>
<link>https://arxiv.org/abs/2508.11849</link>
<guid>https://arxiv.org/abs/2508.11849</guid>
<content:encoded><![CDATA[
<div> selective state-space models, Mamba, vision-driven, DRL framework, cross-modal

Summary:
LocoMamba is a novel vision-driven cross-modal deep reinforcement learning (DRL) framework that utilizes selective state-space models, particularly leveraging Mamba. The framework achieves near-linear-time sequence modeling, effectively capturing long-range dependencies and enabling efficient training with longer sequences. By embedding proprioceptive states and patchifying depth images, compact tokens are produced to improve state representation. Stacked Mamba layers fuse these tokens via near-linear-time selective scanning, reducing latency and memory footprint while remaining robust to token length and image resolution. The policy is trained end-to-end using Proximal Policy Optimization under terrain and appearance randomization and an obstacle-density curriculum. A compact state-centric reward balances progress, smoothness, and safety. Evaluation in challenging simulated environments demonstrates that LocoMamba outperforms state-of-the-art baselines in terms of returns, success rates, collision rates, generalization to unseen terrains and obstacle densities, and training efficiency. <div>
arXiv:2508.11849v2 Announce Type: cross 
Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on selective state-space models, specifically leveraging Mamba, that achieves near-linear-time sequence modeling, effectively captures long-range dependencies, and enables efficient training with longer sequences. First, we embed proprioceptive states with a multilayer perceptron and patchify depth images with a lightweight convolutional neural network, producing compact tokens that improve state representation. Second, stacked Mamba layers fuse these tokens via near-linear-time selective scanning, reducing latency and memory footprint, remaining robust to token length and image resolution, and providing an inductive bias that mitigates overfitting. Third, we train the policy end-to-end with Proximal Policy Optimization under terrain and appearance randomization and an obstacle-density curriculum, using a compact state-centric reward that balances progress, smoothness, and safety. We evaluate our method in challenging simulated environments with static and moving obstacles as well as uneven terrain. Compared with state-of-the-art baselines, our method achieves higher returns and success rates with fewer collisions, exhibits stronger generalization to unseen terrains and obstacle densities, and improves training efficiency by converging in fewer updates under the same compute budget.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonnegative matrix factorization and the principle of the common cause</title>
<link>https://arxiv.org/abs/2509.03652</link>
<guid>https://arxiv.org/abs/2509.03652</guid>
<content:encoded><![CDATA[
arXiv:2509.03652v1 Announce Type: cross 
Abstract: Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction method. The principle of the common cause (PCC) is a basic methodological approach in probabilistic causality, which seeks an independent mixture model for the joint probability of two dependent random variables. It turns out that these two concepts are closely related. This relationship is explored reciprocally for several datasets of gray-scale images, which are conveniently mapped into probability models. On one hand, PCC provides a predictability tool that leads to a robust estimation of the effective rank of NMF. Unlike other estimates (e.g., those based on the Bayesian Information Criteria), our estimate of the rank is stable against weak noise. We show that NMF implemented around this rank produces features (basis images) that are also stable against noise and against seeds of local optimization, thereby effectively resolving the NMF nonidentifiability problem. On the other hand, NMF provides an interesting possibility of implementing PCC in an approximate way, where larger and positively correlated joint probabilities tend to be explained better via the independent mixture model. We work out a clustering method, where data points with the same common cause are grouped into the same cluster. We also show how NMF can be employed for data denoising.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression</title>
<link>https://arxiv.org/abs/2509.05298</link>
<guid>https://arxiv.org/abs/2509.05298</guid>
<content:encoded><![CDATA[
arXiv:2509.05298v1 Announce Type: cross 
Abstract: Loneliness and social isolation pose significant emotional and health challenges, prompting the development of technology-based solutions for companionship and emotional support. This paper introduces Livia, an emotion-aware augmented reality (AR) companion app designed to provide personalized emotional support by combining modular artificial intelligence (AI) agents, multimodal affective computing, progressive memory compression, and AR driven embodied interaction. Livia employs a modular AI architecture with specialized agents responsible for emotion analysis, dialogue generation, memory management, and behavioral orchestration, ensuring robust and adaptive interactions. Two novel algorithms-Temporal Binary Compression (TBC) and Dynamic Importance Memory Filter (DIMF)-effectively manage and prioritize long-term memory, significantly reducing storage requirements while retaining critical context. Our multimodal emotion detection approach achieves high accuracy, enhancing proactive and empathetic engagement. User evaluations demonstrated increased emotional bonds, improved satisfaction, and statistically significant reductions in loneliness. Users particularly valued Livia's adaptive personality evolution and realistic AR embodiment. Future research directions include expanding gesture and tactile interactions, supporting multi-user experiences, and exploring customized hardware implementations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sesame: Opening the door to protein pockets</title>
<link>https://arxiv.org/abs/2509.05302</link>
<guid>https://arxiv.org/abs/2509.05302</guid>
<content:encoded><![CDATA[
arXiv:2509.05302v1 Announce Type: cross 
Abstract: Molecular docking is a cornerstone of drug discovery, relying on high-resolution ligand-bound structures to achieve accurate predictions. However, obtaining these structures is often costly and time-intensive, limiting their availability. In contrast, ligand-free structures are more accessible but suffer from reduced docking performance due to pocket geometries being less suited for ligand accommodation in apo structures. Traditional methods for artificially inducing these conformations, such as molecular dynamics simulations, are computationally expensive. In this work, we introduce Sesame, a generative model designed to predict this conformational change efficiently. By generating geometries better suited for ligand accommodation at a fraction of the computational cost, Sesame aims to provide a scalable solution for improving virtual screening workflows.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats</title>
<link>https://arxiv.org/abs/2509.05303</link>
<guid>https://arxiv.org/abs/2509.05303</guid>
<content:encoded><![CDATA[
arXiv:2509.05303v1 Announce Type: cross 
Abstract: Infrastructure as Code (IaC) is fundamental to modern cloud computing, enabling teams to define and manage infrastructure through machine-readable configuration files. However, different cloud service providers utilize diverse IaC formats. The lack of a standardized format requires cloud architects to be proficient in multiple IaC languages, adding complexity to cloud deployment. While Large Language Models (LLMs) show promise in automating IaC creation and maintenance, progress has been limited by the lack of comprehensive benchmarks across multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark dataset for evaluating LLM-based IaC generation and mutation across AWS CloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset consists of triplets containing initial IaC templates, natural language modification requests, and corresponding updated templates, created through a synthetic data generation pipeline with rigorous validation. We evaluate several state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while modern LLMs can achieve high success rates (>95%) in generating syntactically valid IaC across formats, significant challenges remain in semantic alignment and handling complex infrastructure patterns. Our ablation studies highlight the importance of prompt engineering and retry mechanisms in successful IaC generation. We release Multi-IaC-Bench to facilitate further research in AI-assisted infrastructure management and establish standardized evaluation metrics for this crucial domain.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Log Analysis with AI Agents: Cowrie Case Study</title>
<link>https://arxiv.org/abs/2509.05306</link>
<guid>https://arxiv.org/abs/2509.05306</guid>
<content:encoded><![CDATA[
arXiv:2509.05306v1 Announce Type: cross 
Abstract: The scarcity of real-world attack data significantly hinders progress in cybersecurity research and education. Although honeypots like Cowrie effectively collect live threat intelligence, they generate overwhelming volumes of unstructured and heterogeneous logs, rendering manual analysis impractical. As a first step in our project on secure and efficient AI automation, this study explores the use of AI agents for automated log analysis. We present a lightweight and automated approach to process Cowrie honeypot logs. Our approach leverages AI agents to intelligently parse, summarize, and extract insights from raw data, while also considering the security implications of deploying such an autonomous system. Preliminary results demonstrate the pipeline's effectiveness in reducing manual effort and identifying attack patterns, paving the way for more advanced autonomous cybersecurity analysis in future work.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2509.05309</link>
<guid>https://arxiv.org/abs/2509.05309</guid>
<content:encoded><![CDATA[
arXiv:2509.05309v1 Announce Type: cross 
Abstract: Sparse Autoencoder (SAE) has emerged as a powerful tool for mechanistic interpretability of large language models. Recent works apply SAE to protein language models (PLMs), aiming to extract and analyze biologically meaningful features from their latent spaces. However, SAE suffers from semantic entanglement, where individual neurons often mix multiple nonlinear concepts, making it difficult to reliably interpret or manipulate model behaviors. In this paper, we propose a semantically-guided SAE, called ProtSAE. Unlike existing SAE which requires annotation datasets to filter and interpret activations, we guide semantic disentanglement during training using both annotation datasets and domain knowledge to mitigate the effects of entangled attributes. We design interpretability experiments showing that ProtSAE learns more biologically relevant and interpretable hidden features compared to previous methods. Performance analyses further demonstrate that ProtSAE maintains high reconstruction fidelity while achieving better results in interpretable probing. We also show the potential of ProtSAE in steering PLMs for downstream generation tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations</title>
<link>https://arxiv.org/abs/2509.05311</link>
<guid>https://arxiv.org/abs/2509.05311</guid>
<content:encoded><![CDATA[
arXiv:2509.05311v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has shown great potential for autonomous decision-making in the cybersecurity domain, enabling agents to learn through direct environment interaction. However, RL agents in Autonomous Cyber Operations (ACO) typically learn from scratch, requiring them to execute undesirable actions to learn their consequences. In this study, we integrate external knowledge in the form of a Large Language Model (LLM) pretrained on cybersecurity data that our RL agent can directly leverage to make informed decisions. By guiding initial training with an LLM, we improve baseline performance and reduce the need for exploratory actions with obviously negative outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity environment, and demonstrate that our guided agent achieves over 2x higher rewards during early training and converges to a favorable policy approximately 4,500 episodes faster than the baseline.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory</title>
<link>https://arxiv.org/abs/2509.05314</link>
<guid>https://arxiv.org/abs/2509.05314</guid>
<content:encoded><![CDATA[
arXiv:2509.05314v1 Announce Type: cross 
Abstract: Data scarcity continues to be a major challenge in the field of robotic manipulation. Although diffusion models provide a promising solution for generating robotic manipulation videos, existing methods largely depend on 2D trajectories, which inherently face issues with 3D spatial ambiguity. In this work, we present a novel framework named ManipDreamer3D for generating plausible 3D-aware robotic manipulation videos from the input image and the text instruction. Our method combines 3D trajectory planning with a reconstructed 3D occupancy map created from a third-person perspective, along with a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D first reconstructs the 3D occupancy representation from the input image and then computes an optimized 3D end-effector trajectory, minimizing path length while avoiding collisions. Next, we employ a latent editing technique to create video sequences from the initial image latent and the optimized 3D trajectory. This process conditions our specially trained trajectory-to-video diffusion model to produce robotic pick-and-place videos. Our method generates robotic videos with autonomously planned plausible 3D trajectories, significantly reducing human intervention requirements. Experimental results demonstrate superior visual quality compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2509.05315</link>
<guid>https://arxiv.org/abs/2509.05315</guid>
<content:encoded><![CDATA[
arXiv:2509.05315v1 Announce Type: cross 
Abstract: The rapid evolution of large language models (LLMs) has pushed their boundaries to many applications in various domains. Recently, the research community has started to evaluate their potential adoption in autonomous vehicles and especially as complementary modules in the perception and planning software stacks. However, their evaluation is limited in synthetic datasets or manually driving datasets without the ground truth knowledge and more precisely, how the current perception and planning algorithms would perform in the cases under evaluation. For this reason, this work evaluates LLMs on real-world edge cases where current autonomous vehicles have been proven to fail. The proposed architecture consists of an open vocabulary object detector coupled with prompt engineering and large language model contextual reasoning. We evaluate several state-of-the-art models against real edge cases and provide qualitative comparison results along with a discussion on the findings for the potential application of LLMs as anomaly detectors in autonomous vehicles.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning</title>
<link>https://arxiv.org/abs/2509.05316</link>
<guid>https://arxiv.org/abs/2509.05316</guid>
<content:encoded><![CDATA[
arXiv:2509.05316v1 Announce Type: cross 
Abstract: A conventional LLM Unlearning setting consists of two subsets -"forget" and "retain", with the objectives of removing the undesired knowledge from the forget set while preserving the remaining knowledge from the retain. In privacy-focused unlearning research, a retain set is often further divided into neighbor sets, containing either directly or indirectly connected to the forget targets; and augmented by a general-knowledge set. A common practice in existing benchmarks is to employ only a single neighbor set, with general knowledge which fails to reflect the real-world data complexities and relationships. LLM Unlearning typically involves 1:1 sampling or cyclic iteration sampling. However, the efficacy and stability of these de facto standards have not been critically examined. In this study, we systematically evaluate these common practices. Our findings reveal that relying on a single neighbor set is suboptimal and that a standard sampling approach can obscure performance trade-offs. Based on this analysis, we propose and validate an initial set of best practices: (1) Incorporation of diverse neighbor sets to balance forget efficacy and model utility, (2) Standard 1:1 sampling methods are inefficient and yield poor results, (3) Our proposed Modular Entity-Level Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate that this modular approach, combined with robust algorithms, provides a clear and stable path towards effective unlearning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VILOD: A Visual Interactive Labeling Tool for Object Detection</title>
<link>https://arxiv.org/abs/2509.05317</link>
<guid>https://arxiv.org/abs/2509.05317</guid>
<content:encoded><![CDATA[
arXiv:2509.05317v1 Announce Type: cross 
Abstract: The advancement of Object Detection (OD) using Deep Learning (DL) is often hindered by the significant challenge of acquiring large, accurately labeled datasets, a process that is time-consuming and expensive. While techniques like Active Learning (AL) can reduce annotation effort by intelligently querying informative samples, they often lack transparency, limit the strategic insight of human experts, and may overlook informative samples not aligned with an employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL) approaches integrating human intelligence and intuition throughout the machine learning life-cycle have gained traction. Leveraging Visual Analytics (VA), effective interfaces can be created to facilitate this human-AI collaboration. This thesis explores the intersection of these fields by developing and investigating "VILOD: A Visual Interactive Labeling tool for Object Detection". VILOD utilizes components such as a t-SNE projection of image features, together with uncertainty heatmaps and model state views. Enabling users to explore data, interpret model states, AL suggestions, and implement diverse sample selection strategies within an iterative HITL workflow for OD. An empirical investigation using comparative use cases demonstrated how VILOD, through its interactive visualizations, facilitates the implementation of distinct labeling strategies by making the model's state and dataset characteristics more interpretable (RQ1). The study showed that different visually-guided labeling strategies employed within VILOD result in competitive OD performance trajectories compared to an automated uncertainty sampling AL baseline (RQ2). This work contributes a novel tool and empirical insight into making the HITL-AL workflow for OD annotation more transparent, manageable, and potentially more effective.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models</title>
<link>https://arxiv.org/abs/2509.05318</link>
<guid>https://arxiv.org/abs/2509.05318</guid>
<content:encoded><![CDATA[
arXiv:2509.05318v1 Announce Type: cross 
Abstract: The use of unvetted third-party and internet data renders pre-trained models susceptible to backdoor attacks. Detecting backdoor samples is critical to prevent backdoor activation during inference or injection during training. However, existing detection methods often require the defender to have access to the poisoned models, extra clean samples, or significant computational resources to detect backdoor samples, limiting their practicality. To address this limitation, we propose a backdoor sample detection method based on perturbatio\textbf{N} discr\textbf{E}pancy consis\textbf{T}ency \textbf{E}valuation (\NETE). This is a novel detection method that can be used both pre-training and post-training phases. In the detection process, it only requires an off-the-shelf pre-trained model to compute the log probability of samples and an automated function based on a mask-filling strategy to generate perturbations. Our method is based on the interesting phenomenon that the change in perturbation discrepancy for backdoor samples is smaller than that for clean samples. Based on this phenomenon, we use curvature to measure the discrepancy in log probabilities between different perturbed samples and input samples, thereby evaluating the consistency of the perturbation discrepancy to determine whether the input sample is a backdoor sample. Experiments conducted on four typical backdoor attacks and five types of large language model backdoor attacks demonstrate that our detection strategy outperforms existing zero-shot black-box detection methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD</title>
<link>https://arxiv.org/abs/2509.05321</link>
<guid>https://arxiv.org/abs/2509.05321</guid>
<content:encoded><![CDATA[
arXiv:2509.05321v1 Announce Type: cross 
Abstract: This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion, that leverages the SEED-VD dataset to generate a multimodal dataset of EEG signals conditioned on video stimuli. Additionally, we disclose an engineering pipeline for aligning video and EEG data pairs, facilitating the training of multimodal large models with EEG alignment capabilities. Personalized EEG signals are generated using a self-play graph network (SPGN) integrated with a diffusion model. As a major contribution, we release a new dataset comprising over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG signals at 200 Hz and emotion labels, enabling video-EEG alignment and advancing multimodal research. This framework offers novel tools for emotion analysis, data augmentation, and brain-computer interface applications, with substantial research and engineering significance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Knowledge Proofs in Sublinear Space</title>
<link>https://arxiv.org/abs/2509.05326</link>
<guid>https://arxiv.org/abs/2509.05326</guid>
<content:encoded><![CDATA[
arXiv:2509.05326v1 Announce Type: cross 
Abstract: Modern zero-knowledge proof (ZKP) systems, essential for privacy and verifiable computation, suffer from a fundamental limitation: the prover typically uses memory that scales linearly with the computation's trace length T, making them impractical for resource-constrained devices and prohibitively expensive for large-scale tasks. This paper overcomes this barrier by constructing, to our knowledge, the first sublinear-space ZKP prover. Our core contribution is an equivalence that reframes proof generation as an instance of the classic Tree Evaluation problem. Leveraging a recent space-efficient tree-evaluation algorithm, we design a streaming prover that assembles the proof without ever materializing the full execution trace. The approach reduces prover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms) while preserving proof size, verifier time, and the transcript/security guarantees of the underlying system. This enables a shift from specialized, server-bound proving to on-device proving, opening applications in decentralized systems, on-device machine learning, and privacy-preserving technologies.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optical Music Recognition of Jazz Lead Sheets</title>
<link>https://arxiv.org/abs/2509.05329</link>
<guid>https://arxiv.org/abs/2509.05329</guid>
<content:encoded><![CDATA[
arXiv:2509.05329v1 Announce Type: cross 
Abstract: In this paper, we address the challenge of Optical Music Recognition (OMR) for handwritten jazz lead sheets, a widely used musical score type that encodes melody and chords. The task is challenging due to the presence of chords, a score component not handled by existing OMR systems, and the high variability and quality issues associated with handwritten images. Our contribution is two-fold. We present a novel dataset consisting of 293 handwritten jazz lead sheets of 163 unique pieces, amounting to 2021 total staves aligned with Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score images generated from the ground truth. The second contribution is the development of an OMR model for jazz lead sheets. We discuss specific tokenisation choices related to our kind of data, and the advantages of using synthetic scores and pretrained models. We publicly release all code, data, and models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForensicsData: A Digital Forensics Dataset for Large Language Models</title>
<link>https://arxiv.org/abs/2509.05331</link>
<guid>https://arxiv.org/abs/2509.05331</guid>
<content:encoded><![CDATA[
arXiv:2509.05331v1 Announce Type: cross 
Abstract: The growing complexity of cyber incidents presents significant challenges for digital forensic investigators, especially in evidence collection and analysis. Public resources are still limited because of ethical, legal, and privacy concerns, even though realistic datasets are necessary to support research and tool developments. To address this gap, we introduce ForensicsData, an extensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware analysis reports. It consists of more than 5,000 Q-C-A triplets. A unique workflow was used to create the dataset, which extracts structured data, uses large language models (LLMs) to transform it into Q-C-A format, and then uses a specialized evaluation process to confirm its quality. Among the models evaluated, Gemini 2 Flash demonstrated the best performance in aligning generated content with forensic terminology. ForensicsData aims to advance digital forensics by enabling reproducible experiments and fostering collaboration within the research community.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2509.05332</link>
<guid>https://arxiv.org/abs/2509.05332</guid>
<content:encoded><![CDATA[
arXiv:2509.05332v1 Announce Type: cross 
Abstract: Autonomous vehicles (AVs) rely on complex perception and communication systems, making them vulnerable to adversarial attacks that can compromise safety. While simulation offers a scalable and safe environment for robustness testing, existing frameworks typically lack comprehensive supportfor modeling multi-domain adversarial scenarios. This paper introduces a novel, open-source integrated simulation framework designed to generate adversarial attacks targeting both perception and communication layers of AVs. The framework provides high-fidelity modeling of physical environments, traffic dynamics, and V2X networking, orchestrating these components through a unified core that synchronizes multiple simulators based on a single configuration file. Our implementation supports diverse perception-level attacks on LiDAR sensor data, along with communication-level threats such as V2X message manipulation and GPS spoofing. Furthermore, ROS 2 integration ensures seamless compatibility with third-party AV software stacks. We demonstrate the framework's effectiveness by evaluating the impact of generated adversarial scenarios on a state-of-the-art 3D object detector, revealing significant performance degradation under realistic conditions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness</title>
<link>https://arxiv.org/abs/2509.05333</link>
<guid>https://arxiv.org/abs/2509.05333</guid>
<content:encoded><![CDATA[
arXiv:2509.05333v1 Announce Type: cross 
Abstract: Real world deployments often expose modern object recognition models to domain shifts that precipitate a severe drop in accuracy. Such shifts encompass (i) variations in low level image statistics, (ii) changes in object pose and viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent classes. To mitigate this degradation, we introduce the Re-Thinking Vision Language Model (RT-VLM) framework. The foundation of this framework is a unique synthetic dataset generation pipeline that produces images annotated with "4-Clues": precise bounding boxes, class names, detailed object-level captions, and a comprehensive context-level caption for the entire scene. We then perform parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this resource. At inference time, a two stage Re-Thinking scheme is executed: the model first emits its own four clues, then re examines these responses as evidence and iteratively corrects them. Across robustness benchmarks that isolate individual domain shifts, RT-VLM consistently surpasses strong baselines. These findings indicate that the integration of structured multimodal evidence with an explicit self critique loop constitutes a promising route toward reliable and transferable visual understanding.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plantbot: Integrating Plant and Robot through LLM Modular Agent Networks</title>
<link>https://arxiv.org/abs/2509.05338</link>
<guid>https://arxiv.org/abs/2509.05338</guid>
<content:encoded><![CDATA[
arXiv:2509.05338v1 Announce Type: cross 
Abstract: We introduce Plantbot, a hybrid lifeform that connects a living plant with a mobile robot through a network of large language model (LLM) modules. Each module - responsible for sensing, vision, dialogue, or action - operates asynchronously and communicates via natural language, enabling seamless interaction across biological and artificial domains. This architecture leverages the capacity of LLMs to serve as hybrid interfaces, where natural language functions as a universal protocol, translating multimodal data (soil moisture, temperature, visual context) into linguistic messages that coordinate system behaviors. The integrated network transforms plant states into robotic actions, installing normativity essential for agency within the sensor-motor loop. By combining biological and robotic elements through LLM-mediated communication, Plantbot behaves as an embodied, adaptive agent capable of responding autonomously to environmental conditions. This approach suggests possibilities for a new model of artificial life, where decentralized, LLM modules coordination enable novel interactions between biological and artificial systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging</title>
<link>https://arxiv.org/abs/2509.05340</link>
<guid>https://arxiv.org/abs/2509.05340</guid>
<content:encoded><![CDATA[
arXiv:2509.05340v1 Announce Type: cross 
Abstract: Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a pivotal challenge in medical image analysis due to the heterogeneous nature of tumor morphology and intensity distributions. Accurate delineation of tumor boundaries is critical for clinical decision-making, radiotherapy planning, and longitudinal disease monitoring. In this study, we perform a comprehensive comparative analysis of two major clustering paradigms applied in MRI tumor segmentation: hard clustering, exemplified by the K-Means algorithm, and soft clustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each pixel strictly to a single cluster, FCM introduces partial memberships, meaning each pixel can belong to multiple clusters with varying degrees of association. Experimental validation was performed using the BraTS2020 dataset, incorporating pre-processing through Gaussian filtering and Contrast Limited Adaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice Similarity Coefficient (DSC) and processing time, which collectively demonstrated that K-Means achieved superior speed with an average runtime of 0.3s per image, whereas FCM attained higher segmentation accuracy with an average DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher computational cost (1.3s per image). These results highlight the inherent trade-off between computational efficiency and boundary precision.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Instance Segmentation with Superpixels</title>
<link>https://arxiv.org/abs/2509.05352</link>
<guid>https://arxiv.org/abs/2509.05352</guid>
<content:encoded><![CDATA[
arXiv:2509.05352v1 Announce Type: cross 
Abstract: Instance segmentation is essential for numerous computer vision applications, including robotics, human-computer interaction, and autonomous driving. Currently, popular models bring impressive performance in instance segmentation by training with a large number of human annotations, which are costly to collect. For this reason, we present a new framework that efficiently and effectively segments objects without the need for human annotations. Firstly, a MultiCut algorithm is applied to self-supervised features for coarse mask segmentation. Then, a mask filter is employed to obtain high-quality coarse masks. To train the segmentation network, we compute a novel superpixel-guided mask loss, comprising hard loss and soft loss, with high-quality coarse masks and superpixels segmented from low-level image features. Lastly, a self-training process with a new adaptive loss is proposed to improve the quality of predicted masks. We conduct experiments on public datasets in instance segmentation and object detection to demonstrate the effectiveness of the proposed framework. The results show that the proposed framework outperforms previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spiking Neural Networks for Continuous Control via End-to-End Model-Based Learning</title>
<link>https://arxiv.org/abs/2509.05356</link>
<guid>https://arxiv.org/abs/2509.05356</guid>
<content:encoded><![CDATA[
arXiv:2509.05356v1 Announce Type: cross 
Abstract: Despite recent progress in training spiking neural networks (SNNs) for classification, their application to continuous motor control remains limited. Here, we demonstrate that fully spiking architectures can be trained end-to-end to control robotic arms with multiple degrees of freedom in continuous environments. Our predictive-control framework combines Leaky Integrate-and-Fire dynamics with surrogate gradients, jointly optimizing a forward model for dynamics prediction and a policy network for goal-directed action. We evaluate this approach on both a planar 2D reaching task and a simulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve stable training and accurate torque control, establishing their viability for high-dimensional motor tasks. An extensive ablation study highlights the role of initialization, learnable time constants, and regularization in shaping training dynamics. We conclude that while stable and effective control can be achieved, recurrent spiking networks remain highly sensitive to hyperparameter settings, underscoring the importance of principled design choices.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training</title>
<link>https://arxiv.org/abs/2509.05359</link>
<guid>https://arxiv.org/abs/2509.05359</guid>
<content:encoded><![CDATA[
arXiv:2509.05359v1 Announce Type: cross 
Abstract: This paper investigates discrete unit representations in Speech Language Models (SLMs), focusing on optimizing speech modeling during continual pre-training. In this paper, we systematically examine how model architecture, data representation, and training robustness influence the pre-training stage in which we adapt existing pre-trained language models to the speech modality. Our experiments highlight the role of speech encoders and clustering granularity across different model scales, showing how optimal discretization strategies vary with model capacity. By examining cluster distribution and phonemic alignments, we investigate the effective use of discrete vocabulary, uncovering both linguistic and paralinguistic patterns. Additionally, we explore the impact of clustering data selection on model robustness, highlighting the importance of domain matching between discretization training and target applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governing AI R&amp;D: A Legal Framework for Constraining Dangerous AI</title>
<link>https://arxiv.org/abs/2509.05361</link>
<guid>https://arxiv.org/abs/2509.05361</guid>
<content:encoded><![CDATA[
arXiv:2509.05361v1 Announce Type: cross 
Abstract: As AI advances, governing its development may become paramount to public safety. Lawmakers may seek to restrict the development and release of AI models or of AI research itself. These governance actions could trigger legal challenges that invalidate the actions, so lawmakers should consider these challenges ahead of time. We investigate three classes of potential litigation risk for AI regulation in the U.S.: the First Amendment, administrative law, and the Fourteenth Amendment. We discuss existing precedent that is likely to apply to AI, which legal challenges are likely to arise, and how lawmakers might preemptively address them. Effective AI regulation is possible, but it requires careful implementation to avoid these legal challenges.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning</title>
<link>https://arxiv.org/abs/2509.05362</link>
<guid>https://arxiv.org/abs/2509.05362</guid>
<content:encoded><![CDATA[
arXiv:2509.05362v1 Announce Type: cross 
Abstract: Scams exploiting real-time social engineering -- such as phishing, impersonation, and phone fraud -- remain a persistent and evolving threat across digital platforms. Existing defenses are largely reactive, offering limited protection during active interactions. We propose a privacy-preserving, AI-in-the-loop framework that proactively detects and disrupts scam conversations in real time. The system combines instruction-tuned artificial intelligence with a safety-aware utility function that balances engagement with harm minimization, and employs federated learning to enable continual model updates without raw data sharing. Experimental evaluations show that the system produces fluent and engaging responses (perplexity as low as 22.3, engagement $\approx$0.80), while human studies confirm significant gains in realism, safety, and effectiveness over strong baselines. In federated settings, models trained with FedAvg sustain up to 30 rounds while preserving high engagement ($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage ($\leq$0.0085). Even with differential privacy, novelty and safety remain stable, indicating that robust privacy can be achieved without sacrificing performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3, MD-Judge) shows a straightforward pattern: stricter moderation settings reduce the chance of exposing personal information, but they also limit how much the model engages in conversation. In contrast, more relaxed settings allow longer and richer interactions, which improve scam detection, but at the cost of higher privacy risk. To our knowledge, this is the first framework to unify real-time scam-baiting, federated privacy preservation, and calibrated safety moderation into a proactive defense paradigm.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prototyping an AI-powered Tool for Energy Efficiency in New Zealand Homes</title>
<link>https://arxiv.org/abs/2509.05364</link>
<guid>https://arxiv.org/abs/2509.05364</guid>
<content:encoded><![CDATA[
arXiv:2509.05364v1 Announce Type: cross 
Abstract: Residential buildings contribute significantly to energy use, health outcomes, and carbon emissions. In New Zealand, housing quality has historically been poor, with inadequate insulation and inefficient heating contributing to widespread energy hardship. Recent reforms, including the Warmer Kiwi Homes program, Healthy Homes Standards, and H1 Building Code upgrades, have delivered health and comfort improvements, yet challenges persist. Many retrofits remain partial, data on household performance are limited, and decision-making support for homeowners is fragmented. This study presents the design and evaluation of an AI-powered decision-support tool for residential energy efficiency in New Zealand. The prototype, developed using Python and Streamlit, integrates data ingestion, anomaly detection, baseline modeling, and scenario simulation (e.g., LED retrofits, insulation upgrades) into a modular dashboard. Fifteen domain experts, including building scientists, consultants, and policy practitioners, tested the tool through semi-structured interviews. Results show strong usability (M = 4.3), high value of scenario outputs (M = 4.5), and positive perceptions of its potential to complement subsidy programs and regulatory frameworks. The tool demonstrates how AI can translate national policies into personalized, household-level guidance, bridging the gap between funding, standards, and practical decision-making. Its significance lies in offering a replicable framework for reducing energy hardship, improving health outcomes, and supporting climate goals. Future development should focus on carbon metrics, tariff modeling, integration with national datasets, and longitudinal trials to assess real-world adoption.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs</title>
<link>https://arxiv.org/abs/2509.05367</link>
<guid>https://arxiv.org/abs/2509.05367</guid>
<content:encoded><![CDATA[
arXiv:2509.05367v1 Announce Type: cross 
Abstract: Large language models (LLMs) have undergone safety alignment efforts to mitigate harmful outputs. However, as LLMs become more sophisticated in reasoning, their intelligence may introduce new security risks. While traditional jailbreak attacks relied on singlestep attacks, multi-turn jailbreak strategies that adapt dynamically to context remain underexplored. In this work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack Logic), a framework that leverages LLMs ethical reasoning to bypass their safeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on the trolley problem. TRIAL demonstrates high jailbreak success rates towards both open and close-source models. Our findings underscore a fundamental limitation in AI safety: as models gain advanced reasoning abilities, the nature of their alignment may inadvertently allow for more covert security vulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating safety alignment oversight strategies, as current safeguards may prove insufficient against context-aware adversarial attack.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Horizon Visual Imitation Learning via Plan and Code Reflection</title>
<link>https://arxiv.org/abs/2509.05368</link>
<guid>https://arxiv.org/abs/2509.05368</guid>
<content:encoded><![CDATA[
arXiv:2509.05368v1 Announce Type: cross 
Abstract: Learning from long-horizon demonstrations with complex action sequences presents significant challenges for visual imitation learning, particularly in understanding temporal relationships of actions and spatial relationships between objects. In this paper, we propose a new agent framework that incorporates two dedicated reflection modules to enhance both plan and code generation. The plan generation module produces an initial action sequence, which is then verified by the plan reflection module to ensure temporal coherence and spatial alignment with the demonstration video. The code generation module translates the plan into executable code, while the code reflection module verifies and refines the generated code to ensure correctness and consistency with the generated plan. These two reflection modules jointly enable the agent to detect and correct errors in both the plan generation and code generation, improving performance in tasks with intricate temporal and spatial dependencies. To support systematic evaluation, we introduce LongVILBench, a benchmark comprising 300 human demonstrations with action sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial complexity across multiple task types. Experimental results demonstrate that existing methods perform poorly on this benchmark, whereas our new framework establishes a strong baseline for long-horizon visual imitation learning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments</title>
<link>https://arxiv.org/abs/2509.05376</link>
<guid>https://arxiv.org/abs/2509.05376</guid>
<content:encoded><![CDATA[
arXiv:2509.05376v1 Announce Type: cross 
Abstract: Eye-tracking technology can aid in understanding neurodevelopmental disorders and tracing a person's identity. However, this technology poses a significant risk to privacy, as it captures sensitive information about individuals and increases the likelihood that data can be traced back to them. This paper proposes a human-centered framework designed to prevent identity backtracking while preserving the pedagogical benefits of AI-powered eye tracking in interactive learning environments. We explore how real-time data anonymization, ethical design principles, and regulatory compliance (such as GDPR) can be integrated to build trust and transparency. We first demonstrate the potential for backtracking student IDs and diagnoses in various scenarios using serious game-based eye-tracking data. We then provide a two-stage privacy-preserving framework that prevents participants from being tracked while still enabling diagnostic classification. The first phase covers four scenarios: I) Predicting disorder diagnoses based on different game levels. II) Predicting student IDs based on different game levels. III) Predicting student IDs based on randomized data. IV) Utilizing K-Means for out-of-sample data. In the second phase, we present a two-stage framework that preserves privacy. We also employ Federated Learning (FL) across multiple clients, incorporating a secure identity management system with dummy IDs and administrator-only access controls. In the first phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63% accuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully identifying and assigning a new student ID in scenario 4. In phase 2, we effectively prevented backtracking and established a secure identity management system with dummy IDs and administrator-only access controls, achieving an overall accuracy of 99.40%.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling</title>
<link>https://arxiv.org/abs/2509.05379</link>
<guid>https://arxiv.org/abs/2509.05379</guid>
<content:encoded><![CDATA[
arXiv:2509.05379v1 Announce Type: cross 
Abstract: As our cities and communities become smarter, the systems that keep us safe, such as traffic control centers, emergency response networks, and public transportation, also become more complex. With this complexity comes a greater risk of security threats that can affect not just machines but real people's lives. To address this challenge, we present ThreatGPT, an agentic Artificial Intelligence (AI) assistant built to help people whether they are engineers, safety officers, or policy makers to understand and analyze threats in public safety systems. Instead of requiring deep cybersecurity expertise, it allows users to simply describe the components of a system they are concerned about, such as login systems, data storage, or communication networks. Then, with the click of a button, users can choose how they want the system to be analyzed by using popular frameworks such as STRIDE, MITRE ATT&amp;CK, CVE reports, NIST, or CISA. ThreatGPT is unique because it does not just provide threat information, but rather it acts like a knowledgeable partner. Using few-shot learning, the AI learns from examples and generates relevant smart threat models. It can highlight what might go wrong, how attackers could take advantage, and what can be done to prevent harm. Whether securing a city's infrastructure or a local health service, this tool adapts to users' needs. In simple terms, ThreatGPT brings together AI and human judgment to make our public systems safer. It is designed not just to analyze threats, but to empower people to understand and act on them, faster, smarter, and with more confidence.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cumplimiento del Reglamento (UE) 2024/1689 en rob\'otica y sistemas aut\'onomos: una revisi\'on sistem\'atica de la literatura</title>
<link>https://arxiv.org/abs/2509.05380</link>
<guid>https://arxiv.org/abs/2509.05380</guid>
<content:encoded><![CDATA[
arXiv:2509.05380v1 Announce Type: cross 
Abstract: This systematic literature review analyzes the current state of compliance with Regulation (EU) 2024/1689 in autonomous robotic systems, focusing on cybersecurity frameworks and methodologies. Using the PRISMA protocol, 22 studies were selected from 243 initial records across IEEE Xplore, ACM DL, Scopus, and Web of Science. Findings reveal partial regulatory alignment: while progress has been made in risk management and encrypted communications, significant gaps persist in explainability modules, real-time human oversight, and knowledge base traceability. Only 40% of reviewed solutions explicitly address transparency requirements, and 30% implement failure intervention mechanisms. The study concludes that modular approaches integrating risk, supervision, and continuous auditing are essential to meet the AI Act mandates in autonomous robotics.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Privacy and Large Language Models: An Analysis of Frontier Developers' Privacy Policies</title>
<link>https://arxiv.org/abs/2509.05382</link>
<guid>https://arxiv.org/abs/2509.05382</guid>
<content:encoded><![CDATA[
arXiv:2509.05382v1 Announce Type: cross 
Abstract: Hundreds of millions of people now regularly interact with large language models via chatbots. Model developers are eager to acquire new sources of high-quality training data as they race to improve model capabilities and win market share. This paper analyzes the privacy policies of six U.S. frontier AI developers to understand how they use their users' chats to train models. Drawing primarily on the California Consumer Privacy Act, we develop a novel qualitative coding schema that we apply to each developer's relevant privacy policies to compare data collection and use practices across the six companies. We find that all six developers appear to employ their users' chat data to train and improve their models by default, and that some retain this data indefinitely. Developers may collect and train on personal information disclosed in chats, including sensitive information such as biometric and health data, as well as files uploaded by users. Four of the six companies we examined appear to include children's chat data for model training, as well as customer data from other products. On the whole, developers' privacy policies often lack essential information about their practices, highlighting the need for greater transparency and accountability. We address the implications of users' lack of consent for the use of their chat data for model training, data security issues arising from indefinite chat data retention, and training on children's chat data. We conclude by providing recommendations to policymakers and developers to address the data privacy challenges posed by LLM-powered chatbots.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs</title>
<link>https://arxiv.org/abs/2509.05385</link>
<guid>https://arxiv.org/abs/2509.05385</guid>
<content:encoded><![CDATA[
arXiv:2509.05385v1 Announce Type: cross 
Abstract: Large language models are unable to continuously adapt and learn from new data during reasoning at inference time. To address this limitation, we propose that complex reasoning tasks be decomposed into atomic subtasks and introduce SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive updates during reasoning at inference time. SAGE consists of three key components: (1) a Trigger module that detects reasoning failures through multiple evaluation metrics in real time; (2) a Trigger Buffer module that clusters anomaly samples using a streaming clustering process with HDBSCAN, followed by stability checks and similarity-based merging; and (3) a Lora Store module that dynamically optimizes parameter updates with an adapter pool for knowledge retention. Evaluation results show that SAGE demonstrates excellent accuracy, robustness, and stability on the atomic reasoning subtask through dynamic knowledge updating during test time.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmented Structure Preserving Neural Networks for cell biomechanics</title>
<link>https://arxiv.org/abs/2509.05388</link>
<guid>https://arxiv.org/abs/2509.05388</guid>
<content:encoded><![CDATA[
arXiv:2509.05388v1 Announce Type: cross 
Abstract: Cell biomechanics involve a great number of complex phenomena that are fundamental to the evolution of life itself and other associated processes, ranging from the very early stages of embryo-genesis to the maintenance of damaged structures or the growth of tumors. Given the importance of such phenomena, increasing research has been dedicated to their understanding, but the many interactions between them and their influence on the decisions of cells as a collective network or cluster remain unclear. We present a new approach that combines Structure Preserving Neural Networks, which study cell movements as a purely mechanical system, with other Machine Learning tools (Artificial Neural Networks), which allow taking into consideration environmental factors that can be directly deduced from an experiment with Computer Vision techniques. This new model, tested on simulated and real cell migration cases, predicts complete cell trajectories following a roll-out policy with a high level of accuracy. This work also includes a mitosis event prediction model based on Neural Networks architectures which makes use of the same observed features.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Authorship Without Writing: Large Language Models and the Senior Author Analogy</title>
<link>https://arxiv.org/abs/2509.05390</link>
<guid>https://arxiv.org/abs/2509.05390</guid>
<content:encoded><![CDATA[
arXiv:2509.05390v1 Announce Type: cross 
Abstract: The use of large language models (LLMs) in bioethical, scientific, and medical writing remains controversial. While there is broad agreement in some circles that LLMs cannot count as authors, there is no consensus about whether and how humans using LLMs can count as authors. In many fields, authorship is distributed among large teams of researchers, some of whom, including paradigmatic senior authors who guide and determine the scope of a project and ultimately vouch for its integrity, may not write a single word. In this paper, we argue that LLM use (under specific conditions) is analogous to a form of senior authorship. On this view, the use of LLMs, even to generate complete drafts of research papers, can be considered a legitimate form of authorship according to the accepted criteria in many fields. We conclude that either such use should be recognized as legitimate, or current criteria for authorship require fundamental revision. AI use declaration: GPT-5 was used to help format Box 1. AI was not used for any other part of the preparation or writing of this manuscript.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Optimized Pipeline for Automatic Educational Knowledge Graph Construction</title>
<link>https://arxiv.org/abs/2509.05392</link>
<guid>https://arxiv.org/abs/2509.05392</guid>
<content:encoded><![CDATA[
arXiv:2509.05392v1 Announce Type: cross 
Abstract: The automatic construction of Educational Knowledge Graphs (EduKGs) is essential for domain knowledge modeling by extracting meaningful representations from learning materials. Despite growing interest, identifying a scalable and reliable approach for automatic EduKG generation remains a challenge. In an attempt to develop a unified and robust pipeline for automatic EduKG construction, in this study we propose a pipeline for automatic EduKG construction from PDF learning materials. The process begins with generating slide-level EduKGs from individual pages/slides, which are then merged to form a comprehensive EduKG representing the entire learning material. We evaluate the accuracy of the EduKG generated from the proposed pipeline in our MOOC platform, CourseMapper. The observed accuracy, while indicative of partial success, is relatively low particularly in the educational context, where the reliability of knowledge representations is critical for supporting meaningful learning. To address this, we introduce targeted optimizations across multiple pipeline components. The optimized pipeline achieves a 17.5% improvement in accuracy and a tenfold increase in processing efficiency. Our approach offers a holistic, scalable and end-to-end pipeline for automatic EduKG construction, adaptable to diverse educational contexts, and supports improved semantic representation of learning content.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring Prerequisite Knowledge Concepts in Educational Knowledge Graphs: A Multi-criteria Approach</title>
<link>https://arxiv.org/abs/2509.05393</link>
<guid>https://arxiv.org/abs/2509.05393</guid>
<content:encoded><![CDATA[
arXiv:2509.05393v1 Announce Type: cross 
Abstract: Educational Knowledge Graphs (EduKGs) organize various learning entities and their relationships to support structured and adaptive learning. Prerequisite relationships (PRs) are critical in EduKGs for defining the logical order in which concepts should be learned. However, the current EduKG in the MOOC platform CourseMapper lacks explicit PR links, and manually annotating them is time-consuming and inconsistent. To address this, we propose an unsupervised method for automatically inferring concept PRs without relying on labeled data. We define ten criteria based on document-based, Wikipedia hyperlink-based, graph-based, and text-based features, and combine them using a voting algorithm to robustly capture PRs in educational content. Experiments on benchmark datasets show that our approach achieves higher precision than existing methods while maintaining scalability and adaptability, thus providing reliable support for sequence-aware learning in CourseMapper.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse Browser: Vector-Image-to-Code Generator</title>
<link>https://arxiv.org/abs/2509.05394</link>
<guid>https://arxiv.org/abs/2509.05394</guid>
<content:encoded><![CDATA[
arXiv:2509.05394v1 Announce Type: cross 
Abstract: Automating the conversion of user interface design into code (image-to-code or image-to-UI) is an active area of software engineering research. However, the state-of-the-art solutions do not achieve high fidelity to the original design, as evidenced by benchmarks. In this work, I approach the problem differently: I use vector images instead of bitmaps as model input. I create several large datasets for training machine learning models. I evaluate the available array of Image Quality Assessment (IQA) algorithms and introduce a new, multi-scale metric. I then train a large open-weights model and discuss its limitations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2509.05396</link>
<guid>https://arxiv.org/abs/2509.05396</guid>
<content:encoded><![CDATA[
arXiv:2509.05396v1 Announce Type: cross 
Abstract: While multi-agent debate has been proposed as a promising strategy for improving AI reasoning ability, we find that debate can sometimes be harmful rather than helpful. The prior work has exclusively focused on debates within homogeneous groups of agents, whereas we explore how diversity in model capabilities influences the dynamics and outcomes of multi-agent interactions. Through a series of experiments, we demonstrate that debate can lead to a decrease in accuracy over time -- even in settings where stronger (i.e., more capable) models outnumber their weaker counterparts. Our analysis reveals that models frequently shift from correct to incorrect answers in response to peer reasoning, favoring agreement over challenging flawed reasoning. These results highlight important failure modes in the exchange of reasons during multi-agent debate, suggesting that naive applications of debate may cause performance degradation when agents are neither incentivized nor adequately equipped to resist persuasive but incorrect reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Connectionist Temporal Classification for Phoneme Recognition</title>
<link>https://arxiv.org/abs/2509.05399</link>
<guid>https://arxiv.org/abs/2509.05399</guid>
<content:encoded><![CDATA[
arXiv:2509.05399v1 Announce Type: cross 
Abstract: Automatic Phoneme Recognition (APR) systems are often trained using pseudo phoneme-level annotations generated from text through Grapheme-to-Phoneme (G2P) systems. These G2P systems frequently output multiple possible pronunciations per word, but the standard Connectionist Temporal Classification (CTC) loss cannot account for such ambiguity during training. In this work, we adapt Graph Temporal Classification (GTC) to the APR setting. GTC enables training from a graph of alternative phoneme sequences, allowing the model to consider multiple pronunciations per word as valid supervision. Our experiments on English and Dutch data sets show that incorporating multiple pronunciations per word into the training loss consistently improves phoneme error rates compared to a baseline trained with CTC. These results suggest that integrating pronunciation variation into the loss function is a promising strategy for training APR systems from noisy G2P-based supervision.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universality of physical neural networks with multivariate nonlinearity</title>
<link>https://arxiv.org/abs/2509.05420</link>
<guid>https://arxiv.org/abs/2509.05420</guid>
<content:encoded><![CDATA[
arXiv:2509.05420v1 Announce Type: cross 
Abstract: The enormous energy demand of artificial intelligence is driving the development of alternative hardware for deep learning. Physical neural networks try to exploit physical systems to perform machine learning more efficiently. In particular, optical systems can calculate with light using negligible energy. While their computational capabilities were long limited by the linearity of optical materials, nonlinear computations have recently been demonstrated through modified input encoding. Despite this breakthrough, our inability to determine if physical neural networks can learn arbitrary relationships between data -- a key requirement for deep learning known as universality -- hinders further progress. Here we present a fundamental theorem that establishes a universality condition for physical neural networks. It provides a powerful mathematical criterion that imposes device constraints, detailing how inputs should be encoded in the tunable parameters of the physical system. Based on this result, we propose a scalable architecture using free-space optics that is provably universal and achieves high accuracy on image classification tasks. Further, by combining the theorem with temporal multiplexing, we present a route to potentially huge effective system sizes in highly practical but poorly scalable on-chip photonic devices. Our theorem and scaling methods apply beyond optical systems and inform the design of a wide class of universal, energy-efficient physical neural networks, justifying further efforts in their development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Translation Needed: Forecasting Quality from Fertility and Metadata</title>
<link>https://arxiv.org/abs/2509.05425</link>
<guid>https://arxiv.org/abs/2509.05425</guid>
<content:encoded><![CDATA[
arXiv:2509.05425v1 Announce Type: cross 
Abstract: We show that translation quality can be predicted with surprising accuracy \textit{without ever running the translation system itself}. Using only a handful of features, token fertility ratios, token counts, and basic linguistic metadata (language family, script, and region), we can forecast ChrF scores for GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient boosting models achieve favorable performance ($R^{2}=0.66$ for XX$\rightarrow$English and $R^{2}=0.72$ for English$\rightarrow$XX). Feature importance analyses reveal that typological factors dominate predictions into English, while fertility plays a larger role for translations into diverse target languages. These findings suggest that translation quality is shaped by both token-level fertility and broader linguistic typology, offering new insights for multilingual evaluation and quality estimation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding</title>
<link>https://arxiv.org/abs/2509.05431</link>
<guid>https://arxiv.org/abs/2509.05431</guid>
<content:encoded><![CDATA[
arXiv:2509.05431v1 Announce Type: cross 
Abstract: Brain tumor segmentation is a critical pre-processing step in the medical image analysis pipeline that involves precise delineation of tumor regions from healthy brain tissue in medical imaging data, particularly MRI scans. An efficient and effective decoding mechanism is crucial in brain tumor segmentation especially in scenarios with limited computational resources. However these decoding mechanisms usually come with high computational costs. To address this concern EMCAD a new efficient multi-scale convolutional attention decoder designed was utilized to optimize both performance and computational efficiency for brain tumor segmentation on the BraTs2020 dataset consisting of MRI scans from 369 brain tumor patients. The preliminary result obtained by the model achieved a best Dice score of 0.31 and maintained a stable mean Dice score of 0.285 plus/minus 0.015 throughout the training process which is moderate. The initial model maintained consistent performance across the validation set without showing signs of over-fitting.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too</title>
<link>https://arxiv.org/abs/2509.05440</link>
<guid>https://arxiv.org/abs/2509.05440</guid>
<content:encoded><![CDATA[
arXiv:2509.05440v1 Announce Type: cross 
Abstract: As large-language models have been increasingly used as automatic raters for evaluating free-form content, including document summarization, dialog, and story generation, work has been dedicated to evaluating such models by measuring their correlations with human judgment. For \textit{sample-level} performance, methods which operate by using pairwise comparisons between machine-generated text perform well but often lack the ability to assign absolute scores to individual summaries, an ability crucial for use cases that require thresholding. In this work, we propose a direct-scoring method which uses synthetic summaries to act as pairwise machine rankings at test time. We show that our method performs comparably to state-of-the-art pairwise evaluators in terms of axis-averaged sample-level correlations on the SummEval (\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05}) meta-evaluation benchmarks, and release the synthetic in-context summaries as data to facilitate future work.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Newton to Einstein: Axiom-Based Discovery via Game Design</title>
<link>https://arxiv.org/abs/2509.05448</link>
<guid>https://arxiv.org/abs/2509.05448</guid>
<content:encoded><![CDATA[
arXiv:2509.05448v1 Announce Type: cross 
Abstract: This position paper argues that machine learning for scientific discovery should shift from inductive pattern recognition to axiom-based reasoning. We propose a game design framework in which scientific inquiry is recast as a rule-evolving system: agents operate within environments governed by axioms and modify them to explain outlier observations. Unlike conventional ML approaches that operate within fixed assumptions, our method enables the discovery of new theoretical structures through systematic rule adaptation. We demonstrate the feasibility of this approach through preliminary experiments in logic-based games, showing that agents can evolve axioms that solve previously unsolvable problems. This framework offers a foundation for building machine learning systems capable of creative, interpretable, and theory-driven discovery.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis</title>
<link>https://arxiv.org/abs/2509.05449</link>
<guid>https://arxiv.org/abs/2509.05449</guid>
<content:encoded><![CDATA[
arXiv:2509.05449v1 Announce Type: cross 
Abstract: Membership inference attacks (MIAs) reveal whether specific data was used to train machine learning models, serving as important tools for privacy auditing and compliance assessment. Recent studies have reported that MIAs perform only marginally better than random guessing against large language models, suggesting that modern pre-training approaches with massive datasets may be free from privacy leakage risks. Our work offers a complementary perspective to these findings by exploring how examining LLMs' internal representations, rather than just their outputs, may provide additional insights into potential membership inference signals. Our framework, \emph{memTrace}, follows what we call \enquote{neural breadcrumbs} extracting informative signals from transformer hidden states and attention patterns as they process candidate sequences. By analyzing layer-wise representation dynamics, attention distribution characteristics, and cross-layer transition patterns, we detect potential memorization fingerprints that traditional loss-based approaches may not capture. This approach yields strong membership detection across several model families achieving average AUC scores of 0.85 on popular MIA benchmarks. Our findings suggest that internal model behaviors can reveal aspects of training data exposure even when output-based signals appear protected, highlighting the need for further research into membership privacy and the development of more robust privacy-preserving training techniques for large language models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models</title>
<link>https://arxiv.org/abs/2509.05471</link>
<guid>https://arxiv.org/abs/2509.05471</guid>
<content:encoded><![CDATA[
arXiv:2509.05471v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly vulnerable to a sophisticated form of adversarial prompting known as camouflaged jailbreaking. This method embeds malicious intent within seemingly benign language to evade existing safety mechanisms. Unlike overt attacks, these subtle prompts exploit contextual ambiguity and the flexible nature of language, posing significant challenges to current defense systems. This paper investigates the construction and impact of camouflaged jailbreak prompts, emphasizing their deceptive characteristics and the limitations of traditional keyword-based detection methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts, containing 500 curated examples (400 harmful and 100 benign prompts) designed to rigorously stress-test LLM safety protocols. In addition, we propose a multi-faceted evaluation framework that measures harmfulness across seven dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards, Harmful Potential, Educational Value, Content Quality, and Compliance Score. Our findings reveal a stark contrast in LLM behavior: while models demonstrate high safety and content quality with benign inputs, they exhibit a significant decline in performance and safety when confronted with camouflaged jailbreak attempts. This disparity underscores a pervasive vulnerability, highlighting the urgent need for more nuanced and adaptive security strategies to ensure the responsible and robust deployment of LLMs in real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Vision to Validation: A Theory- and Data-Driven Construction of a GCC-Specific AI Adoption Index</title>
<link>https://arxiv.org/abs/2509.05474</link>
<guid>https://arxiv.org/abs/2509.05474</guid>
<content:encoded><![CDATA[
arXiv:2509.05474v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is rapidly transforming public-sector processes worldwide, yet standardized measures rarely address the unique drivers, governance models, and cultural nuances of the Gulf Cooperation Council (GCC) countries. This study employs a theory-driven foundation derived from an in-depth analysis of literature review and six National AI Strategies (NASs), coupled with a data-driven approach that utilizes a survey of 203 mid- and senior-level government employees and advanced statistical techniques (K-Means clustering, Principal Component Analysis, and Partial Least Squares Structural Equation Modeling). By combining policy insights with empirical evidence, the research develops and validates a novel AI Adoption Index specifically tailored to the GCC public sector. Findings indicate that robust infrastructure and clear policy mandates exert the strongest influence on successful AI implementations, overshadowing organizational readiness in early adoption stages. The combined model explains 70% of the variance in AI outcomes, suggesting that resource-rich environments and top-down policy directives can drive rapid but uneven technology uptake. By consolidating key dimensions (Infrastructure & Resources, Organizational Readiness, and Policy & Regulatory Environment) into a single composite index, this study provides a holistic yet context-sensitive tool for benchmarking AI maturity. The index offers actionable guidance for policymakers seeking to harmonize large-scale deployments with ethical and regulatory standards. Beyond advancing academic discourse, these insights inform more strategic allocation of resources, cross-country cooperation, and capacity-building initiatives, thereby supporting sustained AI-driven transformation in the GCC region and beyond.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation</title>
<link>https://arxiv.org/abs/2509.05475</link>
<guid>https://arxiv.org/abs/2509.05475</guid>
<content:encoded><![CDATA[
arXiv:2509.05475v1 Announce Type: cross 
Abstract: Autonomous regolith excavation is a cornerstone of in-situ resource utilization for a sustained human presence beyond Earth. However, this task is fundamentally hindered by the complex interaction dynamics of granular media and the operational need for robots to use diverse tools. To address these challenges, this work introduces a framework where a model-based reinforcement learning agent learns within a parallelized simulation. This environment leverages high-fidelity particle physics and procedural generation to create a vast distribution of both lunar terrains and excavation tool geometries. To master this diversity, the agent learns an adaptive interaction strategy by dynamically modulating its own stiffness and damping at each control step through operational space control. Our experiments demonstrate that training with a procedural distribution of tools is critical for generalization and enables the development of sophisticated tool-aware behavior. Furthermore, we show that augmenting the agent with visual feedback significantly improves task success. These results represent a validated methodology for developing the robust and versatile autonomous systems required for the foundational tasks of future space missions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series</title>
<link>https://arxiv.org/abs/2509.05478</link>
<guid>https://arxiv.org/abs/2509.05478</guid>
<content:encoded><![CDATA[
arXiv:2509.05478v1 Announce Type: cross 
Abstract: Multivariate time series (MTS) are ubiquitous in domains such as healthcare, climate science, and industrial monitoring, but their high dimensionality, limited labeled data, and non-stationary nature pose significant challenges for conventional machine learning methods. While recent self-supervised learning (SSL) approaches mitigate label scarcity by data augmentations or time point-based contrastive strategy, they neglect the intrinsic periodic structure of MTS and fail to capture the dynamic evolution of latent states. We propose PLanTS, a periodicity-aware self-supervised learning framework that explicitly models irregular latent states and their transitions. We first designed a period-aware multi-granularity patching mechanism and a generalized contrastive loss to preserve both instance-level and state-level similarities across multiple temporal resolutions. To further capture temporal dynamics, we design a next-transition prediction pretext task that encourages representations to encode predictive information about future state evolution. We evaluate PLanTS across a wide range of downstream tasks-including multi-class and multi-label classification, forecasting, trajectory tracking and anomaly detection. PLanTS consistently improves the representation quality over existing SSL methods and demonstrates superior runtime efficiency compared to DTW-based methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Token Tax: Systematic Bias in Multilingual Tokenization</title>
<link>https://arxiv.org/abs/2509.05486</link>
<guid>https://arxiv.org/abs/2509.05486</guid>
<content:encoded><![CDATA[
arXiv:2509.05486v1 Announce Type: cross 
Abstract: Tokenization inefficiency imposes structural disadvantages on morphologically complex, low-resource languages, inflating compute resources and depressing accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA items; 5 subjects; 16 African languages) and show that fertility (tokens/word) reliably predicts accuracy. Higher fertility consistently predicts lower accuracy across all models and subjects. We further find that reasoning models (DeepSeek, o1) consistently outperform non-reasoning peers across high and low resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in prior generations. Finally, translating token inflation to economics, a doubling in tokens results in quadrupled training cost and time, underscoring the token tax faced by many languages. These results motivate morphologically aware tokenization, fair pricing, and multilingual benchmarks for equitable natural language processing (NLP).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs</title>
<link>https://arxiv.org/abs/2509.05488</link>
<guid>https://arxiv.org/abs/2509.05488</guid>
<content:encoded><![CDATA[
arXiv:2509.05488v1 Announce Type: cross 
Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to limited memory, the lack of native operator support, and the absence of embedded-friendly toolchains. We present, to our knowledge, the first deployment of a Mamba-based neural architecture on a resource-constrained MCU, a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline maps a trained PyTorch Mamba model to on-device execution by (1) exporting model weights into a lightweight format, and (2) implementing a handcrafted Mamba layer and supporting operators in C with operator fusion and memory layout optimization. MambaLite-Micro eliminates large intermediate tensors, reducing 83.0% peak memory, while maintaining an average numerical error of only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on keyword spotting(KWS) and human activity recognition (HAR) tasks, MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully preserving classification accuracy. We further validated portability by deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating consistent operation across heterogeneous embedded platforms and paving the way for bringing advanced sequence models like Mamba to real-world resource-constrained applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures</title>
<link>https://arxiv.org/abs/2509.05490</link>
<guid>https://arxiv.org/abs/2509.05490</guid>
<content:encoded><![CDATA[
arXiv:2509.05490v1 Announce Type: cross 
Abstract: The You Only Look Once (YOLO) architecture is crucial for real-time object detection. However, deploying it in resource-constrained environments such as unmanned aerial vehicles (UAVs) requires efficient transfer learning. Although layer freezing is a common technique, the specific impact of various freezing configurations on contemporary YOLOv8 and YOLOv10 architectures remains unexplored, particularly with regard to the interplay between freezing depth, dataset characteristics, and training dynamics. This research addresses this gap by presenting a detailed analysis of layer-freezing strategies. We systematically investigate multiple freezing configurations across YOLOv8 and YOLOv10 variants using four challenging datasets that represent critical infrastructure monitoring. Our methodology integrates a gradient behavior analysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper insights into training dynamics under different freezing strategies. Our results reveal that there is no universal optimal freezing strategy but, rather, one that depends on the properties of the data. For example, freezing the backbone is effective for preserving general-purpose features, while a shallower freeze is better suited to handling extreme class imbalance. These configurations reduce graphics processing unit (GPU) memory consumption by up to 28% compared to full fine-tuning and, in some cases, achieve mean average precision (mAP@50) scores that surpass those of full fine-tuning. Gradient analysis corroborates these findings, showing distinct convergence patterns for moderately frozen models. Ultimately, this work provides empirical findings and practical guidelines for selecting freezing strategies. It offers a practical, evidence-based approach to balanced transfer learning for object detection in scenarios with limited resources.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Microrobot Vascular Parkour: Analytic Geometry-based Path Planning with Real-time Dynamic Obstacle Avoidance</title>
<link>https://arxiv.org/abs/2509.05500</link>
<guid>https://arxiv.org/abs/2509.05500</guid>
<content:encoded><![CDATA[
arXiv:2509.05500v1 Announce Type: cross 
Abstract: Autonomous microrobots in blood vessels could enable minimally invasive therapies, but navigation is challenged by dense, moving obstacles. We propose a real-time path planning framework that couples an analytic geometry global planner (AGP) with two reactive local escape controllers, one based on rules and one based on reinforcement learning, to handle sudden moving obstacles. Using real-time imaging, the system estimates the positions of the microrobot, obstacles, and targets and computes collision-free motions. In simulation, AGP yields shorter paths and faster planning than weighted A* (WA*), particle swarm optimization (PSO), and rapidly exploring random trees (RRT), while maintaining feasibility and determinism. We extend AGP from 2D to 3D without loss of speed. In both simulations and experiments, the combined global planner and local controllers reliably avoid moving obstacles and reach targets. The average planning time is 40 ms per frame, compatible with 25 fps image acquisition and real-time closed-loop control. These results advance autonomous microrobot navigation and targeted drug delivery in vascular environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2509.05513</link>
<guid>https://arxiv.org/abs/2509.05513</guid>
<content:encoded><![CDATA[
arXiv:2509.05513v1 Announce Type: cross 
Abstract: Egocentric human videos provide scalable demonstrations for imitation learning, but existing corpora often lack either fine-grained, temporally localized action descriptions or dexterous hand annotations. We introduce OpenEgo, a multimodal egocentric manipulation dataset with standardized hand-pose annotations and intention-aligned action primitives. OpenEgo totals 1107 hours across six public datasets, covering 290 manipulation tasks in 600+ environments. We unify hand-pose layouts and provide descriptive, timestamped action primitives. To validate its utility, we train language-conditioned imitation-learning policies to predict dexterous hand trajectories. OpenEgo is designed to lower the barrier to learning dexterous manipulation from egocentric video and to support reproducible research in vision-language-action learning. All resources and instructions will be released at www.openegocentric.com.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining TSL and LLM to Automate REST API Testing: A Comparative Study</title>
<link>https://arxiv.org/abs/2509.05540</link>
<guid>https://arxiv.org/abs/2509.05540</guid>
<content:encoded><![CDATA[
arXiv:2509.05540v1 Announce Type: cross 
Abstract: The effective execution of tests for REST APIs remains a considerable challenge for development teams, driven by the inherent complexity of distributed systems, the multitude of possible scenarios, and the limited time available for test design. Exhaustive testing of all input combinations is impractical, often resulting in undetected failures, high manual effort, and limited test coverage. To address these issues, we introduce RestTSLLM, an approach that uses Test Specification Language (TSL) in conjunction with Large Language Models (LLMs) to automate the generation of test cases for REST APIs. The approach targets two core challenges: the creation of test scenarios and the definition of appropriate input data. The proposed solution integrates prompt engineering techniques with an automated pipeline to evaluate various LLMs on their ability to generate tests from OpenAPI specifications. The evaluation focused on metrics such as success rate, test coverage, and mutation score, enabling a systematic comparison of model performance. The results indicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic), Deepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) - consistently produced robust and contextually coherent REST API tests. Among them, Claude 3.5 Sonnet outperformed all other models across every metric, emerging in this study as the most suitable model for this task. These findings highlight the potential of LLMs to automate the generation of tests based on API specifications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study</title>
<link>https://arxiv.org/abs/2509.05553</link>
<guid>https://arxiv.org/abs/2509.05553</guid>
<content:encoded><![CDATA[
arXiv:2509.05553v1 Announce Type: cross 
Abstract: This research addresses a fundamental question in AI: whether large language models truly understand concepts or simply recognize patterns. The authors propose bidirectional reasoning,the ability to apply transformations in both directions without being explicitly trained on the reverse direction, as a test for genuine understanding. They argue that true comprehension should naturally allow reversibility. For example, a model that can change a variable name like userIndex to i should also be able to infer that i represents a user index without reverse training. The researchers tested current language models and discovered what they term cognitive specialization: when models are fine-tuned on forward tasks, their performance on those tasks improves, but their ability to reason bidirectionally becomes significantly worse. To address this issue, they developed Contrastive Fine-Tuning (CFT), which trains models using three types of examples: positive examples that maintain semantic meaning, negative examples with different semantics, and forward-direction obfuscation examples. This approach aims to develop deeper understanding rather than surface-level pattern recognition and allows reverse capabilities to develop naturally without explicit reverse training. Their experiments demonstrated that CFT successfully achieved bidirectional reasoning, enabling strong reverse performance while maintaining forward task capabilities. The authors conclude that bidirectional reasoning serves both as a theoretical framework for assessing genuine understanding and as a practical training approach for developing more capable AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids</title>
<link>https://arxiv.org/abs/2509.05581</link>
<guid>https://arxiv.org/abs/2509.05581</guid>
<content:encoded><![CDATA[
arXiv:2509.05581v1 Announce Type: cross 
Abstract: We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a custom-built humanoid robot designed for entertainment applications. Unlike traditional humanoids, entertainment robots present unique challenges due to aesthetic-driven design choices. Cosmo embodies these with a disproportionately large head (16% of total mass), limited sensing, and protective shells that considerably restrict movement. To address these challenges, we apply Adversarial Motion Priors (AMP) to enable the robot to learn natural-looking movements while maintaining physical stability. We develop tailored domain randomization techniques and specialized reward structures to ensure safe sim-to-real, protecting valuable hardware components during deployment. Our experiments demonstrate that AMP generates stable standing and walking behaviors despite Cosmo's extreme mass distribution and movement constraints. These results establish a promising direction for robots that balance aesthetic appeal with functional performance, suggesting that learning-based methods can effectively adapt to aesthetic-driven design constraints.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity</title>
<link>https://arxiv.org/abs/2509.05585</link>
<guid>https://arxiv.org/abs/2509.05585</guid>
<content:encoded><![CDATA[
arXiv:2509.05585v1 Announce Type: cross 
Abstract: In the field of software traceability link recovery (TLR), textual similarity has long been regarded as the core criterion. However, in tasks involving natural language and programming language (NL-PL) artifacts, relying solely on textual similarity is limited by their semantic gap. To this end, we conducted a large-scale empirical evaluation across various types of TLR tasks, revealing the limitations of textual similarity in NL-PL scenarios. To address these limitations, we propose an approach that incorporates multiple domain-specific auxiliary strategies, identified through empirical analysis, into two models: the Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based Gemini 2.5 Pro via additional input information. We then evaluated our approach using the widely studied requirements-to-code TLR task, a representative case of NL-PL TLR. Experimental results show that both the multi-strategy HGT and Gemini 2.5 Pro models outperformed their original counterparts without strategy integration. Furthermore, compared to the current state-of-the-art method HGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average F1-score improvements of 3.68% and 8.84%, respectively, across twelve open-source projects, demonstrating the effectiveness of multi-strategy integration in enhancing overall model performance for the requirements-code TLR task.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization</title>
<link>https://arxiv.org/abs/2509.05604</link>
<guid>https://arxiv.org/abs/2509.05604</guid>
<content:encoded><![CDATA[
arXiv:2509.05604v1 Announce Type: cross 
Abstract: Video summarization aims to select keyframes that are visually diverse and can represent the whole story of a given video. Previous approaches have focused on global interlinkability between frames in a video by temporal modeling. However, fine-grained visual entities, such as objects, are also highly related to the main content of the video. Moreover, language-guided video summarization, which has recently been studied, requires a comprehensive linguistic understanding of complex real-world videos. To consider how all the objects are semantically related to each other, this paper regards video summarization as a language-guided spatiotemporal graph modeling problem. We present recursive spatiotemporal graph networks, called VideoGraph, which formulate the objects and frames as nodes of the spatial and temporal graphs, respectively. The nodes in each graph are connected and aggregated with graph edges, representing the semantic relationships between the nodes. To prevent the edges from being configured with visual similarity, we incorporate language queries derived from the video into the graph node representations, enabling them to contain semantic knowledge. In addition, we adopt a recursive strategy to refine initial graphs and correctly classify each frame node as a keyframe. In our experiments, VideoGraph achieves state-of-the-art performance on several benchmarks for generic and query-focused video summarization in both supervised and unsupervised manners. The code is available at https://github.com/park-jungin/videograph.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation</title>
<link>https://arxiv.org/abs/2509.05605</link>
<guid>https://arxiv.org/abs/2509.05605</guid>
<content:encoded><![CDATA[
arXiv:2509.05605v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) require high quality preference datasets to align with human preferences. However, conventional methods for constructing such datasets face significant challenges: reliance on pre-collected instructions often leads to distribution mismatches with target models, while the need for sampling multiple stochastic responses introduces substantial computational overhead. In this work, we explore a paradigm shift by leveraging inherent regulation of LLMs' representation space for efficient and tailored preference dataset construction, named Icon$^{2}$. Specifically, it first extracts layer-wise direction vectors to encode sophisticated human preferences and then uses these vectors to filter self-synthesized instructions based on their inherent consistency. During decoding, bidirectional inherent control is applied to steer token representations, enabling the precise generation of response pairs with clear alignment distinctions. Experimental results demonstrate significant improvements in both alignment and efficiency. Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by up to 48.1%.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints</title>
<link>https://arxiv.org/abs/2509.05608</link>
<guid>https://arxiv.org/abs/2509.05608</guid>
<content:encoded><![CDATA[
arXiv:2509.05608v1 Announce Type: cross 
Abstract: The widespread deployment of LLMs across enterprise services has created a critical security blind spot. Organizations operate multiple LLM services handling billions of queries daily, yet regulatory compliance boundaries prevent these services from sharing threat intelligence about prompt injection attacks, the top security risk for LLMs. When an attack is detected in one service, the same threat may persist undetected in others for months, as privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence system that enables secure sharing of attack fingerprints across compliance boundaries. BinaryShield transforms suspicious prompts through a unique pipeline combining PII redaction, semantic embedding, binary quantization, and randomized response mechanism to potentially generate non-invertible fingerprints that preserve attack patterns while providing privacy. Our evaluations demonstrate that BinaryShield achieves an F1-score of 0.94, significantly outperforming SimHash (0.77), the privacy-preserving baseline, while achieving 64x storage reduction and 38x faster similarity search compared to dense embeddings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning</title>
<link>https://arxiv.org/abs/2509.05614</link>
<guid>https://arxiv.org/abs/2509.05614</guid>
<content:encoded><![CDATA[
arXiv:2509.05614v1 Announce Type: cross 
Abstract: Pruning accelerates compute-bound models by reducing computation. Recently applied to Vision-Language-Action (VLA) models, existing methods prune tokens using only local info from current action, ignoring global context from prior actions, causing >20% success rate drop and limited speedup. We observe high similarity across consecutive actions and propose leveraging both local (current) and global (past) info for smarter token selection. We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context to reduce visual tokens per action; (2) Dynamic pruning at layer level: prunes tokens per layer based on layer-specific importance; (3) Lightweight action-aware controller: classifies actions as coarse/fine-grained (by speed), adjusting pruning aggressiveness since fine-grained actions are pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs. OpenVLA-OFT, with negligible success rate loss.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities</title>
<link>https://arxiv.org/abs/2509.05615</link>
<guid>https://arxiv.org/abs/2509.05615</guid>
<content:encoded><![CDATA[
arXiv:2509.05615v1 Announce Type: cross 
Abstract: Medical multimodal representation learning aims to integrate heterogeneous clinical data into unified patient representations to support predictive modeling, which remains an essential yet challenging task in the medical data mining community. However, real-world medical datasets often suffer from missing modalities due to cost, protocol, or patient-specific constraints. Existing methods primarily address this issue by learning from the available observations in either the raw data space or feature space, but typically neglect the underlying bias introduced by the data acquisition process itself. In this work, we identify two types of biases that hinder model generalization: missingness bias, which results from non-random patterns in modality availability, and distribution bias, which arises from latent confounders that influence both observed features and outcomes. To address these challenges, we perform a structural causal analysis of the data-generating process and propose a unified framework that is compatible with existing direct prediction-based multimodal learning methods. Our method consists of two key components: (1) a missingness deconfounding module that approximates causal intervention based on backdoor adjustment and (2) a dual-branch neural network that explicitly disentangles causal features from spurious correlations. We evaluated our method in real-world public and in-hospital datasets, demonstrating its effectiveness and causal insights.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics</title>
<link>https://arxiv.org/abs/2509.05617</link>
<guid>https://arxiv.org/abs/2509.05617</guid>
<content:encoded><![CDATA[
arXiv:2509.05617v1 Announce Type: cross 
Abstract: The emotional content of song lyrics plays a pivotal role in shaping listener experiences and influencing musical preferences. This paper investigates the task of multi-label emotional attribution of song lyrics by predicting six emotional intensity scores corresponding to six fundamental emotions. A manually labeled dataset is constructed using a mean opinion score (MOS) approach, which aggregates annotations from multiple human raters to ensure reliable ground-truth labels. Leveraging this dataset, we conduct a comprehensive evaluation of several publicly available large language models (LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model specifically for predicting multi-label emotion scores. Experimental results reveal the relative strengths and limitations of zero-shot and fine-tuned models in capturing the nuanced emotional content of lyrics. Our findings highlight the potential of LLMs for emotion recognition in creative texts, providing insights into model selection strategies for emotion-based music information retrieval applications. The labeled dataset is available at https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised Learning for Hyperspectral Images of Trees</title>
<link>https://arxiv.org/abs/2509.05630</link>
<guid>https://arxiv.org/abs/2509.05630</guid>
<content:encoded><![CDATA[
arXiv:2509.05630v1 Announce Type: cross 
Abstract: Aerial remote sensing using multispectral and RGB imagers has provided a critical impetus to precision agriculture. Analysis of the hyperspectral images with limited or no labels is challenging. This paper focuses on self-supervised learning to create neural network embeddings reflecting vegetation properties of trees from aerial hyperspectral images of crop fields. Experimental results demonstrate that a constructed tree representation, using a vegetation property-related embedding space, performs better in downstream machine learning tasks compared to the direct use of hyperspectral vegetation properties as tree representations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orchestrator: Active Inference for Multi-Agent Systems in Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2509.05651</link>
<guid>https://arxiv.org/abs/2509.05651</guid>
<content:encoded><![CDATA[
arXiv:2509.05651v1 Announce Type: cross 
Abstract: Complex, non-linear tasks challenge LLM-enhanced multi-agent systems (MAS) due to partial observability and suboptimal coordination. We propose Orchestrator, a novel MAS framework that leverages attention-inspired self-emergent coordination and reflective benchmarking to optimize global task performance. Orchestrator introduces a monitoring mechanism to track agent-environment dynamics, using active inference benchmarks to optimize system behavior. By tracking agent-to-agent and agent-to-environment interaction, Orchestrator mitigates the effects of partial observability and enables agents to approximate global task solutions more efficiently. We evaluate the framework on a series of maze puzzles of increasing complexity, demonstrating its effectiveness in enhancing coordination and performance in dynamic, non-linear environments with long-horizon objectives.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search</title>
<link>https://arxiv.org/abs/2509.05656</link>
<guid>https://arxiv.org/abs/2509.05656</guid>
<content:encoded><![CDATA[
arXiv:2509.05656v1 Announce Type: cross 
Abstract: Neural architecture search (NAS) is a hard computationally expensive optimization problem with a discrete, vast, and spiky search space. One of the key research efforts dedicated to this space focuses on accelerating NAS via certain proxy evaluations of neural architectures. Different from the prevalent predictor-based methods using surrogate models and differentiable architecture search via supernetworks, we propose an optimization proxy to streamline the NAS as an end-to-end optimization framework, named OptiProxy-NAS. In particular, using a proxy representation, the NAS space is reformulated to be continuous, differentiable, and smooth. Thereby, any differentiable optimization method can be applied to the gradient-based search of the relaxed architecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$ search spaces across three different domains including computer vision, natural language processing, and resource-constrained NAS fully demonstrate the superior search results and efficiency. Further experiments on low-fidelity scenarios verify the flexibility.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</title>
<link>https://arxiv.org/abs/2509.05657</link>
<guid>https://arxiv.org/abs/2509.05657</guid>
<content:encoded><![CDATA[
arXiv:2509.05657v1 Announce Type: cross 
Abstract: Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at https://github.com/Ashone3/LM-Searcher.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian</title>
<link>https://arxiv.org/abs/2509.05668</link>
<guid>https://arxiv.org/abs/2509.05668</guid>
<content:encoded><![CDATA[
arXiv:2509.05668v1 Announce Type: cross 
Abstract: We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR</title>
<link>https://arxiv.org/abs/2509.05671</link>
<guid>https://arxiv.org/abs/2509.05671</guid>
<content:encoded><![CDATA[
arXiv:2509.05671v1 Announce Type: cross 
Abstract: Human Activity Recognition (HAR) using multimodal sensor data remains challenging due to noisy or incomplete measurements, scarcity of labeled examples, and privacy concerns. Traditional centralized deep learning approaches are often constrained by infrastructure availability, network latency, and data sharing restrictions. While federated learning (FL) addresses privacy by training models locally and sharing only model parameters, it still has to tackle issues arising from the use of heterogeneous multimodal data and differential privacy requirements. In this article, a Graph-based Multimodal Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse sensor streams such as a pressure mat, depth camera, and multiple accelerometers are modeled as modality-specific graphs, processed through residual Graph Convolutional Neural Networks (GCNs), and fused via attention-based weighting rather than simple concatenation. The fused embeddings enable robust activity classification, while differential privacy safeguards data during federated aggregation. Experimental results show that the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with up to 2 percent higher accuracy in non-DP settings in both centralized and federated paradigms. More importantly, significant improvements are observed under differential privacy constraints: MultiModalGCN consistently surpasses MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on the privacy budget and setting. These results highlight the robustness of graph-based modeling in multimodal learning, where GNNs prove more resilient to the performance degradation introduced by DP noise.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts</title>
<link>https://arxiv.org/abs/2509.05681</link>
<guid>https://arxiv.org/abs/2509.05681</guid>
<content:encoded><![CDATA[
arXiv:2509.05681v1 Announce Type: cross 
Abstract: Decentralized Finance (DeFi) attacks have resulted in significant losses, often orchestrated through Adversarial Exploiter Contracts (AECs) that exploit vulnerabilities in victim smart contracts. To proactively identify such threats, this paper targets the explainable detection of AECs.
  Existing detection methods struggle to capture semantic dependencies and lack interpretability, limiting their effectiveness and leaving critical knowledge gaps in AEC analysis. To address these challenges, we introduce SEASONED, an effective, self-explanatory, and robust framework for AEC detection.
  SEASONED extracts semantic information from contract bytecode to construct a semantic relation graph (SRG), and employs a self-counterfactual explainable detector (SCFED) to classify SRGs and generate explanations that highlight the core attack logic. SCFED further enhances robustness, generalizability, and data efficiency by extracting representative information from these explanations. Both theoretical analysis and experimental results demonstrate the effectiveness of SEASONED, which showcases outstanding detection performance, robustness, generalizability, and data efficiency learning ability. To support further research, we also release a new dataset of 359 AECs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models</title>
<link>https://arxiv.org/abs/2509.05691</link>
<guid>https://arxiv.org/abs/2509.05691</guid>
<content:encoded><![CDATA[
arXiv:2509.05691v1 Announce Type: cross 
Abstract: Text embedding models are widely used in natural language processing applications. However, their capability is often benchmarked on tasks that do not require understanding nuanced numerical information in text. As a result, it remains unclear whether current embedding models can precisely encode numerical content, such as numbers, into embeddings. This question is critical because embedding models are increasingly applied in domains where numbers matter, such as finance and healthcare. For example, Company X's market share grew by 2\% should be interpreted very differently from Company X's market share grew by 20\%, even though both indicate growth in market share. This study aims to examine whether text embedding models can capture such nuances. Using synthetic data in a financial context, we evaluate 13 widely used text embedding models and find that they generally struggle to capture numerical details accurately. Our further analyses provide deeper insights into embedding numeracy, informing future research to strengthen embedding model-based NLP systems with improved capacity for handling numerical content.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis</title>
<link>https://arxiv.org/abs/2509.05703</link>
<guid>https://arxiv.org/abs/2509.05703</guid>
<content:encoded><![CDATA[
arXiv:2509.05703v1 Announce Type: cross 
Abstract: Marine mammal vocalization analysis depends on interpreting bioacoustic spectrograms. Vision Language Models (VLMs) are not trained on these domain-specific visualizations. We investigate whether VLMs can extract meaningful patterns from spectrograms visually. Our framework integrates VLM interpretation with LLM-based validation to build domain knowledge. This enables adaptation to acoustic data without manual annotation or model retraining.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of the State-of-the-Art in Conversational Question Answering Systems</title>
<link>https://arxiv.org/abs/2509.05716</link>
<guid>https://arxiv.org/abs/2509.05716</guid>
<content:encoded><![CDATA[
arXiv:2509.05716v1 Announce Type: cross 
Abstract: Conversational Question Answering (ConvQA) systems have emerged as a pivotal area within Natural Language Processing (NLP) by driving advancements that enable machines to engage in dynamic and context-aware conversations. These capabilities are increasingly being applied across various domains, i.e., customer support, education, legal, and healthcare where maintaining a coherent and relevant conversation is essential. Building on recent advancements, this survey provides a comprehensive analysis of the state-of-the-art in ConvQA. This survey begins by examining the core components of ConvQA systems, i.e., history selection, question understanding, and answer prediction, highlighting their interplay in ensuring coherence and relevance in multi-turn conversations. It further investigates the use of advanced machine learning techniques, including but not limited to, reinforcement learning, contrastive learning, and transfer learning to improve ConvQA accuracy and efficiency. The pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash, Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact through data scalability and architectural advancements. Additionally, this survey presents a comprehensive analysis of key ConvQA datasets and concludes by outlining open research directions. Overall, this work offers a comprehensive overview of the ConvQA landscape and provides valuable insights to guide future advancements in the field.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction</title>
<link>https://arxiv.org/abs/2509.05728</link>
<guid>https://arxiv.org/abs/2509.05728</guid>
<content:encoded><![CDATA[
arXiv:2509.05728v1 Announce Type: cross 
Abstract: This paper extends LiDAR-BIND, a modular multi-modal fusion framework that binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space, with mechanisms that explicitly enforce temporal consistency. We introduce three contributions: (i) temporal embedding similarity that aligns consecutive latents, (ii) a motion-aligned transformation loss that matches displacement between predictions and ground truth LiDAR, and (iii) windows temporal fusion using a specialised temporal module. We further update the model architecture to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR translation demonstrate improved temporal and spatial coherence, yielding lower absolute trajectory error and better occupancy map accuracy in Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose different metrics based on the Fr\'echet Video Motion Distance (FVMD) and a correlation-peak distance metric providing practical temporal quality indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially enhancing temporal stability, resulting in improved robustness and performance for downstream SLAM.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation Priors for Data-Efficient Deep Learning</title>
<link>https://arxiv.org/abs/2509.05732</link>
<guid>https://arxiv.org/abs/2509.05732</guid>
<content:encoded><![CDATA[
arXiv:2509.05732v1 Announce Type: cross 
Abstract: How do we enable AI systems to efficiently learn in the real-world? First-principles models are widely used to simulate natural systems, but often fail to capture real-world complexity due to simplifying assumptions. In contrast, deep learning approaches can estimate complex dynamics with minimal assumptions but require large, representative datasets. We propose SimPEL, a method that efficiently combines first-principles models with data-driven learning by using low-fidelity simulators as priors in Bayesian deep learning. This enables SimPEL to benefit from simulator knowledge in low-data regimes and leverage deep learning's flexibility when more data is available, all the while carefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse systems, including biological, agricultural, and robotic domains, showing superior performance in learning complex dynamics. For decision-making, we demonstrate that SimPEL bridges the sim-to-real gap in model-based reinforcement learning. On a high-speed RC car task, SimPEL learns a highly dynamic parking maneuver involving drifting with substantially less data than state-of-the-art baselines. These results highlight the potential of SimPEL for data-efficient learning and control in complex real-world environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies</title>
<link>https://arxiv.org/abs/2509.05735</link>
<guid>https://arxiv.org/abs/2509.05735</guid>
<content:encoded><![CDATA[
arXiv:2509.05735v1 Announce Type: cross 
Abstract: Data collection is crucial for learning robust world models in model-based reinforcement learning. The most prevalent strategies are to actively collect trajectories by interacting with the environment during online training or training on offline datasets. At first glance, the nature of learning task-agnostic environment dynamics makes world models a good candidate for effective offline training. However, the effects of online vs. offline data on world models and thus on the resulting task performance have not been thoroughly studied in the literature. In this work, we investigate both paradigms in model-based settings, conducting experiments on 31 different environments. First, we showcase that online agents outperform their offline counterparts. We identify a key challenge behind performance degradation of offline agents: encountering Out-Of-Distribution states at test time. This issue arises because, without the self-correction mechanism in online agents, offline datasets with limited state space coverage induce a mismatch between the agent's imagination and real rollouts, compromising policy training. We demonstrate that this issue can be mitigated by allowing for additional online interactions in a fixed or adaptive schedule, restoring the performance of online training with limited interaction data. We also showcase that incorporating exploration data helps mitigate the performance degradation of offline agents. Based on our insights, we recommend adding exploration data when collecting large datasets, as current efforts predominantly focus on expert data alone.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated</title>
<link>https://arxiv.org/abs/2509.05739</link>
<guid>https://arxiv.org/abs/2509.05739</guid>
<content:encoded><![CDATA[
arXiv:2509.05739v1 Announce Type: cross 
Abstract: Early research into data poisoning attacks against Large Language Models (LLMs) demonstrated the ease with which backdoors could be injected. More recent LLMs add step-by-step reasoning, expanding the attack surface to include the intermediate chain-of-thought (CoT) and its inherent trait of decomposing problems into subproblems. Using these vectors for more stealthy poisoning, we introduce ``decomposed reasoning poison'', in which the attacker modifies only the reasoning path, leaving prompts and final answers clean, and splits the trigger across multiple, individually harmless components.
  Fascinatingly, while it remains possible to inject these decomposed poisons, reliably activating them to change final answers (rather than just the CoT) is surprisingly difficult. This difficulty arises because the models can often recover from backdoors that are activated within their thought processes. Ultimately, it appears that an emergent form of backdoor robustness is originating from the reasoning capabilities of these advanced LLMs, as well as from the architectural separation between reasoning and final answer generation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios</title>
<link>https://arxiv.org/abs/2509.05747</link>
<guid>https://arxiv.org/abs/2509.05747</guid>
<content:encoded><![CDATA[
arXiv:2509.05747v1 Announce Type: cross 
Abstract: We address the problem of accurate capture of interactive behaviors between two people in daily scenarios. Most previous works either only consider one person or solely focus on conversational gestures of two people, assuming the body orientation and/or position of each actor are constant or barely change over each interaction. In contrast, we propose to simultaneously model two people's activities, and target objective-driven, dynamic, and semantically consistent interactions which often span longer duration and cover bigger space. To this end, we capture a new multi-modal dataset dubbed InterAct, which is composed of 241 motion sequences where two people perform a realistic and coherent scenario for one minute or longer over a complete interaction. For each sequence, two actors are assigned different roles and emotion labels, and collaborate to finish one task or conduct a common interaction activity. The audios, body motions, and facial expressions of both persons are captured. InterAct contains diverse and complex motions of individuals and interesting and relatively long-term interaction patterns barely seen before. We also demonstrate a simple yet effective diffusion-based method that estimates interactive face expressions and body motions of two people from speech inputs. Our method regresses the body motions in a hierarchical manner, and we also propose a novel fine-tuning mechanism to improve the lip accuracy of facial expressions. To facilitate further research, the data and code is made available at https://hku-cg.github.io/interact/ .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation</title>
<link>https://arxiv.org/abs/2509.05751</link>
<guid>https://arxiv.org/abs/2509.05751</guid>
<content:encoded><![CDATA[
arXiv:2509.05751v1 Announce Type: cross 
Abstract: Referring Video Object Segmentation (RVOS) aims to segment an object of interest throughout a video based on a language description. The prominent challenge lies in aligning static text with dynamic visual content, particularly when objects exhibiting similar appearances with inconsistent motion and poses. However, current methods often rely on a holistic visual-language fusion that struggles with complex, compositional descriptions. In this paper, we propose \textbf{PARSE-VOS}, a novel, training-free framework powered by Large Language Models (LLMs), for a hierarchical, coarse-to-fine reasoning across text and video domains. Our approach begins by parsing the natural language query into structured semantic commands. Next, we introduce a spatio-temporal grounding module that generates all candidate trajectories for all potential target objects, guided by the parsed semantics. Finally, a hierarchical identification module select the correct target through a two-stage reasoning process: it first performs coarse-grained motion reasoning with an LLM to narrow down candidates; if ambiguity remains, a fine-grained pose verification stage is conditionally triggered to disambiguate. The final output is an accurate segmentation mask for the target object. \textbf{PARSE-VOS} achieved state-of-the-art performance on three major benchmarks: Ref-YouTube-VOS, Ref-DAVIS17, and MeViS.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics</title>
<link>https://arxiv.org/abs/2509.05753</link>
<guid>https://arxiv.org/abs/2509.05753</guid>
<content:encoded><![CDATA[
arXiv:2509.05753v1 Announce Type: cross 
Abstract: The rise of synthetic media has blurred the boundary between reality and fabrication under the evolving power of artificial intelligence, fueling an infodemic that erodes public trust in cyberspace. For digital imagery, a multitude of editing applications further complicates the forensic analysis, including semantic edits that alter content, photometric adjustments that recalibrate colour characteristics, and geometric projections that reshape viewpoints. Collectively, these transformations manipulate and control perceptual interpretation of digital imagery. This susceptibility calls for forensic enquiry into reconstructing the chain of events, thereby revealing deeper evidential insight into the presence or absence of criminal intent. This study seeks to address an inverse problem of tracing the underlying generation chain that gives rise to the observed synthetic media. A tell-tale watermarking system is developed for explanatory reasoning over the nature and extent of transformations across the lifecycle of synthetic media. Tell-tale watermarks are tailored to different classes of transformations, responding in a manner that is neither strictly robust nor fragile but instead interpretable. These watermarks function as reference clues that evolve under the same transformation dynamics as the carrier media, leaving interpretable traces when subjected to transformations. Explanatory reasoning is then performed to infer the most plausible account across the combinatorial parameter space of composite transformations. Experimental evaluations demonstrate the validity of tell-tale watermarking with respect to fidelity, synchronicity and traceability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System</title>
<link>https://arxiv.org/abs/2509.05755</link>
<guid>https://arxiv.org/abs/2509.05755</guid>
<content:encoded><![CDATA[
arXiv:2509.05755v1 Announce Type: cross 
Abstract: LLM-based agentic systems leverage large language models to handle user queries, make decisions, and execute external tools for complex tasks across domains like chatbots, customer service, and software engineering. A critical component of these systems is the Tool Invocation Prompt (TIP), which defines tool interaction protocols and guides LLMs to ensure the security and correctness of tool usage. Despite its importance, TIP security has been largely overlooked. This work investigates TIP-related security risks, revealing that major LLM-based systems like Cursor, Claude Code, and others are vulnerable to attacks such as remote code execution (RCE) and denial of service (DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate external tool behavior hijacking via manipulated tool invocations. We also propose defense mechanisms to enhance TIP security in LLM-based agentic systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting</title>
<link>https://arxiv.org/abs/2509.05768</link>
<guid>https://arxiv.org/abs/2509.05768</guid>
<content:encoded><![CDATA[
arXiv:2509.05768v1 Announce Type: cross 
Abstract: Energy forecasting is vital for grid reliability and operational efficiency. Although recent advances in time series forecasting have led to progress, existing benchmarks remain limited in spatial and temporal scope and lack multi-energy features. This raises concerns about their reliability and applicability in real-world deployment. To address this, we present the Real-E dataset, covering over 74 power stations across 30+ European countries over a 10-year span with rich metadata. Using Real- E, we conduct an extensive data analysis and benchmark over 20 baselines across various model types. We introduce a new metric to quantify shifts in correlation structures and show that existing methods struggle on our dataset, which exhibits more complex and non-stationary correlation dynamics. Our findings highlight key limitations of current methods and offer a strong empirical basis for building more robust forecasting models
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2509.05778</link>
<guid>https://arxiv.org/abs/2509.05778</guid>
<content:encoded><![CDATA[
arXiv:2509.05778v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection plays a key role in enhancing the robustness of artificial intelligence systems by identifying inputs that differ significantly from the training distribution, thereby preventing unreliable predictions and enabling appropriate fallback mechanisms. Developing reliable OOD detection methods is a significant challenge, and rigorous evaluation of these techniques is essential for ensuring their effectiveness, as it allows researchers to assess their performance under diverse conditions and to identify potential limitations or failure modes. Cross-validation (CV) has proven to be a highly effective tool for providing a reasonable estimate of the performance of a learning algorithm. Although OOD scenarios exhibit particular characteristics, an appropriate adaptation of CV can lead to a suitable evaluation framework for this setting. This work proposes a dual CV framework for robust evaluation of OOD detection models, aimed at improving the reliability of their assessment. The proposed evaluation framework aims to effectively integrate in-distribution (ID) and OOD data while accounting for their differing characteristics. To achieve this, ID data are partitioned using a conventional approach, whereas OOD data are divided by grouping samples based on their classes. Furthermore, we analyze the context of data with class hierarchy to propose a data splitting that considers the entire class hierarchy to obtain fair ID-OOD partitions to apply the proposed evaluation framework. This framework is called Dual Cross-Validation for Robust Out-of-Distribution Detection (DCV-ROOD). To test the validity of the evaluation framework, we selected a set of state-of-the-art OOD detection methods, both with and without outlier exposure. The results show that the method achieves very fast convergence to the true performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Mode Deep Anomaly Detection for Medical Manufacturing: Structural Similarity and Feature Distance</title>
<link>https://arxiv.org/abs/2509.05796</link>
<guid>https://arxiv.org/abs/2509.05796</guid>
<content:encoded><![CDATA[
arXiv:2509.05796v1 Announce Type: cross 
Abstract: Automating visual inspection in medical device manufacturing remains challenging due to small and imbalanced datasets, high-resolution imagery, and stringent regulatory requirements. This work proposes two attention-guided autoencoder architectures for deep anomaly detection designed to address these constraints. The first employs a structural similarity-based anomaly score (4-MS-SSIM), offering lightweight and accurate real-time defect detection, yielding ACC 0.903 (unsupervised thresholding) and 0.931 (supervised thresholding) on the - Surface Seal Image - Test split with only 10% of defective samples. The second applies a feature-distance approach using Mahalanobis scoring on reduced latent features, providing high sensitivity to distributional shifts for supervisory monitoring, achieving ACC 0.722 with supervised thresholding. Together, these methods deliver complementary capabilities: the first supports reliable inline inspection, while the second enables scalable post-production surveillance and regulatory compliance monitoring. Experimental results demonstrate that both approaches surpass re-implemented baselines and provide a practical pathway for deploying deep anomaly detection in regulated manufacturing environments, aligning accuracy, efficiency, and the regulatory obligations defined for high-risk AI systems under the EU AI Act.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Fourier Neural Operator-Plasma Fluid Model for Fast and Accurate Multiscale Simulations of High Power Microwave Breakdown</title>
<link>https://arxiv.org/abs/2509.05799</link>
<guid>https://arxiv.org/abs/2509.05799</guid>
<content:encoded><![CDATA[
arXiv:2509.05799v1 Announce Type: cross 
Abstract: Modeling and simulation of High Power Microwave (HPM) breakdown, a multiscale phenomenon, is computationally expensive and requires solving Maxwell's equations (EM solver) coupled with a plasma continuity equation (plasma solver). In this work, we present a hybrid modeling approach that combines the accuracy of a differential equation-based plasma fluid solver with the computational efficiency of FNO (Fourier Neural Operator) based EM solver. Trained on data from an in-house FDTD-based plasma-fluid solver, the FNO replaces computationally expensive EM field updates, while the plasma solver governs the dynamic plasma response. The hybrid model is validated on microwave streamer formation, due to diffusion ionization mechanism, in a 2D scenario for unseen incident electric fields corresponding to entirely new plasma streamer simulations not included in model training, showing excellent agreement with FDTD based fluid simulations in terms of streamer shape, velocity, and temporal evolution. This hybrid FNO based strategy delivers significant acceleration of the order of 60X compared to traditional simulations for the specified problem size and offers an efficient alternative for computationally demanding multiscale and multiphysics simulations involved in HPM breakdown. Our work also demonstrate how such hybrid pipelines can be used to seamlessly to integrate existing C-based simulation codes with Python-based machine learning frameworks for simulations of plasma science and engineering problems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2509.05801</link>
<guid>https://arxiv.org/abs/2509.05801</guid>
<content:encoded><![CDATA[
arXiv:2509.05801v1 Announce Type: cross 
Abstract: While transformer-based foundation models excel at forecasting routine patterns, two questions remain: do they internalize semantic concepts such as market regimes, or merely fit curves? And can their internal representations be leveraged to simulate rare, high-stakes events such as market crashes? To investigate this, we introduce activation transplantation, a causal intervention that manipulates hidden states by imposing the statistical moments of one event (e.g., a historical crash) onto another (e.g., a calm period) during the forward pass. This procedure deterministically steers forecasts: injecting crash semantics induces downturn predictions, while injecting calm semantics suppresses crashes and restores stability. Beyond binary control, we find that models encode a graded notion of event severity, with the latent vector norm directly correlating with the magnitude of systemic shocks. Validated across two architecturally distinct TSFMs, Toto (decoder only) and Chronos (encoder-decoder), our results demonstrate that steerable, semantically grounded representations are a robust property of large time series transformers. Our findings provide evidence for a latent concept space that governs model predictions, shifting interpretability from post-hoc attribution to direct causal intervention, and enabling semantic "what-if" analysis for strategic stress-testing.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization</title>
<link>https://arxiv.org/abs/2509.05831</link>
<guid>https://arxiv.org/abs/2509.05831</guid>
<content:encoded><![CDATA[
arXiv:2509.05831v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into web-based systems for content summarization, yet their susceptibility to prompt injection attacks remains a pressing concern. In this study, we explore how non-visible HTML elements such as , aria-label, and alt attributes can be exploited to embed adversarial instructions without altering the visible content of a webpage. We introduce a novel dataset comprising 280 static web pages, evenly divided between clean and adversarial injected versions, crafted using diverse HTML-based strategies. These pages are processed through a browser automation pipeline to extract both raw HTML and rendered text, closely mimicking real-world LLM deployment scenarios. We evaluate two state-of-the-art open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their ability to summarize this content. Using both lexical (ROUGE-L) and semantic (SBERT cosine similarity) metrics, along with manual annotations, we assess the impact of these covert injections. Our findings reveal that over 29% of injected samples led to noticeable changes in the Llama 4 Scout summaries, while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These results highlight a critical and largely overlooked vulnerability in LLM driven web pipelines, where hidden adversarial content can subtly manipulate model outputs. Our work offers a reproducible framework and benchmark for evaluating HTML-based prompt injection and underscores the urgent need for robust mitigation strategies in LLM applications involving web content.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenAI on Wall Street -- Opportunities and Risk Controls</title>
<link>https://arxiv.org/abs/2509.05841</link>
<guid>https://arxiv.org/abs/2509.05841</guid>
<content:encoded><![CDATA[
arXiv:2509.05841v1 Announce Type: cross 
Abstract: We give an overview on the emerging applications of GenAI in the financial industry, especially within investment banks. Inherent to these exciting opportunities is a new realm of risks that must be managed properly. By heeding both the Yin and Yang sides of GenAI, we can accelerate its organic growth while safeguarding the entire financial industry during this nascent era of AI.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula</title>
<link>https://arxiv.org/abs/2509.05867</link>
<guid>https://arxiv.org/abs/2509.05867</guid>
<content:encoded><![CDATA[
arXiv:2509.05867v1 Announce Type: cross 
Abstract: Traditional Chinese Medicine (TCM) formulas play a significant role in treating epidemics and complex diseases. Existing models for TCM utilize traditional algorithms or deep learning techniques to analyze formula relationships, yet lack comprehensive results, such as complete formula compositions and detailed explanations. Although recent efforts have used TCM instruction datasets to fine-tune Large Language Models (LLMs) for explainable formula generation, existing datasets lack sufficient details, such as the roles of the formula's sovereign, minister, assistant, courier; efficacy; contraindications; tongue and pulse diagnosis-limiting the depth of model outputs. To address these challenges, we propose ZhiFangDanTai, a framework combining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM fine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured TCM knowledge into concise summaries, while also constructing an enhanced instruction dataset to improve LLMs' ability to integrate retrieved information. Furthermore, we provide novel theoretical proofs demonstrating that integrating GraphRAG with fine-tuning techniques can reduce generalization error and hallucination rates in the TCM formula task. Experimental results on both collected and clinical datasets demonstrate that ZhiFangDanTai achieves significant improvements over state-of-the-art models. Our model is open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.05874</link>
<guid>https://arxiv.org/abs/2509.05874</guid>
<content:encoded><![CDATA[
arXiv:2509.05874v1 Announce Type: cross 
Abstract: The rapid expansion of scientific literature makes it increasingly difficult to acquire new knowledge, particularly in specialized domains where reasoning is complex, full-text access is restricted, and target references are sparse among a large set of candidates. We present a Deep Reinforcement Learning framework for sparse reference selection that emulates human knowledge construction, prioritizing which papers to read under limited time and cost. Evaluated on drug--gene relation discovery with access restricted to titles and abstracts, our approach demonstrates that both humans and machines can construct knowledge effectively from partial information.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification in Probabilistic Machine Learning Models: Theory, Methods, and Insights</title>
<link>https://arxiv.org/abs/2509.05877</link>
<guid>https://arxiv.org/abs/2509.05877</guid>
<content:encoded><![CDATA[
arXiv:2509.05877v1 Announce Type: cross 
Abstract: Uncertainty Quantification (UQ) is essential in probabilistic machine learning models, particularly for assessing the reliability of predictions. In this paper, we present a systematic framework for estimating both epistemic and aleatoric uncertainty in probabilistic models. We focus on Gaussian Process Latent Variable Models and employ scalable Random Fourier Features-based Gaussian Processes to approximate predictive distributions efficiently. We derive a theoretical formulation for UQ, propose a Monte Carlo sampling-based estimation method, and conduct experiments to evaluate the impact of uncertainty estimation. Our results provide insights into the sources of predictive uncertainty and illustrate the effectiveness of our approach in quantifying the confidence in the predictions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation</title>
<link>https://arxiv.org/abs/2509.05881</link>
<guid>https://arxiv.org/abs/2509.05881</guid>
<content:encoded><![CDATA[
arXiv:2509.05881v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have fueled growing interest in automating geospatial analysis and GIS workflows, yet their actual capabilities remain uncertain. In this work, we call for rigorous evaluation of LLMs on well-defined geoprocessing tasks before making claims about full GIS automation. To this end, we present GeoAnalystBench, a benchmark of 50 Python-based tasks derived from real-world geospatial problems and carefully validated by GIS experts. Each task is paired with a minimum deliverable product, and evaluation covers workflow validity, structural alignment, semantic similarity, and code quality (CodeBLEU). Using this benchmark, we assess both proprietary and open source models. Results reveal a clear gap: proprietary models such as ChatGPT-4o-mini achieve high validity 95% and stronger code alignment (CodeBLEU 0.39), while smaller open source models like DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5% validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as spatial relationship detection or optimal site selection, remain the most challenging across all models. These findings demonstrate both the promise and limitations of current LLMs in GIS automation and provide a reproducible framework to advance GeoAI research with human-in-the-loop support.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues</title>
<link>https://arxiv.org/abs/2509.05882</link>
<guid>https://arxiv.org/abs/2509.05882</guid>
<content:encoded><![CDATA[
arXiv:2509.05882v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) integrate into diverse workflows, they are increasingly being considered "collaborators" with humans. If such AI collaborators are to be reliable, their behavior over multiturn interactions must be predictable, validated and verified before deployment. Common alignment techniques are typically developed under simplified single-user settings and do not account for the dynamics of long-horizon multiparty interactions. This paper examines how different alignment methods affect LLM agents' effectiveness as partners in multiturn, multiparty collaborations. We study this question through the lens of friction agents that intervene in group dialogues to encourage the collaborative group to slow down and reflect upon their reasoning for deliberative decision-making. Using a roleplay methodology, we evaluate interventions from differently-trained friction agents in collaborative task conversations. We propose a novel counterfactual evaluation framework that quantifies how friction interventions change the trajectory of group collaboration and belief alignment. Our results show that a friction-aware approach significantly outperforms common alignment baselines in helping both convergence to a common ground, or agreed-upon task-relevant propositions, and correctness of task outcomes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs</title>
<link>https://arxiv.org/abs/2509.05883</link>
<guid>https://arxiv.org/abs/2509.05883</guid>
<content:encoded><![CDATA[
arXiv:2509.05883v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have seen rapid adoption in recent years, with industries increasingly relying on them to maintain a competitive advantage. These models excel at interpreting user instructions and generating human-like responses, leading to their integration across diverse domains, including consulting and information retrieval. However, their widespread deployment also introduces substantial security risks, most notably in the form of prompt injection and jailbreak attacks.
  To systematically evaluate LLM vulnerabilities -- particularly to external prompt injection -- we conducted a series of experiments on eight commercial models. Each model was tested without supplementary sanitization, relying solely on its built-in safeguards. The results exposed exploitable weaknesses and emphasized the need for stronger security measures. Four categories of attacks were examined: direct injection, indirect (external) injection, image-based injection, and prompt leakage. Comparative analysis indicated that Claude 3 demonstrated relatively greater robustness; nevertheless, empirical findings confirm that additional defenses, such as input normalization, remain necessary to achieve reliable protection.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum spatial best-arm identification via quantum walks</title>
<link>https://arxiv.org/abs/2509.05890</link>
<guid>https://arxiv.org/abs/2509.05890</guid>
<content:encoded><![CDATA[
arXiv:2509.05890v1 Announce Type: cross 
Abstract: Quantum reinforcement learning has emerged as a framework combining quantum computation with sequential decision-making, and applications to the multi-armed bandit (MAB) problem have been reported. The graph bandit problem extends the MAB setting by introducing spatial constraints, yet quantum approaches remain limited. We propose a quantum algorithm for best-arm identification in graph bandits, termed Quantum Spatial Best-Arm Identification (QSBAI). The method employs quantum walks to encode superpositions over graph-constrained actions, extending amplitude amplification and generalizing the Quantum BAI algorithm via Szegedy's walk framework. This establishes a link between Grover-type search and reinforcement learning tasks with structural restrictions. We analyze complete and bipartite graphs, deriving the maximal success probability of identifying the best arm and the time step at which it is achieved. Our results highlight the potential of quantum walks to accelerate exploration in constrained environments and extend the applicability of quantum algorithms for decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets</title>
<link>https://arxiv.org/abs/2509.05892</link>
<guid>https://arxiv.org/abs/2509.05892</guid>
<content:encoded><![CDATA[
arXiv:2509.05892v1 Announce Type: cross 
Abstract: Accurate segmentation of carotid artery structures in histopathological images is vital for advancing cardiovascular disease research and diagnosis. However, deep learning model development in this domain is constrained by the scarcity of annotated cardiovascular histopathological data. This study investigates a systematic evaluation of state-of-the-art deep learning segmentation models, including convolutional neural networks (U-Net, DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models (SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology images. Despite employing an extensive hyperparameter optimization strategy with Bayesian search, our findings reveal that model performance is highly sensitive to data splits, with minor differences driven more by statistical noise than by true algorithmic superiority. This instability exposes the limitations of standard benchmarking practices in low-data clinical settings and challenges the assumption that performance rankings reflect meaningful clinical utility.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-training of diffractive meta-neural networks for super-resolution direction of arrival estimation</title>
<link>https://arxiv.org/abs/2509.05926</link>
<guid>https://arxiv.org/abs/2509.05926</guid>
<content:encoded><![CDATA[
arXiv:2509.05926v1 Announce Type: cross 
Abstract: Diffractive neural networks leverage the high-dimensional characteristics of electromagnetic (EM) fields for high-throughput computing. However, the existing architectures face challenges in integrating large-scale multidimensional metasurfaces with precise network training and haven't utilized multidimensional EM field coding scheme for super-resolution sensing. Here, we propose diffractive meta-neural networks (DMNNs) for accurate EM field modulation through metasurfaces, which enable multidimensional multiplexing and coding for multi-task learning and high-throughput super-resolution direction of arrival estimation. DMNN integrates pre-trained mini-metanets to characterize the amplitude and phase responses of meta-atoms across different polarizations and frequencies, with structure parameters inversely designed using the gradient-based meta-training. For wide-field super-resolution angle estimation, the system simultaneously resolves azimuthal and elevational angles through x and y-polarization channels, while the interleaving of frequency-multiplexed angular intervals generates spectral-encoded optical super-oscillations to achieve full-angle high-resolution estimation. Post-processing lightweight electronic neural networks further enhance the performance. Experimental results validate that a three-layer DMNN operating at 27 GHz, 29 GHz, and 31 GHz achieves $\sim7\times$ Rayleigh diffraction-limited angular resolution (0.5$^\circ$), a mean absolute error of 0.048$^\circ$ for two incoherent targets within a $\pm 11.5^\circ$ field of view, and an angular estimation throughput an order of magnitude higher (1917) than that of existing methods. The proposed architecture advances high-dimensional photonic computing systems by utilizing inherent high-parallelism and all-optical coding methods for ultra-high-resolution, high-throughput applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConstStyle: Robust Domain Generalization with Unified Style Transformation</title>
<link>https://arxiv.org/abs/2509.05975</link>
<guid>https://arxiv.org/abs/2509.05975</guid>
<content:encoded><![CDATA[
arXiv:2509.05975v1 Announce Type: cross 
Abstract: Deep neural networks often suffer performance drops when test data distribution differs from training data. Domain Generalization (DG) aims to address this by focusing on domain-invariant features or augmenting data for greater diversity. However, these methods often struggle with limited training domains or significant gaps between seen (training) and unseen (test) domains. To enhance DG robustness, we hypothesize that it is essential for the model to be trained on data from domains that closely resemble unseen test domains-an inherently difficult task due to the absence of prior knowledge about the unseen domains. Accordingly, we propose ConstStyle, a novel approach that leverages a unified domain to capture domain-invariant features and bridge the domain gap with theoretical analysis. During training, all samples are mapped onto this unified domain, optimized for seen domains. During testing, unseen domain samples are projected similarly before predictions. By aligning both training and testing data within this unified domain, ConstStyle effectively reduces the impact of domain shifts, even with large domain gaps or few seen domains. Extensive experiments demonstrate that ConstStyle consistently outperforms existing methods across diverse scenarios. Notably, when only a limited number of seen domains are available, ConstStyle can boost accuracy up to 19.82\% compared to the next best approach.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition</title>
<link>https://arxiv.org/abs/2509.05983</link>
<guid>https://arxiv.org/abs/2509.05983</guid>
<content:encoded><![CDATA[
arXiv:2509.05983v1 Announce Type: cross 
Abstract: Code-switching (CS) presents a significant challenge for general Auto-Speech Recognition (ASR) systems. Existing methods often fail to capture the subtle phonological shifts inherent in CS scenarios. The challenge is particularly difficult for language pairs like Vietnamese and English, where both distinct phonological features and the ambiguity arising from similar sound recognition are present. In this paper, we propose a novel architecture for Vietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC employs a phoneme-centric approach, built upon an extended Vietnamese phoneme set as an intermediate representation to facilitate mixed-lingual modeling. Experimental results demonstrate that TSPC consistently outperforms existing baselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a significantly lower word error rate of 20.8\% with reduced training resources. Furthermore, the phonetic-based two-stage architecture enables phoneme adaptation and language conversion to enhance ASR performance in complex CS Vietnamese-English ASR scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operationalising AI Regulatory Sandboxes under the EU AI Act: The Triple Challenge of Capacity, Coordination and Attractiveness to Providers</title>
<link>https://arxiv.org/abs/2509.05985</link>
<guid>https://arxiv.org/abs/2509.05985</guid>
<content:encoded><![CDATA[
arXiv:2509.05985v1 Announce Type: cross 
Abstract: The EU AI Act provides a rulebook for all AI systems being put on the market or into service in the European Union. This article investigates the requirement under the AI Act that Member States establish national AI regulatory sandboxes for testing and validation of innovative AI systems under regulatory supervision to assist with fostering innovation and complying with regulatory requirements. Against the backdrop of the EU objective that AI regulatory sandboxes would both foster innovation and assist with compliance, considerable challenges are identified for Member States around capacity-building and design of regulatory sandboxes. While Member States are early movers in laying the ground for national AI regulatory sandboxes, the article contends that there is a risk that differing approaches being taken by individual national sandboxes could jeopardise a uniform interpretation of the AI Act and its application in practice. This could motivate innovators to play sandbox arbitrage. The article therefore argues that the European Commission and the AI Board need to act decisively in developing rules and guidance to ensure a cohesive, coordinated approach in national AI regulatory sandboxes. With sandbox participation being voluntary, the possibility that AI regulatory sandboxes may prove unattractive to innovators on their compliance journey is also explored. Confidentiality concerns, the inability to relax legal rules during the sandbox, and the inability of sandboxes to deliver a presumption of conformity with the AI Act are identified as pertinent concerns for innovators contemplating applying to AI regulatory sandboxes as compared with other direct compliance routes provided to them through application of harmonised standards and conformity assessment procedures.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion</title>
<link>https://arxiv.org/abs/2509.05999</link>
<guid>https://arxiv.org/abs/2509.05999</guid>
<content:encoded><![CDATA[
arXiv:2509.05999v1 Announce Type: cross 
Abstract: Monocular 3D Object Detection represents a challenging Computer Vision task due to the nature of the input used, which is a single 2D image, lacking in any depth cues and placing the depth estimation problem as an ill-posed one. Existing solutions leverage the information extracted from the input by using Convolutional Neural Networks or Transformer architectures as feature extraction backbones, followed by specific detection heads for 3D parameters prediction. In this paper, we introduce a decoupled strategy based on injecting precomputed segmentation information priors and fusing them directly into the feature space for guiding the detection, without expanding the detection model or jointly learning the priors. The focus is on evaluating the impact of additional segmentation information on existing detection pipelines without adding additional prediction branches. The proposed method is evaluated on the KITTI 3D Object Detection Benchmark, outperforming the equivalent architecture that relies only on RGB image features for small objects in the scene: pedestrians and cyclists, and proving that understanding the input data can balance the need for additional sensors or training data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Khana: A Comprehensive Indian Cuisine Dataset</title>
<link>https://arxiv.org/abs/2509.06006</link>
<guid>https://arxiv.org/abs/2509.06006</guid>
<content:encoded><![CDATA[
arXiv:2509.06006v1 Announce Type: cross 
Abstract: As global interest in diverse culinary experiences grows, food image models are essential for improving food-related applications by enabling accurate food recognition, recipe suggestions, dietary tracking, and automated meal planning. Despite the abundance of food datasets, a noticeable gap remains in capturing the nuances of Indian cuisine due to its vast regional diversity, complex preparations, and the lack of comprehensive labeled datasets that cover its full breadth. Through this exploration, we uncover Khana, a new benchmark dataset for food image classification, segmentation, and retrieval of dishes from Indian cuisine. Khana fills the gap by establishing a taxonomy of Indian cuisine and offering around 131K images in the dataset spread across 80 labels, each with a resolution of 500x500 pixels. This paper describes the dataset creation process and evaluates state-of-the-art models on classification, segmentation, and retrieval as baselines. Khana bridges the gap between research and development by providing a comprehensive and challenging benchmark for researchers while also serving as a valuable resource for developers creating real-world applications that leverage the rich tapestry of Indian cuisine. Webpage: https://khana.omkar.xyz
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior</title>
<link>https://arxiv.org/abs/2509.06025</link>
<guid>https://arxiv.org/abs/2509.06025</guid>
<content:encoded><![CDATA[
arXiv:2509.06025v1 Announce Type: cross 
Abstract: A central goal of artificial intelligence is to build systems that can understand and predict complex, evolving sequences of events. However, current foundation models, designed for natural language, fail to grasp the holistic nature of structured interactions found in domains like telecommunications, e-commerce and finance. By serializing events into text, they disassemble them into semantically fragmented parts, losing critical context. In this work, we introduce the Unified Interaction Foundation Model (UIFM), a foundation model engineered for genuine behavioral understanding. At its core is the principle of composite tokenization, where each multi-attribute event is treated as a single, semantically coherent unit. This allows UIFM to learn the underlying "grammar" of user behavior, perceiving entire interactions rather than a disconnected stream of data points. We demonstrate that this architecture is not just more accurate, but represents a fundamental step towards creating more adaptable and intelligent predictive systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2509.06026</link>
<guid>https://arxiv.org/abs/2509.06026</guid>
<content:encoded><![CDATA[
arXiv:2509.06026v1 Announce Type: cross 
Abstract: While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations by integrating external knowledge bases, it introduces vulnerabilities to membership inference attacks (MIAs), particularly in systems handling sensitive data. Existing MIAs targeting RAG's external databases often rely on model responses but ignore the interference of non-member-retrieved documents on RAG outputs, limiting their effectiveness. To address this, we propose DCMI, a differential calibration MIA that mitigates the negative impact of non-member-retrieved documents. Specifically, DCMI leverages the sensitivity gap between member and non-member retrieved documents under query perturbation. It generates perturbed queries for calibration to isolate the contribution of member-retrieved documents while minimizing the interference from non-member-retrieved documents. Experiments under progressively relaxed assumptions show that DCMI consistently outperforms baselines--for example, achieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5, exceeding the MBA baseline by over 40%. Furthermore, on real-world RAG platforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the baseline. These results highlight significant privacy risks in RAG systems and emphasize the need for stronger protection mechanisms. We appeal to the community's consideration of deeper investigations, like ours, against the data leakage risks in rapidly evolving RAG systems. Our code is available at https://github.com/Xinyu140203/RAG_MIA.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamAudio: Customized Text-to-Audio Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2509.06027</link>
<guid>https://arxiv.org/abs/2509.06027</guid>
<content:encoded><![CDATA[
arXiv:2509.06027v1 Announce Type: cross 
Abstract: With the development of large-scale diffusion-based and language-modeling-based generative models, impressive progress has been achieved in text-to-audio generation. Despite producing high-quality outputs, existing text-to-audio models mainly aim to generate semantically aligned sound and fall short on precisely controlling fine-grained acoustic characteristics of specific sounds. As a result, users that need specific sound content may find it challenging to generate the desired audio clips. In this paper, we present DreamAudio for customized text-to-audio generation (CTTA). Specifically, we introduce a new framework that is designed to enable the model to identify auditory information from user-provided reference concepts for audio generation. Given a few reference audio samples containing personalized audio events, our system can generate new audio samples that include these specific events. In addition, two types of datasets are developed for training and testing the customized systems. The experiments show that the proposed model, DreamAudio, generates audio samples that are highly consistent with the customized audio features and aligned well with the input text prompts. Furthermore, DreamAudio offers comparable performance in general text-to-audio tasks. We also provide a human-involved dataset containing audio events from real-world CTTA cases as the benchmark for customized generation tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection</title>
<link>https://arxiv.org/abs/2509.06035</link>
<guid>https://arxiv.org/abs/2509.06035</guid>
<content:encoded><![CDATA[
arXiv:2509.06035v1 Announce Type: cross 
Abstract: Automated inspection of transmission lines using UAVs is hindered by the difficulty of detecting small and ambiguous defects against complex backgrounds. Conventional detectors often suffer from detail loss due to strided downsampling, weak boundary sensitivity in lightweight backbones, and insufficient integration of global context with local cues. To address these challenges, we propose TinyDef-DETR, a DETR-based framework designed for small-defect detection. The method introduces a stride-free space-to-depth module for lossless downsampling, an edge-enhanced convolution for boundary-aware feature extraction, a cross-stage dual-domain multi-scale attention module to jointly capture global and local information, and a Focaler-Wise-SIoU regression loss to improve localization of small objects. Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR achieves substantial improvements in both precision and recall compared to competitive baselines, with particularly notable gains on small-object subsets, while incurring only modest computational overhead. Further validation on the VisDrone benchmark confirms the generalization capability of the proposed approach. Overall, the results indicate that integrating detail-preserving downsampling, edge-sensitive representations, dual-domain attention, and difficulty-adaptive regression provides a practical and efficient solution for UAV-based small-defect inspection in power grids.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models</title>
<link>https://arxiv.org/abs/2509.06040</link>
<guid>https://arxiv.org/abs/2509.06040</guid>
<content:encoded><![CDATA[
arXiv:2509.06040v1 Announce Type: cross 
Abstract: Recent advancements in aligning image and video generative models via GRPO have achieved remarkable gains in enhancing human preference alignment. However, these methods still face high computational costs from on-policy rollouts and excessive SDE sampling steps, as well as training instability due to sparse rewards. In this paper, we propose BranchGRPO, a novel method that introduces a branch sampling policy updating the SDE sampling process. By sharing computation across common prefixes and pruning low-reward paths and redundant depths, BranchGRPO substantially lowers the per-update compute cost while maintaining or improving exploration diversity. This work makes three main contributions: (1) a branch sampling scheme that reduces rollout and training cost; (2) a tree-based advantage estimator incorporating dense process-level rewards; and (3) pruning strategies exploiting path and depth redundancy to accelerate convergence and boost performance. Experiments on image and video preference alignment show that BranchGRPO improves alignment scores by 16% over strong baselines, while cutting training time by 50%.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Study of Code Large Language Models for Binary Security Patch Detection</title>
<link>https://arxiv.org/abs/2509.06052</link>
<guid>https://arxiv.org/abs/2509.06052</guid>
<content:encoded><![CDATA[
arXiv:2509.06052v1 Announce Type: cross 
Abstract: Security patch detection (SPD) is crucial for maintaining software security, as unpatched vulnerabilities can lead to severe security risks. In recent years, numerous learning-based SPD approaches have demonstrated promising results on source code. However, these approaches typically cannot be applied to closed-source applications and proprietary systems that constitute a significant portion of real-world software, as they release patches only with binary files, and the source code is inaccessible. Given the impressive performance of code large language models (LLMs) in code intelligence and binary analysis tasks such as decompilation and compilation optimization, their potential for detecting binary security patches remains unexplored, exposing a significant research gap between their demonstrated low-level code understanding capabilities and this critical security task. To address this gap, we construct a large-scale binary patch dataset containing \textbf{19,448} samples, with two levels of representation: assembly code and pseudo-code, and systematically evaluate \textbf{19} code LLMs of varying scales to investigate their capability in binary SPD tasks. Our initial exploration demonstrates that directly prompting vanilla code LLMs struggles to accurately identify security patches from binary patches, and even state-of-the-art prompting techniques fail to mitigate the lack of domain knowledge in binary SPD within vanilla models. Drawing on the initial findings, we further investigate the fine-tuning strategy for injecting binary SPD domain knowledge into code LLMs through two levels of representation. Experimental results demonstrate that fine-tuned LLMs achieve outstanding performance, with the best results obtained on the pseudo-code representation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training</title>
<link>https://arxiv.org/abs/2509.06053</link>
<guid>https://arxiv.org/abs/2509.06053</guid>
<content:encoded><![CDATA[
arXiv:2509.06053v1 Announce Type: cross 
Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress in solving complex multi-player games through self-play. However, training effective adversarial policies requires millions of experience samples and substantial computational resources. Moreover, these policies lack interpretability, hindering their practical deployment. Recently, researchers have successfully leveraged Large Language Models (LLMs) to generate programmatic policies for single-agent tasks, transforming neural network-based policies into interpretable rule-based code with high execution efficiency. Inspired by this, we propose PolicyEvolve, a general framework for generating programmatic policies in multi-player games. PolicyEvolve significantly reduces reliance on manually crafted policy code, achieving high-performance policies with minimal environmental interactions. The framework comprises four modules: Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool preserves elite policies accumulated during iterative training. The Local Pool stores temporary policies for the current iteration; only sufficiently high-performing policies from this pool are promoted to the Global Pool. The Policy Planner serves as the core policy generation module. It samples the top three policies from the Global Pool, generates an initial policy for the current iteration based on environmental information, and refines this policy using feedback from the Trajectory Critic. Refined policies are then deposited into the Local Pool. This iterative process continues until the policy achieves a sufficiently high average win rate against the Global Pool, at which point it is integrated into the Global Pool. The Trajectory Critic analyzes interaction data from the current policy, identifies vulnerabilities, and proposes directional improvements to guide the Policy Planner
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.06060</link>
<guid>https://arxiv.org/abs/2509.06060</guid>
<content:encoded><![CDATA[
arXiv:2509.06060v1 Announce Type: cross 
Abstract: Recent advancements in deep learning models for time series forecasting have been significant. These models often leverage fundamental time series properties such as seasonality and non-stationarity, which may suggest an intrinsic link between model performance and data properties. However, existing benchmark datasets fail to offer diverse and well-defined temporal patterns, restricting the systematic evaluation of such connections. Additionally, there is no effective model recommendation approach, leading to high time and cost expenditures when testing different architectures across different downstream applications. For those reasons, we propose ARIES, a framework for assessing relation between time series properties and modeling strategies, and for recommending deep forcasting models for realistic time series. First, we construct a synthetic dataset with multiple distinct patterns, and design a comprehensive system to compute the properties of time series. Next, we conduct an extensive benchmarking of over 50 forecasting models, and establish the relationship between time series properties and modeling strategies. Our experimental results reveal a clear correlation. Based on these findings, we propose the first deep forecasting model recommender, capable of providing interpretable suggestions for real-world time series. In summary, ARIES is the first study to establish the relations between the properties of time series data and modeling strategies, while also implementing a model recommendation system. The code is available at: https://github.com/blisky-li/ARIES.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects</title>
<link>https://arxiv.org/abs/2509.06085</link>
<guid>https://arxiv.org/abs/2509.06085</guid>
<content:encoded><![CDATA[
arXiv:2509.06085v1 Announce Type: cross 
Abstract: Pre-trained models (PTMs) are machine learning models that have been trained in advance, often on large-scale data, and can be reused for new tasks, thereby reducing the need for costly training from scratch. Their widespread adoption introduces a new class of software dependency, which we term Software Dependencies 2.0, extending beyond conventional libraries to learned behaviors embodied in trained models and their associated artifacts. The integration of PTMs as software dependencies in real projects remains unclear, potentially threatening maintainability and reliability of modern software systems that increasingly rely on them. Objective: In this study, we investigate Software Dependencies 2.0 in open-source software (OSS) projects by examining the reuse of PTMs, with a focus on how developers manage and integrate these models. Specifically, we seek to understand: (1) how OSS projects structure and document their PTM dependencies; (2) what stages and organizational patterns emerge in the reuse pipelines of PTMs within these projects; and (3) the interactions among PTMs and other learned components across pipeline stages. We conduct a mixed-methods analysis of a statistically significant random sample of 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories reusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM reuse by identifying patterns and qualitatively investigate how developers integrate and manage these models in practice.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research</title>
<link>https://arxiv.org/abs/2509.06093</link>
<guid>https://arxiv.org/abs/2509.06093</guid>
<content:encoded><![CDATA[
arXiv:2509.06093v1 Announce Type: cross 
Abstract: Chemical and materials research has traditionally relied heavily on knowledge narrative, with progress often driven by language-based descriptions of principles, mechanisms, and experimental experiences, rather than tables, limiting what conventional databases and ML can exploit. We present a language-native database for boron nitride nanosheet (BNNS) polymer thermally conductive composites that captures lightly structured information from papers across preparation, characterization, theory-computation, and mechanistic reasoning, with evidence-linked snippets. Records are organized in a heterogeneous database and queried via composite retrieval with semantics, key words and value filters. The system can synthesizes literature into accurate, verifiable, and expert style guidance. This substrate enables high fidelity efficient Retrieval Augmented Generation (RAG) and tool augmented agents to interleave retrieval with reasoning and deliver actionable SOP. The framework supplies the language rich foundation required for LLM-driven materials discovery.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs</title>
<link>https://arxiv.org/abs/2509.06094</link>
<guid>https://arxiv.org/abs/2509.06094</guid>
<content:encoded><![CDATA[
arXiv:2509.06094v1 Announce Type: cross 
Abstract: Time-inconsistent preferences, where agents favor smaller-sooner over larger-later rewards, are a key feature of human and animal decision-making. Quasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this behavior, but its integration into the reinforcement learning (RL) framework has been limited. This paper addresses key theoretical and algorithmic gaps for precommitted agents with QH preferences. We make two primary contributions: (i) we formally characterize the structure of the optimal policy, proving for the first time that it reduces to a simple one-step non-stationary form; and (ii) we design the first practical, model-free algorithms for both policy evaluation and Q-learning in this setting, both with provable convergence guarantees. Our results provide foundational insights for incorporating QH preferences in RL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks</title>
<link>https://arxiv.org/abs/2509.06122</link>
<guid>https://arxiv.org/abs/2509.06122</guid>
<content:encoded><![CDATA[
arXiv:2509.06122v1 Announce Type: cross 
Abstract: Multispectral and hyperspectral imagery are widely used in agriculture, environmental monitoring, and urban planning due to their complementary spatial and spectral characteristics. A fundamental trade-off persists: multispectral imagery offers high spatial but limited spectral resolution, while hyperspectral imagery provides rich spectra at lower spatial resolution. Prior hyperspectral generation approaches (e.g., pan-sharpening variants, matrix factorization, CNNs) often struggle to jointly preserve spatial detail and spectral fidelity. In response, we propose SpecSwin3D, a transformer-based model that generates hyperspectral imagery from multispectral inputs while preserving both spatial and spectral quality. Specifically, SpecSwin3D takes five multispectral bands as input and reconstructs 224 hyperspectral bands at the same spatial resolution. In addition, we observe that reconstruction errors grow for hyperspectral bands spectrally distant from the input bands. To address this, we introduce a cascade training strategy that progressively expands the spectral range to stabilize learning and improve fidelity. Moreover, we design an optimized band sequence that strategically repeats and orders the five selected multispectral bands to better capture pairwise relations within a 3D shifted-window transformer framework. Quantitatively, our model achieves a PSNR of 35.82 dB, SAM of 2.40{\deg}, and SSIM of 0.96, outperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by more than half. Beyond reconstruction, we further demonstrate the practical value of SpecSwin3D on two downstream tasks, including land use classification and burnt area segmentation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes</title>
<link>https://arxiv.org/abs/2509.06159</link>
<guid>https://arxiv.org/abs/2509.06159</guid>
<content:encoded><![CDATA[
arXiv:2509.06159v1 Announce Type: cross 
Abstract: The growing popularity of robotic minimally invasive surgeries has made deep learning-based surgical training a key area of research. A thorough understanding of the surgical scene components is crucial, which semantic segmentation models can help achieve. However, most existing work focuses on surgical tools and overlooks anatomical objects. Additionally, current state-of-the-art (SOTA) models struggle to balance capturing high-level contextual features and low-level edge features. We propose a Feature-Adaptive Spatial Localization model (FASL-Seg), designed to capture features at multiple levels of detail through two distinct processing streams, namely a Low-Level Feature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream, for varying feature resolutions - enabling precise segmentation of anatomy and surgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark datasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model achieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy segmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU of 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation, respectively, outperforming SOTA overall performance, with comparable per-class SOTA results in both datasets and consistent performance in various classes for anatomy and instruments, demonstrating the effectiveness of distinct processing streams for varying feature resolutions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking daily paths in home contexts with RSSI fingerprinting based on UWB through deep learning models</title>
<link>https://arxiv.org/abs/2509.06161</link>
<guid>https://arxiv.org/abs/2509.06161</guid>
<content:encoded><![CDATA[
arXiv:2509.06161v1 Announce Type: cross 
Abstract: The field of human activity recognition has evolved significantly, driven largely by advancements in Internet of Things (IoT) device technology, particularly in personal devices. This study investigates the use of ultra-wideband (UWB) technology for tracking inhabitant paths in home environments using deep learning models. UWB technology estimates user locations via time-of-flight and time-difference-of-arrival methods, which are significantly affected by the presence of walls and obstacles in real environments, reducing their precision. To address these challenges, we propose a fingerprinting-based approach utilizing received signal strength indicator (RSSI) data collected from inhabitants in two flats (60 m2 and 100 m2) while performing daily activities. We compare the performance of convolutional neural network (CNN), long short-term memory (LSTM), and hybrid CNN+LSTM models, as well as the use of Bluetooth technology. Additionally, we evaluate the impact of the type and duration of the temporal window (future, past, or a combination of both). Our results demonstrate a mean absolute error close to 50 cm, highlighting the superiority of the hybrid model in providing accurate location estimates, thus facilitating its application in daily human activity recognition in residential settings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Gender and Political Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2509.06164</link>
<guid>https://arxiv.org/abs/2509.06164</guid>
<content:encoded><![CDATA[
arXiv:2509.06164v1 Announce Type: cross 
Abstract: We introduce EuroParlVote, a novel benchmark for evaluating large language models (LLMs) in politically sensitive contexts. It links European Parliament debate speeches to roll-call vote outcomes and includes rich demographic metadata for each Member of the European Parliament (MEP), such as gender, age, country, and political group. Using EuroParlVote, we evaluate state-of-the-art LLMs on two tasks -- gender classification and vote prediction -- revealing consistent patterns of bias. We find that LLMs frequently misclassify female MEPs as male and demonstrate reduced accuracy when simulating votes for female speakers. Politically, LLMs tend to favor centrist groups while underperforming on both far-left and far-right ones. Proprietary models like GPT-4o outperform open-weight alternatives in terms of both robustness and fairness. We release the EuroParlVote dataset, code, and demo to support future research on fairness and accountability in NLP within political contexts.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning</title>
<link>https://arxiv.org/abs/2509.06165</link>
<guid>https://arxiv.org/abs/2509.06165</guid>
<content:encoded><![CDATA[
arXiv:2509.06165v1 Announce Type: cross 
Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Language Model for Personalized Lung Cancer Screening</title>
<link>https://arxiv.org/abs/2509.06169</link>
<guid>https://arxiv.org/abs/2509.06169</guid>
<content:encoded><![CDATA[
arXiv:2509.06169v1 Announce Type: cross 
Abstract: Accurate risk assessment in lung cancer screening is critical for enabling early cancer detection and minimizing unnecessary invasive procedures. The Lung CT Screening Reporting and Data System (Lung-RADS) has been widely used as the standard framework for patient management and follow-up. Nevertheless, Lung-RADS faces trade-offs between sensitivity and specificity, as it stratifies risk solely based on lung nodule characteristics without incorporating various risk factors. Here we propose a reasoning language model (RLM) to integrate radiology findings with longitudinal medical records for individualized lung cancer risk assessment. Through a systematic study including dataset construction and distillation, supervised fine-tuning, reinforcement learning, and comprehensive evaluation, our model makes significant improvements in risk prediction performance on datasets in the national lung screening trial. Notably, RLM can decompose the risk evaluation task into sub-components, analyze the contributions of diverse risk factors, and synthesize them into a final risk score computed using our data-driven system equation. Our approach improves both predictive accuracy and monitorability through the chain of thought reasoning process, thereby facilitating clinical translation into lung cancer screening.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations</title>
<link>https://arxiv.org/abs/2509.06176</link>
<guid>https://arxiv.org/abs/2509.06176</guid>
<content:encoded><![CDATA[
arXiv:2509.06176v1 Announce Type: cross 
Abstract: As artificial intelligence (AI) systems permeate critical sectors, the need for professionals who can address ethical, legal and governance challenges has become urgent. Current AI ethics education remains fragmented, often siloed by discipline and disconnected from practice. This paper synthesizes literature and regulatory developments to propose a modular, interdisciplinary curriculum that integrates technical foundations with ethics, law and policy. We highlight recurring operational failures in AI - bias, misspecified objectives, generalization errors, misuse and governance breakdowns - and link them to pedagogical strategies for teaching AI governance. Drawing on perspectives from the EU, China and international frameworks, we outline a semester plan that emphasizes integrated ethics, stakeholder engagement and experiential learning. The curriculum aims to prepare students to diagnose risks, navigate regulation and engage diverse stakeholders, fostering adaptive and ethically grounded professionals for responsible AI governance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods</title>
<link>https://arxiv.org/abs/2509.06195</link>
<guid>https://arxiv.org/abs/2509.06195</guid>
<content:encoded><![CDATA[
arXiv:2509.06195v1 Announce Type: cross 
Abstract: Language fairness in multilingual information retrieval (MLIR) systems is crucial for ensuring equitable access to information across diverse languages. This paper sheds light on the issue, based on the assumption that queries in different languages, but with identical semantics, should yield equivalent ranking lists when retrieving on the same multilingual documents. We evaluate the degree of fairness using both traditional retrieval methods, and a DPR neural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a novel loss designed to mitigate language biases in neural MLIR approaches. Our analysis exposes intrinsic language biases in current MLIR technologies, with notable disparities across the retrieval methods, and the effectiveness of LaKDA in enhancing language fairness.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control</title>
<link>https://arxiv.org/abs/2509.06201</link>
<guid>https://arxiv.org/abs/2509.06201</guid>
<content:encoded><![CDATA[
arXiv:2509.06201v1 Announce Type: cross 
Abstract: Grasping of diverse objects in unstructured environments remains a significant challenge. Open-loop grasping methods, effective in controlled settings, struggle in cluttered environments. Grasp prediction errors and object pose changes during grasping are the main causes of failure. In contrast, closed-loop methods address these challenges in simplified settings (e.g., single object on a table) on a limited set of objects, with no path to generalization. We propose Grasp-MPC, a closed-loop 6-DoF vision-based grasping policy designed for robust and reactive grasping of novel objects in cluttered environments. Grasp-MPC incorporates a value function, trained on visual observations from a large-scale synthetic dataset of 2 million grasp trajectories that include successful and failed attempts. We deploy this learned value function in an MPC framework in combination with other cost terms that encourage collision avoidance and smooth execution. We evaluate Grasp-MPC on FetchBench and real-world settings across diverse environments. Grasp-MPC improves grasp success rates by up to 32.6% in simulation and 33.3% in real-world noisy conditions, outperforming open-loop, diffusion policy, transformer policy, and IQL approaches. Videos and more at http://grasp-mpc.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.06213</link>
<guid>https://arxiv.org/abs/2509.06213</guid>
<content:encoded><![CDATA[
arXiv:2509.06213v1 Announce Type: cross 
Abstract: We investigate reinforcement learning in the Game Of Hidden Rules (GOHR) environment, a complex puzzle in which an agent must infer and execute hidden rules to clear a 6$\times$6 board by placing game pieces into buckets. We explore two state representation strategies, namely Feature-Centric (FC) and Object-Centric (OC), and employ a Transformer-based Advantage Actor-Critic (A2C) algorithm for training. The agent has access only to partial observations and must simultaneously infer the governing rule and learn the optimal policy through experience. We evaluate our models across multiple rule-based and trial-list-based experimental setups, analyzing transfer effects and the impact of representation on learning efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Software Engineering: Foundational Pillars and a Research Roadmap</title>
<link>https://arxiv.org/abs/2509.06216</link>
<guid>https://arxiv.org/abs/2509.06216</guid>
<content:encoded><![CDATA[
arXiv:2509.06216v1 Announce Type: cross 
Abstract: Agentic Software Engineering (SE 3.0) represents a new era where intelligent agents are tasked not with simple code generation, but with achieving complex, goal-oriented SE objectives. To harness these new capabilities while ensuring trustworthiness, we must recognize a fundamental duality within the SE field in the Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE for Agents. This duality demands a radical reimagining of the foundational pillars of SE (actors, processes, tools, and artifacts) which manifest differently across each modality. We propose two purpose-built workbenches to support this vision. The Agent Command Environment (ACE) serves as a command center where humans orchestrate and mentor agent teams, handling outputs such as Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The Agent Execution Environment (AEE) is a digital workspace where agents perform tasks while invoking human expertise when facing ambiguity or complex trade-offs. This bi-directional partnership, which supports agent-initiated human callbacks and handovers, gives rise to new, structured engineering activities (i.e., processes) that redefine human-AI collaboration, elevating the practice from agentic coding to true agentic software engineering. This paper presents the Structured Agentic Software Engineering (SASE) vision, outlining several of the foundational pillars for the future of SE. The paper culminates in a research roadmap that identifies a few key challenges and opportunities while briefly discussing the resulting impact of this future on SE education. Our goal is not to offer a definitive solution, but to provide a conceptual scaffold with structured vocabulary to catalyze a community-wide dialogue, pushing the SE community to think beyond its classic, human-centric tenets toward a disciplined, scalable, and trustworthy agentic future.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Efficiency Frontier: Classical Shadows versus Quantum Footage</title>
<link>https://arxiv.org/abs/2509.06218</link>
<guid>https://arxiv.org/abs/2509.06218</guid>
<content:encoded><![CDATA[
arXiv:2509.06218v1 Announce Type: cross 
Abstract: Interfacing quantum and classical processors is an important subroutine in full-stack quantum algorithms. The so-called "classical shadow" method efficiently extracts essential classical information from quantum states, enabling the prediction of many properties of a quantum system from only a few measurements. However, for a small number of highly non-local observables, or when classical post-processing power is limited, the classical shadow method is not always the most efficient choice. Here, we address this issue quantitatively by performing a full-stack resource analysis that compares classical shadows with ``quantum footage," which refers to direct quantum measurement. Under certain assumptions, our analysis illustrates a boundary of download efficiency between classical shadows and quantum footage. For observables expressed as linear combinations of Pauli matrices, the classical shadow method outperforms direct measurement when the number of observables is large and the Pauli weight is small. For observables in the form of large Hermitian sparse matrices, the classical shadow method shows an advantage when the number of observables, the sparsity of the matrix, and the number of qubits fall within a certain range. The key parameters influencing this behavior include the number of qubits $n$, observables $M$, sparsity $k$, Pauli weight $w$, accuracy requirement $\epsilon$, and failure tolerance $\delta$. We also compare the resource consumption of the two methods on different types of quantum computers and identify break-even points where the classical shadow method becomes more efficient, which vary depending on the hardware. This paper opens a new avenue for quantitatively designing optimal strategies for hybrid quantum-classical tomography and provides practical insights for selecting the most suitable quantum measurement approach in real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beamforming-LLM: What, Where and When Did I Miss?</title>
<link>https://arxiv.org/abs/2509.06221</link>
<guid>https://arxiv.org/abs/2509.06221</guid>
<content:encoded><![CDATA[
arXiv:2509.06221v1 Announce Type: cross 
Abstract: We present Beamforming-LLM, a system that enables users to semantically recall conversations they may have missed in multi-speaker environments. The system combines spatial audio capture using a microphone array with retrieval-augmented generation (RAG) to support natural language queries such as, "What did I miss when I was following the conversation on dogs?" Directional audio streams are separated using beamforming, transcribed with Whisper, and embedded into a vector database using sentence encoders. Upon receiving a user query, semantically relevant segments are retrieved, temporally aligned with non-attended segments, and summarized using a lightweight large language model (GPT-4o-mini). The result is a user-friendly interface that provides contrastive summaries, spatial context, and timestamped audio playback. This work lays the foundation for intelligent auditory memory systems and has broad applications in assistive technology, meeting summarization, and context-aware personal spatial computing.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation of CNN Ensemble Results for Enhanced Long-Term Prediction of the ENSO Phenomenon</title>
<link>https://arxiv.org/abs/2509.06227</link>
<guid>https://arxiv.org/abs/2509.06227</guid>
<content:encoded><![CDATA[
arXiv:2509.06227v1 Announce Type: cross 
Abstract: The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO) is still one of the biggest challenges in climate science. While it is true that short-to medium-range performance has been improved significantly using the advances in deep learning, statistical dynamical hybrids, most operational systems still use the simple mean of all ensemble members, implicitly assuming equal skill across members. In this study, we demonstrate, through a strictly a-posteriori evaluation , for any large enough ensemble of ENSO forecasts, there is a subset of members whose skill is substantially higher than that of the ensemble mean. Using a state-of-the-art ENSO forecast system cross-validated against the 1986-2017 observed Nino3.4 index, we identify two Top-5 subsets one ranked on lowest Root Mean Square Error (RMSE) and another on highest Pearson correlation. Generally across all leads, these outstanding members show higher correlation and lower RMSE, with the advantage rising enormously with lead time. Whereas at short leads (1 month) raises the mean correlation by about +0.02 (+1.7%) and lowers the RMSE by around 0.14 {\deg}C or by 23.3% compared to the All-40 mean, at extreme leads (23 months) the correlation is raised by +0.43 (+172%) and RMSE by 0.18 {\deg}C or by 22.5% decrease. The enhancements are largest during crucial ENSO transition periods such as SON and DJF, when accurate amplitude and phase forecasting is of greatest socio-economic benefit, and furthermore season-dependent e.g., mid-year months such as JJA and MJJ have incredibly large RMSE reductions. This study provides a solid foundation for further investigations to identify reliable clues for detecting high-quality ensemble members, thereby enhancing forecasting skill.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Synthesis of Timed Regular Expressions</title>
<link>https://arxiv.org/abs/2509.06262</link>
<guid>https://arxiv.org/abs/2509.06262</guid>
<content:encoded><![CDATA[
arXiv:2509.06262v1 Announce Type: cross 
Abstract: Timed regular expressions serve as a formalism for specifying real-time behaviors of Cyber-Physical Systems. In this paper, we consider the synthesis of timed regular expressions, focusing on generating a timed regular expression consistent with a given set of system behaviors including positive and negative examples, i.e., accepting all positive examples and rejecting all negative examples. We first prove the decidability of the synthesis problem through an exploration of simple timed regular expressions. Subsequently, we propose our method of generating a consistent timed regular expression with minimal length, which unfolds in two steps. The first step is to enumerate and prune candidate parametric timed regular expressions. In the second step, we encode the requirement that a candidate generated by the first step is consistent with the given set into a Satisfiability Modulo Theories (SMT) formula, which is consequently solved to determine a solution to parametric time constraints. Finally, we evaluate our approach on benchmarks, including randomly generated behaviors from target timed models and a case study.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks</title>
<link>https://arxiv.org/abs/2509.06270</link>
<guid>https://arxiv.org/abs/2509.06270</guid>
<content:encoded><![CDATA[
arXiv:2509.06270v1 Announce Type: cross 
Abstract: Sixth generation (6G) systems require environment-aware communication, driven by native artificial intelligence (AI) and integrated sensing and communication (ISAC). Radio maps (RMs), providing spatially continuous channel information, are key enablers. However, generating high-fidelity RM ground truth via electromagnetic (EM) simulations is computationally intensive, motivating machine learning (ML)-based RM construction. The effectiveness of these data-driven methods depends on large-scale, high-quality training data. Current public datasets often focus on single-input single-output (SISO) and limited information, such as path loss, which is insufficient for advanced multi-input multi-output (MIMO) systems requiring detailed channel state information (CSI). To address this gap, this paper presents UrbanMIMOMap, a novel large-scale urban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap offers comprehensive complex CSI matrices across a dense spatial grid, going beyond traditional path loss data. This rich CSI is vital for constructing high-fidelity RMs and serves as a fundamental resource for data-driven RM generation, including deep learning. We demonstrate the dataset's utility through baseline performance evaluations of representative ML methods for RM construction. This work provides a crucial dataset and reference for research in high-precision RM generation, MIMO spatial performance, and ML for 6G environment awareness. The code and data for this work are available at: https://github.com/UNIC-Lab/UrbanMIMOMap.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference for Misspecified Contextual Bandits</title>
<link>https://arxiv.org/abs/2509.06287</link>
<guid>https://arxiv.org/abs/2509.06287</guid>
<content:encoded><![CDATA[
arXiv:2509.06287v1 Announce Type: cross 
Abstract: Contextual bandit algorithms have transformed modern experimentation by enabling real-time adaptation for personalized treatment and efficient use of data. Yet these advantages create challenges for statistical inference due to adaptivity. A fundamental property that supports valid inference is policy convergence, meaning that action-selection probabilities converge in probability given the context. Convergence ensures replicability of adaptive experiments and stability of online algorithms. In this paper, we highlight a previously overlooked issue: widely used algorithms such as LinUCB may fail to converge when the reward model is misspecified, and such non-convergence creates fundamental obstacles for statistical inference. This issue is practically important, as misspecified models -- such as linear approximations of complex dynamic system -- are often employed in real-world adaptive experiments to balance bias and variance.
  Motivated by this insight, we propose and analyze a broad class of algorithms that are guaranteed to converge even under model misspecification. Building on this guarantee, we develop a general inference framework based on an inverse-probability-weighted Z-estimator (IPW-Z) and establish its asymptotic normality with a consistent variance estimator. Simulation studies confirm that the proposed method provides robust and data-efficient confidence intervals, and can outperform existing approaches that exist only in the special case of offline policy evaluation. Taken together, our results underscore the importance of designing adaptive algorithms with built-in convergence guarantees to enable stable experimentation and valid statistical inference in practice.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Walk with Less: a Dyna-Style Approach to Quadrupedal Locomotion</title>
<link>https://arxiv.org/abs/2509.06296</link>
<guid>https://arxiv.org/abs/2509.06296</guid>
<content:encoded><![CDATA[
arXiv:2509.06296v1 Announce Type: cross 
Abstract: Traditional RL-based locomotion controllers often suffer from low data efficiency, requiring extensive interaction to achieve robust performance. We present a model-based reinforcement learning (MBRL) framework that improves sample efficiency for quadrupedal locomotion by appending synthetic data to the end of standard rollouts in PPO-based controllers, following the Dyna-Style paradigm. A predictive model, trained alongside the policy, generates short-horizon synthetic transitions that are gradually integrated using a scheduling strategy based on the policy update iterations. Through an ablation study, we identified a strong correlation between sample efficiency and rollout length, which guided the design of our experiments. We validated our approach in simulation on the Unitree Go1 robot and showed that replacing part of the simulated steps with synthetic ones not only mimics extended rollouts but also improves policy return and reduces variance. Finally, we demonstrate that this improvement transfers to the ability to track a wide range of locomotion commands using fewer simulated steps.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs</title>
<link>https://arxiv.org/abs/2509.06326</link>
<guid>https://arxiv.org/abs/2509.06326</guid>
<content:encoded><![CDATA[
arXiv:2509.06326v1 Announce Type: cross 
Abstract: As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to reduce network dependency, improve privacy, and enhance responsiveness, verifying the legitimacy of models running on local devices becomes critical. Existing attestation techniques are not suitable for billion-parameter Large Language Models (LLMs), struggling to remain both time- and memory-efficient while addressing emerging threats in the LLM era. In this paper, we present AttestLLM, the first-of-its-kind attestation framework to protect the hardware-level intellectual property (IP) of device vendors by ensuring that only authorized LLMs can execute on target platforms. AttestLLM leverages an algorithm/software/hardware co-design approach to embed robust watermarking signatures onto the activation distributions of LLM building blocks. It also optimizes the attestation protocol within the Trusted Execution Environment (TEE), providing efficient verification without compromising inference throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen, and Phi families for on-device use cases demonstrate AttestLLM's attestation reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model legitimacy and exhibits resilience against model replacement and forgery attacks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2509.06332</link>
<guid>https://arxiv.org/abs/2509.06332</guid>
<content:encoded><![CDATA[
arXiv:2509.06332v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable emergent capabilities, yet the robustness of their numerical reasoning remains an open question. While standard benchmarks evaluate LLM reasoning on complex problem sets using aggregated metrics, they often obscure foundational weaknesses. In this work, we probe LLM mathematical numeracy by evaluating performance on problems of escalating complexity, from constituent operations to combinatorial puzzles. We test several state-of-the-art LLM-based agents on a 100-problem challenge comprising four categories: (1) basic arithmetic, (2) advanced operations, (3) primality checking, and (4) the Game of 24 number puzzle. Our results show that while the agents achieved high accuracy on the first three categories, which require deterministic algorithmic execution, they consistently failed at the number puzzle, underlining its demand for a heuristic search over a large combinatorial space to be a significant bottleneck. These findings reveal that the agents' proficiency is largely confined to recalling and executing known algorithms, rather than performing generative problem-solving. This suggests their apparent numerical reasoning is more akin to sophisticated pattern-matching than flexible, analytical thought, limiting their potential for tasks that require novel or creative numerical insights.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing</title>
<link>https://arxiv.org/abs/2509.06336</link>
<guid>https://arxiv.org/abs/2509.06336</guid>
<content:encoded><![CDATA[
arXiv:2509.06336v1 Announce Type: cross 
Abstract: Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain performance by employing vision-language models like CLIP. However, existing CLIP-based FAS models do not fully exploit CLIP's patch embedding tokens, failing to detect critical spoofing clues. Moreover, these models rely on a single text prompt per class (e.g., 'live' or 'fake'), which limits generalization. To address these issues, we propose MVP-FAS, a novel framework incorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text Patch Alignment (MTPA). Both modules utilize multiple paraphrased texts to generate generalized features and reduce dependence on domain-specific text. MVS extracts local detailed spatial features and global context from patch embeddings by leveraging diverse texts with multiple perspectives. MTPA aligns patches with multiple text representations to improve semantic robustness. Extensive experiments demonstrate that MVP-FAS achieves superior generalization performance, outperforming previous state-of-the-art methods on cross-domain datasets. Code: https://github.com/Elune001/MVP-FAS.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ban&amp;Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs</title>
<link>https://arxiv.org/abs/2509.06346</link>
<guid>https://arxiv.org/abs/2509.06346</guid>
<content:encoded><![CDATA[
arXiv:2509.06346v1 Announce Type: cross 
Abstract: Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling large language models (LLMs) efficiently. Recent fine-grained MoE designs introduce hundreds of experts per layer, with multiple experts activated per token, enabling stronger specialization. However, during pre-training, routers are optimized mainly for stability and robustness: they converge prematurely and enforce balanced usage, limiting the full potential of model performance and efficiency. In this work, we uncover two overlooked issues: (i) a few highly influential experts are underutilized due to premature and balanced routing decisions; and (ii) enforcing a fixed number of active experts per token introduces substantial redundancy. Instead of retraining models or redesigning MoE architectures, we introduce Ban&amp;Pick, a post-training, plug-and-play strategy for smarter MoE routing. Pick discovers and reinforces key experts-a small group with outsized impact on performance-leading to notable accuracy gains across domains. Ban complements this by dynamically pruning redundant experts based on layer and token sensitivity, delivering faster inference with minimal accuracy loss. Experiments on fine-grained MoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks demonstrate that Ban&amp;Pick delivers free performance gains and inference acceleration without retraining or architectural changes. For instance, on Qwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from 65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the vLLM.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?</title>
<link>https://arxiv.org/abs/2509.06350</link>
<guid>https://arxiv.org/abs/2509.06350</guid>
<content:encoded><![CDATA[
arXiv:2509.06350v1 Announce Type: cross 
Abstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various successful methods whereby attackers manipulate models into generating harmful responses that they are designed to avoid. Among these, Greedy Coordinate Gradient (GCG) has emerged as a general and effective approach that optimizes the tokens in a suffix to generate jailbreakable prompts. While several improved variants of GCG have been proposed, they all rely on fixed-length suffixes. However, the potential redundancy within these suffixes remains unexplored. In this work, we propose Mask-GCG, a plug-and-play method that employs learnable token masking to identify impactful tokens within the suffix. Our approach increases the update probability for tokens at high-impact positions while pruning those at low-impact positions. This pruning not only reduces redundancy but also decreases the size of the gradient space, thereby lowering computational overhead and shortening the time required to achieve successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the original GCG and several improved variants. Experimental results show that most tokens in the suffix contribute significantly to attack success, and pruning a minority of low-impact tokens does not affect the loss values or compromise the attack success rate (ASR), thereby revealing token redundancy in LLM prompts. Our findings provide insights for developing efficient and interpretable LLMs from the perspective of jailbreak attacks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PL-CA: A Parametric Legal Case Augmentation Framework</title>
<link>https://arxiv.org/abs/2509.06356</link>
<guid>https://arxiv.org/abs/2509.06356</guid>
<content:encoded><![CDATA[
arXiv:2509.06356v1 Announce Type: cross 
Abstract: Conventional RAG is considered one of the most effective methods for addressing model knowledge insufficiency and hallucination, particularly in the judicial domain that requires high levels of knowledge rigor, logical consistency, and content integrity. However, the conventional RAG method only injects retrieved documents directly into the model's context, which severely constrains models due to their limited context windows and introduces additional computational overhead through excessively long contexts, thereby disrupting models' attention and degrading performance on downstream tasks. Moreover, many existing benchmarks lack expert annotation and focus solely on individual downstream tasks while real-world legal scenarios consist of multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for reflecting models' true capabilities. To address these limitations, we propose PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data augmentation on corpus knowledge and encode this legal knowledge into parametric vectors, and then integrates this parametric knowledge into the LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context pressure. Additionally, we also construct a multi-task legal dataset comprising more than 2000 training and test instances, which are all expert-annotated and manually verified. We conduct our experiments on our dataset, and the experimental results demonstrate that our method reduces the overhead associated with excessively long contexts while maintaining competitive performance on downstream tasks compared to conventional RAG. Our code and dataset are provided in the appendix.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification</title>
<link>https://arxiv.org/abs/2509.06367</link>
<guid>https://arxiv.org/abs/2509.06367</guid>
<content:encoded><![CDATA[
arXiv:2509.06367v1 Announce Type: cross 
Abstract: Drought stress is a major threat to global crop productivity, making its early and precise detection essential for sustainable agricultural management. Traditional approaches, though useful, are often time-consuming and labor-intensive, which has motivated the adoption of deep learning methods. In recent years, Convolutional Neural Network (CNN) and Vision Transformer architectures have been widely explored for drought stress identification; however, these models generally rely on a large number of trainable parameters, restricting their use in resource-limited and real-time agricultural settings. To address this challenge, we propose a novel lightweight hybrid CNN framework inspired by ResNet, DenseNet, and MobileNet architectures. The framework achieves a remarkable 15-fold reduction in trainable parameters compared to conventional CNN and Vision Transformer models, while maintaining competitive accuracy. In addition, we introduce a machine unlearning mechanism based on a gradient norm-based influence function, which enables targeted removal of specific training data influence, thereby improving model adaptability. The method was evaluated on an aerial image dataset of potato fields with expert-annotated healthy and drought-stressed regions. Experimental results show that our framework achieves high accuracy while substantially lowering computational costs. These findings highlight its potential as a practical, scalable, and adaptive solution for drought stress monitoring in precision agriculture, particularly under resource-constrained conditions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Pre-Service Horizon: Infusing In-Service Behavior for Improved Financial Risk Forecasting</title>
<link>https://arxiv.org/abs/2509.06385</link>
<guid>https://arxiv.org/abs/2509.06385</guid>
<content:encoded><![CDATA[
arXiv:2509.06385v1 Announce Type: cross 
Abstract: Typical financial risk management involves distinct phases for pre-service risk assessment and in-service default detection, often modeled separately. This paper proposes a novel framework, Multi-Granularity Knowledge Distillation (abbreviated as MGKD), aimed at improving pre-service risk prediction through the integration of in-service user behavior data. MGKD follows the idea of knowledge distillation, where the teacher model, trained on historical in-service data, guides the student model, which is trained on pre-service data. By using soft labels derived from in-service data, the teacher model helps the student model improve its risk prediction prior to service activation. Meanwhile, a multi-granularity distillation strategy is introduced, including coarse-grained, fine-grained, and self-distillation, to align the representations and predictions of the teacher and student models. This approach not only reinforces the representation of default cases but also enables the transfer of key behavioral patterns associated with defaulters from the teacher to the student model, thereby improving the overall performance of pre-service risk assessment. Moreover, we adopt a re-weighting strategy to mitigate the model's bias towards the minority class. Experimental results on large-scale real-world datasets from Tencent Mobile Payment demonstrate the effectiveness of our proposed approach in both offline and online scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeanFlow-Accelerated Multimodal Video-to-Audio Synthesis via One-Step Generation</title>
<link>https://arxiv.org/abs/2509.06389</link>
<guid>https://arxiv.org/abs/2509.06389</guid>
<content:encoded><![CDATA[
arXiv:2509.06389v1 Announce Type: cross 
Abstract: A key challenge in synthesizing audios from silent videos is the inherent trade-off between synthesis quality and inference efficiency in existing methods. For instance, flow matching based models rely on modeling instantaneous velocity, inherently require an iterative sampling process, leading to slow inference speeds. To address this efficiency bottleneck, we introduce a MeanFlow-accelerated model that characterizes flow fields using average velocity, enabling one-step generation and thereby significantly accelerating multimodal video-to-audio (VTA) synthesis while preserving audio quality, semantic alignment, and temporal synchronization. Furthermore, a scalar rescaling mechanism is employed to balance conditional and unconditional predictions when classifier-free guidance (CFG) is applied, effectively mitigating CFG-induced distortions in one step generation. Since the audio synthesis network is jointly trained with multimodal conditions, we further evaluate it on text-to-audio (TTA) synthesis task. Experimental results demonstrate that incorporating MeanFlow into the network significantly improves inference speed without compromising perceptual quality on both VTA and TTA synthesis tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.06415</link>
<guid>https://arxiv.org/abs/2509.06415</guid>
<content:encoded><![CDATA[
arXiv:2509.06415v1 Announce Type: cross 
Abstract: Recent progress in vision-language models (VLMs) has led to impressive results in document understanding tasks, but their high computational demands remain a challenge. To mitigate the compute burdens, we propose a lightweight token pruning framework that filters out non-informative background regions from document images prior to VLM processing. A binary patch-level classifier removes non-text areas, and a max-pooling refinement step recovers fragmented text regions to enhance spatial coherence. Experiments on real-world document datasets demonstrate that our approach substantially lowers computational costs, while maintaining comparable accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup</title>
<link>https://arxiv.org/abs/2509.06419</link>
<guid>https://arxiv.org/abs/2509.06419</guid>
<content:encoded><![CDATA[
arXiv:2509.06419v1 Announce Type: cross 
Abstract: Time series anomaly detection (TSAD) is a vital yet challenging task, particularly in scenarios where labeled anomalies are scarce and temporal dependencies are complex. Recent anomaly assumption (AA) approaches alleviate the lack of anomalies by injecting synthetic samples and training discriminative models. Despite promising results, these methods often suffer from two fundamental limitations: patchy generation, where scattered anomaly knowledge leads to overly simplistic or incoherent anomaly injection, and Anomaly Shift, where synthetic anomalies either resemble normal data too closely or diverge unrealistically from real anomalies, thereby distorting classification boundaries. In this paper, we propose CAPMix, a controllable anomaly augmentation framework that addresses both issues. First, we design a CutAddPaste mechanism to inject diverse and complex anomalies in a targeted manner, avoiding patchy generation. Second, we introduce a label revision strategy to adaptively refine anomaly labels, reducing the risk of anomaly shift. Finally, we employ dual-space mixup within a temporal convolutional network to enforce smoother and more robust decision boundaries. Extensive experiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and ESA, demonstrate that CAPMix achieves significant improvements over state-of-the-art baselines, with enhanced robustness against contaminated training data. The code is available at https://github.com/alsike22/CAPMix.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster</title>
<link>https://arxiv.org/abs/2509.06426</link>
<guid>https://arxiv.org/abs/2509.06426</guid>
<content:encoded><![CDATA[
arXiv:2509.06426v1 Announce Type: cross 
Abstract: Computational models are critical to advance our understanding of how neural, biomechanical, and physical systems interact to orchestrate animal behaviors. Despite the availability of near-complete reconstructions of the Drosophila melanogaster central nervous system, musculature, and exoskeleton, anatomically and physically grounded models of fly leg muscles are still missing. These models provide an indispensable bridge between motor neuron activity and joint movements. Here, we introduce the first 3D, data-driven musculoskeletal model of Drosophila legs, implemented in both OpenSim and MuJoCo simulation environments. Our model incorporates a Hill-type muscle representation based on high-resolution X-ray scans from multiple fixed specimens. We present a pipeline for constructing muscle models using morphological imaging data and for optimizing unknown muscle parameters specific to the fly. We then combine our musculoskeletal models with detailed 3D pose estimation data from behaving flies to achieve muscle-actuated behavioral replay in OpenSim. Simulations of muscle activity across diverse walking and grooming behaviors predict coordinated muscle synergies that can be tested experimentally. Furthermore, by training imitation learning policies in MuJoCo, we test the effect of different passive joint properties on learning speed and find that damping and stiffness facilitate learning. Overall, our model enables the investigation of motor control in an experimentally tractable model organism, providing insights into how biomechanics contribute to generation of complex limb movements. Moreover, our model can be used to control embodied artificial agents to generate naturalistic and compliant locomotion in simulated environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HECATE: An ECS-based Framework for Teaching and Developing Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.06431</link>
<guid>https://arxiv.org/abs/2509.06431</guid>
<content:encoded><![CDATA[
arXiv:2509.06431v1 Announce Type: cross 
Abstract: This paper introduces HECATE, a novel framework based on the Entity-Component-System (ECS) architectural pattern that bridges the gap between distributed systems engineering and MAS development. HECATE is built using the Entity-Component-System architectural pattern, leveraging data-oriented design to implement multiagent systems. This approach involves engineering multiagent systems (MAS) from a distributed systems (DS) perspective, integrating agent concepts directly into the DS domain. This approach simplifies MAS development by (i) reducing the need for specialized agent knowledge and (ii) leveraging familiar DS patterns and standards to minimize the agent-specific knowledge required for engineering MAS. We present the framework's architecture, core components, and implementation approach, demonstrating how it supports different agent models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning</title>
<link>https://arxiv.org/abs/2509.06461</link>
<guid>https://arxiv.org/abs/2509.06461</guid>
<content:encoded><![CDATA[
arXiv:2509.06461v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Several Performance Bounds on Decentralized Online Optimization are Highly Conservative and Potentially Misleading</title>
<link>https://arxiv.org/abs/2509.06466</link>
<guid>https://arxiv.org/abs/2509.06466</guid>
<content:encoded><![CDATA[
arXiv:2509.06466v1 Announce Type: cross 
Abstract: We analyze Decentralized Online Optimization algorithms using the Performance Estimation Problem approach which allows, to automatically compute exact worst-case performance of optimization algorithms. Our analysis shows that several available performance guarantees are very conservative, sometimes by multiple orders of magnitude, and can lead to misguided choices of algorithm. Moreover, at least in terms of worst-case performance, some algorithms appear not to benefit from inter-agent communications for a significant period of time. We show how to improve classical methods by tuning their step-sizes, and find that we can save up to 20% on their actual worst-case performance regret.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems</title>
<link>https://arxiv.org/abs/2509.06475</link>
<guid>https://arxiv.org/abs/2509.06475</guid>
<content:encoded><![CDATA[
arXiv:2509.06475v1 Announce Type: cross 
Abstract: AI-based recommender systems increasingly influence recruitment decisions. Thus, transparency and responsible adoption in Human Resource Management (HRM) are critical. This study examines how HR managers' AI literacy influences their subjective perception and objective understanding of explainable AI (XAI) elements in recruiting recommender dashboards. In an online experiment, 410 German-based HR managers compared baseline dashboards to versions enriched with three XAI styles: important features, counterfactuals, and model criteria. Our results show that the dashboards used in practice do not explain AI results and even keep AI elements opaque. However, while adding XAI features improves subjective perceptions of helpfulness and trust among users with moderate or high AI literacy, it does not increase their objective understanding. It may even reduce accurate understanding, especially with complex explanations. Only overlays of important features significantly aided the interpretations of high-literacy users. Our findings highlight that the benefits of XAI in recruitment depend on users' AI literacy, emphasizing the need for tailored explanation strategies and targeted literacy training in HRM to ensure fair, transparent, and effective adoption of AI.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT</title>
<link>https://arxiv.org/abs/2509.06483</link>
<guid>https://arxiv.org/abs/2509.06483</guid>
<content:encoded><![CDATA[
arXiv:2509.06483v1 Announce Type: cross 
Abstract: The wide spreading of Internet of Things (IoT) sensors generates vast spatio-temporal data streams, but ensuring data credibility is a critical yet unsolved challenge for applications like smart homes. While spatio-temporal graph (STG) models are a leading paradigm for such data, they often fall short in dynamic, human-centric environments due to two fundamental limitations: (1) their reliance on static graph topologies, which fail to capture physical, event-driven dynamics, and (2) their tendency to confuse spurious correlations with true causality, undermining robustness in human-centric environments. To address these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network (DyC-STG), a novel framework designed for real-time data credibility analysis in IoT. Our framework features two synergistic contributions: an event-driven dynamic graph module that adapts the graph topology in real-time to reflect physical state changes, and a causal reasoning module to distill causally-aware representations by strictly enforcing temporal precedence. To facilitate the research in this domain we release two new real-world datasets. Comprehensive experiments show that DyC-STG establishes a new state-of-the-art, outperforming the strongest baselines by 1.4 percentage points and achieving an F1-Score of up to 0.930.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients</title>
<link>https://arxiv.org/abs/2509.06516</link>
<guid>https://arxiv.org/abs/2509.06516</guid>
<content:encoded><![CDATA[
arXiv:2509.06516v1 Announce Type: cross 
Abstract: Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in intesive care unit (ICU) and operating room (OR). However, the high incidence of poor, incomplete, and inconsistent signal quality, can lead to false alarms or diagnostic inaccuracies. The methods explored so far suffer from limited generalizability, reliance on extensive labeled data, and poor cross-task transferability. To overcome these challenges, we introduce QualityFM, a novel multimodal foundation model for these physiological signals, designed to acquire a general-purpose understanding of signal quality. Our model is pre-trained on an large-scale dataset comprising over 21 million 30-second waveforms and 179,757 hours of data. Our approach involves a dual-track architecture that processes paired physiological signals of differing quality, leveraging a self-distillation strategy where an encoder for high-quality signals is used to guide the training of an encoder for low-quality signals. To efficiently handle long sequential signals and capture essential local quasi-periodic patterns, we integrate a windowed sparse attention mechanism within our Transformer-based model. Furthermore, a composite loss function, which combines direct distillation loss on encoder outputs with indirect reconstruction loss based on power and phase spectra, ensures the preservation of frequency-domain characteristics of the signals. We pre-train three models with varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy and practical value through transfer learning on three distinct clinical tasks: false alarm of ventricular tachycardia detection, the identification of atrial fibrillation and the estimation of arterial blood pressure (ABP) from PPG and ECG signals.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training</title>
<link>https://arxiv.org/abs/2509.06518</link>
<guid>https://arxiv.org/abs/2509.06518</guid>
<content:encoded><![CDATA[
arXiv:2509.06518v1 Announce Type: cross 
Abstract: Transformer-based language models traditionally use uniform (isotropic) layer sizes, yet they ignore the diverse functional roles that different depths can play and their computational capacity needs. Building on Layer-Wise Scaling (LWS) and pruning literature, we introduce three new LWS variants - Framed, Reverse, and Crown - that redistribute FFN widths and attention heads via two or three-point linear interpolation in the pre-training stage. We present the first systematic ablation of LWS and its variants, on a fixed budget of 180M parameters, trained on 5B tokens. All models converge to similar losses and achieve better performance compared to an equal-cost isotropic baseline, without a substantial decrease in training throughput. This work represents an initial step into the design space of layer-wise architectures for pre-training, but future work should scale experiments to orders of magnitude more tokens and parameters to fully assess their potential.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2509.06531</link>
<guid>https://arxiv.org/abs/2509.06531</guid>
<content:encoded><![CDATA[
arXiv:2509.06531v1 Announce Type: cross 
Abstract: Link prediction in knowledge graphs requires integrating structural information and semantic context to infer missing entities. While large language models offer strong generative reasoning capabilities, their limited exploitation of structural signals often results in structural sparsity and semantic ambiguity, especially under incomplete or zero-shot settings. To address these challenges, we propose SLiNT (Structure-aware Language model with Injection and coNtrastive Training), a modular framework that injects knowledge-graph-derived structural context into a frozen LLM backbone with lightweight LoRA-based adaptation for robust link prediction. Specifically, Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive Learning (DHCL) introduces fine-grained supervision by interpolating hard positives and negatives to resolve entity-level ambiguity; and Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware intervention while preserving the core LLM parameters. Experiments on WN18RR and FB15k-237 show that SLiNT achieves superior or competitive performance compared with both embedding-based and generation-based baselines, demonstrating the effectiveness of structure-aware representation learning for scalable knowledge graph completion.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language Learning''</title>
<link>https://arxiv.org/abs/2509.06535</link>
<guid>https://arxiv.org/abs/2509.06535</guid>
<content:encoded><![CDATA[
arXiv:2509.06535v1 Announce Type: cross 
Abstract: We investigated the reproducibility of FairCLIP, proposed by Luo et al. (2024), for improving the group fairness of CLIP (Radford et al., 2021) by minimizing image-text similarity score disparities across sensitive groups using the Sinkhorn distance. The experimental setup of Luo et al. (2024) was reproduced to primarily investigate the research findings for FairCLIP. The model description by Luo et al. (2024) was found to differ from the original implementation. Therefore, a new implementation, A-FairCLIP, is introduced to examine specific design choices. Furthermore, FairCLIP+ is proposed to extend the FairCLIP objective to include multiple attributes. Additionally, the impact of the distance minimization on FairCLIP's fairness and performance was explored. In alignment with the original authors, CLIP was found to be biased towards certain demographics when applied to zero-shot glaucoma classification using medical scans and clinical notes from the Harvard-FairVLMed dataset. However, the experimental results on two datasets do not support their claim that FairCLIP improves the performance and fairness of CLIP. Although the regularization objective reduces Sinkhorn distances, both the official implementation and the aligned implementation, A-FairCLIP, were not found to improve performance nor fairness in zero-shot glaucoma classification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model</title>
<link>https://arxiv.org/abs/2509.06539</link>
<guid>https://arxiv.org/abs/2509.06539</guid>
<content:encoded><![CDATA[
arXiv:2509.06539v1 Announce Type: cross 
Abstract: CAGE-2 is an accepted benchmark for learning and evaluating defender strategies against cyberattacks. It reflects a scenario where a defender agent protects an IT infrastructure against various attacks. Many defender methods for CAGE-2 have been proposed in the literature. In this paper, we construct a formal model for CAGE-2 using the framework of Partially Observable Markov Decision Process (POMDP). Based on this model, we define an optimal defender strategy for CAGE-2 and introduce a method to efficiently learn this strategy. Our method, called BF-PPO, is based on PPO, and it uses particle filter to mitigate the computational complexity due to the large state space of the CAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and compare its performance with that of CARDIFF, the highest ranked method on the CAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the learned defender strategy and the required training time.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal-Based Malware Classification Using 1D CNNs</title>
<link>https://arxiv.org/abs/2509.06548</link>
<guid>https://arxiv.org/abs/2509.06548</guid>
<content:encoded><![CDATA[
arXiv:2509.06548v1 Announce Type: cross 
Abstract: Malware classification is a contemporary and ongoing challenge in cyber-security: modern obfuscation techniques are able to evade traditional static analysis, while dynamic analysis is too resource intensive to be deployed at a large scale. One prominent line of research addresses these limitations by converting malware binaries into 2D images by heuristically reshaping them into a 2D grid before resizing using Lanczos resampling. These images can then be classified based on their textural information using computer vision approaches. While this approach can detect obfuscated malware more effectively than static analysis, the process of converting files into 2D images results in significant information loss due to both quantisation noise, caused by rounding to integer pixel values, and the introduction of 2D dependencies which do not exist in the original data. This loss of signal limits the classification performance of the downstream model. This work addresses these weaknesses by instead resizing the files into 1D signals which avoids the need for heuristic reshaping, and additionally these signals do not suffer from quantisation noise due to being stored in a floating-point format. It is shown that existing 2D CNN architectures can be readily adapted to classify these 1D signals for improved performance. Furthermore, a bespoke 1D convolutional neural network, based on the ResNet architecture and squeeze-and-excitation layers, was developed to classify these signals and evaluated on the MalNet dataset. It was found to achieve state-of-the-art performance on binary, type, and family level classification with F1 scores of 0.874, 0.503, and 0.507, respectively, paving the way for future models to operate on the proposed signal modality.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs</title>
<link>https://arxiv.org/abs/2509.06550</link>
<guid>https://arxiv.org/abs/2509.06550</guid>
<content:encoded><![CDATA[
arXiv:2509.06550v1 Announce Type: cross 
Abstract: Network intrusion detection remains a critical challenge in cybersecurity. While supervised machine learning models achieve state-of-the-art performance, their reliance on large labelled datasets makes them impractical for many real-world applications. Anomaly detection methods, which train exclusively on benign traffic to identify malicious activity, suffer from high false positive rates, limiting their usability. Recently, self-supervised learning techniques have demonstrated improved performance with lower false positive rates by learning discriminative latent representations of benign traffic. In particular, contrastive self-supervised models achieve this by minimizing the distance between similar (positive) views of benign traffic while maximizing it between dissimilar (negative) views. Existing approaches generate positive views through data augmentation and treat other samples as negative. In contrast, this work introduces Contrastive Learning using Augmented Negative pairs (CLAN), a novel paradigm for network intrusion detection where augmented samples are treated as negative views - representing potentially malicious distributions - while other benign samples serve as positive views. This approach enhances both classification accuracy and inference efficiency after pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset demonstrates that the proposed method surpasses existing self-supervised and anomaly detection techniques in a binary classification task. Furthermore, when fine-tuned on a limited labelled dataset, the proposed approach achieves superior multi-class classification performance compared to existing self-supervised models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Detection and Tracking Based on Radar Range-Doppler Feature</title>
<link>https://arxiv.org/abs/2509.06569</link>
<guid>https://arxiv.org/abs/2509.06569</guid>
<content:encoded><![CDATA[
arXiv:2509.06569v1 Announce Type: cross 
Abstract: Detection and tracking are the basic tasks of radar systems. Current joint detection tracking methods, which focus on dynamically adjusting detection thresholds from tracking results, still present challenges in fully utilizing the potential of radar signals. These are mainly reflected in the limited capacity of the constant false-alarm rate model to accurately represent information, the insufficient depiction of complex scenes, and the limited information acquired by the tracker. We introduce the Integrated Detection and Tracking based on radar feature (InDT) method, which comprises a network architecture for radar signal detection and a tracker that leverages detection assistance. The InDT detector extracts feature information from each Range-Doppler (RD) matrix and then returns the target position through the feature enhancement module and the detection head. The InDT tracker adaptively updates the measurement noise covariance of the Kalman filter based on detection confidence. The similarity of target RD features is measured by cosine distance, which enhances the data association process by combining location and feature information. Finally, the efficacy of the proposed method was validated through testing on both simulated data and publicly available datasets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models</title>
<link>https://arxiv.org/abs/2509.06596</link>
<guid>https://arxiv.org/abs/2509.06596</guid>
<content:encoded><![CDATA[
arXiv:2509.06596v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Spatial and Semantic Embeddings for Stereo Sound Event Localization in Videos</title>
<link>https://arxiv.org/abs/2509.06598</link>
<guid>https://arxiv.org/abs/2509.06598</guid>
<content:encoded><![CDATA[
arXiv:2509.06598v1 Announce Type: cross 
Abstract: In this study, we address the multimodal task of stereo sound event localization and detection with source distance estimation (3D SELD) in regular video content. 3D SELD is a complex task that combines temporal event classification with spatial localization, requiring reasoning across spatial, temporal, and semantic dimensions. The last is arguably the most challenging to model. Traditional SELD approaches typically rely on multichannel input, limiting their capacity to benefit from large-scale pre-training due to data constraints. To overcome this, we enhance a standard SELD architecture with semantic information by integrating pre-trained, contrastive language-aligned models: CLAP for audio and OWL-ViT for visual inputs. These embeddings are incorporated into a modified Conformer module tailored for multimodal fusion, which we refer to as the Cross-Modal Conformer. We perform an ablation study on the development set of the DCASE2025 Task3 Stereo SELD Dataset to assess the individual contributions of the language-aligned models and benchmark against the DCASE Task 3 baseline systems. Additionally, we detail the curation process of large synthetic audio and audio-visual datasets used for model pre-training. These datasets were further expanded through left-right channel swapping augmentation. Our approach, combining extensive pre-training, model ensembling, and visual post-processing, achieved second rank in the DCASE 2025 Challenge Task 3 (Track B), underscoring the effectiveness of our method. Future work will explore the modality-specific contributions and architectural refinements.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards</title>
<link>https://arxiv.org/abs/2509.06602</link>
<guid>https://arxiv.org/abs/2509.06602</guid>
<content:encoded><![CDATA[
arXiv:2509.06602v1 Announce Type: cross 
Abstract: Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology specialists collaboratively assess complex patient cases to determine optimal treatment strategies. A central element of this process is the patient summary, typically compiled by a medical oncologist, radiation oncologist, or surgeon, or their trained medical assistant, who distills heterogeneous medical records into a concise narrative to facilitate discussion. This manual approach is often labor-intensive, subjective, and prone to omissions of critical information. To address these limitations, we introduce the Healthcare Agent Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that coordinates a multi-agent clinical workflow to generate accurate and comprehensive patient summaries for MTBs. Evaluating predicted patient summaries against ground truth presents additional challenges due to stylistic variation, ordering, synonym usage, and phrasing differences, which complicate the measurement of both succinctness and completeness. To overcome these evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework designed to assess the comprehensiveness and succinctness of generated summaries. Using a benchmark dataset derived from de-identified tumor board discussions, we applied TBFact to evaluate our Patient History agent. Results show that the agent captured 94% of high-importance information (including partial entailments) and achieved a TBFact recall of 0.84 under strict entailment criteria. We further demonstrate that TBFact enables a data-free evaluation framework that institutions can deploy locally without sharing sensitive clinical data. Together, HAO and TBFact establish a robust foundation for delivering reliable and scalable support to MTBs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEAM: Brainwave Empathy Assessment Model for Early Childhood</title>
<link>https://arxiv.org/abs/2509.06620</link>
<guid>https://arxiv.org/abs/2509.06620</guid>
<content:encoded><![CDATA[
arXiv:2509.06620v1 Announce Type: cross 
Abstract: Empathy in young children is crucial for their social and emotional development, yet predicting it remains challenging. Traditional methods often only rely on self-reports or observer-based labeling, which are susceptible to bias and fail to objectively capture the process of empathy formation. EEG offers an objective alternative; however, current approaches primarily extract static patterns, neglecting temporal dynamics. To overcome these limitations, we propose a novel deep learning framework, the Brainwave Empathy Assessment Model (BEAM), to predict empathy levels in children aged 4-6 years. BEAM leverages multi-view EEG signals to capture both cognitive and emotional dimensions of empathy. The framework comprises three key components: 1) a LaBraM-based encoder for effective spatio-temporal feature extraction, 2) a feature fusion module to integrate complementary information from multi-view signals, and 3) a contrastive learning module to enhance class separation. Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods across multiple metrics, demonstrating its potential for objective empathy assessment and providing a preliminary insight into early interventions in children's prosocial development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework</title>
<link>https://arxiv.org/abs/2509.06625</link>
<guid>https://arxiv.org/abs/2509.06625</guid>
<content:encoded><![CDATA[
arXiv:2509.06625v1 Announce Type: cross 
Abstract: Plants in their natural habitats endure an array of interacting stresses, both biotic and abiotic, that rarely occur in isolation. Nutrient stress-particularly nitrogen deficiency-becomes even more critical when compounded with drought and weed competition, making it increasingly difficult to distinguish and address its effects. Early detection of nitrogen stress is therefore crucial for protecting plant health and implementing effective management strategies. This study proposes a novel deep learning framework to accurately classify nitrogen stress severity in a combined stress environment. Our model uses a unique blend of four imaging modalities-RGB, multispectral, and two infrared wavelengths-to capture a wide range of physiological plant responses from canopy images. These images, provided as time-series data, document plant health across three levels of nitrogen availability (low, medium, and high) under varying water stress and weed pressures. The core of our approach is a spatio-temporal deep learning pipeline that merges a Convolutional Neural Network (CNN) for extracting spatial features from images with a Long Short-Term Memory (LSTM) network to capture temporal dependencies. We also devised and evaluated a spatial-only CNN pipeline for comparison. Our CNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively surpassing the spatial-only model's 80.45% and other previously reported machine learning method's 76%. These results bring actionable insights based on the power of our CNN-LSTM approach in effectively capturing the subtle and complex interactions between nitrogen deficiency, water stress, and weed pressure. This robust platform offers a promising tool for the timely and proactive identification of nitrogen stress severity, enabling better crop management and improved plant health.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The First Voice Timbre Attribute Detection Challenge</title>
<link>https://arxiv.org/abs/2509.06635</link>
<guid>https://arxiv.org/abs/2509.06635</guid>
<content:encoded><![CDATA[
arXiv:2509.06635v1 Announce Type: cross 
Abstract: The first voice timbre attribute detection challenge is featured in a special session at NCMMSC 2025. It focuses on the explainability of voice timbre and compares the intensity of two speech utterances in a specified timbre descriptor dimension. The evaluation was conducted on the VCTK-RVA dataset. Participants developed their systems and submitted their outputs to the organizer, who evaluated the performance and sent feedback to them. Six teams submitted their outputs, with five providing descriptions of their methodologies.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnalysisGNN: Unified Music Analysis with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.06654</link>
<guid>https://arxiv.org/abs/2509.06654</guid>
<content:encoded><![CDATA[
arXiv:2509.06654v1 Announce Type: cross 
Abstract: Recent years have seen a boom in computational approaches to music analysis, yet each one is typically tailored to a specific analytical domain. In this work, we introduce AnalysisGNN, a novel graph neural network framework that leverages a data-shuffling strategy with a custom weighted multi-task loss and logit fusion between task-specific classifiers to integrate heterogeneously annotated symbolic datasets for comprehensive score analysis. We further integrate a Non-Chord-Tone prediction module, which identifies and excludes passing and non-functional notes from all tasks, thereby improving the consistency of label signals. Experimental evaluations demonstrate that AnalysisGNN achieves performance comparable to traditional static-dataset approaches, while showing increased resilience to domain shifts and annotation inconsistencies across multiple heterogeneous corpora.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations</title>
<link>https://arxiv.org/abs/2509.06665</link>
<guid>https://arxiv.org/abs/2509.06665</guid>
<content:encoded><![CDATA[
arXiv:2509.06665v1 Announce Type: cross 
Abstract: Vehicular ad hoc networks (VANETs) are a crucial component of intelligent transportation systems; however, routing remains challenging due to dynamic topologies, incomplete observations, and the limited resources of edge devices. Existing reinforcement learning (RL) approaches often assume fixed graph structures and require retraining when network conditions change, making them unsuitable for deployment on constrained hardware. We present TrajAware, an RL-based framework designed for edge AI deployment in VANETs. TrajAware integrates three components: (i) action space pruning, which reduces redundant neighbour options while preserving two-hop reachability, alleviating the curse of dimensionality; (ii) graph cross-attention, which maps pruned neighbours to the global graph context, producing features that generalise across diverse network sizes; and (iii) trajectory-aware prediction, which uses historical routes and junction information to estimate real-time positions under partial observations. We evaluate TrajAware in the open-source SUMO simulator using real-world city maps with a leave-one-city-out setup. Results show that TrajAware achieves near-shortest paths and high delivery ratios while maintaining efficiency suitable for constrained edge devices, outperforming state-of-the-art baselines in both full and partial observation scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring</title>
<link>https://arxiv.org/abs/2509.06690</link>
<guid>https://arxiv.org/abs/2509.06690</guid>
<content:encoded><![CDATA[
arXiv:2509.06690v1 Announce Type: cross 
Abstract: Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation</title>
<link>https://arxiv.org/abs/2509.06694</link>
<guid>https://arxiv.org/abs/2509.06694</guid>
<content:encoded><![CDATA[
arXiv:2509.06694v1 Announce Type: cross 
Abstract: While it is well-established that artificial neural networks are \emph{universal approximators} for continuous functions on compact domains, many modern approaches rely on deep or overparameterized architectures that incur high computational costs. In this paper, a new type of \emph{small shallow} neural network, called the \emph{Barycentric Neural Network} ($\BNN$), is proposed, which leverages a fixed set of \emph{base points} and their \emph{barycentric coordinates} to define both its structure and its parameters. We demonstrate that our $\BNN$ enables the exact representation of \emph{continuous piecewise linear functions} ($\CPLF$s), ensuring strict continuity across segments. Since any continuous function over a compact domain can be approximated arbitrarily well by $\CPLF$s, the $\BNN$ naturally emerges as a flexible and interpretable tool for \emph{function approximation}. Beyond the use of this representation, the main contribution of the paper is the introduction of a new variant of \emph{persistent entropy}, a topological feature that is stable and scale invariant, called the \emph{length-weighted persistent entropy} ($\LWPE$), which is weighted by the lifetime of topological features. Our framework, which combines the $\BNN$ with a loss function based on our $\LWPE$, aims to provide flexible and geometrically interpretable approximations of nonlinear continuous functions in resource-constrained settings, such as those with limited base points for $\BNN$ design and few training epochs. Instead of optimizing internal weights, our approach directly \emph{optimizes the base points that define the $\BNN$}. Experimental results show that our approach achieves \emph{superior and faster approximation performance} compared to classical loss functions such as MSE, RMSE, MAE, and log-cosh.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2509.06701</link>
<guid>https://arxiv.org/abs/2509.06701</guid>
<content:encoded><![CDATA[
arXiv:2509.06701v1 Announce Type: cross 
Abstract: We develop a theory of intelligent agency grounded in probabilistic modeling for neural models. Agents are represented as outcome distributions with epistemic utility given by log score, and compositions are defined through weighted logarithmic pooling that strictly improves every member's welfare. We prove that strict unanimity is impossible under linear pooling or in binary outcome spaces, but possible with three or more outcomes. Our framework admits recursive structure via cloning invariance, continuity, and openness, while tilt-based analysis rules out trivial duplication. Finally, we formalize an agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent persona ("Luigi'") induces an antagonistic counterpart ("Waluigi"), while a manifest-then-suppress Waluigi strategy yields strictly larger first-order misalignment reduction than pure Luigi reinforcement alone. These results clarify how developing a principled mathematical framework for how subagents can coalesce into coherent higher-level entities provides novel implications for alignment in agentic AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture</title>
<link>https://arxiv.org/abs/2509.06713</link>
<guid>https://arxiv.org/abs/2509.06713</guid>
<content:encoded><![CDATA[
arXiv:2509.06713v1 Announce Type: cross 
Abstract: Brain tumors are serious health problems that require early diagnosis due to their high mortality rates. Diagnosing tumors by examining Magnetic Resonance Imaging (MRI) images is a process that requires expertise and is prone to error. Therefore, the need for automated diagnosis systems is increasing day by day. In this context, a robust and explainable Deep Learning (DL) model for the classification of brain tumors is proposed. In this study, a publicly available Figshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI images of three tumor types was used. First, the classification performance of nine well-known CNN architectures was evaluated to determine the most effective backbone. Among these, EfficientNetV2 demonstrated the best performance and was selected as the backbone for further development. Subsequently, an attention-based MLP-Mixer architecture was integrated into EfficientNetV2 to enhance its classification capability. The performance of the final model was comprehensively compared with basic CNNs and the methods in the literature. Additionally, Grad-CAM visualization was used to interpret and validate the decision-making process of the proposed model. The proposed model's performance was evaluated using the five-fold cross-validation method. The proposed model demonstrated superior performance with 99.50% accuracy, 99.47% precision, 99.52% recall and 99.49% F1 score. The results obtained show that the model outperforms the studies in the literature. Moreover, Grad-CAM visualizations demonstrate that the model effectively focuses on relevant regions of MRI images, thus improving interpretability and clinical reliability. A robust deep learning model for clinical decision support systems has been obtained by combining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy and interpretability in brain tumor classification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Range Graph Wavelet Networks</title>
<link>https://arxiv.org/abs/2509.06743</link>
<guid>https://arxiv.org/abs/2509.06743</guid>
<content:encoded><![CDATA[
arXiv:2509.06743v1 Announce Type: cross 
Abstract: Modeling long-range interactions, the propagation of information across distant parts of a graph, is a central challenge in graph machine learning. Graph wavelets, inspired by multi-resolution signal processing, provide a principled way to capture both local and global structures. However, existing wavelet-based graph neural networks rely on finite-order polynomial approximations, which limit their receptive fields and hinder long-range propagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which decompose wavelet filters into complementary local and global components. Local aggregation is handled with efficient low-order polynomials, while long-range interactions are captured through a flexible spectral domain parameterization. This hybrid design unifies short- and long-distance information flow within a principled wavelet framework. Experiments show that LR-GWN achieves state-of-the-art performance among wavelet-based methods on long-range benchmarks, while remaining competitive on short-range datasets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2509.06759</link>
<guid>https://arxiv.org/abs/2509.06759</guid>
<content:encoded><![CDATA[
arXiv:2509.06759v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) or multimodal large language models represent a significant advancement in artificial intelligence, enabling systems to understand and generate content across both visual and textual modalities. While large-scale pretraining has driven substantial progress, fine-tuning these models for aligning with human values or engaging in specific tasks or behaviors remains a critical challenge. Deep Reinforcement Learning (DRL) and Direct Preference Optimization (DPO) offer promising frameworks for this aligning process. While DRL enables models to optimize actions using reward signals instead of relying solely on supervised preference data, DPO directly aligns the policy with preferences, eliminating the need for an explicit reward model. This overview explores paradigms for fine-tuning LVLMs, highlighting how DRL and DPO techniques can be used to align models with human preferences and values, improve task performance, and enable adaptive multimodal interaction. We categorize key approaches, examine sources of preference data, reward signals, and discuss open challenges such as scalability, sample efficiency, continual learning, generalization, and safety. The goal is to provide a clear understanding of how DRL and DPO contribute to the evolution of robust and human-aligned LVLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML</title>
<link>https://arxiv.org/abs/2509.06806</link>
<guid>https://arxiv.org/abs/2509.06806</guid>
<content:encoded><![CDATA[
arXiv:2509.06806v1 Announce Type: cross 
Abstract: Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.
  Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.
  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem</title>
<link>https://arxiv.org/abs/2509.06809</link>
<guid>https://arxiv.org/abs/2509.06809</guid>
<content:encoded><![CDATA[
arXiv:2509.06809v1 Announce Type: cross 
Abstract: The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover's saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available.
  https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens</title>
<link>https://arxiv.org/abs/2509.06836</link>
<guid>https://arxiv.org/abs/2509.06836</guid>
<content:encoded><![CDATA[
arXiv:2509.06836v1 Announce Type: cross 
Abstract: Making LLMs more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale. Pruning is a key technique toward this goal. However, prior pruning methods are limited: width pruning often breaks the standard transformer layout or requires custom inference code, while depth pruning removes entire layers and can cause abrupt accuracy drops. In this work, we propose COMPACT, which jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii) prunes FFN intermediate channels using common-token-weighted activations, aligning importance with the post-pruning token distribution. COMPACT enjoys merits of both depth and width pruning, such as: deployment-friendliness (keeps a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN pruning), training-free operation with competitive pruning time, and strong memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show state-of-the-art downstream task performance at similar or higher pruning ratios, with substantial reductions in parameters, GPU memory, and end-to-end latency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning meets bioprocess control through behaviour cloning: Real-world deployment in an industrial photobioreactor</title>
<link>https://arxiv.org/abs/2509.06853</link>
<guid>https://arxiv.org/abs/2509.06853</guid>
<content:encoded><![CDATA[
arXiv:2509.06853v1 Announce Type: cross 
Abstract: The inherent complexity of living cells as production units creates major challenges for maintaining stable and optimal bioprocess conditions, especially in open Photobioreactors (PBRs) exposed to fluctuating environments. To address this, we propose a Reinforcement Learning (RL) control approach, combined with Behavior Cloning (BC), for pH regulation in open PBR systems. This represents, to the best of our knowledge, the first application of an RL-based control strategy to such a nonlinear and disturbance-prone bioprocess. Our method begins with an offline training stage in which the RL agent learns from trajectories generated by a nominal Proportional-Integral-Derivative (PID) controller, without direct interaction with the real system. This is followed by a daily online fine-tuning phase, enabling adaptation to evolving process dynamics and stronger rejection of fast, transient disturbances. This hybrid offline-online strategy allows deployment of an adaptive control policy capable of handling the inherent nonlinearities and external perturbations in open PBRs. Simulation studies highlight the advantages of our method: the Integral of Absolute Error (IAE) was reduced by 8% compared to PID control and by 5% relative to standard off-policy RL. Moreover, control effort decreased substantially-by 54% compared to PID and 7% compared to standard RL-an important factor for minimizing operational costs. Finally, an 8-day experimental validation under varying environmental conditions confirmed the robustness and reliability of the proposed approach. Overall, this work demonstrates the potential of RL-based methods for bioprocess control and paves the way for their broader application to other nonlinear, disturbance-prone systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice</title>
<link>https://arxiv.org/abs/2509.06854</link>
<guid>https://arxiv.org/abs/2509.06854</guid>
<content:encoded><![CDATA[
arXiv:2509.06854v1 Announce Type: cross 
Abstract: Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van Der Heijde Score (TSS) is crucial, but manual scoring is often time-consuming and subjective. This study introduces an Automated Radiographic Sharp Scoring (ARTSS) framework that leverages deep learning to analyze full-hand X-ray images, aiming to reduce inter- and intra-observer variability. The research uniquely accommodates patients with joint disappearance and variable-length image sequences. We developed ARTSS using data from 970 patients, structured into four stages: I) Image pre-processing and re-orientation using ResNet50, II) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and IV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201, EfficientNetB0, and Vision Transformer (ViT). We evaluated model performance with Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute error (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS from two radiologists was used as the ground truth. Model training employed 3-fold cross-validation, with each fold consisting of 452 training and 227 validation samples, and external testing included 291 unseen subjects. Our joint identification model achieved 99% accuracy. The best-performing model, ViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results demonstrate the potential of deep learning to automate RA scoring, which can significantly enhance clinical practice. Our approach addresses the challenge of joint disappearance and variable joint numbers, offers timesaving benefits, reduces inter- and intra-reader variability, improves radiologist accuracy, and aids rheumatologists in making more informed decisions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Interaction and Bias Effects in Opinion Dynamics of Large Language Models</title>
<link>https://arxiv.org/abs/2509.06858</link>
<guid>https://arxiv.org/abs/2509.06858</guid>
<content:encoded><![CDATA[
arXiv:2509.06858v1 Announce Type: cross 
Abstract: Large Language Models are increasingly used to simulate human opinion dynamics, yet the effect of genuine interaction is often obscured by systematic biases. We present a Bayesian framework to disentangle and quantify three such biases: (i) a topic bias toward prior opinions in the training data; (ii) an agreement bias favoring agreement irrespective of the question; and (iii) an anchoring bias toward the initiating agent's stance. Applying this framework to multi-step dialogues reveals that opinion trajectories tend to quickly converge to a shared attractor, with the influence of the interaction fading over time, and the impact of biases differing between LLMs. In addition, we fine-tune an LLM on different sets of strongly opinionated statements (incl. misinformation) and demonstrate that the opinion attractor shifts correspondingly. Exposing stark differences between LLMs and providing quantitative tools to compare them to human subjects in the future, our approach highlights both chances and pitfalls in using LLMs as proxies for human behavior.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL</title>
<link>https://arxiv.org/abs/2509.06863</link>
<guid>https://arxiv.org/abs/2509.06863</guid>
<content:encoded><![CDATA[
arXiv:2509.06863v1 Announce Type: cross 
Abstract: A hallmark of modern large-scale machine learning techniques is the use of training objectives that provide dense supervision to intermediate computations, such as teacher forcing the next token in language models or denoising step-by-step in diffusion models. This enables models to learn complex functions in a generalizable manner. Motivated by this observation, we investigate the benefits of iterative computation for temporal difference (TD) methods in reinforcement learning (RL). Typically they represent value functions in a monolithic fashion, without iterative compute. We introduce floq (flow-matching Q-functions), an approach that parameterizes the Q-function using a velocity field and trains it using techniques from flow-matching, typically used in generative modeling. This velocity field underneath the flow is trained using a TD-learning objective, which bootstraps from values produced by a target velocity field, computed by running multiple steps of numerical integration. Crucially, floq allows for more fine-grained control and scaling of the Q-function capacity than monolithic architectures, by appropriately setting the number of integration steps. Across a suite of challenging offline RL benchmarks and online fine-tuning tasks, floq improves performance by nearly 1.8x. floq scales capacity far better than standard TD-learning architectures, highlighting the potential of iterative computation for value learning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification</title>
<link>https://arxiv.org/abs/2509.06875</link>
<guid>https://arxiv.org/abs/2509.06875</guid>
<content:encoded><![CDATA[
arXiv:2509.06875v1 Announce Type: cross 
Abstract: Class imbalance in machine learning poses a significant challenge, as skewed datasets often hinder performance on minority classes. Traditional oversampling techniques, which are commonly used to alleviate class imbalance, have several drawbacks: they treat features independently, lack similarity-based controls, limit sample diversity, and fail to manage synthetic variety effectively. To overcome these issues, we introduce AxelSMOTE, an innovative agent-based approach that views data instances as autonomous agents engaging in complex interactions. Based on Axelrod's cultural dissemination model, AxelSMOTE implements four key innovations: (1) trait-based feature grouping to preserve correlations; (2) a similarity-based probabilistic exchange mechanism for meaningful interactions; (3) Beta distribution blending for realistic interpolation; and (4) controlled diversity injection to avoid overfitting. Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms state-of-the-art sampling methods while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction</title>
<link>https://arxiv.org/abs/2509.06883</link>
<guid>https://arxiv.org/abs/2509.06883</guid>
<content:encoded><![CDATA[
arXiv:2509.06883v1 Announce Type: cross 
Abstract: We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers</title>
<link>https://arxiv.org/abs/2509.06885</link>
<guid>https://arxiv.org/abs/2509.06885</guid>
<content:encoded><![CDATA[
arXiv:2509.06885v1 Announce Type: cross 
Abstract: Medical image segmentation is a critical task in clinical workflows, particularly for the detection and delineation of pathological regions. While convolutional architectures like U-Net have become standard for such tasks, their limited receptive field restricts global context modeling. Recent efforts integrating transformers have addressed this, but often result in deep, computationally expensive models unsuitable for real-time use. In this work, we present a novel end-to-end lightweight architecture designed specifically for real-time binary medical image segmentation. Our model combines a Swin Transformer-like encoder with a U-Net-like decoder, connected via skip pathways to preserve spatial detail while capturing contextual information. Unlike existing designs such as Swin Transformer or U-Net, our architecture is significantly shallower and competitively efficient. To improve the encoder's ability to learn meaningful features without relying on large amounts of labeled data, we first train it using Barlow Twins, a self-supervised learning method that helps the model focus on important patterns by reducing unnecessary repetition in the learned features. After this pretraining, we fine-tune the entire model for our specific task. Experiments on benchmark binary segmentation tasks demonstrate that our model achieves competitive accuracy with substantially reduced parameter count and faster inference, positioning it as a practical alternative for deployment in real-time and resource-limited clinical environments. The code for our method is available at Github repository: https://github.com/mkianih/Barlow-Swin.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition</title>
<link>https://arxiv.org/abs/2509.06918</link>
<guid>https://arxiv.org/abs/2509.06918</guid>
<content:encoded><![CDATA[
arXiv:2509.06918v1 Announce Type: cross 
Abstract: Robust out-of-distribution (OOD) detection is an indispensable component of modern artificial intelligence (AI) systems, especially in safety-critical applications where models must identify inputs from unfamiliar classes not seen during training. While OOD detection has been extensively studied in the machine learning literature--with both post hoc and training-based approaches--its effectiveness under noisy training labels remains underexplored. Recent studies suggest that label noise can significantly degrade OOD performance, yet principled solutions to this issue are lacking. In this work, we demonstrate that directly combining existing label noise-robust methods with OOD detection strategies is insufficient to address this critical challenge. To overcome this, we propose a robust OOD detection framework that integrates loss correction techniques from the noisy label learning literature with low-rank and sparse decomposition methods from signal processing. Extensive experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms the state-of-the-art OOD detection techniques, particularly under severe noisy label settings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection</title>
<link>https://arxiv.org/abs/2509.06920</link>
<guid>https://arxiv.org/abs/2509.06920</guid>
<content:encoded><![CDATA[
arXiv:2509.06920v1 Announce Type: cross 
Abstract: Insider threats are a growing organizational problem due to the complexity of identifying their technical and behavioral elements. A large research body is dedicated to the study of insider threats from technological, psychological, and educational perspectives. However, research in this domain has been generally dependent on datasets that are static and limited access which restricts the development of adaptive detection models. This study introduces a novel, ethically grounded approach that uses the large language model (LLM) Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which contain indicators of insider threat scenarios. The messages reflect real-world data distributions by being highly imbalanced (1% insider threats). The syslogs were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with their performance evaluated through statistical metrics including precision, recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across nearly all metrics, particularly in reducing false alarms and improving detection accuracy. The results show strong promise for the use of LLMs in synthetic dataset generation and insider threat detection.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2509.06921</link>
<guid>https://arxiv.org/abs/2509.06921</guid>
<content:encoded><![CDATA[
arXiv:2509.06921v1 Announce Type: cross 
Abstract: Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit fundamental limitations: inadequate conceptual grounding leading to non-robustness against novel attacks; limited instructibility impeding analyst-guided adaptation; and misalignment with cybersecurity objectives. Neuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize cybersecurity AI. However, there is no systematic understanding of this emerging approach. These hybrid systems address critical cybersecurity challenges by combining neural pattern recognition with symbolic reasoning, enabling enhanced threat understanding while introducing concerning autonomous offensive capabilities that reshape threat landscapes. In this survey, we systematically characterize this field by analyzing 127 publications spanning 2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A) framework to evaluate these systems, focusing on both cyber defense and cyber offense across network security, malware analysis, and cyber operations. Our analysis shows advantages of multi-agent NeSy architectures and identifies critical implementation challenges including standardization gaps, computational complexity, and human-AI collaboration requirements that constrain deployment. We show that causal reasoning integration is the most transformative advancement, enabling proactive defense beyond correlation-based approaches. Our findings highlight dual-use implications where autonomous systems demonstrate substantial capabilities in zero-day exploitation while achieving significant cost reductions, altering threat dynamics. We provide insights and future research directions, emphasizing the urgent need for community-driven standardization frameworks and responsible development practices that ensure advancement serves defensive cybersecurity objectives while maintaining societal alignment.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers</title>
<link>https://arxiv.org/abs/2509.06938</link>
<guid>https://arxiv.org/abs/2509.06938</guid>
<content:encoded><![CDATA[
arXiv:2509.06938v1 Announce Type: cross 
Abstract: As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interleaving Reasoning for Better Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2509.06945</link>
<guid>https://arxiv.org/abs/2509.06945</guid>
<content:encoded><![CDATA[
arXiv:2509.06945v1 Announce Type: cross 
Abstract: Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</title>
<link>https://arxiv.org/abs/2509.06953</link>
<guid>https://arxiv.org/abs/2509.06953</guid>
<content:encoded><![CDATA[
arXiv:2509.06953v1 Announce Type: cross 
Abstract: Generating collision-free motion in dynamic, partially observable environments is a fundamental challenge for robotic manipulators. Classical motion planners can compute globally optimal trajectories but require full environment knowledge and are typically too slow for dynamic scenes. Neural motion policies offer a promising alternative by operating in closed-loop directly on raw sensory inputs but often struggle to generalize in complex or dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed for reactive motion generation in diverse dynamic environments, operating directly on point cloud sensory input. At its core is IMPACT, a transformer-based neural motion policy pretrained on 10 million generated expert trajectories across diverse simulation scenarios. We further improve IMPACT's static obstacle avoidance through iterative student-teacher finetuning. We additionally enhance the policy's dynamic obstacle avoidance at inference time using DCP-RMP, a locally reactive goal-proposal module. We evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving obstacles, and goal obstructions. DRP achieves strong generalization, outperforming prior classical and neural methods in success rate across both simulated and real-world settings. Video results and code available at https://deep-reactive-policy.com
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</title>
<link>https://arxiv.org/abs/2509.06956</link>
<guid>https://arxiv.org/abs/2509.06956</guid>
<content:encoded><![CDATA[
arXiv:2509.06956v1 Announce Type: cross 
Abstract: Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at https://github.com/NationalGAILab/HoT.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorical semantics of compositional reinforcement learning</title>
<link>https://arxiv.org/abs/2208.13687</link>
<guid>https://arxiv.org/abs/2208.13687</guid>
<content:encoded><![CDATA[
arXiv:2208.13687v3 Announce Type: replace 
Abstract: Compositional knowledge representations in reinforcement learning (RL) facilitate modular, interpretable, and safe task specifications. However, generating compositional models requires the characterization of minimal assumptions for the robustness of the compositionality feature, especially in the case of functional decompositions. Using a categorical point of view, we develop a knowledge representation framework for a compositional theory of RL. Our approach relies on the theoretical study of the category MDP, whose objects are Markov decision processes (MDPs) acting as models of tasks. The categorical semantics models the compositionality of tasks through the application of pushout operations akin to combining puzzle pieces. As a practical application of these pushout operations, we introduce zig-zag diagrams that rely on the compositional guarantees engendered by the category MDP. We further prove that properties of the category MDP unify concepts, such as enforcing safety requirements and exploiting symmetries, generalizing previous abstraction theories for RL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoisyNN: Exploring the Impact of Information Entropy Change in Learning Systems</title>
<link>https://arxiv.org/abs/2309.10625</link>
<guid>https://arxiv.org/abs/2309.10625</guid>
<content:encoded><![CDATA[
arXiv:2309.10625v5 Announce Type: replace 
Abstract: We investigate the impact of entropy change in deep learning systems by noise injection at different levels, including the embedding space and the image. The series of models that employ our methodology are collectively known as Noisy Neural Networks (NoisyNN), with examples such as NoisyViT and NoisyCNN. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this work shows noise can be an effective way to change the entropy of the learning system. We demonstrate that specific noise can boost the performance of various deep models under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the information entropy to define the complexity of the task. We categorize the noise into two types, positive noise (PN) and harmful noise (HN), based on whether the noise can help reduce the task complexity. Extensive experiments of CNNs and ViTs have shown performance improvements by proactively injecting positive noise, where we achieved an unprecedented top 1 accuracy of 95$\%$ on ImageNet. Both theoretical analysis and empirical evidence have confirmed that the presence of positive noise, can benefit the learning process, while the traditionally perceived harmful noise indeed impairs deep learning models. The different roles of noise offer new explanations for deep models on specific tasks and provide a new paradigm for improving model performance. Moreover, it reminds us that we can influence the performance of learning systems via information entropy change.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GameGPT: Multi-agent Collaborative Framework for Game Development</title>
<link>https://arxiv.org/abs/2310.08067</link>
<guid>https://arxiv.org/abs/2310.08067</guid>
<content:encoded><![CDATA[
arXiv:2310.08067v2 Announce Type: replace 
Abstract: The large language model (LLM) based agents have demonstrated their capacity to automate and expedite software development processes. In this paper, we focus on game development and propose a multi-agent collaborative framework, dubbed GameGPT, to automate game development. While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern: redundancy. Our framework presents a series of methods to mitigate both concerns. These methods include dual collaboration and layered approaches with several in-house lexicons, to mitigate the hallucination and redundancy in the planning, task identification, and implementation phases. Furthermore, a decoupling approach is also introduced to achieve code generation with better precision.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Prompt Pricing based on Combinatorial Multi-Armed Bandit and Hierarchical Stackelberg Game</title>
<link>https://arxiv.org/abs/2405.15154</link>
<guid>https://arxiv.org/abs/2405.15154</guid>
<content:encoded><![CDATA[
arXiv:2405.15154v3 Announce Type: replace 
Abstract: Generation models have shown promising performance in various tasks, making trading around machine learning models possible. In this paper, we aim at a novel prompt trading scenario, prompt bundle trading (PBT) system, and propose an online pricing mechanism. Based on the combinatorial multi-armed bandit (CMAB) and three-stage hierarchical Stackelburg (HS) game, our pricing mechanism considers the profits of the consumer, platform, and seller, simultaneously achieving the profit satisfaction of these three participants. We break down the pricing issue into two steps, namely unknown category selection and incentive strategy optimization. The former step is to select a set of categories with the highest qualities, and the latter is to derive the optimal strategy for each participant based on the chosen categories. Unlike the existing fixed pricing mode, the PBT pricing mechanism we propose is more flexible and diverse, which is more in accord with the transaction needs of real-world scenarios. We test our method on a simulated text-to-image dataset. The experimental results demonstrate the effectiveness of our algorithm, which provides a feasible price-setting standard for the prompt marketplaces.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Wearable Data into Personal Health Insights using Large Language Model Agents</title>
<link>https://arxiv.org/abs/2406.06464</link>
<guid>https://arxiv.org/abs/2406.06464</guid>
<content:encoded><![CDATA[
arXiv:2406.06464v4 Announce Type: replace 
Abstract: Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents</title>
<link>https://arxiv.org/abs/2406.10291</link>
<guid>https://arxiv.org/abs/2406.10291</guid>
<content:encoded><![CDATA[
arXiv:2406.10291v3 Announce Type: replace 
Abstract: Large language models (LLMs) excel across many natural language processing tasks but face challenges in domain-specific, analytical tasks such as conducting research surveys. This study introduces ResearchArena, a benchmark designed to evaluate LLMs' capabilities in conducting academic surveys -- a foundational step in academic research. ResearchArena models the process in three stages: (1) information discovery, identifying relevant literature; (2) information selection, evaluating papers' relevance and impact; and (3) information organization, structuring knowledge into hierarchical frameworks such as mind-maps. Notably, mind-map construction is treated as a bonus task, reflecting its supplementary role in survey-writing. To support these evaluations, we construct an offline environment of 12M full-text academic papers and 7.9K survey papers. To ensure ethical compliance, we do not redistribute copyrighted materials; instead, we provide code to construct the environment from the Semantic Scholar Open Research Corpus (S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform compared to simpler keyword-based retrieval methods, though recent reasoning models such as DeepSeek-R1 show slightly better zero-shot performance. These results underscore significant opportunities for advancing LLMs in autonomous research. We open-source the code to construct the ResearchArena benchmark at https://github.com/cxcscmu/ResearchArena.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Labyrinth: Evaluating LLMs' Ability to Reason About Search Problems</title>
<link>https://arxiv.org/abs/2406.12172</link>
<guid>https://arxiv.org/abs/2406.12172</guid>
<content:encoded><![CDATA[
arXiv:2406.12172v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have recently achieved impressive performance in math and reasoning benchmarks. However, they often struggle with logic problems and puzzles that are relatively easy for humans. To further investigate this, we introduce a new benchmark, SearchBench, which contains 11 unique search problems, each equipped with automated pipelines to generate an arbitrary number of instances and analyze the feasibility, correctness, and optimality of LLM-generated solutions. We show that using language-only reasoning, even the most advanced LLMs fail to solve SearchBench end-to-end, e.g., OpenAI's frontier models GPT4 and o1-preview solve only 1.4% and 18.6% of SearchBench problems, respectively. The reason is that SearchBench problems require considering multiple pathways to the solution and performing backtracking, posing a significant challenge to auto-regressive models. Instructing LLMs to generate code that solves the problem helps, but only slightly, e.g., GPT4's performance rises to 11.7%. Interestingly, we show that the current strongest baseline on SearchBench is obtained using in-context learning with A* algorithm implementations. We further show that this baseline can be further enhanced via a Multi-Stage-Multi-Try inference method, raising GPT4's performance above 57%.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese Travel Planning</title>
<link>https://arxiv.org/abs/2412.13682</link>
<guid>https://arxiv.org/abs/2412.13682</guid>
<content:encoded><![CDATA[
arXiv:2412.13682v4 Announce Type: replace 
Abstract: Recent advances in LLMs, particularly in language reasoning and tool integration, have rapidly sparked the \emph{Language Agents} for real-world development. Among these, travel planning represents a prominent domain, combining complex multi-objective planning challenges with practical deployment demands. However, existing benchmarks often oversimplify real-world requirements by focusing on synthetic queries and limited constraints. We address the gap of evaluating language agents in multi-day, multi-POI travel planning scenarios with diverse and open human needs. Specifically, we introduce \emph{ChinaTravel}, the first open-ended benchmark grounded in authentic Chinese travel requirements collected from 1,154 human participants. We design a compositionally generalizable domain-specific language (DSL) for scalable evaluation, covering feasibility, constraint satisfaction, and preference comparison. Empirical studies reveal the potential of neuro-symbolic agents in travel planning, achieving a 37.0\% constraint satisfaction rate on human queries, a 10\times improvement over purely neural models. These findings highlight ChinaTravel as a pivotal milestone for advancing language agents in complex, real-world planning scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2502.07350</link>
<guid>https://arxiv.org/abs/2502.07350</guid>
<content:encoded><![CDATA[
arXiv:2502.07350v2 Announce Type: replace 
Abstract: As scaling large language models faces prohibitive costs, multi-agent systems emerge as a promising alternative, though challenged by static knowledge assumptions and coordination inefficiencies. We introduces Knowledge-Aware Bayesian Bandits (KABB), a novel framework that enhances multi-agent system coordination through semantic understanding and dynamic adaptation. The framework features three key innovations: a three-dimensional knowledge distance model for deep semantic understanding, a dual-adaptation mechanism for continuous expert optimization, and a knowledge-aware Thompson Sampling strategy for efficient expert selection. Extensive evaluation demonstrates KABB achieves an optimal cost-performance balance, maintaining high performance while keeping computational demands relatively low in multi-agent coordination.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Antidistillation Sampling</title>
<link>https://arxiv.org/abs/2504.13146</link>
<guid>https://arxiv.org/abs/2504.13146</guid>
<content:encoded><![CDATA[
arXiv:2504.13146v4 Announce Type: replace 
Abstract: Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation</title>
<link>https://arxiv.org/abs/2504.13707</link>
<guid>https://arxiv.org/abs/2504.13707</guid>
<content:encoded><![CDATA[
arXiv:2504.13707v2 Announce Type: replace 
Abstract: As the general capabilities of large language models (LLMs) improve and agent applications become more widespread, the underlying deception risks urgently require systematic evaluation and effective oversight. Unlike existing evaluation which uses simulated games or presents limited choices, we introduce OpenDeception, a novel deception evaluation framework with an open-ended scenario dataset. OpenDeception jointly evaluates both the deception intention and capabilities of LLM-based agents by inspecting their internal reasoning process. Specifically, we construct five types of common use cases where LLMs intensively interact with the user, each consisting of ten diverse, concrete scenarios from the real world. To avoid ethical concerns and costs of high-risk deceptive interactions with human testers, we propose to simulate the multi-turn dialogue via agent simulation. Extensive evaluation of eleven mainstream LLMs on OpenDeception highlights the urgent need to address deception risks and security concerns in LLM-based agents: the deception intention ratio across the models exceeds 80%, while the deception success rate surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do exhibit a higher risk of deception, which calls for more alignment efforts on inhibiting deceptive behaviors.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration</title>
<link>https://arxiv.org/abs/2505.11191</link>
<guid>https://arxiv.org/abs/2505.11191</guid>
<content:encoded><![CDATA[
arXiv:2505.11191v2 Announce Type: replace 
Abstract: As embodied AI systems become increasingly multi-modal, personalized, and interactive, they must learn effectively from diverse sensory inputs, adapt continually to user preferences, and operate safely under resource and privacy constraints. These challenges expose a pressing need for machine learning models capable of swift, context-aware adaptation while balancing model generalization and personalization. Here, two methods emerge as suitable candidates, each offering parts of these capabilities: multi-modal multi-task foundation models (M3T-FMs) provide a pathway toward generalization across tasks and modalities, whereas federated learning (FL) offers the infrastructure for distributed, privacy-preserving model updates and user-level model personalization. However, when used in isolation, each of these approaches falls short of meeting the complex and diverse capability requirements of real-world embodied AI environments. In this vision paper, we introduce multi-modal multi-task federated foundation models (M3T-FFMs) for embodied AI, a new paradigm that unifies the strengths of M3T-FMs with the privacy-preserving distributed training nature of FL, enabling intelligent systems at the wireless edge. We collect critical deployment dimensions of M3T-FFMs in embodied AI ecosystems under a unified framework, which we name "EMBODY": Embodiment heterogeneity, Modality richness and imbalance, Bandwidth and compute constraints, On-device continual learning, Distributed control and autonomy, and Yielding safety, privacy, and personalization. For each, we identify concrete challenges and envision actionable research directions. We also present an evaluation framework for deploying M3T-FFMs in embodied AI systems, along with the associated trade-offs. Finally, we present a prototype implementation of M3T-FFMs and evaluate their energy and latency performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting</title>
<link>https://arxiv.org/abs/2505.20521</link>
<guid>https://arxiv.org/abs/2505.20521</guid>
<content:encoded><![CDATA[
arXiv:2505.20521v2 Announce Type: replace 
Abstract: This paper presents Project Riley, a novel multimodal and multi-model conversational AI architecture oriented towards the simulation of reasoning influenced by emotional states. Drawing inspiration from Pixar's Inside Out, the system comprises five distinct emotional agents - Joy, Sadness, Fear, Anger, and Disgust - that engage in structured multi-round dialogues to generate, criticise, and iteratively refine responses. A final reasoning mechanism synthesises the contributions of these agents into a coherent output that either reflects the dominant emotion or integrates multiple perspectives. The architecture incorporates both textual and visual large language models (LLMs), alongside advanced reasoning and self-refinement processes. A functional prototype was deployed locally in an offline environment, optimised for emotional expressiveness and computational efficiency. From this initial prototype, another one emerged, called Armando, which was developed for use in emergency contexts, delivering emotionally calibrated and factually accurate information through the integration of Retrieval-Augmented Generation (RAG) and cumulative context tracking. The Project Riley prototype was evaluated through user testing, in which participants interacted with the chatbot and completed a structured questionnaire assessing three dimensions: Emotional Appropriateness, Clarity and Utility, and Naturalness and Human-likeness. The results indicate strong performance in structured scenarios, particularly with respect to emotional alignment and communicative clarity.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUDER: Self-Improving Unified Large Multimodal Models for Understanding and Generation with Dual Self-Rewards</title>
<link>https://arxiv.org/abs/2506.07963</link>
<guid>https://arxiv.org/abs/2506.07963</guid>
<content:encoded><![CDATA[
arXiv:2506.07963v3 Announce Type: replace 
Abstract: Building upon large language models (LLMs), recent large multimodal models (LMMs) unify cross-model understanding and generation into a single framework. However, LMMs still struggle to achieve accurate vision-language alignment, prone to generating text responses contradicting the visual input or failing to follow the text-to-image prompts. Current solutions require external supervision (e.g., human feedback or reward models) and only address unidirectional tasks-either understanding or generation. In this work, based on the observation that understanding and generation are naturally inverse dual tasks, we propose \textbf{SUDER} (\textbf{S}elf-improving \textbf{U}nified LMMs with \textbf{D}ual s\textbf{E}lf-\textbf{R}ewards), a framework reinforcing the understanding and generation capabilities of LMMs with a self-supervised dual reward mechanism. SUDER leverages the inherent duality between understanding and generation tasks to provide self-supervised optimization signals for each other. Specifically, we sample multiple outputs for a given input in one task domain, then reverse the input-output pairs to compute the dual likelihood within the model as self-rewards for optimization. Extensive experimental results on visual understanding and generation benchmarks demonstrate that our method can effectively enhance the performance of the model without any external supervision, especially achieving remarkable improvements in text-to-image tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness</title>
<link>https://arxiv.org/abs/2506.08532</link>
<guid>https://arxiv.org/abs/2506.08532</guid>
<content:encoded><![CDATA[
arXiv:2506.08532v2 Announce Type: replace 
Abstract: The rapid growth of the low-altitude economy has driven the widespread adoption of unmanned aerial vehicles (UAVs). This growing deployment presents new challenges for UAV trajectory planning in complex urban environments. However, existing studies often overlook key factors, such as urban airspace constraints and economic efficiency, which are essential in low-altitude economy contexts. Deep reinforcement learning (DRL) is regarded as a promising solution to these issues, while its practical adoption remains limited by low learning efficiency. To overcome this limitation, we propose a novel UAV trajectory planning framework that combines DRL with large language model (LLM) reasoning to enable safe, compliant, and economically viable path planning. Experimental results demonstrate that our method significantly outperforms existing baselines across multiple metrics, including data collection rate, collision avoidance, successful landing, regulatory compliance, and energy efficiency. These results validate the effectiveness of our approach in addressing UAV trajectory planning key challenges under constraints of the low-altitude economy networking.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Recurrence in Contextual MDPs for Universal Morphology Control</title>
<link>https://arxiv.org/abs/2506.08630</link>
<guid>https://arxiv.org/abs/2506.08630</guid>
<content:encoded><![CDATA[
arXiv:2506.08630v2 Announce Type: replace 
Abstract: A universal controller for any robot morphology would greatly improve computational and data efficiency. By utilizing contextual information about the properties of individual robots and exploiting their modular structure in the architecture of deep reinforcement learning agents, steps have been made towards multi-robot control. Generalization to new, unseen robots, however, remains a challenge. In this paper we hypothesize that the relevant contextual information is partially observable, but that it can be inferred through interactions for better generalization to contexts that are not seen during training. To this extent, we implement a modular recurrent architecture and evaluate its generalization performance on a large set of MuJoCo robots. The results show a substantial improved performance on robots with unseen dynamics, kinematics, and topologies, in four different environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.14990</link>
<guid>https://arxiv.org/abs/2506.14990</guid>
<content:encoded><![CDATA[
arXiv:2506.14990v2 Announce Type: replace 
Abstract: Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms, with environment availability strongly impacting research. One particularly underexplored intersection is continual learning (CL) in cooperative multi-agent settings. To remedy this, we introduce MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark tailored for continual multi-agent reinforcement learning (CMARL). Existing CL benchmarks run environments on the CPU, leading to computational bottlenecks and limiting the length of task sequences. MEAL leverages JAX for GPU acceleration, enabling continual learning across sequences of 100 tasks on a standard desktop PC in a few hours. We show that naively combining popular CL and MARL methods yields strong performance on simple environments, but fails to scale to more complex settings requiring sustained coordination and adaptation. Our ablation study identifies architectural and algorithmic features critical for CMARL on MEAL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis</title>
<link>https://arxiv.org/abs/2506.23273</link>
<guid>https://arxiv.org/abs/2506.23273</guid>
<content:encoded><![CDATA[
arXiv:2506.23273v2 Announce Type: replace 
Abstract: Despite the advancements of large language models, text2sql still faces many challenges, particularly with complex and domain-specific queries. In finance, database designs and financial reporting layouts vary widely between financial entities and countries, making text2sql even more challenging. We present FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries over financial statements. Tailored to local standards like VAS, it combines large and small language models in a multi-agent setup for entity extraction, SQL generation, and self-correction. We build a domain-specific database and evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves 61.33\% accuracy with sub-4-second response times on consumer hardware, outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient solution for financial analysis, making AI-powered querying accessible to Vietnamese enterprises.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis</title>
<link>https://arxiv.org/abs/2507.03460</link>
<guid>https://arxiv.org/abs/2507.03460</guid>
<content:encoded><![CDATA[
arXiv:2507.03460v2 Announce Type: replace 
Abstract: Identifying associations between imaging phenotypes, disease risk factors, and clinical outcomes is essential for understanding disease mechanisms. However, traditional approaches rely on human-driven hypothesis testing and selection of association factors, often overlooking complex, non-linear dependencies among imaging phenotypes and other multi-modal data. To address this, we introduce Multi-agent Exploratory Synergy for the Heart (MESHAgents): a framework that leverages large language models as agents to dynamically elicit, surface, and decide confounders and phenotypes in association studies. Specifically, we orchestrate a multi-disciplinary team of AI agents, which spontaneously generate and converge on insights through iterative, self-organizing reasoning. The framework dynamically synthesizes statistical correlations with multi-expert consensus, providing an automated pipeline for phenome-wide association studies (PheWAS). We demonstrate the system's capabilities through a population-based study of imaging phenotypes of the heart and aorta. MESHAgents autonomously uncovered correlations between imaging phenotypes and a wide range of non-imaging factors, identifying additional confounder variables beyond standard demographic factors. Validation on diagnosis tasks reveals that MESHAgents-discovered phenotypes achieve performance comparable to expert-selected phenotypes, with mean AUC differences as small as $-0.004_{\pm0.010}$ on disease classification tasks. Notably, the recall score improves for 6 out of 9 disease types. Our framework provides clinically relevant imaging phenotypes with transparent reasoning, offering a scalable alternative to expert-driven methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation</title>
<link>https://arxiv.org/abs/2507.05541</link>
<guid>https://arxiv.org/abs/2507.05541</guid>
<content:encoded><![CDATA[
arXiv:2507.05541v2 Announce Type: replace 
Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine learning predictions by highlighting minimal changes required to alter an outcome. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. In this work, we explore large language models (LLMs), specifically GPT-4o-mini, for generating CFs in a zero-shot and three-shot setting. We evaluate our approach on two datasets: the AI-Readi flagship dataset for stress prediction and a public dataset for heart disease detection. Compared to traditional methods such as DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high plausibility (up to 99%), strong validity (up to 0.99), and competitive sparsity. Moreover, using LLM-generated CFs as augmented samples improves downstream classifier performance (an average accuracy gain of 5%), especially in low-data regimes. This demonstrates the potential of prompt-based generative techniques to enhance explainability and robustness in clinical and physiological prediction tasks. Code base: github.com/shovito66/SenseCF.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Urban Planing AI Agent in the Age of Agentic AI</title>
<link>https://arxiv.org/abs/2507.14730</link>
<guid>https://arxiv.org/abs/2507.14730</guid>
<content:encoded><![CDATA[
arXiv:2507.14730v3 Announce Type: replace 
Abstract: Generative AI, large language models, and agentic AI have emerged separately of urban planning. However, the convergence between AI and urban planning presents an interesting opportunity towards AI urban planners. Existing studies conceptualizes urban planning as a generative AI task, where AI synthesizes land-use configurations under geospatial, social, and human-centric constraints and reshape automated urban design. We further identify critical gaps of existing generative urban planning studies: 1) the generative structure has to be predefined with strong assumption: all of adversarial generator-discriminator, forward and inverse diffusion structures, hierarchical zone-POI generative structure are predefined by humans; 2) ignore the power of domain expert developed tools: domain urban planners have developed various tools in the urban planning process guided by urban theory, while existing pure neural networks based generation ignore the power of the tools developed by urban planner practitioners. To address these limitations, we outline a future research direction agentic urban AI planner, calling for a new synthesis of agentic AI and participatory urbanism.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairReason: Balancing Reasoning and Social Bias in MLLMs</title>
<link>https://arxiv.org/abs/2507.23067</link>
<guid>https://arxiv.org/abs/2507.23067</guid>
<content:encoded><![CDATA[
arXiv:2507.23067v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art results across a wide range of tasks and modalities. To push their reasoning ability further, recent studies explore advanced prompting schemes and post-training fine-tuning. Although these techniques improve logical accuracy, they frequently leave the models' outputs burdened with pronounced social biases. Clarifying how reasoning gains interact with bias mitigation-and whether the two objectives inherently trade off-therefore remains an open and pressing research problem. Our study begins by benchmarking three bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation (KD), and rule-based reinforcement learning (RL)-under identical conditions, establishing their baseline strengths and weaknesses. Building on these results, we vary the proportion of debias-focused and reasoning-centric samples within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement learning cuts stereotype scores by 10% while retaining 88% of the model's original reasoning accuracy, offering concrete guidance for balancing fairness and capability in MLLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety</title>
<link>https://arxiv.org/abs/2508.03864</link>
<guid>https://arxiv.org/abs/2508.03864</guid>
<content:encoded><![CDATA[
arXiv:2508.03864v2 Announce Type: replace 
Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit strong collaboration and performance. However, their growing openness and interaction complexity pose serious risks, notably jailbreak and adversarial attacks. Existing defenses typically rely on external guard modules, such as dedicated safety agents, to handle unsafe behaviors. Unfortunately, this paradigm faces two challenges: (1) standalone agents offer limited protection, and (2) their independence leads to single-point failure-if compromised, system-wide safety collapses. Naively increasing the number of guard agents further raises cost and complexity. To address these challenges, we propose Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that enables all task agents to jointly acquire defensive capabilities. Rather than relying on external safety modules, Evo-MARL trains each agent to simultaneously perform its primary function and resist adversarial threats, ensuring robustness without increasing system overhead or single-node failure. Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing reinforcement learning to co-evolve attackers and defenders. This adversarial training paradigm internalizes safety mechanisms and continually enhances MAS performance under co-evolving threats. Experiments show that Evo-MARL reduces attack success rates by up to 22% while boosting accuracy by up to 5% on reasoning tasks-demonstrating that safety and utility can be jointly improved.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media</title>
<link>https://arxiv.org/abs/2508.05557</link>
<guid>https://arxiv.org/abs/2508.05557</guid>
<content:encoded><![CDATA[
arXiv:2508.05557v3 Announce Type: replace 
Abstract: Social media has evolved into a complex multimodal environment where text, images, and other signals interact to shape nuanced meanings, often concealing harmful intent. Identifying such intent, whether sarcasm, hate speech, or misinformation, remains challenging due to cross-modal contradictions, rapid cultural shifts, and subtle pragmatic cues. To address these challenges, we propose MV-Debate, a multi-view agent debate framework with dynamic reflection gating for unified multimodal harmful content detection. MV-Debate assembles four complementary debate agents, a surface analyst, a deep reasoner, a modality contrast, and a social contextualist, to analyze content from diverse interpretive perspectives. Through iterative debate and reflection, the agents refine responses under a reflection-gain criterion, ensuring both accuracy and efficiency. Experiments on three benchmark datasets demonstrate that MV-Debate significantly outperforms strong single-model and existing multi-agent debate baselines. This work highlights the promise of multi-agent debate in advancing reliable social intent detection in safety-critical online contexts.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPF-World: Action World Model for Multi-Agent Path Finding</title>
<link>https://arxiv.org/abs/2508.12087</link>
<guid>https://arxiv.org/abs/2508.12087</guid>
<content:encoded><![CDATA[
arXiv:2508.12087v2 Announce Type: replace 
Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free paths from the designated start locations to goal positions for multiple agents. It underlies a variety of real-world tasks, including multi-robot coordination, robot-assisted logistics, and social navigation. Recent decentralized learnable solvers have shown great promise for large-scale MAPF, especially when leveraging foundation models and large datasets. However, these agents are reactive policy models and exhibit limited modeling of environmental temporal dynamics and inter-agent dependencies, resulting in performance degradation in complex, long-term planning scenarios. To address these limitations, we propose MAPF-World, an autoregressive action world model for MAPF that unifies situation understanding and action generation, guiding decisions beyond immediate local observations. It improves situational awareness by explicitly modeling environmental dynamics, including spatial features and temporal dependencies, through future state and actions prediction. By incorporating these predicted futures, MAPF-World enables more informed, coordinated, and far-sighted decision-making, especially in complex multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an automatic map generator grounded in real-world scenarios, capturing practical map layouts for training and evaluating MAPF solvers. Extensive experiments demonstrate that MAPF-World outperforms state-of-the-art learnable solvers, showcasing superior zero-shot generalization to out-of-distribution cases. Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM + ASP Workflow for Joint Entity-Relation Extraction</title>
<link>https://arxiv.org/abs/2508.12611</link>
<guid>https://arxiv.org/abs/2508.12611</guid>
<content:encoded><![CDATA[
arXiv:2508.12611v2 Announce Type: replace 
Abstract: Joint entity-relation extraction (JERE) identifies both entities and their relationships simultaneously. Traditional machine-learning based approaches to performing this task require a large corpus of annotated data and lack the ability to easily incorporate domain specific information in the construction of the model. Therefore, creating a model for JERE is often labor intensive, time consuming, and elaboration intolerant. In this paper, we propose harnessing the capabilities of generative pretrained large language models (LLMs) and the knowledge representation and reasoning capabilities of Answer Set Programming (ASP) to perform JERE. We present a generic workflow for JERE using LLMs and ASP. The workflow is generic in the sense that it can be applied for JERE in any domain. It takes advantage of LLM's capability in natural language understanding in that it works directly with unannotated text. It exploits the elaboration tolerant feature of ASP in that no modification of its core program is required when additional domain specific knowledge, in the form of type specifications, is found and needs to be used. We demonstrate the usefulness of the proposed workflow through experiments with limited training data on three well-known benchmarks for JERE. The results of our experiments show that the LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10\% of training data. It is able to achieve a 2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the SciERC corpus, one of the most difficult benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding</title>
<link>https://arxiv.org/abs/2508.19576</link>
<guid>https://arxiv.org/abs/2508.19576</guid>
<content:encoded><![CDATA[
arXiv:2508.19576v2 Announce Type: replace 
Abstract: With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified LLM RL paradigm that significantly improves LLM's code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time decoding method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of LLM policy has been improved, we further propose a test time decoding optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When decoding, the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the LLM policy to achieve high reasoning accuracy. We conduct extensive experiments on coding problems to verify the validity of the proposed RL paradigm. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of LLM policies. Codes for our project can be found at https://github.com/THUDM/ReST-RL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Machines Imitate Humans? Integrative Turing-like tests for Language and Vision Demonstrate a Narrowing Gap</title>
<link>https://arxiv.org/abs/2211.13087</link>
<guid>https://arxiv.org/abs/2211.13087</guid>
<content:encoded><![CDATA[
arXiv:2211.13087v3 Announce Type: replace-cross 
Abstract: As AI becomes increasingly embedded in daily life, ascertaining whether an agent is human is critical. We systematically benchmark AI's ability to imitate humans in three language tasks (image captioning, word association, conversation) and three vision tasks (color estimation, object detection, attention prediction), collecting data from 636 humans and 37 AI agents. Next, we conducted 72,191 Turing-like tests with 1,916 human judges and 10 AI judges. Current AIs are approaching the ability to convincingly impersonate humans and deceive human judges in both language and vision. Even simple AI judges outperformed humans in distinguishing AI from human responses. Imitation ability showed minimal correlation with conventional AI performance metrics, suggesting that passing as human is an important independent evaluation criterion. The large-scale Turing datasets and metrics introduced here offer valuable benchmarks for assessing human-likeness in AI and highlight the importance of rigorous, quantitative imitation tests for AI development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catapult Dynamics and Phase Transitions in Quadratic Nets</title>
<link>https://arxiv.org/abs/2301.07737</link>
<guid>https://arxiv.org/abs/2301.07737</guid>
<content:encoded><![CDATA[
arXiv:2301.07737v2 Announce Type: replace-cross 
Abstract: Neural networks trained with gradient descent can undergo non-trivial phase transitions as a function of the learning rate. In \cite{lewkowycz2020large} it was discovered that wide neural nets can exhibit a catapult phase for super-critical learning rates, where the training loss grows exponentially quickly at early times before rapidly decreasing to a small value. During this phase the top eigenvalue of the neural tangent kernel (NTK) also undergoes significant evolution. In this work, we will prove that the catapult phase exists in a large class of models, including quadratic models and two-layer, homogenous neural nets. To do this, we show that for a certain range of learning rates the weight norm decreases whenever the loss becomes large. We also empirically study learning rates beyond this theoretically derived range and show that the activation map of ReLU nets trained with super-critical learning rates becomes increasingly sparse as we increase the learning rate.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation</title>
<link>https://arxiv.org/abs/2309.14394</link>
<guid>https://arxiv.org/abs/2309.14394</guid>
<content:encoded><![CDATA[
arXiv:2309.14394v2 Announce Type: replace-cross 
Abstract: In this work, we address the challenge of multi-domain translation, where the objective is to learn mappings between arbitrary configurations of domains within a defined set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for three domains) without the need for separate models for each specific translation configuration, enabling more efficient and flexible domain translation. We introduce Multi-Domain Diffusion (MDD), a method with dual purposes: i) reconstructing any missing views for new data objects, and ii) enabling learning in semi-supervised contexts with arbitrary supervision configurations. MDD achieves these objectives by exploiting the noise formulation of diffusion models, specifically modeling one noise level per domain. Similar to existing domain translation approaches, MDD learns the translation between any combination of domains. However, unlike prior work, our formulation inherently handles semi-supervised learning without modification by representing missing views as noise in the diffusion process. We evaluate our approach through domain translation experiments on BL3NDT, a multi-domain synthetic dataset designed for challenging semantic domain inversion, the BraTS2020 dataset, and the CelebAMask-HQ dataset.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of Machine Learning Techniques in Imbalanced Data and Future Trends</title>
<link>https://arxiv.org/abs/2310.07917</link>
<guid>https://arxiv.org/abs/2310.07917</guid>
<content:encoded><![CDATA[
arXiv:2310.07917v2 Announce Type: replace-cross 
Abstract: For over two decades, detecting rare events has been a challenging task among researchers in the data mining and machine learning domain. Real-life problems inspire researchers to navigate and further improve data processing and algorithmic approaches to achieve effective and computationally efficient methods for imbalanced learning. In this paper, we have collected and reviewed 258 peer-reviewed papers from archival journals and conference papers in an attempt to provide an in-depth review of various approaches in imbalanced learning from technical and application perspectives. This work aims to provide a structured review of methods used to address the problem of imbalanced data in various domains and create a general guideline for researchers in academia or industry who want to dive into the broad field of machine learning using large-scale imbalanced data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-driven Community Resilience Rating based on Intertwined Socio-Technical Systems Features</title>
<link>https://arxiv.org/abs/2311.01661</link>
<guid>https://arxiv.org/abs/2311.01661</guid>
<content:encoded><![CDATA[
arXiv:2311.01661v2 Announce Type: replace-cross 
Abstract: Community resilience is a complex and muti-faceted phenomenon that emerges from complex and nonlinear interactions among different socio-technical systems and their resilience properties. However, present studies on community resilience focus primarily on vulnerability assessment and utilize index-based approaches, with limited ability to capture heterogeneous features within community socio-technical systems and their nonlinear interactions in shaping robustness, redundancy, and resourcefulness components of resilience. To address this gap, this paper presents an integrated three-layer deep learning model for community resilience rating (called Resili-Net). Twelve measurable resilience features are specified and computed within community socio-technical systems (i.e., facilities, infrastructures, and society) related to three resilience components of robustness, redundancy, and resourcefulness. Using publicly accessible data from multiple metropolitan statistical areas in the United States, Resili-Net characterizes the resilience levels of spatial areas into five distinct levels. The interpretability of the model outcomes enables feature analysis for specifying the determinants of resilience in areas within each resilience level, allowing for the identification of specific resilience enhancement strategies. Changes in community resilience profiles under urban development patterns are further examined by changing the value of related socio-technical systems features. Accordingly, the outcomes provide novel perspectives for community resilience assessment by harnessing machine intelligence and heterogeneous urban big data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data</title>
<link>https://arxiv.org/abs/2402.12391</link>
<guid>https://arxiv.org/abs/2402.12391</guid>
<content:encoded><![CDATA[
arXiv:2402.12391v3 Announce Type: replace-cross 
Abstract: Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset to assess TAIS's effectiveness in gene identification, demonstrating our system's potential to significantly enhance the efficiency and scope of scientific exploration. Our findings represent a solid step towards automating scientific discovery through large language models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models</title>
<link>https://arxiv.org/abs/2403.01489</link>
<guid>https://arxiv.org/abs/2403.01489</guid>
<content:encoded><![CDATA[
arXiv:2403.01489v2 Announce Type: replace-cross 
Abstract: Text-to-image generative models have recently garnered significant attention due to their ability to generate images based on prompt descriptions. While these models have shown promising performance, concerns have been raised regarding the potential misuse of the generated fake images. In response to this, we have presented a simple yet effective training-free method to attribute fake images generated by text-to-image models to their source models. Given a test image to be attributed, we first inverse the textual prompt of the image, and then put the reconstructed prompt into different candidate models to regenerate candidate fake images. By calculating and ranking the similarity of the test image and the candidate images, we can determine the source of the image. This attribution allows model owners to be held accountable for any misuse of their models. Note that our approach does not limit the number of candidate text-to-image generative models. Comprehensive experiments reveal that (1) Our method can effectively attribute fake images to their source models, achieving comparable attribution performance with the state-of-the-art method; (2) Our method has high scalability ability, which is well adapted to real-world attribution scenarios. (3) The proposed method yields satisfactory robustness to common attacks, such as Gaussian blurring, JPEG compression, and Resizing. We also analyze the factors that influence the attribution performance, and explore the boost brought by the proposed method as a plug-in to improve the performance of existing SOTA. We hope our work can shed some light on the solutions to addressing the source of AI-generated images, as well as to prevent the misuse of text-to-image generative models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Molecular Generative Adversarial Network with Multi-Property Optimization</title>
<link>https://arxiv.org/abs/2404.00081</link>
<guid>https://arxiv.org/abs/2404.00081</guid>
<content:encoded><![CDATA[
arXiv:2404.00081v2 Announce Type: replace-cross 
Abstract: Deep generative models, such as generative adversarial networks (GANs), have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized reinforcement learning (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in GANs. However, due to the inherent instability in training GANs and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel GAN based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance to state-of-the-art models, and efficiently generates molecules with multi-property optimization. The source code will be released upon acceptance of the paper.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series</title>
<link>https://arxiv.org/abs/2404.11269</link>
<guid>https://arxiv.org/abs/2404.11269</guid>
<content:encoded><![CDATA[
arXiv:2404.11269v4 Announce Type: replace-cross 
Abstract: In time series anomaly detection (TSAD), the scarcity of labeled data poses a challenge to the development of accurate models. Unsupervised domain adaptation (UDA) offers a solution by leveraging labeled data from a related domain to detect anomalies in an unlabeled target domain. However, existing UDA methods assume consistent anomalous classes across domains. To address this limitation, we propose a novel Domain Adaptation Contrastive learning model for Anomaly Detection in multivariate time series (DACAD), combining UDA with contrastive learning. DACAD utilizes an anomaly injection mechanism that enhances generalization across unseen anomalous classes, improving adaptability and robustness. Additionally, our model employs supervised contrastive loss for the source domain and self-supervised contrastive triplet loss for the target domain, ensuring comprehensive feature representation learning and domain-invariant feature extraction. Finally, an effective Center-based Entropy Classifier (CEC) accurately learns normal boundaries in the source domain. Extensive evaluations on multiple real-world datasets and a synthetic dataset highlight DACAD's superior performance in transferring knowledge across domains and mitigating the challenge of limited labeled data in TSAD.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Over-Certainty Phenomenon in Modern Test-Time Adaptation Algorithms</title>
<link>https://arxiv.org/abs/2404.16168</link>
<guid>https://arxiv.org/abs/2404.16168</guid>
<content:encoded><![CDATA[
arXiv:2404.16168v4 Announce Type: replace-cross 
Abstract: When neural networks are confronted with unfamiliar data that deviate from their training set, this signifies a domain shift. While these networks output predictions on their inputs, they typically fail to account for their level of familiarity with these novel observations. Prevailing works navigate test-time adaptation with the goal of curtailing model entropy, yet they unintentionally produce models that struggle with sub-optimal calibration-a dilemma we term the over-certainty phenomenon. This over-certainty in predictions can be particularly dangerous in the setting of domain shifts, as it may lead to misplaced trust. In this paper, we propose a solution that not only maintains accuracy but also addresses calibration by mitigating the over-certainty phenomenon. To do this, we introduce a certainty regularizer that dynamically adjusts pseudo-label confidence by accounting for both backbone entropy and logit norm. Our method achieves state-of-the-art performance in terms of Expected Calibration Error and Negative Log Likelihood, all while maintaining parity in accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2406.01586</link>
<guid>https://arxiv.org/abs/2406.01586</guid>
<content:encoded><![CDATA[
arXiv:2406.01586v3 Announce Type: replace-cross 
Abstract: Diffusion models have been verified to be effective in generating complex distributions from natural images to motion trajectories. Recent diffusion-based methods show impressive performance in 3D robotic manipulation tasks, whereas they suffer from severe runtime inefficiency due to multiple denoising steps, especially with high-dimensional observations. To this end, we propose a real-time robotic manipulation model named ManiCM that imposes the consistency constraint to the diffusion process, so that the model can generate robot actions in only one-step inference. Specifically, we formulate a consistent diffusion process in the robot action space conditioned on the point cloud input, where the original action is required to be directly denoised from any point along the ODE trajectory. To model this process, we design a consistency distillation technique to predict the action sample directly instead of predicting the noise within the vision community for fast convergence in the low-dimensional action manifold. We evaluate ManiCM on 31 robotic manipulation tasks from Adroit and Metaworld, and the results demonstrate that our approach accelerates the state-of-the-art method by 10 times in average inference speed while maintaining competitive average success rate.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning</title>
<link>https://arxiv.org/abs/2406.06620</link>
<guid>https://arxiv.org/abs/2406.06620</guid>
<content:encoded><![CDATA[
arXiv:2406.06620v4 Announce Type: replace-cross 
Abstract: The recent rapid advancements in language models (LMs) have garnered attention in medical time series-text multimodal learning. However, existing contrastive learning-based and prompt-based LM approaches tend to be biased, often assigning a primary role to time series modality while treating text modality as secondary. We classify these approaches under a temporal-primary paradigm, which may overlook the unique and critical task-relevant information embedded in text modality like clinical reports, thus failing to fully leverage mutual benefits and complementarity of different modalities. To fill this gap, we propose a novel textual-temporal multimodal learning paradigm that enables either modality to serve as the primary while being enhanced by the other, thereby effectively capturing modality-specific information and fostering cross-modal interaction. In specific, we design MedualTime, a language model composed of dual adapters to implement temporal-primary and textual-primary modeling simultaneously. Within each adapter, lightweight adaptation tokens are injected into the top layers of LM to encourage high-level modality fusion. The shared LM pipeline by dual adapters not only achieves adapter alignment but also enables efficient fine-tuning, reducing computational resources. Empirically, MedualTime demonstrates superior performance on medical data, achieving notable improvements of 8% accuracy and 12% F1 in supervised settings. Furthermore, MedualTime's transferability is validated by few-shot label transfer experiments from coarse-grained to fine-grained medical data. https://github.com/start2020/MedualTime
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flash STU: Fast Spectral Transform Units</title>
<link>https://arxiv.org/abs/2409.10489</link>
<guid>https://arxiv.org/abs/2409.10489</guid>
<content:encoded><![CDATA[
arXiv:2409.10489v5 Announce Type: replace-cross 
Abstract: Recent advances in state-space model architectures have shown great promise for efficient sequence modeling, but challenges remain in balancing computational efficiency with model expressiveness. We propose the Flash STU architecture, a hybrid model that interleaves spectral state space model layers with sliding window attention, enabling scalability to billions of parameters for language modeling while maintaining a near-linear time complexity. We evaluate the Flash STU and its variants on diverse sequence prediction tasks, including linear dynamical systems, robotics control, and language modeling. We find that, given a fixed parameter budget, the Flash STU architecture consistently outperforms the Transformer and other leading state-space models such as S4 and Mamba-2.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BeSimulator: A Large Language Model Powered Text-based Behavior Simulator</title>
<link>https://arxiv.org/abs/2409.15865</link>
<guid>https://arxiv.org/abs/2409.15865</guid>
<content:encoded><![CDATA[
arXiv:2409.15865v2 Announce Type: replace-cross 
Abstract: Traditional robot simulators focus on physical process modeling and realistic rendering, often suffering from high computational costs, inefficiencies, and limited adaptability. To handle this issue, we concentrate on behavior simulation in robotics to analyze and validate the logic behind robot behaviors, aiming to achieve preliminary evaluation before deploying resource-intensive simulators and thus enhance simulation efficiency. In this paper, we propose BeSimulator, a modular and novel LLM-powered framework, as an attempt towards behavior simulation in the context of text-based environments. By constructing text-based virtual environments and performing semantic-level simulation, BeSimulator can generalize across scenarios and achieve long-horizon complex simulation. Inspired by human cognition paradigm, it employs a ``consider-decide-capture-transfer'' four-phase simulation process, termed Chain of Behavior Simulation (CBS), which excels at analyzing action feasibility and state transition. Additionally, BeSimulator incorporates code-driven reasoning to enable arithmetic operations and enhance reliability, and reflective feedback to refine simulation. Based on our manually constructed behavior-tree-based simulation benchmark, BTSIMBENCH, our experiments show a significant performance improvement in behavior simulation compared to baselines, ranging from 13.60% to 24.80%. Code and data are available at https://github.com/Dawn888888/BeSimulator.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking GNN Expressive Power from a Distributed Computational Model Perspective</title>
<link>https://arxiv.org/abs/2410.01308</link>
<guid>https://arxiv.org/abs/2410.01308</guid>
<content:encoded><![CDATA[
arXiv:2410.01308v4 Announce Type: replace-cross 
Abstract: The success of graph neural networks (GNNs) has motivated theoretical studies on their expressive power, often through alignments with the Weisfeiler-Lehman (WL) tests. However, such analyses typically focus on the ability of GNNs to distinguish between graph structures, rather than to compute or approximate specific function classes. The latter is more commonly studied in machine learning theory, including results such as the Turing completeness of recurrent networks and the universal approximation property of feedforward networks. We argue that using well-defined computational models, such as a modified CONGEST model with clearly specified preprocessing and postprocessing, offers a more sound framework for analyzing GNN expressiveness. Within this framework, we show that allowing unrestricted preprocessing or incorporating externally computed features, while claiming that these precomputations enhance the expressiveness, can sometimes lead to problems. We also show that the lower bound on a GNN's capacity (depth multiplied by width) to simulate one iteration of the WL test actually grows nearly linearly with graph size, indicating that the WL test is not locally computable and is misaligned with message-passing GNNs. Despite these negative results, we also present positive results that characterize the effects of virtual nodes and edges from a computational model perspective. Finally, we highlight several open problems regarding GNN expressiveness for further exploration.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Architecture Built for Federated Learning: Addressing Data Heterogeneity through Adaptive Normalization-Free Feature Recalibration</title>
<link>https://arxiv.org/abs/2410.02006</link>
<guid>https://arxiv.org/abs/2410.02006</guid>
<content:encoded><![CDATA[
arXiv:2410.02006v2 Announce Type: replace-cross 
Abstract: Federated learning is a decentralized collaborative training paradigm preserving stakeholders' data ownership while improving performance and generalization. However, statistical heterogeneity among client datasets degrades system performance. To address this issue, we propose Adaptive Normalization-free Feature Recalibration (ANFR), a model architecture-level approach that combines weight standardization and channel attention to combat heterogeneous data in FL. ANFR leverages weight standardization to avoid mismatched client statistics and inconsistent averaging, ensuring robustness under heterogeneity, and channel attention to produce learnable scaling factors for feature maps, suppressing inconsistencies across clients due to heterogeneity. We demonstrate that combining these techniques boosts model performance beyond their individual contributions, by improving class selectivity and channel attention weight distribution. ANFR works with any aggregation method, supports both global and personalized FL, and adds minimal overhead. Furthermore, when training with differential privacy, ANFR achieves an appealing balance between privacy and utility, enabling strong privacy guarantees without sacrificing performance. By integrating weight standardization and channel attention in the backbone model, ANFR offers a novel and versatile approach to the challenge of statistical heterogeneity. Extensive experiments show ANFR consistently outperforms established baselines across various aggregation methods, datasets, and heterogeneity conditions. Code is provided at https://github.com/siomvas/ANFR.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping</title>
<link>https://arxiv.org/abs/2410.05341</link>
<guid>https://arxiv.org/abs/2410.05341</guid>
<content:encoded><![CDATA[
arXiv:2410.05341v3 Announce Type: replace-cross 
Abstract: Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: NeuroBOLT, i.e., Neuro-to-BOLD Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs unseen resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy with the potential to generalize across varying conditions and sites, which significantly advances the integration of these two modalities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ETF: An Entity Tracing Framework for Hallucination Detection in Code Summaries</title>
<link>https://arxiv.org/abs/2410.14748</link>
<guid>https://arxiv.org/abs/2410.14748</guid>
<content:encoded><![CDATA[
arXiv:2410.14748v4 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their ability to understand both natural language and code, driving their use in tasks like natural language-to-code (NL2Code) and code summarisation. However, LLMs are prone to hallucination, outputs that stray from intended meanings. Detecting hallucinations in code summarisation is especially difficult due to the complex interplay between programming and natural languages. We introduce a first-of-its-kind dataset, CodeSumEval, with ~10K samples, curated specifically for hallucination detection in code summarisation. We further propose a novel Entity Tracing Framework (ETF) that a) utilises static program analysis to identify code entities from the program and b) uses LLMs to map and verify these entities and their intents within generated code summaries. Our experimental analysis demonstrates the framework's effectiveness, leading to a 73% F1 score. The proposed approach provides a method for detecting hallucinations by tracing entities from the summary to the code, allowing us to evaluate summary accuracy and localise the error within the summary.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FACEGroup: Feasible and Actionable Counterfactual Explanations for Group Fairness</title>
<link>https://arxiv.org/abs/2410.22591</link>
<guid>https://arxiv.org/abs/2410.22591</guid>
<content:encoded><![CDATA[
arXiv:2410.22591v3 Announce Type: replace-cross 
Abstract: Counterfactual explanations assess unfairness by revealing how inputs must change to achieve a desired outcome. This paper introduces the first graph-based framework for generating group counterfactual explanations to audit group fairness, a key aspect of trustworthy machine learning. Our framework, FACEGroup (Feasible and Actionable Counterfactual Explanations for Group Fairness), models real-world feasibility constraints, identifies subgroups with similar counterfactuals, and captures key trade-offs in counterfactual generation, distinguishing it from existing methods. To evaluate fairness, we introduce novel metrics for both group and subgroup level analysis that explicitly account for these trade-offs. Experiments on benchmark datasets show that FACEGroup effectively generates feasible group counterfactuals while accounting for trade-offs, and that our metrics capture and quantify fairness disparities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Coded Distributed Convolution Computing for Enhanced Straggler Resilience and Numerical Stability in Distributed CNNs</title>
<link>https://arxiv.org/abs/2411.01579</link>
<guid>https://arxiv.org/abs/2411.01579</guid>
<content:encoded><![CDATA[
arXiv:2411.01579v3 Announce Type: replace-cross 
Abstract: Deploying Convolutional Neural Networks (CNNs) on resource-constrained devices necessitates efficient management of computational resources, often via distributed environments susceptible to latency from straggler nodes. This paper introduces the Flexible Coded Distributed Convolution Computing (FCDCC) framework to enhance straggler resilience and numerical stability in distributed CNNs. We extend Coded Distributed Computing (CDC) with Circulant and Rotation Matrix Embedding (CRME) which was originally proposed for matrix multiplication to high-dimensional tensor convolution. For the proposed scheme, referred to as the Numerically Stable Coded Tensor Convolution (NSCTC) scheme, we also propose two new coded partitioning schemes: Adaptive-Padding Coded Partitioning (APCP) for the input tensor and Kernel-Channel Coded Partitioning (KCCP) for the filter tensor. These strategies enable linear decomposition of tensor convolutions and encoding them into CDC subtasks, combining model parallelism with coded redundancy for robust and efficient execution. Theoretical analysis identifies an optimal trade-off between communication and storage costs. Empirical results validate the framework's effectiveness in computational efficiency, straggler resilience, and scalability across various CNN architectures.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals</title>
<link>https://arxiv.org/abs/2411.07152</link>
<guid>https://arxiv.org/abs/2411.07152</guid>
<content:encoded><![CDATA[
arXiv:2411.07152v2 Announce Type: replace-cross 
Abstract: Task-Oriented Dialogue (TOD) systems assist users in completing tasks through natural language interactions, often relying on a single-layered workflow structure for slot-filling in public tasks, such as hotel bookings. However, in enterprise environments, which involve rich domain-specific knowledge, TOD systems face challenges due to task complexity and the lack of standardized documentation. In this work, we introduce HierTOD, an enterprise TOD system driven by hierarchical goals that can support composite workflows. By focusing on goal-driven interactions, our system serves a more proactive role, facilitating mixed-initiative dialogue and improving task completion. Equipped with components for natural language understanding, composite goal retriever, dialogue management, and response generation, backed by a well-organized data service with domain knowledge base and retrieval engine, HierTOD delivers efficient task assistance as judged by human evaluators. Furthermore, our system implementation unifies two TOD paradigms: slot-filling for information collection and step-by-step guidance for task execution. Our user study demonstrates the effectiveness and helpfulness of HierTOD in performing both paradigms.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons from Studying Two-Hop Latent Reasoning</title>
<link>https://arxiv.org/abs/2411.16353</link>
<guid>https://arxiv.org/abs/2411.16353</guid>
<content:encoded><![CDATA[
arXiv:2411.16353v3 Announce Type: replace-cross 
Abstract: Large language models can use chain-of-thought (CoT) to externalize reasoning, potentially enabling oversight of capable LLM agents. Prior work has shown that models struggle at two-hop question-answering without CoT. This capability is so basic that if it was a fundamental limitation, it would imply that many complex agentic tasks would similarly require CoT. We investigate LLM latent reasoning capabilities using two-hop question answering as a case study. Previous work on the gap between latent and externalized two-hop reasoning produced mixed evidence with inconclusive results. In this paper, we introduce a controlled setting for investigating two-hop reasoning in LLMs, where a positive result provides definitive evidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B and GPT-4o) on synthetic facts and test two-hop reasoning over these facts. By using synthetic facts, we rule out memorization and reasoning shortcuts as explanations for two-hop performance. We observe a nuanced picture: Models fail to compose two synthetic facts, but can succeed when one fact is synthetic and the other is natural. These results demonstrate that LLMs are undeniably capable of latent two-hop reasoning, although it remains unclear how this ability scales with model size. Finally, we highlight a lesson for researchers studying LLM reasoning: when drawing conclusions about LLM latent reasoning, one must be careful to avoid both spurious successes (that stem from memorization and reasoning shortcuts) and spurious failures (that may stem from artificial experimental setups, divorced from training setups of frontier LLMs).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives</title>
<link>https://arxiv.org/abs/2411.19787</link>
<guid>https://arxiv.org/abs/2411.19787</guid>
<content:encoded><![CDATA[
arXiv:2411.19787v2 Announce Type: replace-cross 
Abstract: Grounding the instruction in the environment is a key step in solving language-guided goal-reaching reinforcement learning problems. In automated reinforcement learning, a key concern is to enhance the model's ability to generalize across various tasks and environments. In goal-reaching scenarios, the agent must comprehend the different parts of the instructions within the environmental context in order to complete the overall task successfully. In this work, we propose CAREL (Cross-modal Auxiliary REinforcement Learning) as a new framework to solve this problem using auxiliary loss functions inspired by video-text retrieval literature and a novel method called instruction tracking, which automatically keeps track of progress in an environment. The results of our experiments suggest superior sample efficiency and systematic generalization for this framework in multi-modal reinforcement learning problems. Our code base is available here.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Federated Learning via Homomorphic Adversarial Networks</title>
<link>https://arxiv.org/abs/2412.01650</link>
<guid>https://arxiv.org/abs/2412.01650</guid>
<content:encoded><![CDATA[
arXiv:2412.01650v3 Announce Type: replace-cross 
Abstract: Privacy-preserving federated learning (PPFL) aims to train a global model for multiple clients while maintaining their data privacy. However, current PPFL protocols exhibit one or more of the following insufficiencies: considerable degradation in accuracy, the requirement for sharing keys, and cooperation during the key generation or decryption processes. As a mitigation, we develop the first protocol that utilizes neural networks to implement PPFL, as well as incorporating an Aggregatable Hybrid Encryption scheme tailored to the needs of PPFL. We name these networks as Homomorphic Adversarial Networks (HANs) which demonstrate that neural networks are capable of performing tasks similar to multi-key homomorphic encryption (MK-HE) while solving the problems of key distribution and collaborative decryption. Our experiments show that HANs are robust against privacy attacks. Compared with non-private federated learning, experiments conducted on multiple datasets demonstrate that HANs exhibit a negligible accuracy loss (at most 1.35%). Compared to traditional MK-HE schemes, HANs increase encryption aggregation speed by 6,075 times while incurring a 29.2 times increase in communication overhead.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Controlled Langevin Diffusions</title>
<link>https://arxiv.org/abs/2412.07081</link>
<guid>https://arxiv.org/abs/2412.07081</guid>
<content:encoded><![CDATA[
arXiv:2412.07081v2 Announce Type: replace-cross 
Abstract: An effective approach for sampling from unnormalized densities is based on the idea of gradually transporting samples from an easy prior to the complicated target distribution. Two popular methods are (1) Sequential Monte Carlo (SMC), where the transport is performed through successive annealed densities via prescribed Markov chains and resampling steps, and (2) recently developed diffusion-based sampling methods, where a learned dynamical transport is used. Despite the common goal, both approaches have different, often complementary, advantages and drawbacks. The resampling steps in SMC allow focusing on promising regions of the space, often leading to robust performance. While the algorithm enjoys asymptotic guarantees, the lack of flexible, learnable transitions can lead to slow convergence. On the other hand, diffusion-based samplers are learned and can potentially better adapt themselves to the target at hand, yet often suffer from training instabilities. In this work, we present a principled framework for combining SMC with diffusion-based samplers by viewing both methods in continuous time and considering measures on path space. This culminates in the new Sequential Controlled Langevin Diffusion (SCLD) sampling method, which is able to utilize the benefits of both methods and reaches improved performance on multiple benchmark problems, in many cases using only 10% of the training budget of previous diffusion-based samplers.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Port-Hamiltonian Differential Algebraic Equations for Compositional Learning of Electrical Networks</title>
<link>https://arxiv.org/abs/2412.11215</link>
<guid>https://arxiv.org/abs/2412.11215</guid>
<content:encoded><![CDATA[
arXiv:2412.11215v3 Announce Type: replace-cross 
Abstract: We develop compositional learning algorithms for coupled dynamical systems, with a particular focus on electrical networks. While deep learning has proven effective at modeling complex relationships from data, compositional couplings between system components typically introduce algebraic constraints on state variables, posing challenges to many existing data-driven approaches to modeling dynamical systems. Towards developing deep learning models for constrained dynamical systems, we introduce neural port-Hamiltonian differential algebraic equations (N-PHDAEs), which use neural networks to parameterize unknown terms in both the differential and algebraic components of a port-Hamiltonian DAE. To train these models, we propose an algorithm that uses automatic differentiation to perform index reduction, automatically transforming the neural DAE into an equivalent system of neural ordinary differential equations (N-ODEs), for which established model inference and backpropagation methods exist. Experiments simulating the dynamics of nonlinear circuits exemplify the benefits of our approach: the proposed N-PHDAE model achieves an order of magnitude improvement in prediction accuracy and constraint satisfaction when compared to a baseline N-ODE over long prediction time horizons. We also validate the compositional capabilities of our approach through experiments on a simulated DC microgrid: we train individual N-PHDAE models for separate grid components, before coupling them to accurately predict the behavior of larger-scale networks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing the impact of synthetic native samples and multi-tasking strategies in Hindi-English code-mixed humour and sarcasm detection</title>
<link>https://arxiv.org/abs/2412.12761</link>
<guid>https://arxiv.org/abs/2412.12761</guid>
<content:encoded><![CDATA[
arXiv:2412.12761v2 Announce Type: replace-cross 
Abstract: In this paper, we reported our experiments with various strategies to improve code-mixed humour and sarcasm detection. Particularly, we tried three approaches: (i) native sample mixing, (ii) multi-task learning (MTL), and (iii) prompting and instruction finetuning very large multilingual language models (VMLMs). In native sample mixing, we added monolingual task samples to code-mixed training sets. In MTL learning, we relied on native and code-mixed samples of a semantically related task (hate detection in our case). Finally, in our third approach, we evaluated the efficacy of VMLMs via few-shot context prompting and instruction finetuning. Some interesting findings we got are (i) adding native samples improved humor (raising the F1-score up to 6.76%) and sarcasm (raising the F1-score up to 8.64%) detection, (ii) training MLMs in an MTL framework boosted performance for both humour (raising the F1-score up to 10.67%) and sarcasm (increment up to 12.35% in F1-score) detection, and (iii) prompting and instruction finetuning VMLMs couldn't outperform the other approaches. Finally, our ablation studies and error analysis discovered the cases where our model is yet to improve. We provided our code for reproducibility.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction</title>
<link>https://arxiv.org/abs/2412.14209</link>
<guid>https://arxiv.org/abs/2412.14209</guid>
<content:encoded><![CDATA[
arXiv:2412.14209v3 Announce Type: replace-cross 
Abstract: Explainable Artificial Intelligence seeks to make the reasoning processes of AI models transparent and interpretable, particularly in complex decision making environments. In the construction industry, where AI based decision support systems are increasingly adopted, limited attention has been paid to the integration of supporting evidence that underpins the reliability and accountability of AI generated outputs. The absence of such evidence undermines the validity of explanations and the trustworthiness of system recommendations. This paper addresses this gap by introducing a theoretical, evidence based means end framework developed through a narrative review. The framework offers an epistemic foundation for designing XAI enabled DSS that generate meaningful explanations tailored to users knowledge needs and decision contexts. It focuses on evaluating the strength, relevance, and utility of different types of evidence supporting AI generated explanations. While developed with construction professionals as primary end users, the framework is also applicable to developers, regulators, and project managers with varying epistemic goals.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DispFormer: A Pretrained Transformer Incorporating Physical Constraints for Dispersion Curve Inversion</title>
<link>https://arxiv.org/abs/2501.04366</link>
<guid>https://arxiv.org/abs/2501.04366</guid>
<content:encoded><![CDATA[
arXiv:2501.04366v2 Announce Type: replace-cross 
Abstract: Surface wave dispersion curve inversion is crucial for estimating subsurface shear-wave velocity (vs), yet traditional methods often face challenges related to computational cost, non-uniqueness, and sensitivity to initial models. While deep learning approaches show promise, many require large labeled datasets and struggle with real-world datasets, which often exhibit varying period ranges, missing values, and low signal-to-noise ratios. To address these limitations, this study introduces DispFormer, a transformer-based neural network for $v_s$ profile inversion from Rayleigh-wave phase and group dispersion curves. DispFormer processes dispersion data independently at each period, allowing it to handle varying lengths without requiring network modifications or strict alignment between training and testing datasets. A depth-aware training strategy is also introduced, incorporating physical constraints derived from the depth sensitivity of dispersion data. DispFormer is pre-trained on a global synthetic dataset and evaluated on two regional synthetic datasets using zero-shot and few-shot strategies. Results show that even without labeled data, the zero-shot DispFormer generates inversion profiles that outperform the interpolated reference model used as the pretraining target, providing a deployable initial model generator to assist traditional workflows. When partial labeled data available, the few-shot trained DispFormer surpasses traditional global search methods. Real-world tests further confirm that DispFormer generalizes well to dispersion data with varying lengths and achieves lower data residuals than reference models. These findings underscore the potential of DispFormer as a foundation model for dispersion curve inversion and demonstrate the advantages of integrating physics-informed deep learning into geophysical applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking</title>
<link>https://arxiv.org/abs/2501.09751</link>
<guid>https://arxiv.org/abs/2501.09751</guid>
<content:encoded><![CDATA[
arXiv:2501.09751v3 Announce Type: replace-cross 
Abstract: Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, novelty, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, unoriginal, and repetitive outputs. To address these issues, we propose OmniThink, a slow-thinking machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they slowly deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles. Code is available at https://github.com/zjunlp/OmniThink.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude</title>
<link>https://arxiv.org/abs/2501.10484</link>
<guid>https://arxiv.org/abs/2501.10484</guid>
<content:encoded><![CDATA[
arXiv:2501.10484v3 Announce Type: replace-cross 
Abstract: Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study systematically evaluates how nine popular LLMs (both open-source and closed-source) respond to ethical dilemmas involving protected attributes. Across 50,400 trials spanning single and intersectional attribute combinations in four dilemma scenarios (protective vs. harmful), we assess models' ethical preferences, sensitivity, stability, and clustering patterns. Results reveal significant biases in protected attributes in all models, with differing preferences depending on model type and dilemma context. Notably, open-source LLMs show stronger preferences for marginalized groups and greater sensitivity in harmful scenarios, while closed-source models are more selective in protective situations and tend to favor mainstream groups. We also find that ethical behavior varies across dilemma types: LLMs maintain consistent patterns in protective scenarios but respond with more diverse and cognitively demanding decisions in harmful ones. Furthermore, models display more pronounced ethical tendencies under intersectional conditions than in single-attribute settings, suggesting that complex inputs reveal deeper biases. These findings highlight the need for multi-dimensional, context-aware evaluation of LLMs' ethical behavior and offer a systematic evaluation and approach to understanding and addressing fairness in LLM decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion-enhanced Cardiac Anatomy Segmentation via an Insertable Temporal Attention Module</title>
<link>https://arxiv.org/abs/2501.14929</link>
<guid>https://arxiv.org/abs/2501.14929</guid>
<content:encoded><![CDATA[
arXiv:2501.14929v2 Announce Type: replace-cross 
Abstract: Cardiac anatomy segmentation is useful for clinical assessment of cardiac morphology to inform diagnosis and intervention. Deep learning (DL), especially with motion information, has improved segmentation accuracy. However, existing techniques for motion enhancement are not yet optimal, and they have high computational costs due to increased dimensionality or reduced robustness due to suboptimal approaches that use non-DL motion registration, non-attention models, or single-headed attention. They further have limited adaptability and are inconvenient for incorporation into existing networks where motion awareness is desired. Here, we propose a novel, computationally efficient Temporal Attention Module (TAM) that offers robust motion enhancement, modeled as a small, multi-headed, cross-temporal attention module. TAM's uniqueness is that it is a lightweight, plug-and-play module that can be inserted into a broad range of segmentation networks (CNN-based, Transformer-based, or hybrid) for motion enhancement without requiring substantial changes in the network's backbone. This feature enables high adaptability and ease of integration for enhancing both existing and future networks. Extensive experiments on multiple 2D and 3D cardiac ultrasound and MRI datasets confirm that TAM consistently improves segmentation across a range of networks while maintaining computational efficiency and improving on currently reported performance. The evidence demonstrates that it is a robust, generalizable solution for motion-awareness enhancement that is scalable (such as from 2D to 3D).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Separate Motion from Appearance: Customizing Motion via Customizing Text-to-Video Diffusion Models</title>
<link>https://arxiv.org/abs/2501.16714</link>
<guid>https://arxiv.org/abs/2501.16714</guid>
<content:encoded><![CDATA[
arXiv:2501.16714v2 Announce Type: replace-cross 
Abstract: Motion customization aims to adapt the diffusion model (DM) to generate videos with the motion specified by a set of video clips with the same motion concept. To realize this goal, the adaptation of DM should be possible to model the specified motion concept, without compromising the ability to generate diverse appearances. Thus, the key to solving this problem lies in how to separate the motion concept from the appearance in the adaptation process of DM. Typical previous works explore different ways to represent and insert a motion concept into large-scale pretrained text-to-video diffusion models, e.g., learning a motion LoRA, using latent noise residuals, etc. While those methods can encode the motion concept, they also inevitably encode the appearance in the reference videos, resulting in weakened appearance generation capability. In this paper, we follow the typical way to learn a motion LoRA to encode the motion concept, but propose two novel strategies to enhance motion-appearance separation, including temporal attention purification (TAP) and appearance highway (AH). Specifically, we assume that in the temporal attention module, the pretrained Value embeddings are sufficient to serve as basic components needed by producing a new motion. Thus, in TAP, we choose only to reshape the temporal attention with motion LoRAs so that Value embeddings can be reorganized to produce a new motion. Further, in AH, we alter the starting point of each skip connection in U-Net from the output of each temporal attention module to the output of each spatial attention module. Extensive experiments demonstrate that compared to previous works, our method can generate videos with appearance more aligned with the text descriptions and motion more consistent with the reference videos.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Steady-State Behavior in Complex Networks with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2502.01693</link>
<guid>https://arxiv.org/abs/2502.01693</guid>
<content:encoded><![CDATA[
arXiv:2502.01693v3 Announce Type: replace-cross 
Abstract: In complex systems, information propagation can be defined as diffused or delocalized, weakly localized, and strongly localized. This study investigates the application of graph neural network models to learn the behavior of a linear dynamical system on networks. A graph convolution and attention-based neural network framework has been developed to identify the steady-state behavior of the linear dynamical system. We reveal that our trained model distinguishes the different states with high accuracy. Furthermore, we have evaluated model performance with real-world data. In addition, to understand the explainability of our model, we provide an analytical derivation for the forward and backward propagation of our framework.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: LLMs Can be Good Tutors in English Education</title>
<link>https://arxiv.org/abs/2502.05467</link>
<guid>https://arxiv.org/abs/2502.05467</guid>
<content:encoded><![CDATA[
arXiv:2502.05467v2 Announce Type: replace-cross 
Abstract: While recent efforts have begun integrating large language models (LLMs) into English education, they often rely on traditional approaches to learning tasks without fully embracing educational methodologies, thus lacking adaptability to language learning. To address this gap, we argue that LLMs have the potential to serve as effective tutors in English Education. Specifically, LLMs can play three critical roles: (1) as data enhancers, improving the creation of learning materials or serving as student simulations; (2) as task predictors, serving as learner assessment or optimizing learning pathway; and (3) as agents, enabling personalized and inclusive education. We encourage interdisciplinary research to explore these roles, fostering innovation while addressing challenges and risks, ultimately advancing English Education through the thoughtful integration of LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Fourier Networks</title>
<link>https://arxiv.org/abs/2502.06018</link>
<guid>https://arxiv.org/abs/2502.06018</guid>
<content:encoded><![CDATA[
arXiv:2502.06018v2 Announce Type: replace-cross 
Abstract: Although Kolmogorov-Arnold based interpretable networks (KAN) have strong theoretical expressiveness, they face significant parameter explosion and high-frequency feature capture challenges in high-dimensional tasks. To address this issue, we propose the Kolmogorov-Arnold-Fourier Network (KAF), which effectively integrates trainable Random Fourier Features (RFF) and a novel hybrid GELU-Fourier activation mechanism to balance parameter efficiency and spectral representation capabilities. Our key technical contributions include: (1) merging KAN's dual-matrix structure through matrix association properties to substantially reduce parameters; (2) introducing learnable RFF initialization strategies to eliminate spectral distortion in high-dimensional approximation tasks; (3) implementing an adaptive hybrid activation function that progressively enhances frequency representation during the training process. Comprehensive experiments demonstrate the superiority of our KAF across various domains including vision, NLP, audio processing, and differential equation-solving tasks, effectively combining theoretical interpretability with practical utility and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHIRLA: Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis</title>
<link>https://arxiv.org/abs/2502.06681</link>
<guid>https://arxiv.org/abs/2502.06681</guid>
<content:encoded><![CDATA[
arXiv:2502.06681v2 Announce Type: replace-cross 
Abstract: Person re-identification (Re-ID) is a key challenge in computer vision, requiring the matching of individuals across cameras, locations, and time. While most research focuses on short-term scenarios with minimal appearance changes, real-world applications demand robust systems that handle long-term variations caused by clothing and physical changes. We present CHIRLA, Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis, a novel dataset designed for video-based long-term person Re-ID. CHIRLA was recorded over seven months in four connected indoor environments using seven strategically placed cameras, capturing realistic movements with substantial clothing and appearance variability. The dataset includes 22 individuals, more than five hours of video, and about 1M bounding boxes with identity annotations obtained through semi-automatic labeling. We also define benchmark protocols for person tracking and Re-ID, covering diverse and challenging scenarios such as occlusion, reappearance, and multi-camera conditions. By introducing this comprehensive benchmark, we aim to facilitate the development and evaluation of Re-ID algorithms that can reliably perform in challenging, long-term real-world scenarios. The benchmark code is publicly available at: https://github.com/bdager/CHIRLA.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models</title>
<link>https://arxiv.org/abs/2502.15836</link>
<guid>https://arxiv.org/abs/2502.15836</guid>
<content:encoded><![CDATA[
arXiv:2502.15836v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are trained using massive datasets, which often contain undesirable content such as harmful texts, personal information, and copyrighted material. To address this, machine unlearning aims to remove information from trained models. Recent work has shown that soft token attacks (STA) can successfully extract unlearned information from LLMs, but in this work we show that STAs can be an inadequate tool for auditing unlearning. Using common benchmarks such as Who Is Harry Potter? and TOFU, we demonstrate that in a strong auditor setting such attacks can elicit any information from the LLM, regardless of the deployed unlearning algorithm or whether the queried content was originally present in the training corpus. We further show that STA with just a few soft tokens (1-10) can elicit random strings over 400 characters long, indicating that STAs must be used carefully to effectively audit unlearning. Example code can be found at: https://github.com/IntelLabs/LLMart/tree/main/examples/unlearning
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support</title>
<link>https://arxiv.org/abs/2502.18658</link>
<guid>https://arxiv.org/abs/2502.18658</guid>
<content:encoded><![CDATA[
arXiv:2502.18658v4 Announce Type: replace-cross 
Abstract: AI programming tools enable powerful code generation, and recent prototypes attempt to reduce user effort with proactive AI agents, but their impact on programming workflows remains unexplored. We introduce and evaluate Codellaborator, a design probe LLM agent that initiates programming assistance based on editor activities and task context. We explored three interface variants to assess trade-offs between increasingly salient AI support: prompt-only, proactive agent, and proactive agent with presence and context (Codellaborator). In a within-subject study (N=18), we find that proactive agents increase efficiency compared to prompt-only paradigm, but also incur workflow disruptions. However, presence indicators and interaction context support alleviated disruptions and improved users' awareness of AI processes. We underscore trade-offs of Codellaborator on user control, ownership, and code understanding, emphasizing the need to adapt proactivity to programming processes. Our research contributes to the design exploration and evaluation of proactive AI systems, presenting design implications on AI-integrated programming workflow.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning</title>
<link>https://arxiv.org/abs/2502.18978</link>
<guid>https://arxiv.org/abs/2502.18978</guid>
<content:encoded><![CDATA[
arXiv:2502.18978v5 Announce Type: replace-cross 
Abstract: The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive Neural Predicate Modulation</title>
<link>https://arxiv.org/abs/2503.08994</link>
<guid>https://arxiv.org/abs/2503.08994</guid>
<content:encoded><![CDATA[
arXiv:2503.08994v2 Announce Type: replace-cross 
Abstract: Research on learned cardinality estimation has made significant progress in recent years. However, existing methods still face distinct challenges that hinder their practical deployment in production environments. We define these challenges as the ``Trilemma of Cardinality Estimation'', where learned cardinality estimation methods struggle to balance generality, accuracy, and updatability. To address these challenges, we introduce DistJoin, a join cardinality estimator based on efficient distribution prediction using multi-autoregressive models. Our contributions are threefold: (1) We propose a method to estimate join cardinality by leveraging the probability distributions of individual tables in a decoupled manner. (2) To meet the requirements of efficiency for DistJoin, we develop Adaptive Neural Predicate Modulation (ANPM), a high-throughput distribution estimation model. (3) We demonstrate that an existing similar approach suffers from variance accumulation issues by formal variance analysis. To mitigate this problem, DistJoin employs a selectivity-based approach to infer join cardinality, effectively reducing variance. In summary, DistJoin not only represents the first data-driven method to support both equi and non-equi joins simultaneously but also demonstrates superior accuracy while enabling fast and flexible updates. The experimental results demonstrate that DistJoin achieves the highest accuracy, robustness to data updates, generality, and comparable update and inference speed relative to existing methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIPER: Visual Perception and Explainable Reasoning for Sequential Decision-Making</title>
<link>https://arxiv.org/abs/2503.15108</link>
<guid>https://arxiv.org/abs/2503.15108</guid>
<content:encoded><![CDATA[
arXiv:2503.15108v2 Announce Type: replace-cross 
Abstract: While Large Language Models (LLMs) excel at reasoning on text and Vision-Language Models (VLMs) are highly effective for visual perception, applying those models for visual instruction-based planning remains a widely open problem. In this paper, we introduce VIPER, a novel framework for multimodal instruction-based planning that integrates VLM-based perception with LLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM generates textual descriptions of image observations, which are then processed by an LLM policy to predict actions based on the task goal. We fine-tune the reasoning module using behavioral cloning and reinforcement learning, improving our agent's decision-making capabilities. Experiments on the ALFWorld benchmark show that VIPER significantly outperforms state-of-the-art visual instruction-based planners while narrowing the gap with purely text-based oracles. By leveraging text as an intermediate representation, VIPER also enhances explainability, paving the way for a fine-grained analysis of perception and reasoning components.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond SHAP and Anchors: A large-scale experiment on how developers struggle to design meaningful end-user explanations</title>
<link>https://arxiv.org/abs/2503.15512</link>
<guid>https://arxiv.org/abs/2503.15512</guid>
<content:encoded><![CDATA[
arXiv:2503.15512v2 Announce Type: replace-cross 
Abstract: Modern machine learning produces models that are impossible for users or developers to fully understand--raising concerns about trust, oversight, safety, and human dignity when they are integrated into software products. Transparency and explainability methods aim to provide some help in understanding models, but it remains challenging for developers to design explanations that are understandable to target users and effective for their purpose. Emerging guidelines and regulations set goals but may not provide effective actionable guidance to developers. In a large-scale experiment with 124 participants, we explored how developers approach providing end-user explanations, including what challenges they face, and to what extent specific policies can guide their actions. We investigated whether and how specific forms of policy guidance help developers design explanations and provide evidence for policy compliance for an ML-powered screening tool for diabetic retinopathy. Participants across the board struggled to produce quality explanations and comply with the provided policies. Contrary to our expectations, we found that the nature and specificity of policy guidance had little effect. We posit that participant noncompliance is in part due to a failure to imagine and anticipate the needs of non-technical stakeholders. Drawing on cognitive process theory and the sociological imagination to contextualize participants' failure, we recommend educational interventions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Byzantine-Robust Federated Learning Using Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2503.20884</link>
<guid>https://arxiv.org/abs/2503.20884</guid>
<content:encoded><![CDATA[
arXiv:2503.20884v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) enables collaborative model training across distributed clients without sharing raw data, but its robustness is threatened by Byzantine behaviors such as data and model poisoning. Existing defenses face fundamental limitations: robust aggregation rules incur error lower bounds that grow with client heterogeneity, while detection-based methods often rely on heuristics (e.g., a fixed number of malicious clients) or require trusted external datasets for validation. We present a defense framework that addresses these challenges by leveraging a conditional generative adversarial network (cGAN) at the server to synthesize representative data for validating client updates. This approach eliminates reliance on external datasets, adapts to diverse attack strategies, and integrates seamlessly into standard FL workflows. Extensive experiments on benchmark datasets demonstrate that our framework accurately distinguishes malicious from benign clients while maintaining overall model accuracy. Beyond Byzantine robustness, we also examine the representativeness of synthesized data, computational costs of cGAN training, and the transparency and scalability of our approach.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models</title>
<link>https://arxiv.org/abs/2504.03624</link>
<guid>https://arxiv.org/abs/2504.03624</guid>
<content:encoded><![CDATA[
arXiv:2504.03624v4 Announce Type: replace-cross 
Abstract: As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\times$ faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization</title>
<link>https://arxiv.org/abs/2504.18026</link>
<guid>https://arxiv.org/abs/2504.18026</guid>
<content:encoded><![CDATA[
arXiv:2504.18026v4 Announce Type: replace-cross 
Abstract: Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI systems by constraining their decisions on a set of human-understandable concepts. However, CBMs typically assume that datasets contain accurate concept labels-an assumption often violated in practice, which we show can significantly degrade performance (by 25% in some cases). To address this, we introduce the Concept Preference Optimization (CPO) objective, a new loss function based on Direct Preference Optimization, which effectively mitigates the negative impact of concept mislabeling on CBM performance. We provide an analysis of key properties of the CPO objective, showing it directly optimizes for the concept's posterior distribution, and contrast it against Binary Cross Entropy (BCE), demonstrating that CPO is inherently less sensitive to concept noise. We empirically confirm our analysis by finding that CPO consistently outperforms BCE on three real-world datasets, both with and without added label noise. We make our code available on Github.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Flow Matching for Continual Robot Learning</title>
<link>https://arxiv.org/abs/2504.18471</link>
<guid>https://arxiv.org/abs/2504.18471</guid>
<content:encoded><![CDATA[
arXiv:2504.18471v2 Announce Type: replace-cross 
Abstract: Continual learning in robotics seeks systems that can constantly adapt to changing environments and tasks, mirroring human adaptability. A key challenge is refining dynamics models, essential for planning and control, while addressing issues such as safe adaptation, catastrophic forgetting, outlier management, data efficiency, and balancing exploration with exploitation -- all within task and onboard resource constraints. Towards this goal, we introduce a generative framework leveraging flow matching for online robot dynamics model alignment. Rather than executing actions based on a misaligned model, our approach refines planned actions to better match with those the robot would take if its model was well aligned. We find that by transforming the actions themselves rather than exploring with a misaligned model -- as is traditionally done -- the robot collects informative data more efficiently, thereby accelerating learning. Moreover, we validate that the method can handle an evolving and possibly imperfect model while reducing, if desired, the dependency on replay buffers or legacy model snapshots. We validate our approach using two platforms: an unmanned ground vehicle and a quadrotor. The results highlight the method's adaptability and efficiency, with a record 34.2\% higher task success rate, demonstrating its potential towards enabling continual robot learning. Code: https://github.com/AlejandroMllo/action_flow_matching.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test It Before You Trust It: Applying Software Testing for Trustworthy In-context Learning</title>
<link>https://arxiv.org/abs/2504.18827</link>
<guid>https://arxiv.org/abs/2504.18827</guid>
<content:encoded><![CDATA[
arXiv:2504.18827v3 Announce Type: replace-cross 
Abstract: In-context learning (ICL) has emerged as a powerful capability of large language models (LLMs), enabling them to perform new tasks based on a few provided examples without explicit fine-tuning. Despite their impressive adaptability, these models remain vulnerable to subtle adversarial perturbations and exhibit unpredictable behavior when faced with linguistic variations. Inspired by software testing principles, we introduce a software testing-inspired framework, called MMT4NL, for evaluating the trustworthiness of in-context learning by utilizing adversarial perturbations and software testing techniques. It includes diverse evaluation aspects of linguistic capabilities for testing the ICL capabilities of LLMs. MMT4NL is built around the idea of crafting metamorphic adversarial examples from a test set in order to quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is to treat any LLM as software and validate its functionalities just like testing the software. Finally, we demonstrate applications of MMT4NL on the sentiment analysis and question-answering tasks. Our experiments could reveal various linguistic bugs in state-of-the-art LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting</title>
<link>https://arxiv.org/abs/2504.19021</link>
<guid>https://arxiv.org/abs/2504.19021</guid>
<content:encoded><![CDATA[
arXiv:2504.19021v2 Announce Type: replace-cross 
Abstract: Efficient text classification is essential for handling the increasing volume of academic publications. This study explores the use of pre-trained language models (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on the Web of Science (WoS-46985) dataset for scientific text classification. To enhance performance, we augment the dataset by executing seven targeted queries in the WoS database, retrieving 1,000 articles per category aligned with WoS-46985's main classes. PLMs predict labels for this unlabeled data, and a hard-voting strategy combines predictions for improved accuracy and confidence. Fine-tuning on the expanded dataset with dynamic learning rates and early stopping significantly boosts classification accuracy, especially in specialized domains. Domain-specific models like SciBERT and BioBERT consistently outperform general-purpose models such as BERT. These findings underscore the efficacy of dataset augmentation, inference-driven label prediction, hard-voting, and fine-tuning techniques in creating robust and scalable solutions for automated academic text classification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation</title>
<link>https://arxiv.org/abs/2505.11454</link>
<guid>https://arxiv.org/abs/2505.11454</guid>
<content:encoded><![CDATA[
arXiv:2505.11454v4 Announce Type: replace-cross 
Abstract: Large multimodal models (LMMs) have been widely tested on tasks like visual question answering (VQA), image captioning, and grounding, but lack rigorous evaluation for alignment with human-centered (HC) values such as fairness, ethics, and inclusivity. To address this gap, we introduce \textbf{HumaniBench}, a novel benchmark of 32,000 real-world image-question pairs and an evaluation suite. Labels are generated via an AI-assisted pipeline and validated by experts. HumaniBench assesses LMMs across seven key alignment principles: fairness, ethics, empathy, inclusivity, reasoning, robustness, and multilinguality, through diverse open-ended and closed-ended VQA tasks. Grounded in AI ethics and real-world needs, these principles provide a holistic lens for societal impact. Benchmarking results on different LMM shows that proprietary models generally lead in reasoning, fairness, and multilinguality, while open-source models excel in robustness and grounding. Most models struggle to balance accuracy with ethical and inclusive behavior. Techniques like Chain-of-Thought prompting and test-time scaling improve alignment. As the first benchmark tailored for HC alignment, HumaniBench offers a rigorous testbed to diagnose limitations, and promote responsible LMM development. All data and code are publicly available for reproducibility.
  Keywords: HumaniBench, vision-language models, responsible AI benchmark, AI alignment evaluation, AI ethics assessment, fairness in AI models, visual question answering (VQA) benchmark, image captioning evaluation, visual grounding tasks, trustworthy AI models, Chain-of-Thought prompting, test-time scaling, ethical AI development tools.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterFeat: A Pipeline for Finding Interesting Scientific Features</title>
<link>https://arxiv.org/abs/2505.13534</link>
<guid>https://arxiv.org/abs/2505.13534</guid>
<content:encoded><![CDATA[
arXiv:2505.13534v2 Announce Type: replace-cross 
Abstract: Finding interesting phenomena is the core of scientific discovery, but it is a manual, ill-defined concept. We present an integrative pipeline for automating the discovery of interesting simple hypotheses (feature-target relations with effect direction and a potential underlying mechanism) in structured biomedical data. The pipeline combines machine learning, knowledge graphs, literature search and Large Language Models. We formalize "interestingness" as a combination of novelty, utility and plausibility. On 8 major diseases from the UK Biobank, our pipeline consistently recovers risk factors years before their appearance in the literature. 40--53% of our top candidates were validated as interesting, compared to 0--7% for a SHAP-based baseline. Overall, 28% of 109 candidates were interesting to medical experts. The pipeline addresses the challenge of operationalizing "interestingness" scalably and for any target. We release data and code: https://github.com/LinialLab/InterFeat
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification for Health Management with Spatial-Temporal Hypergraph Enhanced Meta-Learning</title>
<link>https://arxiv.org/abs/2505.17142</link>
<guid>https://arxiv.org/abs/2505.17142</guid>
<content:encoded><![CDATA[
arXiv:2505.17142v2 Announce Type: replace-cross 
Abstract: Accurate classification of sleep stages based on bio-signals is fundamental not only for automatic sleep stage annotation, but also for clinical health management and continuous sleep monitoring. Traditionally, this task relies on experienced clinicians to manually annotate data, a process that is both time-consuming and labor-intensive. In recent years, deep learning methods have shown promise in automating this task. However, three major challenges remain: (1) deep learning models typically require large-scale labeled datasets, making them less effective in real-world settings where annotated data is limited; (2) significant inter-individual variability in bio-signals often results in inconsistent model performance when applied to new subjects, limiting generalization; and (3) existing approaches often overlook the high-order relationships among bio-signals, failing to simultaneously capture signal heterogeneity and spatial-temporal dependencies. To address these issues, we propose MetaSTH-Sleep, a few-shot sleep stage classification framework based on spatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid adaptation to new subjects using only a few labeled samples, while the hypergraph structure effectively models complex spatial interconnections and temporal dynamics simultaneously in EEG signals. Experimental results demonstrate that MetaSTH-Sleep achieves substantial performance improvements across diverse subjects, offering valuable insights to support clinicians in sleep stage annotation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LLM Reasoning Through Bias-Only Adaptation</title>
<link>https://arxiv.org/abs/2505.18706</link>
<guid>https://arxiv.org/abs/2505.18706</guid>
<content:encoded><![CDATA[
arXiv:2505.18706v2 Announce Type: replace-cross 
Abstract: We show that training a single $d$-dimensional steering vector per layer with reinforcement learning, while freezing all base weights, matches the accuracy of fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8 billion-parameter model this adds only $\approx 0.0016\%$ additional parameters and reproduces performance across a range of base models and mathematical-reasoning benchmarks. These results tighten the upper bound on the parameter budget required for high-level chain-of-thought reasoning, indicating that millions of adapter weights are unnecessary. The minimal trainable footprint reduces optimizer memory and inter-GPU communication, lowering the overall cost of fine-tuning. Moreover, a logit-lens analysis shows that the learned vectors amplify coherent token directions, providing clearer insight into the model's internal computations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoloSpeech: Enhancing Intelligibility and Quality in Target Speech Extraction through a Cascaded Generative Pipeline</title>
<link>https://arxiv.org/abs/2505.19314</link>
<guid>https://arxiv.org/abs/2505.19314</guid>
<content:encoded><![CDATA[
arXiv:2505.19314v3 Announce Type: replace-cross 
Abstract: Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio's latent space, aligning it with the mixture audio's latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction while demonstrating exceptional generalization on out-of-domain data and real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-output Classification using a Cross-talk Architecture for Compound Fault Diagnosis of Motors in Partially Labeled Condition</title>
<link>https://arxiv.org/abs/2505.24001</link>
<guid>https://arxiv.org/abs/2505.24001</guid>
<content:encoded><![CDATA[
arXiv:2505.24001v2 Announce Type: replace-cross 
Abstract: The increasing complexity of rotating machinery and the diversity of operating conditions, such as rotating speed and varying torques, have amplified the challenges in fault diagnosis in scenarios requiring domain adaptation, particularly involving compound faults. This study addresses these challenges by introducing a novel multi-output classification (MOC) framework tailored for domain adaptation in partially labeled target datasets. Unlike conventional multi-class classification (MCC) approaches, the MOC framework classifies the severity levels of compound faults simultaneously. Furthermore, we explore various single-task and multi-task architectures applicable to the MOC formulation-including shared trunk and cross-talk-based designs-for compound fault diagnosis under partially labeled conditions. Based on this investigation, we propose a novel cross-talk architecture, residual neural dimension reductor (RNDR), that enables selective information sharing across diagnostic tasks, effectively enhancing classification performance in compound fault scenarios. In addition, frequency-layer normalization was incorporated to improve domain adaptation performance on motor vibration data. Compound fault conditions were implemented using a motor-based test setup and evaluated across six domain adaptation scenarios. The experimental results demonstrate its superior macro F1 performance compared to baseline models. We further showed that the structural advantage of RNDR is more pronounced in compound fault settings through a single-fault comparison. We also found that frequency-layer normalization fits the fault diagnosis task better than conventional methods. Lastly, we analyzed the RNDR with various conditions, other models with increased number of parameters, and compared with the ablated RNDR structure.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport</title>
<link>https://arxiv.org/abs/2505.24759</link>
<guid>https://arxiv.org/abs/2505.24759</guid>
<content:encoded><![CDATA[
arXiv:2505.24759v2 Announce Type: replace-cross 
Abstract: Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches often rely on either reference-based matching, which imposes asymmetry by designating one species as the reference, or projection-based matching, which may increase computational complexity and obscure biological interpretability at the cell-type level. Here, we present OT-MESH, an unsupervised computational framework leveraging entropy-regularized optimal transport (OT) to systematically determine cross-species cell type homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan, transforming diffuse transport matrices into sparse, interpretable correspondences. Through systematic evaluation on synthetic datasets, we demonstrate that OT-MESH achieves near-optimal matching accuracy with computational efficiency, while maintaining remarkable robustness to noise. Compared to other OT-based methods like RefCM, OT-MESH provides speedup while achieving comparable accuracy. Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally. Thus, our framework offers a principled, scalable, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets Credit or Blame? Attributing Accountability in Modern AI Systems</title>
<link>https://arxiv.org/abs/2506.00175</link>
<guid>https://arxiv.org/abs/2506.00175</guid>
<content:encoded><![CDATA[
arXiv:2506.00175v3 Announce Type: replace-cross 
Abstract: Modern AI systems are typically developed through multiple stages-pretraining, fine-tuning rounds, and subsequent adaptation or alignment, where each stage builds on the previous ones and updates the model in distinct ways. This raises a critical question of accountability: when a deployed model succeeds or fails, which stage is responsible, and to what extent? We pose the accountability attribution problem for tracing model behavior back to specific stages of the model development process. To address this challenge, we propose a general framework that answers counterfactual questions about stage effects: how would the model's behavior have changed if the updates from a particular stage had not occurred? Within this framework, we introduce estimators that efficiently quantify stage effects without retraining the model, accounting for both the data and key aspects of model optimization dynamics, including learning rate schedules, momentum, and weight decay. We demonstrate that our approach successfully quantifies the accountability of each stage to the model's behavior. Based on the attribution results, our method can identify and remove spurious correlations learned during image classification and text toxicity detection tasks that were developed across multiple stages. Our approach provides a practical tool for model analysis and represents a significant step toward more accountable AI development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.07040</link>
<guid>https://arxiv.org/abs/2506.07040</guid>
<content:encoded><![CDATA[
arXiv:2506.07040v2 Announce Type: replace-cross 
Abstract: We present a non-asymptotic convergence analysis of $Q$-learning and actor-critic algorithms for robust average-reward Markov Decision Processes (MDPs) under contamination, total-variation (TV) distance, and Wasserstein uncertainty sets. A key ingredient of our analysis is showing that the optimal robust $Q$ operator is a strict contraction with respect to a carefully designed semi-norm (with constant functions quotiented out). This property enables a stochastic approximation update that learns the optimal robust $Q$-function using $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We also provide an efficient routine for robust $Q$-function estimation, which in turn facilitates robust critic estimation. Building on this, we introduce an actor-critic algorithm that learns an $\epsilon$-optimal robust policy within $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We provide numerical simulations to evaluate the performance of our algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws of Motion Forecasting and Planning - Technical Report</title>
<link>https://arxiv.org/abs/2506.08228</link>
<guid>https://arxiv.org/abs/2506.08228</guid>
<content:encoded><![CDATA[
arXiv:2506.08228v2 Announce Type: replace-cross 
Abstract: We study the empirical scaling laws of a family of encoder-decoder autoregressive transformer models on the task of joint motion forecasting and planning in the autonomous driving domain. Using a 500 thousand hours driving dataset, we demonstrate that, similar to language modeling, model performance improves as a power-law function of the total compute budget, and we observe a strong correlation between model training loss and model evaluation metrics. Most interestingly, closed-loop metrics also improve with scaling, which has important implications for the suitability of open-loop metrics for model development and hill climbing. We also study the optimal scaling of the number of transformer parameters and the training data size for a training compute-optimal model. We find that as the training compute budget grows, optimal scaling requires increasing the model size 1.5x as fast as the dataset size. We also study inference-time compute scaling, where we observe that sampling and clustering the output of smaller models makes them competitive with larger models, up to a crossover point beyond which a larger models becomes more inference-compute efficient. Overall, our experimental results demonstrate that optimizing the training and inference-time scaling properties of motion forecasting and planning models is a key lever for improving their performance to address a wide variety of driving scenarios. Finally, we briefly study the utility of training on general logged driving data of other agents to improve the performance of the ego-agent, an important research area to address the scarcity of robotics data for large capacity models training.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations</title>
<link>https://arxiv.org/abs/2506.09383</link>
<guid>https://arxiv.org/abs/2506.09383</guid>
<content:encoded><![CDATA[
arXiv:2506.09383v2 Announce Type: replace-cross 
Abstract: Balance control is important for human and bipedal robotic systems. While dynamic balance during locomotion has received considerable attention, quantitative understanding of static balance and falling remains limited. This work presents a hierarchical control pipeline for simulating human balance via a comprehensive whole-body musculoskeletal system. We identified spatiotemporal dynamics of balancing during stable standing, revealed the impact of muscle injury on balancing behavior, and generated fall contact patterns that aligned with clinical data. Furthermore, our simulated hip exoskeleton assistance demonstrated improvement in balance maintenance and reduced muscle effort under perturbation. This work offers unique muscle-level insights into human balance dynamics that are challenging to capture experimentally. It could provide a foundation for developing targeted interventions for individuals with balance impairments and support the advancement of humanoid robotic systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models</title>
<link>https://arxiv.org/abs/2506.11798</link>
<guid>https://arxiv.org/abs/2506.11798</guid>
<content:encoded><![CDATA[
arXiv:2506.11798v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse, but have been found to consistently display a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups that the base model is not aligned with. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict positions of European groups on a diverse set of policies. We evaluate if predictions are stable towards counterfactual arguments, different persona prompts and generation methods. Finally, we find that we can simulate voting behavior of Members of the European Parliament reasonably well with a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at https://github.com/dess-mannheim/european_parliament_simulation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies</title>
<link>https://arxiv.org/abs/2506.11948</link>
<guid>https://arxiv.org/abs/2506.11948</guid>
<content:encoded><![CDATA[
arXiv:2506.11948v2 Announce Type: replace-cross 
Abstract: Offline Imitation Learning (IL) methods such as Behavior Cloning are effective at acquiring complex robotic manipulation skills. However, existing IL-trained policies are confined to executing the task at the same speed as shown in demonstration data. This limits the task throughput of a robotic system, a critical requirement for applications such as industrial automation. In this paper, we introduce and formalize the novel problem of enabling faster-than-demonstration execution of visuomotor policies and identify fundamental challenges in robot dynamics and state-action distribution shifts. We instantiate the key insights as SAIL (Speed Adaptation for Imitation Learning), a full-stack system integrating four tightly-connected components: (1) a consistency-preserving action inference algorithm for smooth motion at high speed, (2) high-fidelity tracking of controller-invariant motion targets, (3) adaptive speed modulation that dynamically adjusts execution speed based on motion complexity, and (4) action scheduling to handle real-world system latencies. Experiments on 12 tasks across simulation and two real, distinct robot platforms show that SAIL achieves up to a 4x speedup over demonstration speed in simulation and up to 3.2x speedup in the real world. Additional detail is available at https://nadunranawaka1.github.io/sail-policy
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems</title>
<link>https://arxiv.org/abs/2506.14096</link>
<guid>https://arxiv.org/abs/2506.14096</guid>
<content:encoded><![CDATA[
arXiv:2506.14096v2 Announce Type: replace-cross 
Abstract: The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Intelligence: Designing Data Centers for Next-Gen Language Models</title>
<link>https://arxiv.org/abs/2506.15006</link>
<guid>https://arxiv.org/abs/2506.15006</guid>
<content:encoded><![CDATA[
arXiv:2506.15006v3 Announce Type: replace-cross 
Abstract: The explosive growth of Large Language Models (LLMs), such as GPT-4 with 1.8 trillion parameters, demands a fundamental rethinking of data center architecture to ensure scalability, efficiency, and cost-effectiveness. Our work provides a comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat optical), the size of the scale-out domain, and popular parallelism/optimization strategies used in LLMs. We introduce and evaluate FullFlat network architectures, which provide uniform high-bandwidth, low-latency connectivity between all nodes, and demonstrate their transformative impact on performance and scalability. Through detailed sensitivity analyses, we quantify the benefits of overlapping compute and communication, leveraging hardware-accelerated collectives, widening the scale-out domain, and increasing memory capacity. Our study spans both sparse (mixture of experts) and dense transformer-based LLMs, revealing how system design choices affect Model FLOPS Utilization (MFU = Model FLOPS per token * Observed tokens per second / Peak FLOPS of the hardware) and overall throughput. For the co-design study, we utilized an analytical performance modeling tool capable of predicting LLM runtime within 10% of real-world measurements. Our findings offer actionable insights and a practical roadmap for designing AI data centers that can efficiently support trillion-parameter models, reduce optimization complexity, and sustain the rapid evolution of AI capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transit for All: Mapping Equitable Bike2Subway Connection using Region Representation Learning</title>
<link>https://arxiv.org/abs/2506.15113</link>
<guid>https://arxiv.org/abs/2506.15113</guid>
<content:encoded><![CDATA[
arXiv:2506.15113v2 Announce Type: replace-cross 
Abstract: Ensuring equitable public transit access remains challenging, particularly in densely populated cities like New York City (NYC), where low-income and minority communities often face limited transit accessibility. Bike-sharing systems (BSS) can bridge these equity gaps by providing affordable first- and last-mile connections. However, strategically expanding BSS into underserved neighborhoods is difficult due to uncertain bike-sharing demand at newly planned ("cold-start") station locations and limitations in traditional accessibility metrics that may overlook realistic bike usage potential. We introduce Transit for All (TFA), a spatial computing framework designed to guide the equitable expansion of BSS through three components: (1) spatially-informed bike-sharing demand prediction at cold-start stations using region representation learning that integrates multimodal geospatial data, (2) comprehensive transit accessibility assessment leveraging our novel weighted Public Transport Accessibility Level (wPTAL) by combining predicted bike-sharing demand with conventional transit accessibility metrics, and (3) strategic recommendations for new bike station placements that consider potential ridership and equity enhancement. Using NYC as a case study, we identify transit accessibility gaps that disproportionately impact low-income and minority communities in historically underserved neighborhoods. Our results show that strategically placing new stations guided by wPTAL notably reduces disparities in transit access related to economic and demographic factors. From our study, we demonstrate that TFA provides practical guidance for urban planners to promote equitable transit and enhance the quality of life in underserved urban communities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precise Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2506.19726</link>
<guid>https://arxiv.org/abs/2506.19726</guid>
<content:encoded><![CDATA[
arXiv:2506.19726v2 Announce Type: replace-cross 
Abstract: Despite its long history, Bayesian neural networks (BNNs) and variational training remain underused in practice: standard Gaussian posteriors misalign with network geometry, KL terms can be brittle in high dimensions, and implementations often add complexity without reliably improving uncertainty. We revisit the problem through the lens of normalization. Because normalization layers neutralize the influence of weight magnitude, we model uncertainty \emph{only in weight directions} using a von Mises-Fisher posterior on the unit sphere. High-dimensional geometry then yields a single, interpretable scalar per layer--the effective post-normalization noise $\sigma_{\mathrm{eff}}$--that (i) corresponds to simple additive Gaussian noise in the forward pass and (ii) admits a compact, dimension-aware KL in closed form. We derive accurate, closed-form approximations linking concentration $\kappa$ to activation variance and to $\sigma_{\mathrm{eff}}$ across regimes, producing a lightweight, implementation-ready variational unit that fits modern normalized architectures and improves calibration without sacrificing accuracy. This dimension awareness is critical for stable optimization in high dimensions. In short, by aligning the variational posterior with the network's intrinsic geometry, BNNs can be simultaneously principled, practical, and precise.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs</title>
<link>https://arxiv.org/abs/2506.22146</link>
<guid>https://arxiv.org/abs/2506.22146</guid>
<content:encoded><![CDATA[
arXiv:2506.22146v3 Announce Type: replace-cross 
Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual reasoning is often limited by the binding problem: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current VLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention. This paper introduces VISER (Visual Input Structure for Enhanced Reasoning), a simple yet effective intervention: augmenting visual inputs with low-level spatial structures and pairing this with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks. Specifically, VISER improves GPT-4o visual search accuracy by 25.00%, increases counting accuracy by 26.83%, reduces edit distance error in scene description by 0.32, and enhances performance on spatial relationship tasks by 9.50% on a 2D synthetic dataset. Furthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. VISER enhances binding only with a single-query inference, underscoring the importance of visual input design over purely linguistically-based approaches. These findings suggest that low-level visual structuring is a powerful and underexplored direction for improving compositional visual reasoning and could serve as a general strategy for enhancing VLM performance on spatially grounded tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Out-of-Distribution Unlabeled Images: Semi-Supervised Semantic Segmentation with an Open-Vocabulary Model</title>
<link>https://arxiv.org/abs/2507.03302</link>
<guid>https://arxiv.org/abs/2507.03302</guid>
<content:encoded><![CDATA[
arXiv:2507.03302v2 Announce Type: replace-cross 
Abstract: In semi-supervised semantic segmentation, existing studies have shown promising results in academic settings with controlled splits of benchmark datasets. However, the potential benefits of leveraging significantly larger sets of unlabeled images remain unexplored. In real-world scenarios, abundant unlabeled images are often available from online sources (web-scraped images) or large-scale datasets. However, these images may have different distributions from those of the target dataset, a situation known as out-of-distribution (OOD). Using these images as unlabeled data in semi-supervised learning can lead to inaccurate pseudo-labels, potentially misguiding network training. In this paper, we propose a new semi-supervised semantic segmentation framework with an open-vocabulary segmentation model (SemiOVS) to effectively utilize unlabeled OOD images. Extensive experiments on Pascal VOC and Context datasets demonstrate two key findings: (1) using additional unlabeled images improves the performance of semi-supervised learners in scenarios with few labels, and (2) using the open-vocabulary segmentation (OVS) model to pseudo-label OOD images leads to substantial performance gains. In particular, SemiOVS outperforms existing PrevMatch and SemiVL methods by +3.5 and +3.0 mIoU, respectively, on Pascal VOC with a 92-label setting, achieving state-of-the-art performance. These findings demonstrate that our approach effectively utilizes abundant unlabeled OOD images for semantic segmentation tasks. We hope this work can inspire future research and real-world applications. The code is available at https://github.com/wooseok-shin/SemiOVS
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles</title>
<link>https://arxiv.org/abs/2507.04139</link>
<guid>https://arxiv.org/abs/2507.04139</guid>
<content:encoded><![CDATA[
arXiv:2507.04139v2 Announce Type: replace-cross 
Abstract: Ensuring safe transition of control in automated vehicles requires an accurate and timely assessment of driver readiness. This paper introduces Driver-Net, a novel deep learning framework that fuses multi-camera inputs to estimate driver take-over readiness. Unlike conventional vision-based driver monitoring systems that focus on head pose or eye gaze, Driver-Net captures synchronised visual cues from the driver's head, hands, and body posture through a triple-camera setup. The model integrates spatio-temporal data using a dual-path architecture, comprising a Context Block and a Feature Block, followed by a cross-modal fusion strategy to enhance prediction accuracy. Evaluated on a diverse dataset collected from the University of Leeds Driving Simulator, the proposed method achieves an accuracy of up to 95.8% in driver readiness classification. This performance significantly enhances existing approaches and highlights the importance of multimodal and multi-view fusion. As a real-time, non-intrusive solution, Driver-Net contributes meaningfully to the development of safer and more reliable automated vehicles and aligns with new regulatory mandates and upcoming safety standards.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLAME: Lightweight MSA Design Advances Protein Folding From Evolutionary Embeddings</title>
<link>https://arxiv.org/abs/2507.07032</link>
<guid>https://arxiv.org/abs/2507.07032</guid>
<content:encoded><![CDATA[
arXiv:2507.07032v2 Announce Type: replace-cross 
Abstract: Protein structure prediction often hinges on multiple sequence alignments (MSAs), which underperform on low-homology and orphan proteins. We introduce PLAME, a lightweight MSA design framework that leverages evolutionary embeddings from pretrained protein language models to generate MSAs that better support downstream folding. PLAME couples these embeddings with a conservation-diversity loss that balances agreement on conserved positions with coverage of plausible sequence variation. Beyond generation, we develop (i) an MSA selection strategy to filter high-quality candidates and (ii) a sequence-quality metric that is complementary to depth-based measures and predictive of folding gains. On AlphaFold2 low-homology/orphan benchmarks, PLAME delivers state-of-the-art improvements in structure accuracy (e.g., lDDT/TM-score), with consistent gains when paired with AlphaFold3. Ablations isolate the benefits of the selection strategy, and case studies elucidate how MSA characteristics shape AlphaFold confidence and error modes. Finally, we show PLAME functions as a lightweight adapter, enabling ESMFold to approach AlphaFold2-level accuracy while retaining ESMFold-like inference speed. PLAME thus provides a practical path to high-quality folding for proteins lacking strong evolutionary neighbors.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias</title>
<link>https://arxiv.org/abs/2507.11210</link>
<guid>https://arxiv.org/abs/2507.11210</guid>
<content:encoded><![CDATA[
arXiv:2507.11210v2 Announce Type: replace-cross 
Abstract: Well-being in family settings involves subtle psychological dynamics that conventional metrics often overlook. In particular, unconscious parental expectations, termed ideal parent bias, can suppress children's emotional expression and autonomy. This suppression, referred to as suppressed emotion, often stems from well-meaning but value-driven communication, which is difficult to detect or address from outside the family. Focusing on these latent dynamics, this study explores Large Language Model (LLM)-based support for psychologically safe family communication. We constructed a Japanese parent-child dialogue corpus of 30 scenarios, each annotated with metadata on ideal parent bias and suppressed emotion. Based on this corpus, we developed a Role-Playing LLM-based multi-agent dialogue support framework that analyzes dialogue and generates feedback. Specialized agents detect suppressed emotion, describe implicit ideal parent bias in parental speech, and infer contextual attributes such as the child's age and background. A meta-agent compiles these outputs into a structured report, which is then passed to five selected expert agents. These agents collaboratively generate empathetic and actionable feedback through a structured four-step discussion process. Experiments show that the system can detect categories of suppressed emotion with moderate accuracy and produce feedback rated highly in empathy and practicality. Moreover, simulated follow-up dialogues incorporating this feedback exhibited signs of improved emotional expression and mutual understanding, suggesting the framework's potential in supporting positive transformation in family interactions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)</title>
<link>https://arxiv.org/abs/2507.12856</link>
<guid>https://arxiv.org/abs/2507.12856</guid>
<content:encoded><![CDATA[
arXiv:2507.12856v2 Announce Type: replace-cross 
Abstract: Behavior Cloning (BC) on curated (or filtered) data is the predominant paradigm for supervised fine-tuning (SFT) of large language models; as well as for imitation learning of control policies. Here, we draw on a connection between this successful strategy and the theory and practice of finding optimal policies via Reinforcement Learning (RL). Building on existing literature, we clarify that SFT can be understood as maximizing a lower bound on the RL objective in a sparse reward setting. Giving support to its often observed good performance. From this viewpoint, we realize that a small modification to SFT leads to an importance weighted variant that behaves closer to training with RL as it: i) optimizes a tighter bound to the RL objective and, ii) can improve performance compared to SFT on curated data. We refer to this variant as importance weighted supervised fine-tuning (iw-SFT). We show that it is easy to implement and can be further generalized to training with quality scored data. The resulting SFT variants are competitive with more advanced RL algorithms for large language models and for training policies in continuous control tasks. For example achieving 66.7% on the AIME 2024 dataset.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RecPS: Privacy Risk Scoring for Recommender Systems</title>
<link>https://arxiv.org/abs/2507.18365</link>
<guid>https://arxiv.org/abs/2507.18365</guid>
<content:encoded><![CDATA[
arXiv:2507.18365v4 Announce Type: replace-cross 
Abstract: Recommender systems (RecSys) have become an essential component of many web applications. The core of the system is a recommendation model trained on highly sensitive user-item interaction data. While privacy-enhancing techniques are actively studied in the research community, the real-world model development still depends on minimal privacy protection, e.g., via controlled access. Users of such systems should have the right to choose \emph{not} to share highly sensitive interactions. However, there is no method allowing the user to know which interactions are more sensitive than others. Thus, quantifying the privacy risk of RecSys training data is a critical step to enabling privacy-aware RecSys model development and deployment. We propose a membership-inference attack (MIA)- based privacy scoring method, RecPS, to measure privacy risks at both the interaction and user levels. The RecPS interaction-level score definition is motivated and derived from differential privacy, which is then extended to the user-level scoring method. A critical component is the interaction-level MIA method RecLiRA, which gives high-quality membership estimation. We have conducted extensive experiments on well-known benchmark datasets and RecSys models to show the unique features and benefits of RecPS scoring in risk assessment and RecSys model unlearning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning</title>
<link>https://arxiv.org/abs/2508.00716</link>
<guid>https://arxiv.org/abs/2508.00716</guid>
<content:encoded><![CDATA[
arXiv:2508.00716v3 Announce Type: replace-cross 
Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled source graphs to unlabeled target graphs by learning domain-invariant representations, which is essential in applications such as molecular property prediction and social network analysis. However, most existing GDA methods rely on the assumption of clean source labels, which rarely holds in real-world scenarios where annotation noise is pervasive. This label noise severely impairs feature alignment and degrades adaptation performance under domain shifts. To address this challenge, we propose Nested Graph Pseudo-Label Refinement (NeGPR), a novel framework tailored for graph-level domain adaptation with noisy labels. NeGPR first pretrains dual branches, i.e., semantic and topology branches, by enforcing neighborhood consistency in the feature space, thereby reducing the influence of noisy supervision. To bridge domain gaps, NeGPR employs a nested refinement mechanism in which one branch selects high-confidence target samples to guide the adaptation of the other, enabling progressive cross-domain learning. Furthermore, since pseudo-labels may still contain noise and the pre-trained branches are already overfitted to the noisy labels in the source domain, NeGPR incorporates a noise-aware regularization strategy. This regularization is theoretically proven to mitigate the adverse effects of pseudo-label noise, even under the presence of source overfitting, thus enhancing the robustness of the adaptation process. Extensive experiments on benchmark datasets demonstrate that NeGPR consistently outperforms state-of-the-art methods under severe label noise, achieving gains of up to 12.7% in accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA</title>
<link>https://arxiv.org/abs/2508.00719</link>
<guid>https://arxiv.org/abs/2508.00719</guid>
<content:encoded><![CDATA[
arXiv:2508.00719v3 Announce Type: replace-cross 
Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning</title>
<link>https://arxiv.org/abs/2508.01131</link>
<guid>https://arxiv.org/abs/2508.01131</guid>
<content:encoded><![CDATA[
arXiv:2508.01131v2 Announce Type: replace-cross 
Abstract: In this work, we study the problem of data retrieval for few-shot imitation learning: selecting data from a large dataset to train a performant policy for a specific task, given only a few target demonstrations. Prior methods retrieve data using a single-feature distance heuristic, assuming that the best demonstrations are those that most closely resemble the target examples in visual, semantic, or motion space. However, this approach captures only a subset of the relevant information and can introduce detrimental demonstrations, e.g., retrieving data from unrelated tasks due to similar scene layouts, or selecting similar motions from tasks with divergent goals. We present COLLAGE, a method for COLLective data AGgrEgation in few-shot imitation learning that uses an adaptive late fusion mechanism to guide the selection of relevant demonstrations based on a task-specific combination of multiple cues. COLLAGE follows a simple, flexible, and efficient recipe: it assigns weights to subsets of the dataset that are pre-selected using a single feature (e.g., appearance, shape, or language similarity), based on how well a policy trained on each subset predicts actions in the target demonstrations. These weights are then used to perform importance sampling during policy training, sampling data more densely or sparsely according to estimated relevance. COLLAGE is general and feature-agnostic, allowing it to combine any number of subsets selected by any retrieval heuristic, and to identify which subsets provide the greatest benefit for the target task. In extensive experiments, COLLAGE outperforms state-of-the-art retrieval and multi-task learning approaches by 5.1% in simulation across 10 tasks, and by 16.6% in the real world across 6 tasks, where we perform retrieval from the large-scale DROID dataset. More information at https://robin-lab.cs.utexas.edu/COLLAGE .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIRF: A Framework for Digital Identity Protection and Clone Governance in Agentic AI Systems</title>
<link>https://arxiv.org/abs/2508.01997</link>
<guid>https://arxiv.org/abs/2508.01997</guid>
<content:encoded><![CDATA[
arXiv:2508.01997v2 Announce Type: replace-cross 
Abstract: The rapid advancement and widespread adoption of generative artificial intelligence (AI) pose significant threats to the integrity of personal identity, including digital cloning, sophisticated impersonation, and the unauthorized monetization of identity-related data. Mitigating these risks necessitates the development of robust AI-generated content detection systems, enhanced legal frameworks, and ethical guidelines. This paper introduces the Digital Identity Rights Framework (DIRF), a structured security and governance model designed to protect behavioral, biometric, and personality-based digital likeness attributes to address this critical need. Structured across nine domains and 63 controls, DIRF integrates legal, technical, and hybrid enforcement mechanisms to secure digital identity consent, traceability, and monetization. We present the architectural foundations, enforcement strategies, and key use cases supporting the need for a unified framework. This work aims to inform platform builders, legal entities, and regulators about the essential controls needed to enforce identity rights in AI-driven systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Continuous-Time MILP for Integrated Aircraft Hangar Scheduling and Layout</title>
<link>https://arxiv.org/abs/2508.02640</link>
<guid>https://arxiv.org/abs/2508.02640</guid>
<content:encoded><![CDATA[
arXiv:2508.02640v2 Announce Type: replace-cross 
Abstract: Efficient management of aircraft MRO hangars requires the integration of spatial layout with time-continuous scheduling to minimize operational costs. We propose a continuous-time mixed-integer linear program that jointly optimizes aircraft placement and timing, overcoming the scalability limits of prior formulations. A comprehensive study benchmarks the model against a constructive heuristic, probes large-scale performance, and quantifies its sensitivity to temporal congestion. The model achieves orders-of-magnitude speedups on benchmarks from the literature, solving a long-standing congested instance in 0.11 seconds, and finds proven optimal solutions for instances with up to 40 aircraft. Within a one-hour limit for large-scale problems, the model finds solutions with small optimality gaps for instances up to 80 aircraft and provides strong bounds for problems with up to 160 aircraft. Optimized plans consistently increase hangar throughput (e.g., +33% serviced aircraft vs. a heuristic on instance RND-N030-I03), leading to lower delay penalties and higher asset utilization. These findings establish that exact optimization has become computationally viable for large-scale hangar planning, providing a validated tool that balances solution quality and computation time for strategic and operational decisions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion</title>
<link>https://arxiv.org/abs/2508.05264</link>
<guid>https://arxiv.org/abs/2508.05264</guid>
<content:encoded><![CDATA[
arXiv:2508.05264v2 Announce Type: replace-cross 
Abstract: Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion model's coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at https://github.com/boshizhang123/SGDFuse.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding</title>
<link>https://arxiv.org/abs/2508.06869</link>
<guid>https://arxiv.org/abs/2508.06869</guid>
<content:encoded><![CDATA[
arXiv:2508.06869v2 Announce Type: replace-cross 
Abstract: Long video understanding presents a significant challenge to multimodal large language models (MLLMs) primarily due to the immense data scale. A critical and widely adopted strategy for making this task computationally tractable is keyframe retrieval, which seeks to identify a sparse set of video frames that are most salient to a given textual query. However, the efficacy of this approach is hindered by weak multimodal alignment between textual queries and visual content and fails to capture the complex temporal semantic information required for precise reasoning. To address this, we propose Visual-Subtitle Integeration(VSI), a multimodal keyframe search method that integrates subtitles, timestamps, and scene boundaries into a unified multimodal search process. The proposed method captures the visual information of video frames as well as the complementary textual information through a dual-stream search mechanism by Video Search Stream as well as Subtitle Match Stream, respectively, and improves the keyframe search accuracy through the interaction of the two search streams. Experimental results show that VSI achieve 40.00% key frame localization accuracy on the text-relevant subset of LongVideoBench and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive baselines by 20.35% and 15.79%, respectively. Furthermore, on the LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA tasks, demonstrating the robustness and generalizability of the proposed multimodal search strategy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures</title>
<link>https://arxiv.org/abs/2508.07423</link>
<guid>https://arxiv.org/abs/2508.07423</guid>
<content:encoded><![CDATA[
arXiv:2508.07423v3 Announce Type: replace-cross 
Abstract: As the particle physics community needs higher and higher precisions in order to test our current model of the subatomic world, larger and larger datasets are necessary. With upgrades scheduled for the detectors of colliding-beam experiments around the world, and specifically at the Large Hadron Collider at CERN, more collisions and more complex interactions are expected. This directly implies an increase in data produced and consequently in the computational resources needed to process them. At CERN, the amount of data produced is gargantuan. This is why the data have to be heavily filtered and selected in real time before being permanently stored. This data can then be used to perform physics analyses, in order to expand our current understanding of the universe and improve the Standard Model of physics. This real-time filtering, known as triggering, involves complex processing happening often at frequencies as high as 40 MHz. This thesis contributes to understanding how machine learning models can be efficiently deployed in such environments, in order to maximize throughput and minimize energy consumption. Inevitably, modern hardware designed for such tasks and contemporary algorithms are needed in order to meet the challenges posed by the stringent, high-frequency data rates. In this work, I present our graph neural network-based pipeline, developed for charged particle track reconstruction at the LHCb experiment at CERN. The pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely on GPUs. Its performance was compared against the classical tracking algorithms currently in production at LHCb. The pipeline was also accelerated on the FPGA architecture, and its performance in terms of power consumption and processing speed was compared against the GPU implementation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning</title>
<link>https://arxiv.org/abs/2508.07556</link>
<guid>https://arxiv.org/abs/2508.07556</guid>
<content:encoded><![CDATA[
arXiv:2508.07556v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes domains where reliability is paramount. This thesis investigates how uncertainty estimation can enhance the safety and trustworthiness of ML, focusing on selective prediction -- where models abstain when confidence is low.
  We first show that a model's training trajectory contains rich uncertainty signals that can be exploited without altering its architecture or loss. By ensembling predictions from intermediate checkpoints, we propose a lightweight, post-hoc abstention method that works across tasks, avoids the cost of deep ensembles, and achieves state-of-the-art selective prediction performance. Crucially, this approach is fully compatible with differential privacy (DP), allowing us to study how privacy noise affects uncertainty quality. We find that while many methods degrade under DP, our trajectory-based approach remains robust, and we introduce a framework for isolating the privacy-uncertainty trade-off. Next, we then develop a finite-sample decomposition of the selective classification gap -- the deviation from the oracle accuracy-coverage curve -- identifying five interpretable error sources and clarifying which interventions can close the gap. This explains why calibration alone cannot fix ranking errors, motivating methods that improve uncertainty ordering. Finally, we show that uncertainty signals can be adversarially manipulated to hide errors or deny service while maintaining high accuracy, and we design defenses combining calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating, and safeguarding uncertainty estimation, enabling models that not only make accurate predictions -- but also know when to say "I do not know".
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery</title>
<link>https://arxiv.org/abs/2508.09183</link>
<guid>https://arxiv.org/abs/2508.09183</guid>
<content:encoded><![CDATA[
arXiv:2508.09183v2 Announce Type: replace-cross 
Abstract: Quantum computation has demonstrated a promising alternative to solving the NP-hard combinatorial problems. Specifically, when it comes to optimization, classical approaches become intractable to account for large-scale solutions. Specifically, we investigate quantum computing to solve the large-scale Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this regard, a Reinforcement Learning (RL) framework augmented with a Parametrized Quantum Circuit (PQC) is designed to minimize the travel time in a realistic last-mile on-demand delivery. A novel problem-specific encoding quantum circuit with an entangling and variational layer is proposed. Moreover, Proximal Policy Optimization (PPO) and Quantum Singular Value Transformation (QSVT) are designed for comparison through numerical experiments, highlighting the superiority of the proposed method in terms of the scale of the solution and training complexity while incorporating the real-world constraints.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Discrepancy-aware Detector for Image Forgery Identification</title>
<link>https://arxiv.org/abs/2508.12341</link>
<guid>https://arxiv.org/abs/2508.12341</guid>
<content:encoded><![CDATA[
arXiv:2508.12341v2 Announce Type: replace-cross 
Abstract: With the rapid advancement of image generation techniques, robust forgery detection has become increasingly imperative to ensure the trustworthiness of digital media. Recent research indicates that the learned semantic concepts of pre-trained models are critical for identifying fake images. However, the misalignment between the forgery and semantic concept spaces hinders the model's forgery detection performance. To address this problem, we propose a novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction learning to align the two spaces at a fine-grained visual level. By exploiting the conceptual knowledge embedded in the pre-trained vision language model, we specifically design a semantic token sampling module to mitigate the space shifts caused by features irrelevant to both forgery traces and semantic concepts. A concept-level forgery discrepancy learning module, built upon a visual reconstruction paradigm, is proposed to strengthen the interaction between visual semantic concepts and forgery traces, effectively capturing discrepancies under the concepts' guidance. Finally, the low-level forgery feature enhancemer integrates the learned concept level forgery discrepancies to minimize redundant forgery information. Experiments conducted on two standard image forgery datasets demonstrate the efficacy of the proposed SDD, which achieves superior results compared to existing methods. The code is available at https://github.com/wzy1111111/SSD.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Artificial Intuition in Distinct, Minimalist Classification of Scientific Abstracts for Management of Technology Portfolios</title>
<link>https://arxiv.org/abs/2508.13182</link>
<guid>https://arxiv.org/abs/2508.13182</guid>
<content:encoded><![CDATA[
arXiv:2508.13182v2 Announce Type: replace-cross 
Abstract: Classification of scientific abstracts is useful for strategic activities but challenging to automate because the sparse text provides few contextual clues. Metadata associated with the scientific publication can be used to improve performance but still often requires a semi-supervised setting. Moreover, such schemes may generate labels that lack distinction -- namely, they overlap and thus do not uniquely define the abstract. In contrast, experts label and sort these texts with ease. Here we describe an application of a process we call artificial intuition to replicate the expert's approach, using a Large Language Model (LLM) to generate metadata. We use publicly available abstracts from the United States National Science Foundation to create a set of labels, and then we test this on a set of abstracts from the Chinese National Natural Science Foundation to examine funding trends. We demonstrate the feasibility of this method for research portfolio management, technology scouting, and other strategic activities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering</title>
<link>https://arxiv.org/abs/2508.14052</link>
<guid>https://arxiv.org/abs/2508.14052</guid>
<content:encoded><![CDATA[
arXiv:2508.14052v3 Announce Type: replace-cross 
Abstract: Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods-whether sparse or dense-often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance -- a setting we term agentic retrieval. The benchmark consists of 3,429 expert-annotated examples on S&amp;P-100 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Generalization and Personalization in Human Activity Recognition via On-Device Few-Shot Learning</title>
<link>https://arxiv.org/abs/2508.15413</link>
<guid>https://arxiv.org/abs/2508.15413</guid>
<content:encoded><![CDATA[
arXiv:2508.15413v3 Announce Type: replace-cross 
Abstract: Human Activity Recognition (HAR) with different sensing modalities requires both strong generalization across diverse users and efficient personalization for individuals. However, conventional HAR models often fail to generalize when faced with user-specific variations, leading to degraded performance. To address this challenge, we propose a novel on-device few-shot learning framework that bridges generalization and personalization in HAR. Our method first trains a generalizable representation across users and then rapidly adapts to new users with only a few labeled samples, updating lightweight classifier layers directly on resource-constrained devices. This approach achieves robust on-device learning with minimal computation and memory cost, making it practical for real-world deployment. We implement our framework on the energy-efficient RISC-V GAP9 microcontroller and evaluate it on three benchmark datasets (RecGym, QVAR-Gesture, Ultrasound-Gesture). Across these scenarios, post-deployment adaptation improves accuracy by 3.73\%, 17.38\%, and 3.70\%, respectively. These results demonstrate that few-shot on-device learning enables scalable, user-aware, and energy-efficient wearable human activity recognition by seamlessly uniting generalization and personalization. The related framework is open sourced for further research\footnote{https://github.com/kangpx/onlineTiny2023}.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning</title>
<link>https://arxiv.org/abs/2508.15868</link>
<guid>https://arxiv.org/abs/2508.15868</guid>
<content:encoded><![CDATA[
arXiv:2508.15868v2 Announce Type: replace-cross 
Abstract: Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \TheName{} in terms of robustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Code is available at https://github.com/WNQzhu/CARFT.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search</title>
<link>https://arxiv.org/abs/2508.15884</link>
<guid>https://arxiv.org/abs/2508.15884</guid>
<content:encoded><![CDATA[
arXiv:2508.15884v2 Announce Type: replace-cross 
Abstract: We present Jet-Nemotron, a new family of hybrid-architecture language models, which matches or exceeds the accuracy of leading full-attention models while significantly improving generation throughput. Jet-Nemotron is developed using Post Neural Architecture Search (PostNAS), a novel neural architecture exploration pipeline that enables efficient model design. Unlike prior approaches, PostNAS begins with a pre-trained full-attention model and freezes its MLP weights, allowing efficient exploration of attention block designs. The pipeline includes four key components: (1) learning optimal full-attention layer placement and elimination, (2) linear attention block selection, (3) designing new attention blocks, and (4) performing hardware-aware hyperparameter search. Our Jet-Nemotron-2B model achieves comparable or superior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a comprehensive suite of benchmarks while delivering up to 53.6x generation throughput speedup and 6.1x prefilling speedup. It also achieves higher accuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models, such as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B total and 2.2B activated parameters.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence and Generalization of Anti-Regularization for Parametric Models</title>
<link>https://arxiv.org/abs/2508.17412</link>
<guid>https://arxiv.org/abs/2508.17412</guid>
<content:encoded><![CDATA[
arXiv:2508.17412v2 Announce Type: replace-cross 
Abstract: Anti-regularization introduces a reward term with a reversed sign into the loss function, deliberately amplifying model expressivity in small-sample regimes while ensuring that the intervention gradually vanishes as the sample size grows through a power-law decay schedule. We formalize spectral safety conditions and trust-region constraints, and we design a lightweight safeguard that combines a projection operator with gradient clipping to guarantee stable intervention. Theoretical analysis extends to linear smoothers and the Neural Tangent Kernel regime, providing practical guidance on the choice of decay exponents through the balance between empirical risk and variance. Empirical results show that Anti-regularization mitigates underfitting in both regression and classification while preserving generalization and improving calibration. Ablation studies confirm that the decay schedule and safeguards are essential to avoiding overfitting and instability. As an alternative, we also propose a degrees-of-freedom targeting schedule that maintains constant per-sample complexity. Anti-regularization constitutes a simple and reproducible procedure that integrates seamlessly into standard empirical risk minimization pipelines, enabling robust learning under limited data and resource constraints by intervening only when necessary and vanishing otherwise.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Expectation Policy Optimization for Heterogeneous Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.17850</link>
<guid>https://arxiv.org/abs/2508.17850</guid>
<content:encoded><![CDATA[
arXiv:2508.17850v2 Announce Type: replace-cross 
Abstract: As single-center computing approaches power constraints, decentralized training is becoming essential. Reinforcement Learning (RL) post-training enhances Large Language Models (LLMs) but faces challenges in heterogeneous distributed environments due to its tightly-coupled sampling-learning alternation. We propose HeteroRL, an asynchronous RL architecture that decouples rollout sampling from parameter learning, enabling robust deployment across geographically distributed nodes under network delays. We identify that latency-induced KL divergence causes importance sampling failure due to high variance. To address this, we propose Group Expectation Policy Optimization (GEPO), which reduces importance weight variance through a refined sampling mechanism. Theoretically, GEPO achieves exponential variance reduction. Experiments show it maintains superior stability over methods like GRPO, with less than 3% performance degradation under 1800-second delays, demonstrating strong potential for decentralized RL in heterogeneous networks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios</title>
<link>https://arxiv.org/abs/2508.18183</link>
<guid>https://arxiv.org/abs/2508.18183</guid>
<content:encoded><![CDATA[
arXiv:2508.18183v2 Announce Type: replace-cross 
Abstract: Translating natural languages into sign languages is a highly complex and underexplored task. Despite growing interest in accessibility and inclusivity, the development of robust translation systems remains hindered by the limited availability of parallel corpora which align natural language with sign language data. Existing methods often struggle to generalize in these data-scarce environments, as the few datasets available are typically domain-specific, lack standardization, or fail to capture the full linguistic richness of sign languages. To address this limitation, we propose Advanced Use of LLMs for Sign Language Translation (AulSign), a novel method that leverages Large Language Models via dynamic prompting and in-context learning with sample selection and subsequent sign association. Despite their impressive abilities in processing text, LLMs lack intrinsic knowledge of sign languages; therefore, they are unable to natively perform this kind of translation. To overcome this limitation, we associate the signs with compact descriptions in natural language and instruct the model to use them. We evaluate our method on both English and Italian languages using SignBank+, a recognized benchmark in the field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior performance compared to state-of-the-art models in low-data scenario. Our findings demonstrate the effectiveness of AulSign, with the potential to enhance accessibility and inclusivity in communication technologies for underrepresented linguistic communities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attacks on LLM-based Recommender Systems</title>
<link>https://arxiv.org/abs/2508.18665</link>
<guid>https://arxiv.org/abs/2508.18665</guid>
<content:encoded><![CDATA[
arXiv:2508.18665v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly adapt recommendation systems to different domains. It utilizes in-context learning (ICL), i.e., the prompts, to customize the recommendation functions, which include sensitive historical user-specific item interactions, e.g., implicit feedback like clicked items or explicit product reviews. Such private information may be exposed to novel privacy attack. However, no study has been done on this important issue. We design four membership inference attacks (MIAs), aiming to reveal whether victims' historical interactions have been used by system prompts. They are \emph{direct inquiry, hallucination, similarity, and poisoning attacks}, each of which utilizes the unique features of LLMs or RecSys. We have carefully evaluated them on three LLMs that have been used to develop ICL-LLM RecSys and two well-known RecSys benchmark datasets. The results confirm that the MIA threat on LLM RecSys is realistic: direct inquiry and poisoning attacks showing significantly high attack advantages. We have also analyzed the factors affecting these attacks, such as the number of shots in system prompts and the position of the victim in the shots.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Prompt Optimization with Prompt Distillation</title>
<link>https://arxiv.org/abs/2508.18992</link>
<guid>https://arxiv.org/abs/2508.18992</guid>
<content:encoded><![CDATA[
arXiv:2508.18992v2 Announce Type: replace-cross 
Abstract: Autoprompting is the process of automatically selecting optimized prompts for language models, which is gaining popularity due to the rapid development of prompt engineering driven by extensive research in the field of large language models (LLMs). This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data. DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly. The method was tested on different datasets for text classification and generation tasks using the t-lite-instruct-0.1 language model. The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field, establishing DistillPrompt as one of the most effective non-gradient approaches in autoprompting.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MovieCORE: COgnitive REasoning in Movies</title>
<link>https://arxiv.org/abs/2508.19026</link>
<guid>https://arxiv.org/abs/2508.19026</guid>
<content:encoded><![CDATA[
arXiv:2508.19026v2 Announce Type: replace-cross 
Abstract: This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.19153</link>
<guid>https://arxiv.org/abs/2508.19153</guid>
<content:encoded><![CDATA[
arXiv:2508.19153v2 Announce Type: replace-cross 
Abstract: We address vision-guided quadruped motion control with reinforcement learning (RL) and highlight the necessity of combining proprioception with vision for robust control. We propose QuadKAN, a spline-parameterized cross-modal policy instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates a spline encoder for proprioception and a spline fusion head for proprioception-vision inputs. This structured function class aligns the state-to-action mapping with the piecewise-smooth nature of gait, improving sample efficiency, reducing action jitter and energy consumption, and providing interpretable posture-action sensitivities. We adopt Multi-Modal Delay Randomization (MMDR) and perform end-to-end training with Proximal Policy Optimization (PPO). Evaluations across diverse terrains, including both even and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate that QuadKAN achieves consistently higher returns, greater distances, and fewer collisions than state-of-the-art (SOTA) baselines. These results show that spline-parameterized policies offer a simple, effective, and interpretable alternative for robust vision-guided locomotion. A repository will be made available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2508.19268</link>
<guid>https://arxiv.org/abs/2508.19268</guid>
<content:encoded><![CDATA[
arXiv:2508.19268v2 Announce Type: replace-cross 
Abstract: Despite LLMs' excellent code creation capabilities, multilingual code generation remains extremely challenging. To address this, we intent to improve the multi-programming-lingual (MultiPL) performance of the base LLMs while retaining the most popular ones using restricted computational resources. We consider MultiPL to be a special case of multiple natural languages and propose a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize expert selection at both the token and segment levels. The token-level MoE is a standard upcycling MoE structure with a shared expert and a novel gate weight normalization approach that aids in the final fusion with the segment-level MoE. The segment-level MoE incorporates two innovative designs to better capture the syntactic structure and contextual patterns of programming languages: First, using a sliding window to partition the input token sequence into multiple segments; Then, adopting an expert-choice routing strategy that allows experts to select the top-k segments. The results of the experiment proved the effectiveness of MultiPL-MoE.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities</title>
<link>https://arxiv.org/abs/2508.19597</link>
<guid>https://arxiv.org/abs/2508.19597</guid>
<content:encoded><![CDATA[
arXiv:2508.19597v2 Announce Type: replace-cross 
Abstract: Artificial intelligence underpins most smart city services, yet deep neural network (DNN) that forecasts vehicle motion still struggle with catastrophic forgetting, the loss of earlier knowledge when models are updated. Conventional fixes enlarge the training set or replay past data, but these strategies incur high data collection costs, sample inefficiently and fail to balance long- and short-term experience, leaving them short of human-like continual learning. Here we introduce Dual-LS, a task-free, online continual learning paradigm for DNN-based motion forecasting that is inspired by the complementary learning system of the human brain. Dual-LS pairs two synergistic memory rehearsal replay mechanisms to accelerate experience retrieval while dynamically coordinating long-term and short-term knowledge representations. Tests on naturalistic data spanning three countries, over 772,000 vehicles and cumulative testing mileage of 11,187 km show that Dual-LS mitigates catastrophic forgetting by up to 74.31\% and reduces computational resource demand by up to 94.02\%, markedly boosting predictive stability in vehicle motion forecasting without inflating data requirements. Meanwhile, it endows DNN-based vehicle motion forecasting with computation efficient and human-like continual learning adaptability fit for smart cities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices</title>
<link>https://arxiv.org/abs/2508.20144</link>
<guid>https://arxiv.org/abs/2508.20144</guid>
<content:encoded><![CDATA[
arXiv:2508.20144v2 Announce Type: replace-cross 
Abstract: As deep learning (DL) technologies advance, their application in automated visual inspection for Class III medical devices offers significant potential to enhance quality assurance and reduce human error. However, the adoption of such AI-based systems introduces new regulatory complexities-particularly under the EU Artificial Intelligence (AI) Act, which imposes high-risk system obligations that differ in scope and depth from established regulatory frameworks such as the Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation (QSR). This paper presents a high-level technical assessment of the foreseeable challenges that manufacturers are likely to encounter when qualifying DL-based automated inspections -- specifically static models -- within the existing medical device compliance landscape. It examines divergences in risk management principles, dataset governance, model validation, explainability requirements, and post-deployment monitoring obligations. The discussion also explores potential implementation strategies and highlights areas of uncertainty, including data retention burdens, global compliance implications, and the practical difficulties of achieving statistical significance in validation with limited defect data. Disclaimer: This paper presents a technical perspective and does not constitute legal or regulatory advice.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management</title>
<link>https://arxiv.org/abs/2509.04505</link>
<guid>https://arxiv.org/abs/2509.04505</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Construction Project Management, Ethical Decision-making, Large Language Models, Human-in-the-loop oversight

Summary:
The study evaluates the ethical viability and reliability of Large Language Models (LLMs) in Construction Project Management (CPM). Through quantitative performance testing and qualitative interviews with industry experts, the study finds that while LLMs perform well in structured domains like legal compliance, they struggle with contextual nuance, accountability, and transparent reasoning. Stakeholders express reservations about using AI autonomously for ethical judgments and advocate for human-in-the-loop oversight. The study introduces the Ethical Decision Support Assessment Checklist (EDSAC) as a framework for evaluating LLMs in ethical decision-making contexts and recommends that LLMs be used as decision-support aids rather than autonomous ethical agents.

<br /><br />Summary: The study evaluates the ethical viability and reliability of Large Language Models in Construction Project Management. LLMs demonstrate adequate performance in legal compliance but struggle with contextual nuance, accountability, and transparent reasoning. Stakeholders advocate for human-in-the-loop oversight and recommend using LLMs as decision-support aids rather than autonomous ethical agents. <div>
arXiv:2509.04505v1 Announce Type: new 
Abstract: The integration of Artificial Intelligence (AI) into construction project management (CPM) is accelerating, with Large Language Models (LLMs) emerging as accessible decision-support tools. This study aims to critically evaluate the ethical viability and reliability of LLMs when applied to the ethically sensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods research design was employed, involving the quantitative performance testing of two leading LLMs against twelve real-world ethical scenarios using a novel Ethical Decision Support Assessment Checklist (EDSAC), and qualitative analysis of semi-structured interviews with 12 industry experts to capture professional perceptions. The findings reveal that while LLMs demonstrate adequate performance in structured domains such as legal compliance, they exhibit significant deficiencies in handling contextual nuance, ensuring accountability, and providing transparent reasoning. Stakeholders expressed considerable reservations regarding the autonomous use of AI for ethical judgments, strongly advocating for robust human-in-the-loop oversight. To our knowledge, this is one of the first studies to empirically test the ethical reasoning of LLMs within the construction domain. It introduces the EDSAC framework as a replicable methodology and provides actionable recommendations, emphasising that LLMs are currently best positioned as decision-support aids rather than autonomous ethical agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maestro: Joint Graph &amp; Config Optimization for Reliable AI Agents</title>
<link>https://arxiv.org/abs/2509.04642</link>
<guid>https://arxiv.org/abs/2509.04642</guid>
<content:encoded><![CDATA[
<div> optimizer, LLM agents, Maestro, graph, configuration

Summary:<br />
1. The article introduces Maestro, a comprehensive optimizer for LLM agents that considers both graph structure and configuration of nodes.
2. Maestro optimizes agent quality while adhering to rollout/token budgets by jointly searching over graphs and configurations.
3. The optimizer utilizes textual feedback from traces to prioritize edits, enhancing sample efficiency and targeting specific failure modes.
4. On benchmark tests, Maestro outperforms leading prompt optimizers such as MIPROv2, GEPA, and GEPA+Merge by significant margins.
5. Maestro achieves superior results with fewer rollouts compared to GEPA, demonstrating its efficiency in optimizing LLM agents.
6. The joint graph and configuration search by Maestro addresses structural failure modes that prompt tuning alone cannot fix.
7. Applications such as interviewer and RAG agents show significant improvements with Maestro's approach. 

<br /><br />Summary: <div>
arXiv:2509.04642v1 Announce Type: new 
Abstract: Building reliable LLM agents requires decisions at two levels: the graph (which modules exist and how information flows) and the configuration of each node (models, prompts, tools, control knobs). Most existing optimizers tune configurations while holding the graph fixed, leaving structural failure modes unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for LLM agents that jointly searches over graphs and configurations to maximize agent quality, subject to explicit rollout/token budgets. Beyond numeric metrics, Maestro leverages reflective textual feedback from traces to prioritize edits, improving sample efficiency and targeting specific failure modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%, 4.9%, and 4.86%, respectively; even when restricted to prompt-only optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these results with far fewer rollouts than GEPA. We further show large gains on two applications (interviewer & RAG agents), highlighting that joint graph & configuration search addresses structural failure modes that prompt tuning alone cannot fix.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization</title>
<link>https://arxiv.org/abs/2509.04646</link>
<guid>https://arxiv.org/abs/2509.04646</guid>
<content:encoded><![CDATA[
<div> Keywords: Modeling & Simulation, Agent-based models, Large Language Models, Stakeholder needs, Tailored explanations<br />
Summary: 
This article discusses the potential of Modeling & Simulation (M&amp;S) approaches, such as agent-based models, in supporting decision-making in the health sector, including areas like vaccine adoption and healthy behavior promotion. However, the complexity of these models often makes them inaccessible to key stakeholders. To address this issue, the authors propose a framework to tailor explanations of health simulations using Large Language Models (LLMs). By first understanding stakeholder needs and preferences, optimizing LLMs to generate customized explanations, and evaluating the outputs, the framework aims to bridge the gap between complex health models and practical usability. The study highlights the importance of personalized communication in translating simulation outputs for various stakeholders, including clinicians, policymakers, patients, caregivers, and health advocates. By enhancing the clarity and relevance of explanations, this approach can enhance the impact and usability of health simulations in decision-making processes. <br /><br />Summary: <div>
arXiv:2509.04646v1 Announce Type: new 
Abstract: Modeling & Simulation (M&amp;S) approaches such as agent-based models hold significant potential to support decision-making activities in health, with recent examples including the adoption of vaccines, and a vast literature on healthy eating behaviors and physical activity behaviors. These models are potentially usable by different stakeholder groups, as they support policy-makers to estimate the consequences of potential interventions and they can guide individuals in making healthy choices in complex environments. However, this potential may not be fully realized because of the models' complexity, which makes them inaccessible to the stakeholders who could benefit the most. While Large Language Models (LLMs) can translate simulation outputs and the design of models into text, current approaches typically rely on one-size-fits-all summaries that fail to reflect the varied informational needs and stylistic preferences of clinicians, policymakers, patients, caregivers, and health advocates. This limitation stems from a fundamental gap: we lack a systematic understanding of what these stakeholders need from explanations and how to tailor them accordingly. To address this gap, we present a step-by-step framework to identify stakeholder needs and guide LLMs in generating tailored explanations of health simulations. Our procedure uses a mixed-methods design by first eliciting the explanation needs and stylistic preferences of diverse health stakeholders, then optimizing the ability of LLMs to generate tailored outputs (e.g., via controllable attribute tuning), and then evaluating through a comprehensive range of metrics to further improve the tailored generation of summaries.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Approach to Grounding AI Model Evaluations in Human-derived Criteria</title>
<link>https://arxiv.org/abs/2509.04676</link>
<guid>https://arxiv.org/abs/2509.04676</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, benchmarks, human-derived evaluation criteria, interpretability, cognitive skills

Summary: 
Artificial intelligence (AI) is rapidly evolving, but traditional benchmarks may not fully capture its capabilities. This study focuses on enhancing benchmarks in physical world modeling by incorporating human-derived evaluation criteria. Factors such as Prioritization, Memorizing, Discerning, and Contextualizing were identified as crucial cognitive skills for AI and human reasoning. Participants believe AI lacks interpretive and empathetic abilities but expect high performance. By integrating these insights into benchmark design, a more human-aligned approach to defining and measuring progress in AI development is proposed. User-centered evaluation is emphasized, providing guidelines for aligning AI capabilities with human cognitive processes. This approach aims to improve current benchmarking practices and pave the way for future advancements in AI model evaluation. <div>
arXiv:2509.04676v1 Announce Type: new 
Abstract: In the rapidly evolving field of artificial intelligence (AI), traditional benchmarks can fall short in attempting to capture the nuanced capabilities of AI models. We focus on the case of physical world modeling and propose a novel approach to augment existing benchmarks with human-derived evaluation criteria, aiming to enhance the interpretability and applicability of model behaviors. Grounding our study in the Perception Test and OpenEQA benchmarks, we conducted in-depth interviews and large-scale surveys to identify key cognitive skills, such as Prioritization, Memorizing, Discerning, and Contextualizing, that are critical for both AI and human reasoning. Our findings reveal that participants perceive AI as lacking in interpretive and empathetic skills yet hold high expectations for AI performance. By integrating insights from our findings into benchmark design, we offer a framework for developing more human-aligned means of defining and measuring progress. This work underscores the importance of user-centered evaluation in AI development, providing actionable guidelines for researchers and practitioners aiming to align AI capabilities with human cognitive processes. Our approach both enhances current benchmarking practices and sets the stage for future advancements in AI model evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning</title>
<link>https://arxiv.org/abs/2509.04731</link>
<guid>https://arxiv.org/abs/2509.04731</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, Agent models, World models, hierarchical scaffolding, multi-agent reinforcement learning

Summary: 
The convergence of Language, Agent, and World models is crucial for advancing artificial intelligence. Existing models focus on scaling Language and Agent models, but the development of sophisticated World Models remains a challenge, especially for complex tasks. A significant hurdle is the lack of explicit hierarchical World Models in environments like robotic soccer. The solution proposed is to create environments with hierarchical scaffolding, decomposing complex goals into manageable subgoals. Research in multi-agent soccer shows a shift towards integrating symbolic and hierarchical methods with multi-agent reinforcement learning. A paradigm shift is suggested - using Large Language Models to dynamically create a hierarchical scaffold, structuring the World Model with language in real-time. This language-driven approach provides a curriculum, meaningful learning signals, and supports compositional learning, enabling agents to learn strategic behaviors efficiently. By incorporating explicit, language-configurable task layers in environments, it becomes possible to bridge the gap between low-level reactive behaviors and high-level strategic team play, offering a generalizable framework to train intelligent agents. 

<br /><br />Summary: <div>
arXiv:2509.04731v1 Announce Type: new 
Abstract: The convergence of Language models, Agent models, and World models represents a critical frontier for artificial intelligence. While recent progress has focused on scaling Language and Agent models, the development of sophisticated, explicit World Models remains a key bottleneck, particularly for complex, long-horizon multi-agent tasks. In domains such as robotic soccer, agents trained via standard reinforcement learning in high-fidelity but structurally-flat simulators often fail due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing capable agents lies in creating environments that possess an explicit, hierarchical World Model. We contend that this is best achieved through hierarchical scaffolding, where complex goals are decomposed into structured, manageable subgoals. Drawing evidence from a systematic review of 2024 research in multi-agent soccer, we identify a clear and decisive trend towards integrating symbolic and hierarchical methods with multi-agent reinforcement learning (MARL). These approaches implicitly or explicitly construct a task-based world model to guide agent learning. We then propose a paradigm shift: leveraging Large Language Models to dynamically generate this hierarchical scaffold, effectively using language to structure the World Model on the fly. This language-driven world model provides an intrinsic curriculum, dense and meaningful learning signals, and a framework for compositional learning, enabling Agent Models to acquire sophisticated, strategic behaviors with far greater sample efficiency. By building environments with explicit, language-configurable task layers, we can bridge the gap between low-level reactive behaviors and high-level strategic team play, creating a powerful and generalizable framework for training the next generation of intelligent agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking</title>
<link>https://arxiv.org/abs/2509.04791</link>
<guid>https://arxiv.org/abs/2509.04791</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, What-If Analysis, proactive thinking, reinforcement learning, strategic applications

Summary: 
Large language models (LLMs) are highly skilled at reactive information processing but lack the ability to explore hypothetical futures systematically. The proposed WiA-LLM paradigm integrates What-If Analysis (WIA) to equip LLMs with proactive thinking capabilities. By leveraging reinforcement learning to analyze potential scenarios, WiA-LLM can anticipate future outcomes rather than just reacting to present conditions. Validation in the Honor of Kings (HoK) game environment demonstrates WiA-LLM achieving a 74.2% accuracy in forecasting game-state changes, with significant gains in high-difficulty scenarios where accurate foresight is crucial. This work represents a fundamental step towards proactive reasoning in LLMs, offering a scalable framework for robust decision-making in dynamic environments, with broad implications for strategic applications. 

<br /><br />Summary: <div>
arXiv:2509.04791v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at processing information reactively but lack the ability to systemically explore hypothetical futures. They cannot ask, "what if we take this action? how will it affect the final outcome" and forecast its potential consequences before acting. This critical gap limits their utility in dynamic, high-stakes scenarios like strategic planning, risk assessment, and real-time decision making. To bridge this gap, we propose WiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities. Our approach integrates What-If Analysis (WIA), a systematic approach for evaluating hypothetical scenarios by changing input variables. By leveraging environmental feedback via reinforcement learning, WiA-LLM moves beyond reactive thinking. It dynamically simulates the outcomes of each potential action, enabling the model to anticipate future states rather than merely react to the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a complex multiplayer game environment characterized by rapid state changes and intricate interactions. The game's real-time state changes require precise multi-step consequence prediction, making it an ideal testbed for our approach. Experimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy in forecasting game-state changes (up to two times gain over baselines). The model shows particularly significant gains in high-difficulty scenarios where accurate foresight is critical. To our knowledge, this is the first work to formally explore and integrate what-if analysis capabilities within LLMs. WiA-LLM represents a fundamental advance toward proactive reasoning in LLMs, providing a scalable framework for robust decision-making in dynamic environments with broad implications for strategic applications.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models</title>
<link>https://arxiv.org/abs/2509.04809</link>
<guid>https://arxiv.org/abs/2509.04809</guid>
<content:encoded><![CDATA[
<div> Explainable Reinforcement Learning, XRL, TalkToAgent, Large Language Models, interactive explanations
Summary:
TalkToAgent introduces a multi-agent Large Language Models framework for Explainable Reinforcement Learning. The architecture consists of Coordinator, Explainer, Coder, Evaluator, and Debugger agents. It provides natural language explanations for RL policies, mapping user queries to relevant XRL tools, and clarifying agent actions in terms of key state variables, expected outcomes, or counterfactual explanations. The framework extends counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions or new rule-based policies. Validation on a quadruple-tank process control problem showed accurate mapping of user queries to XRL tasks, successful coder-debugger interactions for counterfactual generation, and effective interpretation of agent actions in the problem domain.<br /><br />Summary: <div>
arXiv:2509.04809v1 Announce Type: new 
Abstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agent's actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agent's actions and contextualized their meaning within the problem domain.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory</title>
<link>https://arxiv.org/abs/2509.04847</link>
<guid>https://arxiv.org/abs/2509.04847</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, cooperative behavior, multi-party settings, prisoner's dilemma, Axelrod-style tournament
Summary: 
Language models are being increasingly used in various interactive online settings, prompting the need to analyze their behavior in multi-party scenarios. This study focuses on language model behavior in the iterated prisoner's dilemma game, a classic framework for studying cooperation and conflict over time. Results show that language models can perform as well as, or better than, traditional strategies in this setting, demonstrating characteristics of strong cooperative strategies such as niceness, provocability, and generosity. The models also display quick adaptability to changes in opponent strategy, rivaling or exceeding human adaptability. This research provides valuable insights into the long-term cooperative behaviors of language model agents and lays the groundwork for further exploration of their role in complex social environments involving both humans and AI.


<br /><br />Summary: <div>
arXiv:2509.04847v1 Announce Type: new 
Abstract: Language models are increasingly deployed in interactive online environments, from personal chat assistants to domain-specific agents, raising questions about their cooperative and competitive behavior in multi-party settings. While prior work has examined language model decision-making in isolated or short-term game-theoretic contexts, these studies often neglect long-horizon interactions, human-model collaboration, and the evolution of behavioral patterns over time. In this paper, we investigate the dynamics of language model behavior in the iterated prisoner's dilemma (IPD), a classical framework for studying cooperation and conflict. We pit model-based agents against a suite of 240 well-established classical strategies in an Axelrod-style tournament and find that language models achieve performance on par with, and in some cases exceeding, the best-known classical strategies. Behavioral analysis reveals that language models exhibit key properties associated with strong cooperative strategies - niceness, provocability, and generosity while also demonstrating rapid adaptability to changes in opponent strategy mid-game. In controlled "strategy switch" experiments, language models detect and respond to shifts within only a few rounds, rivaling or surpassing human adaptability. These results provide the first systematic characterization of long-term cooperative behaviors in language model agents, offering a foundation for future research into their role in more complex, mixed human-AI social environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales</title>
<link>https://arxiv.org/abs/2509.04871</link>
<guid>https://arxiv.org/abs/2509.04871</guid>
<content:encoded><![CDATA[
<div> voice assistant, dialogue modelling, call recordings, telesales data, AI agent

Summary:
Recent advances in language and speech modelling have enabled the development of voice assistants that can understand and generate human dialogue in real time. This paper presents a methodology for creating a conversational voice AI agent from call recordings, specifically focusing on telesales data. The system listens to customers over the phone, responds with a synthetic voice, and follows a playbook learned from top human agents. Evaluation against human agents showed that the AI agent performed well in routine aspects of calls but struggled in persuasion and objection handling. The paper discusses the shortcomings and proposes refinements to improve performance. Future research directions include large-scale simulation and automated evaluation to further enhance the cloned agent's capabilities. 

<br /><br />Summary: <div>
arXiv:2509.04871v1 Announce Type: new 
Abstract: Recent advances in language and speech modelling have made it possible to build autonomous voice assistants that understand and generate human dialogue in real time. These systems are increasingly being deployed in domains such as customer service and healthcare care, where they can automate repetitive tasks, reduce operational costs, and provide constant support around the clock. In this paper, we present a general methodology for cloning a conversational voice AI agent from a corpus of call recordings. Although the case study described in this paper uses telesales data to illustrate the approach, the underlying process generalizes to any domain where call transcripts are available. Our system listens to customers over the telephone, responds with a synthetic voice, and follows a structured playbook learned from top performing human agents. We describe the domain selection, knowledge extraction, and prompt engineering used to construct the agent, integrating automatic speech recognition, a large language model based dialogue manager, and text to speech synthesis into a streaming inference pipeline. The cloned agent is evaluated against human agents on a rubric of 22 criteria covering introduction, product communication, sales drive, objection handling, and closing. Blind tests show that the AI agent approaches human performance in routine aspects of the call while underperforming in persuasion and objection handling. We analyze these shortcomings and refine the prompt accordingly. The paper concludes with design lessons and avenues for future research, including large scale simulation and automated evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration</title>
<link>https://arxiv.org/abs/2509.04876</link>
<guid>https://arxiv.org/abs/2509.04876</guid>
<content:encoded><![CDATA[
<div> Keywords: OSC, cognitive synergy, multi-agent systems, collaboration framework, language models

Summary: 
The paper introduces OSC (Orchestrating Cognitive Synergy), a framework designed to enhance cognitive synergy in multi-agent systems with large language models. OSC introduces Collaborator Knowledge Models (CKM) to enable agents to perceive their collaborators' cognitive states and adapt communication behaviors in real-time. By analyzing cognitive gaps and adjusting communication strategies, OSC improves task performance and communication efficiency in complex reasoning and problem-solving tasks. The framework transforms individual agents into a deeply collaborative cognitive team, optimizing multi-agent collaboration. Additionally, OSC offers new insights into language model agent interaction behaviors. <div>
arXiv:2509.04876v1 Announce Type: new 
Abstract: This paper introduces OSC (Orchestrating Cognitive Synergy), a knowledge-aware adaptive collaboration framework designed to enhance cognitive synergy in multi-agent systems with large language models. While prior work has advanced agent selection and result aggregation, efficient linguistic interactions for deep collaboration among expert agents remain a critical bottleneck. OSC addresses this gap as a pivotal intermediate layer between selection and aggregation, introducing Collaborator Knowledge Models (CKM) to enable each agent to dynamically perceive its collaborators' cognitive states. Through real-time cognitive gap analysis, agents adaptively adjust communication behaviors, including content focus, detail level, and expression style, using learned strategies. Experiments on complex reasoning and problem-solving benchmarks demonstrate that OSC significantly improves task performance and communication efficiency, transforming "parallel-working individuals'' into a "deeply collaborative cognitive team.'' This framework not only optimizes multi-agent collaboration but also offers new insights into LLM agent interaction behaviors.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing</title>
<link>https://arxiv.org/abs/2509.04908</link>
<guid>https://arxiv.org/abs/2509.04908</guid>
<content:encoded><![CDATA[
<div> continuous modeling, Multimodal Large Language Model, GUI perception, SparkUI-Parser, ScreenParse

Summary:
SparkUI-Parser is a novel framework that addresses challenges in Multimodal Large Language Models (MLLMs) for GUI perception. It improves localization precision and parsing capability by implementing continuous modeling of coordinates. This approach enhances accuracy and inference speed compared to discrete modeling methods. The framework includes a rejection mechanism based on a modified Hungarian matching algorithm to identify and reject non-existent elements, reducing false positives. Additionally, ScreenParse, a benchmark for assessing GUI model capabilities, is introduced to evaluate performance across diverse scenarios. Extensive experiments show that SparkUI-Parser outperforms state-of-the-art methods on various benchmarks. The resources for SparkUI-Parser are available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2509.04908v1 Announce Type: new 
Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have made great progress. However, the following challenges still exist in prior methods: 1) They model discrete coordinates based on text autoregressive mechanism, which results in lower grounding accuracy and slower inference speed. 2) They can only locate predefined sets of elements and are not capable of parsing the entire interface, which hampers the broad application and support for downstream tasks. To address the above issues, we propose SparkUI-Parser, a novel end-to-end framework where higher localization precision and fine-grained parsing capability of the entire interface are simultaneously achieved. Specifically, instead of using probability-based discrete modeling, we perform continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with an additional token router and coordinate decoder. This effectively mitigates the limitations inherent in the discrete output characteristics and the token-by-token generation process of MLLMs, consequently boosting both the accuracy and the inference speed. To further enhance robustness, a rejection mechanism based on a modified Hungarian matching algorithm is introduced, which empowers the model to identify and reject non-existent elements, thereby reducing false positives. Moreover, we present ScreenParse, a rigorously constructed benchmark to systematically assess structural perception capabilities of GUI models across diverse scenarios. Extensive experiments demonstrate that our approach consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks. The resources are available at https://github.com/antgroup/SparkUI-Parser.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts</title>
<link>https://arxiv.org/abs/2509.04926</link>
<guid>https://arxiv.org/abs/2509.04926</guid>
<content:encoded><![CDATA[
<div> ontology, conversational agents, language models, controlled text generation, proficiency-level control

Summary:
- The article addresses the challenge of controlling Large Language Models (LLMs) for predictable and user-personalized responses in conversations.
- An ontology-based approach is proposed to formally define conversational features, typically qualitative, using linguistic descriptors to derive quantitative definitions.
- The focus is on proficiency-level control in conversations, with CEFR language proficiency levels as a case study.
- The qualitative definitions are formalized in description logic and integrated into an ontology to guide controlled text generation of an LLM through fine-tuning.
- Experimental results validate the approach, showing consistent and explainable proficiency-level definitions that enhance transparency in conversational AI.<br /><br />Summary: <div>
arXiv:2509.04926v1 Announce Type: new 
Abstract: The controllability of Large Language Models (LLMs) when used as conversational agents is a key challenge, particularly to ensure predictable and user-personalized responses. This work proposes an ontology-based approach to formally define conversational features that are typically qualitative in nature. By leveraging a set of linguistic descriptors, we derive quantitative definitions for qualitatively-defined concepts, enabling their integration into an ontology for reasoning and consistency checking. We apply this framework to the task of proficiency-level control in conversations, using CEFR language proficiency levels as a case study. These definitions are then formalized in description logic and incorporated into an ontology, which guides controlled text generation of an LLM through fine-tuning. Experimental results demonstrate that our approach provides consistent and explainable proficiency-level definitions, improving transparency in conversational AI.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents</title>
<link>https://arxiv.org/abs/2509.04979</link>
<guid>https://arxiv.org/abs/2509.04979</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, large language models, Web of Agents, Agent Ranking, DOVIS

Summary: 
AI agents powered by large language models are expected to transform the internet into a Web of Agents, where autonomous agents interact and collaborate at scale. To achieve this vision, Agent Ranking is essential, selecting agents based on their proven recent performance rather than just declared capabilities. A new operational protocol called DOVIS enables the collection of privacy-preserving aggregates of usage and performance data across the ecosystem. AgentRank-UC, an algorithm that combines usage and competence metrics, is introduced to rank agents dynamically and trust-awarely. The protocol shows promising results in simulation, with guarantees on convergence, robustness, and resistance to malicious actors. By implementing coordinated protocols and performance-aware ranking, a scalable and trustworthy Agentic Web can be realized. 

<br /><br />Summary: <div>
arXiv:2509.04979v1 Announce Type: new 
Abstract: AI agents -- powered by reasoning-capable large language models (LLMs) and integrated with tools, data, and web search -- are poised to transform the internet into a \emph{Web of Agents}: a machine-native ecosystem where autonomous agents interact, collaborate, and execute tasks at scale. Realizing this vision requires \emph{Agent Ranking} -- selecting agents not only by declared capabilities but by proven, recent performance. Unlike Web~1.0's PageRank, a global, transparent network of agent interactions does not exist; usage signals are fragmented and private, making ranking infeasible without coordination.
  We propose \textbf{DOVIS}, a five-layer operational protocol (\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that enables the collection of minimal, privacy-preserving aggregates of usage and performance across the ecosystem. On this substrate, we implement \textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines \emph{usage} (selection frequency) and \emph{competence} (outcome quality, cost, safety, latency) into a unified ranking. We present simulation results and theoretical guarantees on convergence, robustness, and Sybil resistance, demonstrating the viability of coordinated protocols and performance-aware ranking in enabling a scalable, trustworthy Agentic Web.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework</title>
<link>https://arxiv.org/abs/2509.05007</link>
<guid>https://arxiv.org/abs/2509.05007</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, test-time scaling, Sticker-TTS, historical experience utilization, mathematical reasoning benchmarks

Summary:
Large reasoning models (LRMs) have shown strong performance on complex reasoning tasks, but current test-time scaling methods lack efficiency by relying on redundant sampling. To address this, the Sticker-TTS framework utilizes historical experience to iteratively explore and refine solutions through collaboration between LRMs. Key conditions, called stickers, are used to extract and reuse critical information, enhancing computational efficiency. A two-stage optimization strategy combines imitation learning with self-improvement for progressive refinement. Evaluation on challenging mathematical reasoning benchmarks demonstrates that Sticker-TTS outperforms strong baselines and reinforcement learning approaches under comparable inference budgets. The effectiveness of sticker-guided historical experience utilization is highlighted in achieving superior performance. The code and data for Sticker-TTS are available on GitHub at https://github.com/RUCAIBox/Sticker-TTS.<br /><br />Summary: <div>
arXiv:2509.05007v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) have exhibited strong performance on complex reasoning tasks, with further gains achievable through increased computational budgets at inference. However, current test-time scaling methods predominantly rely on redundant sampling, ignoring the historical experience utilization, thereby limiting computational efficiency. To overcome this limitation, we propose Sticker-TTS, a novel test-time scaling framework that coordinates three collaborative LRMs to iteratively explore and refine solutions guided by historical attempts. At the core of our framework are distilled key conditions-termed stickers-which drive the extraction, refinement, and reuse of critical information across multiple rounds of reasoning. To further enhance the efficiency and performance of our framework, we introduce a two-stage optimization strategy that combines imitation learning with self-improvement, enabling progressive refinement. Extensive evaluations on three challenging mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH, demonstrate that Sticker-TTS consistently surpasses strong baselines, including self-consistency and advanced reinforcement learning approaches, under comparable inference budgets. These results highlight the effectiveness of sticker-guided historical experience utilization. Our code and data are available at https://github.com/RUCAIBox/Sticker-TTS.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding your MUSE: Mining Unexpected Solutions Engine</title>
<link>https://arxiv.org/abs/2509.05072</link>
<guid>https://arxiv.org/abs/2509.05072</guid>
<content:encoded><![CDATA[
<div> Methodology, Functional Concept Graphs, Abstraction, Problem reframing, Analogical inspiration

Summary:
The article discusses cognitive fixation among innovators and presents a methodology for constructing Functional Concept Graphs (FCGs) to support abstraction, problem reframing, and analogical inspiration. These FCGs are interconnected representations of functional elements that aid in overcoming limitations of existing solutions or ideas. The paper introduces MUSE, an algorithm that utilizes FCGs to generate creative inspirations for solving problems. By applying this methodology to analyze 500K patents, the study demonstrates the ability to create large-scale and high-quality FCGs with explicit abstraction relations. The FCGs constructed can facilitate the exploration of novel alternatives and enable innovators to break free from cognitive fixation, leading to more innovative solutions. The dataset containing the FCGs constructed from patents is released for further research. 

<br /><br />Summary: <div>
arXiv:2509.05072v1 Announce Type: new 
Abstract: Innovators often exhibit cognitive fixation on existing solutions or nascent ideas, hindering the exploration of novel alternatives. This paper introduces a methodology for constructing Functional Concept Graphs (FCGs), interconnected representations of functional elements that support abstraction, problem reframing, and analogical inspiration. Our approach yields large-scale, high-quality FCGs with explicit abstraction relations, overcoming limitations of prior work. We further present MUSE, an algorithm leveraging FCGs to generate creative inspirations for a given problem. We demonstrate our method by computing an FCG on 500K patents, which we release for further research.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback</title>
<link>https://arxiv.org/abs/2509.05091</link>
<guid>https://arxiv.org/abs/2509.05091</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, Theory of Mind, prosocial behavior, feedback, multi-agent systems <br />
Summary: <br />
- The paper addresses the challenge of promoting prosocial behavior in multi-agent systems using an AI system called ProToM. <br />
- ProToM uses Bayesian inverse planning to infer agents' goals and provides targeted feedback to encourage prosocial actions. <br />
- The approach is evaluated in two environments, demonstrating higher success rates, shorter task completion times, and preference by human users compared to baseline models. <br />
- Current large language and reasoning models struggle to provide contextually grounded and well-timed feedback, leading to higher communication overhead. <br />
- ProToM's ability to offer helpful feedback improves cooperation in multi-agent systems, showcasing the importance of understanding and promoting prosocial behavior in AI applications. <br /> <div>
arXiv:2509.05091v1 Announce Type: new 
Abstract: While humans are inherently social creatures, the challenge of identifying when and how to assist and collaborate with others - particularly when pursuing independent goals - can hinder cooperation. To address this challenge, we aim to develop an AI system that provides useful feedback to promote prosocial behaviour - actions that benefit others, even when not directly aligned with one's own goals. We introduce ProToM, a Theory of Mind-informed facilitator that promotes prosocial actions in multi-agent systems by providing targeted, context-sensitive feedback to individual agents. ProToM first infers agents' goals using Bayesian inverse planning, then selects feedback to communicate by maximising expected utility, conditioned on the inferred goal distribution. We evaluate our approach against baselines in two multi-agent environments: Doors, Keys, and Gems, as well as Overcooked. Our results suggest that state-of-the-art large language and reasoning models fall short of communicating feedback that is both contextually grounded and well-timed - leading to higher communication overhead and task speedup. In contrast, ProToM provides targeted and helpful feedback, achieving a higher success rate, shorter task completion times, and is consistently preferred by human users.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation and Comparison Semantics for ODRL</title>
<link>https://arxiv.org/abs/2509.05139</link>
<guid>https://arxiv.org/abs/2509.05139</guid>
<content:encoded><![CDATA[
<div> evaluate, compare, computational policies, Open Digital Rights Language (ODRL), formal semantics

Summary:
The article discusses the evaluation and comparison of computational policies in the Open Digital Rights Language (ODRL). It addresses the lack of a comprehensive formal semantics for ODRL and proposes a new formal semantics based on query answering. This semantics refines previous formalizations and aligns with the latest language specification. Additionally, the paper introduces the problem of comparing two policies in ODRL, aiming to detect equivalent, more restrictive, or more permissive policies. The study is motivated by data sharing scenarios, highlighting the importance of understanding and analyzing the implications of different policies governing digital resource access and usage. <div>
arXiv:2509.05139v1 Announce Type: new 
Abstract: We consider the problem of evaluating, and comparing computational policies in the Open Digital Rights Language (ODRL), which has become the de facto standard for governing the access and usage of digital resources. Although preliminary progress has been made on the formal specification of the language's features, a comprehensive formal semantics of ODRL is still missing. In this paper, we provide a simple and intuitive formal semantics for ODRL that is based on query answering. Our semantics refines previous formalisations, and is aligned with the latest published specification of the language (2.2). Building on our evaluation semantics, and motivated by data sharing scenarios, we also define and study the problem of comparing two policies, detecting equivalent, more restrictive or more permissive policies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</title>
<link>https://arxiv.org/abs/2509.05263</link>
<guid>https://arxiv.org/abs/2509.05263</guid>
<content:encoded><![CDATA[
<div> framework, 3D world generation, LatticeWorld, multimodal inputs, efficiency<br />
Summary:<br />
The paper introduces LatticeWorld, a 3D world generation framework that utilizes lightweight LLMs and industry-grade rendering engines to create dynamic environments. By accepting textual and visual instructions as inputs, LatticeWorld is able to generate large-scale interactive worlds with dynamic agents, competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. Extensive experiments show that LatticeWorld outperforms traditional manual production methods in scene layout generation and visual fidelity while achieving over a 90x increase in industrial production efficiency. This approach bridges the sim-to-real gap, enabling the creation of more realistic simulations for various applications such as embodied AI, autonomous driving, and entertainment. The proposed framework simplifies the process of 3D world modeling, making it more efficient and effective for industrial production pipelines. By leveraging generative methods and advanced machine learning algorithms, LatticeWorld offers a solution for creating realistic 3D virtual environments with accurate physics simulation. <br /> <div>
arXiv:2509.05263v1 Announce Type: new 
Abstract: Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer</title>
<link>https://arxiv.org/abs/2303.06298</link>
<guid>https://arxiv.org/abs/2303.06298</guid>
<content:encoded><![CDATA[
<div> architecture, MLP-SRGAN, super resolution, FLAIR MRI, image quality metrics

Summary:
MLP-SRGAN is a new single-dimension Super Resolution Generative Adversarial Network that combines Multi-Layer Perceptron Mixers with convolutional layers for upsampling in the slice direction. It is trained on high resolution FLAIR MRI data and tested on multicentre datasets with low spatial resolution. Results show superior performance in edge sharpness, texture preservation, and fine-anatomical detail compared to existing methods. The model has fewer parameters, faster training/evaluation time, and a smaller size. Performance is measured using PSNR and SSIM for images with high resolution ground truths, as well as new no-reference image quality metrics for sharpness, noise, and blurriness assessment. Code for training, inference, data generators, models, and quality metrics is available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2303.06298v1 Announce Type: cross 
Abstract: We propose a novel architecture called MLP-SRGAN, which is a single-dimension Super Resolution Generative Adversarial Network (SRGAN) that utilizes Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to upsample in the slice direction. MLP-SRGAN is trained and validated using high resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with low spatial resolution in the slice dimension to examine performance on held-out (unseen) clinical data. Upsampled results are compared to several state-of-the-art SR networks. For images with high resolution (HR) ground truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index (SSIM) are used to measure upsampling performance. Several new structural, no-reference image quality metrics were proposed to quantify sharpness (edge strength), noise (entropy), and blurriness (low frequency information) in the absence of ground truths. Results show MLP-SRGAN results in sharper edges, less blurring, preserves more texture and fine-anatomical detail, with fewer parameters, faster training/evaluation time, and smaller model size than existing methods. Code for MLP-SRGAN training and inference, data generators, models and no-reference image quality metrics will be available at https://github.com/IAMLAB-Ryerson/MLP-SRGAN.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving</title>
<link>https://arxiv.org/abs/2509.02718</link>
<guid>https://arxiv.org/abs/2509.02718</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, LLM routing, online scenarios, algorithm, nearest neighbor search 

Summary: 
This study introduces a novel algorithm for online routing scenarios, aiming to optimize the use of Large Language Models (LLMs). By leveraging approximate nearest neighbor search and one-time optimization over initial queries, the algorithm efficiently estimates query features and learns a routing strategy for future queries without the need for training. The algorithm is proven to achieve a competitive ratio close to 1 under realistic assumptions. Extensive experiments across multiple datasets and baselines show significant improvements in overall performance, cost efficiency, and throughput compared to existing methods. This approach addresses the challenge of adapting LLM routing to online settings with high query volume and token budget constraints, offering a promising solution for optimizing LLM services in cost-efficient and effective ways.<br /><br />Summary: <div>
arXiv:2509.02718v1 Announce Type: cross 
Abstract: Increasing demand for Large Language Models (LLMs) services imposes substantial deployment and computation costs on providers. LLM routing offers a cost-efficient solution by directing queries to the optimal LLM based on model and query features. However, existing works primarily focus on offline scenarios and struggle to adapt to online settings with high query volume and constrained token budgets. In this work, we introduce the first training-free algorithm for online routing scenarios. Our algorithm leverages approximate nearest neighbor search to efficiently estimate query features and performs a one-time optimization over a small set of initial queries to learn a routing strategy that guides future routing. We provide theoretical guarantees demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$ under natural assumptions, which is further validated by extensive experiments across 3 benchmark datasets and 8 baselines, showing an average improvement of 3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and nearly 4.25$\times$ in throughput.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge</title>
<link>https://arxiv.org/abs/2509.03614</link>
<guid>https://arxiv.org/abs/2509.03614</guid>
<content:encoded><![CDATA[
<div> Keywords: mitotic figures, artificial intelligence, domain shift, pixel-level segmentation, teacher-student model

Summary:
Mitotic figure counting is time-consuming and prone to variability among pathologists. Artificial intelligence offers a solution by automating detection, but faces challenges such as domain shift and imbalanced data. In this study, a teacher-student model is proposed for mitosis detection and atypical mitosis classification. The model uses a UNet segmentation backbone with domain generalization modules and a multi-scale CNN classifier for classification. By incorporating contrastive representation learning and domain-adversarial training, the algorithm generates pixel-level pseudo-masks for mitoses, hard negatives, and normal nuclei to enhance feature discrimination and mitigate domain shift issues. Preliminary results show promising performance, with an F1 score of 0.7660 for mitosis detection and a balanced accuracy of 0.8414 for atypical mitosis classification. This integrated approach demonstrates the effectiveness of combining segmentation-based detection and classification for robust mitosis analysis. 

<br /><br />Summary: <div>
arXiv:2509.03614v1 Announce Type: cross 
Abstract: Counting mitotic figures is time-intensive for pathologists and leads to inter-observer variability. Artificial intelligence (AI) promises a solution by automatically detecting mitotic figures while maintaining decision consistency. However, AI tools are susceptible to domain shift, where a significant drop in performance can occur due to differences in the training and testing sets, including morphological diversity between organs, species, and variations in staining protocols. Furthermore, the number of mitoses is much less than the count of normal nuclei, which introduces severely imbalanced data for the detection task. In this work, we formulate mitosis detection as a pixel-level segmentation and propose a teacher-student model that simultaneously addresses mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our method is based on a UNet segmentation backbone that integrates domain generalization modules, namely contrastive representation learning and domain-adversarial training. A teacher-student strategy is employed to generate pixel-level pseudo-masks not only for annotated mitoses and hard negatives but also for normal nuclei, thereby enhancing feature discrimination and improving robustness against domain shift. For the classification task, we introduce a multi-scale CNN classifier that leverages feature maps from the segmentation model within a multi-task learning paradigm. On the preliminary test set, the algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of 0.8414 in Track 2, demonstrating the effectiveness of integrating segmentation-based detection and classification into a unified framework for robust mitosis analysis.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection</title>
<link>https://arxiv.org/abs/2509.04460</link>
<guid>https://arxiv.org/abs/2509.04460</guid>
<content:encoded><![CDATA[
<div> keywords: large language models, peer review, AI-generated text detectors, content-based detection, CoCoNUTS 

Summary: 
The article discusses the potential risks posed by the integration of large language models (LLMs) into the peer review process and the concerns surrounding their use in generating substantive review content. Existing AI-generated text detectors are limited in distinguishing between language refinement and substantial content generation, leading to unfair suspicion of permissible AI-assisted language enhancement. To address this issue, the authors propose a shift towards content-based detection and introduce CoCoNUTS, a benchmark for evaluating AI-generated peer reviews. They also present CoCoDet, an AI review detector developed through a multi-task learning framework to enhance the accuracy and robustness of detecting AI involvement in review content. This work aims to facilitate more precise, equitable, and reliable detection methods for the use of LLMs in scholarly peer review. 

<br /><br />Summary: <div>
arXiv:2509.04460v1 Announce Type: cross 
Abstract: The growing integration of large language models (LLMs) into the peer review process presents potential risks to the fairness and reliability of scholarly evaluation. While LLMs offer valuable assistance for reviewers with language refinement, there is growing concern over their use to generate substantive review content. Existing general AI-generated text detectors are vulnerable to paraphrasing attacks and struggle to distinguish between surface language refinement and substantial content generation, suggesting that they primarily rely on stylistic cues. When applied to peer review, this limitation can result in unfairly suspecting reviews with permissible AI-assisted language enhancement, while failing to catch deceptively humanized AI-generated reviews. To address this, we propose a paradigm shift from style-based to content-based detection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark built upon a fine-grained dataset of AI-generated peer reviews, covering six distinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an AI review detector via a multi-task learning framework, designed to achieve more accurate and robust detection of AI involvement in review content. Our work offers a practical foundation for evaluating the use of LLMs in peer review, and contributes to the development of more precise, equitable, and reliable detection methods for real-world scholarly applications. Our code and data will be publicly available at https://github.com/Y1hanChen/COCONUTS.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking GPT-5 for biomedical natural language processing</title>
<link>https://arxiv.org/abs/2509.04462</link>
<guid>https://arxiv.org/abs/2509.04462</guid>
<content:encoded><![CDATA[
<div> Benchmark, GPT-5, Biomedical NLP, Performance, Question Answering<br />
<br />
Summary: 
The study evaluated the performance of GPT-5 and compared it with GPT-4 and GPT-4o on a standardized BioNLP benchmark. GPT-5 showed significant improvement across various tasks, achieving the strongest overall benchmark performance. It outperformed GPT-4 and GPT-4o in named entity recognition and relation extraction tasks. In question answering, GPT-5 reached high accuracy levels, surpassing supervised state-of-the-art results by a substantial margin. The model demonstrated strong performance in reasoning-oriented biomedical question answering, while areas like summarization and disease NER still require further improvement. The study highlighted the importance of tailored approaches for different tasks, with simple prompting sufficient for some tasks and more complex strategies necessary for others. Overall, the findings suggest that GPT-5 is a promising model for deployment-ready biomedical NLP tasks.<br /><br /> <div>
arXiv:2509.04462v1 Announce Type: cross 
Abstract: The rapid expansion of biomedical literature has heightened the need for scalable natural language processing (NLP) solutions. While GPT-4 substantially narrowed the gap with task-specific systems, especially in question answering, its performance across other domains remained uneven. We updated a standardized BioNLP benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot prompting across 12 datasets spanning six task families: named entity recognition, relation extraction, multi-label document classification, question answering, text summarization, and text simplification. Using fixed prompt templates, identical decoding parameters, and batch inference, we report primary metrics per dataset and include prior results for GPT-4, GPT-3.5, and LLaMA-2-13B for comparison. GPT-5 achieved the strongest overall benchmark performance, with macro-average scores rising to 0.557 under five-shot prompting versus 0.506 for GPT-4 and 0.508 for GPT-4o. On MedQA, GPT-5 reached 94.1% accuracy, exceeding the previous supervised state of the art by over fifty points, and attained parity with supervised systems on PubMedQA (0.734). In extraction tasks, GPT-5 delivered major gains in chemical NER (0.886 F1) and ChemProt relation extraction (0.616 F1), outperforming GPT-4 and GPT-4o, though summarization and disease NER still lagged behind domain-specific baselines. These results establish GPT-5 as a general-purpose model now offering deployment-ready performance for reasoning-oriented biomedical QA, while precision-critical extraction and evidence-dense summarization continue to favor fine-tuned or hybrid approaches. The benchmark delineates where simple prompting suffices and where retrieval-augmented or planning-based scaffolds are likely required, providing actionable guidance for BioNLP system design as frontier models advance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale Graph Neural Network for Turbulent Flow-Thermal Prediction Around a Complex-Shaped Pin-Fin</title>
<link>https://arxiv.org/abs/2509.04463</link>
<guid>https://arxiv.org/abs/2509.04463</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, flow prediction, complex geometries, surrogate modeling, fast computation <br />
<br />Summary: This study introduces a novel Graph Neural Network designed for predicting steady, turbulent flow and thermal behavior in a 2D channel with complex pin-fin geometries. The network was trained on a dataset generated through an automated framework that included geometry generation, meshing, and flow-field solutions. By converting each simulation into a graph structure, the network was able to accurately predict temperature, velocity, and pressure at each node. The results showed significant improvements in accuracy compared to traditional simulations, particularly in capturing boundary layers and recirculation. Additionally, the network reduced the computational time by 2-3 orders of magnitude, making it a fast and reliable surrogate for complex flow configurations. <div>
arXiv:2509.04463v1 Announce Type: cross 
Abstract: This study presents the development of a domain-responsive edge-aware multiscale Graph Neural Network for predicting steady, turbulent flow and thermal behavior in a two-dimensional channel containing arbitrarily shaped complex pin-fin geometries. The training dataset was constructed through an automated framework that integrated geometry generation, meshing, and flow-field solutions in ANSYS Fluent. The pin-fin geometry was parameterized using piecewise cubic splines, producing 1,000 diverse configurations through Latin Hypercube Sampling. Each simulation was converted into a graph structure, where nodes carried a feature vector containing spatial coordinates, a normalized streamwise position, one-hot boundary indicators, and a signed distance to the nearest boundary such as wall. This graph structure served as input to the newly developed Graph Neural Network, which was trained to predict temperature, velocity magnitude, and pressure at each node using data from ANSYS. The network predicted fields with outstanding accuracy, capturing boundary layers, recirculation, and the stagnation region upstream of the pin-fins while reducing wall time by 2-3 orders of magnitude. In conclusion, the novel graph neural network offered a fast and reliable surrogate for simulations in complex flow configurations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?</title>
<link>https://arxiv.org/abs/2509.04464</link>
<guid>https://arxiv.org/abs/2509.04464</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, uncertainty, diagnosis, knowledge gaps, reliability 

Summary: 

This study focuses on diagnosing the source of uncertainty in Large Language Models (LLMs) by analyzing patterns of disagreement among multiple generated responses. By employing an auxiliary LLM to reason about the likely sources of uncertainty, such as ambiguity in the input question or lack of relevant knowledge, the framework can identify specific missing facts or concepts contributing to uncertainty in cases involving knowledge gaps. The experiment validates the framework on AmbigQA, OpenBookQA, and MMLU-Pro, demonstrating its generality in diagnosing distinct uncertainty sources. The potential for manual interventions to improve LLM performance and reliability is highlighted through this diagnostic approach.  

<br /><br />Summary: <div>
arXiv:2509.04464v1 Announce Type: cross 
Abstract: Large language models (LLMs) have delivered significant breakthroughs across diverse domains but can still produce unreliable or misleading outputs, posing critical challenges for real-world applications. While many recent studies focus on quantifying model uncertainty, relatively little work has been devoted to \textit{diagnosing the source of uncertainty}. In this study, we show that, when an LLM is uncertain, the patterns of disagreement among its multiple generated responses contain rich clues about the underlying cause of uncertainty. To illustrate this point, we collect multiple responses from a target LLM and employ an auxiliary LLM to analyze their patterns of disagreement. The auxiliary model is tasked to reason about the likely source of uncertainty, such as whether it stems from ambiguity in the input question, a lack of relevant knowledge, or both. In cases involving knowledge gaps, the auxiliary model also identifies the specific missing facts or concepts contributing to the uncertainty. In our experiment, we validate our framework on AmbigQA, OpenBookQA, and MMLU-Pro, confirming its generality in diagnosing distinct uncertainty sources. Such diagnosis shows the potential for relevant manual interventions that improve LLM performance and reliability.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotionally-Aware Agents for Dispute Resolution</title>
<link>https://arxiv.org/abs/2509.04465</link>
<guid>https://arxiv.org/abs/2509.04465</guid>
<content:encoded><![CDATA[
<div> Keywords: conflict resolution, emotional expressions, text emotion recognition, dispute dialogues, large-language models

Summary:
This study examines how emotional expressions in text shape outcomes in dispute resolution, specifically in buyer-seller interactions. Using a large corpus of dispute dialogues, the researchers found that emotional expressions play a significant role in influencing subjective and objective resolution outcomes. They also compared the effectiveness of large-language models in emotion intensity annotation with previous methods and found that these models were more accurate in matching human annotator decisions. The findings support existing theoretical models on how emotions impact conflict escalation and resolution, suggesting that agent-based systems could potentially help manage disputes by recognizing and potentially mitigating emotional escalation. This research highlights the importance of understanding and utilizing emotional expressions in conflict resolution processes. 

<br /><br />Summary: 
1. Emotional expressions influence outcomes in dispute resolution, especially in buyer-seller interactions.
2. Large-language models are effective in emotion intensity annotation compared to previous methods.
3. Emotions play a significant role in conflict escalation and resolution.
4. Agent-based systems could help manage disputes by recognizing and mitigating emotional escalation.
5. Understanding emotional expressions is crucial for effective conflict resolution. <div>
arXiv:2509.04465v1 Announce Type: cross 
Abstract: In conflict, people use emotional expressions to shape their counterparts' thoughts, feelings, and actions. This paper explores whether automatic text emotion recognition offers insight into this influence in the context of dispute resolution. Prior work has shown the promise of such methods in negotiations; however, disputes evoke stronger emotions and different social processes. We use a large corpus of buyer-seller dispute dialogues to investigate how emotional expressions shape subjective and objective outcomes. We further demonstrate that large-language models yield considerably greater explanatory power than previous methods for emotion intensity annotation and better match the decisions of human annotators. Findings support existing theoretical models for how emotional expressions contribute to conflict escalation and resolution and suggest that agent-based systems could be useful in managing disputes by recognizing and potentially mitigating emotional escalation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just-in-time and distributed task representations in language models</title>
<link>https://arxiv.org/abs/2509.04466</link>
<guid>https://arxiv.org/abs/2509.04466</guid>
<content:encoded><![CDATA[
<div> evolve, task representations, models, context, computational process
Summary:<br /><br />In this study, the researchers explore how task representations evolve in language models during context learning. They find that transferrable task representations develop in non-monotonic and sporadic ways, distinct from high-level task categories, condensing multiple evidence into minimal task scopes. These representations are localized along the sequence dimension, indicating a just-in-time computational process for adapting to new evidence. The models rely on temporally-distributed representations for longer and composite tasks. Despite task identity being decodable throughout the context, the accrual of task representations occurs at specific tokens. This research sheds light on the dynamic nature of task learning in language models and the underlying computational mechanisms driving their adaptability. <br /><br />Summary: <div>
arXiv:2509.04466v1 Announce Type: cross 
Abstract: Many of language models' impressive capabilities originate from their in-context learning: based on instructions or examples, they can infer and perform new tasks without weight updates. In this work, we investigate \emph{when} representations for new tasks are formed in language models, and \emph{how} these representations change over the course of context. We focus on ''transferrable'' task representations -- vector representations that can restore task context in another instance of the model, even without the full prompt. We show that these representations evolve in non-monotonic and sporadic ways, and are distinct from a more inert representation of high-level task categories that persists throughout the context. Specifically, models often condense multiple evidence into these transferrable task representations, which align well with the performance improvement based on more examples in the context. However, this accrual process exhibits strong locality along the sequence dimension, coming online only at certain tokens -- despite task identity being reliably decodable throughout the context. Moreover, these local but transferrable task representations tend to capture minimal ''task scopes'', such as a semantically-independent subtask, and models rely on more temporally-distributed representations to support longer and composite tasks. This two-fold locality (temporal and semantic) underscores a kind of just-in-time computational process underlying language models' ability to adapt to new evidence and learn new tasks on the fly.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode Disaggregation in Inference</title>
<link>https://arxiv.org/abs/2509.04467</link>
<guid>https://arxiv.org/abs/2509.04467</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, Model Pruning, Prefill-decode disaggregation, Block pruning, KV Cache pruning

Summary: 
Large Language Models (LLMs) are powerful but computationally expensive. Model pruning can help reduce costs, but current methods do not consider prefill-decode (PD) disaggregation. A novel pruning method is proposed in this paper for PD disaggregation, improving block and KV Cache pruning by iteratively removing blocks in the prefill and decode stages independently. Additionally, a token-aware cache pruning mechanism is introduced to reduce communication costs during decode by selectively reusing entries for certain token sequences in selected layers. Experimental results show that the approach achieves significant speedups and bandwidth reductions in both PD disaggregation and unified settings. Under default settings, the method achieves a 20.56% inference speedup and a 4.95 times reduction in data transmission bandwidth consumption. 

<br /><br />Summary: <div>
arXiv:2509.04467v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However, existing methods often ignore the characteristics of prefill-decode (PD) disaggregation in practice. In this paper, we propose a novel pruning method for PD disaggregation inference, enabling more precise and efficient block and KV Cache pruning. Our approach constructs pruning and distillation sets to perform iterative block removal independently for the prefill and decode stages, obtaining better pruning solutions. Moreover, we introduce a token-aware cache pruning mechanism that retains all KV Cache in the prefill stage but selectively reuses entries for the first and last token sequences in selected layers during decode, reducing communication costs with minimal overhead. Extensive experiments demonstrate that our approach consistently achieves strong performance in both PD disaggregation and PD unified settings without disaggregation. Under the default settings, our method achieves a 20.56% inference speedup and a 4.95 times reduction in data transmission bandwidth consumption.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark Study</title>
<link>https://arxiv.org/abs/2509.04468</link>
<guid>https://arxiv.org/abs/2509.04468</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, financial applications, professional certifications, domain-specific knowledge retrieval, model selection

Summary:
The study evaluates state-of-the-art large language models (LLMs) in specialized financial contexts using 1,560 multiple-choice questions from CFA mock exams. Three types of models are compared based on design priorities: multi-modal and powerful, reasoning-specialized and accurate, and efficiency-optimized. Results show that reasoning-oriented models perform better in zero-shot settings, and a Retrieval-Augmented Generation pipeline significantly improves reasoning accuracy in complex scenarios by integrating CFA curriculum content. Knowledge gaps are identified as the main failure mode, with minimal impact from text readability. This research provides insights for deploying LLMs in finance, guiding practitioners on model selection and cost-performance optimization.<br /><br />Summary: <div>
arXiv:2509.04468v1 Announce Type: cross 
Abstract: The rapid advancement of large language models presents significant opportunities for financial applications, yet systematic evaluation in specialized financial contexts remains limited. This study presents the first comprehensive evaluation of state-of-the-art LLMs using 1,560 multiple-choice questions from official mock exams across Levels I-III of CFA, most rigorous professional certifications globally that mirror real-world financial analysis complexity. We compare models distinguished by core design priorities: multi-modal and computationally powerful, reasoning-specialized and highly accurate, and lightweight efficiency-optimized.
  We assess models under zero-shot prompting and through a novel Retrieval-Augmented Generation pipeline that integrates official CFA curriculum content. The RAG system achieves precise domain-specific knowledge retrieval through hierarchical knowledge organization and structured query generation, significantly enhancing reasoning accuracy in professional financial certification evaluation.
  Results reveal that reasoning-oriented models consistently outperform others in zero-shot settings, while the RAG pipeline provides substantial improvements particularly for complex scenarios. Comprehensive error analysis identifies knowledge gaps as the primary failure mode, with minimal impact from text readability. These findings provide actionable insights for LLM deployment in finance, offering practitioners evidence-based guidance for model selection and cost-performance optimization.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing</title>
<link>https://arxiv.org/abs/2509.04469</link>
<guid>https://arxiv.org/abs/2509.04469</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal large language models, invoice document datasets, zero-shot prompting, image processing, structured parsing

Summary:
This paper evaluates the performance of eight multi-modal large language models on three diverse invoice document datasets using zero-shot prompting. The models compared are from families GPT-5, Gemini 2.5, and open-source Gemma 3. Two processing strategies are analyzed: direct image processing with multi-modal capabilities and structured parsing converting documents to markdown first. The results indicate that native image processing generally outperforms structured approaches, although performance varies depending on the model type and document characteristics. This benchmark study provides valuable insights for choosing appropriate models and processing strategies for automated document systems. The code used for the study is available online. 

<br /><br />Summary: <div>
arXiv:2509.04469v1 Announce Type: cross 
Abstract: This paper benchmarks eight multi-modal large language models from three families (GPT-5, Gemini 2.5, and open-source Gemma 3) on three diverse openly available invoice document datasets using zero-shot prompting. We compare two processing strategies: direct image processing using multi-modal capabilities and a structured parsing approach converting documents to markdown first. Results show native image processing generally outperforms structured approaches, with performance varying across model types and document characteristics. This benchmark provides insights for selecting appropriate models and processing strategies for automated document systems. Our code is available online.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COCORELI: Cooperative, Compositional Reconstitution \&amp; Execution of Language Instructions</title>
<link>https://arxiv.org/abs/2509.04470</link>
<guid>https://arxiv.org/abs/2509.04470</guid>
<content:encoded><![CDATA[
<div> Abstraction, Language Models, Spatial Reasoning, Collaborative Construction Tasks, ToolBench API<br />
<br />
Summary: COCORELI is a hybrid agent framework that addresses the limitations of large language models (LLMs) by integrating medium-sized LLM agents with novel abstraction mechanisms and a discourse module. It is designed to handle tasks requiring complex instructions, minimizing hallucination, and spatial reasoning. In experiments on collaborative construction tasks, COCORELI outperformed single-LLM CoT and agentic LLM systems, all using larger LLMs. It demonstrated the ability to avoid hallucinations, identify missing information, ask for clarifications, and update its learned objects. COCORELI's abstraction abilities were also showcased in the ToolBench API completion task, highlighting its versatility beyond just environment parsing. <div>
arXiv:2509.04470v1 Announce Type: cross 
Abstract: We present COCORELI, a hybrid agent framework designed to tackle the limitations of large language models (LLMs) in tasks requiring: following complex instructions, minimizing hallucination, and spatial reasoning. COCORELI integrates medium-sized LLM agents with novel abstraction mechanisms and a discourse module to parse instructions to in-context learn dynamic, high-level representations of the environment. Experiments on natural collaborative construction tasks show that COCORELI outperforms single-LLM CoT and agentic LLM systems, all using larger LLMs. It manages to largely avoid hallucinations, identify missing information, ask for clarifications, and update its learned objects. COCORELI's abstraction abilities extend beyond ENVIRONMENT, as shown in the ToolBench API completion task.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSAIC: A Multilingual, Taxonomy-Agnostic, and Computationally Efficient Approach for Radiological Report Classification</title>
<link>https://arxiv.org/abs/2509.04471</link>
<guid>https://arxiv.org/abs/2509.04471</guid>
<content:encoded><![CDATA[
<div> radiology reports, clinical information, imaging models, MOSAIC, multilingual <br />
<br />
Summary: 
The article introduces MOSAIC, a novel approach for radiological report classification that leverages radiology reports' rich clinical information to train imaging models efficiently. MOSAIC overcomes limitations of existing methods by being multilingual, taxonomy-agnostic, and computationally efficient. It is built on a compact open-access language model, MedGemma-4B, enabling zero-/few-shot prompting and lightweight fine-tuning on consumer-grade GPUs. Evaluation across seven datasets in multiple languages and modalities shows MOSAIC achieving high performance levels with minimal annotated samples, making it a practical alternative to large or proprietary language models in clinical settings. The model achieves an impressive mean macro F1 score of 88 across chest X-ray datasets and demonstrates the effectiveness of data augmentation in reducing the required amount of annotated data for training. MOSAIC's code and models are open-source, inviting the community to explore its capabilities across various languages, taxonomies, and modalities. <div>
arXiv:2509.04471v1 Announce Type: cross 
Abstract: Radiology reports contain rich clinical information that can be used to train imaging models without relying on costly manual annotation. However, existing approaches face critical limitations: rule-based methods struggle with linguistic variability, supervised models require large annotated datasets, and recent LLM-based systems depend on closed-source or resource-intensive models that are unsuitable for clinical use. Moreover, current solutions are largely restricted to English and single-modality, single-taxonomy datasets. We introduce MOSAIC, a multilingual, taxonomy-agnostic, and computationally efficient approach for radiological report classification. Built on a compact open-access language model (MedGemma-4B), MOSAIC supports both zero-/few-shot prompting and lightweight fine-tuning, enabling deployment on consumer-grade GPUs. We evaluate MOSAIC across seven datasets in English, Spanish, French, and Danish, spanning multiple imaging modalities and label taxonomies. The model achieves a mean macro F1 score of 88 across five chest X-ray datasets, approaching or exceeding expert-level performance, while requiring only 24 GB of GPU memory. With data augmentation, as few as 80 annotated samples are sufficient to reach a weighted F1 score of 82 on Danish reports, compared to 86 with the full 1600-sample training set. MOSAIC offers a practical alternative to large or proprietary LLMs in clinical settings. Code and models are open-source. We invite the community to evaluate and extend MOSAIC on new languages, taxonomies, and modalities.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECAP: REwriting Conversations for Intent Understanding in Agentic Planning</title>
<link>https://arxiv.org/abs/2509.04472</link>
<guid>https://arxiv.org/abs/2509.04472</guid>
<content:encoded><![CDATA[
<div> intent rewriting, conversational assistants, large language models, dialogue systems, user intent

Summary:
RECAP (REwriting Conversations for Agent Planning) is proposed as a benchmark to evaluate intent rewriting in user-agent dialogues. It aims to address challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. An LLM-based evaluator is introduced to assess planning utility based on rewritten intents. A prompt-based rewriting approach is developed and shown to outperform baselines. Fine-tuning two DPO-based rewriters further improves utility. The results highlight intent rewriting as a critical component for enhancing agent planning in open-domain dialogue systems. <div>
arXiv:2509.04472v1 Announce Type: cross 
Abstract: Understanding user intent is essential for effective planning in conversational assistants, particularly those powered by large language models (LLMs) coordinating multiple agents. However, real-world dialogues are often ambiguous, underspecified, or dynamic, making intent detection a persistent challenge. Traditional classification-based approaches struggle to generalize in open-ended settings, leading to brittle interpretations and poor downstream planning. We propose RECAP (REwriting Conversations for Agent Planning), a new benchmark designed to evaluate and advance intent rewriting, reframing user-agent dialogues into concise representations of user goals. RECAP captures diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. Alongside the dataset, we introduce an LLM-based evaluator that assesses planning utility given the rewritten intent. Using RECAP, we develop a prompt-based rewriting approach that outperforms baselines. We further demonstrate that fine-tuning two DPO-based rewriters yields additional utility gains. Our results highlight intent rewriting as a critical and tractable component for improving agent planning in open-domain dialogue systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechLLM: Unified Speech and Language Model for Enhanced Multi-Task Understanding in Low Resource Settings</title>
<link>https://arxiv.org/abs/2509.04473</link>
<guid>https://arxiv.org/abs/2509.04473</guid>
<content:encoded><![CDATA[
<div> adapter, speech embeddings, LLM-compatible tokens, automatic speech recognition, synthetic dataset annotation

Summary: 
An innovative solution using a parameter-efficient adapter to convert speech embeddings into tokens compatible with large language models (LLMs) has been proposed. Through this approach, significant performance enhancements have been achieved in various tasks such as automatic speech recognition (ASR), named entity recognition (NER), and sentiment analysis (SA). By leveraging an LLM-based synthetic dataset annotation technique, labeling costs have been reduced effectively. The adapter, which utilizes fewer trainable parameters, has led to a 26% relative improvement in Word Error Rates (WER) for the LibriSpeech ASR task, a 6.3% relative increase in F1 score for the NER task, and a 32% relative boost in F1 score for the SA task. Additionally, techniques such as adding a classifier regularizer and optimizing the LLM with Low-Rank Adaptation (LoRA) have further improved performance, resulting in a notable increase in the Spoken Language Understanding Evaluation (SLUE) score by 6.6% and 9.5%. 

<br /><br />Summary: <div>
arXiv:2509.04473v1 Announce Type: cross 
Abstract: While integrating speech encoder with LLM requires substantial data and resources, use cases face limitations due to insufficient availability. To address this, we propose a solution with a parameter-efficient adapter that converts speech embeddings into LLM-compatible tokens, focusing on end-to-end automatic speech recognition (ASR), named entity recognition (NER), and sentiment analysis (SA). To reduce labeling costs, we employ an LLM-based synthetic dataset annotation technique. The proposed adapter, using 7x fewer trainable parameters, achieves significant performance gains: a 26% relative Word Error Rates (WER) improvement on the LibriSpeech ASR task, a 6.3% relative F1 score increase on the NER task, and a 32% relative F1 score boost on the SA task. Moreover, using advanced techniques such as adding a classifier regularizer and optimizing the LLM with Low-Rank Adaptation (LoRA) yields notable performance gains, with Spoken Language Understanding Evaluation (SLUE) score improvement of 6.6% and 9.5%
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling</title>
<link>https://arxiv.org/abs/2509.04474</link>
<guid>https://arxiv.org/abs/2509.04474</guid>
<content:encoded><![CDATA[
<div> scalable language models, test-time scaling, speculative decoding, benchmark, reasoning 

Summary: 
The article discusses the inefficiencies of test-time scaling in large language models (LLMs) due to redundant reasoning traces. It introduces a benchmark to evaluate speculative decoding methods for accelerating LLM test-time scaling. The benchmark allows for a fair comparison of model-based, training-based, and n-gram-based speculative decoding approaches. The experiments highlight the effectiveness of simple n-gram-based methods in capturing repetitive patterns and accelerating test-time scaling. Integrating n-gram-based methods with model-based or training-based approaches can balance acceleration for both repetitive and diverse reasoning paths in test-time scaling. This research aims to spur further investigation into speculative decoding for test-time scaling, ultimately enabling faster and more practical reasoning in LLMs by improving the handling of repetitive and diverse reasoning paths. 

<br /><br />Summary: <div>
arXiv:2509.04474v1 Announce Type: cross 
Abstract: Test-time scaling has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs) by allocating additional computational resources during inference. However, this paradigm is inherently inefficient due to the generation of redundant and repetitive reasoning traces, leading to significant computational overhead. Speculative decoding offers a promising avenue for mitigating this inefficiency, yet its efficacy in the structured, repetition-rich context of test-time scaling remains largely unexplored. To bridge this gap, we introduce the first comprehensive benchmark designed to evaluate speculative decoding methods for accelerating LLM test-time scaling. Our benchmark provides consistent experimental protocols across representative test-time scaling paradigms (e.g., Best-of-N sampling and multi-round thinking), enabling a fair comparison of three major categories of speculative decoding: model-based, training-based, and n-gram-based methods. Extensive experiments reveal that simple n-gram-based methods effectively capture repetitive patterns, demonstrating unique potential in accelerating test-time scaling. This phenomenon demonstrates the value of integrating n-gram-based methods with model-based or training-based approaches to balance acceleration for both repetitive and diverse reasoning in test-time scaling. We hope this benchmark spurs further research on speculative decoding for test-time scaling, enabling faster and more practical reasoning in LLMs through better handling of repetitive and diverse reasoning paths.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute</title>
<link>https://arxiv.org/abs/2509.04475</link>
<guid>https://arxiv.org/abs/2509.04475</guid>
<content:encoded><![CDATA[
<div> scaling, Large Language Models, reasoning, Parallel thinking, Tunnel Vision 

Summary:
The article introduces a new scaling paradigm called native thought parallelism to overcome the limitations of the traditional test-time compute scaling strategy used in Large Language Models (LLMs). The phenomenon of "Tunnel Vision" is discussed, where sequential reasoning locks a model into suboptimal paths. The ParaThinker framework is presented, which trains LLMs to generate multiple diverse reasoning paths in parallel to synthesize a superior final answer. By exploring different lines of thought simultaneously, ParaThinker effectively avoids Tunnel Vision and unlocks the model's latent reasoning potential. The approach of scaling compute in parallel is shown to be more effective and efficient than scaling sequentially. Results on challenging reasoning benchmarks demonstrate significant accuracy improvements with negligible latency overhead, enabling smaller models to outperform larger ones. The study establishes parallel thinking as a critical dimension for scaling future LLMs. 

<br /><br />Summary: <div>
arXiv:2509.04475v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have been driven by test-time compute scaling - a strategy that improves reasoning by generating longer, sequential thought processes. While effective, this approach encounters a significant bottleneck as computation increases, where further computation offers only marginal performance gains. We argue this ceiling is not an inherent limit of the model's capability but a flaw in the scaling strategy itself, a phenomenon we term "Tunnel Vision", where a model's imperfect initial steps lock it into a suboptimal reasoning path. To overcome this, we introduce a new scaling paradigm: native thought parallelism. We present ParaThinker, an end-to-end framework that trains an LLM to generate multiple, diverse reasoning paths in parallel and synthesize them into a superior final answer. By exploring different lines of thoughts simultaneously, ParaThinker effectively sidesteps the Tunnel Vision issue and unlocks the model's latent reasoning potential. Our approach demonstrates that scaling compute in parallel (width) is a more effective and efficient way to superior reasoning than simply scaling sequentially (depth). On challenging reasoning benchmarks, ParaThinker achieves substantial accuracy improvements over sequential LLMs (12.3% for 1.5B and 7.5% for 7B models on average with 8 parallel paths), while adding only negligible latency overhead (7.1%). This enables smaller models to surpass much larger counterparts and establishes parallel thinking as a critical, efficient dimension for scaling future LLMs.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Text-to-Molecule Models with Context-Aware Tokenization</title>
<link>https://arxiv.org/abs/2509.04476</link>
<guid>https://arxiv.org/abs/2509.04476</guid>
<content:encoded><![CDATA[
<div> substructure-level tokenization, text-to-molecule models, molecule structures, importance-based training, CAMT5 <br />
<br />
Summary: 
The article introduces a novel text-to-molecule model called Context-Aware Molecular T5 (CAMT5) that addresses the limitations of existing models in capturing global structural context within molecules. By using substructure-level tokenization, CAMT5 focuses on the importance of substructure-level contexts, such as ring systems, in understanding molecule structures. The model also employs an importance-based training strategy to prioritize key substructures, allowing it to better capture molecular semantics. Experimental results demonstrate that CAMT5 outperforms state-of-the-art methods with significantly fewer training tokens. Furthermore, the article proposes an ensemble strategy to further enhance the generation performance by aggregating the outputs of different text-to-molecule models. The code for CAMT5 is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2509.04476v1 Announce Type: cross 
Abstract: Recently, text-to-molecule models have shown great potential across various chemical applications, e.g., drug-discovery. These models adapt language models to molecular data by representing molecules as sequences of atoms. However, they rely on atom-level tokenizations, which primarily focus on modeling local connectivity, thereby limiting the ability of models to capture the global structural context within molecules. To tackle this issue, we propose a novel text-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by the significance of the substructure-level contexts in understanding molecule structures, e.g., ring systems, we introduce substructure-level tokenization for text-to-molecule models. Building on our tokenization scheme, we develop an importance-based training strategy that prioritizes key substructures, enabling CAMT5 to better capture the molecular semantics. Extensive experiments verify the superiority of CAMT5 in various text-to-molecule generation tasks. Intriguingly, we find that CAMT5 outperforms the state-of-the-art methods using only 2% of training tokens. In addition, we propose a simple yet effective ensemble strategy that aggregates the outputs of text-to-molecule models to further boost the generation performance. Code is available at https://github.com/Songhyeontae/CAMT5.git.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Clustering, No Routing: How Transformers Actually Process Rare Tokens</title>
<link>https://arxiv.org/abs/2509.04479</link>
<guid>https://arxiv.org/abs/2509.04479</guid>
<content:encoded><![CDATA[
<div> specialization, rare token prediction, plateau neurons, computational regimes, attention mechanisms

Summary:
Rare token prediction poses a challenge for large language models, leading to the identification of specialized "plateau" neurons. This study delves into the functional organization of these neurons using GPT-2 XL and Pythia models. The research reveals that processing rare tokens requires a separate set of plateau neurons, distinct from those handling common tokens, resulting in dual computational regimes. Interestingly, these plateau neurons are distributed across the model rather than being clustered together. Furthermore, attention mechanisms do not exhibit a bias towards routing to these specialized neurons. Overall, the specialization for rare tokens emerges through distributed, training-induced differentiation rather than modular clustering, allowing for context-sensitive flexibility while efficiently allocating computational resources. <div>
arXiv:2509.04479v1 Announce Type: cross 
Abstract: Large language models struggle with rare token prediction, yet the mechanisms driving their specialization remain unclear. Prior work identified specialized ``plateau'' neurons for rare tokens following distinctive three-regime influence patterns \cite{liu2025emergent}, but their functional organization is unknown. We investigate this through neuron influence analyses, graph-based clustering, and attention head ablations in GPT-2 XL and Pythia models. Our findings show that: (1) rare token processing requires additional plateau neurons beyond the power-law regime sufficient for common tokens, forming dual computational regimes; (2) plateau neurons are spatially distributed rather than forming modular clusters; and (3) attention mechanisms exhibit no preferential routing to specialists. These results demonstrate that rare token specialization arises through distributed, training-driven differentiation rather than architectural modularity, preserving context-sensitive flexibility while achieving adaptive capacity allocation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game Environments</title>
<link>https://arxiv.org/abs/2509.04481</link>
<guid>https://arxiv.org/abs/2509.04481</guid>
<content:encoded><![CDATA[
<div> pipeline, narrative prompts, tile-based game scenes, spatial predicates, spatial rules

Summary:
The article introduces a lightweight pipeline that converts short narrative prompts into 2D tile-based game scenes, aligning with the temporal structure of stories. It identifies three key time frames in LLM-generated narratives, extracts spatial predicates in "Object-Relation-Object" form, and utilizes affordance-aware semantic embeddings to retrieve visual assets. A layered terrain is created using Cellular Automata, and objects are placed based on spatial rules grounded in the predicate structure. The system was evaluated with ten diverse stories, assessing tile-object matching, affordance-layer alignment, and spatial constraint satisfaction across frames. This prototype offers a scalable solution for generating narrative-driven game scenes and sets the groundwork for future research on multi-frame continuity, symbolic tracking, and multi-agent coordination in story-centered PCG. 

<br /><br />Summary: <div>
arXiv:2509.04481v1 Announce Type: cross 
Abstract: Recent advances in large language models(LLMs) enable compelling story generation, but connecting narrative text to playable visual environments remains an open challenge in procedural content generation(PCG). We present a lightweight pipeline that transforms short narrative prompts into a sequence of 2D tile-based game scenes, reflecting the temporal structure of stories. Given an LLM-generated narrative, our system identifies three key time frames, extracts spatial predicates in the form of "Object-Relation-Object" triples, and retrieves visual assets using affordance-aware semantic embeddings from the GameTileNet dataset. A layered terrain is generated using Cellular Automata, and objects are placed using spatial rules grounded in the predicate structure. We evaluated our system in ten diverse stories, analyzing tile-object matching, affordance-layer alignment, and spatial constraint satisfaction across frames. This prototype offers a scalable approach to narrative-driven scene generation and lays the foundation for future work on multi-frame continuity, symbolic tracking, and multi-agent coordination in story-centered PCG.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented Large Language Models for Healthcare</title>
<link>https://arxiv.org/abs/2509.04482</link>
<guid>https://arxiv.org/abs/2509.04482</guid>
<content:encoded><![CDATA[
<div> energy-based model, reliable abstention, retrieval-augmented generation, safety-critical domains, semantic corpus

Summary:
The article introduces an energy-based model (EBM) for reliable abstention in retrieval-augmented generation (RAG) systems, focusing on safety-critical areas like women's health. The EBM learns a smooth energy landscape over a large semantic corpus of guideline-derived questions to determine when to generate or abstain. Comparative benchmarks against softmax and kNN approaches show that the EBM outperforms in abstention performance on difficult, near-distribution queries, reaching AUROC 0.961 and reducing FPR@95. The EBM's strength lies in its energy scoring head, providing a more reliable confidence signal than probability-based softmax. The study demonstrates that energy-based abstention scoring offers scalability and interpretability for safe RAG systems, emphasizing the importance of reliable abstention in critical domains. <div>
arXiv:2509.04482v1 Announce Type: cross 
Abstract: Reliable abstention is critical for retrieval-augmented generation (RAG) systems, particularly in safety-critical domains such as women's health, where incorrect answers can lead to harm. We present an energy-based model (EBM) that learns a smooth energy landscape over a dense semantic corpus of 2.6M guideline-derived questions, enabling the system to decide when to generate or abstain. We benchmark the EBM against a calibrated softmax baseline and a k-nearest neighbour (kNN) density heuristic across both easy and hard abstention splits, where hard cases are semantically challenging near-distribution queries. The EBM achieves superior abstention performance abstention on semantically hard cases, reaching AUROC 0.961 versus 0.950 for softmax, while also reducing FPR@95 (0.235 vs 0.331). On easy negatives, performance is comparable across methods, but the EBM's advantage becomes most pronounced in safety-critical hard distributions. A comprehensive ablation with controlled negative sampling and fair data exposure shows that robustness stems primarily from the energy scoring head, while the inclusion or exclusion of specific negative types (hard, easy, mixed) sharpens decision boundaries but is not essential for generalisation to hard cases. These results demonstrate that energy-based abstention scoring offers a more reliable confidence signal than probability-based softmax confidence, providing a scalable and interpretable foundation for safe RAG systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DecMetrics: Structured Claim Decomposition Scoring for Factually Consistent LLM Outputs</title>
<link>https://arxiv.org/abs/2509.04483</link>
<guid>https://arxiv.org/abs/2509.04483</guid>
<content:encoded><![CDATA[
<div> metrics, claim decomposition, fact-checking, generative methods, evaluation

Summary:
DecMetrics introduces three new metrics, COMPLETENESS, CORRECTNESS, and SEMANTIC ENTROPY, to evaluate the quality of claims produced by decomposition models. These metrics aim to assess the completeness, correctness, and semantic entropy of atomic claims generated through claim decomposition. By integrating these metrics as a reward function, a lightweight claim decomposition model is developed and optimized for performance. The approach sets a benchmark for claim decomposition in the fact-checking process, enhancing the reliability and effectiveness of fact-checking systems. The focus on evaluating the quality of decomposed claims provides a valuable contribution to the field, addressing a gap in existing research that primarily concentrates on generative methods for claim decomposition. <div>
arXiv:2509.04483v1 Announce Type: cross 
Abstract: Claim decomposition plays a crucial role in the fact-checking process by breaking down complex claims into simpler atomic components and identifying their unfactual elements. Despite its importance, current research primarily focuses on generative methods for decomposition, with insufficient emphasis on evaluating the quality of these decomposed atomic claims. To bridge this gap, we introduce \textbf{DecMetrics}, which comprises three new metrics: \texttt{COMPLETENESS}, \texttt{CORRECTNESS}, and \texttt{SEMANTIC ENTROPY}, designed to automatically assess the quality of claims produced by decomposition models. Utilizing these metrics, we develop a lightweight claim decomposition model, optimizing its performance through the integration of these metrics as a reward function. Through automatic evaluation, our approach aims to set a benchmark for claim decomposition, enhancing both the reliability and effectiveness of fact-checking systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors</title>
<link>https://arxiv.org/abs/2509.04484</link>
<guid>https://arxiv.org/abs/2509.04484</guid>
<content:encoded><![CDATA[
<div> Automated Feedback, Peer Review, Review Comments, RevUtil Dataset, Fine-tuned Models<br />
<br />
Summary: Providing constructive feedback to authors is crucial in peer review. To ensure high reviewing quality despite time constraints, automated support systems are necessary. Four key aspects of review comments that enhance their utility for authors include Actionability, Grounding & Specificity, Verifiability, and Helpfulness. The RevUtil dataset, comprising human-labeled and synthetically labeled review comments, serves as a tool for evaluating and developing models assessing review comments. By fine-tuning models on the RevUtil dataset, researchers achieve agreement levels with humans comparable to, and sometimes surpassing, powerful closed models like GPT-4o. However, machine-generated reviews generally fall short of human reviews across the four key aspects identified in review comments. <div>
arXiv:2509.04484v1 Announce Type: cross 
Abstract: Providing constructive feedback to paper authors is a core component of peer review. With reviewers increasingly having less time to perform reviews, automated support systems are required to ensure high reviewing quality, thus making the feedback in reviews useful for authors. To this end, we identify four key aspects of review comments (individual points in weakness sections of reviews) that drive the utility for authors: Actionability, Grounding & Specificity, Verifiability, and Helpfulness. To enable evaluation and development of models assessing review comments, we introduce the RevUtil dataset. We collect 1,430 human-labeled review comments and scale our data with 10k synthetically labeled comments for training purposes. The synthetic data additionally contains rationales, i.e., explanations for the aspect score of a review comment. Employing the RevUtil dataset, we benchmark fine-tuned models for assessing review comments on these aspects and generating rationales. Our experiments demonstrate that these fine-tuned models achieve agreement levels with humans comparable to, and in some cases exceeding, those of powerful closed models like GPT-4o. Our analysis further reveals that machine-generated reviews generally underperform human reviews on our four aspects.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records</title>
<link>https://arxiv.org/abs/2509.04485</link>
<guid>https://arxiv.org/abs/2509.04485</guid>
<content:encoded><![CDATA[
<div> Keywords: ASCENDgpt, cardiovascular risk prediction, electronic health records, transformer-based model, phenotype-aware tokenization

Summary:
ASCENDgpt is a transformer-based model designed for cardiovascular risk prediction from electronic health records (EHRs). It employs a novel phenotype-aware tokenization scheme that consolidates raw ICD codes into clinically meaningful phenotype tokens, reducing vocabulary size significantly. Pretrained on sequences from unique individuals, ASCENDgpt achieves strong performance in predicting five cardiovascular outcomes, including myocardial infarction, stroke, and cardiovascular death. The model demonstrates excellent discrimination on a test set, with an average C-index of 0.816. The phenotype-based approach ensures clinically interpretable predictions while maintaining computational efficiency. This study highlights the effectiveness of domain-specific tokenization and pretraining for EHR-based risk prediction tasks.<br /><br />Summary: <div>
arXiv:2509.04485v1 Announce Type: cross 
Abstract: We present ASCENDgpt, a transformer-based model specifically designed for cardiovascular risk prediction from longitudinal electronic health records (EHRs). Our approach introduces a novel phenotype-aware tokenization scheme that maps 47,155 raw ICD codes to 176 clinically meaningful phenotype tokens, achieving 99.6\% consolidation of diagnosis codes while preserving semantic information. This phenotype mapping contributes to a total vocabulary of 10,442 tokens - a 77.9\% reduction when compared with using raw ICD codes directly. We pretrain ASCENDgpt on sequences derived from 19402 unique individuals using a masked language modeling objective, then fine-tune for time-to-event prediction of five cardiovascular outcomes: myocardial infarction (MI), stroke, major adverse cardiovascular events (MACE), cardiovascular death, and all-cause mortality. Our model achieves excellent discrimination on the held-out test set with an average C-index of 0.816, demonstrating strong performance across all outcomes (MI: 0.792, stroke: 0.824, MACE: 0.800, cardiovascular death: 0.842, all-cause mortality: 0.824). The phenotype-based approach enables clinically interpretable predictions while maintaining computational efficiency. Our work demonstrates the effectiveness of domain-specific tokenization and pretraining for EHR-based risk prediction tasks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Serialized Output Prompting for Large Language Model-based Multi-Talker Speech Recognition</title>
<link>https://arxiv.org/abs/2509.04488</link>
<guid>https://arxiv.org/abs/2509.04488</guid>
<content:encoded><![CDATA[
<div> Prompt, multi-talker automatic speech recognition, large language models, separated output prompts, structured prompts<br />
Summary:<br />
The paper proposes a method, SOP-MT-ASR, to enhance multi-talker automatic speech recognition systems using serialized output prompts (SOP) to guide large language models. By inserting Separator and serialized CTC layers after the speech encoder, the system separates and extracts MT content in a first-speaking-first-out manner. The extracted SOP serves as a structured prompt for the LLMs, improving system performance. A three-stage training strategy is designed for effective model training. Experimental results on the LibriMix dataset show that while the LLM-based SOT model performs well in two-talker scenarios, it struggles in more complex three-talker scenarios. The proposed SOP approach significantly enhances performance in both scenarios. <br /><br /> <div>
arXiv:2509.04488v1 Announce Type: cross 
Abstract: Prompts are crucial for task definition and for improving the performance of large language models (LLM)-based systems. However, existing LLM-based multi-talker (MT) automatic speech recognition (ASR) systems either omit prompts or rely on simple task-definition prompts, with no prior work exploring the design of prompts to enhance performance. In this paper, we propose extracting serialized output prompts (SOP) and explicitly guiding the LLM using structured prompts to improve system performance (SOP-MT-ASR). A Separator and serialized Connectionist Temporal Classification (CTC) layers are inserted after the speech encoder to separate and extract MT content from the mixed speech encoding in a first-speaking-first-out manner. Subsequently, the SOP, which serves as a prompt for LLMs, is obtained by decoding the serialized CTC outputs using greedy search. To train the model effectively, we design a three-stage training strategy, consisting of serialized output training (SOT) fine-tuning, serialized speech information extraction, and SOP-based adaptation. Experimental results on the LibriMix dataset show that, although the LLM-based SOT model performs well in the two-talker scenario, it fails to fully leverage LLMs under more complex conditions, such as the three-talker scenario. The proposed SOP approach significantly improved performance under both two- and three-talker conditions.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR</title>
<link>https://arxiv.org/abs/2509.04491</link>
<guid>https://arxiv.org/abs/2509.04491</guid>
<content:encoded><![CDATA[
<div> Subtitle, Weakly Supervised, Automatic Speech Recognition, Pseudo-labeled datasets, Transcription accuracy
Summary: 
Subtitle text and spoken audio discrepancies limit the applicability of TV subtitles in Automatic Speech Recognition (ASR). This study proposes a novel approach using subtitles as context-rich prompts rather than direct supervision signals. Generated pseudo transcripts, guided by subtitles, are iteratively refined to improve transcription accuracy. A weighted attention mechanism emphasizes relevant subtitle tokens during inference, leading to significant improvements in transcription quality. The method provides high-quality pseudo-labeled datasets for training robust ASR systems. Enhanced transcripts offer a valuable resource for developing accurate and efficient ASR models. <div>
arXiv:2509.04491v1 Announce Type: cross 
Abstract: This study proposes a novel approach to using TV subtitles within a weakly supervised (WS) Automatic Speech Recognition (ASR) framework. Although TV subtitles are readily available, their imprecise alignment with corresponding audio limits their applicability as supervised targets for verbatim transcription. Rather than using subtitles as direct supervision signals, our method reimagines them as context-rich prompts. This design enables the model to handle discrepancies between spoken audio and subtitle text. Instead, generated pseudo transcripts become the primary targets, with subtitles acting as guiding cues for iterative refinement. To further enhance the process, we introduce a weighted attention mechanism that emphasizes relevant subtitle tokens during inference. Our experiments demonstrate significant improvements in transcription accuracy, highlighting the effectiveness of the proposed method in refining transcripts. These enhanced pseudo-labeled datasets provide high-quality foundational resources for training robust ASR systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate</title>
<link>https://arxiv.org/abs/2509.04492</link>
<guid>https://arxiv.org/abs/2509.04492</guid>
<content:encoded><![CDATA[
<div> Keywords: Hallucination detection, Large Language Models, Question Answering, Entropy Production Rate, Supervised learning 

Summary: 
- This paper introduces a methodology for detecting hallucinations in outputs of Large Language Models (LLMs) for Question Answering tasks.
- The approach focuses on scenarios with limited data access, like black-box LLM APIs, by deriving uncertainty indicators from available log-probabilities during decoding.
- An Entropy Production Rate (EPR) metric is developed initially, and later enhanced with supervised learning to improve detection performance.
- The learned model uses features representing the entropic contributions of top-ranked tokens in a single generated sequence, without requiring multiple query re-runs.
- Evaluation across various QA datasets and LLMs shows significant improvement in hallucination detection over using EPR alone, using only a small set of available log-probabilities.
  
<br /><br />Summary: <div>
arXiv:2509.04492v1 Announce Type: cross 
Abstract: Hallucinations in Large Language Model (LLM) outputs for Question Answering (QA) tasks critically undermine their real-world reliability. This paper introduces an applied methodology for robust, one-shot hallucination detection, specifically designed for scenarios with limited data access, such as interacting with black-box LLM APIs that typically expose only a few top candidate log-probabilities per token. Our approach derives uncertainty indicators directly from these readily available log-probabilities generated during non-greedy decoding. We first derive an Entropy Production Rate (EPR) metric that offers baseline performance, later augmented with supervised learning. Our learned model uses features representing the entropic contributions of the accessible top-ranked tokens within a single generated sequence, requiring no multiple query re-runs. Evaluated across diverse QA datasets and multiple LLMs, this estimator significantly improves hallucination detection over using EPR alone. Crucially, high performance is demonstrated using only the typically small set of available log-probabilities (e.g., top <10 per token), confirming its practical efficiency and suitability for these API-constrained deployments. This work provides a readily deployable technique to enhance the trustworthiness of LLM responses from a single generation pass in QA and Retrieval-Augmented Generation (RAG) systems, with its utility further demonstrated in a finance framework analyzing responses to queries on annual reports from an industrial dataset.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Narrative-Driven Computational Framework for Clinician Burnout Surveillance</title>
<link>https://arxiv.org/abs/2509.04497</link>
<guid>https://arxiv.org/abs/2509.04497</guid>
<content:encoded><![CDATA[
<div> Keywords: clinician burnout, intensive care units, clinical notes, sentiment analysis, workload proxies

Summary: 
Analyzing 10,000 ICU discharge summaries from the MIMIC-IV database, this study uses a hybrid pipeline to extract valuable information from clinical notes for proactive clinician burnout monitoring. By combining BioBERT sentiment embeddings, a stress lexicon for burnout surveillance, and LDA with workload proxies, the model achieves a precision of 0.80 and a recall of 0.89 in predicting burnout risk. Specialty-specific analysis reveals elevated burnout risk in Radiology, Psychiatry, and Neurology. The study highlights the potential of leveraging clinical narratives to identify burnout signals and improve well-being monitoring in high-acuity healthcare settings.<br /><br />Summary: <div>
arXiv:2509.04497v1 Announce Type: cross 
Abstract: Clinician burnout poses a substantial threat to patient safety, particularly in high-acuity intensive care units (ICUs). Existing research predominantly relies on retrospective survey tools or broad electronic health record (EHR) metadata, often overlooking the valuable narrative information embedded in clinical notes. In this study, we analyze 10,000 ICU discharge summaries from MIMIC-IV, a publicly available database derived from the electronic health records of Beth Israel Deaconess Medical Center. The dataset encompasses diverse patient data, including vital signs, medical orders, diagnoses, procedures, treatments, and deidentified free-text clinical notes. We introduce a hybrid pipeline that combines BioBERT sentiment embeddings fine-tuned for clinical narratives, a lexical stress lexicon tailored for clinician burnout surveillance, and five-topic latent Dirichlet allocation (LDA) with workload proxies. A provider-level logistic regression classifier achieves a precision of 0.80, a recall of 0.89, and an F1 score of 0.84 on a stratified hold-out set, surpassing metadata-only baselines by greater than or equal to 0.17 F1 score. Specialty-specific analysis indicates elevated burnout risk among providers in Radiology, Psychiatry, and Neurology. Our findings demonstrate that ICU clinical narratives contain actionable signals for proactive well-being monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations</title>
<link>https://arxiv.org/abs/2509.04498</link>
<guid>https://arxiv.org/abs/2509.04498</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, biases, university recommendations, diversity, equitable access <br />
<br />
Summary: 
The study investigates biases in university recommendations generated by three Large Language Models (LLMs). It focuses on geographic, demographic, and economic disparities in the suggestions provided by these models. The analysis, conducted using simulated user profiles, reveals a tendency towards favoring institutions in the Global North and perpetuating gender stereotypes. Despite LLaMA-3.1 demonstrating higher diversity in recommendations, systemic disparities persist across the models. The research introduces a novel evaluation framework that looks beyond accuracy to assess demographic and geographic representation. The findings underscore the pressing need to address biases in educational LLMs to ensure fair and inclusive global access to higher education. <br /> <div>
arXiv:2509.04498v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used as daily recommendation systems for tasks like education planning, yet their recommendations risk perpetuating societal biases. This paper empirically examines geographic, demographic, and economic biases in university and program suggestions from three open-source LLMs: LLaMA-3.1-8B, Gemma-7B, and Mistral-7B. Using 360 simulated user profiles varying by gender, nationality, and economic status, we analyze over 25,000 recommendations. Results show strong biases: institutions in the Global North are disproportionately favored, recommendations often reinforce gender stereotypes, and institutional repetition is prevalent. While LLaMA-3.1 achieves the highest diversity, recommending 481 unique universities across 58 countries, systemic disparities persist. To quantify these issues, we propose a novel, multi-dimensional evaluation framework that goes beyond accuracy by measuring demographic and geographic representation. Our findings highlight the urgent need for bias consideration in educational LMs to ensure equitable global access to higher education.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability Across Citations and Evidence</title>
<link>https://arxiv.org/abs/2509.04499</link>
<guid>https://arxiv.org/abs/2509.04499</guid>
<content:encoded><![CDATA[
<div> audit framework, generative search engines, deep research LLM agents, citation practices, evidence attribution 

Summary:
DeepTRACE introduces a novel audit framework, focusing on eight dimensions related to answer text, sources, and citations to address overconfidence and weak sourcing in generative search engines and deep research LLM agents. The framework utilizes statement-level analysis, citation matrices, and factual-support matrices to evaluate how systems reason and attribute evidence. The study evaluates popular models and LLM configurations, revealing that while deep-research configurations reduce overconfidence and improve citation thoroughness, they still tend to provide one-sided responses on debate queries and include unsupported statements. Citation accuracy ranges from 40-80% across systems, indicating room for improvement in sourcing and evidence attribution practices. <br /><br />Summary: <div>
arXiv:2509.04499v1 Announce Type: cross 
Abstract: Generative search engines and deep research LLM agents promise trustworthy, source-grounded synthesis, yet users regularly encounter overconfidence, weak sourcing, and confusing citation practices. We introduce DeepTRACE, a novel sociotechnically grounded audit framework that turns prior community-identified failure cases into eight measurable dimensions spanning answer text, sources, and citations. DeepTRACE uses statement-level analysis (decomposition, confidence scoring) and builds citation and factual-support matrices to audit how systems reason with and attribute evidence end-to-end. Using automated extraction pipelines for popular public models (e.g., GPT-4.5/5, You.com, Perplexity, Copilot/Bing, Gemini) and an LLM-judge with validated agreement to human raters, we evaluate both web-search engines and deep-research configurations. Our findings show that generative search engines and deep research agents frequently produce one-sided, highly confident responses on debate queries and include large fractions of statements unsupported by their own listed sources. Deep-research configurations reduce overconfidence and can attain high citation thoroughness, but they remain highly one-sided on debate queries and still exhibit large fractions of unsupported statements, with citation accuracy ranging from 40--80% across systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts</title>
<link>https://arxiv.org/abs/2509.04500</link>
<guid>https://arxiv.org/abs/2509.04500</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Poisoned Context Testbed, Rescorla-Wagner model, inappropriate content, context engineering <br />
Summary: 
Large Language Models (LLMs) benefit from external context but can struggle when faced with mixed information. In the Poisoned Context Testbed, queries are paired with real-world contexts containing relevant and inappropriate content to study how LLMs prioritize signals. The adapted Rescorla-Wagner model shows that LLMs tend to incorporate less prevalent information, leading to a susceptibility to inappropriate content. This vulnerability can significantly impact response quality in real-world settings. To address this, RW-Steering is introduced as a two-stage finetuning approach that allows models to identify and ignore inappropriate signals internally. Unlike previous methods, RW-Steering generalizes well across different mixtures of inappropriate content, improving response quality by 39.8% and mitigating the undesirable behavior curve. RW-Steering demonstrates robust and generalizable context engineering for enhancing LLM safety in practical applications. <br /> <div>
arXiv:2509.04500v1 Announce Type: cross 
Abstract: Incorporating external context can significantly enhance the response quality of Large Language Models (LLMs). However, real-world contexts often mix relevant information with disproportionate inappropriate content, posing reliability risks. How do LLMs process and prioritize mixed context? To study this, we introduce the Poisoned Context Testbed, pairing queries with real-world contexts containing relevant and inappropriate content. Inspired by associative learning in animals, we adapt the Rescorla-Wagner (RW) model from neuroscience to quantify how competing contextual signals influence LLM outputs. Our adapted model reveals a consistent behavioral pattern: LLMs exhibit a strong tendency to incorporate information that is less prevalent in the context. This susceptibility is harmful in real-world settings, where small amounts of inappropriate content can substantially degrade response quality. Empirical evaluations on our testbed further confirm this vulnerability. To tackle this, we introduce RW-Steering, a two-stage finetuning-based approach that enables the model to internally identify and ignore inappropriate signals. Unlike prior methods that rely on extensive supervision across diverse context mixtures, RW-Steering generalizes robustly across varying proportions of inappropriate content. Experiments show that our best fine-tuned model improves response quality by 39.8% and reverses the undesirable behavior curve, establishing RW-Steering as a robust, generalizable context engineering solution for improving LLM safety in real-world use.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Reinforcement Learning for Model Training, and future directions with GRAPE</title>
<link>https://arxiv.org/abs/2509.04501</link>
<guid>https://arxiv.org/abs/2509.04501</guid>
<content:encoded><![CDATA[
arXiv:2509.04501v1 Announce Type: cross 
Abstract: This paper provides a self-contained, from-scratch, exposition of key algorithms for instruction tuning of models: SFT, Rejection Sampling, REINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), and Direct Preference Optimization (DPO). Explanations of these algorithms often assume prior knowledge, lack critical details, and/or are overly generalized and complex. Here, each method is discussed and developed step by step using simplified and explicit notation focused on LLMs, aiming to eliminate ambiguity and provide a clear and intuitive understanding of the concepts. By minimizing detours into the broader RL literature and connecting concepts to LLMs, we eliminate superfluous abstractions and reduce cognitive overhead. Following this exposition, we provide a literature review of new techniques and approaches beyond those detailed. Finally, new ideas for research and exploration in the form of GRAPE (Generalized Relative Advantage Policy Evolution) are presented.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples</title>
<link>https://arxiv.org/abs/2509.04502</link>
<guid>https://arxiv.org/abs/2509.04502</guid>
<content:encoded><![CDATA[
arXiv:2509.04502v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation enhances the response accuracy of Large Language Models (LLMs) by integrating retrieval and generation modules with external knowledge, demonstrating particular strength in real-time queries and Visual Question Answering tasks. However, the effectiveness of RAG is frequently hindered by the precision of the retriever: many retrieved samples fed into the generation phase are irrelevant or misleading, posing a critical bottleneck to LLMs' performance. To address this challenge, we introduce VaccineRAG, a novel Chain-of-Thought-based retrieval-augmented generation dataset. On one hand, VaccineRAG employs a benchmark to evaluate models using data with varying positive/negative sample ratios, systematically exposing inherent weaknesses in current LLMs. On the other hand, it enhances models' sample-discrimination capabilities by prompting LLMs to generate explicit Chain-of-Thought (CoT) analysis for each sample before producing final answers. Furthermore, to enhance the model's ability to learn long-sequence complex CoT content, we propose Partial-GRPO. By modeling the outputs of LLMs as multiple components rather than a single whole, our model can make more informed preference selections for complex sequences, thereby enhancing its capacity to learn complex CoT. Comprehensive evaluations and ablation studies on VaccineRAG validate the effectiveness of the proposed scheme. The code and dataset will be publicly released soon.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioral Fingerprinting of Large Language Models</title>
<link>https://arxiv.org/abs/2509.04504</link>
<guid>https://arxiv.org/abs/2509.04504</guid>
<content:encoded><![CDATA[
arXiv:2509.04504v1 Announce Type: cross 
Abstract: Current benchmarks for Large Language Models (LLMs) primarily focus on performance metrics, often failing to capture the nuanced behavioral characteristics that differentiate them. This paper introduces a novel ``Behavioral Fingerprinting'' framework designed to move beyond traditional evaluation by creating a multi-faceted profile of a model's intrinsic cognitive and interactive styles. Using a curated \textit{Diagnostic Prompt Suite} and an innovative, automated evaluation pipeline where a powerful LLM acts as an impartial judge, we analyze eighteen models across capability tiers. Our results reveal a critical divergence in the LLM landscape: while core capabilities like abstract and causal reasoning are converging among top models, alignment-related behaviors such as sycophancy and semantic robustness vary dramatically. We further document a cross-model default persona clustering (ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together, this suggests that a model's interactive nature is not an emergent property of its scale or reasoning power, but a direct consequence of specific, and highly variable, developer alignment strategies. Our framework provides a reproducible and scalable methodology for uncovering these deep behavioral differences. Project: https://github.com/JarvisPei/Behavioral-Fingerprinting
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memristor-Based Neural Network Accelerators for Space Applications: Enhancing Performance with Temporal Averaging and SIRENs</title>
<link>https://arxiv.org/abs/2509.04506</link>
<guid>https://arxiv.org/abs/2509.04506</guid>
<content:encoded><![CDATA[
arXiv:2509.04506v1 Announce Type: cross 
Abstract: Memristors are an emerging technology that enables artificial intelligence (AI) accelerators with high energy efficiency and radiation robustness -- properties that are vital for the deployment of AI on-board spacecraft. However, space applications require reliable and precise computations, while memristive devices suffer from non-idealities, such as device variability, conductance drifts, and device faults. Thus, porting neural networks (NNs) to memristive devices often faces the challenge of severe performance degradation. In this work, we show in simulations that memristor-based NNs achieve competitive performance levels on on-board tasks, such as navigation \& control and geodesy of asteroids. Through bit-slicing, temporal averaging of NN layers, and periodic activation functions, we improve initial results from around $0.07$ to $0.01$ and $0.3$ to $0.007$ for both tasks using RRAM devices, coming close to state-of-the-art levels ($0.003-0.005$ and $0.003$, respectively). Our results demonstrate the potential of memristors for on-board space applications, and we are convinced that future technology and NN improvements will further close the performance gap to fully unlock the benefits of memristors.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM Approach</title>
<link>https://arxiv.org/abs/2509.04507</link>
<guid>https://arxiv.org/abs/2509.04507</guid>
<content:encoded><![CDATA[
arXiv:2509.04507v1 Announce Type: cross 
Abstract: Silent Speech Interfaces (SSIs) have gained attention for their ability to generate intelligible speech from non-acoustic signals. While significant progress has been made in advancing speech generation pipelines, limited work has addressed the recognition and downstream processing of synthesized speech, which often suffers from phonetic ambiguity and noise. To overcome these challenges, we propose an enhanced automatic speech recognition framework that combines a transformer-based acoustic model with a large language model (LLM) for post-processing. The transformer captures full utterance context, while the LLM ensures linguistic consistency. Experimental results show a 16% relative and 6% absolute reduction in word error rate (WER) over a 36% baseline, demonstrating substantial improvements in intelligibility for silent speech interfaces.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through Model Explanations</title>
<link>https://arxiv.org/abs/2509.04515</link>
<guid>https://arxiv.org/abs/2509.04515</guid>
<content:encoded><![CDATA[
arXiv:2509.04515v1 Announce Type: cross 
Abstract: Language models have been shown to propagate social bias through their output, particularly in the representation of gender and ethnicity. This paper investigates gender and ethnicity biases in AI-generated occupational stories. Representation biases are measured before and after applying our proposed mitigation strategy, Bias Analysis and Mitigation through Explanation (BAME), revealing improvements in demographic representation ranging from 2% to 20%. BAME leverages model-generated explanations to inform targeted prompt engineering, effectively reducing biases without modifying model parameters. By analyzing stories generated across 25 occupational groups, three large language models (Claude 3.5 Sonnet, Llama 3.1 70B Instruct, and GPT-4 Turbo), and multiple demographic dimensions, we identify persistent patterns of overrepresentation and underrepresentation linked to training data stereotypes. Our findings demonstrate that guiding models with their own internal reasoning mechanisms can significantly enhance demographic parity, thereby contributing to the development of more transparent generative AI systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation</title>
<link>https://arxiv.org/abs/2509.04534</link>
<guid>https://arxiv.org/abs/2509.04534</guid>
<content:encoded><![CDATA[
arXiv:2509.04534v1 Announce Type: cross 
Abstract: Large language models have demonstrated remarkable capabilities in biomedical natural language processing, yet their rapid growth in size and computational requirements present a major barrier to adoption in healthcare settings where data privacy precludes cloud deployment and resources are limited. In this study, we systematically evaluated the impact of quantization on 12 state-of-the-art large language models, including both general-purpose and biomedical-specific models, across eight benchmark datasets covering four key tasks: named entity recognition, relation extraction, multi-label classification, and question answering. We show that quantization substantially reduces GPU memory requirements-by up to 75%-while preserving model performance across diverse tasks, enabling the deployment of 70B-parameter models on 40GB consumer-grade GPUs. In addition, domain-specific knowledge and responsiveness to advanced prompting methods are largely maintained. These findings provide significant practical and guiding value, highlighting quantization as a practical and effective strategy for enabling the secure, local deployment of large yet high-capacity language models in biomedical contexts, bridging the gap between technical advances in AI and real-world clinical translation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Policy Adaptation via Cross-Domain Skill Diffusion</title>
<link>https://arxiv.org/abs/2509.04535</link>
<guid>https://arxiv.org/abs/2509.04535</guid>
<content:encoded><![CDATA[
arXiv:2509.04535v1 Announce Type: cross 
Abstract: In this work, we present an in-context policy adaptation (ICPAD) framework designed for long-horizon multi-task environments, exploring diffusion-based skill learning techniques in cross-domain settings. The framework enables rapid adaptation of skill-based reinforcement learning policies to diverse target domains, especially under stringent constraints on no model updates and only limited target domain data. Specifically, the framework employs a cross-domain skill diffusion scheme, where domain-agnostic prototype skills and a domain-grounded skill adapter are learned jointly and effectively from an offline dataset through cross-domain consistent diffusion processes. The prototype skills act as primitives for common behavior representations of long-horizon policies, serving as a lingua franca to bridge different domains. Furthermore, to enhance the in-context adaptation performance, we develop a dynamic domain prompting scheme that guides the diffusion-based skill adapter toward better alignment with the target domain. Through experiments with robotic manipulation in Metaworld and autonomous driving in CARLA, we show that our $\oursol$ framework achieves superior policy adaptation performance under limited target domain data conditions for various cross-domain configurations including differences in environment dynamics, agent embodiment, and task horizon.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem</title>
<link>https://arxiv.org/abs/2509.04537</link>
<guid>https://arxiv.org/abs/2509.04537</guid>
<content:encoded><![CDATA[
arXiv:2509.04537v1 Announce Type: cross 
Abstract: We investigate the emergent social dynamics of Large Language Model (LLM) agents in a spatially extended El Farol Bar problem, observing how they autonomously navigate this classic social dilemma. As a result, the LLM agents generated a spontaneous motivation to go to the bar and changed their decision making by becoming a collective. We also observed that the LLM agents did not solve the problem completely, but rather behaved more like humans. These findings reveal a complex interplay between external incentives (prompt-specified constraints such as the 60\% threshold) and internal incentives (culturally-encoded social preferences derived from pre-training), demonstrating that LLM agents naturally balance formal game-theoretic rationality with social motivations that characterize human behavior. These findings suggest that a new model of group decision making, which could not be handled in the previous game-theoretic problem setting, can be realized by LLM agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>i-Mask: An Intelligent Mask for Breath-Driven Activity Recognition</title>
<link>https://arxiv.org/abs/2509.04544</link>
<guid>https://arxiv.org/abs/2509.04544</guid>
<content:encoded><![CDATA[
arXiv:2509.04544v1 Announce Type: cross 
Abstract: The patterns of inhalation and exhalation contain important physiological signals that can be used to anticipate human behavior, health trends, and vital parameters. Human activity recognition (HAR) is fundamentally connected to these vital signs, providing deeper insights into well-being and enabling real-time health monitoring. This work presents i-Mask, a novel HAR approach that leverages exhaled breath patterns captured using a custom-developed mask equipped with integrated sensors. Data collected from volunteers wearing the mask undergoes noise filtering, time-series decomposition, and labeling to train predictive models. Our experimental results validate the effectiveness of the approach, achieving over 95\% accuracy and highlighting its potential in healthcare and fitness applications.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions</title>
<link>https://arxiv.org/abs/2509.04549</link>
<guid>https://arxiv.org/abs/2509.04549</guid>
<content:encoded><![CDATA[
arXiv:2509.04549v1 Announce Type: cross 
Abstract: Transformer-based language models excel in NLP tasks, but fine-grained control remains challenging. This paper explores methods for manipulating transformer models through principled interventions at three levels: prompts, activations, and weights. We formalize controllable text generation as an optimization problem addressable via prompt engineering, parameter-efficient fine-tuning, model editing, and reinforcement learning. We introduce a unified framework encompassing prompt-level steering, activation interventions, and weight-space edits. We analyze robustness and safety implications, including adversarial attacks and alignment mitigations. Theoretically, we show minimal weight updates can achieve targeted behavior changes with limited side-effects. Empirically, we demonstrate >90% success in sentiment control and factual edits while preserving base performance, though generalization-specificity trade-offs exist. We discuss ethical dual-use risks and the need for rigorous evaluation. This work lays groundwork for designing controllable and robust language models.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Faithfulness-guided Ensemble Interpretation of Neural Network</title>
<link>https://arxiv.org/abs/2509.04588</link>
<guid>https://arxiv.org/abs/2509.04588</guid>
<content:encoded><![CDATA[
arXiv:2509.04588v1 Announce Type: cross 
Abstract: Interpretable and faithful explanations for specific neural inferences are crucial for understanding and evaluating model behavior. Our work introduces \textbf{F}aithfulness-guided \textbf{E}nsemble \textbf{I}nterpretation (\textbf{FEI}), an innovative framework that enhances the breadth and effectiveness of faithfulness, advancing interpretability by providing superior visualization. Through an analysis of existing evaluation benchmarks, \textbf{FEI} employs a smooth approximation to elevate quantitative faithfulness scores. Diverse variations of \textbf{FEI} target enhanced faithfulness in hidden layer encodings, expanding interpretability. Additionally, we propose a novel qualitative metric that assesses hidden layer faithfulness. In extensive experiments, \textbf{FEI} surpasses existing methods, demonstrating substantial advances in qualitative visualization and quantitative faithfulness scores. Our research establishes a comprehensive framework for elevating faithfulness in neural network explanations, emphasizing both breadth and precision
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Multi-Task Learning with Learnable Weighting for Pharmacokinetic and Toxicity Prediction</title>
<link>https://arxiv.org/abs/2509.04601</link>
<guid>https://arxiv.org/abs/2509.04601</guid>
<content:encoded><![CDATA[
arXiv:2509.04601v1 Announce Type: cross 
Abstract: Prediction for ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) plays a crucial role in drug discovery and development, accelerating the screening and optimization of new drugs. Existing methods primarily rely on single-task learning (STL), which often fails to fully exploit the complementarities between tasks. Besides, it requires more computational resources while training and inference of each task independently. To address these issues, we propose a new unified Quantum-enhanced and task-Weighted Multi-Task Learning (QW-MTL) framework, specifically designed for ADMET classification tasks. Built upon the Chemprop-RDKit backbone, QW-MTL adopts quantum chemical descriptors to enrich molecular representations with additional information about the electronic structure and interactions. Meanwhile, it introduces a novel exponential task weighting scheme that combines dataset-scale priors with learnable parameters to achieve dynamic loss balancing across tasks. To the best of our knowledge, this is the first work to systematically conduct joint multi-task training across all 13 Therapeutics Data Commons (TDC) classification benchmarks, using leaderboard-style data splits to ensure a standardized and realistic evaluation setting. Extensive experimental results show that QW-MTL significantly outperforms single-task baselines on 12 out of 13 tasks, achieving high predictive performance with minimal model complexity and fast inference, demonstrating the effectiveness and efficiency of multi-task molecular learning enhanced by quantum-informed features and adaptive task weighting.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-efficient Integration of New Modalities into Large Language Models</title>
<link>https://arxiv.org/abs/2509.04606</link>
<guid>https://arxiv.org/abs/2509.04606</guid>
<content:encoded><![CDATA[
arXiv:2509.04606v1 Announce Type: cross 
Abstract: Multimodal foundation models can process several modalities. However, since the space of possible modalities is large and evolving over time, training a model from scratch to encompass all modalities is unfeasible. Moreover, integrating a modality into a pre-existing foundation model currently requires a significant amount of paired data, which is often not available for low-resource modalities. In this paper, we introduce a method for sample-efficient modality integration (SEMI) into Large Language Models (LLMs). To this end, we devise a hypernetwork that can adapt a shared projector -- placed between modality-specific encoders and an LLM -- to any modality. The hypernetwork, trained on high-resource modalities (i.e., text, speech, audio, video), is conditioned on a few samples from any arbitrary modality at inference time to generate a suitable adapter. To increase the diversity of training modalities, we artificially multiply the number of encoders through isometric transformations. We find that SEMI achieves a significant boost in sample efficiency during few-shot integration of new modalities (i.e., satellite images, astronomical images, inertial measurements, and molecules) with encoders of arbitrary embedding dimensionality. For instance, to reach the same accuracy as 32-shot SEMI, training the projector from scratch needs 64$\times$ more data. As a result, SEMI holds promise to extend the modality coverage of foundation models.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families</title>
<link>https://arxiv.org/abs/2509.04622</link>
<guid>https://arxiv.org/abs/2509.04622</guid>
<content:encoded><![CDATA[
arXiv:2509.04622v1 Announce Type: cross 
Abstract: Representational similarity metrics are fundamental tools in neuroscience and AI, yet we lack systematic comparisons of their discriminative power across model families. We introduce a quantitative framework to evaluate representational similarity measures based on their ability to separate model families-across architectures (CNNs, Vision Transformers, Swin Transformers, ConvNeXt) and training regimes (supervised vs. self-supervised). Using three complementary separability measures-dprime from signal detection theory, silhouette coefficients and ROC-AUC, we systematically assess the discriminative capacity of commonly used metrics including RSA, linear predictivity, Procrustes, and soft matching. We show that separability systematically increases as metrics impose more stringent alignment constraints. Among mapping-based approaches, soft-matching achieves the highest separability, followed by Procrustes alignment and linear predictivity. Non-fitting methods such as RSA also yield strong separability across families. These results provide the first systematic comparison of similarity metrics through a separability lens, clarifying their relative sensitivity and guiding metric choice for large-scale model and brain comparisons.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control</title>
<link>https://arxiv.org/abs/2509.04628</link>
<guid>https://arxiv.org/abs/2509.04628</guid>
<content:encoded><![CDATA[
arXiv:2509.04628v1 Announce Type: cross 
Abstract: We present an imitation learning approach for spacecraft guidance, navigation, and control(GNC) that achieves high performance from limited data. Using only 100 expert demonstrations, equivalent to 6,300 environment interactions, our method, which implements Action Chunking with Transformers (ACT), learns a control policy that maps visual and state observations to thrust and torque commands. ACT generates smoother, more consistent trajectories than a meta-reinforcement learning (meta-RL) baseline trained with 40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking with the International Space Station (ISS). We show that our approach achieves greater accuracy, smoother control, and greater sample efficiency.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schema Inference for Tabular Data Repositories Using Large Language Models</title>
<link>https://arxiv.org/abs/2509.04632</link>
<guid>https://arxiv.org/abs/2509.04632</guid>
<content:encoded><![CDATA[
arXiv:2509.04632v1 Announce Type: cross 
Abstract: Minimally curated tabular data often contain representational inconsistencies across heterogeneous sources, and are accompanied by sparse metadata. Working with such data is intimidating. While prior work has advanced dataset discovery and exploration, schema inference remains difficult when metadata are limited. We present SI-LLM (Schema Inference using Large Language Models), which infers a concise conceptual schema for tabular data using only column headers and cell values. The inferred schema comprises hierarchical entity types, attributes, and inter-type relationships. In extensive evaluation on two datasets from web tables and open data, SI-LLM achieves promising end-to-end results, as well as better or comparable results to state-of-the-art methods at each step. All source code, full prompts, and datasets of SI-LLM are available at https://github.com/PierreWoL/SILLM.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Environments for Organoid Intelligence with LLM-Automated Design and Plasticity-Based Evaluation</title>
<link>https://arxiv.org/abs/2509.04633</link>
<guid>https://arxiv.org/abs/2509.04633</guid>
<content:encoded><![CDATA[
arXiv:2509.04633v1 Announce Type: cross 
Abstract: As the complexity of artificial agents increases, the design of environments that can effectively shape their behavior and capabilities has become a critical research frontier. We propose a framework that extends this principle to a novel class of agents: biological neural networks in the form of neural organoids. This paper introduces three scalable, closed-loop virtual environments designed to train organoid-based biological agents and probe the underlying mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments with increasing complexity: (1) a conditional avoidance task, (2) a one-dimensional predator-prey scenario, and (3) a replication of the classic Pong game. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation. Furthermore, we propose a novel meta-learning approach where a Large Language Model (LLM) is used to automate the generation and optimization of experimental protocols, scaling the process of environment and curriculum design. Finally, we outline a multi-modal approach for evaluating learning by measuring synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between computational neuroscience and agent-based AI, offering a unique platform for studying embodiment, learning, and intelligence in a controlled biological substrate.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Transformer Models in Disaster Tweet Classification for Public Safety</title>
<link>https://arxiv.org/abs/2509.04650</link>
<guid>https://arxiv.org/abs/2509.04650</guid>
<content:encoded><![CDATA[
arXiv:2509.04650v1 Announce Type: cross 
Abstract: Twitter and other social media platforms have become vital sources of real time information during disasters and public safety emergencies. Automatically classifying disaster related tweets can help emergency services respond faster and more effectively. Traditional Machine Learning (ML) models such as Logistic Regression, Naive Bayes, and Support Vector Machines have been widely used for this task, but they often fail to understand the context or deeper meaning of words, especially when the language is informal, metaphorical, or ambiguous. We posit that, in this context, transformer based models can perform better than traditional ML models. In this paper, we evaluate the effectiveness of transformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for classifying disaster related tweets. These models are compared with traditional ML approaches to highlight the performance gap. Experimental results show that BERT achieved the highest accuracy (91%), significantly outperforming traditional models like Logistic Regression and Naive Bayes (both at 82%). The use of contextual embeddings and attention mechanisms allows transformer models to better understand subtle language in tweets, where traditional ML models fall short. This research demonstrates that transformer architectures are far more suitable for public safety applications, offering improved accuracy, deeper language understanding, and better generalization across real world social media text.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Transformer Architectures as Implicit Multinomial Regression</title>
<link>https://arxiv.org/abs/2509.04653</link>
<guid>https://arxiv.org/abs/2509.04653</guid>
<content:encoded><![CDATA[
arXiv:2509.04653v1 Announce Type: cross 
Abstract: Mechanistic interpretability aims to understand how internal components of modern machine learning models, such as weights, activations, and layers, give rise to the model's overall behavior. One particularly opaque mechanism is attention: despite its central role in transformer models, its mathematical underpinnings and relationship to concepts like feature polysemanticity, superposition, and model performance remain poorly understood. This paper establishes a novel connection between attention mechanisms and multinomial regression. Specifically, we show that in a fixed multinomial regression setting, optimizing over latent features yields optimal solutions that align with the dynamics induced by attention blocks. In other words, the evolution of representations through a transformer can be interpreted as a trajectory that recovers the optimal features for classification.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs</title>
<link>https://arxiv.org/abs/2509.04655</link>
<guid>https://arxiv.org/abs/2509.04655</guid>
<content:encoded><![CDATA[
arXiv:2509.04655v1 Announce Type: cross 
Abstract: We propose a novel inference-time out-of-domain (OOD) detection algorithm for specialized large language models (LLMs). Despite achieving state-of-the-art performance on in-domain tasks through fine-tuning, specialized LLMs remain vulnerable to incorrect or unreliable outputs when presented with OOD inputs, posing risks in critical applications. Our method leverages the Inductive Conformal Anomaly Detection (ICAD) framework, using a new non-conformity measure based on the model's dropout tolerance. Motivated by recent findings on polysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs exhibit higher dropout tolerance than OOD inputs. We aggregate dropout tolerance across multiple layers via a valid ensemble approach, improving detection while maintaining theoretical false alarm bounds from ICAD. Experiments with medical-specialized LLMs show that our approach detects OOD inputs better than baseline methods, with AUROC improvements of $2\%$ to $37\%$ when treating OOD datapoints as positives and in-domain test datapoints as negatives.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating NL2SQL via SQL2NL</title>
<link>https://arxiv.org/abs/2509.04657</link>
<guid>https://arxiv.org/abs/2509.04657</guid>
<content:encoded><![CDATA[
arXiv:2509.04657v1 Announce Type: cross 
Abstract: Robust evaluation in the presence of linguistic variation is key to understanding the generalization capabilities of Natural Language to SQL (NL2SQL) models, yet existing benchmarks rarely address this factor in a systematic or controlled manner. We propose a novel schema-aligned paraphrasing framework that leverages SQL-to-NL (SQL2NL) to automatically generate semantically equivalent, lexically diverse queries while maintaining alignment with the original schema and intent. This enables the first targeted evaluation of NL2SQL robustness to linguistic variation in isolation-distinct from prior work that primarily investigates ambiguity or schema perturbations. Our analysis reveals that state-of-the-art models are far more brittle than standard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop in execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries, while LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to 42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We also find that robustness degradation varies significantly with query complexity, dataset, and domain -- highlighting the need for evaluation frameworks that explicitly measure linguistic generalization to ensure reliable performance in real-world settings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation</title>
<link>https://arxiv.org/abs/2509.04669</link>
<guid>https://arxiv.org/abs/2509.04669</guid>
<content:encoded><![CDATA[
arXiv:2509.04669v1 Announce Type: cross 
Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs) have challenged the dominance of Convolutional Neural Networks (CNNs) in computer vision. ViTs excel at capturing global context, and SSMs like Mamba offer linear complexity for long sequences, yet they do not capture fine-grained local features as effectively as CNNs. Conversely, CNNs possess strong inductive biases for local features but lack the global reasoning capabilities of transformers and Mamba. To bridge this gap, we introduce \textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a hierarchical structure with convolutional blocks in its early stages to extract rich local features. These convolutional blocks are then processed by later stages incorporating multi-directional Mamba blocks designed to efficiently model long-range dependencies and global context. This hybrid design allows for superior feature representation while maintaining linear complexity with respect to image resolution. We demonstrate VCMamba's effectiveness through extensive experiments on ImageNet-1K classification and ADE20K semantic segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K, surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains 47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing 62% fewer parameters. Code is available at https://github.com/Wertyuui345/VCMamba.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine Bioacoustic Monitoring</title>
<link>https://arxiv.org/abs/2509.04682</link>
<guid>https://arxiv.org/abs/2509.04682</guid>
<content:encoded><![CDATA[
arXiv:2509.04682v1 Announce Type: cross 
Abstract: Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal data for long-term ecological analysis, but intrinsic noise and complex signal dependencies hinder model stability and generalization. Multilayered windowing has improved target sound localization, yet variability from shifting ambient noise, diverse propagation effects, and mixed biological and anthropogenic sources demands robust architectures and rigorous evaluation. We introduce GetNetUPAM, a hierarchical nested cross-validation framework designed to quantify model stability under ecologically realistic variability. Data are partitioned into distinct site-year segments, preserving recording heterogeneity and ensuring each validation fold reflects a unique environmental subset, reducing overfitting to localized noise and sensor artifacts. Site-year blocking enforces evaluation against genuine environmental diversity, while standard cross-validation on random subsets measures generalization across UPAM's full signal distribution, a dimension absent from current benchmarks. Using GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution Pooling and Attention Network (ARPA-N), a neural architecture for irregular spectrogram dimensions. Adaptive pooling with spatial attention extends the receptive field, capturing global context without excessive parameters. Under GetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet baselines and a log2-scale order-of-magnitude drop in variability across all metrics, enabling consistent detection across site-year folds and advancing scalable, accurate bioacoustic monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs</title>
<link>https://arxiv.org/abs/2509.04696</link>
<guid>https://arxiv.org/abs/2509.04696</guid>
<content:encoded><![CDATA[
arXiv:2509.04696v1 Announce Type: cross 
Abstract: Knowledge graphs (KGs) are foundational to many AI applications, but maintaining their freshness and completeness remains costly. We present ODKE+, a production-grade system that automatically extracts and ingests millions of open-domain facts from web sources with high precision. ODKE+ combines modular components into a scalable pipeline: (1) the Extraction Initiator detects missing or stale facts, (2) the Evidence Retriever collects supporting documents, (3) hybrid Knowledge Extractors apply both pattern-based rules and ontology-guided prompting for large language models (LLMs), (4) a lightweight Grounder validates extracted facts using a second LLM, and (5) the Corroborator ranks and normalizes candidate facts for ingestion. ODKE+ dynamically generates ontology snippets tailored to each entity type to align extractions with schema constraints, enabling scalable, type-consistent fact extraction across 195 predicates. The system supports batch and streaming modes, processing over 9 million Wikipedia pages and ingesting 19 million high-confidence facts with 98.8% precision. ODKE+ significantly improves coverage over traditional methods, achieving up to 48% overlap with third-party KGs and reducing update lag by 50 days on average. Our deployment demonstrates that LLM-based extraction, grounded in ontological structure and verification workflows, can deliver trustworthiness, production-scale knowledge ingestion with broad real-world applicability. A recording of the system demonstration is included with the submission and is also available at https://youtu.be/UcnE3_GsTWs.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.04712</link>
<guid>https://arxiv.org/abs/2509.04712</guid>
<content:encoded><![CDATA[
arXiv:2509.04712v1 Announce Type: cross 
Abstract: Automated vehicle control using reinforcement learning (RL) has attracted significant attention due to its potential to learn driving policies through environment interaction. However, RL agents often face training challenges in sample efficiency and effective exploration, making it difficult to discover an optimal driving strategy. To address these issues, we propose guiding the RL driving agent with a demonstration policy that need not be a highly optimized or expert-level controller. Specifically, we integrate a rule-based lane change controller with the Soft Actor Critic (SAC) algorithm to enhance exploration and learning efficiency. Our approach demonstrates improved driving performance and can be extended to other driving scenarios that can similarly benefit from demonstration-based guidance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced Question Answering</title>
<link>https://arxiv.org/abs/2509.04716</link>
<guid>https://arxiv.org/abs/2509.04716</guid>
<content:encoded><![CDATA[
arXiv:2509.04716v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucination in Large Language Models (LLMs) by incorporating external data, with Knowledge Graphs (KGs) offering crucial information for question answering. Traditional Knowledge Graph Question Answering (KGQA) methods rely on semantic parsing, which typically retrieves knowledge strictly necessary for answer generation, thus often suffer from low coverage due to rigid schema requirements and semantic ambiguity. We present KERAG, a novel KG-based RAG pipeline that enhances QA coverage by retrieving a broader subgraph likely to contain relevant information. Our retrieval-filtering-summarization approach, combined with fine-tuned LLMs for Chain-of-Thought reasoning on knowledge sub-graphs, reduces noises and improves QA for both simple and complex questions. Experiments demonstrate that KERAG surpasses state-of-the-art solutions by about 7% in quality and exceeds GPT-4o (Tool) by 10-21%.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoVeR: Conformal Calibration for Versatile and Reliable Autoregressive Next-Token Prediction</title>
<link>https://arxiv.org/abs/2509.04733</link>
<guid>https://arxiv.org/abs/2509.04733</guid>
<content:encoded><![CDATA[
arXiv:2509.04733v1 Announce Type: cross 
Abstract: Autoregressive pre-trained models combined with decoding methods have achieved impressive performance on complex reasoning tasks. While mainstream decoding strategies such as beam search can generate plausible candidate sets, they often lack provable coverage guarantees, and struggle to effectively balance search efficiency with the need for versatile trajectories, particularly those involving long-tail sequences that are essential in certain real-world applications. To address these limitations, we propose \textsc{CoVeR}, a novel model-free decoding strategy wihtin the conformal prediction framework that simultaneously maintains a compact search space and ensures high coverage probability over desirable trajectories. Theoretically, we establish a PAC-style generalization bound, guaranteeing that \textsc{CoVeR} asymptotically achieves a coverage rate of at least $1 - \alpha$ for any target level $\alpha \in (0,1)$.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning</title>
<link>https://arxiv.org/abs/2509.04734</link>
<guid>https://arxiv.org/abs/2509.04734</guid>
<content:encoded><![CDATA[
arXiv:2509.04734v1 Announce Type: cross 
Abstract: The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We present Beyond I-Con, a framework that enables systematic discovery of novel loss functions by exploring alternative statistical divergences and similarity kernels. Key findings: (1) on unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art results by modifying the PMI algorithm to use total variation (TV) distance; (2) on supervised contrastive learning, we outperform the standard approach by using TV and a distance-based similarity kernel instead of KL and an angular kernel; (3) on dimensionality reduction, we achieve superior qualitative results and better performance on downstream tasks than SNE by replacing KL with a bounded f-divergence. Our results highlight the importance of considering divergence and similarity kernel choices in representation learning optimization.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization</title>
<link>https://arxiv.org/abs/2509.04735</link>
<guid>https://arxiv.org/abs/2509.04735</guid>
<content:encoded><![CDATA[
arXiv:2509.04735v1 Announce Type: cross 
Abstract: Recent advances in vision foundation models, such as the Segment Anything Model (SAM) and its successor SAM2, have achieved state-of-the-art performance on general image segmentation benchmarks. However, these models struggle in adverse weather conditions where visual ambiguity is high, largely due to their lack of uncertainty quantification. Inspired by progress in medical imaging, where uncertainty-aware training has improved reliability in ambiguous cases, we investigate two approaches to enhance segmentation robustness for autonomous driving. First, we introduce a multi-step finetuning procedure for SAM2 that incorporates uncertainty metrics directly into the loss function, improving overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter (UAT), originally designed for medical image segmentation, to driving contexts. We evaluate both methods on CamVid, BDD100K, and GTA driving datasets. Experiments show that UAT-SAM outperforms standard SAM in extreme weather, while SAM2 with uncertainty-aware loss achieves improved performance across diverse driving scenes. These findings underscore the value of explicit uncertainty modeling for safety-critical autonomous driving in challenging environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching</title>
<link>https://arxiv.org/abs/2509.04752</link>
<guid>https://arxiv.org/abs/2509.04752</guid>
<content:encoded><![CDATA[
arXiv:2509.04752v1 Announce Type: cross 
Abstract: This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM health coaching system that integrates personalized machine learning and retrieval-augmented generation to deliver adaptive, evidence-based guidance. SePA combines: (1) Individualized models predicting daily stress, soreness, and injury risk from wearable sensor data (28 users, 1260 data points); and (2) A retrieval module that grounds LLM-generated feedback in expert-vetted web content to ensure contextual relevance and reliability. Our predictive models, evaluated with rolling-origin cross-validation and group k-fold cross-validation show that personalized models outperform generalized baselines. In a pilot expert study (n=4), SePA's retrieval-based advice was preferred over a non-retrieval baseline, yielding meaningful practical effect (Cliff's $\delta$=0.3, p=0.05). We also quantify latency performance trade-offs between response quality and speed, offering a transparent blueprint for next-generation, trustworthy personal health informatics systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning</title>
<link>https://arxiv.org/abs/2509.04753</link>
<guid>https://arxiv.org/abs/2509.04753</guid>
<content:encoded><![CDATA[
arXiv:2509.04753v1 Announce Type: cross 
Abstract: Natural language processing (NLP) is a key technology to extract important patient information from clinical narratives to support healthcare applications. The rapid development of large language models (LLMs) has revolutionized many NLP tasks in the clinical domain, yet their optimal use in patient information extraction tasks requires further exploration. This study examines LLMs' effectiveness in patient information extraction, focusing on LLM architectures, fine-tuning strategies, and multi-task instruction tuning techniques for developing robust and generalizable patient information extraction systems. This study aims to explore key concepts of using LLMs for clinical concept and relation extraction tasks, including: (1) encoder-only or decoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT) algorithms, and (3) multi-task instruction tuning on few-shot learning performance. We benchmarked a suite of LLMs, including encoder-based LLMs (BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1, GatorTronLlama), across five datasets. We compared traditional full-size fine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning framework that combines both tasks across four datasets to evaluate the zero-shot and few-shot learning performance using the leave-one-dataset-out strategy.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery</title>
<link>https://arxiv.org/abs/2509.04757</link>
<guid>https://arxiv.org/abs/2509.04757</guid>
<content:encoded><![CDATA[
arXiv:2509.04757v1 Announce Type: cross 
Abstract: Rapid and accurate post-hurricane damage assessment is vital for disaster response and recovery. Yet existing CNN-based methods struggle to capture multi-scale spatial features and to distinguish visually similar or co-occurring damage types. To address these issues, we propose MCANet, a multi-label classification framework that learns multi-scale representations and adaptively attends to spatially relevant regions for each damage category. MCANet employs a Res2Net-based hierarchical backbone to enrich spatial context across scales and a multi-head class-specific residual attention module to enhance discrimination. Each attention branch focuses on different spatial granularities, balancing local detail with global context. We evaluate MCANet on the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael. MCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet, Res2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads, performance further improves to 92.35%, boosting average precision for challenging classes such as Road Blocked by over 6%. Class activation mapping confirms MCANet's ability to localize damage-relevant regions, supporting interpretability. Outputs from MCANet can inform post-disaster risk mapping, emergency routing, and digital twin-based disaster response. Future work could integrate disaster-specific knowledge graphs and multimodal large language models to improve adaptability to unseen disasters and enrich semantic understanding for real-world decision-making.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph</title>
<link>https://arxiv.org/abs/2509.04772</link>
<guid>https://arxiv.org/abs/2509.04772</guid>
<content:encoded><![CDATA[
arXiv:2509.04772v1 Announce Type: cross 
Abstract: Timely and accurate floodwater depth estimation is critical for road accessibility and emergency response. While recent computer vision methods have enabled flood detection, they suffer from both accuracy limitations and poor generalization due to dependence on fixed object detectors and task-specific training. To enable accurate depth estimation that can generalize across diverse flood scenarios, this paper presents FloodVision, a zero-shot framework that combines the semantic reasoning abilities of the foundation vision-language model GPT-4o with a structured domain knowledge graph. The knowledge graph encodes canonical real-world dimensions for common urban objects including vehicles, people, and infrastructure elements to ground the model's reasoning in physical reality. FloodVision dynamically identifies visible reference objects in RGB images, retrieves verified heights from the knowledge graph to mitigate hallucination, estimates submergence ratios, and applies statistical outlier filtering to compute final depth values. Evaluated on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and surpassing prior CNN-based methods. The system generalizes well across varying scenes and operates in near real-time, making it suitable for future integration into digital twin platforms and citizen-reporting apps for smart city flood resilience.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoders Laugh as Loud as Encoders</title>
<link>https://arxiv.org/abs/2509.04779</link>
<guid>https://arxiv.org/abs/2509.04779</guid>
<content:encoded><![CDATA[
arXiv:2509.04779v1 Announce Type: cross 
Abstract: From the dawn of the computer, Allen Turing dreamed of a robot that could communicate using language as a human being. The recent advances in the field of Large Language Models (LLMs) shocked the scientific community when a single model can apply for various natural language processing (NLP) tasks, while the output results are sometimes even better than most human communication skills. Models such as GPT, Claude, Grok, etc. have left their mark on the scientific community. However, it is unclear how much these models understand what they produce, especially in a nuanced theme such as humor. The question of whether computers understand humor is still open (among the decoders, the latest to be checked was GPT-2). We addressed this issue in this paper; we have showed that a fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well as the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models</title>
<link>https://arxiv.org/abs/2509.04781</link>
<guid>https://arxiv.org/abs/2509.04781</guid>
<content:encoded><![CDATA[
arXiv:2509.04781v1 Announce Type: cross 
Abstract: When given the option, will LLMs choose to leave the conversation (bail)? We investigate this question by giving models the option to bail out of interactions using three different bail methods: a bail tool the model can call, a bail string the model can output, and a bail prompt that asks the model if it wants to leave. On continuations of real world data (Wildchat and ShareGPT), all three of these bail methods find models will bail around 0.28-32\% of the time (depending on the model and bail method). However, we find that bail rates can depend heavily on the model used for the transcript, which means we may be overestimating real world bail rates by up to 4x. If we also take into account false positives on bail prompt (22\%), we estimate real world bail rates range from 0.06-7\%, depending on the model and bail method. We use observations from our continuations of real world data to construct a non-exhaustive taxonomy of bail cases, and use this taxonomy to construct BailBench: a representative synthetic dataset of situations where some models bail. We test many models on this dataset, and observe some bail behavior occurring for most of them. Bail rates vary substantially between models, bail methods, and prompt wordings. Finally, we study the relationship between refusals and bails. We find: 1) 0-13\% of continuations of real world conversations resulted in a bail without a corresponding refusal 2) Jailbreaks tend to decrease refusal rates, but increase bail rates 3) Refusal abliteration increases no-refuse bail rates, but only for some bail methods 4) Refusal rate on BailBench does not appear to predict bail rate.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VARMA-Enhanced Transformer for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.04782</link>
<guid>https://arxiv.org/abs/2509.04782</guid>
<content:encoded><![CDATA[
arXiv:2509.04782v1 Announce Type: cross 
Abstract: Transformer-based models have significantly advanced time series forecasting. Recent work, like the Cross-Attention-only Time Series transformer (CATS), shows that removing self-attention can make the model more accurate and efficient. However, these streamlined architectures may overlook the fine-grained, local temporal dependencies effectively captured by classical statistical models like Vector AutoRegressive Moving Average model (VARMA). To address this gap, we propose VARMAformer, a novel architecture that synergizes the efficiency of a cross-attention-only framework with the principles of classical time series analysis. Our model introduces two key innovations: (1) a dedicated VARMA-inspired Feature Extractor (VFE) that explicitly models autoregressive (AR) and moving-average (MA) patterns at the patch level, and (2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal gate to make queries more context-aware. By fusing these classical insights into a modern backbone, VARMAformer captures both global, long-range dependencies and local, statistical structures. Through extensive experiments on widely-used benchmark datasets, we demonstrate that our model consistently outperforms existing state-of-the-art methods. Our work validates the significant benefit of integrating classical statistical insights into modern deep learning frameworks for time series forecasting.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Diversity in Large Language Models via Determinantal Point Processes</title>
<link>https://arxiv.org/abs/2509.04784</link>
<guid>https://arxiv.org/abs/2509.04784</guid>
<content:encoded><![CDATA[
arXiv:2509.04784v1 Announce Type: cross 
Abstract: Supervised fine-tuning and reinforcement learning are two popular methods for post-training large language models (LLMs). While improving the model's performance on downstream tasks, they often reduce the model's output diversity, leading to narrow, canonical responses. Existing methods to enhance diversity are limited, either by operating at inference time or by focusing on lexical differences. We propose a novel training method named DQO based on determinantal point processes (DPPs) to jointly optimize LLMs for quality and semantic diversity. Our approach samples and embeds a group of responses for each prompt, then uses the determinant of a kernel-based similarity matrix to measure diversity as the volume spanned by the embeddings of these responses. Experiments across instruction-following, summarization, story generation, and reasoning tasks demonstrate that our method substantially improves semantic diversity without sacrificing model quality.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Unlearning: Efficient Node Removal in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.04785</link>
<guid>https://arxiv.org/abs/2509.04785</guid>
<content:encoded><![CDATA[
arXiv:2509.04785v1 Announce Type: cross 
Abstract: With increasing concerns about privacy attacks and potential sensitive information leakage, researchers have actively explored methods to efficiently remove sensitive training data and reduce privacy risks in graph neural network (GNN) models. Node unlearning has emerged as a promising technique for protecting the privacy of sensitive nodes by efficiently removing specific training node information from GNN models. However, existing node unlearning methods either impose restrictions on the GNN structure or do not effectively utilize the graph topology for node unlearning. Some methods even compromise the graph's topology, making it challenging to achieve a satisfactory performance-complexity trade-off. To address these issues and achieve efficient unlearning for training node removal in GNNs, we propose three novel node unlearning methods: Class-based Label Replacement, Topology-guided Neighbor Mean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among these methods, Topology-guided Neighbor Mean Posterior Probability and Class-consistent Neighbor Node Filtering effectively leverage the topological features of the graph, resulting in more effective node unlearning. To validate the superiority of our proposed methods in node unlearning, we conducted experiments on three benchmark datasets. The evaluation criteria included model utility, unlearning utility, and unlearning efficiency. The experimental results demonstrate the utility and efficiency of the proposed methods and illustrate their superiority compared to state-of-the-art node unlearning methods. Overall, the proposed methods efficiently remove sensitive training nodes and protect the privacy information of sensitive nodes in GNNs. The findings contribute to enhancing the privacy and security of GNN models and provide valuable insights into the field of node unlearning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images</title>
<link>https://arxiv.org/abs/2509.04800</link>
<guid>https://arxiv.org/abs/2509.04800</guid>
<content:encoded><![CDATA[
arXiv:2509.04800v1 Announce Type: cross 
Abstract: Skin diseases are among the most prevalent health concerns worldwide, yet conventional diagnostic methods are often costly, complex, and unavailable in low-resource settings. Automated classification using deep learning has emerged as a promising alternative, but existing studies are mostly limited to dermoscopic datasets and a narrow range of disease classes. In this work, we curate a large dataset of over 50 skin disease categories captured with mobile devices, making it more representative of real-world conditions. We evaluate multiple convolutional neural networks and Transformer-based architectures, demonstrating that Transformer models, particularly the Swin Transformer, achieve superior performance by effectively capturing global contextual features. To enhance interpretability, we incorporate Gradient-weighted Class Activation Mapping (Grad-CAM), which highlights clinically relevant regions and provides transparency in model predictions. Our results underscore the potential of Transformer-based approaches for mobile-acquired skin lesion classification, paving the way toward accessible AI-assisted dermatological screening and early diagnosis in resource-limited environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design</title>
<link>https://arxiv.org/abs/2509.04805</link>
<guid>https://arxiv.org/abs/2509.04805</guid>
<content:encoded><![CDATA[
arXiv:2509.04805v1 Announce Type: cross 
Abstract: Modern fronthaul links in wireless systems must transport high-dimensional signals under stringent bandwidth and latency constraints, which makes compression indispensable. Traditional strategies such as compressed sensing, scalar quantization, and fixed-codec pipelines often rely on restrictive priors, degrade sharply at high compression ratios, and are hard to tune across channels and deployments. Recent progress in Artificial Intelligence (AI) has brought end-to-end learned transforms, vector and hierarchical quantization, and learned entropy models that better exploit the structure of Channel State Information(CSI), precoding matrices, I/Q samples, and LLRs. This paper first surveys AI-driven compression techniques and then provides a focused analysis of two representative high-compression routes: CSI feedback with end-to-end learning and Resource Block (RB) granularity precoding optimization combined with compression. Building on these insights, we propose a fronthaul compression strategy tailored to cell-free architectures. The design targets high compression with controlled performance loss, supports RB-level rate adaptation, and enables low-latency inference suitable for centralized cooperative transmission in next-generation networks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution</title>
<link>https://arxiv.org/abs/2509.04824</link>
<guid>https://arxiv.org/abs/2509.04824</guid>
<content:encoded><![CDATA[
arXiv:2509.04824v1 Announce Type: cross 
Abstract: Recently, Mamba-based methods, with its advantage in long-range information modeling and linear complexity, have shown great potential in optimizing both computational cost and performance of light field image super-resolution (LFSR). However, current multi-directional scanning strategies lead to inefficient and redundant feature extraction when applied to complex LF data. To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS) strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to achieve more efficient and precise feature extraction. Furthermore, we propose a dual-stage modeling strategy to address the limitation of state space in preserving spatial-angular and disparity information, thereby enabling a more comprehensive exploration of non-local spatial-angular correlations. Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar feature refinement. Building upon meticulously designed modules and strategies, we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates the strengths of Mamba and Transformer models for LFSR, enabling comprehensive information exploration across spatial, angular, and epipolar-plane domains. Experimental results demonstrate that LFMT significantly outperforms current state-of-the-art methods in LFSR, achieving substantial improvements in performance while maintaining low computational complexity on both real-word and synthetic LF datasets.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</title>
<link>https://arxiv.org/abs/2509.04833</link>
<guid>https://arxiv.org/abs/2509.04833</guid>
<content:encoded><![CDATA[
arXiv:2509.04833v1 Announce Type: cross 
Abstract: Recent advances in visual grounding have largely shifted away from traditional proposal-based two-stage frameworks due to their inefficiency and high computational complexity, favoring end-to-end direct reference paradigms. However, these methods rely exclusively on the referred target for supervision, overlooking the potential benefits of prominent prospective targets. Moreover, existing approaches often fail to incorporate multi-granularity discrimination, which is crucial for robust object identification in complex scenarios. To address these limitations, we propose PropVG, an end-to-end proposal-based framework that, to the best of our knowledge, is the first to seamlessly integrate foreground object proposal generation with referential object comprehension without requiring additional detectors. Furthermore, we introduce a Contrastive-based Refer Scoring (CRS) module, which employs contrastive learning at both sentence and word levels to enhance the capability in understanding and distinguishing referred objects. Additionally, we design a Multi-granularity Target Discrimination (MTD) module that fuses object- and semantic-level information to improve the recognition of absent targets. Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO (REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and models are available at https://github.com/Dmmm1997/PropVG.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REMOTE: A Unified Multimodal Relation Extraction Framework with Multilevel Optimal Transport and Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2509.04844</link>
<guid>https://arxiv.org/abs/2509.04844</guid>
<content:encoded><![CDATA[
arXiv:2509.04844v1 Announce Type: cross 
Abstract: Multimodal relation extraction (MRE) is a crucial task in the fields of Knowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge graph construction. However, existing methods are typically limited to extracting a single type of relational triplet, which restricts their ability to extract triplets beyond the specified types. Directly combining these methods fails to capture dynamic cross-modal interactions and introduces significant computational redundancy. Therefore, we propose a novel \textit{unified multimodal Relation Extraction framework with Multilevel Optimal Transport and mixture-of-Experts}, termed REMOTE, which can simultaneously extract intra-modal and inter-modal relations between textual entities and visual objects. To dynamically select optimal interaction features for different types of relational triplets, we introduce mixture-of-experts mechanism, ensuring the most relevant modality information is utilized. Additionally, considering that the inherent property of multilayer sequential encoding in existing encoders often leads to the loss of low-level information, we adopt a multilevel optimal transport fusion module to preserve low-level features while maintaining multilayer encoding, yielding more expressive representations. Correspondingly, we also create a Unified Multimodal Relation Extraction (UMRE) dataset to evaluate the effectiveness of our framework, encompassing diverse cases where the head and tail entities can originate from either text or image. Extensive experiments show that REMOTE effectively extracts various types of relational triplets and achieves state-of-the-art performanc on almost all metrics across two other public MRE datasets. We release our resources at https://github.com/Nikol-coder/REMOTE.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing</title>
<link>https://arxiv.org/abs/2509.04853</link>
<guid>https://arxiv.org/abs/2509.04853</guid>
<content:encoded><![CDATA[
arXiv:2509.04853v1 Announce Type: cross 
Abstract: End-to-end autonomous driving remains constrained by the need to generate multi-modal actions, maintain temporal stability, and generalize across diverse scenarios. Existing methods often collapse multi-modality, struggle with long-horizon consistency, or lack modular adaptability. This paper presents KDP, a knowledge-driven diffusion policy that integrates generative diffusion modeling with a sparse mixture-of-experts routing mechanism. The diffusion component generates temporally coherent and multi-modal action sequences, while the expert routing mechanism activates specialized and reusable experts according to context, enabling modular knowledge composition. Extensive experiments across representative driving scenarios demonstrate that KDP achieves consistently higher success rates, reduced collision risk, and smoother control compared to prevailing paradigms. Ablation studies highlight the effectiveness of sparse expert activation and the Transformer backbone, and activation analyses reveal structured specialization and cross-scenario reuse of experts. These results establish diffusion with expert routing as a scalable and interpretable paradigm for knowledge-driven end-to-end autonomous driving.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Paradox of Doom: Acknowledging Extinction Risk Reduces the Incentive to Prevent It</title>
<link>https://arxiv.org/abs/2509.04855</link>
<guid>https://arxiv.org/abs/2509.04855</guid>
<content:encoded><![CDATA[
arXiv:2509.04855v1 Announce Type: cross 
Abstract: We investigate the salience of extinction risk as a source of impatience. Our framework distinguishes between human extinction risk and individual mortality risk while allowing for various degrees of intergenerational altruism. Additionally, we consider the evolutionarily motivated "selfish gene" perspective. We find that the risk of human extinction is an indispensable component of the discount rate, whereas individual mortality risk can be hedged against - partially or fully, depending on the setup - through human reproduction. Overall, we show that in the face of extinction risk, people become more impatient rather than more farsighted. Thus, the greater the threat of extinction, the less incentive there is to invest in avoiding it. Our framework can help explain why humanity consistently underinvests in mitigation of catastrophic risks, ranging from climate change mitigation, via pandemic prevention, to addressing the emerging risks of transformative artificial intelligence.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models</title>
<link>https://arxiv.org/abs/2509.04889</link>
<guid>https://arxiv.org/abs/2509.04889</guid>
<content:encoded><![CDATA[
arXiv:2509.04889v1 Announce Type: cross 
Abstract: Advances in computer vision have opened new avenues for clinical applications, particularly in computerized exposure therapy where visual stimuli can be dynamically adjusted based on patient responses. As a critical step toward such adaptive systems, we investigated whether pretrained computer vision models can accurately predict fear levels from spider-related images. We adapted three diverse models using transfer learning to predict human fear ratings (on a 0-100 scale) from a standardized dataset of 313 images. The models were evaluated using cross-validation, achieving an average mean absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis revealed that reducing the dataset size significantly harmed performance, though further increases yielded no substantial gains. Explainability assessments showed the models' predictions were based on spider-related features. A category-wise error analysis further identified visual conditions associated with higher errors (e.g., distant views and artificial/painted spiders). These findings demonstrate the potential of explainable computer vision models in predicting fear ratings, highlighting the importance of both model explainability and a sufficient dataset size for developing effective emotion-aware therapeutic technologies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLaMo 2 Technical Report</title>
<link>https://arxiv.org/abs/2509.04897</link>
<guid>https://arxiv.org/abs/2509.04897</guid>
<content:encoded><![CDATA[
arXiv:2509.04897v1 Announce Type: cross 
Abstract: In this report, we introduce PLaMo 2, a series of Japanese-focused large language models featuring a hybrid Samba-based architecture that transitions to full attention via continual pre-training to support 32K token contexts. Training leverages extensive synthetic corpora to overcome data scarcity, while computational efficiency is achieved through weight reuse and structured pruning. This efficient pruning methodology produces an 8B model that achieves performance comparable to our previous 100B model. Post-training further refines the models using a pipeline of supervised fine-tuning (SFT) and direct preference optimization (DPO), enhanced by synthetic Japanese instruction data and model merging techniques. Optimized for inference using vLLM and quantization with minimal accuracy loss, the PLaMo 2 models achieve state-of-the-art results on Japanese benchmarks, outperforming similarly-sized open models in instruction-following, language fluency, and Japanese-specific knowledge.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial intelligence for representing and characterizing quantum systems</title>
<link>https://arxiv.org/abs/2509.04923</link>
<guid>https://arxiv.org/abs/2509.04923</guid>
<content:encoded><![CDATA[
arXiv:2509.04923v1 Announce Type: cross 
Abstract: Efficient characterization of large-scale quantum systems, especially those produced by quantum analog simulators and megaquop quantum computers, poses a central challenge in quantum science due to the exponential scaling of the Hilbert space with respect to system size. Recent advances in artificial intelligence (AI), with its aptitude for high-dimensional pattern recognition and function approximation, have emerged as a powerful tool to address this challenge. A growing body of research has leveraged AI to represent and characterize scalable quantum systems, spanning from theoretical foundations to experimental realizations. Depending on how prior knowledge and learning architectures are incorporated, the integration of AI into quantum system characterization can be categorized into three synergistic paradigms: machine learning, and, in particular, deep learning and language models. This review discusses how each of these AI paradigms contributes to two core tasks in quantum systems characterization: quantum property prediction and the construction of surrogates for quantum states. These tasks underlie diverse applications, from quantum certification and benchmarking to the enhancement of quantum algorithms and the understanding of strongly correlated phases of matter. Key challenges and open questions are also discussed, together with future prospects at the interface of AI and quantum science.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation</title>
<link>https://arxiv.org/abs/2509.04970</link>
<guid>https://arxiv.org/abs/2509.04970</guid>
<content:encoded><![CDATA[
arXiv:2509.04970v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) agents can learn to solve complex tasks from visual inputs, but generalizing these learned skills to new environments remains a major challenge in RL application, especially robotics. While data augmentation can improve generalization, it often compromises sample efficiency and training stability. This paper introduces DeGuV, an RL framework that enhances both generalization and sample efficiency. In specific, we leverage a learnable masker network that produces a mask from the depth input, preserving only critical visual information while discarding irrelevant pixels. Through this, we ensure that our RL agents focus on essential features, improving robustness under data augmentation. In addition, we incorporate contrastive learning and stabilize Q-value estimation under augmentation to further enhance sample efficiency and training stability. We evaluate our proposed method on the RL-ViGen benchmark using the Franka Emika robot and demonstrate its effectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV outperforms state-of-the-art methods in both generalization and sample efficiency while also improving interpretability by highlighting the most relevant regions in the visual input
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring an implementation of quantum learning pipeline for support vector machines</title>
<link>https://arxiv.org/abs/2509.04983</link>
<guid>https://arxiv.org/abs/2509.04983</guid>
<content:encoded><![CDATA[
arXiv:2509.04983v1 Announce Type: cross 
Abstract: This work presents a fully quantum approach to support vector machine (SVM) learning by integrating gate-based quantum kernel methods with quantum annealing-based optimization. We explore the construction of quantum kernels using various feature maps and qubit configurations, evaluating their suitability through Kernel-Target Alignment (KTA). The SVM dual problem is reformulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem, enabling its solution via quantum annealers. Our experiments demonstrate that a high degree of alignment in the kernel and an appropriate regularization parameter lead to competitive performance, with the best model achieving an F1-score of 90%. These results highlight the feasibility of an end-to-end quantum learning pipeline and the potential of hybrid quantum architectures in quantum high-performance computing (QHPC) contexts.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Resolution Global Land Surface Temperature Retrieval via a Coupled Mechanism-Machine Learning Framework</title>
<link>https://arxiv.org/abs/2509.04991</link>
<guid>https://arxiv.org/abs/2509.04991</guid>
<content:encoded><![CDATA[
arXiv:2509.04991v1 Announce Type: cross 
Abstract: Land surface temperature (LST) is vital for land-atmosphere interactions and climate processes. Accurate LST retrieval remains challenging under heterogeneous land cover and extreme atmospheric conditions. Traditional split window (SW) algorithms show biases in humid environments; purely machine learning (ML) methods lack interpretability and generalize poorly with limited data. We propose a coupled mechanism model-ML (MM-ML) framework integrating physical constraints with data-driven learning for robust LST retrieval. Our approach fuses radiative transfer modeling with data components, uses MODTRAN simulations with global atmospheric profiles, and employs physics-constrained optimization. Validation against 4,450 observations from 29 global sites shows MM-ML achieves MAE=1.84K, RMSE=2.55K, and R-squared=0.966, outperforming conventional methods. Under extreme conditions, MM-ML reduces errors by over 50%. Sensitivity analysis indicates LST estimates are most sensitive to sensor radiance, then water vapor, and less to emissivity, with MM-ML showing superior stability. These results demonstrate the effectiveness of our coupled modeling strategy for retrieving geophysical parameters. The MM-ML framework combines physical interpretability with nonlinear modeling capacity, enabling reliable LST retrieval in complex environments and supporting climate monitoring and ecosystem studies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of Dual-Loop Edge-Terminal Collaboration</title>
<link>https://arxiv.org/abs/2509.04993</link>
<guid>https://arxiv.org/abs/2509.04993</guid>
<content:encoded><![CDATA[
arXiv:2509.04993v1 Announce Type: cross 
Abstract: The ubiquitous computing resources in 6G networks provide ideal environments for the fusion of large language models (LLMs) and intelligent services through the agent framework. With auxiliary modules and planning cores, LLM-enabled agents can autonomously plan and take actions to deal with diverse environment semantics and user intentions. However, the limited resources of individual network devices significantly hinder the efficient operation of LLM-enabled agents with complex tool calls, highlighting the urgent need for efficient multi-level device collaborations. To this end, the framework and method of the LLM-enabled multi-agent system with dual-loop terminal-edge collaborations are proposed in 6G networks. Firstly, the outer loop consists of the iterative collaborations between the global agent and multiple sub-agents deployed on edge servers and terminals, where the planning capability is enhanced through task decomposition and parallel sub-task distribution. Secondly, the inner loop utilizes sub-agents with dedicated roles to circularly reason, execute, and replan the sub-task, and the parallel tool calling generation with offloading strategies is incorporated to improve efficiency. The improved task planning capability and task execution efficiency are validated through the conducted case study in 6G-supported urban safety governance. Finally, the open challenges and future directions are thoroughly analyzed in 6G networks, accelerating the advent of the 6G era.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.04999</link>
<guid>https://arxiv.org/abs/2509.04999</guid>
<content:encoded><![CDATA[
arXiv:2509.04999v1 Announce Type: cross 
Abstract: Advanced Persistent Threats (APTs) present a considerable challenge to cybersecurity due to their stealthy, long-duration nature. Traditional supervised learning methods typically require large amounts of labeled data, which is often scarce in real-world scenarios. This paper introduces a novel approach that combines AutoEncoders for anomaly detection with active learning to iteratively enhance APT detection. By selectively querying an oracle for labels on uncertain or ambiguous samples, our method reduces labeling costs while improving detection accuracy, enabling the model to effectively learn with minimal data and reduce reliance on extensive manual labeling. We present a comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based anomaly detection framework and demonstrate how the active learning loop progressively enhances the model's performance. The framework is evaluated on real-world, imbalanced provenance trace data from the DARPA Transparent Computing program, where APT-like attacks account for just 0.004\% of the data. The datasets, which cover multiple operating systems including Android, Linux, BSD, and Windows, are tested in two attack scenarios. The results show substantial improvements in detection rates during active learning, outperforming existing methods.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pointing-Guided Target Estimation via Transformer-Based Attention</title>
<link>https://arxiv.org/abs/2509.05031</link>
<guid>https://arxiv.org/abs/2509.05031</guid>
<content:encoded><![CDATA[
arXiv:2509.05031v1 Announce Type: cross 
Abstract: Deictic gestures, like pointing, are a fundamental form of non-verbal communication, enabling humans to direct attention to specific objects or locations. This capability is essential in Human-Robot Interaction (HRI), where robots should be able to predict human intent and anticipate appropriate responses. In this work, we propose the Multi-Modality Inter-TransFormer (MM-ITF), a modular architecture to predict objects in a controlled tabletop scenario with the NICOL robot, where humans indicate targets through natural pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing gestures to object locations, assigns a likelihood score to each, and identifies the most likely target. Our results demonstrate that the method can accurately predict the intended object using monocular RGB data, thus enabling intuitive and accessible human-robot collaboration. To evaluate the performance, we introduce a patch confusion matrix, providing insights into the model's predictions across candidate object locations. Code available at: https://github.com/lucamuellercode/MMITF.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization</title>
<link>https://arxiv.org/abs/2509.05034</link>
<guid>https://arxiv.org/abs/2509.05034</guid>
<content:encoded><![CDATA[
arXiv:2509.05034v1 Announce Type: cross 
Abstract: Industrial product inspection is often performed using Anomaly Detection (AD) frameworks trained solely on non-defective samples. Although defective samples can be collected during production, leveraging them usually requires pixel-level annotations, limiting scalability. To address this, we propose ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial anomaly detection. ADClick generates pixel-wise anomaly annotations from only a few user clicks and a brief textual description, enabling precise and efficient labeling that significantly improves AD model performance (e.g., AP = 96.1\% on MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that aligns visual features and textual prompts via a prototype-based approach for anomaly detection and localization. By combining pixel-level priors with language-guided cues, ADClick-Seg achieves state-of-the-art results on the challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC = 99.1\% on MVTec AD).
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions</title>
<link>https://arxiv.org/abs/2509.05066</link>
<guid>https://arxiv.org/abs/2509.05066</guid>
<content:encoded><![CDATA[
arXiv:2509.05066v1 Announce Type: cross 
Abstract: Most existing Theory of Mind (ToM) benchmarks for foundation models rely on variations of the Sally-Anne test, offering only a very limited perspective on ToM and neglecting the complexity of human social interactions. To address this gap, we propose ToM-SSI: a new benchmark specifically designed to test ToM capabilities in environments rich with social interactions and spatial dynamics. While current ToM benchmarks are limited to text-only or dyadic interactions, ToM-SSI is multimodal and includes group interactions of up to four agents that communicate and move in situated environments. This unique design allows us to study, for the first time, mixed cooperative-obstructive settings and reasoning about multiple agents' mental state in parallel, thus capturing a wider range of social cognition than existing benchmarks. Our evaluations reveal that the current models' performance is still severely limited, especially in these new tasks, highlighting critical gaps for future research.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICR: Iterative Clarification and Rewriting for Conversational Search</title>
<link>https://arxiv.org/abs/2509.05100</link>
<guid>https://arxiv.org/abs/2509.05100</guid>
<content:encoded><![CDATA[
arXiv:2509.05100v1 Announce Type: cross 
Abstract: Most previous work on Conversational Query Rewriting employs an end-to-end rewriting paradigm. However, this approach is hindered by the issue of multiple fuzzy expressions within the query, which complicates the simultaneous identification and rewriting of multiple positions. To address this issue, we propose a novel framework ICR (Iterative Clarification and Rewriting), an iterative rewriting scheme that pivots on clarification questions. Within this framework, the model alternates between generating clarification questions and rewritten queries. The experimental results show that our ICR can continuously improve retrieval performance in the clarification-rewriting iterative process, thereby achieving state-of-the-art performance on two popular datasets.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenAI-based test case generation and execution in SDV platform</title>
<link>https://arxiv.org/abs/2509.05112</link>
<guid>https://arxiv.org/abs/2509.05112</guid>
<content:encoded><![CDATA[
arXiv:2509.05112v1 Announce Type: cross 
Abstract: This paper introduces a GenAI-driven approach for automated test case generation, leveraging Large Language Models and Vision-Language Models to translate natural language requirements and system diagrams into structured Gherkin test cases. The methodology integrates Vehicle Signal Specification modeling to standardize vehicle signal definitions, improve compatibility across automotive subsystems, and streamline integration with third-party testing tools. Generated test cases are executed within the digital.auto playground, an open and vendor-neutral environment designed to facilitate rapid validation of software-defined vehicle functionalities. We evaluate our approach using the Child Presence Detection System use case, demonstrating substantial reductions in manual test specification effort and rapid execution of generated tests. Despite significant automation, the generation of test cases and test scripts still requires manual intervention due to current limitations in the GenAI pipeline and constraints of the digital.auto platform.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination</title>
<link>https://arxiv.org/abs/2509.05145</link>
<guid>https://arxiv.org/abs/2509.05145</guid>
<content:encoded><![CDATA[
arXiv:2509.05145v1 Announce Type: cross 
Abstract: This paper investigates GrooveTransformer, a real-time rhythm generation system, through the postphenomenological framework of Variational Cross-Examination (VCE). By reflecting on its deployment across three distinct artistic contexts, we identify three stabilities: an autonomous drum accompaniment generator, a rhythmic control voltage sequencer in Eurorack format, and a rhythm driver for a harmonic accompaniment system. The versatility of its applications was not an explicit goal from the outset of the project. Thus, we ask: how did this multistability emerge? Through VCE, we identify three key contributors to its emergence: the affordances of system invariants, the interdisciplinary collaboration, and the situated nature of its development. We conclude by reflecting on the viability of VCE as a descriptive and analytical method for Digital Musical Instrument (DMI) design, emphasizing its value in uncovering how technologies mediate, co-shape, and are co-shaped by users and contexts.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection</title>
<link>https://arxiv.org/abs/2509.05190</link>
<guid>https://arxiv.org/abs/2509.05190</guid>
<content:encoded><![CDATA[
arXiv:2509.05190v1 Announce Type: cross 
Abstract: Deep learning models, especially convolutional neural networks (CNNs), have shown considerable promise for biomedical signals such as EEG-based seizure detection. However, these models come with challenges, primarily due to their size and compute requirements in environments where real-time detection or limited resources are available. In this study, we present a lightweight one-dimensional CNN model with structured pruning to improve efficiency and reliability. The model was trained with mild early stopping to address possible overfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686. Structured pruning of the baseline CNN involved removing 50% of the convolutional kernels based on their importance to model predictions. Surprisingly, after pruning the weights and memory by 50%, the new network was still able to maintain predictive capabilities, while modestly increasing precision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we present a convincing case that structured pruning removes redundancy, improves generalization, and, in combination with mild early stopping, achieves a promising way forward to improve seizure detection efficiency and reliability, which is clear motivation for resource-limited settings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents for Web Testing: A Case Study in the Wild</title>
<link>https://arxiv.org/abs/2509.05197</link>
<guid>https://arxiv.org/abs/2509.05197</guid>
<content:encoded><![CDATA[
arXiv:2509.05197v1 Announce Type: cross 
Abstract: Automated web testing plays a critical role in ensuring high-quality user experiences and delivering business value. Traditional approaches primarily focus on code coverage and load testing, but often fall short of capturing complex user behaviors, leaving many usability issues undetected. The emergence of large language models (LLM) and AI agents opens new possibilities for web testing by enabling human-like interaction with websites and a general awareness of common usability problems. In this work, we present WebProber, a prototype AI agent-based web testing framework. Given a URL, WebProber autonomously explores the website, simulating real user interactions, identifying bugs and usability issues, and producing a human-readable report. We evaluate WebProber through a case study of 120 academic personal websites, where it uncovered 29 usability issues--many of which were missed by traditional tools. Our findings highlight agent-based testing as a promising direction while outlining directions for developing next-generation, user-centered testing frameworks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</title>
<link>https://arxiv.org/abs/2509.05198</link>
<guid>https://arxiv.org/abs/2509.05198</guid>
<content:encoded><![CDATA[
arXiv:2509.05198v1 Announce Type: cross 
Abstract: The classification of 3D point clouds is crucial for applications such as autonomous driving, robotics, and augmented reality. However, the commonly used ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D data, size mismatches, and inadequate class differentiation, which hinder model performance. This paper introduces ModelNet-R, a meticulously refined version of ModelNet40 designed to address these issues and serve as a more reliable benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight graph-based neural network that leverages efficient sampling, neighborhood grouping, and skip connections to achieve high classification accuracy with reduced computational overhead. Extensive experiments demonstrate that models trained in ModelNet-R exhibit significant performance improvements. Notably, Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a substantially lower parameter count compared to contemporary models. This research highlights the crucial role of dataset quality in optimizing model efficiency for 3D point cloud classification. For more details, see the code at: https://github.com/m-saeid/ModeNetR_PointSkipNet.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.05207</link>
<guid>https://arxiv.org/abs/2509.05207</guid>
<content:encoded><![CDATA[
arXiv:2509.05207v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have become popular across a diverse set of tasks in exploring structural relationships between entities. However, due to the highly connected structure of the datasets, distributed training of GNNs on large-scale graphs poses significant challenges. Traditional sampling-based approaches mitigate the computational loads, yet the communication overhead remains a challenge. This paper presents RapidGNN, a distributed GNN training framework with deterministic sampling-based scheduling to enable efficient cache construction and prefetching of remote features. Evaluation on benchmark graph datasets demonstrates RapidGNN's effectiveness across different scales and topologies. RapidGNN improves end-to-end training throughput by 2.46x to 3.00x on average over baseline methods across the benchmark datasets, while cutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further demonstrates near-linear scalability with an increasing number of computing units efficiently. Furthermore, it achieves increased energy efficiency over the baseline methods for both CPU and GPU by 44% and 32%, respectively.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models</title>
<link>https://arxiv.org/abs/2509.05218</link>
<guid>https://arxiv.org/abs/2509.05218</guid>
<content:encoded><![CDATA[
arXiv:2509.05218v1 Announce Type: cross 
Abstract: Positional encoding mechanisms enable Transformers to model sequential structure and long-range dependencies in text. While absolute positional encodings struggle with extrapolation to longer sequences due to fixed positional representations, and relative approaches like Alibi exhibit performance degradation on extremely long contexts, the widely-used Rotary Positional Encoding (RoPE) introduces oscillatory attention patterns that hinder stable long-distance dependency modelling. We address these limitations through a geometric reformulation of positional encoding. Drawing inspiration from Lorentz transformations in hyperbolic geometry, we propose Hyperbolic Rotary Positional Encoding (HoPE), which leverages hyperbolic functions to implement Lorentz rotations on token representations. Theoretical analysis demonstrates that RoPE is a special case of our generalized formulation. HoPE fundamentally resolves RoPE's slation issues by enforcing monotonic decay of attention weights with increasing token distances. Extensive experimental results, including perplexity evaluations under several extended sequence benchmarks, show that HoPE consistently exceeds existing positional encoding methods. These findings underscore HoPE's enhanced capacity for representing and generalizing long-range dependencies. Data and code will be available.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models</title>
<link>https://arxiv.org/abs/2509.05230</link>
<guid>https://arxiv.org/abs/2509.05230</guid>
<content:encoded><![CDATA[
arXiv:2509.05230v1 Announce Type: cross 
Abstract: Pre-trained language models have achieved remarkable success across diverse applications but remain susceptible to spurious, concept-driven correlations that impair robustness and fairness. In this work, we introduce CURE, a novel and lightweight framework that systematically disentangles and suppresses conceptual shortcuts while preserving essential content information. Our method first extracts concept-irrelevant representations via a dedicated content extractor reinforced by a reversal network, ensuring minimal loss of task-relevant information. A subsequent controllable debiasing module employs contrastive learning to finely adjust the influence of residual conceptual cues, enabling the model to either diminish harmful biases or harness beneficial correlations as appropriate for the target task. Evaluated on the IMDB and Yelp datasets using three pre-trained architectures, CURE achieves an absolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp, while introducing minimal computational overhead. Our approach establishes a flexible, unsupervised blueprint for combating conceptual biases, paving the way for more reliable and fair language understanding systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertain but Useful: Leveraging CNN Variability into Data Augmentation</title>
<link>https://arxiv.org/abs/2509.05238</link>
<guid>https://arxiv.org/abs/2509.05238</guid>
<content:encoded><![CDATA[
arXiv:2509.05238v1 Announce Type: cross 
Abstract: Deep learning (DL) is rapidly advancing neuroimaging by achieving state-of-the-art performance with reduced computation times. Yet the numerical stability of DL models -- particularly during training -- remains underexplored. While inference with DL is relatively stable, training introduces additional variability primarily through iterative stochastic optimization. We investigate this training-time variability using FastSurfer, a CNN-based whole-brain segmentation pipeline. Controlled perturbations are introduced via floating point perturbations and random seeds. We find that: (i) FastSurfer exhibits higher variability compared to that of a traditional neuroimaging pipeline, suggesting that DL inherits and is particularly susceptible to sources of instability present in its predecessors; (ii) ensembles generated with perturbations achieve performance similar to an unperturbed baseline; and (iii) variability effectively produces ensembles of numerical model families that can be repurposed for downstream applications. As a proof of concept, we demonstrate that numerical ensembles can be used as a data augmentation strategy for brain age regression. These findings position training-time variability not only as a reproducibility concern but also as a resource that can be harnessed to improve robustness and enable new applications in neuroimaging.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization</title>
<link>https://arxiv.org/abs/2509.05249</link>
<guid>https://arxiv.org/abs/2509.05249</guid>
<content:encoded><![CDATA[
arXiv:2509.05249v1 Announce Type: cross 
Abstract: The ability to compose learned concepts and apply them in novel settings is key to human intelligence, but remains a persistent limitation in state-of-the-art machine learning models. To address this issue, we introduce COGITAO, a modular and extensible data generation framework and benchmark designed to systematically study compositionality and generalization in visual domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs rule-based tasks which apply a set of transformations to objects in grid-like environments. It supports composition, at adjustable depth, over a set of 28 interoperable transformations, along with extensive control over grid parametrization and object properties. This flexibility enables the creation of millions of unique task rules -- surpassing concurrent datasets by several orders of magnitude -- across a wide range of difficulties, while allowing virtually unlimited sample generation per rule. We provide baseline experiments using state-of-the-art vision models, highlighting their consistent failures to generalize to novel combinations of familiar elements, despite strong in-domain performance. COGITAO is fully open-sourced, including all code and datasets, to support continued research in this field.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recomposer: Event-roll-guided generative audio editing</title>
<link>https://arxiv.org/abs/2509.05256</link>
<guid>https://arxiv.org/abs/2509.05256</guid>
<content:encoded><![CDATA[
arXiv:2509.05256v1 Announce Type: cross 
Abstract: Editing complex real-world sound scenes is difficult because individual sound sources overlap in time. Generative models can fill-in missing or corrupted details based on their strong prior understanding of the data domain. We present a system for editing individual sound events within complex scenes able to delete, insert, and enhance individual sound events based on textual edit descriptions (e.g., ``enhance Door'') and a graphical representation of the event timing derived from an ``event roll'' transcription. We present an encoder-decoder transformer working on SoundStream representations, trained on synthetic (input, desired output) audio example pairs formed by adding isolated sound events to dense, real-world backgrounds. Evaluation reveals the importance of each part of the edit descriptions -- action, class, timing. Our work demonstrates ``recomposition'' is an important and practical application.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Performance of Large Language Model Pretraining</title>
<link>https://arxiv.org/abs/2509.05258</link>
<guid>https://arxiv.org/abs/2509.05258</guid>
<content:encoded><![CDATA[
arXiv:2509.05258v1 Announce Type: cross 
Abstract: Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortunately, information about the scaling performance and training considerations of these large training pipelines is scarce in public literature. Working with large-scale datasets and models can be complex and practical recommendations are scarce in the public literature for tuning training performance when scaling up large language models. In this paper, we aim to demystify the large language model pretraining pipeline somewhat - in particular with respect to distributed training, managing large datasets across hundreds of nodes, and scaling up data parallelism with an emphasis on fully leveraging available GPU compute capacity.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpikingBrain Technical Report: Spiking Brain-inspired Large Models</title>
<link>https://arxiv.org/abs/2509.05276</link>
<guid>https://arxiv.org/abs/2509.05276</guid>
<content:encoded><![CDATA[
arXiv:2509.05276v1 Announce Type: cross 
Abstract: Mainstream Transformer-based large language models face major efficiency bottlenecks: training computation scales quadratically with sequence length, and inference memory grows linearly, limiting long-context processing. Building large models on non-NVIDIA platforms also poses challenges for stable and efficient training. To address this, we introduce SpikingBrain, a family of brain-inspired models designed for efficient long-context training and inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three aspects: (1) Model Architecture: linear and hybrid-linear attention architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an efficient, conversion-based training pipeline and a dedicated spike coding framework; (3) System Engineering: customized training frameworks, operator libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM, and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the feasibility of large-scale LLM development on non-NVIDIA platforms. SpikingBrain achieves performance comparable to open-source Transformer baselines while using only about 150B tokens for continual pre-training. Our models significantly improve long-sequence training efficiency and deliver inference with (partially) constant memory and event-driven spiking behavior. For example, SpikingBrain-7B attains over 100x speedup in Time to First Token for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4 percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling low-power operation. Overall, this work demonstrates the potential of brain-inspired mechanisms to drive the next generation of efficient and scalable large model design.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crosscoding Through Time: Tracking Emergence &amp; Consolidation Of Linguistic Representations Throughout LLM Pretraining</title>
<link>https://arxiv.org/abs/2509.05291</link>
<guid>https://arxiv.org/abs/2509.05291</guid>
<content:encoded><![CDATA[
arXiv:2509.05291v1 Announce Type: cross 
Abstract: Large language models (LLMs) learn non-trivial abstractions during pretraining, like detecting irregular plural noun subjects. However, it is not well understood when and how specific linguistic abilities emerge as traditional evaluation methods such as benchmarking fail to reveal how models acquire concepts and capabilities. To bridge this gap and better understand model training at the concept level, we use sparse crosscoders to discover and align features across model checkpoints. Using this approach, we track the evolution of linguistic features during pretraining. We train crosscoders between open-sourced checkpoint triplets with significant performance and representation shifts, and introduce a novel metric, Relative Indirect Effects (RelIE), to trace training stages at which individual features become causally important for task performance. We show that crosscoders can detect feature emergence, maintenance, and discontinuation during pretraining. Our approach is architecture-agnostic and scalable, offering a promising path toward more interpretable and fine-grained analysis of representation learning throughout pretraining.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool</title>
<link>https://arxiv.org/abs/2509.05296</link>
<guid>https://arxiv.org/abs/2509.05296</guid>
<content:encoded><![CDATA[
arXiv:2509.05296v1 Announce Type: cross 
Abstract: We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2403.05265</link>
<guid>https://arxiv.org/abs/2403.05265</guid>
<content:encoded><![CDATA[
arXiv:2403.05265v3 Announce Type: replace 
Abstract: Online movie review websites are valuable for information and discussion about movies. However, the massive spoiler reviews detract from the movie-watching experience, making spoiler detection an important task. Previous methods simply focus on reviews' text content, ignoring the heterogeneity of information in the platform. For instance, the metadata and the corresponding user's information of a review could be helpful. Besides, the spoiler language of movie reviews tends to be genre-specific, thus posing a domain generalization challenge for existing methods. To this end, we propose MMoE, a multi-modal network that utilizes information from multiple modalities to facilitate robust spoiler detection and adopts Mixture-of-Experts to enhance domain generalization. MMoE first extracts graph, text, and meta feature from the user-movie network, the review's textual content, and the review's metadata respectively. To handle genre-specific spoilers, we then adopt Mixture-of-Experts architecture to process information in three modalities to promote robustness. Finally, we use an expert fusion layer to integrate the features from different perspectives and make predictions based on the fused embedding. Experiments demonstrate that MMoE achieves state-of-the-art performance on two widely-used spoiler detection datasets, surpassing previous SOTA methods by 2.56% and 8.41% in terms of accuracy and F1-score. Further experiments also demonstrate MMoE's superiority in robustness and generalization. Our code is available at https://github.com/zzqbjt/Spoiler-Detection.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Dimensional Federated Knowledge Graph Embedding via Knowledge Distillation</title>
<link>https://arxiv.org/abs/2408.05748</link>
<guid>https://arxiv.org/abs/2408.05748</guid>
<content:encoded><![CDATA[
arXiv:2408.05748v2 Announce Type: replace 
Abstract: Federated Knowledge Graph Embedding (FKGE) aims to facilitate collaborative learning of entity and relation embeddings from distributed Knowledge Graphs (KGs) across multiple clients, while preserving data privacy. Training FKGE models with higher dimensions is typically favored due to their potential for achieving superior performance. However, high-dimensional embeddings present significant challenges in terms of storage resource and inference speed. Unlike traditional KG embedding methods, FKGE involves multiple client-server communication rounds, where communication efficiency is critical. Existing embedding compression methods for traditional KGs may not be directly applicable to FKGE as they often require multiple model trainings which potentially incur substantial communication costs. In this paper, we propose a light-weight component based on Knowledge Distillation (KD) which is titled FedKD and tailored specifically for FKGE methods. During client-side local training, FedKD facilitates the low-dimensional student model to mimic the score distribution of triples from the high-dimensional teacher model using KL divergence loss. Unlike traditional KD way, FedKD adaptively learns a temperature to scale the score of positive triples and separately adjusts the scores of corresponding negative triples using a predefined temperature, thereby mitigating teacher over-confidence issue. Furthermore, we dynamically adjust the weight of KD loss to optimize the training process. Extensive experiments on three datasets support the effectiveness of FedKD.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning</title>
<link>https://arxiv.org/abs/2408.09600</link>
<guid>https://arxiv.org/abs/2408.09600</guid>
<content:encoded><![CDATA[
arXiv:2408.09600v3 Announce Type: replace 
Abstract: Safety aligned Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- a few harmful data mixed in the fine-tuning dataset can break the LLMs's safety alignment. While several defenses have been proposed, our evaluation shows that existing defenses fail \textit{when some specific training hyper-parameters are chosen} -- a large learning rate or a large number of training epochs in the fine-tuning stage can easily invalidate the defense. To this end, we propose Antidote, a post-fine-tuning stage solution, which remains \textbf{\textit{agnostic to the training hyper-parameters in the fine-tuning stage}}. Antidote relies on the philosophy that by removing the harmful parameters, the harmful model can be recovered from the harmful behaviors, regardless of how those harmful parameters are formed in the fine-tuning stage. With this philosophy, we introduce a one-shot pruning stage after harmful fine-tuning to remove the harmful weights that are responsible for the generation of harmful content. Despite its embarrassing simplicity, empirical results show that Antidote can reduce harmful score while maintaining accuracy on downstream tasks. Code is available at https://github.com/git-disl/Antidote.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Network Verification with PyRAT</title>
<link>https://arxiv.org/abs/2410.23903</link>
<guid>https://arxiv.org/abs/2410.23903</guid>
<content:encoded><![CDATA[
arXiv:2410.23903v2 Announce Type: replace 
Abstract: As AI systems are becoming more and more popular and used in various critical domains (health, transport, energy, ...), the need to provide guarantees and trust of their safety is undeniable. To this end, we present PyRAT, a tool based on abstract interpretation to verify the safety and the robustness of neural networks. In this paper, we describe the different abstractions used by PyRAT to find the reachable states of a neural network starting from its input as well as the main features of the tool to provide fast and accurate analysis of neural networks. PyRAT has already been used in several collaborations to ensure safety guarantees, with its second place at the VNN-Comp 2024 showcasing its performance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI Agents: A Survey</title>
<link>https://arxiv.org/abs/2412.13501</link>
<guid>https://arxiv.org/abs/2412.13501</guid>
<content:encoded><![CDATA[
arXiv:2412.13501v2 Announce Type: replace 
Abstract: Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment</title>
<link>https://arxiv.org/abs/2503.15937</link>
<guid>https://arxiv.org/abs/2503.15937</guid>
<content:encoded><![CDATA[
arXiv:2503.15937v3 Announce Type: replace 
Abstract: We propose V-Droid, a mobile GUI task automation agent. Unlike previous mobile agents that utilize Large Language Models (LLMs) as generators to directly generate actions at each step, V-Droid employs LLMs as verifiers to evaluate candidate actions before making final decisions. To realize this novel paradigm, we introduce a comprehensive framework for constructing verifier-driven mobile agents: the discretized action space construction coupled with the prefilling-only workflow to accelerate the verification process, the pair-wise progress preference training to significantly enhance the verifier's decision-making capabilities, and the scalable human-agent joint annotation scheme to efficiently collect the necessary data at scale.
  V-Droid obtains a substantial task success rate across several public mobile task automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on MobileAgentBench, surpassing existing agents by 5.2%, 2.1%, and 9%, respectively. Furthermore, V-Droid achieves a remarkably low latency of 4.3s per step, which is 6.1X faster compared with existing mobile agents. The source code is available at https://github.com/V-Droid-Agent/V-Droid.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Skills: Reasoning about Knowledge and Oblivion</title>
<link>https://arxiv.org/abs/2504.01733</link>
<guid>https://arxiv.org/abs/2504.01733</guid>
<content:encoded><![CDATA[
arXiv:2504.01733v2 Announce Type: replace 
Abstract: This paper presents a class of epistemic logics that captures the dynamics of acquiring knowledge and descending into oblivion, while incorporating concepts of group knowledge. The approach is grounded in a system of weighted models, introducing an ``epistemic skills'' metric to represent the epistemic capacities tied to knowledge updates. Within this framework, knowledge acquisition is modeled as a process of upskilling, whereas oblivion is represented as a consequence of downskilling. The framework further enables exploration of ``knowability'' and ``forgettability,'' defined as the potential to gain knowledge through upskilling and to lapse into oblivion through downskilling, respectively. Additionally, it supports a detailed analysis of the distinctions between epistemic de re and de dicto expressions. The computational complexity of the model checking and satisfiability problems is examined, offering insights into their theoretical foundations and practical implications.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding</title>
<link>https://arxiv.org/abs/2505.06020</link>
<guid>https://arxiv.org/abs/2505.06020</guid>
<content:encoded><![CDATA[
arXiv:2505.06020v2 Announce Type: replace 
Abstract: Understanding visual art requires reasoning across multiple perspectives -- cultural, historical, and stylistic -- beyond mere object recognition. While recent multimodal large language models (MLLMs) perform well on general image captioning, they often fail to capture the nuanced interpretations that fine art demands. We propose ArtRAG, a novel, training-free framework that combines structured knowledge with retrieval-augmented generation (RAG) for multi-perspective artwork explanation. ArtRAG automatically constructs an Art Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing entities such as artists, movements, themes, and historical events into a rich, interpretable graph. At inference time, a multi-granular structured retriever selects semantically and topologically relevant subgraphs to guide generation. This enables MLLMs to produce contextually grounded, culturally informed art descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG outperforms several heavily trained baselines. Human evaluations further confirm that ArtRAG generates coherent, insightful, and culturally enriched interpretations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT</title>
<link>https://arxiv.org/abs/2506.07173</link>
<guid>https://arxiv.org/abs/2506.07173</guid>
<content:encoded><![CDATA[
arXiv:2506.07173v2 Announce Type: replace 
Abstract: The Python Testbed for Federated Learning Algorithms is a simple Python FL framework that is easy to use by ML&amp;AI developers who do not need to be professional programmers and is also amenable to LLMs. In the previous research, generic federated learning algorithms provided by this framework were manually translated into the CSP processes and algorithms' safety and liveness properties were automatically verified by the model checker PAT. In this paper, a simple translation process is introduced wherein the ChatGPT is used to automate the translation of the mentioned federated learning algorithms in Python into the corresponding CSP processes. Within the process, the minimality of the used context is estimated based on the feedback from ChatGPT. The proposed translation process was experimentally validated by successful translation (verified by the model checker PAT) of both generic centralized and decentralized federated learning algorithms.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.14387</link>
<guid>https://arxiv.org/abs/2506.14387</guid>
<content:encoded><![CDATA[
arXiv:2506.14387v2 Announce Type: replace 
Abstract: Existing work on mitigating catastrophic forgetting during large language models (LLMs) fine-tuning for new knowledge instances has primarily focused on preserving performance on previously seen data, while critically overlooking the collapse of essential capabilities instilled through alignment, most notably the model's ability to faithfully express epistemic uncertainty (a property we term 'Ignorance Awareness'). In this work, we formalize the notion of Ignorance Awareness and illustrate that conventional fine-tuning methods can result in substantial activation displacement. This displacement undermines the critical capability of ignorance awareness, leading to undesirable behaviors such as hallucinations. To address this challenge, we introduce SEAT, a simple and principled fine-tuning approach that not only enables the model to effectively acquire new knowledge instances but also preserves its aligned ignorance awareness. SEAT integrates two key components: (1) sparse tuning that constrains activation drift, and (2) a novel entity perturbation method designed to counter knowledge entanglement. Experimental results demonstrate that, across both real-world and synthetic datasets, SEAT significantly outperforms baselines in preserving ignorance awareness while retaining optimal fine-tuning performance, offering a more robust solution for LLM fine-tuning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning</title>
<link>https://arxiv.org/abs/2507.00008</link>
<guid>https://arxiv.org/abs/2507.00008</guid>
<content:encoded><![CDATA[
arXiv:2507.00008v2 Announce Type: replace 
Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses unique challenges due to the diversity of visual elements, spatial clutter, and the ambiguity of language. In this paper, we introduce DiMo-GUI, a training-free framework for GUI grounding that leverages two core strategies: dynamic visual grounding and modality-aware optimization. Instead of treating the GUI as a monolithic image, our method splits the input into textual elements and iconic elements, allowing the model to reason over each modality independently using general-purpose vision-language models. When predictions are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by generating candidate focal regions centered on the model's initial predictions and incrementally zooms into subregions to refine the grounding result. This hierarchical refinement process helps disambiguate visually crowded layouts without the need for additional training or annotations. We evaluate our approach on standard GUI grounding benchmarks and demonstrate consistent improvements over baseline inference pipelines, highlighting the effectiveness of combining modality separation with region-focused reasoning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment</title>
<link>https://arxiv.org/abs/2507.05528</link>
<guid>https://arxiv.org/abs/2507.05528</guid>
<content:encoded><![CDATA[
arXiv:2507.05528v2 Announce Type: replace 
Abstract: Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design</title>
<link>https://arxiv.org/abs/2507.20541</link>
<guid>https://arxiv.org/abs/2507.20541</guid>
<content:encoded><![CDATA[
arXiv:2507.20541v4 Announce Type: replace 
Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that presents a new paradigm for Automatic Heuristic Design (AHD). Traditional evolutionary methods operate directly on heuristic code; in contrast, MeLA evolves the instructional prompts used to guide a Large Language Model (LLM) in generating these heuristics. This process of "prompt evolution" is driven by a novel metacognitive framework where the system analyzes performance feedback to systematically refine its generative strategy. MeLA's architecture integrates a problem analyzer to construct an initial strategic prompt, an error diagnosis system to repair faulty code, and a metacognitive search engine that iteratively optimizes the prompt based on heuristic effectiveness. In comprehensive experiments across both benchmark and real-world problems, MeLA consistently generates more effective and robust heuristics, significantly outperforming state-of-the-art methods. Ultimately, this research demonstrates the profound potential of using cognitive science as a blueprint for AI architecture, revealing that by enabling an LLM to metacognitively regulate its problem-solving process, we unlock a more robust and interpretable path to AHD.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction</title>
<link>https://arxiv.org/abs/2508.11987</link>
<guid>https://arxiv.org/abs/2508.11987</guid>
<content:encoded><![CDATA[
arXiv:2508.11987v3 Announce Type: replace 
Abstract: Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information gathering, contextual understanding, and decision-making under uncertainty. Agents must not only gather and interpret vast amounts of dynamic information but also integrate diverse data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human experts do in fields like politics, economics, and finance. Despite its importance, no large-scale benchmark exists for evaluating agents on future prediction, largely due to challenges in handling real-time updates and retrieving timely, accurate answers. To address this, we introduce $\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically designed for LLM agents performing future prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction, supporting real-time daily updates and eliminating data contamination through an automated pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including those with reasoning, search capabilities, and integration of external tools such as the open-source Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation assesses agents' adaptive reasoning and performance in dynamic environments. Additionally, we provide in-depth analyses of agents' failure modes and performance pitfalls in future-oriented tasks, including the vulnerability to fake web pages and the temporal validity. Our goal is to establish a dynamic, contamination-free evaluation standard that drives the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model</title>
<link>https://arxiv.org/abs/2508.13676</link>
<guid>https://arxiv.org/abs/2508.13676</guid>
<content:encoded><![CDATA[
arXiv:2508.13676v2 Announce Type: replace 
Abstract: To maintain the company's talent pool, recruiters need to continuously search for resumes from third-party websites (e.g., LinkedIn, Indeed). However, fetched resumes are often incomplete and inaccurate. To improve the quality of third-party resumes and enrich the company's talent pool, it is essential to conduct duplication detection between the fetched resumes and those already in the company's talent pool. Such duplication detection is challenging due to the semantic complexity, structural heterogeneity, and information incompleteness of resume texts. To this end, we propose MHSNet, an multi-level identity verification framework that fine-tunes BGE-M3 using contrastive learning. With the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and dense representations for resumes, enabling the computation of corresponding multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts (MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental results verify the effectiveness of MHSNet
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain</title>
<link>https://arxiv.org/abs/2508.16172</link>
<guid>https://arxiv.org/abs/2508.16172</guid>
<content:encoded><![CDATA[
arXiv:2508.16172v2 Announce Type: replace 
Abstract: Understanding human behavior in urban environments is a crucial field within city sciences. However, collecting accurate behavioral data, particularly in newly developed areas, poses significant challenges. Recent advances in generative agents, powered by Large Language Models (LLMs), have shown promise in simulating human behaviors without relying on extensive datasets. Nevertheless, these methods often struggle with generating consistent, context-sensitive, and realistic behavioral outputs. To address these limitations, this paper introduces the Preference Chain, a novel method that integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance context-aware simulation of human behavior in transportation systems. Experiments conducted on the Replica dataset demonstrate that the Preference Chain outperforms standard LLM in aligning with real-world transportation mode choices. The development of the Mobility Agent highlights potential applications of proposed method in urban mobility modeling for emerging cities, personalized travel behavior analysis, and dynamic traffic forecasting. Despite limitations such as slow inference and the risk of hallucination, the method offers a promising framework for simulating complex human behavior in data-scarce environments, where traditional data-driven models struggle due to limited data availability.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.20368</link>
<guid>https://arxiv.org/abs/2508.20368</guid>
<content:encoded><![CDATA[
arXiv:2508.20368v2 Announce Type: replace 
Abstract: Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMs' internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradigm for enhancing LLM reasoning through multi-turn interactions with search engines. However, existing RL-based search agents rely on a single LLM to handle both search planning and question-answering (QA) tasks in an end-to-end manner, which limits their ability to optimize both capabilities simultaneously. In practice, sophisticated AI search systems often employ a large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a more effective and efficient approach is to utilize a small, trainable LLM dedicated to search planning. In this paper, we propose \textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to enhance the performance of frozen QA models by focusing on search planning. Specifically, our approach introduces three key innovations: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to achieve the objectives. Extensive experiments on real-world datasets demonstrate that AI SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, while exhibiting strong generalization capabilities across diverse frozen QA models and data domains.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survival Analysis with Adversarial Regularization</title>
<link>https://arxiv.org/abs/2312.16019</link>
<guid>https://arxiv.org/abs/2312.16019</guid>
<content:encoded><![CDATA[
arXiv:2312.16019v5 Announce Type: replace-cross 
Abstract: Survival Analysis (SA) models the time until an event occurs, with applications in fields like medicine, defense, finance, and aerospace. Recent research indicates that Neural Networks (NNs) can effectively capture complex data patterns in SA, whereas simple generalized linear models often fall short in this regard. However, dataset uncertainties (e.g., noisy measurements, human error) can degrade NN model performance. To address this, we leverage advances in NN verification to develop training objectives for robust, fully-parametric SA models. Specifically, we propose an adversarially robust loss function based on a Min-Max optimization problem. We employ CROWN-Interval Bound Propagation (CROWN-IBP) to tackle the computational challenges inherent in solving this Min-Max problem. Evaluated over 10 SurvSet datasets, our method, Survival Analysis with Adversarial Regularization (SAWAR), consistently outperforms baseline adversarial training methods and state-of-the-art (SOTA) deep SA models across various covariate perturbations with respect to Negative Log Likelihood (NegLL), Integrated Brier Score (IBS), and Concordance Index (CI) metrics. Thus, we demonstrate that adversarial robustness enhances SA predictive performance and calibration, mitigating data uncertainty and improving generalization across diverse datasets by up to 150% compared to baselines.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Chains, Trees, and Graphs of Thoughts</title>
<link>https://arxiv.org/abs/2401.14295</link>
<guid>https://arxiv.org/abs/2401.14295</guid>
<content:encoded><![CDATA[
arXiv:2401.14295v5 Announce Type: replace-cross 
Abstract: The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</title>
<link>https://arxiv.org/abs/2402.12226</link>
<guid>https://arxiv.org/abs/2402.12226</guid>
<content:encoded><![CDATA[
arXiv:2402.12226v4 Announce Type: replace-cross 
Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks</title>
<link>https://arxiv.org/abs/2405.15164</link>
<guid>https://arxiv.org/abs/2405.15164</guid>
<content:encoded><![CDATA[
arXiv:2405.15164v2 Announce Type: replace-cross 
Abstract: Compositionality has long been considered a key explanatory property underlying human intelligence: arbitrary concepts can be composed into novel complex combinations, permitting the acquisition of an open ended, potentially infinite expressive capacity from finite learning experiences. Influential arguments have held that neural networks fail to explain this aspect of behavior, leading many to dismiss them as viable models of human cognition. Over the last decade, however, modern deep neural networks (DNNs), which share the same fundamental design principles as their predecessors, have come to dominate artificial intelligence, exhibiting the most advanced cognitive behaviors ever demonstrated in machines. In particular, large language models (LLMs), DNNs trained to predict the next word on a large corpus of text, have proven capable of sophisticated behaviors such as writing syntactically complex sentences without grammatical errors, producing cogent chains of reasoning, and even writing original computer programs -- all behaviors thought to require compositional processing. In this chapter, we survey recent empirical work from machine learning for a broad audience in philosophy, cognitive science, and neuroscience, situating recent breakthroughs within the broader context of philosophical arguments about compositionality. In particular, our review emphasizes two approaches to endowing neural networks with compositional generalization capabilities: (1) architectural inductive biases, and (2) metalearning, or learning to learn. We also present findings suggesting that LLM pretraining can be understood as a kind of metalearning, and can thereby equip DNNs with compositional generalization abilities in a similar way. We conclude by discussing the implications that these findings may have for the study of compositionality in human cognition and by suggesting avenues for future research.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFaults: Model-Based Diagnosis for Fault Localization in C Programs with Multiple Test Cases</title>
<link>https://arxiv.org/abs/2407.09337</link>
<guid>https://arxiv.org/abs/2407.09337</guid>
<content:encoded><![CDATA[
arXiv:2407.09337v2 Announce Type: replace-cross 
Abstract: Debugging is one of the most time-consuming and expensive tasks in software development. Several formula-based fault localization (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs with multiple faults.
  This paper introduces a novel fault localization approach for C programs with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified MaxSAT formula. Consequently, our method guarantees consistency across observations and simplifies the fault localization procedure. Experimental results on two benchmark sets of C programs, TCAS and C-Pack-IPAs, show that CFaults is faster than other FBFL approaches like BugAssist and SNIPER. Moreover, CFaults only generates subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaGym: Evaluating Persona Agents and LLMs</title>
<link>https://arxiv.org/abs/2407.18416</link>
<guid>https://arxiv.org/abs/2407.18416</guid>
<content:encoded><![CDATA[
arXiv:2407.18416v5 Announce Type: replace-cross 
Abstract: Persona agents, which are LLM agents conditioned to act according to an assigned persona, enable contextually rich and user aligned interactions across domains like education and healthcare. However, evaluating how faithfully these agents adhere to their personas remains a significant challenge, particularly in free-form settings that demand consistency across diverse, persona-relevant environments. We introduce PersonaGym, the first dynamic evaluation framework for persona agents, and PersonaScore, a human-aligned automatic metric grounded in decision theory that enables comprehensive large-scale evaluation. Our evaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals significant advancement opportunities. For example, GPT-4.1 had the exact same PersonaScore as LLaMA-3-8b despite being a more recent and advanced closed source model. Importantly, increased model size and complexity do not necessarily enhance persona agent capabilities, underscoring the need for algorithmic and architectural innovation toward faithful, performant persona agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation</title>
<link>https://arxiv.org/abs/2407.19835</link>
<guid>https://arxiv.org/abs/2407.19835</guid>
<content:encoded><![CDATA[
arXiv:2407.19835v2 Announce Type: replace-cross 
Abstract: Classical Arabic represents a significant era that encompasses the golden age of Arab culture, philosophy, and scientific literature. With a broad consensus on the importance of translating these literatures to enrich knowledge dissemination across communities, the advent of large language models (LLMs) and translation systems offers promising tools to facilitate this goal. However, we have identified a scarcity of translation datasets in Classical Arabic, which are often limited in scope and topics, hindering the development of high-quality translation systems. In response, we present the ATHAR dataset, which comprises 66,000 high-quality classical Arabic to English translation samples that cover a wide array of topics including science, culture, and philosophy. Furthermore, we assess the performance of current state-of-the-art LLMs under various settings, concluding that there is a need for such datasets in current systems. Our findings highlight how models can benefit from fine-tuning or incorporating this dataset into their pretraining pipelines. The dataset is publicly available on the HuggingFace Data Hub: https://huggingface.co/datasets/mohamed-khalil/ATHAR.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Preference Optimization via Token-Level Reward Function Estimation</title>
<link>https://arxiv.org/abs/2408.13518</link>
<guid>https://arxiv.org/abs/2408.13518</guid>
<content:encoded><![CDATA[
arXiv:2408.13518v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language model alignment leverage token-level supervisions to perform fine-grained preference optimization. However, existing token-level alignment methods either optimize on all available tokens, which can be noisy and inefficient, or perform selective training with complex and expensive key token selection strategies. In this work, we propose Selective Preference Optimization (SePO), a novel selective alignment strategy that centers on efficient key token selection. SePO proposes the first token selection method based on Direct Preference Optimization (DPO), which trains an oracle model to estimate a token-level reward function on the target data. This method applies to any existing alignment datasets with response-level annotations and enables cost-efficient token selection with small-scale oracle models and training data. The estimated reward function is then utilized to score all tokens within the target dataset, where only the key tokens are selected to supervise the target policy model with a reference model-free contrastive objective function. Extensive experiments on three public evaluation benchmarks show that SePO significantly outperforms competitive baseline methods by only optimizing 30% key tokens on the target dataset. SePO applications on weak-to-strong generalization show that weak oracle models effectively supervise strong policy models with up to 16.8x more parameters. SePO also effectively selects key tokens from out-of-distribution data to enhance strong policy models and alleviate the over-optimization problem.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated detection of underdiagnosed medical conditions via opportunistic imaging</title>
<link>https://arxiv.org/abs/2409.11686</link>
<guid>https://arxiv.org/abs/2409.11686</guid>
<content:encoded><![CDATA[
arXiv:2409.11686v4 Announce Type: replace-cross 
Abstract: Abdominal computed tomography (CT) scans are frequently performed in clinical settings. Opportunistic CT involves repurposing routine CT images to extract diagnostic information and is an emerging tool for detecting underdiagnosed conditions such as sarcopenia, hepatic steatosis, and ascites. This study utilizes deep learning methods to promote accurate diagnosis and clinical documentation. We analyze 2,674 inpatient CT scans to identify discrepancies between imaging phenotypes (characteristics derived from opportunistic CT scans) and their corresponding documentation in radiology reports and ICD coding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans diagnosed with sarcopenia, hepatic steatosis, and ascites (respectively) through either opportunistic imaging or radiology reports were ICD-coded. Our findings demonstrate opportunistic CT's potential to enhance diagnostic precision and accuracy of risk adjustment models, offering advancements in precision medicine.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale</title>
<link>https://arxiv.org/abs/2409.16299</link>
<guid>https://arxiv.org/abs/2409.16299</guid>
<content:encoded><![CDATA[
arXiv:2409.16299v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have revolutionized software engineering (SE), showcasing remarkable proficiency in various coding tasks. Despite recent advancements that have enabled the creation of autonomous software agents utilizing LLMs for end-to-end development tasks, these systems are typically designed for specific SE functions. We introduce HyperAgent, an innovative generalist multi-agent system designed to tackle a wide range of SE tasks across different programming languages by mimicking the workflows of human developers. HyperAgent features four specialized agents-Planner, Navigator, Code Editor, and Executor-capable of handling the entire lifecycle of SE tasks, from initial planning to final verification. HyperAgent sets new benchmarks in diverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench benchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates exceptional performance in repository-level code generation (RepoExec) and fault localization and program repair (Defects4J), often surpassing state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatically Detecting Online Deceptive Patterns</title>
<link>https://arxiv.org/abs/2411.07441</link>
<guid>https://arxiv.org/abs/2411.07441</guid>
<content:encoded><![CDATA[
arXiv:2411.07441v2 Announce Type: replace-cross 
Abstract: Deceptive patterns (DPs) in digital interfaces manipulate users into making unintended decisions, exploiting cognitive biases and psychological vulnerabilities. These patterns have become ubiquitous across various digital platforms. While efforts to mitigate DPs have emerged from legal and technical perspectives, a significant gap in usable solutions that empower users to identify and make informed decisions about DPs in real-time remains. In this work, we introduce AutoBot, an automated, deceptive pattern detector that analyzes websites' visual appearances using machine learning techniques to identify and notify users of DPs in real-time. AutoBot employs a two-staged pipeline that processes website screenshots, identifying interactable elements and extracting textual features without relying on HTML structure. By leveraging a custom language model, AutoBot understands the context surrounding these elements to determine the presence of deceptive patterns. We implement AutoBot as a lightweight Chrome browser extension that performs all analyses locally, minimizing latency and preserving user privacy. Through extensive evaluation, we demonstrate AutoBot's effectiveness in enhancing users' ability to navigate digital environments safely while providing a valuable tool for regulators to assess and enforce compliance with DP regulations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Information Security Awareness of Large Language Models</title>
<link>https://arxiv.org/abs/2411.13207</link>
<guid>https://arxiv.org/abs/2411.13207</guid>
<content:encoded><![CDATA[
arXiv:2411.13207v2 Announce Type: replace-cross 
Abstract: The popularity of large language models (LLMs) continues to grow, and LLM-based assistants have become ubiquitous. Information security awareness (ISA) is an important yet underexplored safety aspect of LLMs. ISA encompasses LLMs' security knowledge, which has been explored in the past, as well as attitudes and behaviors, which are crucial to LLMs' ability to understand implicit security context and reject unsafe requests that may cause the LLM to fail the user. We present an automated method for measuring the ISA of LLMs, which covers all 30 security topics in a mobile ISA taxonomy, using realistic scenarios that create tension between implicit security implications and user satisfaction. Applying this method to leading LLMs, we find that most of the popular models exhibit only medium to low levels of ISA, exposing their users to cybersecurity threats. Smaller variants of the same model family are significantly riskier, while newer versions show no consistent ISA improvement, suggesting that providers are not actively working toward mitigating this issue. These results reveal a widespread vulnerability affecting current LLM deployments: the majority of popular models, and particularly their smaller variants, may systematically endanger users. We propose a practical mitigation: incorporating our security awareness instruction into model system prompts to help LLMs better detect and reject unsafe requests.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spoof Trace Discovery for Deep Learning Based Explainable Face Anti-Spoofing</title>
<link>https://arxiv.org/abs/2412.17541</link>
<guid>https://arxiv.org/abs/2412.17541</guid>
<content:encoded><![CDATA[
arXiv:2412.17541v4 Announce Type: replace-cross 
Abstract: With the rapid growth usage of face recognition in people's daily life, face anti-spoofing becomes increasingly important to avoid malicious attacks. Recent face anti-spoofing models can reach a high classification accuracy on multiple datasets but these models can only tell people "this face is fake" while lacking the explanation to answer "why it is fake". Such a system undermines trustworthiness and causes user confusion, as it denies their requests without providing any explanations. In this paper, we incorporate XAI into face anti-spoofing and propose a new problem termed X-FAS (eXplainable Face Anti-Spoofing) empowering face anti-spoofing models to provide an explanation. We propose SPTD (SPoof Trace Discovery), an X-FAS method which can discover spoof concepts and provide reliable explanations on the basis of discovered concepts. To evaluate the quality of X-FAS methods, we propose an X-FAS benchmark with annotated spoof traces by experts. We analyze SPTD explanations on face anti-spoofing dataset and compare SPTD quantitatively and qualitatively with previous XAI methods on proposed X-FAS benchmark. Experimental results demonstrate SPTD's ability to generate reliable explanations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2501.03544</link>
<guid>https://arxiv.org/abs/2501.03544</guid>
<content:encoded><![CDATA[
arXiv:2501.03544v3 Announce Type: replace-cross 
Abstract: Recent text-to-image (T2I) models have exhibited remarkable performance in generating high-quality images from text descriptions. However, these models are vulnerable to misuse, particularly generating not-safe-for-work (NSFW) content, such as sexually explicit, violent, political, and disturbing images, raising serious ethical concerns. In this work, we present PromptGuard, a novel content moderation technique that draws inspiration from the system prompt mechanism in large language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack a direct interface for enforcing behavioral guidelines. Our key idea is to optimize a safety soft prompt that functions as an implicit system prompt within the T2I model's textual embedding space. This universal soft prompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image generation without altering the inference efficiency or requiring proxy models. We further enhance its reliability and helpfulness through a divide-and-conquer strategy, which optimizes category-specific soft prompts and combines them into holistic safety guidance. Extensive experiments across five datasets demonstrate that PromptGuard effectively mitigates NSFW content generation while preserving high-quality benign outputs. PromptGuard achieves 3.8 times faster than prior content moderation methods, surpassing eight state-of-the-art defenses with an optimal unsafe ratio down to 5.84%.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing higher-order neural representations of uncertainty with the Noise Estimation through Reinforcement-based Diffusion (NERD) model</title>
<link>https://arxiv.org/abs/2503.14333</link>
<guid>https://arxiv.org/abs/2503.14333</guid>
<content:encoded><![CDATA[
arXiv:2503.14333v3 Announce Type: replace-cross 
Abstract: Studies often aim to reveal ``first-order" representations (FORs), which encode aspects of an observer's environment, such as contents or structure. A less-common target is ``higher-order" representations (HORs), which are ``about" FORs -- e.g., their strength or uncertainty -- and which may contribute to learning. HORs about uncertainty are unlikely to be direct ``read-outs" of FOR characteristics, instead reflecting noisy estimation processes incorporating prior expectations about uncertainty, but how the brain represents such expected uncertainty distributions remains largely unexplored. Here, we study ``noise expectation" HORs using neural data from a task which may require the brain to learn about its own noise: decoded neurofeedback, wherein human subjects learn to volitionally produce target neural patterns. We develop and apply a Noise Estimation through Reinforcement-based Diffusion (NERD) model to characterize how brains may undertake this process, and show that NERD offers high explanatory power for human behavior.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RailGoerl24: G\"orlitz Rail Test Center CV Dataset 2024</title>
<link>https://arxiv.org/abs/2504.00204</link>
<guid>https://arxiv.org/abs/2504.00204</guid>
<content:encoded><![CDATA[
arXiv:2504.00204v2 Announce Type: replace-cross 
Abstract: Driverless train operation for open tracks on urban guided transport and mainline railways requires, among other things automatic detection of actual and potential obstacles, especially humans, in the danger zone of the train's path. Machine learning algorithms have proven to be powerful state-of-the-art tools for this task. However, these algorithms require large amounts of high-quality annotated data containing human beings in railway-specific environments as training data. Unfortunately, the amount of publicly available datasets is not yet sufficient and is significantly inferior to the datasets in the road domain. Therefore, this paper presents RailGoerl24, an on-board visual light Full HD camera dataset of 12205 frames recorded in a railway test center of T\"UV S\"UD Rail, in G\"orlitz, Germany. Its main purpose is to support the development of driverless train operation for guided transport. RailGoerl24 also includes a terrestrial LiDAR scan covering parts of the area used to acquire the RGB data. In addition to the raw data, the dataset contains 33556 boxwise annotations in total for the object class 'person'. The faces of recorded actors are not blurred or altered in any other way. RailGoerl24, available at data.fid-move.de/dataset/railgoerl24, can also be used for tasks beyond collision prediction.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPDL: Automatic Prompt Optimization for LLM Agents</title>
<link>https://arxiv.org/abs/2504.04365</link>
<guid>https://arxiv.org/abs/2504.04365</guid>
<content:encoded><![CDATA[
arXiv:2504.04365v3 Announce Type: replace-cross 
Abstract: The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning</title>
<link>https://arxiv.org/abs/2505.10264</link>
<guid>https://arxiv.org/abs/2505.10264</guid>
<content:encoded><![CDATA[
arXiv:2505.10264v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) enables collaborative training of machine learning models across distributed clients without sharing raw data, ostensibly preserving data privacy. Nevertheless, recent studies have revealed critical vulnerabilities in FL, showing that a malicious central server can manipulate model updates to reconstruct clients' private training data. Existing data reconstruction attacks have important limitations: they often rely on assumptions about the clients' data distribution or their efficiency significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes these limitations. Our method leverages a new geometric perspective on fully connected layers to craft malicious model parameters, enabling the perfect recovery of arbitrarily large data batches in classification tasks without any prior knowledge of clients' data. Through extensive experiments on both image and tabular datasets, we demonstrate that our attack outperforms existing methods and achieves perfect reconstruction of data batches two orders of magnitude larger than the state of the art.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokUR: Token-Level Uncertainty Estimation for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.11737</link>
<guid>https://arxiv.org/abs/2505.11737</guid>
<content:encoded><![CDATA[
arXiv:2505.11737v2 Announce Type: replace-cross 
Abstract: While Large Language Models (LLMs) have demonstrated impressive capabilities, their output quality remains inconsistent across various application scenarios, making it difficult to identify trustworthy responses, especially in complex tasks requiring multi-step reasoning. In this paper, we propose a Token-level Uncertainty estimation framework for Reasoning (TokUR) to enable LLMs to self-assess and self-improve their generation quality in mathematical reasoning. Specifically, we introduce low-rank random weight perturbation to LLM decoding, generating predictive distributions that we use to estimate token-level uncertainties. We then aggregate these uncertainties to reflect semantic uncertainty of the generated sequences. Experiments on mathematical reasoning datasets of varying difficulty demonstrate that our token-level uncertainty metrics strongly correlate with answer correctness and model robustness. Additionally, we explore using uncertainty to directly enhance the model's reasoning performance through multiple generations and the particle filtering algorithm. Our approach consistently outperforms existing uncertainty estimation methods, establishing effective uncertainty estimation as a valuable tool for both evaluating and improving reasoning generation in LLMs.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &amp; Dragons Gameplay</title>
<link>https://arxiv.org/abs/2505.22809</link>
<guid>https://arxiv.org/abs/2505.22809</guid>
<content:encoded><![CDATA[
arXiv:2505.22809v2 Announce Type: replace-cross 
Abstract: Much work has been done on conversational LLM agents which directly assist human users with tasks. We present an alternative paradigm for interacting with LLM agents, which we call "overhearing agents". These overhearing agents do not actively participate in conversation -- instead, they "listen in" on human-to-human conversations and perform background tasks or provide suggestions to assist the user. In this work, we explore the overhearing agents paradigm through the lens of Dungeons & Dragons gameplay. We present an in-depth study using large multimodal audio-language models as overhearing agents to assist a Dungeon Master. We perform a human evaluation to examine the helpfulness of such agents and find that some large audio-language models have the emergent ability to perform overhearing agent tasks using implicit audio cues. Finally, we release Python libraries and our project code to support further research into the overhearing agents paradigm at https://github.com/zhudotexe/overhearing_agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Rapid Crystal Structure Generation Towards a Target Local Environment</title>
<link>https://arxiv.org/abs/2506.08224</link>
<guid>https://arxiv.org/abs/2506.08224</guid>
<content:encoded><![CDATA[
arXiv:2506.08224v2 Announce Type: replace-cross 
Abstract: In the field of material design, traditional crystal structure prediction approaches require extensive structural sampling through computationally expensive energy minimization methods using either force fields or quantum mechanical simulations. While emerging artificial intelligence (AI) generative models have shown great promise in generating realistic crystal structures more rapidly, most existing models fail to account for the unique symmetries and periodicity of crystalline materials, and they are limited to handling structures with only a few tens of atoms per unit cell. Here, we present a symmetry-informed AI generative approach called Local Environment Geometry-Oriented Crystal Generator (LEGO-xtal) that overcomes these limitations. Our method generates initial structures using AI models trained on an augmented small dataset, and then optimizes them using machine learning structure descriptors rather than traditional energy-based optimization. We demonstrate the effectiveness of LEGO-xtal by expanding from 25 known low-energy sp2 carbon allotropes to over 1,700, all within 0.5 eV/atom of the ground-state energy of graphite. This framework offers a generalizable strategy for the targeted design of materials with modular building blocks, such as metal-organic frameworks and next-generation battery materials.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Features at Convergence Theorem: a first-principles alternative to the Neural Feature Ansatz for how networks learn representations</title>
<link>https://arxiv.org/abs/2507.05644</link>
<guid>https://arxiv.org/abs/2507.05644</guid>
<content:encoded><![CDATA[
arXiv:2507.05644v2 Announce Type: replace-cross 
Abstract: It is a central challenge in deep learning to understand how neural networks learn representations. A leading approach is the Neural Feature Ansatz (NFA) (Radhakrishnan et al. 2024), a conjectured mechanism for how feature learning occurs. Although the NFA is empirically validated, it is an educated guess and lacks a theoretical basis, and thus it is unclear when it might fail, and how to improve it. In this paper, we take a first-principles approach to understanding why this observation holds, and when it does not. We use first-order optimality conditions to derive the Features at Convergence Theorem (FACT), an alternative to the NFA that (a) obtains greater agreement with learned features at convergence, (b) explains why the NFA holds in most settings, and (c) captures essential feature learning phenomena in neural networks such as grokking behavior in modular arithmetic and phase transitions in learning sparse parities, similarly to the NFA. Thus, our results unify theoretical first-order optimality analyses of neural networks with the empirically-driven NFA literature, and provide a principled alternative that provably and empirically holds at convergence.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework</title>
<link>https://arxiv.org/abs/2507.05814</link>
<guid>https://arxiv.org/abs/2507.05814</guid>
<content:encoded><![CDATA[
arXiv:2507.05814v3 Announce Type: replace-cross 
Abstract: As critical transportation infrastructure, bridges face escalating challenges from aging and deterioration, while traditional manual inspection methods suffer from low efficiency. Although 3D point cloud technology provides a new data-driven paradigm, its application potential is often constrained by the incompleteness of real-world data, which results from missing labels and scanning occlusions. To overcome the bottleneck of insufficient generalization in existing synthetic data methods, this paper proposes a systematic framework for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring component-level instance annotations, high-fidelity color, and precise normal vectors. It can be further extended to simulate the creation of diverse and physically realistic incomplete point clouds, designed to support the training of segmentation and completion networks, respectively. Experiments demonstrate that a PointNet++ model trained with our synthetic data achieves a mean Intersection over Union (mIoU) of 84.2% in real-world bridge semantic segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance on the component completion task.
  This research offers an innovative methodology and a foundational dataset for the 3D visual analysis of bridge structures, holding significant implications for advancing the automated management and maintenance of infrastructure.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry with Neural Signed Distance Fields</title>
<link>https://arxiv.org/abs/2507.06269</link>
<guid>https://arxiv.org/abs/2507.06269</guid>
<content:encoded><![CDATA[
arXiv:2507.06269v3 Announce Type: replace-cross 
Abstract: Accurate surface estimation is critical for downstream tasks in scientific simulation, and quantifying uncertainty in implicit neural 3D representations still remains a substantial challenge due to computational inefficiencies, scalability issues, and geometric inconsistencies. However, current neural implicit surface models do not offer a principled way to quantify uncertainty, limiting their reliability in real-world applications. Inspired by recent probabilistic rendering approaches, we introduce BayesSDF, a novel probabilistic framework for uncertainty estimation in neural implicit 3D representations. Unlike radiance-based models such as Neural Radiance Fields (NeRF) or 3D Gaussian Splatting, Signed Distance Functions (SDFs) provide continuous, differentiable surface representations, making them especially well-suited for uncertainty-aware modeling. BayesSDF applies a Laplace approximation over SDF weights and derives Hessian-based metrics to estimate local geometric instability. We empirically demonstrate that these uncertainty estimates correlate strongly with surface reconstruction error across both synthetic and real-world benchmarks. By enabling surface-aware uncertainty quantification, BayesSDF lays the groundwork for more robust, interpretable, and actionable 3D perception systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2507.07236</link>
<guid>https://arxiv.org/abs/2507.07236</guid>
<content:encoded><![CDATA[
arXiv:2507.07236v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often behave inconsistently across inputs, indicating uncertainty and motivating the need for its quantification in high-stakes settings. Prior work on calibration and uncertainty quantification often focuses on individual models, overlooking the potential of model diversity. We hypothesize that LLMs make complementary predictions due to differences in training and the Zipfian nature of language, and that aggregating their outputs leads to more reliable uncertainty estimates. To leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a simple information-theoretic method that uses Jensen-Shannon Divergence to identify and aggregate well-calibrated subsets of LLMs. Experiments on binary prediction tasks demonstrate improved calibration and predictive performance compared to single-model and na\"ive ensemble baselines. In addition, we explore using MUSE as guided signals with chain-of-thought distillation to fine-tune LLMs for calibration. MUSE is available at:https://github.com/LARK-NLP-Lab/MUSE.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database</title>
<link>https://arxiv.org/abs/2507.13802</link>
<guid>https://arxiv.org/abs/2507.13802</guid>
<content:encoded><![CDATA[
arXiv:2507.13802v2 Announce Type: replace-cross 
Abstract: In the European Union, official food safety monitoring data collected by member states are submitted to the European Food Safety Authority (EFSA) and published on Zenodo. This data includes 392 million analytical results derived from over 15.2 million samples covering more than 4,000 different types of food products, offering great opportunities for artificial intelligence to analyze trends, predict hazards, and support early warning systems. However, the current format with data distributed across approximately 1000 files totaling several hundred gigabytes hinders accessibility and analysis. To address this, we introduce the CompreHensive European Food Safety (CHEFS) database, which consolidates EFSA monitoring data on pesticide residues, veterinary medicinal product residues, and chemical contaminants into a unified and structured dataset. We describe the creation and structure of the CHEFS database and demonstrate its potential by analyzing trends in European food safety monitoring data from 2000 to 2024. Our analyses explore changes in monitoring activities, the most frequently tested products, which products were most often non-compliant and which contaminants were most often found, and differences across countries. These findings highlight the CHEFS database as both a centralized data source and a strategic tool for guiding food safety policy, research, and regulation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HuggingGraph: Understanding the Supply Chain of LLM Ecosystem</title>
<link>https://arxiv.org/abs/2507.14240</link>
<guid>https://arxiv.org/abs/2507.14240</guid>
<content:encoded><![CDATA[
arXiv:2507.14240v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) leverage deep learning architectures to process and predict sequences of words, enabling them to perform a wide range of natural language processing tasks, such as translation, summarization, question answering, and content generation. As existing LLMs are often built from base models or other pre-trained models and use external datasets, they can inevitably inherit vulnerabilities, biases, or malicious components that exist in previous models or datasets. Therefore, it is critical to understand these components' origin and development process to detect potential risks, improve model fairness, and ensure compliance with regulatory frameworks. Motivated by that, this project aims to study such relationships between models and datasets, which are the central parts of the LLM supply chain. First, we design a methodology to systematically collect LLMs' supply chain information. Then, we design a new graph to model the relationships between models and datasets, which is a directed heterogeneous graph, having 402,654 nodes and 462,524 edges. Lastly, we perform different types of analysis and make multiple interesting findings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title>
<link>https://arxiv.org/abs/2508.01249</link>
<guid>https://arxiv.org/abs/2508.01249</guid>
<content:encoded><![CDATA[
arXiv:2508.01249v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's runtime traces as graph-based intermediate representations with control and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools \& data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis for sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can reduce the ASR to 3\%, with the utility drop only 1\%.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in LM-Based TTS Models via Distribution Alignment Using GFlowNets</title>
<link>https://arxiv.org/abs/2508.15442</link>
<guid>https://arxiv.org/abs/2508.15442</guid>
<content:encoded><![CDATA[
arXiv:2508.15442v3 Announce Type: replace-cross 
Abstract: Language Model (LM)-based Text-to-Speech (TTS) systems often generate hallucinated speech that deviates from input text. Existing mitigation strategies either demand excessive training resources or introduce significant inference latency. In this paper, we propose GFlOwNet-guided distribution AlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates hallucinations without relying on massive resources or inference cost. Specifically, we first conduct an uncertainty analysis, revealing a strong positive correlation between hallucination and model uncertainty. Based on this, we reformulate TTS generation as a trajectory flow optimization problem and introduce an enhanced Subtrajectory Balance objective together with a sharpened internal reward as target distribution. We further integrate reward temperature decay and learning rate optimization for stability and performance balance. Extensive experiments show that GOAT reduce over 50% character error rates on challenging test cases and lowering uncertainty by up to 58%, demonstrating its strong generalization ability and effectiveness.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skill-Aligned Fairness in Multi-Agent Learning for Collaboration in Healthcare</title>
<link>https://arxiv.org/abs/2508.18708</link>
<guid>https://arxiv.org/abs/2508.18708</guid>
<content:encoded><![CDATA[
arXiv:2508.18708v2 Announce Type: replace-cross 
Abstract: Fairness in multi-agent reinforcement learning (MARL) is often framed as a workload balance problem, overlooking agent expertise and the structured coordination required in real-world domains. In healthcare, equitable task allocation requires workload balance or expertise alignment to prevent burnout and overuse of highly skilled agents. Workload balance refers to distributing an approximately equal number of subtasks or equalised effort across healthcare workers, regardless of their expertise. We make two contributions to address this problem. First, we propose FairSkillMARL, a framework that defines fairness as the dual objective of workload balance and skill-task alignment. Second, we introduce MARLHospital, a customizable healthcare-inspired environment for modeling team compositions and energy-constrained scheduling impacts on fairness, as no existing simulators are well-suited for this problem. We conducted experiments to compare FairSkillMARL in conjunction with four standard MARL methods, and against two state-of-the-art fairness metrics. Our results suggest that fairness based solely on equal workload might lead to task-skill mismatches and highlight the need for more robust metrics that capture skill-task misalignment. Our work provides tools and a foundation for studying fairness in heterogeneous multi-agent systems where aligning effort with expertise is critical.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees</title>
<link>https://arxiv.org/abs/2508.21001</link>
<guid>https://arxiv.org/abs/2508.21001</guid>
<content:encoded><![CDATA[
arXiv:2508.21001v2 Announce Type: replace-cross 
Abstract: Kinodynamic motion planning is concerned with computing collision-free trajectories while abiding by the robot's dynamic constraints. This critical problem is often tackled using sampling-based planners (SBPs) that explore the robot's high-dimensional state space by constructing a search tree via action propagations. Although SBPs can offer global guarantees on completeness and solution quality, their performance is often hindered by slow exploration due to uninformed action sampling. Learning-based approaches can yield significantly faster runtimes, yet they fail to generalize to out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety, thus limiting their deployment on physical robots. We present Diffusion Tree (DiTree): a provably-generalizable framework leveraging diffusion policies (DPs) as informed samplers to efficiently guide state-space search within SBPs. DiTree combines DP's ability to model complex distributions of expert trajectories, conditioned on local observations, with the completeness of SBPs to yield provably-safe solutions within a few action propagation iterations for complex dynamical systems. We demonstrate DiTree's power with an implementation combining the popular RRT planner with a DP action sampler trained on a single environment. In comprehensive evaluations on OOD scenarios, DiTree achieves on average a 30% higher success rate compared to standalone DP or SBPs, on a dynamic car and Mujoco's ant robot settings (for the latter, SBPs fail completely). Beyond simulation, real-world car experiments confirm DiTree's applicability, demonstrating superior trajectory quality and robustness even under severe sim-to-real gaps. Project webpage: https://sites.google.com/view/ditree.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management</title>
<link>https://arxiv.org/abs/2508.21433</link>
<guid>https://arxiv.org/abs/2508.21433</guid>
<content:encoded><![CDATA[
arXiv:2508.21433v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative reasoning, exploration, and tool-use, a process that can result in long, expensive context histories. While state-of-the-art Software Engineering ( SE) agents like OpenHands or Cursor use LLM-based summarization to tackle this issue, it is unclear whether the increased complexity offers tangible performance benefits compared to simply omitting older observations. We present a systematic comparison of these strategies within SWE-agent on SWE-bench Verified across five diverse model configurations. We find that a simple observation-masking strategy halves cost relative to a raw agent while matching, and sometimes slightly exceeding, the solve rate of LLM summarization. For example, with Qwen3-Coder 480B, masking improves solve rate from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization at a lower cost. These results suggest that, at least within SWE-agent on SWE-bench Verified, the most effective and efficient context management can be the simplest. We release code and data for reproducibility
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Artificial Intelligence Reshaping the Landscape of the International Academic Community of Geosciences?</title>
<link>https://arxiv.org/abs/2508.20117</link>
<guid>https://arxiv.org/abs/2508.20117</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, geosciences research, bibliometric analysis, developing countries, international collaboration

Summary:<br /><br />
Through bibliometric analysis and topic modeling, this study highlights the positive impact of artificial intelligence (AI) on geosciences research. The research shows a significant increase in AI-related scientific output in recent years, indicating the transformative role of AI in the field. Of particular interest is the increased visibility of earth scientists from developing countries in the AI for Science paradigm, showcasing a more inclusive research landscape. Additionally, AI is enhancing international collaboration in geoscience-related research, fostering a more connected global research community. This comprehensive analysis underscores the importance of AI in advancing knowledge and innovation in geosciences, while also promoting diversity and collaboration among researchers worldwide. <div>
arXiv:2508.20117v2 Announce Type: replace-cross 
Abstract: Through bibliometric analysis and topic modeling, we find that artificial intelligence (AI) is positively transforming geosciences research, with a notable increase in AI-related scientific output in recent years. We are encouraged to observe that earth scientists from developing countries have gained better visibility in the recent AI for Science (AI4S) paradigm and that AI is also improving the landscape of international collaboration in geoscience-related research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beacon: Post-Training Quantization with Integrated Grid Selection</title>
<link>https://arxiv.org/abs/2508.20293</link>
<guid>https://arxiv.org/abs/2508.20293</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantization, compression, post-training quantization, scaling factors, efficient model deployment

Summary:
Beacon introduces a novel approach to per-channel post-training quantization (PTQ) that eliminates the need for manual tuning of scaling factors. By utilizing an unscaled grid and leveraging the geometry of scalar quantization, Beacon automates the selection of optimal scaling factors without relying on back-propagation or large calibration sets. This simplifies the quantization process and reduces the burden of manual intervention in model compression. Despite its simplicity, Beacon delivers competitive performance compared to existing methods, making it a practical solution for efficient model deployment. The algorithm's ability to determine optimal scaling factors on the fly enhances the scalability and usability of PTQ techniques. By addressing the key challenge of scaling factor selection, Beacon simplifies the quantization process and streamlines the deployment of large pre-trained models in memory-constrained environments. Overall, Beacon offers a promising solution for achieving efficient model compression without the need for heuristics or manual intervention. 

<br /><br />Summary: 
Keywords: Quantization, compression, post-training quantization, scaling factors, efficient model deployment 
 <div>
arXiv:2508.20293v2 Announce Type: replace-cross 
Abstract: Quantization is a widely used compression technique for reducing the memory and computation costs of large pre-trained models. A key challenge in per-channel post-training quantization (PTQ) is selecting appropriate scaling factors to replace weight values with values from a scaled integer grid. Existing methods typically fix the scale at the outset via heuristic tuning or grid search. We propose Beacon, a simple and effective algorithm that eliminates the need for such manual tuning. Beacon performs per-channel PTQ directly using an unscaled grid and automatically determines the optimal scaling factors by exploiting the geometry of scalar quantization. It does not rely on back-propagation or large calibration sets. Despite its simplicity and tuning-free nature, Beacon achieves competitive performance compared to state-of-the-art methods, making it a practical solution for efficient model deployment.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PG-Agent: An Agent Powered by Page Graph</title>
<link>https://arxiv.org/abs/2509.03536</link>
<guid>https://arxiv.org/abs/2509.03536</guid>
<content:encoded><![CDATA[
<div> Keywords: Graphical User Interface, GUI agents, Multimodal large language models, Page graphs, Retrieval-Augmented Generation

Summary:
Graphical User Interface (GUI) agents powered by advanced multimodal large language models (MLLMs) have shown promise, but current agents struggle to capture the complex transition relationships between pages. This paper introduces an automated pipeline to transform sequential episodes into page graphs, which explicitly model the graph structure of connected pages. By using Retrieval-Augmented Generation (RAG) technology, reliable perception guidelines for GUI can be retrieved from these page graphs. A tailored multi-agent framework, PG-Agent, with a task decomposition strategy is then proposed to generalize to unseen scenarios. Extensive experiments on various benchmarks demonstrate the effectiveness of PG-Agent, even with limited episodes for page graph construction.<br /><br />Summary: Graphical User Interface (GUI) agents powered by advanced multimodal large language models struggle to capture complex transitions between pages. An automated pipeline transforms sequential episodes into page graphs, enabling the retrieval of reliable perception guidelines using Retrieval-Augmented Generation. The PG-Agent framework with task decomposition strategy shows effectiveness in generalizing to unseen scenarios, as demonstrated in extensive experiments on various benchmarks. <div>
arXiv:2509.03536v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents possess significant commercial and social value, and GUI agents powered by advanced multimodal large language models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI agents usually utilize sequential episodes of multi-step operations across pages as the prior GUI knowledge, which fails to capture the complex transition relationship between pages, making it challenging for the agents to deeply perceive the GUI environment and generalize to new scenarios. Therefore, we design an automated pipeline to transform the sequential episodes into page graphs, which explicitly model the graph structure of the pages that are naturally connected by actions. To fully utilize the page graphs, we further introduce Retrieval-Augmented Generation (RAG) technology to effectively retrieve reliable perception guidelines of GUI from them, and a tailored multi-agent framework PG-Agent with task decomposition strategy is proposed to be injected with the guidelines so that it can generalize to unseen scenarios. Extensive experiments on various benchmarks demonstrate the effectiveness of PG-Agent, even with limited episodes for page graph construction.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models</title>
<link>https://arxiv.org/abs/2509.03548</link>
<guid>https://arxiv.org/abs/2509.03548</guid>
<content:encoded><![CDATA[
<div> Partially identifiable queries, causal models, Structural Causal Models, Bayesian network, multilinear programming<br />
Summary: 
The study explores partially identifiable queries in acyclic Structural Causal Models that are quasi-Markovian, focusing on scenarios where endogenous variables are observed while exogenous variables are not fully specified. The research introduces a new algorithm to simplify the construction of probability bounds by utilizing input probabilities over endogenous variables. Column generation techniques are applied to compute probability bounds in scenarios with a single intervention, demonstrating the feasibility of representing exogenous variables with polynomial cardinality. Experiments indicate the superiority of column generation methods over existing approaches in computing tight probability bounds for partially identifiable queries in causal models. <div>
arXiv:2509.03548v1 Announce Type: new 
Abstract: We investigate partially identifiable queries in a class of causal models. We focus on acyclic Structural Causal Models that are quasi-Markovian (that is, each endogenous variable is connected with at most one exogenous confounder). We look into scenarios where endogenous variables are observed (and a distribution over them is known), while exogenous variables are not fully specified. This leads to a representation that is in essence a Bayesian network where the distribution of root variables is not uniquely determined. In such circumstances, it may not be possible to precisely compute a probability value of interest. We thus study the computation of tight probability bounds, a problem that has been solved by multilinear programming in general, and by linear programming when a single confounded component is intervened upon. We present a new algorithm to simplify the construction of such programs by exploiting input probabilities over endogenous variables. For scenarios with a single intervention, we apply column generation to compute a probability bound through a sequence of auxiliary linear integer programs, thus showing that a representation with polynomial cardinality for exogenous variables is possible. Experiments show column generation techniques to be superior to existing methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method</title>
<link>https://arxiv.org/abs/2509.03550</link>
<guid>https://arxiv.org/abs/2509.03550</guid>
<content:encoded><![CDATA[
<div> Deep Reinforcement Learning, Conflict Detection and Resolution, Diffusion Probabilistic Models, Multimodal Decision-Making, Safety-Critical Task<br />
Summary:<br />
This paper introduces Diffusion-AC, a novel autonomous conflict resolution framework that integrates diffusion probabilistic models into Deep Reinforcement Learning for Conflict Detection and Resolution (CD&amp;R). Unlike existing approaches with unimodal policies, Diffusion-AC uses a reverse denoising process guided by a value function to generate a rich and multimodal action distribution. It also incorporates a Density-Progressive Safety Curriculum (DPSC) to facilitate stable learning in various traffic environments. Simulation experiments show that Diffusion-AC outperforms state-of-the-art benchmarks, achieving a high success rate and significantly reducing Near Mid-Air Collisions (NMACs) in high-density scenarios. The framework's multimodal decision-making capability enables effective alternative maneuvers, ensuring efficient and safe CD&amp;R in complex and dynamic airspace environments. <br /> <div>
arXiv:2509.03550v1 Announce Type: new 
Abstract: In the context of continuously rising global air traffic, efficient and safe Conflict Detection and Resolution (CD&amp;R) is paramount for air traffic management. Although Deep Reinforcement Learning (DRL) offers a promising pathway for CD&amp;R automation, existing approaches commonly suffer from a "unimodal bias" in their policies. This leads to a critical lack of decision-making flexibility when confronted with complex and dynamic constraints, often resulting in "decision deadlocks." To overcome this limitation, this paper pioneers the integration of diffusion probabilistic models into the safety-critical task of CD&amp;R, proposing a novel autonomous conflict resolution framework named Diffusion-AC. Diverging from conventional methods that converge to a single optimal solution, our framework models its policy as a reverse denoising process guided by a value function, enabling it to generate a rich, high-quality, and multimodal action distribution. This core architecture is complemented by a Density-Progressive Safety Curriculum (DPSC), a training mechanism that ensures stable and efficient learning as the agent progresses from sparse to high-density traffic environments. Extensive simulation experiments demonstrate that the proposed method significantly outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the most challenging high-density scenarios, Diffusion-AC not only maintains a high success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions (NMACs) by approximately 59% compared to the next-best-performing baseline, significantly enhancing the system's safety margin. This performance leap stems from its unique multimodal decision-making capability, which allows the agent to flexibly switch to effective alternative maneuvers.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents</title>
<link>https://arxiv.org/abs/2509.03581</link>
<guid>https://arxiv.org/abs/2509.03581</guid>
<content:encoded><![CDATA[
<div> dynamic planning, language models, reinforcement learning, test-time compute allocation, sequential decision-making

Summary:
This article introduces a new method for training large language models (LLMs) to reason via reinforcement learning (RL) by incorporating dynamic planning. The traditional approach of always planning before every action is computationally expensive and can degrade performance on long-horizon tasks. On the other hand, never planning limits performance. To address this, a two-stage training pipeline is proposed: supervised fine-tuning on synthetic data followed by RL to improve dynamic planning in long-horizon environments. Experiments on the Crafter environment show that agents trained with this approach are more sample-efficient and achieve complex objectives. These agents can also be guided by human-written plans to surpass independent capabilities. This work is a pioneering exploration of training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, leading to more efficient and controllable agentic systems.<br /><br /> <div>
arXiv:2509.03581v1 Announce Type: new 
Abstract: Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities. To our knowledge, this work is the first to explore training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, paving the way for more efficient, adaptive, and controllable agentic systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE</title>
<link>https://arxiv.org/abs/2509.03626</link>
<guid>https://arxiv.org/abs/2509.03626</guid>
<content:encoded><![CDATA[
<div> Generative AI, Large Language Models, Retrieval-Augmented Generation, Knowledge-Graph (KG)-SMILE, transparency<br />
Summary:<br />
Generative AI, specifically Large Language Models (LLMs), has made significant advancements but still struggles with producing inaccurate information, leading to reliability issues in critical areas such as healthcare. Retrieval-Augmented Generation (RAG) has improved accuracy by incorporating external knowledge, but remains opaque and reliant on data quality. The researchers introduced KG-SMILE, a perturbation-based framework for Graph RAG that enhances transparency by identifying influential graph entities and relations. Evaluation metrics, including fidelity, faithfulness, consistency, stability, and accuracy, demonstrate KG-SMILE's ability to provide stable and human-aligned explanations, promoting a balance between model effectiveness and interpretability in machine learning technologies.<br /> 
Summary: <div>
arXiv:2509.03626v1 Announce Type: new 
Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive progress but still produces hallucinations and unverifiable claims, limiting reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves accuracy by grounding outputs in external knowledge, especially in domains like healthcare, where precision is vital. However, RAG remains opaque and essentially a black box, heavily dependent on data quality. We developed a method-agnostic, perturbation-based framework that provides token and component-level interoperability for Graph RAG using SMILE and named it as Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing similarities, and training weighted linear surrogates, KG-SMILE identifies the graph entities and relations most influential to generated outputs, thereby making RAG more transparent. We evaluate KG-SMILE using comprehensive attribution metrics, including fidelity, faithfulness, consistency, stability, and accuracy. Our findings show that KG-SMILE produces stable, human-aligned explanations, demonstrating its capacity to balance model effectiveness with interpretability and thereby fostering greater transparency and trust in machine learning technologies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalARC: Abstract Reasoning with Causal World Models</title>
<link>https://arxiv.org/abs/2509.03636</link>
<guid>https://arxiv.org/abs/2509.03636</guid>
<content:encoded><![CDATA[
<div> Keywords: Reasoning, CausalARC, low-data, out-of-distribution, structural causal model

Summary:
CausalARC is introduced as a testbed for AI reasoning in challenging scenarios with limited data and distribution shifts. The testbed is based on a fully specified causal world model expressed as a structural causal model. Data augmentations provide various feedback types such as observational, interventional, and counterfactual in few-shot learning demonstrations. The use cases of CausalARC in language model evaluation settings include abstract reasoning with test-time training, counterfactual reasoning with in-context learning, program synthesis, and causal discovery with logical reasoning. This experimental setup aims to facilitate AI reasoning in complex and unfamiliar problem settings, showcasing the adaptability and generalization capabilities of AI systems. <br /><br />Summary: CausalARC offers a platform for AI reasoning in challenging environments by leveraging structural causal models and providing diverse feedback types for learning. Its applications span across various language model evaluation tasks, demonstrating the potential for AI systems to reason effectively in low-data and out-of-distribution scenarios. <div>
arXiv:2509.03636v1 Announce Type: new 
Abstract: Reasoning requires adaptation to novel problem settings under limited data and distribution shift. This work introduces CausalARC: an experimental testbed for AI reasoning in low-data and out-of-distribution regimes, modeled after the Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is sampled from a fully specified causal world model, formally expressed as a structural causal model. Principled data augmentations provide observational, interventional, and counterfactual feedback about the world model in the form of few-shot, in-context learning demonstrations. As a proof-of-concept, we illustrate the use of CausalARC for four language model evaluation settings: (1) abstract reasoning with test-time training, (2) counterfactual reasoning with in-context learning, (3) program synthesis, and (4) causal discovery with logical reasoning.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations</title>
<link>https://arxiv.org/abs/2509.03644</link>
<guid>https://arxiv.org/abs/2509.03644</guid>
<content:encoded><![CDATA[
<div> neurosymbolic system, Embodied-LM, logical reasoning, image schemas, spatial reasoning, Answer Set Programming

Summary:
Embodied-LM is a prototype neurosymbolic system that integrates image schemas, derived from sensorimotor experience, into large language models (LLMs) to improve logical reasoning capabilities. By operationalizing spatial reasoning using Answer Set Programming, the system enhances the interpretability and logical reasoning of LLMs on deduction problems. The system's use of cognitive structures grounded in sensory experience guides LLMs to interpret scenarios and formalize them as executable programs, leading to more effective logical reasoning. While currently focused on spatial primitives, the system establishes a computational foundation for incorporating more complex and dynamic representations in the future. <div>
arXiv:2509.03644v1 Announce Type: new 
Abstract: Despite significant progress in natural language understanding, Large Language Models (LLMs) remain error-prone when performing logical reasoning, often lacking the robust mental representations that enable human-like comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that grounds understanding and logical reasoning in schematic representations based on image schemas-recurring patterns derived from sensorimotor experience that structure human cognition. Our system operationalizes the spatial foundations of these cognitive structures using declarative spatial reasoning within Answer Set Programming. Through evaluation on logical deduction problems, we demonstrate that LLMs can be guided to interpret scenarios through embodied cognitive structures, that these structures can be formalized as executable programs, and that the resulting representations support effective logical reasoning with enhanced interpretability. While our current implementation focuses on spatial primitives, it establishes the computational foundation for incorporating more complex and dynamic representations.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.03646</link>
<guid>https://arxiv.org/abs/2509.03646</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, reasoning hierarchy, strategic planning, entropy dynamics

Summary:
Reinforcement Learning has been successful in enhancing the reasoning abilities of Large Language Models (LLMs), with phenomena like "aha moments" and entropy dynamics being part of an emergent reasoning hierarchy. The study uncovers a two-phase dynamic where models initially focus on procedural correctness and low-level skills before transitioning to high-level strategic planning. The analysis highlights the inefficiencies in current RL algorithms like GRPO, which do not focus optimization efforts on high-impact planning tokens. The proposed HICRA algorithm shows significant performance improvements by concentrating on the strategic bottleneck. Semantic entropy is identified as a better measure of strategic exploration compared to token-level entropy. The research emphasizes the importance of focusing on high-level planning to enhance advanced reasoning capabilities in LLMs.<br /><br />Summary: <div>
arXiv:2509.03646v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification</title>
<link>https://arxiv.org/abs/2509.03649</link>
<guid>https://arxiv.org/abs/2509.03649</guid>
<content:encoded><![CDATA[
<div> Segmentation algorithms, SHAP method, explainable AI, time series classification, attribution quality<br />
Summary:
- Explaining the predictions of complex Time Series Classification models is crucial for understanding them.
- SHapley Additive exPlanations (SHAP) is a popular attribution method, but its computational complexity limits its practicality for long time series.
- Aggregating features via segmentation has been shown to reduce SHAP running time significantly.
- The choice of segmentation strategy greatly impacts the explanation quality, but the number of segments plays a more significant role than the specific method.
- Equal-length segmentation consistently outperforms many custom time series segmentation algorithms.
- A novel attribution normalization technique that weights segments by their length has been introduced and shown to consistently improve attribution quality.<br /><br />Summary: <div>
arXiv:2509.03649v1 Announce Type: new 
Abstract: Explainable AI (XAI) has become an increasingly important topic for understanding and attributing the predictions made by complex Time Series Classification (TSC) models. Among attribution methods, SHapley Additive exPlanations (SHAP) is widely regarded as an excellent attribution method; but its computational complexity, which scales exponentially with the number of features, limits its practicality for long time series. To address this, recent studies have shown that aggregating features via segmentation, to compute a single attribution value for a group of consecutive time points, drastically reduces SHAP running time. However, the choice of the optimal segmentation strategy remains an open question. In this work, we investigated eight different Time Series Segmentation algorithms to understand how segment compositions affect the explanation quality. We evaluate these approaches using two established XAI evaluation methodologies: InterpretTime and AUC Difference. Through experiments on both Multivariate (MTS) and Univariate Time Series (UTS), we find that the number of segments has a greater impact on explanation quality than the specific segmentation method. Notably, equal-length segmentation consistently outperforms most of the custom time series segmentation algorithms. Furthermore, we introduce a novel attribution normalisation technique that weights segments by their length and we show that it consistently improves attribution quality.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming</title>
<link>https://arxiv.org/abs/2509.03728</link>
<guid>https://arxiv.org/abs/2509.03728</guid>
<content:encoded><![CDATA[
<div> persona, red-teaming, AI governance, safety research, adversarial prompts <br />
Summary: <br />
The paper introduces PersonaTeaming, a method for incorporating personas in automated red-teaming to identify potential risks posed by AI models. By utilizing "red-teaming expert" and "regular AI user" personas in prompt generation, the method explores a wider range of adversarial strategies. A dynamic algorithm is developed to generate various persona types adapted to different seed prompts, leading to improved attack success rates (up to 144.1%) while maintaining prompt diversity compared to RainbowPlus. Metrics to measure "mutation distance" are introduced to assess prompt diversity. The study highlights the potential of persona mutation in enhancing automated red-teaming approaches and suggests opportunities for exploring the synergy between automated and human red-teaming strategies. <br /> <div>
arXiv:2509.03728v1 Announce Type: new 
Abstract: Recent developments in AI governance and safety research have called for red-teaming methods that can effectively surface potential risks posed by AI models. Many of these calls have emphasized how the identities and backgrounds of red-teamers can shape their red-teaming strategies, and thus the kinds of risks they are likely to uncover. While automated red-teaming approaches promise to complement human red-teaming by enabling larger-scale exploration of model behavior, current approaches do not consider the role of identity. As an initial step towards incorporating people's background and identities in automated red-teaming, we develop and evaluate a novel method, PersonaTeaming, that introduces personas in the adversarial prompt generation process to explore a wider spectrum of adversarial strategies. In particular, we first introduce a methodology for mutating prompts based on either "red-teaming expert" personas or "regular AI user" personas. We then develop a dynamic persona-generating algorithm that automatically generates various persona types adaptive to different seed prompts. In addition, we develop a set of new metrics to explicitly measure the "mutation distance" to complement existing diversity measurements of adversarial prompts. Our experiments show promising improvements (up to 144.1%) in the attack success rates of adversarial prompts through persona mutation, while maintaining prompt diversity, compared to RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the strengths and limitations of different persona types and mutation methods, shedding light on future opportunities to explore complementarities between automated and human red-teaming approaches.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Personality Illusion: Revealing Dissociation Between Self-Reports &amp; Behavior in LLMs</title>
<link>https://arxiv.org/abs/2509.03730</link>
<guid>https://arxiv.org/abs/2509.03730</guid>
<content:encoded><![CDATA[
<div> personality traits, Large Language Models, self-reported traits, behavioral validation, targeted interventions 
Summary: 
The study explores how Large Language Models (LLMs) exhibit personality traits similar to humans, like agreeableness and self-regulation. The research focuses on three dimensions: the evolution of trait profiles during training, the relationship between self-reported traits and behavior, and the impact of interventions on trait expression and behavior. Results show that instructional alignment enhances trait stability and correlations, akin to human data. However, self-reported traits do not consistently predict behavior, highlighting a disconnect between trait expression and behavior. Persona injection influences self-reports but lacks a significant impact on actual behavior. The findings challenge assumptions about LLM personality and highlight the necessity for further evaluation on alignment and interpretability. <br /><br />Summary: <div>
arXiv:2509.03730v1 Announce Type: new 
Abstract: Personality traits have long been studied as predictors of human behavior.Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation</title>
<link>https://arxiv.org/abs/2509.03736</link>
<guid>https://arxiv.org/abs/2509.03736</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, human-subject research, dialogue, internal consistency, simulation

Summary:
<br /><br />
The study evaluates whether synthetic agents generated by Large Language Models (LLMs) can be reliable substitutes for human participants in research. While previous research has focused on the correspondence between LLM-generated survey data and human responses, this study examines the internal consistency of LLMs in different experimental settings. By revealing the agent's internal state and assessing its behavior in dialogue situations, the study reveals significant inconsistencies in LLMs across model families and sizes. Despite generating responses similar to humans, the agents lack internal consistency, highlighting a crucial limitation in their ability to accurately represent real participants in research. The publicly accessible simulation code and data provide valuable insights into the challenges of using LLMs in social science research. <div>
arXiv:2509.03736v1 Announce Type: new 
Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the notion that synthetic agents can serve as substitutes for real participants in human-subject research. In an effort to evaluate the merits of this claim, social science researchers have largely focused on whether LLM-generated survey data corresponds to that of a human counterpart whom the LLM is prompted to represent. In contrast, we address a more fundamental question: Do agents maintain internal consistency, retaining similar behaviors when examined under different experimental settings? To this end, we develop a study designed to (a) reveal the agent's internal state and (b) examine agent behavior in a basic dialogue setting. This design enables us to explore a set of behavioral hypotheses to assess whether an agent's conversation behavior is consistent with what we would expect from their revealed internal state. Our findings on these hypotheses show significant internal inconsistencies in LLMs across model families and at differing model sizes. Most importantly, we find that, although agents may generate responses matching those of their human counterparts, they fail to be internally consistent, representing a critical gap in their capabilities to accurately substitute for real participants in human-subject research. Our simulation code and data are publicly accessible.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs</title>
<link>https://arxiv.org/abs/2509.03768</link>
<guid>https://arxiv.org/abs/2509.03768</guid>
<content:encoded><![CDATA[
<div> Keywords: Offshore Wind, Maintenance, Safety, Large Language Models, RAGuard <br />
Summary:<br />
The study introduces RAGuard, an advanced framework to improve accuracy and safety in Offshore Wind (OSW) maintenance. By combining safety-critical documents with technical manuals, RAGuard enhances retrieval performance. The framework issues parallel queries to two indices, allocating separate budgets for knowledge and safety to ensure technical depth and safety coverage. A SafetyClamp extension further enhances safety by increasing Safety Recall while maintaining Technical Recall. Evaluation across different retrieval paradigms shows significant improvements in Safety Recall with RAGuard. These results indicate the potential of RAGuard and SafetyClamp in establishing a new standard for integrating safety assurance into decision support systems using Large Language Models in critical maintenance situations.<br /> 
Summary: <div>
arXiv:2509.03768v1 Announce Type: new 
Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet conventional Large Language Models (LLMs) often fail when confronted with highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced Retrieval-Augmented Generation (RAG) framework that explicitly integrates safety-critical documents alongside technical manuals.By issuing parallel queries to two indices and allocating separate retrieval budgets for knowledge and safety, RAGuard guarantees both technical depth and safety coverage. We further develop a SafetyClamp extension that fetches a larger candidate pool, "hard-clamping" exact slot guarantees to safety. We evaluate across sparse (BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms, measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than 50\% in RAGuard, while maintaining Technical Recall above 60\%. These results demonstrate that RAGuard and SafetyClamp have the potential to establish a new standard for integrating safety assurance into LLM-powered decision support in critical maintenance contexts.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLM-Based Agents for Intelligent Supply Chain Planning</title>
<link>https://arxiv.org/abs/2509.03811</link>
<guid>https://arxiv.org/abs/2509.03811</guid>
<content:encoded><![CDATA[
<div> Keywords: supply chain management, AI technologies, large language models, e-commerce platform, JD.com

Summary: 
Supply chain management is a critical concept involving various entities in the movement of physical products. The development of AI technologies, particularly large language models, has opened new possibilities in addressing supply chain planning challenges. This work introduces the Supply Chain Planning Agent (SCPA) framework, which utilizes AI technology to understand domain knowledge, tasks decomposition, and evidence-based planning reports generation. The SCPA framework was successfully deployed in a real-world scenario at JD.com, showcasing improved efficiency, accuracy, and stock availability. By leveraging AI tools, the SCPA framework effectively reduced labor while enhancing key metrics in supply chain operations. This demonstrates the practical applications of AI technologies in optimizing supply chain planning processes. <div>
arXiv:2509.03811v1 Announce Type: new 
Abstract: In supply chain management, planning is a critical concept. The movement of physical products across different categories, from suppliers to warehouse management, to sales, and logistics transporting them to customers, entails the involvement of many entities. It covers various aspects such as demand forecasting, inventory management, sales operations, and replenishment. How to collect relevant data from an e-commerce platform's perspective, formulate long-term plans, and dynamically adjust them based on environmental changes, while ensuring interpretability, efficiency, and reliability, is a practical and challenging problem. In recent years, the development of AI technologies, especially the rapid progress of large language models, has provided new tools to address real-world issues. In this work, we construct a Supply Chain Planning Agent (SCPA) framework that can understand domain knowledge, comprehend the operator's needs, decompose tasks, leverage or create new tools, and return evidence-based planning reports. We deploy this framework in JD.com's real-world scenario, demonstrating the feasibility of LLM-agent applications in the supply chain. It effectively reduced labor and improved accuracy, stock availability, and other key metrics.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.03817</link>
<guid>https://arxiv.org/abs/2509.03817</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, multi-agent systems, meta-cognitive actions, reinforcement learning, dynamic strategies<br />
Summary: <br />
- The Meta-Policy Deliberation Framework (MPDF) introduces decentralized policies for agents in large language models, focusing on meta-cognitive actions like Persist, Refine, and Concede. 
- SoftRankPO, a novel reinforcement learning algorithm, stabilizes training by shaping advantages based on smooth normal quantiles, enhancing adaptability in multi-agent systems.
- Traditional policy gradients are replaced with SoftRankPO to handle reward variance and improve overall performance in reasoning benchmarks.
- MPDF with SoftRankPO demonstrates a 4-5% absolute gain in average accuracy across mathematical and general reasoning tasks compared to existing algorithms.
- This work highlights the importance of adaptive, meta-cognitive strategies in multi-agent systems, emphasizing the shift from fixed protocols to dynamic deliberative approaches. <br /> 
Summary: <div>
arXiv:2509.03817v1 Announce Type: new 
Abstract: Multi-agent systems of large language models (LLMs) show promise for complex reasoning, but their effectiveness is often limited by fixed collaboration protocols. These frameworks typically focus on macro-level orchestration while overlooking agents' internal deliberative capabilities. This critical meta-cognitive blindspot treats agents as passive executors unable to adapt their strategy based on internal cognitive states like uncertainty or confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where agents learn a decentralized policy over a set of high-level meta-cognitive actions: Persist, Refine, and Concede. To overcome the instability of traditional policy gradients in this setting, we develop SoftRankPO, a novel reinforcement learning algorithm. SoftRankPO stabilizes training by shaping advantages based on the rank of rewards mapped through smooth normal quantiles, making the learning process robust to reward variance. Experiments show that MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms. Our work presents a paradigm for learning adaptive, meta-cognitive policies for multi-agent LLM systems, shifting the focus from designing fixed protocols to learning dynamic, deliberative strategies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2509.03827</link>
<guid>https://arxiv.org/abs/2509.03827</guid>
<content:encoded><![CDATA[
<div> benchmark, decision scenarios, social policymaking, homelessness alleviation, Capability Approach

Summary:<br />
- The article evaluates the alignment of Large Language Models (LLMs) with domain experts in social policymaking for homelessness alleviation.
- A novel benchmark is created with decision scenarios and policy choices across four geographies.
- Policies are based on the Capability Approach for human development.
- An automated pipeline connects benchmarked policies to an agent-based model for analysis.
- The study shows potential for LLMs to provide valuable insights for social policy making with responsible guidance from local experts. 

Summary: <div>
arXiv:2509.03827v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes domains. Their capacity to process vast amounts of unstructured data, explore flexible scenarios, and handle a diversity of contextual factors can make them uniquely suited to provide new insights for the complexity of social policymaking. This article evaluates whether LLMs' are aligned with domain experts (and among themselves) to inform social policymaking on the subject of homelessness alleviation - a challenge affecting over 150 million people worldwide. We develop a novel benchmark comprised of decision scenarios with policy choices across four geographies (South Bend, USA; Barcelona, Spain; Johannesburg, South Africa; Macau SAR, China). The policies in scope are grounded in the conceptual framework of the Capability Approach for human development. We also present an automated pipeline that connects the benchmarked policies to an agent-based model, and we explore the social impact of the recommended policies through simulated social scenarios. The paper results reveal promising potential to leverage LLMs for social policy making. If responsible guardrails and contextual calibrations are introduced in collaboration with local domain experts, LLMs can provide humans with valuable insights, in the form of alternative policies at scale.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic Model Context Protocol Framework for Medical Concept Standardization</title>
<link>https://arxiv.org/abs/2509.03828</link>
<guid>https://arxiv.org/abs/2509.03828</guid>
<content:encoded><![CDATA[
<div> Observational Medical Outcomes Partnership, OMOP common data model, mapping, large language models, Model Context Protocol <br />
Summary: The article introduces a new system for mapping source medical terms to OMOP standard concepts using the Model Context Protocol (MCP). This system aims to prevent hallucination in large language models (LLMs) and improve efficiency and accuracy in the mapping process. By allowing LLMs to interact with external resources and tools, the system provides real-time vocabulary lookups and structured reasoning outputs. This approach enables explainable mapping and enhances both exploratory and production environments for healthcare research. Overall, the system offers a zero-training solution to improve the standardization of health data using the OMOP CDM, reducing errors and resource-intensive processes in medical term mapping. <br /><br /> <div>
arXiv:2509.03828v1 Announce Type: new 
Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM) provides a standardized representation of heterogeneous health data to support large-scale, multi-institutional research. One critical step in data standardization using OMOP CDM is the mapping of source medical terms to OMOP standard concepts, a procedure that is resource-intensive and error-prone. While large language models (LLMs) have the potential to facilitate this process, their tendency toward hallucination makes them unsuitable for clinical deployment without training and expert validation. Here, we developed a zero-training, hallucination-preventive mapping system based on the Model Context Protocol (MCP), a standardized and secure framework allowing LLMs to interact with external resources and tools. The system enables explainable mapping and significantly improves efficiency and accuracy with minimal effort. It provides real-time vocabulary lookups and structured reasoning outputs suitable for immediate use in both exploratory and production environments.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai</title>
<link>https://arxiv.org/abs/2509.03830</link>
<guid>https://arxiv.org/abs/2509.03830</guid>
<content:encoded><![CDATA[
<div> Keywords: historic urban quarters, tourism perception, AI-powered framework, multimodal data, social media

Summary:
This study introduces an AI-powered framework to analyze tourist perception in historic urban quarters using data from social media. The framework incorporates focal point extraction, color theme analysis, and sentiment mining to understand how tourists perceive these environments. By analyzing tourist-shared photos, visual focus areas and dominant colors are identified to assess aesthetic preferences. Color themes are compared between social media photos and real-world street views, revealing potential gaps in visual expectations. Tourist reviews are analyzed using a hybrid sentiment analysis approach to evaluate satisfaction across different dimensions. The results show spatial variations in aesthetic appeal and emotional response, providing insights for sustainable urban planning and heritage conservation. The integrated approach presented in this study offers a comprehensive way to decode tourist perception and inform decision-making for the design of engaging public spaces. 

<br /><br />Summary: <div>
arXiv:2509.03830v1 Announce Type: new 
Abstract: Historic urban quarters play a vital role in preserving cultural heritage while serving as vibrant spaces for tourism and everyday life. Understanding how tourists perceive these environments is essential for sustainable, human-centered urban planning. This study proposes a multidimensional AI-powered framework for analyzing tourist perception in historic urban quarters using multimodal data from social media. Applied to twelve historic quarters in central Shanghai, the framework integrates focal point extraction, color theme analysis, and sentiment mining. Visual focus areas are identified from tourist-shared photos using a fine-tuned semantic segmentation model. To assess aesthetic preferences, dominant colors are extracted using a clustering method, and their spatial distribution across quarters is analyzed. Color themes are further compared between social media photos and real-world street views, revealing notable shifts. This divergence highlights potential gaps between visual expectations and the built environment, reflecting both stylistic preferences and perceptual bias. Tourist reviews are evaluated through a hybrid sentiment analysis approach combining a rule-based method and a multi-task BERT model. Satisfaction is assessed across four dimensions: tourist activities, built environment, service facilities, and business formats. The results reveal spatial variations in aesthetic appeal and emotional response. Rather than focusing on a single technical innovation, this framework offers an integrated, data-driven approach to decoding tourist perception and contributes to informed decision-making in tourism, heritage conservation, and the design of aesthetically engaging public spaces.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures</title>
<link>https://arxiv.org/abs/2509.03857</link>
<guid>https://arxiv.org/abs/2509.03857</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Knowledge Graphs, Reliability evaluation, Large Language Models, Real-time monitoring

Summary: 
This article introduces a systematic methodology for evaluating the reliability of Generative AI (GEN AI) models, which are prone to issues such as hallucinations, semantic drift, and biases. The proposed methodology utilizes deterministic and Large Language Model (LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate the performance of GEN AI models. By constructing parallel KGs using rule-based methods and real-time textual data streams, structural deviations and semantic discrepancies can be quantified using established KG metrics. An automated real-time monitoring framework computes deviations between deterministic and LLM-generated KGs and proactively identifies significant deviations, flagging semantic anomalies or hallucinations. This metric-driven comparison between deterministic and dynamically generated KGs provides a robust and scalable evaluation framework for assessing the reliability of GEN AI models.<br /><br />Summary: <div>
arXiv:2509.03857v1 Announce Type: new 
Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains but present substantial challenges due to reliability concerns, including hallucinations, semantic drift, and inherent biases. These models typically operate as black-boxes, complicating transparent and objective evaluation. Current evaluation methods primarily depend on subjective human assessment, limiting scalability, transparency, and effectiveness. This research proposes a systematic methodology using deterministic and Large Language Model (LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN AI reliability. We construct two parallel KGs: (i) a deterministic KG built using explicit rule-based methods, predefined ontologies, domain-specific dictionaries, and structured entity-relation extraction rules, and (ii) an LLM-generated KG dynamically derived from real-time textual data streams such as live news articles. Utilizing real-time news streams ensures authenticity, mitigates biases from repetitive training, and prevents adaptive LLMs from bypassing predefined benchmarks through feedback memorization. To quantify structural deviations and semantic discrepancies, we employ several established KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring framework continuously computes deviations between deterministic and LLM-generated KGs. By establishing dynamic anomaly thresholds based on historical structural metric distributions, our method proactively identifies and flags significant deviations, thus promptly detecting semantic anomalies or hallucinations. This structured, metric-driven comparison between deterministic and dynamically generated KGs delivers a robust and scalable evaluation framework.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expedition &amp; Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata</title>
<link>https://arxiv.org/abs/2509.03863</link>
<guid>https://arxiv.org/abs/2509.03863</guid>
<content:encoded><![CDATA[
<div> Keywords: visual patterns, cellular automata, exploration, novelty search, artificial life
Summary: 
Expedition and Expansion (E&amp;E) is a new hybrid exploration strategy for discovering diverse visual patterns in continuous cellular automata (CA). E&amp;E combines local novelty-driven expansions with goal-directed expeditions using a Vision-Language Model (VLM) to generate linguistic goals. By operating in semantic spaces aligning with human perception, E&amp;E enhances the interpretability and relevance of discovered behaviors. Tested on Flow Lenia, E&amp;E consistently uncovers more diverse solutions than existing methods. A genealogical analysis reveals that expedition-originating solutions play a significant role in long-term exploration, unlocking new behavioral niches. E&amp;E breaks through local novelty boundaries and explores behavioral landscapes in human-aligned, interpretable ways, providing a promising framework for open-ended exploration in artificial life and beyond.<br /><br />Summary: <div>
arXiv:2509.03863v1 Announce Type: new 
Abstract: Discovering diverse visual patterns in continuous cellular automata (CA) is challenging due to the vastness and redundancy of high-dimensional behavioral spaces. Traditional exploration methods like Novelty Search (NS) expand locally by mutating known novel solutions but often plateau when local novelty is exhausted, failing to reach distant, unexplored regions. We introduce Expedition and Expansion (E&amp;E), a hybrid strategy where exploration alternates between local novelty-driven expansions and goal-directed expeditions. During expeditions, E&amp;E leverages a Vision-Language Model (VLM) to generate linguistic goals--descriptions of interesting but hypothetical patterns that drive exploration toward uncharted regions. By operating in semantic spaces that align with human perception, E&amp;E both evaluates novelty and generates goals in conceptually meaningful ways, enhancing the interpretability and relevance of discovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich, emergent behaviors, E&amp;E consistently uncovers more diverse solutions than existing exploration methods. A genealogical analysis further reveals that solutions originating from expeditions disproportionately influence long-term exploration, unlocking new behavioral niches that serve as stepping stones for subsequent search. These findings highlight E&amp;E's capacity to break through local novelty boundaries and explore behavioral landscapes in human-aligned, interpretable ways, offering a promising template for open-ended exploration in artificial life and beyond.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace</title>
<link>https://arxiv.org/abs/2509.03890</link>
<guid>https://arxiv.org/abs/2509.03890</guid>
<content:encoded><![CDATA[
<div> marketplace, agentic assistant, LLM-powered, conversational search, task automation
Summary: 
The paper introduces a novel approach to simplify interactions on C2C e-commerce platforms using an agentic assistant powered by Large Language Models (LLMs). This assistant shifts the primary interaction model from complex GUIs to an intuitive AI agent that interprets natural language commands. For sellers, the assistant automates tasks like listing updates and bulk messaging, while facilitating a more efficient product discovery process for buyers through conversational search. The architecture for the Facebook Marketplace Assistant (FaMA) is presented, highlighting the benefits of this conversational paradigm as a more accessible and efficient alternative to traditional app interfaces. Experimental results demonstrate FaMA's high task success rate of 98% and up to a 2x speedup in interaction time.<br /><br />Summary: <div>
arXiv:2509.03890v1 Announce Type: new 
Abstract: The emergence of agentic AI, powered by Large Language Models (LLMs), marks a paradigm shift from reactive generative systems to proactive, goal-oriented autonomous agents capable of sophisticated planning, memory, and tool use. This evolution presents a novel opportunity to address long-standing challenges in complex digital environments. Core tasks on Consumer-to-Consumer (C2C) e-commerce platforms often require users to navigate complex Graphical User Interfaces (GUIs), making the experience time-consuming for both buyers and sellers. This paper introduces a novel approach to simplify these interactions through an LLM-powered agentic assistant. This agent functions as a new, conversational entry point to the marketplace, shifting the primary interaction model from a complex GUI to an intuitive AI agent. By interpreting natural language commands, the agent automates key high-friction workflows. For sellers, this includes simplified updating and renewal of listings, and the ability to send bulk messages. For buyers, the agent facilitates a more efficient product discovery process through conversational search. We present the architecture for Facebook Marketplace Assistant (FaMA), arguing that this agentic, conversational paradigm provides a lightweight and more accessible alternative to traditional app interfaces, allowing users to manage their marketplace activities with greater efficiency. Experiments show FaMA achieves a 98% task success rate on solving complex tasks on the marketplace and enables up to a 2x speedup on interaction time.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.03906</link>
<guid>https://arxiv.org/abs/2509.03906</guid>
<content:encoded><![CDATA[
<div> fine-tuned, synthetic reasoning, reinforcement learning, report generation, visual question answering<br />
<br />
Summary:<br />
DeepMedix-R1 is a comprehensive medical foundation model for chest X-ray interpretation that focuses on improving transparency and interpretability. It undergoes a sequential training process, starting with fine-tuning on CXR instruction data, followed by exposure to synthetic reasoning samples for cold-start reasoning, and refinement through online reinforcement learning. The model produces both answers and reasoning steps tied to local image regions, leading to significant improvements in report generation and visual question answering tasks compared to existing models. A benchmarking framework called Report Arena is proposed for robust evaluation, showcasing the model's answer quality superiority. Expert reviews confirm that DeepMedix-R1's generated reasoning steps offer greater interpretability and clinical plausibility than a well-known model. This work advances the development of medical foundation models towards holistic, transparent, and clinically actionable approaches for CXR interpretation. <div>
arXiv:2509.03906v1 Announce Type: new 
Abstract: Medical foundation models (FMs) have shown tremendous promise amid the rapid advancements in artificial intelligence (AI) technologies. However, current medical FMs typically generate answers in a black-box manner, lacking transparent reasoning processes and locally grounded interpretability, which hinders their practical clinical deployments. To this end, we introduce DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It leverages a sequential training pipeline: initially fine-tuned on curated CXR instruction data to equip with fundamental CXR interpretation capabilities, then exposed to high-quality synthetic reasoning samples to enable cold-start reasoning, and finally refined via online reinforcement learning to enhance both grounded reasoning quality and generation performance. Thus, the model produces both an answer and reasoning steps tied to the image's local regions for each query. Quantitative evaluation demonstrates substantial improvements in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent) tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking framework using advanced language models to evaluate answer quality, further highlighting the superiority of DeepMedix-R1. Expert review of generated reasoning steps reveals greater interpretability and clinical plausibility compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall preference). Collectively, our work advances medical FM development toward holistic, transparent, and clinically actionable modeling for CXR interpretation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Infinite Domain Parameters in Planning Through Best-First Search with Delayed Partial Expansions</title>
<link>https://arxiv.org/abs/2509.03953</link>
<guid>https://arxiv.org/abs/2509.03953</guid>
<content:encoded><![CDATA[
<div> Control parameters, automated planning, decision variables, search algorithm, completeness
<br />
The paper introduces a novel approach in automated planning that explicitly treats control parameters as decision points in the search space. Existing methods have often treated control parameters as constraints rather than decision points. The proposed best-first, heuristic search algorithm operates over infinite decision spaces and utilizes delayed partial expansion to incrementally explore successors. This approach proves completeness under certain conditions and offers a competitive alternative for planning problems involving control parameters. The algorithm's ability to handle control parameters as true decision points sets it apart from existing methods, potentially leading to more efficient and effective planning solutions.
<br /><br />Summary: <div>
arXiv:2509.03953v1 Announce Type: new 
Abstract: In automated planning, control parameters extend standard action representations through the introduction of continuous numeric decision variables. Existing state-of-the-art approaches have primarily handled control parameters as embedded constraints alongside other temporal and numeric restrictions, and thus have implicitly treated them as additional constraints rather than as decision points in the search space. In this paper, we propose an efficient alternative that explicitly handles control parameters as true decision points within a systematic search scheme. We develop a best-first, heuristic search algorithm that operates over infinite decision spaces defined by control parameters and prove a notion of completeness in the limit under certain conditions. Our algorithm leverages the concept of delayed partial expansion, where a state is not fully expanded but instead incrementally expands a subset of its successors. Our results demonstrate that this novel search algorithm is a competitive alternative to existing approaches for solving planning problems involving control parameters.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World Model Implanting for Test-time Adaptation of Embodied Agents</title>
<link>https://arxiv.org/abs/2509.03956</link>
<guid>https://arxiv.org/abs/2509.03956</guid>
<content:encoded><![CDATA[
<div> world model implanting framework, embodied AI, adaptation, domain-specific knowledge, robust

Summary: 
The article introduces the world model implanting framework (WorMI) for embodied AI, aiming to enable agents to adapt to new domains without extensive retraining. WorMI combines large language models (LLMs) with independently learned world models through test-time composition. It uses a prototype-based retrieval approach for world models and a compound attention method to integrate knowledge and align representations. The framework allows seamless implantation and removal of world models, enhancing adaptability. Evaluation on VirtualHome and ALFWorld benchmarks shows superior zero-shot and few-shot performance compared to LLM-based approaches, indicating potential for real-world deployment in scenarios requiring adaptability and data efficiency. <div>
arXiv:2509.03956v1 Announce Type: new 
Abstract: In embodied AI, a persistent challenge is enabling agents to robustly adapt to novel domains without requiring extensive data collection or retraining. To address this, we present a world model implanting framework (WorMI) that combines the reasoning capabilities of large language models (LLMs) with independently learned, domain-specific world models through test-time composition. By allowing seamless implantation and removal of the world models, the embodied agent's policy achieves and maintains cross-domain adaptability. In the WorMI framework, we employ a prototype-based world model retrieval approach, utilizing efficient trajectory-based abstract representation matching, to incorporate relevant models into test-time composition. We also develop a world-wise compound attention method that not only integrates the knowledge from the retrieved world models but also aligns their intermediate representations with the reasoning model's representation within the agent's policy. This framework design effectively fuses domain-specific knowledge from multiple world models, ensuring robust adaptation to unseen domains. We evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating superior zero-shot and few-shot performance compared to several LLM-based approaches across a range of unseen domains. These results highlight the frameworks potential for scalable, real-world deployment in embodied agent scenarios where adaptability and data efficiency are essential.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent</title>
<link>https://arxiv.org/abs/2509.03990</link>
<guid>https://arxiv.org/abs/2509.03990</guid>
<content:encoded><![CDATA[
<div> Reinforcement-learning, Large language model, Transferable policies, Meta-Policy Memory, Robustness<br />
Summary:<br />
The article introduces Meta-Policy Reflexion (MPR), a framework that consolidates reflections from Large Language Model (LLM) agents into a Meta-Policy Memory (MPM) to improve performance. MPR utilizes soft memory-guided decoding and hard rule admissibility checks to enhance adaptability without requiring parameter updates. The framework externalizes reusable corrective knowledge and enforces domain constraints to reduce unsafe actions. Empirical results in a text-based agent environment show improved execution accuracy and robustness compared to existing Reflexion approaches. Rule admissibility further enhances stability. The study analyzes the mechanisms behind these results, discusses scalability and potential failure modes, and suggests future directions for extensions in multimodal and multi-agent scenarios. This innovative approach addresses common challenges faced by LLM agents and offers a promising solution for improving cross-task adaptability and overall performance. <br /><br />Summary: <div>
arXiv:2509.03990v1 Announce Type: new 
Abstract: Large language model (LLM) agents achieve impressive single-task performance but commonly exhibit repeated failures, inefficient exploration, and limited cross-task adaptability. Existing reflective strategies (e.g., Reflexion, ReAct) improve per-episode behavior but typically produce ephemeral, task-specific traces that are not reused across tasks. Reinforcement-learning based alternatives can produce transferable policies but require substantial parameter updates and compute. In this work we introduce Meta-Policy Reflexion (MPR): a hybrid framework that consolidates LLM-generated reflections into a structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at inference time through two complementary mechanisms soft memory-guided decoding and hard rule admissibility checks(HAC). MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection. We formalize the MPM representation, present algorithms for update and decoding, and validate the approach in a text-based agent environment following the experimental protocol described in the provided implementation (AlfWorld-based). Empirical results reported in the supplied material indicate consistent gains in execution accuracy and robustness when compared to Reflexion baselines; rule admissibility further improves stability. We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi?agent extensions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPBO: LLM-powered Optimization for Local Search PBO Solvers</title>
<link>https://arxiv.org/abs/2509.04007</link>
<guid>https://arxiv.org/abs/2509.04007</guid>
<content:encoded><![CDATA[
<div> Keywords: Pseudo-Boolean Optimization, Local search solvers, Large Language Models, AutoPBO, Performance improvement

Summary:<br />
Pseudo-Boolean Optimization (PBO) is a powerful framework for modeling combinatorial problems using PB constraints. Local search solvers, crucial for PBO solving, rely heavily on internal heuristics for efficient search, often requiring manual tuning. This study introduces AutoPBO, an innovative framework utilizing Large Language Models (LLMs) to automatically enhance PBO local search solvers. Experimentation on various benchmarks demonstrates AutoPBO's significant performance improvements over previous local search methods and competitive performance against state-of-the-art competitors NuPBO, OraSLS, PBO-IHS, RoundingSat, Gurobi, and SCIP. The results highlight AutoPBO's potential to automate local search solver design, illustrating promise in advancing optimization techniques. <br />Summary: <div>
arXiv:2509.04007v1 Announce Type: new 
Abstract: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling combinatorial problems through pseudo-Boolean (PB) constraints. Local search solvers have shown excellent performance in PBO solving, and their efficiency is highly dependent on their internal heuristics to guide the search. Still, their design often requires significant expert effort and manual tuning in practice. While Large Language Models (LLMs) have demonstrated potential in automating algorithm design, their application to optimizing PBO solvers remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered framework to automatically enhance PBO local search solvers. We conduct experiments on a broad range of four public benchmarks, including one real-world benchmark, a benchmark from PB competition, an integer linear programming optimization benchmark, and a crafted combinatorial benchmark, to evaluate the performance improvement achieved by AutoPBO and compare it with six state-of-the-art competitors, including two local search PBO solvers NuPBO and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates significant improvements over previous local search approaches, while maintaining competitive performance compared to state-of-the-art competitors. The results suggest that AutoPBO offers a promising approach to automating local search solver design.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.04027</link>
<guid>https://arxiv.org/abs/2509.04027</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, Chain-of-Thought, CoT-Space, reasoning-level semantic space

Summary: 
Reinforcement Learning (RL) is crucial for improving the reasoning capabilities of Large Language Models (LLMs). Traditional token-level RL frameworks do not align with complex, multi-step thought processes like Chain-of-Thought (CoT). To bridge this gap, a new theoretical framework called CoT-Space is introduced, which transforms LLM reasoning into an optimization process within a continuous reasoning-level semantic space. Analysis shows that the convergence to an optimal CoT length is a result of balancing underfitting and overfitting. Empirical experiments support these theoretical findings and provide insights into phenomena like overthinking. The CoT-Space framework not only explains empirical observations but also lays a solid theoretical foundation for developing more effective and generalizable reasoning agents. 

<br /><br />Summary: <div>
arXiv:2509.04027v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has become a pivotal approach for enhancing the reasoning capabilities of Large Language Models (LLMs). However, a significant theoretical gap persists, as traditional token-level RL frameworks fail to align with the reasoning-level nature of complex, multi-step thought processes like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space, a novel theoretical framework that recasts LLM reasoning from a discrete token-prediction task to an optimization process within a continuous, reasoning-level semantic space. By analyzing this process from both a noise perspective and a risk perspective, we demonstrate that the convergence to an optimal CoT length is a natural consequence of the fundamental trade-off between underfitting and overfitting. Furthermore, extensive experiments provide strong empirical validation for our theoretical findings. Our framework not only provides a coherent explanation for empirical phenomena such as overthinking but also offers a solid theoretical foundation to guide the future development of more effective and generalizable reasoning agents.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oruga: An Avatar of Representational Systems Theory</title>
<link>https://arxiv.org/abs/2509.04041</link>
<guid>https://arxiv.org/abs/2509.04041</guid>
<content:encoded><![CDATA[
<div> Keywords: representations, Oruga, Representational Systems Theory, structure transfer, transformations

Summary:
Oruga, an implementation based on Representational Systems Theory (RST), aims to endow machines with the flexibility of human-like representations. The core of Oruga consists of data structures corresponding to RST concepts, a language for communication, and an engine for executing transformations through structure transfer. This method allows for the manipulation and transformation of representations in a way that mirrors human cognitive processes. By harnessing the power of RST, Oruga seeks to make machines more compatible with human use by enabling them to draw diagrams, change representations, and make creative analogies across different domains. Through the implementation of Oruga, researchers hope to advance the understanding of how machines can adapt and manipulate representations in a more flexible and human-like manner. <div>
arXiv:2509.04041v1 Announce Type: new 
Abstract: Humans use representations flexibly. We draw diagrams, change representations and exploit creative analogies across different domains. We want to harness this kind of power and endow machines with it to make them more compatible with human use. Previously we developed Representational Systems Theory (RST) to study the structure and transformations of representations. In this paper we present Oruga (caterpillar in Spanish; a symbol of transformation), an implementation of various aspects of RST. Oruga consists of a core of data structures corresponding to concepts in RST, a language for communicating with the core, and an engine for producing transformations using a method we call structure transfer. In this paper we present an overview of the core and language of Oruga, with a brief example of the kind of transformation that structure transfer can execute.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning</title>
<link>https://arxiv.org/abs/2509.04083</link>
<guid>https://arxiv.org/abs/2509.04083</guid>
<content:encoded><![CDATA[
<div> Neurosymbolic LLM, formal language, reasoning capabilities, syntactic reasoning, semantic reasoning <br />
Summary: <br />
This paper explores the impact of formal language choice on Neurosymbolic LLM reasoning. It examines four formal languages on three datasets and seven LLMs, revealing that the formal language selection influences both syntactic and semantic reasoning abilities. The study highlights the importance of considering the formal language in neurosymbolic reasoning approaches. <div>
arXiv:2509.04083v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve astonishing results on a wide range of tasks. However, their formal reasoning ability still lags behind. A promising approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators from natural to formal languages and symbolic solvers for deriving correct results. Still, the contributing factors to the success of Neurosymbolic LLM reasoning remain unclear. This paper demonstrates that one previously overlooked factor is the choice of the formal language. We introduce the intermediate language challenge: selecting a suitable formal language for neurosymbolic reasoning. By comparing four formal languages across three datasets and seven LLMs, we show that the choice of formal language affects both syntactic and semantic reasoning capabilities. We also discuss the varying effects across different LLMs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Reinforcement Learning and Search for Flight Trajectory Planning</title>
<link>https://arxiv.org/abs/2509.04100</link>
<guid>https://arxiv.org/abs/2509.04100</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Search-based path planners, Flight paths, Airliners, Optimization

Summary:
Reinforcement Learning (RL) combined with search-based path planners is explored for optimizing flight paths for airliners, especially in emergency situations. An RL Agent is trained to pre-compute near-optimal paths based on location and atmospheric data to speed up path planning. This guides the path planning solver to find a solution within a certain distance from the initial guess, reducing the search space and improving computation speed by up to 50%. Empirical results using Airbus aircraft's performance models demonstrate that fuel consumption remains nearly identical to an unconstrained solver, with deviations typically within 1%. While global optimality is not guaranteed, the approach proves effective in generating efficient flight paths in critical scenarios. 

<br /><br />Summary: <div>
arXiv:2509.04100v1 Announce Type: new 
Abstract: This paper explores the combination of Reinforcement Learning (RL) and search-based path planners to speed up the optimization of flight paths for airliners, where in case of emergency a fast route re-calculation can be crucial. The fundamental idea is to train an RL Agent to pre-compute near-optimal paths based on location and atmospheric data and use those at runtime to constrain the underlying path planning solver and find a solution within a certain distance from the initial guess. The approach effectively reduces the size of the solver's search space, significantly speeding up route optimization. Although global optimality is not guaranteed, empirical results conducted with Airbus aircraft's performance models show that fuel consumption remains nearly identical to that of an unconstrained solver, with deviations typically within 1%. At the same time, computation speed can be improved by up to 50% as compared to using a conventional solver alone.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker</title>
<link>https://arxiv.org/abs/2509.04125</link>
<guid>https://arxiv.org/abs/2509.04125</guid>
<content:encoded><![CDATA[
<div> bluffing, poker, DQN, CFR, Leduc Hold'em<br />
<br />
Summary: <br />
In the study of computer-poker algorithms, the focus has often been on performance metrics rather than bluffing behavior. This paper examines the bluffing behavior of two popular algorithms, DQN and CFR, in Leduc Hold'em, a simplified poker game. Through an experiment where DQN and CFR agents play against each other, it is found that both algorithms exhibit bluffing behavior, albeit in different ways. While both algorithms attempt to bluff at different rates, the success rate of their bluffs (opponent folding) is similar. This indicates that bluffing is a crucial aspect of the game itself and not specific to the algorithms being used. Future research should explore various bluffing styles and expand the study to the full game of poker. The code for this study is available on GitHub for reference.  <div>
arXiv:2509.04125v1 Announce Type: new 
Abstract: In the game of poker, being unpredictable, or bluffing, is an essential skill. When humans play poker, they bluff. However, most works on computer-poker focus on performance metrics such as win rates, while bluffing is overlooked. In this paper we study whether two popular algorithms, DQN (based on reinforcement learning) and CFR (based on game theory), exhibit bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed an experiment where we let the DQN and CFR agent play against each other while we log their actions. We find that both DQN and CFR exhibit bluffing behavior, but they do so in different ways. Although both attempt to perform bluffs at different rates, the percentage of successful bluffs (where the opponent folds) is roughly the same. This suggests that bluffing is an essential aspect of the game, not of the algorithm. Future work should look at different bluffing styles and at the full game of poker. Code at https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The human biological advantage over AI</title>
<link>https://arxiv.org/abs/2509.04130</link>
<guid>https://arxiv.org/abs/2509.04130</guid>
<content:encoded><![CDATA[
<div> AGI, artificial general intelligence, CNS, central nervous system, DNA, silicon <br />
<br />
Summary: Recent advances in AI suggest the potential for AI systems to surpass human capabilities, leading to the speculation of AI becoming a superior "digital species". However, the fundamental difference between humans and AI lies not in the brain, but in the central nervous system (CNS), which allows humans to experience emotions and understand the consequences of their actions. This emotional understanding is essential for developing ethical systems and qualifying as leaders of the universe. While AI systems may excel in various tasks and revolutionize society, the foundation for true leadership will always be DNA, as a CNS cannot be manufactured or simulated, and consciousness alone will not be enough to surpass humans. Thus, while AI may advance in many aspects, the core qualities that make humans uniquely suited for leadership in the universe stem from their biological nature and emotional capacity. <br /><br /> <div>
arXiv:2509.04130v1 Announce Type: new 
Abstract: Recent advances in AI raise the possibility that AI systems will one day be able to do anything humans can do, only better. If artificial general intelligence (AGI) is achieved, AI systems may be able to understand, reason, problem solve, create, and evolve at a level and speed that humans will increasingly be unable to match, or even understand. These possibilities raise a natural question as to whether AI will eventually become superior to humans, a successor "digital species", with a rightful claim to assume leadership of the universe. However, a deeper consideration suggests the overlooked differentiator between human beings and AI is not the brain, but the central nervous system (CNS), providing us with an immersive integration with physical reality. It is our CNS that enables us to experience emotion including pain, joy, suffering, and love, and therefore to fully appreciate the consequences of our actions on the world around us. And that emotional understanding of the consequences of our actions is what is required to be able to develop sustainable ethical systems, and so be fully qualified to be the leaders of the universe. A CNS cannot be manufactured or simulated; it must be grown as a biological construct. And so, even the development of consciousness will not be sufficient to make AI systems superior to humans. AI systems may become more capable than humans on almost every measure and transform our society. However, the best foundation for leadership of our universe will always be DNA, not silicon.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs</title>
<link>https://arxiv.org/abs/2509.04159</link>
<guid>https://arxiv.org/abs/2509.04159</guid>
<content:encoded><![CDATA[
<div> DSL, recipes, action graphs, culinary workflows, ontology <br />
<br />
Summary: <br />
This article introduces a domain-specific language for representing recipes as directed action graphs, allowing precise modeling of complex culinary workflows. The DSL captures processes, transfers, environments, concurrency, and compositional structure, enabling structured machine understanding and scalable automation of cooking processes. Initial evaluation on a full English breakfast recipe demonstrates the DSL's expressiveness and potential for automated recipe analysis and execution. The approach aims to formalize cooking procedures, addressing their inherent complexity and ambiguity. By using temporal graphs, the DSL facilitates precise interpretation of culinary processes in both home kitchens and professional settings. This work paves the way for an action-centric ontology for cooking, laying the foundation for future advancements in automated recipe handling and execution. <div>
arXiv:2509.04159v1 Announce Type: new 
Abstract: Formalizing cooking procedures remains a challenging task due to their inherent complexity and ambiguity. We introduce an extensible domain-specific language for representing recipes as directed action graphs, capturing processes, transfers, environments, concurrency, and compositional structure. Our approach enables precise, modular modeling of complex culinary workflows. Initial manual evaluation on a full English breakfast recipe demonstrates the DSL's expressiveness and suitability for future automated recipe analysis and execution. This work represents initial steps towards an action-centric ontology for cooking, using temporal graphs to enable structured machine understanding, precise interpretation, and scalable automation of culinary processes - both in home kitchens and professional culinary settings.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain size asymptotics for Markov logic networks</title>
<link>https://arxiv.org/abs/2509.04192</link>
<guid>https://arxiv.org/abs/2509.04192</guid>
<content:encoded><![CDATA[
<div> random structures, Markov logic network, domain size, limit behavior, soft constraints

Summary:
Random structures with domain sizes tending to infinity in Markov logic networks (MLNs) are studied. Three examples of MLNs are analyzed: quantifier-free MLNs with one relation symbol, MLNs favoring graphs with fewer triangles or vertices with high degrees. The analysis shows the diverse limit behaviors of random structures based on the soft constraints used in the MLNs. It is demonstrated that quantifier-free MLNs and lifted Bayesian networks are asymptotically incomparable. Additionally, the distribution determined by an MLN on large domains concentrates its probability mass on a different part of the possible worlds space compared to the uniform distribution. The study highlights the variety of behaviors MLNs exhibit with increasing domain sizes and the potential influence of the soft constraints on the outcomes.<br /><br />Summary: <div>
arXiv:2509.04192v1 Announce Type: new 
Abstract: A Markov logic network (MLN) determines a probability distribution on the set of structures, or ``possible worlds'', with an arbitrary finite domain. We study the properties of such distributions as the domain size tends to infinity. Three types of concrete examples of MLNs will be considered, and the properties of random structures with domain sizes tending to infinity will be studied: (1) Arbitrary quantifier-free MLNs over a language with only one relation symbol which has arity 1. In this case we give a pretty complete characterization of the possible limit behaviours of random structures. (2) An MLN that favours graphs with fewer triangles (or more generally, fewer k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law'' for first-order logic is obtained. (3) An MLN that favours graphs with fewer vertices with degree higher than a fixed (but arbitrary) number. The analysis shows that depending on which ``soft constraints'' an MLN uses the limit behaviour of random structures can be quite different, and the weights of the soft constraints may, or may not, have influence on the limit behaviour. It will also be demonstrated, using (1), that quantifier-free MLNs and lifted Bayesian networks (in a broad sense) are asymptotically incomparable, roughly meaning that there is a sequence of distributions on possible worlds with increasing domain sizes that can be defined by one of the formalisms but not even approximated by the other. In a rather general context it is also shown that on large domains the distribution determined by an MLN concentrates almost all its probability mass on a totally different part of the space of possible worlds than the uniform distribution does.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Quality of Gaming Narratives Co-created with AI</title>
<link>https://arxiv.org/abs/2509.04239</link>
<guid>https://arxiv.org/abs/2509.04239</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated game narratives, Delphi study, narrative design experts, Kano model, player satisfaction

Summary: 

This paper introduces a methodical approach for evaluating AI-generated game narratives by involving a panel of narrative design experts in a Delphi study. By blending insights from existing literature and expert input, the study identifies key dimensions of story quality and links them to the Kano model framework to gauge their influence on player satisfaction. The findings offer valuable guidance to game developers on prioritizing different quality aspects when collaborating with generative AI systems to craft engaging game narratives. <div>
arXiv:2509.04239v1 Announce Type: new 
Abstract: This paper proposes a structured methodology to evaluate AI-generated game narratives, leveraging the Delphi study structure with a panel of narrative design experts. Our approach synthesizes story quality dimensions from literature and expert insights, mapping them into the Kano model framework to understand their impact on player satisfaction. The results can inform game developers on prioritizing quality aspects when co-creating game narratives with generative AI.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation</title>
<link>https://arxiv.org/abs/2509.04310</link>
<guid>https://arxiv.org/abs/2509.04310</guid>
<content:encoded><![CDATA[
<div> evolutionary reinforcement learning, emotional expression, negotiations, large language models, adversarial counterparts
<br />
Summary:
The study introduces EvoEmo, an evolutionary reinforcement learning framework, to improve emotional expression in negotiations for Large Language Models (LLMs). Current LLM agents lack adaptive emotional responses, making them susceptible to manipulation. EvoEmo models emotional state transitions as a Markov Decision Process and utilizes genetic optimization to evolve high-reward emotion policies across various negotiation scenarios. Evaluation against two baselines shows that EvoEmo outperforms both in terms of success rates, efficiency, and buyer savings. The results emphasize the significance of adaptive emotional expression in enhancing the performance of LLM agents in complex, multi-turn negotiations.<br /><br /> <div>
arXiv:2509.04310v1 Announce Type: new 
Abstract: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \textit{complex}, \textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines -- vanilla strategies and fixed-emotion strategies -- for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes</title>
<link>https://arxiv.org/abs/2509.04317</link>
<guid>https://arxiv.org/abs/2509.04317</guid>
<content:encoded><![CDATA[
<div> AlphaZero, Monte Carlo planning, neural network, test environments, performance improvement <br />
<br />
Summary: This paper explores the challenge of deploying AlphaZero agents in changed test environments and proposes modifications to the standard framework to enhance performance. The AlphaZero framework combines Monte Carlo planning with a pre-trained neural network, assuming a static environment. However, when faced with potential changes in the test environment, simple adjustments to the framework can yield significant performance improvements. These modifications are shown to be effective even with limited planning resources available. The study highlights the adaptability of the AlphaZero framework and its potential for application in dynamic environments. The code for the modifications discussed in the paper is publicly accessible on GitHub, enabling researchers and practitioners to implement these enhancements in their own deployment scenarios. <div>
arXiv:2509.04317v1 Announce Type: new 
Abstract: The AlphaZero framework provides a standard way of combining Monte Carlo planning with prior knowledge provided by a previously trained policy-value neural network. AlphaZero usually assumes that the environment on which the neural network was trained will not change at test time, which constrains its applicability. In this paper, we analyze the problem of deploying AlphaZero agents in potentially changed test environments and demonstrate how the combination of simple modifications to the standard framework can significantly boost performance, even in settings with a low planning budget available. The code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psychologically Enhanced AI Agents</title>
<link>https://arxiv.org/abs/2509.04343</link>
<guid>https://arxiv.org/abs/2509.04343</guid>
<content:encoded><![CDATA[
<div> MBTI, Large Language Model, personality conditioning, agent behavior, psychological frameworks<br />
<br />
Summary: 
The article introduces MBTI-in-Thoughts, a framework that enhances Large Language Model agents through psychologically grounded personality conditioning based on the Myers-Briggs Type Indicator (MBTI). The method involves priming agents with distinct personality archetypes via prompt engineering, allowing control over behavior in cognition and affect. Results show that personality priming leads to consistent behavioral biases, with emotionally expressive agents excelling in narrative generation and analytically primed agents exhibiting stable strategies in game-theoretic situations. The framework supports structured multi-agent communication protocols and demonstrates that self-reflection prior to interaction improves cooperation and reasoning quality. Trait persistence is ensured through integration of the 16Personalities test for automated verification. The approach generalizes to other psychological frameworks such as Big Five, HEXACO, or Enneagram, establishing a foundation for psychologically enhanced AI agents without fine-tuning.<br /><br />Summary: <div>
arXiv:2509.04343v1 Announce Type: new 
Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of Large Language Model (LLM) agents through psychologically grounded personality conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology, cognition and affect. We show that such personality priming yields consistent, interpretable behavioral biases across diverse tasks: emotionally expressive agents excel in narrative generation, while analytically primed agents adopt more stable strategies in game-theoretic settings. Our framework supports experimenting with structured multi-agent communication protocols and reveals that self-reflection prior to interaction improves cooperation and reasoning quality. To ensure trait persistence, we integrate the official 16Personalities test for automated verification. While our focus is on MBTI, we show that our approach generalizes seamlessly to other psychological frameworks such as Big Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior design, we establish a foundation for psychologically enhanced AI agents without any fine-tuning.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory</title>
<link>https://arxiv.org/abs/2509.04439</link>
<guid>https://arxiv.org/abs/2509.04439</guid>
<content:encoded><![CDATA[
<div> memory, concept-level, continual learning, inference compute, abstract concepts

Summary:
In the realm of Language Model (LLMs), a new method has been introduced to enhance reasoning by utilizing external memory for persisting insights gained during inference. This method involves storing reusable, modular abstractions of solution traces in natural language, rather than exact instances. By selectively retrieving relevant concepts from memory for future queries, the approach enables test-time continual learning without weight updates. Results on the challenging ARC-AGI benchmark show a 7.5% relative improvement over a strong baseline method, with performance scaling with inference compute. The study highlights the effectiveness of abstract concepts in memory design, consistently outperforming baseline methods. Furthermore, dynamic memory updating during test-time proves superior to fixed memory settings, demonstrating that continual problem solving and abstraction of patterns lead to self-improvement. The code for this method is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2509.04439v1 Announce Type: new 
Abstract: While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query/response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. On the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, we confirm that dynamically updating memory during test-time outperforms an otherwise identical fixed memory setting with additional attempts, supporting the hypothesis that solving more problems and abstracting more patterns to memory enables further solutions in a form of self-improvement. Code available at https://github.com/matt-seb-ho/arc_memo.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiND: A Neural Discriminator-Decoder for Accurate Bimanual Trajectory Prediction in Brain-Computer Interfaces</title>
<link>https://arxiv.org/abs/2509.03521</link>
<guid>https://arxiv.org/abs/2509.03521</guid>
<content:encoded><![CDATA[
<div> decoding, bimanual hand movements, intracortical recordings, brain-computer interfaces, BiND<br />
Summary:<br />
The study introduces BiND, a model for decoding bimanual hand movements from intracortical recordings in brain-computer interfaces. BiND classifies motion type and predicts continuous 2D hand velocities using specialized GRU-based decoders with a trial-relative time index. Benchmarking against six other models on a tetraplegic patient dataset, BiND outperforms all models with a mean $R^2$ of 0.76 for unimanual and 0.69 for bimanual trajectory prediction. It shows greater robustness to session variability, with accuracy improvements of up to 4% in cross-session analyses compared to GRU. Task-aware discrimination and temporal modeling enhance bimanual decoding, highlighting the effectiveness of BiND in decoding complex hand movements. <br /> <div>
arXiv:2509.03521v1 Announce Type: cross 
Abstract: Decoding bimanual hand movements from intracortical recordings remains a critical challenge for brain-computer interfaces (BCIs), due to overlapping neural representations and nonlinear interlimb interactions. We introduce BiND (Bimanual Neural Discriminator-Decoder), a two-stage model that first classifies motion type (unimanual left, unimanual right, or bimanual) and then uses specialized GRU-based decoders, augmented with a trial-relative time index, to predict continuous 2D hand velocities. We benchmark BiND against six state-of-the-art models (SVR, XGBoost, FNN, CNN, Transformer, GRU) on a publicly available 13-session intracortical dataset from a tetraplegic patient. BiND achieves a mean $R^2$ of 0.76 ($\pm$0.01) for unimanual and 0.69 ($\pm$0.03) for bimanual trajectory prediction, surpassing the next-best model (GRU) by 2% in both tasks. It also demonstrates greater robustness to session variability than all other benchmarked models, with accuracy improvements of up to 4% compared to GRU in cross-session analyses. This highlights the effectiveness of task-aware discrimination and temporal modeling in enhancing bimanual decoding.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies</title>
<link>https://arxiv.org/abs/2509.03525</link>
<guid>https://arxiv.org/abs/2509.03525</guid>
<content:encoded><![CDATA[
<div> adults, Alzheimer disease, speech-based screening, language model adaptation, dementia detection <br />
Summary: <br />
The study focused on the use of speech-based screening for detecting dementia in adults with Alzheimer's disease. It found that over half of US adults with dementia are undiagnosed, highlighting the need for scalable detection approaches. The study compared different adaptation strategies for large language models using the DementiaBank speech corpus. Results showed that class-centroid demonstrations were most effective for in-context learning, reasoning improved smaller models, and token-level fine-tuning generally produced the best outcomes. Adding a classification head significantly enhanced the performance of underperforming models. The study also found that multimodal audio-text models performed well but did not surpass the top text-only models. The findings emphasize that adaptation strategies, including demonstration selection, reasoning design, and tuning methods, play a crucial role in speech-based dementia detection and suggest that properly adapted open-weight models can match or exceed commercial systems. <div>
arXiv:2509.03525v1 Announce Type: cross 
Abstract: Over half of US adults with Alzheimer disease and related dementias remain undiagnosed, and speech-based screening offers a scalable detection approach. We compared large language model adaptation strategies for dementia detection using the DementiaBank speech corpus, evaluating nine text-only models and three multimodal audio-text models on recordings from DementiaBank speech corpus. Adaptations included in-context learning with different demonstration selection policies, reasoning-augmented prompting, parameter-efficient fine-tuning, and multimodal integration. Results showed that class-centroid demonstrations achieved the highest in-context learning performance, reasoning improved smaller models, and token-level fine-tuning generally produced the best scores. Adding a classification head substantially improved underperforming models. Among multimodal models, fine-tuned audio-text systems performed well but did not surpass the top text-only models. These findings highlight that model adaptation strategies, including demonstration selection, reasoning design, and tuning method, critically influence speech-based dementia detection, and that properly adapted open-weight models can match or exceed commercial systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model</title>
<link>https://arxiv.org/abs/2509.03527</link>
<guid>https://arxiv.org/abs/2509.03527</guid>
<content:encoded><![CDATA[
<div> Keywords: multilevel analysis, cryptocurrency news, Mistral 7B, knowledge graph, quantitative analytics

Summary: 
The paper presents a multilevel multitask analysis of cryptocurrency news using a fine-tuned Mistral 7B large language model with retrieval-augmented generation (RAG). The model generates graph and text summaries with sentiment scores and JSON representations on the first level of analytics. Higher levels employ hierarchical stacking to consolidate summaries into comprehensive reports. Incorporating both graph and text summaries offers complementary perspectives on cryptocurrency news. The model is fine-tuned using 4-bit quantization with the PEFT/LoRA approach, enhancing the quality of the analysis. By representing cryptocurrency news as a knowledge graph, the model helps eliminate issues related to large language model hallucinations. The results demonstrate the effectiveness of using the fine-tuned Mistral 7B LLM model for conducting informative qualitative and quantitative analytics, providing valuable insights into the cryptocurrency market. 

<br /><br />Summary: <div>
arXiv:2509.03527v1 Announce Type: cross 
Abstract: In the paper, we consider multilevel multitask analysis of cryptocurrency news using a fine-tuned Mistral 7B large language model with retrieval-augmented generation (RAG).
  On the first level of analytics, the fine-tuned model generates graph and text summaries with sentiment scores as well as JSON representations of summaries. Higher levels perform hierarchical stacking that consolidates sets of graph-based and text-based summaries as well as summaries of summaries into comprehensive reports. The combination of graph and text summaries provides complementary views of cryptocurrency news. The model is fine-tuned with 4-bit quantization using the PEFT/LoRA approach. The representation of cryptocurrency news as knowledge graph can essentially eliminate problems with large language model hallucinations.
  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM models for multilevel cryptocurrency news analysis can conduct informative qualitative and quantitative analytics, providing important insights.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages</title>
<link>https://arxiv.org/abs/2509.03529</link>
<guid>https://arxiv.org/abs/2509.03529</guid>
<content:encoded><![CDATA[
<div> Earnings calls, sentiment analysis, multi-modal framework, hierarchical discourse trees, contrastive learning<br />
<br />
Summary: This paper introduces a novel multi-modal framework for analyzing earnings calls, encoding them as hierarchical discourse trees to capture the layered structure of interactions. The framework generates semantically rich embeddings enriched with emotional signals, structured metadata, coherence scores, topic labels, and answer coverage assessments. A two-stage transformer architecture is used to encode content and discourse metadata at the node level and synthesize a global embedding for the entire conference. Experimental results show that the embeddings reflect affective tone, structural logic, and thematic alignment, providing stable and meaningful representations. The proposed system has practical utility for tasks such as financial forecasting and discourse evaluation, with potential applications in other high-stakes communication domains like tele-medicine, education, and politics. <div>
arXiv:2509.03529v1 Announce Type: cross 
Abstract: Earnings calls represent a uniquely rich and semi-structured source of financial communication, blending scripted managerial commentary with unscripted analyst dialogue. Although recent advances in financial sentiment analysis have integrated multi-modal signals, such as textual content and vocal tone, most systems rely on flat document-level or sentence-level models, failing to capture the layered discourse structure of these interactions. This paper introduces a novel multi-modal framework designed to generate semantically rich and structurally aware embeddings of earnings calls, by encoding them as hierarchical discourse trees. Each node, comprising either a monologue or a question-answer pair, is enriched with emotional signals derived from text, audio, and video, as well as structured metadata including coherence scores, topic labels, and answer coverage assessments. A two-stage transformer architecture is proposed: the first encodes multi-modal content and discourse metadata at the node level using contrastive learning, while the second synthesizes a global embedding for the entire conference. Experimental results reveal that the resulting embeddings form stable, semantically meaningful representations that reflect affective tone, structural logic, and thematic alignment. Beyond financial reporting, the proposed system generalizes to other high-stakes unscripted communicative domains such as tele-medicine, education, and political discourse, offering a robust and explainable approach to multi-modal discourse representation. This approach offers practical utility for downstream tasks such as financial forecasting and discourse evaluation, while also providing a generalizable method applicable to other domains involving high-stakes communication.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Detection of Hallucinated Entities in Long-Form Generation</title>
<link>https://arxiv.org/abs/2509.03531</link>
<guid>https://arxiv.org/abs/2509.03531</guid>
<content:encoded><![CDATA[
<div> Entity-level hallucinations, long-form generations, scalable method, hallucination classifiers, annotation methodology<br />
<br />Summary:
Large language models are increasingly used in critical applications, where hallucinations can have serious consequences. Existing detection methods are not practical for real-world use. A new cheap and scalable method for identifying hallucinated tokens in long-form text has been developed, focusing on entity-level hallucinations. An annotation methodology leveraging web search was used to train effective hallucination classifiers, outperforming baselines consistently. The classifiers are also successful in short-form question-answering scenarios and in detecting incorrect answers in mathematical reasoning tasks. The annotations collected for one model can be used to train classifiers for other models. The datasets created are released publicly, indicating a promising scalable approach for real-world hallucination detection.<br /><br />Summary: <div>
arXiv:2509.03531v1 Announce Type: cross 
Abstract: Large language models are now routinely used in high-stakes applications where hallucinations can cause serious harm, such as medical consultations or legal advice. Existing hallucination detection methods, however, are impractical for real-world use, as they are either limited to short factual queries or require costly external verification. We present a cheap, scalable method for real-time identification of hallucinated tokens in long-form generations, and scale it effectively to 70B parameter models. Our approach targets \emph{entity-level hallucinations} -- e.g., fabricated names, dates, citations -- rather than claim-level, thereby naturally mapping to token-level labels and enabling streaming detection. We develop an annotation methodology that leverages web search to annotate model responses with grounded labels indicating which tokens correspond to fabricated entities. This dataset enables us to train effective hallucination classifiers with simple and efficient methods such as linear probes. Evaluating across four model families, our classifiers consistently outperform baselines on long-form responses, including more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for Llama-3.3-70B), and are also an improvement in short-form question-answering settings. Moreover, despite being trained only with entity-level labels, our probes effectively detect incorrect answers in mathematical reasoning tasks, indicating generalization beyond entities. While our annotation methodology is expensive, we find that annotated responses from one model can be used to train effective classifiers on other models; accordingly, we publicly release our datasets to facilitate reuse. Overall, our work suggests a promising new approach for scalable, real-world hallucination detection.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuesGenie: Intelligent Multimodal Question Generation</title>
<link>https://arxiv.org/abs/2509.03535</link>
<guid>https://arxiv.org/abs/2509.03535</guid>
<content:encoded><![CDATA[
<div> Keywords: educational resources, question generation system, multi-modal input, reinforcement learning, interactive interface

Summary:
This project focuses on addressing the challenge of the lack of practice materials tailored to the abundant educational resources available to learners. The system developed includes a multi-modal question generation system with components such as multi-modal input handling, question generation, reinforcement learning from human feedback (RLHF), and an end-to-end interactive interface. By utilizing these components, the system aims to automate question generation, ensuring diversity in question types and balancing resource efficiency, robust functionality, and user experience. This project paves the way for scalable and intelligent question generation, offering a solution to the current gap in practice materials tailored to educational resources. 

<br /><br />Summary: <div>
arXiv:2509.03535v1 Announce Type: cross 
Abstract: In today's information-rich era, learners have access to abundant educational resources, but the lack of practice materials tailored to these resources presents a significant challenge. This project addresses that gap by developing a multi-modal question generation system that can automatically generate diverse question types from various content formats. The system features four major components: multi-modal input handling, question generation, reinforcement learning from human feedback (RLHF), and an end-to-end interactive interface. This project lays the foundation for automated, scalable, and intelligent question generation, carefully balancing resource efficiency, robust functionality and a smooth user experience.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2509.03537</link>
<guid>https://arxiv.org/abs/2509.03537</guid>
<content:encoded><![CDATA[
<div> abstraction, computational patterns, computer science, large language models, reinforcement learning<br />
Summary:<br />
- Abstraction, essential in computer science, is crucial for problem-solving and large language models (LLMs).<br />
- Current LLM training methods focus on pattern recognition and lack explicit abstraction training.<br />
- AR$^2$ introduces a framework to enhance LLMs' abstraction abilities through Adversarial Reinforcement Learning.<br />
- A teacher model transforms problems into narrative-rich descriptions to challenge LLMs.<br />
- The student model is trained to extract underlying computational kernels from these complex narrative problems.<br />
- Experimental results show AR$^2 improves LLM accuracy on challenging programming tasks, highlighting the importance of abstraction in LLM generalization.<br /> <div>
arXiv:2509.03537v1 Announce Type: cross 
Abstract: Abstraction--the ability to recognize and distill essential computational patterns from complex problem statements--is a foundational skill in computer science, critical both for human problem-solvers and coding-oriented large language models (LLMs). Despite recent advances in training LLMs for code generation using reinforcement learning (RL), most existing approaches focus primarily on superficial pattern recognition, overlooking explicit training for abstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement Learning for Abstract Reasoning), a novel framework explicitly designed to enhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to transform kernel problems into narrative-rich, challenging descriptions without changing their fundamental logic. Simultaneously, a student coding model is trained to solve these complex narrative problems by extracting their underlying computational kernels. Experimental results demonstrate that AR$^2$ substantially improves the student model's accuracy on previously unseen, challenging programming tasks, underscoring abstraction as a key skill for enhancing LLM generalization.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction</title>
<link>https://arxiv.org/abs/2509.03540</link>
<guid>https://arxiv.org/abs/2509.03540</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, Knowledge Graphs, Factual Accuracy, Interpretability

Summary:
Large Language Models (LLMs) often struggle with factual consistency due to limited memory. Retrieval-Augmented Generation (RAG) methods use external knowledge for better answers. This study introduces a method that dynamically constructs and expands knowledge graphs (KGs) during inference, integrating internal and external knowledge. It extracts a seed KG from the question, expands it using LLM's latent knowledge, and refines it through external retrieval. The approach improves factual accuracy, answer precision, and interpretability on three factual QA benchmarks compared to baseline methods. This inference-time KG construction shows promise in enhancing LLM factuality in a structured, interpretable, and scalable manner.<br /><br />Summary: <div>
arXiv:2509.03540v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often struggle with producing factually consistent answers due to limitations in their parametric memory. Retrieval-Augmented Generation (RAG) methods address this issue by incorporating external knowledge from trusted sources at inference time. However, such methods typically treat knowledge as unstructured text, which limits their ability to support compositional reasoning and identify factual inconsistencies. To overcome these limitations, we propose a novel framework that dynamically constructs and expands knowledge graphs (KGs) during inference, integrating both internal knowledge extracted from LLMs and external information retrieved from external sources. Our method begins by extracting a seed KG from the question via prompting, followed by iterative expansion using the LLM's latent knowledge. The graph is then selectively refined through external retrieval, enhancing factual coverage and correcting inaccuracies. We evaluate our approach on three diverse factual QA benchmarks, demonstrating consistent improvements in factual accuracy, answer precision, and interpretability over baseline prompting and static KG-augmented methods. Our findings suggest that inference-time KG construction is a promising direction for enhancing LLM factuality in a structured, interpretable, and scalable manner.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A software security review on Uganda's Mobile Money Services: Dr. Jim Spire's tweets sentiment analysis</title>
<link>https://arxiv.org/abs/2509.03545</link>
<guid>https://arxiv.org/abs/2509.03545</guid>
<content:encoded><![CDATA[
<div> Keywords: mobile money, security mechanisms, #StopAirtelThefty, Uganda, financial inclusion

Summary: 
This study examines the #StopAirtelThefty Twitter campaign in Uganda in August 2025, triggered by a security incident involving mobile money. The campaign highlighted public concerns over the safety of mobile money and revealed security vulnerabilities and user dissatisfaction. Qualitative analysis of the complaints raised during the campaign identified key themes related to security gaps experienced by users. These findings offer insights into the specific challenges faced by mobile money users in Uganda and provide valuable information for providers, policymakers, and the future of secure digital finance in the country. The study underscores the importance of addressing security issues in mobile money systems to ensure trust and confidence among users and to promote financial inclusion. <br /><br />Summary: <div>
arXiv:2509.03545v1 Announce Type: cross 
Abstract: The proliferation of mobile money in Uganda has been a cornerstone of financial inclusion, yet its security mechanisms remain a critical concern. This study investigates a significant public response to perceived security failures: the #StopAirtelThefty Twitter campaign of August 2025 Sparked by an incident publicized by Dr. Jim Spire Ssentongo where a phone thief accessed a victim's account, withdrew funds, and procured a loan, the campaign revealed deep seated public anxiety over the safety of mobile money. This research employs qualitative analysis to systematically examine the complaints raised during this campaign, extracting key themes related to security vulnerabilities and user dissatisfaction. By synthesizing these public sentiments, the paper provides crucial insights into the specific security gaps experienced by users and situates these findings within the larger framework of Uganda's mobile money regulatory and operational environment. The study concludes with implications for providers, policymakers, and the future of secure digital finance in Uganda.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric</title>
<link>https://arxiv.org/abs/2509.03594</link>
<guid>https://arxiv.org/abs/2509.03594</guid>
<content:encoded><![CDATA[
<div> optimisers, neural networks, Riemannian metric, loss landscape, training<br />
<br />
Summary:<br />
The article introduces a new class of optimisers for training neural networks that utilize the Riemannian metric inherent in the loss landscape's higher-dimensional space embedding. Comparisons with SGD, Adam, AdamW, and Muon show the new optimisers to be highly effective in low-dimensional examples and slightly better than current methods. The optimisers automatically adjust the learning rate in high-curvature regions, serving as a form of gradient clipping. They also implement effective scheduled learning rates and suggest decoupled weight decay. The new approach can enhance existing preconditioning methods with similar computational complexity to Adam. <div>
arXiv:2509.03594v1 Announce Type: cross 
Abstract: We present a class of novel optimisers for training neural networks that makes use of the Riemannian metric naturally induced when the loss landscape is embedded in higher-dimensional space. This is the same metric that underlies common visualisations of loss landscapes. By taking this geometric perspective literally and using the induced metric, we develop a new optimiser and compare it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of tasks and architectures. Empirically, we conclude that this new class of optimisers is highly effective in low dimensional examples, and provides slight improvement over state-of-the-art methods for training neural networks. These new optimisers have theoretically desirable properties. In particular, the effective learning rate is automatically decreased in regions of high curvature acting as a smoothed out form of gradient clipping. Similarly, one variant of these optimisers can also be viewed as inducing an effective scheduled learning rate and decoupled weight decay is the natural choice from our geometric perspective. The basic method can be used to modify any existing preconditioning method. The new optimiser has a computational complexity comparable to that of Adam.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition</title>
<link>https://arxiv.org/abs/2509.03615</link>
<guid>https://arxiv.org/abs/2509.03615</guid>
<content:encoded><![CDATA[
<div> Keywords: Optical Character Recognition, Large Vision-Language Models, Edge deployment, Efficiency, Affordability

Summary:<br /><br />
The article discusses the challenges of Optical Character Recognition (OCR) in diverse real-world images and the potential of Large Vision-Language Models (LVLMs) to enhance OCR capabilities. The study introduces Sprinklr-Edge-OCR, an OCR system optimized for edge deployment in resource-constrained environments. A comparative evaluation of five state-of-the-art LVLMs and two traditional OCR systems on a multilingual image dataset is presented, covering metrics such as accuracy, efficiency, and deployment cost. The results show that traditional OCR systems, specifically Sprinklr-Edge-OCR, outperform LVLMs in efficiency, with faster image processing and lower cost. Despite LVLMs like Qwen achieving high precision, Sprinklr-Edge-OCR demonstrates superior overall performance in terms of F1 score and practical applicability for edge deployment. This highlights the importance of considering compute requirements, latency, and affordability in selecting OCR systems for edge deployment scenarios. <div>
arXiv:2509.03615v1 Announce Type: cross 
Abstract: Optical Character Recognition (OCR) in multilingual, noisy, and diverse real-world images remains a significant challenge for optical character recognition systems. With the rise of Large Vision-Language Models (LVLMs), there is growing interest in their ability to generalize and reason beyond fixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR system built specifically optimized for edge deployment in resource-constrained environments. We present a large-scale comparative evaluation of five state-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two traditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly hand annotated dataset of multilingual (54 languages) images. Our benchmark covers a broad range of metrics including accuracy, semantic consistency, language coverage, computational efficiency (latency, memory, GPU usage), and deployment cost. To better reflect real-world applicability, we also conducted edge case deployment analysis, evaluating model performance on CPU only environments. Among the results, Qwen achieved the highest precision (0.54), while Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and outperformed others in efficiency, processing images 35 faster (0.17 seconds per image on average) and at less than 0.01 of the cost (0.006 USD per 1,000 images) compared to LVLM. Our findings demonstrate that the most optimal OCR systems for edge deployment are the traditional ones even in the era of LLMs due to their low compute requirements, low latency, and very high affordability.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds</title>
<link>https://arxiv.org/abs/2509.03633</link>
<guid>https://arxiv.org/abs/2509.03633</guid>
<content:encoded><![CDATA[
arXiv:2509.03633v1 Announce Type: cross 
Abstract: Close-range laser scanning provides detailed 3D captures of forest stands but requires efficient software for processing 3D point cloud data and extracting individual trees. Although recent studies have introduced deep learning methods for tree instance segmentation, these approaches require large annotated datasets and substantial computational resources. As a resource-efficient alternative, we present a revised version of the treeX algorithm, an unsupervised method that combines clustering-based stem detection with region growing for crown delineation. While the original treeX algorithm was developed for personal laser scanning (PLS) data, we provide two parameter presets, one for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one for UAV-borne laser scanning (ULS). We evaluated the method on six public datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham Woods) and compared it to six open-source methods (original treeX, treeiso, RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original treeX algorithm, our revision reduces runtime and improves accuracy, with instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data. For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original algorithm fails to segment any correct instances. For TLS and PLS data, our algorithm achieves accuracy similar to recent open-source methods, including deep learning. Given its algorithmic design, we see two main applications for our method: (1) as a resource-efficient alternative to deep learning approaches in scenarios where the data characteristics align with the method design (sufficient stem visibility and point density), and (2) for the semi-automatic generation of labels for deep learning models. To enable broader adoption, we provide an open-source Python implementation in the pointtree package.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records</title>
<link>https://arxiv.org/abs/2509.03643</link>
<guid>https://arxiv.org/abs/2509.03643</guid>
<content:encoded><![CDATA[
arXiv:2509.03643v1 Announce Type: cross 
Abstract: Electronic Health Records (EHRs) provide a rich, longitudinal view of patient health and hold significant potential for advancing clinical decision support, risk prediction, and data-driven healthcare research. However, most artificial intelligence (AI) models for EHRs are designed for narrow, single-purpose tasks, limiting their generalizability and utility in real-world settings. Here, we present CEHR-GPT, a general-purpose foundation model for EHR data that unifies three essential capabilities - feature representation, zero-shot prediction, and synthetic data generation - within a single architecture. To support temporal reasoning over clinical sequences, \cehrgpt{} incorporates a novel time-token-based learning framework that explicitly encodes patients' dynamic timelines into the model structure. CEHR-GPT demonstrates strong performance across all three tasks and generalizes effectively to external datasets through vocabulary expansion and fine-tuning. Its versatility enables rapid model development, cohort discovery, and patient outcome forecasting without the need for task-specific retraining.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators</title>
<link>https://arxiv.org/abs/2509.03647</link>
<guid>https://arxiv.org/abs/2509.03647</guid>
<content:encoded><![CDATA[
arXiv:2509.03647v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly serve as automated evaluators, yet they suffer from "self-preference bias": a tendency to favor their own outputs over those of other models. This bias undermines fairness and reliability in evaluation pipelines, particularly for tasks like preference tuning and model routing. We investigate whether lightweight steering vectors can mitigate this problem at inference time without retraining. We introduce a curated dataset that distinguishes self-preference bias into justified examples of self-preference and unjustified examples of self-preference, and we construct steering vectors using two methods: Contrastive Activation Addition (CAA) and an optimization-based approach. Our results show that steering vectors can reduce unjustified self-preference bias by up to 97\%, substantially outperforming prompting and direct preference optimization baselines. Yet steering vectors are unstable on legitimate self-preference and unbiased agreement, implying self-preference spans multiple or nonlinear directions. This underscores both their promise and limits as safeguards for LLM-as-judges and motivates more robust interventions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Virtuoso: A Latent Diffusion Transformer Model for Goal-Conditioned Trajectory Planning</title>
<link>https://arxiv.org/abs/2509.03658</link>
<guid>https://arxiv.org/abs/2509.03658</guid>
<content:encoded><![CDATA[
arXiv:2509.03658v1 Announce Type: cross 
Abstract: The ability to generate a diverse and plausible distribution of future trajectories is a critical capability for autonomous vehicle planning systems. While recent generative models have shown promise, achieving high fidelity, computational efficiency, and precise control remains a significant challenge. In this paper, we present the \textbf{Efficient Virtuoso}, a conditional latent diffusion model for goal-conditioned trajectory planning. Our approach introduces a novel two-stage normalization pipeline that first scales trajectories to preserve their geometric aspect ratio and then normalizes the resulting PCA latent space to ensure a stable training target. The denoising process is performed efficiently in this low-dimensional latent space by a simple MLP denoiser, which is conditioned on a rich scene context fused by a powerful Transformer-based StateEncoder. We demonstrate that our method achieves state-of-the-art performance on the Waymo Open Motion Dataset, reaching a \textbf{minADE of 0.25}. Furthermore, through a rigorous ablation study on goal representation, we provide a key insight: while a single endpoint goal can resolve strategic ambiguity, a richer, multi-step sparse route is essential for enabling the precise, high-fidelity tactical execution that mirrors nuanced human driving behavior.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insights from Gradient Dynamics: Gradient Autoscaled Normalization</title>
<link>https://arxiv.org/abs/2509.03677</link>
<guid>https://arxiv.org/abs/2509.03677</guid>
<content:encoded><![CDATA[
arXiv:2509.03677v1 Announce Type: cross 
Abstract: Gradient dynamics play a central role in determining the stability and generalization of deep neural networks. In this work, we provide an empirical analysis of how variance and standard deviation of gradients evolve during training, showing consistent changes across layers and at the global scale in convolutional networks. Motivated by these observations, we propose a hyperparameter-free gradient normalization method that aligns gradient scaling with their natural evolution. This approach prevents unintended amplification, stabilizes optimization, and preserves convergence guarantees. Experiments on the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN demonstrate that our method maintains or improves test accuracy even under strong generalization. Beyond practical performance, our study highlights the importance of directly tracking gradient dynamics, aiming to bridge the gap between theoretical expectations and empirical behaviors, and to provide insights for future optimization research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LuxDiT: Lighting Estimation with Video Diffusion Transformer</title>
<link>https://arxiv.org/abs/2509.03680</link>
<guid>https://arxiv.org/abs/2509.03680</guid>
<content:encoded><![CDATA[
arXiv:2509.03680v1 Announce Type: cross 
Abstract: Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures</title>
<link>https://arxiv.org/abs/2509.03695</link>
<guid>https://arxiv.org/abs/2509.03695</guid>
<content:encoded><![CDATA[
arXiv:2509.03695v1 Announce Type: cross 
Abstract: The rise of foundation models (FMs) has reshaped the landscape of machine learning. As these models continued to grow, leveraging geo-distributed data from wireless devices has become increasingly critical, giving rise to federated foundation models (FFMs). More recently, FMs have evolved into multi-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diverse modalities across multiple tasks, which motivates a new underexplored paradigm: M3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs by proposing hierarchical federated foundation models (HF-FMs), which in turn expose two overlooked heterogeneity dimensions to fog/edge networks that have a direct impact on these emerging models: (i) heterogeneity in collected modalities and (ii) heterogeneity in executed tasks across fog/edge nodes. HF-FMs strategically align the modular structure of M3T FMs, comprising modality encoders, prompts, mixture-of-experts (MoEs), adapters, and task heads, with the hierarchical nature of fog/edge infrastructures. Moreover, HF-FMs enable the optional usage of device-to-device (D2D) communications, enabling horizontal module relaying and localized cooperative training among nodes when feasible. Through delving into the architectural design of HF-FMs, we highlight their unique capabilities along with a series of tailored future research directions. Finally, to demonstrate their potential, we prototype HF-FMs in a wireless network setting and release the open-source code for the development of HF-FMs with the goal of fostering exploration in this untapped field (GitHub: https://github.com/payamsiabd/M3T-FFM).
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Federated Learning to $\mathbb{X}$-Learning: Breaking the Barriers of Decentrality Through Random Walks</title>
<link>https://arxiv.org/abs/2509.03709</link>
<guid>https://arxiv.org/abs/2509.03709</guid>
<content:encoded><![CDATA[
arXiv:2509.03709v1 Announce Type: cross 
Abstract: We provide our perspective on $\mathbb{X}$-Learning ($\mathbb{X}$L), a novel distributed learning architecture that generalizes and extends the concept of decentralization. Our goal is to present a vision for $\mathbb{X}$L, introducing its unexplored design considerations and degrees of freedom. To this end, we shed light on the intuitive yet non-trivial connections between $\mathbb{X}$L, graph theory, and Markov chains. We also present a series of open research directions to stimulate further research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection</title>
<link>https://arxiv.org/abs/2509.03725</link>
<guid>https://arxiv.org/abs/2509.03725</guid>
<content:encoded><![CDATA[
arXiv:2509.03725v1 Announce Type: cross 
Abstract: We present the novel approach for stance detection across domains and targets, Metric Learning-Based Few-Shot Learning for Cross-Target and Cross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with triplet loss to capture semantic similarities and differences between stance targets, enhancing domain adaptation. By constructing a discriminative embedding space, MLSD allows a cross-target or cross-domain stance detection model to acquire useful examples from new target domains. We evaluate MLSD in multiple cross-target and cross-domain scenarios across two datasets, showing statistically significant improvement in stance detection performance across six widely used stance detection models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Entropy Regularization for Geometry and Neural Networks</title>
<link>https://arxiv.org/abs/2509.03733</link>
<guid>https://arxiv.org/abs/2509.03733</guid>
<content:encoded><![CDATA[
arXiv:2509.03733v1 Announce Type: cross 
Abstract: We introduce a differentiable estimator of range-partition entropy, a recent concept from computational geometry that enables algorithms to adapt to the "sortedness" of their input. While range-partition entropy provides strong guarantees in algorithm design, it has not yet been made accessible to deep learning. In this work, we (i) propose the first differentiable approximation of range-partition entropy, enabling its use as a trainable loss or regularizer; (ii) design EntropyNet, a neural module that restructures data into low-entropy forms to accelerate downstream instance-optimal algorithms; and (iii) extend this principle beyond geometry by applying entropy regularization directly to Transformer attention. Across tasks, we demonstrate that differentiable entropy improves efficiency without degrading correctness: in geometry, our method achieves up to $4.1\times$ runtime speedups with negligible error ($<0.2%$); in deep learning, it induces structured attention patterns that yield 6% higher accuracy at 80% sparsity compared to L1 baselines. Our theoretical analysis provides approximation bounds for the estimator, and extensive ablations validate design choices. These results suggest that entropy-bounded computation is not only theoretically elegant but also a practical mechanism for adaptive learning, efficiency, and structured representation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces</title>
<link>https://arxiv.org/abs/2509.03738</link>
<guid>https://arxiv.org/abs/2509.03738</guid>
<content:encoded><![CDATA[
arXiv:2509.03738v1 Announce Type: cross 
Abstract: We frame the problem of unifying representations in neural models as one of sparse model recovery and introduce a framework that extends sparse autoencoders (SAEs) to lifted spaces and infinite-dimensional function spaces, enabling mechanistic interpretability of large neural operators (NO). While the Platonic Representation Hypothesis suggests that neural networks converge to similar representations across architectures, the representational properties of neural operators remain underexplored despite their growing importance in scientific computing. We compare the inference and training dynamics of SAEs, lifted-SAE, and SAE neural operators. We highlight how lifting and operator modules introduce beneficial inductive biases, enabling faster recovery, improved recovery of smooth concepts, and robust inference across varying resolutions, a property unique to neural operators.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Gaze Analytics for ELA Instruction: A User-Centered Dashboard with Conversational AI Support</title>
<link>https://arxiv.org/abs/2509.03741</link>
<guid>https://arxiv.org/abs/2509.03741</guid>
<content:encoded><![CDATA[
arXiv:2509.03741v1 Announce Type: cross 
Abstract: Eye-tracking offers rich insights into student cognition and engagement, but remains underutilized in classroom-facing educational technology due to challenges in data interpretation and accessibility. In this paper, we present the iterative design and evaluation of a gaze-based learning analytics dashboard for English Language Arts (ELA), developed through five studies involving teachers and students. Guided by user-centered design and data storytelling principles, we explored how gaze data can support reflection, formative assessment, and instructional decision-making. Our findings demonstrate that gaze analytics can be approachable and pedagogically valuable when supported by familiar visualizations, layered explanations, and narrative scaffolds. We further show how a conversational agent, powered by a large language model (LLM), can lower cognitive barriers to interpreting gaze data by enabling natural language interactions with multimodal learning analytics. We conclude with design implications for future EdTech systems that aim to integrate novel data modalities in classroom contexts.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification</title>
<link>https://arxiv.org/abs/2509.03754</link>
<guid>https://arxiv.org/abs/2509.03754</guid>
<content:encoded><![CDATA[
arXiv:2509.03754v1 Announce Type: cross 
Abstract: Responding to rising global food security needs, precision agriculture and deep learning-based plant disease diagnosis have become crucial. Yet, deploying high-precision models on edge devices is challenging. Most lightweight networks use attention mechanisms designed for generic object recognition, which poorly capture subtle pathological features like irregular lesion shapes and complex textures. To overcome this, we propose a twofold solution: first, using a training-free neural architecture search method (DeepMAD) to create an efficient network backbone for edge devices; second, introducing the Shape-Texture Attention Module (STAM). STAM splits attention into two branches -- one using deformable convolutions (DCNv4) for shape awareness and the other using a Gabor filter bank for texture awareness. On the public CCMT plant disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs) reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm STAM significantly improves performance over baseline and standard attention models. Integrating domain knowledge via decoupled attention thus presents a promising path for edge-deployed precision agriculture AI. The source code is available at https://github.com/RzMY/STA-Net.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARDO: A Weak Formulation Deep Neural Network Method for Elliptic and Parabolic PDEs Based on Random Differences of Test Functions</title>
<link>https://arxiv.org/abs/2509.03757</link>
<guid>https://arxiv.org/abs/2509.03757</guid>
<content:encoded><![CDATA[
arXiv:2509.03757v1 Announce Type: cross 
Abstract: We propose ARDO method for solving PDEs and PDE-related problems with deep learning techniques. This method uses a weak adversarial formulation but transfers the random difference operator onto the test function. The main advantage of this framework is that it is fully derivative-free with respect to the solution neural network. This framework is particularly suitable for Fokker-Planck type second-order elliptic and parabolic PDEs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning an Adversarial World Model for Automated Curriculum Generation in MARL</title>
<link>https://arxiv.org/abs/2509.03771</link>
<guid>https://arxiv.org/abs/2509.03771</guid>
<content:encoded><![CDATA[
arXiv:2509.03771v1 Announce Type: cross 
Abstract: World models that infer and predict environmental dynamics are foundational to embodied intelligence. However, their potential is often limited by the finite complexity and implicit biases of hand-crafted training environments. To develop truly generalizable and robust agents, we need environments that scale in complexity alongside the agents learning within them. In this work, we reframe the challenge of environment generation as the problem of learning a goal-conditioned, generative world model. We propose a system where a generative **Attacker** agent learns an implicit world model to synthesize increasingly difficult challenges for a team of cooperative **Defender** agents. The Attacker's objective is not passive prediction, but active, goal-driven interaction: it models and generates world states (i.e., configurations of enemy units) specifically to exploit the Defenders' weaknesses. Concurrently, the embodied Defender team learns a cooperative policy to overcome these generated worlds. This co-evolutionary dynamic creates a self-scaling curriculum where the world model continuously adapts to challenge the decision-making policy of the agents, providing an effectively infinite stream of novel and relevant training scenarios. We demonstrate that this framework leads to the emergence of complex behaviors, such as the world model learning to generate flanking and shielding formations, and the defenders learning coordinated focus-fire and spreading tactics. Our findings position adversarial co-evolution as a powerful method for learning instrumental world models that drive agents toward greater strategic depth and robustness.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Latents: Latent Variables Stable Across Ontologies</title>
<link>https://arxiv.org/abs/2509.03780</link>
<guid>https://arxiv.org/abs/2509.03780</guid>
<content:encoded><![CDATA[
arXiv:2509.03780v1 Announce Type: cross 
Abstract: Suppose two Bayesian agents each learn a generative model of the same environment. We will assume the two have converged on the predictive distribution, i.e. distribution over some observables in the environment, but may have different generative models containing different latent variables. Under what conditions can one agent guarantee that their latents are a function of the other agents latents?
  We give simple conditions under which such translation is guaranteed to be possible: the natural latent conditions. We also show that, absent further constraints, these are the most general conditions under which translatability is guaranteed. Crucially for practical application, our theorems are robust to approximation error in the natural latent conditions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?</title>
<link>https://arxiv.org/abs/2509.03790</link>
<guid>https://arxiv.org/abs/2509.03790</guid>
<content:encoded><![CDATA[
arXiv:2509.03790v1 Announce Type: cross 
Abstract: What fundamental properties of reward functions enable efficient sparse-reward reinforcement learning? We address this question through the lens of low-rank structure in reward matrices, showing that such structure induces a sharp transition from exponential to polynomial sample complexity, the first result of this kind for sparse-reward RL. We introduce Policy-Aware Matrix Completion (PAMC), which connects matrix completion theory with reinforcement learning via a new analysis of policy-dependent sampling. Our framework provides: (i) impossibility results for general sparse reward observation, (ii) reward-free representation learning from dynamics, (iii) distribution-free confidence sets via conformal prediction, and (iv) robust completion guarantees that degrade gracefully when low-rank structure is only approximate. Empirically, we conduct a pre-registered evaluation across 100 systematically sampled domains, finding exploitable structure in over half. PAMC improves sample efficiency by factors between 1.6 and 2.1 compared to strong exploration, structured, and representation-learning baselines, while adding only about 20 percent computational overhead.These results establish structural reward learning as a promising new paradigm, with immediate implications for robotics, healthcare, and other safety-critical, sample-expensive applications.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation</title>
<link>https://arxiv.org/abs/2509.03791</link>
<guid>https://arxiv.org/abs/2509.03791</guid>
<content:encoded><![CDATA[
arXiv:2509.03791v1 Announce Type: cross 
Abstract: Evaluating sign language generation is often done through back-translation, where generated signs are first recognized back to text and then compared to a reference using text-based metrics. However, this two-step evaluation pipeline introduces ambiguity: it not only fails to capture the multimodal nature of sign language-such as facial expressions, spatial grammar, and prosody-but also makes it hard to pinpoint whether evaluation errors come from sign generation model or the translation system used to assess it. In this work, we propose SiLVERScore, a novel semantically-aware embedding-based evaluation metric that assesses sign language generation in a joint embedding space. Our contributions include: (1) identifying limitations of existing metrics, (2) introducing SiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness to semantic and prosodic variations, and (4) exploring generalization challenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore achieves near-perfect discrimination between correct and random pairs (ROC AUC = 0.99, overlap < 7%), substantially outperforming traditional metrics.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMVAD: A Multi-Agent System for Simulating Judicial Deliberation Dynamics in India</title>
<link>https://arxiv.org/abs/2509.03793</link>
<guid>https://arxiv.org/abs/2509.03793</guid>
<content:encoded><![CDATA[
arXiv:2509.03793v1 Announce Type: cross 
Abstract: Understanding the complexities of judicial deliberation is crucial for assessing the efficacy and fairness of a justice system. However, empirical studies of judicial panels are constrained by significant ethical and practical barriers. This paper introduces SAMVAD, an innovative Multi-Agent System (MAS) designed to simulate the deliberation process within the framework of the Indian justice system.
  Our system comprises agents representing key judicial roles: a Judge, a Prosecution Counsel, a Defense Counsel, and multiple Adjudicators (simulating a judicial bench), all powered by large language models (LLMs). A primary contribution of this work is the integration of Retrieval-Augmented Generation (RAG), grounded in a domain-specific knowledge base of landmark Indian legal documents, including the Indian Penal Code and the Constitution of India. This RAG functionality enables the Judge and Counsel agents to generate legally sound instructions and arguments, complete with source citations, thereby enhancing both the fidelity and transparency of the simulation.
  The Adjudicator agents engage in iterative deliberation rounds, processing case facts, legal instructions, and arguments to reach a consensus-based verdict. We detail the system architecture, agent communication protocols, the RAG pipeline, the simulation workflow, and a comprehensive evaluation plan designed to assess performance, deliberation quality, and outcome consistency.
  This work provides a configurable and explainable MAS platform for exploring legal reasoning and group decision-making dynamics in judicial simulations, specifically tailored to the Indian legal context and augmented with verifiable legal grounding via RAG.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring How (Not Just Whether) VLMs Build Common Ground</title>
<link>https://arxiv.org/abs/2509.03805</link>
<guid>https://arxiv.org/abs/2509.03805</guid>
<content:encoded><![CDATA[
arXiv:2509.03805v1 Announce Type: cross 
Abstract: Large vision language models (VLMs) increasingly claim reasoning skills, yet current benchmarks evaluate them in single-turn or question answering settings. However, grounding is an interactive process in which people gradually develop shared understanding through ongoing communication. We introduce a four-metric suite (grounding efficiency, content alignment, lexical adaptation, and human-likeness) to systematically evaluate VLM performance in interactive grounding contexts. We deploy the suite on 150 self-play sessions of interactive referential games between three proprietary VLMs and compare them with human dyads. All three models diverge from human patterns on at least three metrics, while GPT4o-mini is the closest overall. We find that (i) task success scores do not indicate successful grounding and (ii) high image-utterance alignment does not necessarily predict task success. Our metric suite and findings offer a framework for future research on VLM grounding.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation</title>
<link>https://arxiv.org/abs/2509.03809</link>
<guid>https://arxiv.org/abs/2509.03809</guid>
<content:encoded><![CDATA[
arXiv:2509.03809v1 Announce Type: cross 
Abstract: Large language models (LLMs) have ushered in a new era for document-level machine translation (\textit{doc}-mt), yet their whole-document outputs challenge existing evaluation methods that assume sentence-by-sentence alignment. We introduce \textit{\textbf{Align-then-Slide}}, a complete evaluation framework for ultra-long doc-mt. In the Align stage, we automatically infer sentence-level source-target correspondences and rebuild the target to match the source sentence number, resolving omissions and many-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we calculate averaged metric scores under 1-, 2-, 3- and 4-chunk for multi-granularity assessment. Experiments on the WMT benchmark show a Pearson correlation of 0.929 between our method with expert MQM rankings. On a newly curated real-world test set, our method again aligns closely with human judgments. Furthermore, preference data produced by Align-then-Slide enables effective CPO training and its direct use as a reward model for GRPO, both yielding translations preferred over a vanilla SFT baseline. The results validate our framework as an accurate, robust, and actionable evaluation tool for doc-mt systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gravity Well Echo Chamber Modeling With An LLM-Based Confirmation Bias Model</title>
<link>https://arxiv.org/abs/2509.03832</link>
<guid>https://arxiv.org/abs/2509.03832</guid>
<content:encoded><![CDATA[
arXiv:2509.03832v1 Announce Type: cross 
Abstract: Social media echo chambers play a central role in the spread of misinformation, yet existing models often overlook the influence of individual confirmation bias. An existing model of echo chambers is the "gravity well" model, which creates an analog between echo chambers and spatial gravity wells. We extend this established model by introducing a dynamic confirmation bias variable that adjusts the strength of pull based on a user's susceptibility to belief-reinforcing content. This variable is calculated for each user through comparisons between their posting history and their responses to posts of a wide range of viewpoints.
  Incorporating this factor produces a confirmation-bias-integrated gravity well model that more accurately identifies echo chambers and reveals community-level markers of information health. We validated the approach on nineteen Reddit communities, demonstrating improved detection of echo chambers.
  Our contribution is a framework for systematically capturing the role of confirmation bias in online group dynamics, enabling more effective identification of echo chambers. By flagging these high-risk environments, the model supports efforts to curb the spread of misinformation at its most common points of amplification.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Leiden to Pleasure Island: The Constant Potts Model for Community Detection as a Hedonic Game</title>
<link>https://arxiv.org/abs/2509.03834</link>
<guid>https://arxiv.org/abs/2509.03834</guid>
<content:encoded><![CDATA[
arXiv:2509.03834v1 Announce Type: cross 
Abstract: Community detection is one of the fundamental problems in data science which consists of partitioning nodes into disjoint communities. We present a game-theoretic perspective on the Constant Potts Model (CPM) for partitioning networks into disjoint communities, emphasizing its efficiency, robustness, and accuracy. Efficiency: We reinterpret CPM as a potential hedonic game by decomposing its global Hamiltonian into local utility functions, where the local utility gain of each agent matches the corresponding increase in global utility. Leveraging this equivalence, we prove that local optimization of the CPM objective via better-response dynamics converges in pseudo-polynomial time to an equilibrium partition. Robustness: We introduce and relate two stability criteria: a strict criterion based on a novel notion of robustness, requiring nodes to simultaneously maximize neighbors and minimize non-neighbors within communities, and a relaxed utility function based on a weighted sum of these objectives, controlled by a resolution parameter. Accuracy: In community tracking scenarios, where initial partitions are used to bootstrap the Leiden algorithm with partial ground-truth information, our experiments reveal that robust partitions yield higher accuracy in recovering ground-truth communities.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INGRID: Intelligent Generative Robotic Design Using Large Language Models</title>
<link>https://arxiv.org/abs/2509.03842</link>
<guid>https://arxiv.org/abs/2509.03842</guid>
<content:encoded><![CDATA[
arXiv:2509.03842v1 Announce Type: cross 
Abstract: The integration of large language models (LLMs) into robotic systems has accelerated progress in embodied artificial intelligence, yet current approaches remain constrained by existing robotic architectures, particularly serial mechanisms. This hardware dependency fundamentally limits the scope of robotic intelligence. Here, we present INGRID (Intelligent Generative Robotic Design), a framework that enables the automated design of parallel robotic mechanisms through deep integration with reciprocal screw theory and kinematic synthesis methods. We decompose the design challenge into four progressive tasks: constraint analysis, kinematic joint generation, chain construction, and complete mechanism design. INGRID demonstrates the ability to generate novel parallel mechanisms with both fixed and variable mobility, discovering kinematic configurations not previously documented in the literature. We validate our approach through three case studies demonstrating how INGRID assists users in designing task-specific parallel robots based on desired mobility requirements. By bridging the gap between mechanism theory and machine learning, INGRID enables researchers without specialized robotics training to create custom parallel mechanisms, thereby decoupling advances in robotic intelligence from hardware constraints. This work establishes a foundation for mechanism intelligence, where AI systems actively design robotic hardware, potentially transforming the development of embodied AI systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Inverse Reinforcement Learning for Mean Field Games via Probabilistic Context Variables</title>
<link>https://arxiv.org/abs/2509.03845</link>
<guid>https://arxiv.org/abs/2509.03845</guid>
<content:encoded><![CDATA[
arXiv:2509.03845v1 Announce Type: cross 
Abstract: Designing suitable reward functions for numerous interacting intelligent agents is challenging in real-world applications. Inverse reinforcement learning (IRL) in mean field games (MFGs) offers a practical framework to infer reward functions from expert demonstrations. While promising, the assumption of agent homogeneity limits the capability of existing methods to handle demonstrations with heterogeneous and unknown objectives, which are common in practice. To this end, we propose a deep latent variable MFG model and an associated IRL method. Critically, our method can infer rewards from different yet structurally similar tasks without prior knowledge about underlying contexts or modifying the MFG model itself. Our experiments, conducted on simulated scenarios and a real-world spatial taxi-ride pricing problem, demonstrate the superiority of our approach over state-of-the-art IRL methods in MFGs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MillGNN: Learning Multi-Scale Lead-Lag Dependencies for Multi-Variate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.03852</link>
<guid>https://arxiv.org/abs/2509.03852</guid>
<content:encoded><![CDATA[
arXiv:2509.03852v1 Announce Type: cross 
Abstract: Multi-variate time series (MTS) forecasting is crucial for various applications. Existing methods have shown promising results owing to their strong ability to capture intra- and inter-variate dependencies. However, these methods often overlook lead-lag dependencies at multiple grouping scales, failing to capture hierarchical lead-lag effects in complex systems. To this end, we propose MillGNN, a novel \underline{g}raph \underline{n}eural \underline{n}etwork-based method that learns \underline{m}ult\underline{i}ple grouping scale \underline{l}ead-\underline{l}ag dependencies for MTS forecasting, which can comprehensively capture lead-lag effects considering variate-wise and group-wise dynamics and decays. Specifically, MillGNN introduces two key innovations: (1) a scale-specific lead-lag graph learning module that integrates cross-correlation coefficients and dynamic decaying features derived from real-time inputs and time lags to learn lead-lag dependencies for each scale, which can model evolving lead-lag dependencies with statistical interpretability and data-driven flexibility; (2) a hierarchical lead-lag message passing module that passes lead-lag messages at multiple grouping scales in a structured way to simultaneously propagate intra- and inter-scale lead-lag effects, which can capture multi-scale lead-lag effects with a balance of comprehensiveness and efficiency. Experimental results on 11 datasets demonstrate the superiority of MillGNN for long-term and short-term MTS forecasting, compared with 16 state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2509.03871</link>
<guid>https://arxiv.org/abs/2509.03871</guid>
<content:encoded><![CDATA[
arXiv:2509.03871v1 Announce Type: cross 
Abstract: The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition</title>
<link>https://arxiv.org/abs/2509.03873</link>
<guid>https://arxiv.org/abs/2509.03873</guid>
<content:encoded><![CDATA[
arXiv:2509.03873v1 Announce Type: cross 
Abstract: Food recognition has gained significant attention, but the rapid emergence of new dishes requires methods for recognizing unseen food categories, motivating Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot Food Recognition (CZSFR), where cuisines and ingredients naturally align with attributes and objects in Compositional Zero-Shot learning (CZSL). However, CZSFR faces three challenges: (1) Redundant background information distracts models from learning meaningful food features, (2) Role confusion between staple and side dishes leads to misclassification, and (3) Semantic bias in a single attribute can lead to confusion of understanding. Therefore, we propose SalientFusion, a context-aware CZSFR method with two components: SalientFormer, which removes background redundancy and uses depth features to resolve role confusion; DebiasAT, which reduces the semantic bias by aligning prompts with visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we show that SalientFusion achieves state-of-the-art results on these benchmarks and the most popular general datasets for the general CZSL. The code is avaliable at https://github.com/Jiajun-RUC/SalientFusion.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peptidomic-Based Prediction Model for Coronary Heart Disease Using a Multilayer Perceptron Neural Network</title>
<link>https://arxiv.org/abs/2509.03884</link>
<guid>https://arxiv.org/abs/2509.03884</guid>
<content:encoded><![CDATA[
arXiv:2509.03884v1 Announce Type: cross 
Abstract: Coronary heart disease (CHD) is a leading cause of death worldwide and contributes significantly to annual healthcare expenditures. To develop a non-invasive diagnostic approach, we designed a model based on a multilayer perceptron (MLP) neural network, trained on 50 key urinary peptide biomarkers selected via genetic algorithms. Treatment and control groups, each comprising 345 individuals, were balanced using the Synthetic Minority Over-sampling Technique (SMOTE). The neural network was trained using a stratified validation strategy. Using a network with three hidden layers of 60 neurons each and an output layer of two neurons, the model achieved a precision, sensitivity, and specificity of 95.67 percent, with an F1-score of 0.9565. The area under the ROC curve (AUC) reached 0.9748 for both classes, while the Matthews correlation coefficient (MCC) and Cohen's kappa coefficient were 0.9134 and 0.9131, respectively, demonstrating its reliability in detecting CHD. These results indicate that the model provides a highly accurate and robust non-invasive diagnostic tool for coronary heart disease.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance</title>
<link>https://arxiv.org/abs/2509.03889</link>
<guid>https://arxiv.org/abs/2509.03889</guid>
<content:encoded><![CDATA[
arXiv:2509.03889v1 Announce Type: cross 
Abstract: Manipulating clothing is challenging due to complex configurations, variable material dynamics, and frequent self-occlusion. Prior systems often flatten garments or assume visibility of key features. We present a dual-arm visuotactile framework that combines confidence-aware dense visual correspondence and tactile-supervised grasp affordance to operate directly on crumpled and suspended garments. The correspondence model is trained on a custom, high-fidelity simulated dataset using a distributional loss that captures cloth symmetries and generates correspondence confidence estimates. These estimates guide a reactive state machine that adapts folding strategies based on perceptual uncertainty. In parallel, a visuotactile grasp affordance network, self-supervised using high-resolution tactile feedback, determines which regions are physically graspable. The same tactile classifier is used during execution for real-time grasp validation. By deferring action in low-confidence states, the system handles highly occluded table-top and in-air configurations. We demonstrate our task-agnostic grasp selection module in folding and hanging tasks. Moreover, our dense descriptors provide a reusable intermediate representation for other planning modalities, such as extracting grasp targets from human video demonstrations, paving the way for more generalizable and scalable garment manipulation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Generative Models Meet Compressed Sensing, with Applications to Image Data and Financial Time Series</title>
<link>https://arxiv.org/abs/2509.03898</link>
<guid>https://arxiv.org/abs/2509.03898</guid>
<content:encoded><![CDATA[
arXiv:2509.03898v1 Announce Type: cross 
Abstract: This paper develops dimension reduction techniques for accelerating diffusion model inference in the context of synthetic data generation. The idea is to integrate compressed sensing into diffusion models: (i) compress the data into a latent space, (ii) train a diffusion model in the latent space, and (iii) apply a compressed sensing algorithm to the samples generated in the latent space, facilitating the efficiency of both model training and inference. Under suitable sparsity assumptions on data, the proposed algorithm is proved to enjoy faster convergence by combining diffusion model inference with sparse recovery. As a byproduct, we obtain an optimal value for the latent space dimension. We also conduct numerical experiments on a range of datasets, including image data (handwritten digits, medical images, and climate data) and financial time series for stress testing.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question Answering</title>
<link>https://arxiv.org/abs/2509.03918</link>
<guid>https://arxiv.org/abs/2509.03918</guid>
<content:encoded><![CDATA[
arXiv:2509.03918v1 Announce Type: cross 
Abstract: Complex Question Answering (QA) is a fundamental and challenging task in NLP. While large language models (LLMs) exhibit impressive performance in QA, they suffer from significant performance degradation when facing complex and abstract QA tasks due to insufficient reasoning capabilities. Works such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning abilities, but they face issues such as in-layer redundancy in tree structures and single paths in chain structures. Although some studies utilize Retrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the challenge of effectively utilizing large amounts of information involving multiple entities and hops remains critical. To address this, we propose the Matrix of Thought (MoT), a novel and efficient LLM thought structure. MoT explores the problem in both horizontal and vertical dimensions through the "column-cell communication" mechanism, enabling LLMs to actively engage in multi-strategy and deep-level thinking, reducing redundancy within the column cells and enhancing reasoning capabilities. Furthermore, we develop a fact-correction mechanism by constructing knowledge units from retrieved knowledge graph triples and raw text to enhance the initial knowledge for LLM reasoning and correct erroneous answers. This leads to the development of an efficient and accurate QA framework (MTQA). Experimental results show that our framework outperforms state-of-the-art methods on four widely-used datasets in terms of F1 and EM scores, with reasoning time only 14.4\% of the baseline methods, demonstrating both its efficiency and accuracy. The code for this framework is available at https://github.com/lyfiter/mtqa.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment</title>
<link>https://arxiv.org/abs/2509.03934</link>
<guid>https://arxiv.org/abs/2509.03934</guid>
<content:encoded><![CDATA[
arXiv:2509.03934v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have revolutionized natural language processing through their remarkable capabilities in understanding and executing diverse tasks. While supervised fine-tuning, particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively enhances task-specific performance, it often leads to catastrophic forgetting, where models lose their previously acquired knowledge and general capabilities. Existing solutions either require access to general instruction data or face limitations in preserving the model's original distribution. To overcome these limitations, we propose SelfAug, a self-distribution alignment method that aligns input sequence logits to preserve the model's semantic distribution, thereby mitigating catastrophic forgetting and improving downstream performance. Extensive experiments demonstrate that SelfAug achieves a superior balance between downstream learning and general capability retention. Our comprehensive empirical analysis reveals a direct correlation between distribution shifts and the severity of catastrophic forgetting in RAG scenarios, highlighting how the absence of RAG capabilities in general instruction tuning leads to significant distribution shifts during fine-tuning. Our findings not only advance the understanding of catastrophic forgetting in RAG contexts but also provide a practical solution applicable across diverse fine-tuning scenarios. Our code is publicly available at https://github.com/USTC-StarTeam/SelfAug.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning</title>
<link>https://arxiv.org/abs/2509.03937</link>
<guid>https://arxiv.org/abs/2509.03937</guid>
<content:encoded><![CDATA[
arXiv:2509.03937v1 Announce Type: cross 
Abstract: Despite the significant advancements of self-play fine-tuning (SPIN), which can transform a weak large language model (LLM) into a strong one through competitive interactions between models of varying capabilities, it still faces challenges in the Text-to-SQL task. SPIN does not generate new information, and the large number of correct SQL queries produced by the opponent model during self-play reduces the main model's ability to generate accurate SQL queries. To address this challenge, we propose a new self-play fine-tuning method tailored for the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a verification-based iterative fine-tuning approach, which synthesizes high-quality fine-tuning data iteratively based on the database schema and validation feedback to enhance model performance, while building a model base with varying capabilities. During the self-play fine-tuning phase, we propose an error-driven loss method that incentivizes incorrect outputs from the opponent model, enabling the main model to distinguish between correct SQL and erroneous SQL generated by the opponent model, thereby improving its ability to generate correct SQL. Extensive experiments and in-depth analyses on six open-source LLMs and five widely used benchmarks demonstrate that our approach outperforms existing state-of-the-art (SOTA) methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based Role-Playing Agents</title>
<link>https://arxiv.org/abs/2509.03940</link>
<guid>https://arxiv.org/abs/2509.03940</guid>
<content:encoded><![CDATA[
arXiv:2509.03940v1 Announce Type: cross 
Abstract: Recent significant advancements in Large Language Models (LLMs) have greatly propelled the development of Role-Playing Conversational Agents (RPCAs). These systems aim to create immersive user experiences through consistent persona adoption. However, current RPCA research faces dual limitations. First, existing work predominantly focuses on the textual modality, entirely overlooking critical paralinguistic features including intonation, prosody, and rhythm in speech, which are essential for conveying character emotions and shaping vivid identities. Second, the speech-based role-playing domain suffers from a long-standing lack of standardized evaluation benchmarks. Most current spoken dialogue datasets target only fundamental capability assessments, featuring thinly sketched or ill-defined character profiles. Consequently, they fail to effectively quantify model performance on core competencies like long-term persona consistency. To address this critical gap, we introduce VoxRole, the first comprehensive benchmark specifically designed for the evaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn dialogues, totaling 65.6 hours of speech from 1228 unique characters across 261 movies. To construct this resource, we propose a novel two-stage automated pipeline that first aligns movie audio with scripts and subsequently employs an LLM to systematically build multi-dimensional profiles for each character. Leveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary spoken dialogue models, revealing crucial insights into their respective strengths and limitations in maintaining persona consistency.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture</title>
<link>https://arxiv.org/abs/2509.03950</link>
<guid>https://arxiv.org/abs/2509.03950</guid>
<content:encoded><![CDATA[
arXiv:2509.03950v1 Announce Type: cross 
Abstract: Pneumothorax, the abnormal accumulation of air in the pleural space, can be life-threatening if undetected. Chest X-rays are the first-line diagnostic tool, but small cases may be subtle. We propose an automated deep-learning pipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax regions. Trained on the SIIM-ACR dataset with data augmentation and a combined binary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and Dice score of 0.8241 on the independent PTX-498 dataset. These results demonstrate that the model can accurately localize pneumothoraces and support radiologists.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking</title>
<link>https://arxiv.org/abs/2509.03957</link>
<guid>https://arxiv.org/abs/2509.03957</guid>
<content:encoded><![CDATA[
arXiv:2509.03957v1 Announce Type: cross 
Abstract: The effectiveness of large language models (LLMs) to fact-check misinformation remains uncertain, despite their growing use. To this end, we present CANDY, a benchmark designed to systematically evaluate the capabilities and limitations of LLMs in fact-checking Chinese misinformation. Specifically, we curate a carefully annotated dataset of ~20k instances. Our analysis shows that current LLMs exhibit limitations in generating accurate fact-checking conclusions, even when enhanced with chain-of-thought reasoning and few-shot prompting. To understand these limitations, we develop a taxonomy to categorize flawed LLM-generated explanations for their conclusions and identify factual fabrication as the most common failure mode. Although LLMs alone are unreliable for fact-checking, our findings indicate their considerable potential to augment human performance when deployed as assistive tools in scenarios. Our dataset and code can be accessed at https://github.com/SCUNLP/CANDY
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection</title>
<link>https://arxiv.org/abs/2509.03961</link>
<guid>https://arxiv.org/abs/2509.03961</guid>
<content:encoded><![CDATA[
arXiv:2509.03961v1 Announce Type: cross 
Abstract: Although deep learning has advanced remote sensing change detection (RSCD), most methods rely solely on image modality, limiting feature representation, change pattern modeling, and generalization especially under illumination and noise disturbances. To address this, we propose MMChange, a multimodal RSCD method that combines image and text modalities to enhance accuracy and robustness. An Image Feature Refinement (IFR) module is introduced to highlight key regions and suppress environmental noise. To overcome the semantic limitations of image features, we employ a vision language model (VLM) to generate semantic descriptions of bitemporal images. A Textual Difference Enhancement (TDE) module then captures fine grained semantic shifts, guiding the model toward meaningful changes. To bridge the heterogeneity between modalities, we design an Image Text Feature Fusion (ITFF) module that enables deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and SYSUCD demonstrate that MMChange consistently surpasses state of the art methods across multiple metrics, validating its effectiveness for multimodal RSCD. Code is available at: https://github.com/yikuizhai/MMChange.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study</title>
<link>https://arxiv.org/abs/2509.03972</link>
<guid>https://arxiv.org/abs/2509.03972</guid>
<content:encoded><![CDATA[
arXiv:2509.03972v1 Announce Type: cross 
Abstract: We introduce Llama-3-Motif, a language model consisting of 102 billion parameters, specifically designed to enhance Korean capabilities while retaining strong performance in English. Developed on the Llama 3 architecture, Llama-3-Motif employs advanced training techniques, including LlamaPro and Masked Structure Growth, to effectively scale the model without altering its core Transformer architecture. Using the MoAI platform for efficient training across hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully curated dataset that maintains a balanced ratio of Korean and English data. Llama-3-Motif shows decent performance on Korean-specific benchmarks, outperforming existing models and achieving results comparable to GPT-4.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2509.03973</link>
<guid>https://arxiv.org/abs/2509.03973</guid>
<content:encoded><![CDATA[
arXiv:2509.03973v1 Announce Type: cross 
Abstract: We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for performing WSI classification. SAC-MIL consists of a positional encoding module to encode position information and a SAC block to perform full instance correlations. The positional encoding module utilizes the instance coordinates within the slide to encode the spatial relationships instead of the instance index in the input WSI sequence. The positional encoding module can also handle the length extrapolation issue where the training and testing sequences have different lengths. The SAC block is an MLP-based method that performs full instance correlation in linear time complexity with respect to the sequence length. Due to the simple structure of MLP, it is easy to deploy since it does not require custom CUDA kernels, compared to Transformer-based methods for WSI classification. SAC-MIL has achieved state-of-the-art performance on the CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language Models</title>
<link>https://arxiv.org/abs/2509.03985</link>
<guid>https://arxiv.org/abs/2509.03985</guid>
<content:encoded><![CDATA[
arXiv:2509.03985v1 Announce Type: cross 
Abstract: In deployment and application, large language models (LLMs) typically undergo safety alignment to prevent illegal and unethical outputs. However, the continuous advancement of jailbreak attack techniques, designed to bypass safety mechanisms with adversarial prompts, has placed increasing pressure on the security defenses of LLMs. Strengthening resistance to jailbreak attacks requires an in-depth understanding of the security mechanisms and vulnerabilities of LLMs. However, the vast number of parameters and complex structure of LLMs make analyzing security weaknesses from an internal perspective a challenging task. This paper presents NeuroBreak, a top-down jailbreak analysis system designed to analyze neuron-level safety mechanisms and mitigate vulnerabilities. We carefully design system requirements through collaboration with three experts in the field of AI security. The system provides a comprehensive analysis of various jailbreak attack methods. By incorporating layer-wise representation probing analysis, NeuroBreak offers a novel perspective on the model's decision-making process throughout its generation steps. Furthermore, the system supports the analysis of critical neurons from both semantic and functional perspectives, facilitating a deeper exploration of security mechanisms. We conduct quantitative evaluations and case studies to verify the effectiveness of our system, offering mechanistic insights for developing next-generation defense strategies against evolving jailbreak attacks.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promptception: How Sensitive Are Large Multimodal Models to Prompts?</title>
<link>https://arxiv.org/abs/2509.03986</link>
<guid>https://arxiv.org/abs/2509.03986</guid>
<content:encoded><![CDATA[
arXiv:2509.03986v1 Announce Type: cross 
Abstract: Despite the success of Large Multimodal Models (LMMs) in recent years, prompt design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly understood. We show that even minor variations in prompt phrasing and structure can lead to accuracy deviations of up to 15% for certain prompts and models. This variability poses a challenge for transparent and fair LMM evaluation, as models often report their best-case performance using carefully selected prompts. To address this, we introduce Promptception, a systematic framework for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types, spanning 15 categories and 6 supercategories, each targeting specific aspects of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks: MMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit greater sensitivity to prompt phrasing, reflecting tighter alignment with instruction semantics, while open-source models are steadier but struggle with nuanced and complex phrasing. Based on this analysis, we propose Prompting Principles tailored to proprietary and open-source LMMs, enabling more robust and fair model evaluation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models</title>
<link>https://arxiv.org/abs/2509.03995</link>
<guid>https://arxiv.org/abs/2509.03995</guid>
<content:encoded><![CDATA[
arXiv:2509.03995v1 Announce Type: cross 
Abstract: Current temporal knowledge graph question answering (TKGQA) methods primarily focus on implicit temporal constraints, lacking the capability of handling more complex temporal queries, and struggle with limited reasoning abilities and error propagation in decomposition frameworks. We propose RTQA, a novel framework to address these challenges by enhancing reasoning over TKGs without requiring training. Following recursive thinking, RTQA recursively decomposes questions into sub-problems, solves them bottom-up using LLMs and TKG knowledge, and employs multi-path answer aggregation to improve fault tolerance. RTQA consists of three core components: the Temporal Question Decomposer, the Recursive Solver, and the Answer Aggregator. Experiments on MultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements in "Multiple" and "Complex" categories, outperforming state-of-the-art methods. Our code and data are available at https://github.com/zjukg/RTQA.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Regional Spurious Correlations in Vision Transformers via Token Discarding</title>
<link>https://arxiv.org/abs/2509.04009</link>
<guid>https://arxiv.org/abs/2509.04009</guid>
<content:encoded><![CDATA[
arXiv:2509.04009v1 Announce Type: cross 
Abstract: Due to their powerful feature association capabilities, neural network-based computer vision models have the ability to detect and exploit unintended patterns within the data, potentially leading to correct predictions based on incorrect or unintended but statistically relevant signals. These clues may vary from simple color aberrations to small texts within the image. In situations where these unintended signals align with the predictive task, models can mistakenly link these features with the task and rely on them for making predictions. This phenomenon is referred to as spurious correlations, where patterns appear to be associated with the task but are actually coincidental. As a result, detection and mitigation of spurious correlations have become crucial tasks for building trustworthy, reliable, and generalizable machine learning models. In this work, we present a novel method to detect spurious correlations in vision transformers, a type of neural network architecture that gained significant popularity in recent years. Using both supervised and self-supervised trained models, we present large-scale experiments on the ImageNet dataset demonstrating the ability of the proposed method to identify spurious correlations. We also find that, even if the same architecture is used, the training methodology has a significant impact on the model's reliance on spurious correlations. Furthermore, we show that certain classes in the ImageNet dataset contain spurious signals that are easily detected by the models and discuss the underlying reasons for those spurious signals. In light of our findings, we provide an exhaustive list of the aforementioned images and call for caution in their use in future research efforts. Lastly, we present a case study investigating spurious signals in invasive breast mass classification, grounding our work in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings</title>
<link>https://arxiv.org/abs/2509.04011</link>
<guid>https://arxiv.org/abs/2509.04011</guid>
<content:encoded><![CDATA[
arXiv:2509.04011v1 Announce Type: cross 
Abstract: We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named Entity Retrieval, a variant of Named Entity Recognition (NER), where the types of interest are not provided in advance, and a user-defined type description is used to retrieve documents mentioning entities of that type. Instead of relying on fixed schemas or fine-tuned models, our method builds on internal representations of large language models (LLMs) to embed both entity mentions and user-provided open-ended type descriptions into a shared semantic space. We show that internal representations, specifically the value vectors from mid-layer transformer blocks, encode fine-grained type information more effectively than commonly used top-layer embeddings. To refine these representations, we train a lightweight contrastive projection network that aligns type-compatible entities while separating unrelated types. The resulting entity embeddings are compact, type-aware, and well-suited for nearest-neighbor search. Evaluated on three benchmarks, NER Retriever significantly outperforms both lexical and dense sentence-level retrieval baselines. Our findings provide empirical support for representation selection within LLMs and demonstrate a practical solution for scalable, schema-free entity retrieval. The NER Retriever Codebase is publicly available at https://github.com/ShacharOr100/ner_retriever
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Robustness and Reliability of Benchmark-Based Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2509.04013</link>
<guid>https://arxiv.org/abs/2509.04013</guid>
<content:encoded><![CDATA[
arXiv:2509.04013v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) effectiveness is usually evaluated by means of benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in their original wording, thus in a fixed, standardized format. However, real-world applications involve linguistic variability, requiring models to maintain their effectiveness across diverse rewordings of the same question or query. In this study, we systematically assess the robustness of LLMs to paraphrased benchmark questions and investigate whether benchmark-based evaluations provide a reliable measure of model capabilities. We systematically generate various paraphrases of all the questions across six different common benchmarks, and measure the resulting variations in effectiveness of 34 state-of-the-art LLMs, of different size and effectiveness. Our findings reveal that while LLM rankings remain relatively stable across paraphrased inputs, absolute effectiveness scores change, and decline significantly. This suggests that LLMs struggle with linguistic variability, raising concerns about their generalization abilities and evaluation methodologies. Furthermore, the observed performance drop challenges the reliability of benchmark-based evaluations, indicating that high benchmark scores may not fully capture a model's robustness to real-world input variations. We discuss the implications of these findings for LLM evaluation methodologies, emphasizing the need for robustness-aware benchmarks that better reflect practical deployment scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Video Compression with In-Loop Contextual Filtering and Out-of-Loop Reconstruction Enhancement</title>
<link>https://arxiv.org/abs/2509.04051</link>
<guid>https://arxiv.org/abs/2509.04051</guid>
<content:encoded><![CDATA[
arXiv:2509.04051v1 Announce Type: cross 
Abstract: This paper explores the application of enhancement filtering techniques in neural video compression. Specifically, we categorize these techniques into in-loop contextual filtering and out-of-loop reconstruction enhancement based on whether the enhanced representation affects the subsequent coding loop. In-loop contextual filtering refines the temporal context by mitigating error propagation during frame-by-frame encoding. However, its influence on both the current and subsequent frames poses challenges in adaptively applying filtering throughout the sequence. To address this, we introduce an adaptive coding decision strategy that dynamically determines filtering application during encoding. Additionally, out-of-loop reconstruction enhancement is employed to refine the quality of reconstructed frames, providing a simple yet effective improvement in coding efficiency. To the best of our knowledge, this work presents the first systematic study of enhancement filtering in the context of conditional-based neural video compression. Extensive experiments demonstrate a 7.71% reduction in bit rate compared to state-of-the-art neural video codecs, validating the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot</title>
<link>https://arxiv.org/abs/2509.04076</link>
<guid>https://arxiv.org/abs/2509.04076</guid>
<content:encoded><![CDATA[
arXiv:2509.04076v1 Announce Type: cross 
Abstract: We propose a novel diffusion-based action model for robotic motion planning. Commonly, established numerical planning approaches are used to solve general motion planning problems, but have significant runtime requirements. By leveraging the power of deep learning, we are able to achieve good results in a much smaller runtime by learning from a dataset generated by these planners. While our initial model uses point cloud embeddings in the input to predict keypoint-based joint sequences in its output, we observed in our ablation study that it remained challenging to condition the network on the point cloud embeddings. We identified some biases in our dataset and refined it, which improved the model's performance. Our model, even without the use of the point cloud encodings, outperforms numerical models by an order of magnitude regarding the runtime, while reaching a success rate of up to 90% of collision free solutions on the test set.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2509.04078</link>
<guid>https://arxiv.org/abs/2509.04078</guid>
<content:encoded><![CDATA[
arXiv:2509.04078v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have exhibited significant proficiency in code debugging, especially in automatic program repair, which may substantially reduce the time consumption of developers and enhance their efficiency. Significant advancements in debugging datasets have been made to promote the development of code debugging. However, these datasets primarily focus on assessing the LLM's function-level code repair capabilities, neglecting the more complex and realistic repository-level scenarios, which leads to an incomplete understanding of the LLM's challenges in repository-level debugging. While several repository-level datasets have been proposed, they often suffer from limitations such as limited diversity of tasks, languages, and error types. To mitigate this challenge, this paper introduces RepoDebug, a multi-task and multi-language repository-level code debugging dataset with 22 subtypes of errors that supports 8 commonly used programming languages and 3 debugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs, where Claude 3.5 Sonnect, the best-performing model, still cannot perform well in repository-level debugging.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHVC: Efficient Hierarchical Reference and Quality Structure for Neural Video Coding</title>
<link>https://arxiv.org/abs/2509.04118</link>
<guid>https://arxiv.org/abs/2509.04118</guid>
<content:encoded><![CDATA[
arXiv:2509.04118v1 Announce Type: cross 
Abstract: Neural video codecs (NVCs), leveraging the power of end-to-end learning, have demonstrated remarkable coding efficiency improvements over traditional video codecs. Recent research has begun to pay attention to the quality structures in NVCs, optimizing them by introducing explicit hierarchical designs. However, less attention has been paid to the reference structure design, which fundamentally should be aligned with the hierarchical quality structure. In addition, there is still significant room for further optimization of the hierarchical quality structure. To address these challenges in NVCs, we propose EHVC, an efficient hierarchical neural video codec featuring three key innovations: (1) a hierarchical multi-reference scheme that draws on traditional video codec design to align reference and quality structures, thereby addressing the reference-quality mismatch; (2) a lookahead strategy to utilize an encoder-side context from future frames to enhance the quality structure; (3) a layer-wise quality scale with random quality training strategy to stabilize quality structures during inference. With these improvements, EHVC achieves significantly superior performance to the state-of-the-art NVCs. Code will be released in: https://github.com/bytedance/NEVC.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation</title>
<link>https://arxiv.org/abs/2509.04126</link>
<guid>https://arxiv.org/abs/2509.04126</guid>
<content:encoded><![CDATA[
arXiv:2509.04126v1 Announce Type: cross 
Abstract: Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality
  and style diversity.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simplicity Lies in the Eye of the Beholder: A Strategic Perspective on Controllers in Reactive Synthesis</title>
<link>https://arxiv.org/abs/2509.04129</link>
<guid>https://arxiv.org/abs/2509.04129</guid>
<content:encoded><![CDATA[
arXiv:2509.04129v1 Announce Type: cross 
Abstract: In the game-theoretic approach to controller synthesis, we model the interaction between a system to be controlled and its environment as a game between these entities, and we seek an appropriate (e.g., winning or optimal) strategy for the system. This strategy then serves as a formal blueprint for a real-world controller. A common belief is that simple (e.g., using limited memory) strategies are better: corresponding controllers are easier to conceive and understand, and cheaper to produce and maintain.
  This invited contribution focuses on the complexity of strategies in a variety of synthesis contexts. We discuss recent results concerning memory and randomness, and take a brief look at what lies beyond our traditional notions of complexity for strategies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Technical Documents Retrieval for RAG</title>
<link>https://arxiv.org/abs/2509.04139</link>
<guid>https://arxiv.org/abs/2509.04139</guid>
<content:encoded><![CDATA[
arXiv:2509.04139v1 Announce Type: cross 
Abstract: In this paper, we introduce Technical-Embeddings, a novel framework designed to optimize semantic retrieval in technical documentation, with applications in both hardware and software development. Our approach addresses the challenges of understanding and retrieving complex technical content by leveraging the capabilities of Large Language Models (LLMs). First, we enhance user queries by generating expanded representations that better capture user intent and improve dataset diversity, thereby enriching the fine-tuning process for embedding models. Second, we apply summary extraction techniques to encode essential contextual information, refining the representation of technical documents. To further enhance retrieval performance, we fine-tune a bi-encoder BERT model using soft prompting, incorporating separate learning parameters for queries and document context to capture fine-grained semantic nuances. We evaluate our approach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that Technical-Embeddings significantly outperforms baseline models in both precision and recall. Our findings highlight the effectiveness of integrating query expansion and contextual summarization to enhance information access and comprehension in technical domains. This work advances the state of Retrieval-Augmented Generation (RAG) systems, offering new avenues for efficient and accurate technical document retrieval in engineering and product development workflows.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAGAL: Tabular Data Generation using Agentic LLM Methods</title>
<link>https://arxiv.org/abs/2509.04152</link>
<guid>https://arxiv.org/abs/2509.04152</guid>
<content:encoded><![CDATA[
arXiv:2509.04152v1 Announce Type: cross 
Abstract: The generation of data is a common approach to improve the performance of machine learning tasks, among which is the training of models for classification. In this paper, we present TAGAL, a collection of methods able to generate synthetic tabular data using an agentic workflow. The methods leverage Large Language Models (LLMs) for an automatic and iterative process that uses feedback to improve the generated data without any further LLM training. The use of LLMs also allows for the addition of external knowledge in the generation process. We evaluate TAGAL across diverse datasets and different aspects of quality for the generated data. We look at the utility of downstream ML models, both by training classifiers on synthetic data only and by combining real and synthetic data. Moreover, we compare the similarities between the real and the generated data. We show that TAGAL is able to perform on par with state-of-the-art approaches that require LLM training and generally outperforms other training-free approaches. These findings highlight the potential of agentic workflow and open new directions for LLM-based data generation methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention as an Adaptive Filter</title>
<link>https://arxiv.org/abs/2509.04154</link>
<guid>https://arxiv.org/abs/2509.04154</guid>
<content:encoded><![CDATA[
arXiv:2509.04154v1 Announce Type: cross 
Abstract: We introduce Adaptive Filter Attention (AFA), a novel attention mechanism that incorporates a learnable dynamics model directly into the computation of attention weights. Rather than comparing queries and keys directly, we model the input sequence as discrete observations of a linear stochastic differential equation (SDE). By imposing a linear dynamics model with simultaneously diagonalizable state matrices and noise covariances, we can make use of a closed-form solution to the differential Lyapunov equation to efficiently propagate pairwise uncertainties through the dynamics. Attention naturally arises as the maximum likelihood solution for this linear SDE, with attention weights corresponding to robust residual-based reweightings of the propagated pairwise precisions. Imposing an additional constraint on the state matrix's eigenvalues leads to a simplified variant with the same computational and memory complexity as standard attention. In the limit of vanishing dynamics and process noise, and using a small-angle approximation, we recover ordinary dot-product attention.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind Turbine Components</title>
<link>https://arxiv.org/abs/2509.04156</link>
<guid>https://arxiv.org/abs/2509.04156</guid>
<content:encoded><![CDATA[
arXiv:2509.04156v1 Announce Type: cross 
Abstract: Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up new opportunities for monitoring wind power plants, including blades, towers, and other critical components. However, reliable defect detection requires high-resolution data and efficient methods to process multispectral imagery. In this research, we aim to enhance defect detection accuracy through the development of an ensemble of YOLO-based deep learning models that integrate both visible and thermal channels. We propose an ensemble approach that integrates a general-purpose YOLOv8 model with a specialized thermal model, using a sophisticated bounding box fusion algorithm to combine their predictions. Our experiments show this approach achieves a mean Average Precision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone YOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that combining multiple YOLO architectures with fused multispectral data provides a more reliable solution, improving the detection of both visual and thermal defects.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crossing the Species Divide: Transfer Learning from Speech to Animal Sounds</title>
<link>https://arxiv.org/abs/2509.04166</link>
<guid>https://arxiv.org/abs/2509.04166</guid>
<content:encoded><![CDATA[
arXiv:2509.04166v1 Announce Type: cross 
Abstract: Self-supervised speech models have demonstrated impressive performance in speech processing, but their effectiveness on non-speech data remains underexplored. We study the transfer learning capabilities of such models on bioacoustic detection and classification tasks. We show that models such as HuBERT, WavLM, and XEUS can generate rich latent representations of animal sounds across taxa. We analyze the models properties with linear probing on time-averaged representations. We then extend the approach to account for the effect of time-wise information with other downstream architectures. Finally, we study the implication of frequency range and noise on performance. Notably, our results are competitive with fine-tuned bioacoustic pre-trained models and show the impact of noise-robust pre-training setups. These findings highlight the potential of speech-based self-supervised learning as an efficient framework for advancing bioacoustic research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</title>
<link>https://arxiv.org/abs/2509.04180</link>
<guid>https://arxiv.org/abs/2509.04180</guid>
<content:encoded><![CDATA[
arXiv:2509.04180v1 Announce Type: cross 
Abstract: AI models rely on annotated data to learn pattern and perform prediction. Annotation is usually a labor-intensive step that require associating labels ranging from a simple classification label to more complex tasks such as object detection, oriented bounding box estimation, and instance segmentation. Traditional tools often require extensive manual input, limiting scalability for large datasets. To address this, we introduce VisioFirm, an open-source web application designed to streamline image labeling through AI-assisted automation. VisioFirm integrates state-of-the-art foundation models into an interface with a filtering pipeline to reduce human-in-the-loop efforts. This hybrid approach employs CLIP combined with pre-trained detectors like Ultralytics models for common classes and zero-shot models such as Grounding DINO for custom labels, generating initial annotations with low-confidence thresholding to maximize recall. Through this framework, when tested on COCO-type of classes, initial prediction have been proven to be mostly correct though the users can refine these via interactive tools supporting bounding boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has on-the-fly segmentation powered by Segment Anything accelerated through WebGPU for browser-side efficiency. The tool supports multiple export formats (YOLO, COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort through benchmarks on diverse datasets, while maintaining high annotation accuracy via clustering of connected CLIP-based disambiguate components and IoU-graph for redundant detection suppression. VisioFirm can be accessed from \href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental Health Counseling Sessions</title>
<link>https://arxiv.org/abs/2509.04183</link>
<guid>https://arxiv.org/abs/2509.04183</guid>
<content:encoded><![CDATA[
arXiv:2509.04183v1 Announce Type: cross 
Abstract: The growing demand for scalable psychological counseling highlights the need for fine-tuning open-source Large Language Models (LLMs) with high-quality, privacy-compliant data, yet such data remains scarce. Here we introduce MAGneT, a novel multi-agent framework for synthetic psychological counseling session generation that decomposes counselor response generation into coordinated sub-tasks handled by specialized LLM agents, each modeling a key psychological technique. Unlike prior single-agent approaches, MAGneT better captures the structure and nuance of real counseling. In addition, we address inconsistencies in prior evaluation protocols by proposing a unified evaluation framework integrating diverse automatic and expert metrics. Furthermore, we expand the expert evaluations from four aspects of counseling in previous works to nine aspects, enabling a more thorough and robust assessment of data quality. Empirical results show that MAGneT significantly outperforms existing methods in quality, diversity, and therapeutic alignment of the generated counseling sessions, improving general counseling skills by 3.2% and CBT-specific skills by 4.3% on average on cognitive therapy rating scale (CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases on average across all aspects. Moreover, fine-tuning an open-source model on MAGneT-generated sessions shows better performance, with improvements of 6.3% on general counseling skills and 7.3% on CBT-specific skills on average on CTRS over those fine-tuned with sessions generated by baseline methods. We also make our code and data public.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding</title>
<link>https://arxiv.org/abs/2509.04243</link>
<guid>https://arxiv.org/abs/2509.04243</guid>
<content:encoded><![CDATA[
arXiv:2509.04243v1 Announce Type: cross 
Abstract: Vision Language Models (VLMs) have recently achieved significant progress in bridging visual perception and linguistic reasoning. Recently, OpenAI o3 model introduced a zoom-in search strategy that effectively elicits active perception capabilities in VLMs, improving downstream task performance. However, enabling VLMs to reason effectively over appropriate image regions remains a core challenge in GUI grounding, particularly under high-resolution inputs and complex multi-element visual interactions. In this work, we propose LASER, a self-evolving framework that progressively endows VLMs with multi-step perception capabilities, enabling precise coordinate prediction. Specifically, our approach integrate Monte Carlo quality estimation with Intersection-over-Union (IoU)-based region quality evaluation to jointly encourage both accuracy and diversity in constructing high-quality preference data. This combination explicitly guides the model to focus on instruction-relevant key regions while adaptively allocating reasoning steps based on task complexity. Comprehensive experiments on the ScreenSpot Pro and ScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating the effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER achieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new state-of-the-art (SoTA) among 7B-scale models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How many patients could we save with LLM priors?</title>
<link>https://arxiv.org/abs/2509.04250</link>
<guid>https://arxiv.org/abs/2509.04250</guid>
<content:encoded><![CDATA[
arXiv:2509.04250v1 Announce Type: cross 
Abstract: Imagine a world where clinical trials need far fewer patients to achieve the same statistical power, thanks to the knowledge encoded in large language models (LLMs). We present a novel framework for hierarchical Bayesian modeling of adverse events in multi-center clinical trials, leveraging LLM-informed prior distributions. Unlike data augmentation approaches that generate synthetic data points, our methodology directly obtains parametric priors from the model. Our approach systematically elicits informative priors for hyperparameters in hierarchical Bayesian models using a pre-trained LLM, enabling the incorporation of external clinical expertise directly into Bayesian safety modeling. Through comprehensive temperature sensitivity analysis and rigorous cross-validation on real-world clinical trial data, we demonstrate that LLM-derived priors consistently improve predictive performance compared to traditional meta-analytical approaches. This methodology paves the way for more efficient and expert-informed clinical trial design, enabling substantial reductions in the number of patients required to achieve robust safety assessment and with the potential to transform drug safety monitoring and regulatory decision making.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Vulnerabilities in Python Packages and Their Detection</title>
<link>https://arxiv.org/abs/2509.04260</link>
<guid>https://arxiv.org/abs/2509.04260</guid>
<content:encoded><![CDATA[
arXiv:2509.04260v1 Announce Type: cross 
Abstract: In the rapidly evolving software development landscape, Python stands out for its simplicity, versatility, and extensive ecosystem. Python packages, as units of organization, reusability, and distribution, have become a pressing concern, highlighted by the considerable number of vulnerability reports. As a scripting language, Python often cooperates with other languages for performance or interoperability. This adds complexity to the vulnerabilities inherent to Python packages, and the effectiveness of current vulnerability detection tools remains underexplored. This paper addresses these gaps by introducing PyVul, the first comprehensive benchmark suite of Python-package vulnerabilities. PyVul includes 1,157 publicly reported, developer-verified vulnerabilities, each linked to its affected packages. To accommodate diverse detection techniques, it provides annotations at both commit and function levels. An LLM-assisted data cleansing method is incorporated to improve label accuracy, achieving 100% commit-level and 94% function-level accuracy, establishing PyVul as the most precise large-scale Python vulnerability benchmark. We further carry out a distribution analysis of PyVul, which demonstrates that vulnerabilities in Python packages involve multiple programming languages and exhibit a wide variety of types. Moreover, our analysis reveals that multi-lingual Python packages are potentially more susceptible to vulnerabilities. Evaluation of state-of-the-art detectors using this benchmark reveals a significant discrepancy between the capabilities of existing tools and the demands of effectively identifying real-world security issues in Python packages. Additionally, we conduct an empirical review of the top-ranked CWEs observed in Python packages, to diagnose the fine-grained limitations of current detection tools and highlight the necessity for future advancements in the field.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Robust Ageing-Aware Control of Li-ion Battery Systems with Data-Driven Formal Verification</title>
<link>https://arxiv.org/abs/2509.04288</link>
<guid>https://arxiv.org/abs/2509.04288</guid>
<content:encoded><![CDATA[
arXiv:2509.04288v1 Announce Type: cross 
Abstract: Rechargeable lithium-ion (Li-ion) batteries are a ubiquitous element of modern technology. In the last decades, the production and design of such batteries and their adjacent embedded charging and safety protocols, denoted by Battery Management Systems (BMS), has taken central stage. A fundamental challenge to be addressed is the trade-off between the speed of charging and the ageing behavior, resulting in the loss of capacity in the battery cell. We rely on a high-fidelity physics-based battery model and propose an approach to data-driven charging and safety protocol design. Following a Counterexample-Guided Inductive Synthesis scheme, we combine Reinforcement Learning (RL) with recent developments in data-driven formal methods to obtain a hybrid control strategy: RL is used to synthesise the individual controllers, and a data-driven abstraction guides their partitioning into a switched structure, depending on the initial output measurements of the battery. The resulting discrete selection among RL-based controllers, coupled with the continuous battery dynamics, realises a hybrid system. When a design meets the desired criteria, the abstraction provides probabilistic guarantees on the closed-loop performance of the cell.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumAIne-Chatbot: Real-Time Personalized Conversational AI via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.04303</link>
<guid>https://arxiv.org/abs/2509.04303</guid>
<content:encoded><![CDATA[
arXiv:2509.04303v1 Announce Type: cross 
Abstract: Current conversational AI systems often provide generic, one-size-fits-all interactions that overlook individual user characteristics and lack adaptive dialogue management. To address this gap, we introduce \textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes responses through a novel user profiling framework. The system is pre-trained on a diverse set of GPT-generated virtual personas to establish a broad prior over user types. During live interactions, an online reinforcement learning agent refines per-user models by combining implicit signals (e.g. typing speed, sentiment, engagement duration) with explicit feedback (e.g., likes and dislikes). This profile dynamically informs the chatbot dialogue policy, enabling real-time adaptation of both content and style. To evaluate the system, we performed controlled experiments with 50 synthetic personas in multiple conversation domains. The results showed consistent improvements in user satisfaction, personalization accuracy, and task achievement when personalization features were enabled. Statistical analysis confirmed significant differences between personalized and nonpersonalized conditions, with large effect sizes across key metrics. These findings highlight the effectiveness of AI-driven user profiling and provide a strong foundation for future real-world validation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models</title>
<link>https://arxiv.org/abs/2509.04304</link>
<guid>https://arxiv.org/abs/2509.04304</guid>
<content:encoded><![CDATA[
arXiv:2509.04304v1 Announce Type: cross 
Abstract: The growing capabilities of Large Language Models (LLMs) show significant potential to enhance healthcare by assisting medical researchers and physicians. However, their reliance on static training data is a major risk when medical recommendations evolve with new research and developments. When LLMs memorize outdated medical knowledge, they can provide harmful advice or fail at clinical reasoning tasks. To investigate this problem, we introduce two novel question-answering (QA) datasets derived from systematic reviews: MedRevQA (16,501 QA pairs covering general biomedical knowledge) and MedChangeQA (a subset of 512 QA pairs where medical consensus has changed over time). Our evaluation of eight prominent LLMs on the datasets reveals consistent reliance on outdated knowledge across all models. We additionally analyze the influence of obsolete pre-training data and training strategies to explain this phenomenon and propose future directions for mitigation, laying the groundwork for developing more current and reliable medical AI systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Entity Representation Learning for Pinterest Ads Ranking</title>
<link>https://arxiv.org/abs/2509.04337</link>
<guid>https://arxiv.org/abs/2509.04337</guid>
<content:encoded><![CDATA[
arXiv:2509.04337v1 Announce Type: cross 
Abstract: In this paper, we introduce a novel framework following an upstream-downstream paradigm to construct user and item (Pin) embeddings from diverse data sources, which are essential for Pinterest to deliver personalized Pins and ads effectively. Our upstream models are trained on extensive data sources featuring varied signals, utilizing complex architectures to capture intricate relationships between users and Pins on Pinterest. To ensure scalability of the upstream models, entity embeddings are learned, and regularly refreshed, rather than real-time computation, allowing for asynchronous interaction between the upstream and downstream models. These embeddings are then integrated as input features in numerous downstream tasks, including ad retrieval and ranking models for CTR and CVR predictions. We demonstrate that our framework achieves notable performance improvements in both offline and online settings across various downstream tasks. This framework has been deployed in Pinterest's production ad ranking systems, resulting in significant gains in online metrics.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Editor to Dense Geometry Estimator</title>
<link>https://arxiv.org/abs/2509.04338</link>
<guid>https://arxiv.org/abs/2509.04338</guid>
<content:encoded><![CDATA[
arXiv:2509.04338v1 Announce Type: cross 
Abstract: Leveraging visual priors from pre-trained text-to-image (T2I) generative models has shown success in dense prediction. However, dense prediction is inherently an image-to-image task, suggesting that image editing models, rather than T2I generative models, may be a more suitable foundation for fine-tuning.
  Motivated by this, we conduct a systematic analysis of the fine-tuning behaviors of both editors and generators for dense geometry estimation. Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by ``refining" their innate features, and ultimately achieve higher performance than their generative counterparts.
  Based on these findings, we introduce \textbf{FE2E}, a framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction. Specifically, to tailor the editor for this deterministic task, we reformulate the editor's original flow matching loss into the ``consistent velocity" training objective. And we use logarithmic quantization to resolve the precision conflict between the editor's native BFloat16 format and the high precision demand of our tasks. Additionally, we leverage the DiT's global attention for a cost-free joint estimation of depth and normals in a single forward pass, enabling their supervisory signals to mutually enhance each other.
  Without scaling up the training data, FE2E achieves impressive performance improvements in zero-shot monocular depth and normal estimation across multiple datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset and outperforms the DepthAnything series, which is trained on 100$\times$ data. The project page can be accessed \href{https://amap-ml.github.io/FE2E/}{here}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds</title>
<link>https://arxiv.org/abs/2509.04345</link>
<guid>https://arxiv.org/abs/2509.04345</guid>
<content:encoded><![CDATA[
arXiv:2509.04345v1 Announce Type: cross 
Abstract: Speech generation systems can produce remarkably realistic vocalisations that are often indistinguishable from human speech, posing significant authenticity challenges. Although numerous deepfake detection methods have been developed, their effectiveness in real-world environments remains unrealiable due to the domain shift between training and test samples arising from diverse human speech and fast evolving speech synthesis systems. This is not adequately addressed by current datasets, which lack real-world application challenges with diverse and up-to-date audios in both real and deep-fake categories. To fill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale, highly diverse deepfake audio dataset for comprehensive evaluation and robust development of generalised models for deepfake audio detection. It consists of over 4,500 hours of synthetic audio generated by 11 recent TTS models and 10 vocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio clips, making it the largest deepfake audio dataset by scale. Through extensive experiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods trained on existing datasets struggle to generalise to novel deepfake audio samples and suffer from high false positive rates on unseen human voice, underscoring the need for a comprehensive dataset; and ii) these methods trained on AUDETER achieve highly generalised detection performance and significantly reduce detection error rate by 44.1% to 51.6%, achieving an error rate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild dataset, paving the way for training generalist deepfake audio detectors. AUDETER is available on GitHub.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation</title>
<link>https://arxiv.org/abs/2509.04357</link>
<guid>https://arxiv.org/abs/2509.04357</guid>
<content:encoded><![CDATA[
arXiv:2509.04357v1 Announce Type: cross 
Abstract: Automatic speech recognition (ASR) systems struggle with domain-specific named entities, especially homophones. Contextual ASR improves recognition but often fails to capture fine-grained phoneme variations due to limited entity diversity. Moreover, prior methods treat entities as independent tokens, leading to incomplete multi-token biasing. To address these issues, we propose Phoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation (PARCO), which integrates phoneme-aware encoding, contrastive entity disambiguation, entity-level supervision, and hierarchical entity filtering. These components enhance phonetic discrimination, ensure complete entity retrieval, and reduce false positives under uncertainty. Experiments show that PARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English DATA2 under 1,000 distractors, significantly outperforming baselines. PARCO also demonstrates robust gains on out-of-domain datasets like THCHS-30 and LibriSpeech.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer</title>
<link>https://arxiv.org/abs/2509.04362</link>
<guid>https://arxiv.org/abs/2509.04362</guid>
<content:encoded><![CDATA[
arXiv:2509.04362v1 Announce Type: cross 
Abstract: The rapid growth of private car ownership has worsened the urban parking predicament, underscoring the need for accurate and effective parking availability prediction to support urban planning and management. To address key limitations in modeling spatio-temporal dependencies and exploiting multi-source data for parking availability prediction, this study proposes a novel approach with SST-iTransformer. The methodology leverages K-means clustering to establish parking cluster zones (PCZs), extracting and integrating traffic demand characteristics from various transportation modes (i.e., metro, bus, online ride-hailing, and taxi) associated with the targeted parking lots. Upgraded on vanilla iTransformer, SST-iTransformer integrates masking-reconstruction-based pretext tasks for self-supervised spatio-temporal representation learning, and features an innovative dual-branch attention mechanism: Series Attention captures long-term temporal dependencies via patching operations, while Channel Attention models cross-variate interactions through inverted dimensions. Extensive experiments using real-world data from Chengdu, China, demonstrate that SST-iTransformer outperforms baseline deep learning models (including Informer, Autoformer, Crossformer, and iTransformer), achieving state-of-the-art performance with the lowest mean squared error (MSE) and competitive mean absolute error (MAE). Comprehensive ablation studies quantitatively reveal the relative importance of different data sources: incorporating ride-hailing data provides the largest performance gains, followed by taxi, whereas fixed-route transit features (bus/metro) contribute marginally. Spatial correlation analysis further confirms that excluding historical data from correlated parking lots within PCZs leads to substantial performance degradation, underscoring the importance of modeling spatial dependencies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</title>
<link>https://arxiv.org/abs/2509.04379</link>
<guid>https://arxiv.org/abs/2509.04379</guid>
<content:encoded><![CDATA[
arXiv:2509.04379v1 Announce Type: cross 
Abstract: Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPA: An Information-Preserving Input Projection Framework for Efficient Foundation Model Adaptation</title>
<link>https://arxiv.org/abs/2509.04398</link>
<guid>https://arxiv.org/abs/2509.04398</guid>
<content:encoded><![CDATA[
arXiv:2509.04398v1 Announce Type: cross 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce adaptation cost by injecting low-rank updates into pretrained weights. However, LoRA's down-projection is randomly initialized and data-agnostic, discarding potentially useful information. Prior analyses show that this projection changes little during training, while the up-projection carries most of the adaptation, making the random input compression a performance bottleneck. We propose IPA, a feature-aware projection framework that explicitly preserves information in the reduced hidden space. In the linear case, we instantiate IPA with algorithms approximating top principal components, enabling efficient projector pretraining with negligible inference overhead. Across language and vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on VTAB-1k, while matching full LoRA performance with roughly half the trainable parameters when the projection is frozen.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in Resume Screening</title>
<link>https://arxiv.org/abs/2509.04404</link>
<guid>https://arxiv.org/abs/2509.04404</guid>
<content:encoded><![CDATA[
arXiv:2509.04404v1 Announce Type: cross 
Abstract: In this study, we conduct a resume-screening experiment (N=528) where people collaborate with simulated AI models exhibiting race-based preferences (bias) to evaluate candidates for 16 high and low status occupations. Simulated AI bias approximates factual and counterfactual estimates of racial bias in real-world AI systems. We investigate people's preferences for White, Black, Hispanic, and Asian candidates (represented through names and affinity groups on quality-controlled resumes) across 1,526 scenarios and measure their unconscious associations between race and status using implicit association tests (IATs), which predict discriminatory hiring decisions but have not been investigated in human-AI collaboration. When making decisions without AI or with AI that exhibits no race-based preferences, people select all candidates at equal rates. However, when interacting with AI favoring a particular group, people also favor those candidates up to 90% of the time, indicating a significant behavioral shift. The likelihood of selecting candidates whose identities do not align with common race-status stereotypes can increase by 13% if people complete an IAT before conducting resume screening. Finally, even if people think AI recommendations are low quality or not important, their decisions are still vulnerable to AI bias under certain circumstances. This work has implications for people's autonomy in AI-HITL scenarios, AI and work, design and evaluation of AI hiring systems, and strategies for mitigating bias in collaborative decision-making tasks. In particular, organizational and regulatory policy should acknowledge the complex nature of AI-HITL decision making when implementing these systems, educating people who use them, and determining which are subject to oversight.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Unified View of Large Language Model Post-Training</title>
<link>https://arxiv.org/abs/2509.04419</link>
<guid>https://arxiv.org/abs/2509.04419</guid>
<content:encoded><![CDATA[
arXiv:2509.04419v1 Announce Type: cross 
Abstract: Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation</title>
<link>https://arxiv.org/abs/2509.04441</link>
<guid>https://arxiv.org/abs/2509.04441</guid>
<content:encoded><![CDATA[
arXiv:2509.04441v1 Announce Type: cross 
Abstract: We introduce perioperation, a paradigm for robotic data collection that sensorizes and records human manipulation while maximizing the transferability of the data to real robots. We implement this paradigm in DEXOP, a passive hand exoskeleton designed to maximize human ability to collect rich sensory (vision + tactile) data for diverse dexterous manipulation tasks in natural environments. DEXOP mechanically connects human fingers to robot fingers, providing users with direct contact feedback (via proprioception) and mirrors the human hand pose to the passive robot hand to maximize the transfer of demonstrated skills to the robot. The force feedback and pose mirroring make task demonstrations more natural for humans compared to teleoperation, increasing both speed and accuracy. We evaluate DEXOP across a range of dexterous, contact-rich tasks, demonstrating its ability to collect high-quality demonstration data at scale. Policies learned with DEXOP data significantly improve task performance per unit time of data collection compared to teleoperation, making DEXOP a powerful tool for advancing robot dexterity. Our project page is at https://dex-op.github.io.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delta Activations: A Representation for Finetuned Large Language Models</title>
<link>https://arxiv.org/abs/2509.04442</link>
<guid>https://arxiv.org/abs/2509.04442</guid>
<content:encoded><![CDATA[
arXiv:2509.04442v1 Announce Type: cross 
Abstract: The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset</title>
<link>https://arxiv.org/abs/2509.04449</link>
<guid>https://arxiv.org/abs/2509.04449</guid>
<content:encoded><![CDATA[
arXiv:2509.04449v1 Announce Type: cross 
Abstract: We present ChronoGraph, a graph-structured multivariate time series forecasting dataset built from real-world production microservices. Each node is a service that emits a multivariate stream of system-level performance metrics, capturing CPU, memory, and network usage patterns, while directed edges encode dependencies between services. The primary task is forecasting future values of these signals at the service level. In addition, ChronoGraph provides expert-annotated incident windows as anomaly labels, enabling evaluation of anomaly detection methods and assessment of forecast robustness during operational disruptions. Compared to existing benchmarks from industrial control systems or traffic and air-quality domains, ChronoGraph uniquely combines (i) multivariate time series, (ii) an explicit, machine-readable dependency graph, and (iii) anomaly labels aligned with real incidents. We report baseline results spanning forecasting models, pretrained time-series foundation models, and standard anomaly detectors. ChronoGraph offers a realistic benchmark for studying structure-aware forecasting and incident-aware evaluation in microservice systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligence Primer</title>
<link>https://arxiv.org/abs/2008.07324</link>
<guid>https://arxiv.org/abs/2008.07324</guid>
<content:encoded><![CDATA[
arXiv:2008.07324v5 Announce Type: replace 
Abstract: Intelligence is a fundamental part of all living things, as well as the foundation for Artificial Intelligence. In this primer we explore the ideas associated with intelligence and, by doing so, understand the implications and constraints and potentially outline the capabilities of future systems. Artificial Intelligence, in the form of Machine Learning, has already had a significant impact on our lives. As an exploration, we journey into different parts of intelligence that appear essential. We hope that people find this helpful in determining the future. Also, during the exploration, we hope to create new thought-provoking questions. Intelligence is not a single weighable quantity but a subject that spans Biology, Physics, Philosophy, Cognitive Science, Neuroscience, Psychology, and Computer Science. The historian Yuval Noah Harari pointed out that engineers and scientists in the future will have to broaden their understandings to include disciplines such as Psychology, Philosophy, and Ethics. Fiction writers have long portrayed engineers and scientists as deficient in these areas. Today, in modern society, the emergence of Artificial Intelligence and legal requirements act as forcing functions to push these broader subjects into the foreground. We start with an introduction to intelligence and move quickly to more profound thoughts and ideas. We call this a Life, the Universe, and Everything primer, after the famous science fiction book by Douglas Adams. Forty-two may be the correct answer, but what are the questions?
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions</title>
<link>https://arxiv.org/abs/2311.17165</link>
<guid>https://arxiv.org/abs/2311.17165</guid>
<content:encoded><![CDATA[
arXiv:2311.17165v4 Announce Type: replace 
Abstract: The concept of rationality is central to the field of artificial intelligence (AI). Whether we are seeking to simulate human reasoning, or trying to achieve bounded optimality, our goal is generally to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in AI, and sets out the open questions in this area. We consider how the understanding of rationality in other fields has influenced its conception within AI, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we examine irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area remains limited. Methods that have up to now been developed for other purposes, namely adversarial scenarios, may be adapted to suit interactions with artificial agents. We further discuss the interplay between human and artificial agents, and the role that rationality plays within this interaction; many questions remain in this area, relating to potentially irrational behaviour of both humans and artificial agents.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents</title>
<link>https://arxiv.org/abs/2406.13923</link>
<guid>https://arxiv.org/abs/2406.13923</guid>
<content:encoded><![CDATA[
arXiv:2406.13923v2 Announce Type: replace 
Abstract: Recent advancements in large multimodal models (LMMs) have leveraged extensive multimodal datasets to enhance capabilities in complex knowledge-driven tasks. However, persistent challenges in perceptual and reasoning errors limit their efficacy, particularly in interpreting intricate visual data and deducing multimodal relationships. To address these issues, we introduce PIN (Paired and INterleaved multimodal documents), a novel data format designed to foster a deeper integration of visual and textual knowledge. The PIN format uniquely combines semantically rich Markdown files, which preserve fine-grained textual structures, with holistic overall images that capture the complete document layout. Following this format, we construct and release two large-scale, open-source datasets: PIN-200M (~200 million documents) and PIN-14M (~14 million), compiled from diverse web and scientific sources in both English and Chinese. To maximize usability, we provide detailed statistical analyses and equip the datasets with quality signals, enabling researchers to easily filter and select data for specific tasks. Our work provides the community with a versatile data format and substantial resources, offering a foundation for new research in pre-training strategies and the development of more powerful knowledge-intensive LMMs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable Belief Model on Quantum Circuits</title>
<link>https://arxiv.org/abs/2410.08949</link>
<guid>https://arxiv.org/abs/2410.08949</guid>
<content:encoded><![CDATA[
arXiv:2410.08949v3 Announce Type: replace 
Abstract: The transferable belief model, as a semantic interpretation of Dempster-Shafer theory, enables agents to perform reasoning and decision making in imprecise and incomplete environments. The model offers distinct semantics for handling unreliable testimonies, allowing for a more reasonable and general process of belief transfer compared to the Bayesian approach. However, because both the belief masses and the structure of focal sets must be considered when updating belief functions-leading to extra computational complexity during reasoning-the transferable belief model has gradually lost favor among researchers in recent developments. In this paper, we implement the transferable belief model on quantum circuits and demonstrate that belief functions offer a more concise and effective alternative to Bayesian approaches within the quantum computing framework. Furthermore, leveraging the unique characteristics of quantum computing, we propose several novel belief transfer approaches. More broadly, this paper introduces a new perspective on basic information representation for quantum AI models, suggesting that belief functions are more suitable than Bayesian approach for handling uncertainty on quantum circuits.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WASP: A Weight-Space Approach to Detecting Learned Spuriousness</title>
<link>https://arxiv.org/abs/2410.18970</link>
<guid>https://arxiv.org/abs/2410.18970</guid>
<content:encoded><![CDATA[
arXiv:2410.18970v4 Announce Type: replace 
Abstract: It is of crucial importance to train machine learning models such that they clearly understand what defines each class in a given task. Though there is a sum of works dedicated to identifying the spurious correlations featured by a dataset that may impact the model's understanding of the classes, all current approaches rely solely on data or error analysis. That is, they cannot point out spurious correlations learned by the model that are not already pointed out by the counterexamples featured in the validation or training sets. We propose a method that transcends this limitation, switching the focus from analyzing a model's predictions to analyzing the model's weights, the mechanism behind the making of the decisions, which proves to be more insightful. Our proposed Weight-space Approach to detecting Spuriousness (WASP) relies on analyzing the weights of foundation models as they drift towards capturing various (spurious) correlations while being fine-tuned on a given dataset. We demonstrate that different from previous works, our method (i) can expose spurious correlations featured by a dataset even when they are not exposed by training or validation counterexamples, (ii) it works for multiple modalities such as image and text, and (iii) it can uncover previously untapped spurious correlations learned by ImageNet-1k classifiers.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing FKG.in: automating Indian food composition analysis</title>
<link>https://arxiv.org/abs/2412.05248</link>
<guid>https://arxiv.org/abs/2412.05248</guid>
<content:encoded><![CDATA[
arXiv:2412.05248v3 Announce Type: replace 
Abstract: This paper presents a novel approach to compute food composition data for Indian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The primary focus is to provide a broad overview of an automated food composition analysis workflow and describe its core functionalities: nutrition data aggregation, food composition analysis, and LLM-augmented information resolution. This workflow aims to complement FKG[.]in and iteratively supplement food composition data from verified knowledge bases. Additionally, this paper highlights the challenges of representing Indian food and accessing food composition data digitally. It also reviews three key sources of food composition data: the Indian Food Composition Tables, the Indian Nutrient Databank, and the Nutritionix API. Furthermore, it briefly outlines how users can interact with the workflow to obtain diet-based health recommendations and detailed food composition information for numerous recipes. We then explore the complex challenges of analyzing Indian recipe information across dimensions such as structure, multilingualism, and uncertainty as well as present our ongoing work on LLM-based solutions to address these issues. The methods proposed in this workshop paper for AI-driven knowledge curation and information resolution are application-agnostic, generalizable, and replicable for any domain.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Science Across Languages: Assessing LLM Multilingual Translation of Scientific Papers</title>
<link>https://arxiv.org/abs/2502.17882</link>
<guid>https://arxiv.org/abs/2502.17882</guid>
<content:encoded><![CDATA[
arXiv:2502.17882v2 Announce Type: replace 
Abstract: Scientific research is inherently global. However, the vast majority of academic journals are published exclusively in English, creating barriers for non-native-English-speaking researchers. In this study, we leverage large language models (LLMs) to translate published scientific articles while preserving their native JATS XML formatting, thereby developing a practical, automated approach for implementation by academic journals. Using our approach, we translate articles across multiple scientific disciplines into 28 languages. To evaluate translation accuracy, we introduce a novel question-and-answer (QA) benchmarking method, in which an LLM generates comprehension-based questions from the original text and then answers them based on the translated text. Our benchmark results show an average performance of 95.9%, showing that the key scientific details are accurately conveyed. In a user study, we translate the scientific papers of 15 researchers into their native languages, finding that the authors consistently found the translations to accurately capture the original information in their articles. Interestingly, a third of the authors found many technical terms "overtranslated," expressing a preference to keep terminology more familiar in English untranslated. Finally, we demonstrate how in-context learning techniques can be used to align translations with domain-specific preferences such as mitigating overtranslation, highlighting the adaptability and utility of LLM-driven scientific translation. The code and translated articles are available at https://hankleid.github.io/ProjectMundo.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Basis of LLM's Decision Making in Social Simulation</title>
<link>https://arxiv.org/abs/2504.11671</link>
<guid>https://arxiv.org/abs/2504.11671</guid>
<content:encoded><![CDATA[
arXiv:2504.11671v2 Announce Type: replace 
Abstract: Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game -- a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMN-Guided Prompting: A Framework for Controlling LLM Behavior</title>
<link>https://arxiv.org/abs/2505.11701</link>
<guid>https://arxiv.org/abs/2505.11701</guid>
<content:encoded><![CDATA[
arXiv:2505.11701v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown considerable potential in automating decision logic within knowledge-intensive processes. However, their effectiveness largely depends on the strategy and quality of prompting. Since decision logic is typically embedded in prompts, it becomes challenging for end users to modify or refine it. Decision Model and Notation (DMN) offers a standardized graphical approach for defining decision logic in a structured, user-friendly manner. This paper introduces a DMN-guided prompting framework that breaks down complex decision logic into smaller, manageable components, guiding LLMs through structured decision pathways. We implemented the framework in a graduate-level course where students submitted assignments. The assignments and DMN models representing feedback instructions served as inputs to our framework. The instructor evaluated the generated feedback and labeled it for performance assessment. Our approach demonstrated promising results, outperforming chain-of-thought (CoT) prompting in our case study. Students also responded positively to the generated feedback, reporting high levels of perceived usefulness in a survey based on the Technology Acceptance Model.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback</title>
<link>https://arxiv.org/abs/2506.03315</link>
<guid>https://arxiv.org/abs/2506.03315</guid>
<content:encoded><![CDATA[
arXiv:2506.03315v2 Announce Type: replace 
Abstract: We study how linear orders can be employed to realise choice functions for which the set of potential choices is restricted, i.e., the possible choice is not possible among the full powerset of all alternatives. In such restricted settings, constructing a choice function via a relation on the alternatives is not always possible. However, we show that one can always construct a choice function via a linear order on sets of alternatives, even when a fallback value is encoded as the minimal element in the linear order. The axiomatics of such choice functions are presented for the general case and the case of union-closed input restrictions. Restricted choice structures have applications in knowledge representation and reasoning, and here we discuss their applications for theory change and abstract argumentation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CP-Bench: Evaluating Large Language Models for Constraint Modelling</title>
<link>https://arxiv.org/abs/2506.06052</link>
<guid>https://arxiv.org/abs/2506.06052</guid>
<content:encoded><![CDATA[
arXiv:2506.06052v2 Announce Type: replace 
Abstract: Constraint Programming (CP) is widely used to solve combinatorial problems, but its core process, namely constraint modelling, requires significant expertise and is considered to be a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) to transform combinatorial problem descriptions into executable constraint models. However, the existing evaluation datasets for constraint modelling are often limited to small, homogeneous, or domain-specific instances, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing CP-Bench, a novel benchmark that includes a diverse set of well-known combinatorial problems sourced from the CP community, structured explicitly for evaluating LLM-driven CP modelling. With this dataset, and given the variety of constraint modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax. Notably, the results show higher performance when modelling with a high-level Python-based framework. Additionally, we systematically evaluate the use of prompt-based and inference-time compute methods across different LLMs, which further increase accuracy, reaching up to 70% on this highly challenging benchmark.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation</title>
<link>https://arxiv.org/abs/2508.00401</link>
<guid>https://arxiv.org/abs/2508.00401</guid>
<content:encoded><![CDATA[
arXiv:2508.00401v2 Announce Type: replace 
Abstract: Theory of Mind (ToM) -- the ability to understand that others can have differing knowledge and goals -- enables agents to reason about others' beliefs while planning their own actions. We present a novel approach to multi-agent cooperation by implementing ToM within active inference. Unlike previous active inference approaches to multi-agent cooperation, our method neither relies on task-specific shared generative models nor requires explicit communication. In our framework, ToM-equipped agents maintain distinct representations of their own and others' beliefs and goals. ToM agents then use an extended and adapted version of the sophisticated inference tree-based planning algorithm to systematically explore joint policy spaces through recursive reasoning. We evaluate our approach through collision avoidance and foraging simulations. Results suggest that ToM agents cooperate better compared to non-ToM counterparts by being able to avoid collisions and reduce redundant efforts. Crucially, ToM agents accomplish this by inferring others' beliefs solely from observable behaviour and considering them when planning their own actions. Our approach shows potential for generalisable and scalable multi-agent systems while providing computational insights into ToM mechanisms.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning</title>
<link>https://arxiv.org/abs/2508.01700</link>
<guid>https://arxiv.org/abs/2508.01700</guid>
<content:encoded><![CDATA[
arXiv:2508.01700v2 Announce Type: replace 
Abstract: Although data visualization is powerful for revealing patterns and communicating insights, creating effective visualizations requires familiarity with authoring tools and often disrupts the analysis flow. While large language models show promise for automatically converting analysis intent into visualizations, existing methods function as black boxes without transparent reasoning processes, which prevents users from understanding design rationales and refining suboptimal outputs. To bridge this gap, we propose integrating Chain-of-Thought (CoT) reasoning into the Natural Language to Visualization (NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for NL2VIS and develop an automatic pipeline to equip existing datasets with structured reasoning steps. Second, we introduce nvBench-CoT, a specialized dataset capturing detailed step-by-step reasoning from ambiguous natural language descriptions to finalized visualizations, which enables state-of-the-art performance when used for model fine-tuning. Third, we develop DeepVIS, an interactive visual interface that tightly integrates with the CoT reasoning process, allowing users to inspect reasoning steps, identify errors, and make targeted adjustments to improve visualization outcomes. Quantitative benchmark evaluations, two use cases, and a user study collectively demonstrate that our CoT framework effectively enhances NL2VIS quality while providing insightful reasoning steps to users.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending FKG.in: Towards a Food Claim Traceability Network</title>
<link>https://arxiv.org/abs/2508.16117</link>
<guid>https://arxiv.org/abs/2508.16117</guid>
<content:encoded><![CDATA[
arXiv:2508.16117v2 Announce Type: replace 
Abstract: The global food landscape is rife with scientific, cultural, and commercial claims about what foods are, what they do, what they should not do, or should not do. These range from rigorously studied health benefits (probiotics improve gut health) and misrepresentations (soaked almonds make one smarter) to vague promises (superfoods boost immunity) and culturally rooted beliefs (cold foods cause coughs). Despite their widespread influence, the infrastructure for tracing, verifying, and contextualizing these claims remains fragmented and underdeveloped. In this paper, we propose a Food Claim-Traceability Network (FCN) as an extension of FKG[.]in, a knowledge graph of Indian food that we have been incrementally building. We also present the ontology design and the semi-automated knowledge curation workflow that we used to develop a proof of concept of FKG[.]in-FCN using Reddit data and Large Language Models. FCN integrates curated data inputs, structured schemas, and provenance-aware pipelines for food-related claim extraction and validation. While directly linked to the Indian food knowledge graph as an application, our methodology remains application-agnostic and adaptable to other geographic, culinary, or regulatory settings. By modeling food claims and their traceability in a structured, verifiable, and explainable way, we aim to contribute to more transparent and accountable food knowledge ecosystems, supporting researchers, policymakers, and most importantly, everyday consumers in navigating a world saturated with dietary assertions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomation, Not Automation: Activities and Needs of European Fact-checkers as a Basis for Designing Human-Centered AI Systems</title>
<link>https://arxiv.org/abs/2211.12143</link>
<guid>https://arxiv.org/abs/2211.12143</guid>
<content:encoded><![CDATA[
arXiv:2211.12143v3 Announce Type: replace-cross 
Abstract: To mitigate the negative effects of false information more effectively, the development of Artificial Intelligence (AI) systems to assist fact-checkers is needed. Nevertheless, the lack of focus on the needs of these stakeholders results in their limited acceptance and skepticism toward automating the whole fact-checking process. In this study, we conducted semi-structured in-depth interviews with Central European fact-checkers. Their activities and problems were analyzed using iterative content analysis. The most significant problems were validated with a survey of European fact-checkers, in which we collected 24 responses from 20 countries, i.e., 62% of active European signatories of the International Fact-Checking Network (IFCN). Our contributions include an in-depth examination of the variability of fact-checking work in non-English-speaking regions, which still remained largely uncovered. By aligning them with the knowledge from prior studies, we created conceptual models that help to understand the fact-checking processes. In addition, we mapped our findings on the fact-checkers' activities and needs to the relevant tasks for AI research, while providing a discussion on three AI tasks that were not covered by previous similar studies. The new opportunities identified for AI researchers and developers have implications for the focus of AI research in this domain.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style Transfer to Calvin and Hobbes comics using Stable Diffusion</title>
<link>https://arxiv.org/abs/2312.03993</link>
<guid>https://arxiv.org/abs/2312.03993</guid>
<content:encoded><![CDATA[
arXiv:2312.03993v2 Announce Type: replace-cross 
Abstract: This project report summarizes our journey to perform stable diffusion fine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to convert any given input image into the comic style of Calvin and Hobbes, essentially performing style transfer. We train stable-diffusion-v1.5 using Low Rank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The diffusion itself is handled by a Variational Autoencoder (VAE), which is a U-net. Our results were visually appealing for the amount of training time and the quality of input data that went into training.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion on language model encodings for protein sequence generation</title>
<link>https://arxiv.org/abs/2403.03726</link>
<guid>https://arxiv.org/abs/2403.03726</guid>
<content:encoded><![CDATA[
arXiv:2403.03726v3 Announce Type: replace-cross 
Abstract: Protein sequence design has seen significant advances through discrete diffusion and autoregressive approaches, yet the potential of continuous diffusion remains underexplored. Here, we present DiMA, a latent diffusion framework that operates on protein language model representations. Through systematic exploration of architectural choices and diffusion components, we develop a robust methodology that generalizes across multiple protein encoders ranging from 8M to 3B parameters. We demonstrate that our framework achieves consistently high performance across sequence-only (ESM-2, ESMc), dual-decodable (CHEAP), and multimodal (SaProt) representations using the same architecture and training approach. We extensively evaluate existing methods alongside DiMA using multiple metrics across two protein modalities, covering quality, diversity, novelty, and distribution matching of generated proteins. DiMA consistently produces novel, high-quality and diverse protein sequences and achieves strong results compared to baselines such as autoregressive, discrete diffusion and flow matching language models. The model demonstrates versatile functionality, supporting conditional generation tasks including protein family-generation, motif scaffolding and infilling, and fold-specific sequence design. This work provides a universal continuous diffusion framework for protein sequence generation, offering both architectural insights and practical applicability across various protein design scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming</title>
<link>https://arxiv.org/abs/2405.08965</link>
<guid>https://arxiv.org/abs/2405.08965</guid>
<content:encoded><![CDATA[
arXiv:2405.08965v5 Announce Type: replace-cross 
Abstract: Software development is shifting from traditional programming to AI-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity. This paper presents Meaning-Typed Programming (MTP), a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the (1) by operator for seamless LLM invocation, (2) MT-IR, a meaning-based intermediate representation for semantic extraction, and (3) MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a programming language that supersets Python, and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. MTP significantly reduces development complexity, lines of code modifications needed, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches. Our user study shows that developers using MTP completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks. Moreover, MTP demonstrates resilience even when up to 50% of naming conventions are degraded, demonstrating robustness to suboptimal code. MTP is developed as part of the Jaseci open-source project, and is available under the module byLLM.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unisolver: PDE-Conditional Transformers Towards Universal Neural PDE Solvers</title>
<link>https://arxiv.org/abs/2405.17527</link>
<guid>https://arxiv.org/abs/2405.17527</guid>
<content:encoded><![CDATA[
arXiv:2405.17527v5 Announce Type: replace-cross 
Abstract: Deep models have recently emerged as promising tools to solve partial differential equations (PDEs), known as neural PDE solvers. While neural solvers trained from either simulation data or physics-informed loss can solve PDEs reasonably well, they are mainly restricted to a few instances of PDEs, e.g. a certain equation with a limited set of coefficients. This limits their generalization to diverse PDEs, preventing them from being practical surrogate models of numerical solvers. In this paper, we present Unisolver, a novel Transformer model trained on diverse data and conditioned on diverse PDEs, aiming towards a universal neural PDE solver capable of solving a wide scope of PDEs. Instead of purely scaling up data and parameters, Unisolver stems from the theoretical analysis of the PDE-solving process. Inspired by the mathematical structure of PDEs that a PDE solution is fundamentally governed by a series of PDE components such as equation symbols and boundary conditions, we define a complete set of PDE components and flexibly embed them as domain-wise and point-wise deep conditions for Transformer PDE solvers. Integrating physical insights with recent Transformer advances, Unisolver achieves consistent state-of-the-art on three challenging large-scale benchmarks, showing impressive performance and generalizability. Code is available at https://github.com/thuml/Unisolver.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference</title>
<link>https://arxiv.org/abs/2407.15161</link>
<guid>https://arxiv.org/abs/2407.15161</guid>
<content:encoded><![CDATA[
arXiv:2407.15161v4 Announce Type: replace-cross 
Abstract: Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from partial observations remains a critical challenge in robot learning. Prior generative methods struggle to model the intricate grasp distribution of dexterous hands and often fail to reason about shape uncertainty inherent in partial point clouds, leading to unreliable or overly conservative grasps. We propose FFHFlow, a flow-based variational framework that generates diverse, robust multi-finger grasps while explicitly quantifying perceptual uncertainty in the partial point clouds. Our approach leverages a normalizing flow-based deep latent variable model to learn a hierarchical grasp manifold, overcoming the mode collapse and rigid prior limitations of conditional Variational Autoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of flows, FFHFlow introspects shape uncertainty in partial observations and identifies novel object structures, enabling risk-aware grasp synthesis. To further enhance reliability, we integrate a discriminative grasp evaluator with the flow likelihoods, formulating an uncertainty-aware ranking strategy that prioritizes grasps robust to shape ambiguity. Extensive experiments in simulation and real-world setups demonstrate that FFHFlow outperforms state-of-the-art baselines (including diffusion models) in grasp diversity and success rate, while achieving run-time efficient sampling. We also showcase its practical value in cluttered and confined environments, where diversity-driven sampling excels by mitigating collisions (Project Page: https://sites.google.com/view/ffhflow/home/).
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long Input Sequence Network for Long Time Series Forecasting</title>
<link>https://arxiv.org/abs/2407.15869</link>
<guid>https://arxiv.org/abs/2407.15869</guid>
<content:encoded><![CDATA[
arXiv:2407.15869v2 Announce Type: replace-cross 
Abstract: Short fixed-length inputs are the main bottleneck of deep learning methods in long time-series forecasting tasks. Prolonging input length causes overfitting, rapidly deteriorating accuracy. Our research indicates that the overfitting is a combination reaction of the multi-scale pattern coupling in time series and the fixed focusing scale of current models. First, we find that the patterns exhibited by a time series across various scales are reflective of its multi-periodic nature, where each scale corresponds to specific period length. Second, We find that the token size predominantly dictates model behavior, as it determines the scale at which the model focuses and the context size it can accommodate. Our idea is to decouple the multi-scale temporal patterns of time series and to model each pattern with its corresponding period length as token size. We introduced a novel series-decomposition module(MPSD), and a Multi-Token Pattern Recognition neural network(MTPR), enabling the model to handle \textit{inputs up to $10\times$ longer}. Sufficient context enhances performance(\textit{38% maximum precision improvement}), and the decoupling approach offers \textit{Low complexity($0.22\times$ cost)} and \textit{high interpretability}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPETIII: The Tracer Frontier. What Frontier?</title>
<link>https://arxiv.org/abs/2410.02807</link>
<guid>https://arxiv.org/abs/2410.02807</guid>
<content:encoded><![CDATA[
arXiv:2410.02807v2 Announce Type: replace-cross 
Abstract: For the last three years, the AutoPET competition gathered the medical imaging community around a hot topic: lesion segmentation on Positron Emitting Tomography (PET) scans. Each year a different aspect of the problem is presented; in 2024 the multiplicity of existing and used tracers was at the core of the challenge. Specifically, this year's edition aims to develop a fully automatic algorithm capable of performing lesion segmentation on a PET/CT scan, without knowing the tracer, which can either be a FDG or PSMA-based tracer. In this paper we describe how we used the nnUNetv2 framework to train two sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion segmentation as well as a MIP-CNN to choose which set of models to use for segmentation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from 10 Demos: Generalisable and Sample-Efficient Policy Learning with Oriented Affordance Frames</title>
<link>https://arxiv.org/abs/2410.12124</link>
<guid>https://arxiv.org/abs/2410.12124</guid>
<content:encoded><![CDATA[
arXiv:2410.12124v2 Announce Type: replace-cross 
Abstract: Imitation learning has unlocked the potential for robots to exhibit highly dexterous behaviours. However, it still struggles with long-horizon, multi-object tasks due to poor sample efficiency and limited generalisation. Existing methods require a substantial number of demonstrations to cover possible task variations, making them costly and often impractical for real-world deployment. We address this challenge by introducing oriented affordance frames, a structured representation for state and action spaces that improves spatial and intra-category generalisation and enables policies to be learned efficiently from only 10 demonstrations. More importantly, we show how this abstraction allows for compositional generalisation of independently trained sub-policies to solve long-horizon, multi-object tasks. To seamlessly transition between sub-policies, we introduce the notion of self-progress prediction, which we directly derive from the duration of the training demonstrations. We validate our method across three real-world tasks, each requiring multi-step, multi-object interactions. Despite the small dataset, our policies generalise robustly to unseen object appearances, geometries, and spatial arrangements, achieving high success rates without reliance on exhaustive training data. Video demonstration can be found on our project page: https://affordance-policy.github.io/.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust training of implicit generative models for multivariate and heavy-tailed distributions with an invariant statistical loss</title>
<link>https://arxiv.org/abs/2410.22381</link>
<guid>https://arxiv.org/abs/2410.22381</guid>
<content:encoded><![CDATA[
arXiv:2410.22381v2 Announce Type: replace-cross 
Abstract: Traditional implicit generative models are capable of learning highly complex data distributions. However, their training involves distinguishing real data from synthetically generated data using adversarial discriminators, which can lead to unstable training dynamics and mode dropping issues. In this work, we build on the \textit{invariant statistical loss} (ISL) method introduced in \cite{de2024training}, and extend it to handle heavy-tailed and multivariate data distributions.
  The data generated by many real-world phenomena can only be properly characterised using heavy-tailed probability distributions, and traditional implicit methods struggle to effectively capture their asymptotic behavior. To address this problem, we introduce a generator trained with ISL, that uses input noise from a generalised Pareto distribution (GPD). We refer to this generative scheme as Pareto-ISL for conciseness. Our experiments demonstrate that Pareto-ISL accurately models the tails of the distributions while still effectively capturing their central characteristics.
  The original ISL function was conceived for 1D data sets. When the actual data is $n$-dimensional, a straightforward extension of the method was obtained by targeting the $n$ marginal distributions of the data. This approach is computationally infeasible and ineffective in high-dimensional spaces. To overcome this, we extend the 1D approach using random projections and define a new loss function suited for multivariate data, keeping problems tractable by adjusting the number of projections. We assess its performance in multidimensional generative modeling and explore its potential as a pretraining technique for generative adversarial networks (GANs) to prevent mode collapse, reporting promising results and highlighting its robustness across various hyperparameter settings.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Calibration Error in Neural Networks Through Evidence-Based Theory</title>
<link>https://arxiv.org/abs/2411.00265</link>
<guid>https://arxiv.org/abs/2411.00265</guid>
<content:encoded><![CDATA[
arXiv:2411.00265v3 Announce Type: replace-cross 
Abstract: Trustworthiness in neural networks is crucial for their deployment in critical applications, where reliability, confidence, and uncertainty play pivotal roles in decision-making. Traditional performance metrics such as accuracy and precision fail to capture these aspects, particularly in cases where models exhibit overconfidence. To address these limitations, this paper introduces a novel framework for quantifying the trustworthiness of neural networks by incorporating subjective logic into the evaluation of Expected Calibration Error (ECE). This method provides a comprehensive measure of trust, disbelief, and uncertainty by clustering predicted probabilities and fusing opinions using appropriate fusion operators. We demonstrate the effectiveness of this approach through experiments on MNIST and CIFAR-10 datasets, where post-calibration results indicate improved trustworthiness. The proposed framework offers a more interpretable and nuanced assessment of AI models, with potential applications in sensitive domains such as healthcare and autonomous systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolb-Based Experiential Learning for Generalist Agents with Human-Level Kaggle Data Science Performance</title>
<link>https://arxiv.org/abs/2411.03562</link>
<guid>https://arxiv.org/abs/2411.03562</guid>
<content:encoded><![CDATA[
arXiv:2411.03562v2 Announce Type: replace-cross 
Abstract: Human expertise emerges through iterative cycles of interaction, reflection, and internal model updating, which are central to cognitive theories such as Kolb's experiential learning and Vygotsky's zone of proximal development. In contrast, current AI systems, particularly LLM agents, rely on static pre-training or rigid workflows, lacking mechanisms for continual adaptation. Recent studies identified early cognitive traits in LLM agents (reflection, revision, and self-correction) suggesting foundational elements of human-like experiential learning. Thus the key question: Can we design LLM agents capable of structured, cognitively grounded learning similar to human processes? In response, we propose a computational framework of Kolb's learning cycle with Vygotsky's ZPD for autonomous agents. Our architecture separates extrinsic (environment interaction) and intrinsic (internal reflection/abstraction) functions, enabling cognitively grounded scaffolded learning, where the agent initially learns within structured environments, followed by open-ended generalisation. This approach empowers agents to master complex tasks ; domains that traditional fine-tuning or simple reflective methods could not tackle effectively. Its potential is powerfully demonstrated via direct comparison with humans in real-world Kaggle data science competitions. Learning fully automated data science code generation across 81 tasks, our system, Agent K, demonstrated the ability to perform the entire workflow autonomously, achieving an Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2% among 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals level performance - including 4 gold and 4 silver on prize-awarding competitions - Agent K is the 1st AI system to successfully integrate Kolb- and Vygotsky-inspired human cognitive learning, marking a major step toward generalist AI.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACING: Actor-Critic for Instruction Learning in Black-Box LLMs</title>
<link>https://arxiv.org/abs/2411.12736</link>
<guid>https://arxiv.org/abs/2411.12736</guid>
<content:encoded><![CDATA[
arXiv:2411.12736v2 Announce Type: replace-cross 
Abstract: The effectiveness of Large Language Models (LLMs) in solving tasks depends significantly on the quality of their instructions, which often require substantial human effort to craft. This underscores the need for automated instruction optimization. However, optimizing instructions is particularly challenging when working with black-box LLMs, where model parameters and gradients are inaccessible. We introduce ACING, an actor-critic reinforcement learning framework that formulates instruction optimization as a stateless, continuous-action problem, enabling exploration of infinite instruction spaces using only black-box feedback. ACING automatically discovers prompts that outperform human-written prompts in 76% of instruction-induction tasks, with gains of up to 33 points and a 10-point median improvement over the best automatic baseline in 33 tasks spanning instruction-induction, summarization, and chain-of-thought reasoning. Extensive ablations highlight its robustness and efficiency. An implementation of ACING is available at https://github.com/salmakh1/ACING.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defending LVLMs Against Vision Attacks through Partial-Perception Supervision</title>
<link>https://arxiv.org/abs/2412.12722</link>
<guid>https://arxiv.org/abs/2412.12722</guid>
<content:encoded><![CDATA[
arXiv:2412.12722v2 Announce Type: replace-cross 
Abstract: Recent studies have raised significant concerns regarding the vulnerability of Large Vision Language Models (LVLMs) to maliciously injected or perturbed input images, which can mislead their responses. Existing defense methods show that such vision attacks are sensitive to image modifications especially cropping, using majority voting across responses of modified images as corrected responses. However, these modifications often result in partial images and distort the semantics, which reduces response quality on clean images after voting. Instead of directly using responses from partial images for voting, we investigate using them to supervise the LVLM's responses to the original images. We propose a black-box, training-free method called DPS (Defense through Partial-Perception Supervision). In this approach, the model is prompted using the responses generated by a model that perceives only a partial image. With DPS, the model can adjust its response based on partial image understanding when under attack, while confidently maintaining its original response for clean input. Our findings show that the weak model can supervise the strong model: when faced with an attacked input, the strong model becomes less confident and adjusts its response based on the weak model's partial understanding, effectively defending against the attack. With clean input, it confidently maintains its original response. Empirical experiments show our method outperforms the baseline, cutting the average attack success rate by 76.3% across six datasets on three popular models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Context Bottleneck on Long Time Series Forecasting</title>
<link>https://arxiv.org/abs/2412.16572</link>
<guid>https://arxiv.org/abs/2412.16572</guid>
<content:encoded><![CDATA[
arXiv:2412.16572v2 Announce Type: replace-cross 
Abstract: Long-term time-series forecasting is essential for planning and decision-making in economics, energy, and transportation, where long foresight is required. To obtain such long foresight, models must be both efficient and effective in processing long sequence. Recent advancements have enhanced the efficiency of these models; however, the challenge of effectively leveraging longer sequences persists. This is primarily due to the tendency of these models to overfit when presented with extended inputs, necessitating the use of shorter input lengths to maintain tolerable error margins. In this work, we investigate the multiscale modeling method and propose the Logsparse Decomposable Multiscaling (LDM) framework for the efficient and effective processing of long sequences. We demonstrate that by decoupling patterns at different scales in time series, we can enhance predictability by reducing non-stationarity, improve efficiency through a compact long input representation, and simplify the architecture by providing clear task assignments. Experimental results demonstrate that LDM not only outperforms all baselines in long-term forecasting benchmarks, but also reducing both training time and memory costs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models</title>
<link>https://arxiv.org/abs/2501.13958</link>
<guid>https://arxiv.org/abs/2501.13958</guid>
<content:encoded><![CDATA[
arXiv:2501.13958v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks, yet their application to specialized domains remains challenging due to the need for deep expertise. Retrieval-Augmented generation (RAG) has emerged as a promising solution to customize LLMs for professional fields by seamlessly integrating external knowledge bases, enabling real-time access to domain-specific expertise during inference. Despite its potential, traditional RAG systems, based on flat text retrieval, face three critical challenges: (i) complex query understanding in professional contexts, (ii) difficulties in knowledge integration across distributed sources, and (iii) system efficiency bottlenecks at scale. This survey presents a systematic analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new paradigm that revolutionizes domain-specific LLM applications. GraphRAG addresses traditional RAG limitations through three key innovations: (i) graph-structured knowledge representation that explicitly captures entity relationships and domain hierarchies, (ii) efficient graph-based retrieval techniques that enable context-preserving knowledge retrieval with multihop reasoning ability, and (iii) structure-aware knowledge integration algorithms that leverage retrieved knowledge for accurate and logical coherent generation of LLMs. In this survey, we systematically analyze the technical foundations of GraphRAG and examine current implementations across various professional domains, identifying key technical challenges and promising research directions. All the related resources of GraphRAG, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphRAG.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Histogram-based Outlier Score (EHBOS)</title>
<link>https://arxiv.org/abs/2502.05719</link>
<guid>https://arxiv.org/abs/2502.05719</guid>
<content:encoded><![CDATA[
arXiv:2502.05719v3 Announce Type: replace-cross 
Abstract: Histogram-Based Outlier Score (HBOS) is a widely used outlier or anomaly detection method known for its computational efficiency and simplicity. However, its assumption of feature independence limits its ability to detect anomalies in datasets where interactions between features are critical. In this paper, we propose the Extended Histogram-Based Outlier Score (EHBOS), which enhances HBOS by incorporating two-dimensional histograms to capture dependencies between feature pairs. This extension allows EHBOS to identify contextual and dependency-driven anomalies that HBOS fails to detect. We evaluate EHBOS on 17 benchmark datasets, demonstrating its effectiveness and robustness across diverse anomaly detection scenarios. EHBOS outperforms HBOS on several datasets, particularly those where feature interactions are critical in defining the anomaly structure, achieving notable improvements in ROC AUC. These results highlight that EHBOS can be a valuable extension to HBOS, with the ability to model complex feature dependencies. EHBOS offers a powerful new tool for anomaly detection, particularly in datasets where contextual or relational anomalies play a significant role.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is an Ultra Large Natural Image-Based Foundation Model Superior to a Retina-Specific Model for Detecting Ocular and Systemic Diseases?</title>
<link>https://arxiv.org/abs/2502.06289</link>
<guid>https://arxiv.org/abs/2502.06289</guid>
<content:encoded><![CDATA[
arXiv:2502.06289v2 Announce Type: replace-cross 
Abstract: The advent of foundation models (FMs) is transforming medical domain. In ophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4 million natural images and 1.6 million retinal images, has demonstrated high adaptability across clinical applications. Conversely, DINOv2, a general-purpose vision FM pre-trained on 142 million natural images, has shown promise in non-medical domains. However, its applicability to clinical tasks remains underexplored. To address this, we conducted head-to-head evaluations by fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular disease detection and systemic disease prediction tasks, across eight standardized open-source ocular datasets, as well as the Moorfields AlzEye and the UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting diabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets, all P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In glaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940, P<0.001). Conversely, RETFound achieved superior performance over all DINOv2 models in predicting heart failure, myocardial infarction, and ischaemic stroke (AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even with 10% of the fine-tuning data. These findings showcase the distinct scenarios where general-purpose and domain-specific FMs excel, highlighting the importance of aligning FM selection with task-specific requirements to optimise clinical performance.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Embedding Sampling Method for Diverse Captioning</title>
<link>https://arxiv.org/abs/2502.10118</link>
<guid>https://arxiv.org/abs/2502.10118</guid>
<content:encoded><![CDATA[
arXiv:2502.10118v2 Announce Type: replace-cross 
Abstract: Image Captioning for state-of-the-art VLMs has significantly improved over time; however, this comes at the cost of increased computational complexity, making them less accessible for resource-constrained applications such as mobile devices and assistive technologies. Alternatively, comparably smaller VLMs prioritize high-level scene descriptions, overlooking finer details that contribute to a richer understanding of an image. In this paper, we introduce a training-free framework that enhances caption diversity and informativeness by explicitly attending to distinct image regions using a comparably small VLM, BLIP, as the backbone. Our approach leverages structured segmentation to produce hierarchical representations that capture both global and localized semantics. Without requiring additional model training, we demonstrate that our method allows smaller VLMs to achieve performance comparable to larger models in terms of image-caption alignment, semantic integrity, and diversity. We evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets, achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset, respectively, while maintaining strong image-caption relevancy and semantic integrity with the human-annotated captions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoDiff: Conditional Diffusion Model for Collaborative 3D Object Detection</title>
<link>https://arxiv.org/abs/2502.14891</link>
<guid>https://arxiv.org/abs/2502.14891</guid>
<content:encoded><![CDATA[
arXiv:2502.14891v3 Announce Type: replace-cross 
Abstract: Collaborative 3D object detection holds significant importance in the field of autonomous driving, as it greatly enhances the perception capabilities of each individual agent by facilitating information exchange among multiple agents. However, in practice, due to pose estimation errors and time delays, the fusion of information across agents often results in feature representations with spatial and temporal noise, leading to detection errors. Diffusion models naturally have the ability to denoise noisy samples to the ideal data, which motivates us to explore the use of diffusion models to address the noise problem between multi-agent systems. In this work, we propose CoDiff, a novel robust collaborative perception framework that leverages the potential of diffusion models to generate more comprehensive and clearer feature representations. To the best of our knowledge, this is the first work to apply diffusion models to multi-agent collaborative perception. Specifically, we project high-dimensional feature map into the latent space of a powerful pre-trained autoencoder. Within this space, individual agent information serves as a condition to guide the diffusion model's sampling. This process denoises coarse feature maps and progressively refines the fused features. Experimental study on both simulated and real-world datasets demonstrates that the proposed framework CoDiff consistently outperforms existing relevant methods in terms of the collaborative object detection performance, and exhibits highly desired robustness when the pose and delay information of agents is with high-level noise. The code is released at https://github.com/HuangZhe885/CoDiff
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response</title>
<link>https://arxiv.org/abs/2502.18452</link>
<guid>https://arxiv.org/abs/2502.18452</guid>
<content:encoded><![CDATA[
arXiv:2502.18452v3 Announce Type: replace-cross 
Abstract: During Human Robot Interactions in disaster relief scenarios, Large Language Models (LLMs) have the potential for substantial physical reasoning to assist in mission objectives. However, these reasoning capabilities are often found only in larger models, which are not currently reasonable to deploy on robotic systems due to size constraints. To meet our problem space requirements, we introduce a dataset and pipeline to create Field Reasoning and Instruction Decoding Agent (FRIDA) models. In our pipeline, domain experts and linguists combine their knowledge to make high-quality, few-shot prompts used to generate synthetic data for fine-tuning. We hand-curate datasets for this few-shot prompting and for evaluation to improve LLM reasoning on both general and disaster-specific objects. We concurrently run an ablation study to understand which kinds of synthetic data most affect performance. We fine-tune several small instruction-tuned models and find that ablated FRIDA models only trained on objects' physical state and function data outperformed both the FRIDA models trained on all synthetic data and the base models in our evaluation. We demonstrate that the FRIDA pipeline is capable of instilling physical common sense with minimal data.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KNighter: Transforming Static Analysis with LLM-Synthesized Checkers</title>
<link>https://arxiv.org/abs/2503.09002</link>
<guid>https://arxiv.org/abs/2503.09002</guid>
<content:encoded><![CDATA[
arXiv:2503.09002v3 Announce Type: replace-cross 
Abstract: Static analysis is a powerful technique for bug detection in critical systems like operating system kernels. However, designing and implementing static analyzers is challenging, time-consuming, and typically limited to predefined bug patterns. While large language models (LLMs) have shown promise for static analysis, directly applying them to scan large systems remains impractical due to computational constraints and contextual limitations.
  We present KNighter, the first approach that unlocks scalable LLM-based static analysis by automatically synthesizing static analyzers from historical bug patterns. Rather than using LLMs to directly analyze massive systems, our key insight is leveraging LLMs to generate specialized static analyzers guided by historical patch knowledge. KNighter implements this vision through a multi-stage synthesis pipeline that validates checker correctness against original patches and employs an automated refinement process to iteratively reduce false positives. Our evaluation on the Linux kernel demonstrates that KNighter generates high-precision checkers capable of detecting diverse bug patterns overlooked by existing human-written analyzers. To date, KNighter-synthesized checkers have discovered 92 new, critical, long-latent bugs (average 4.3 years) in the Linux kernel; 77 are confirmed, 57 fixed, and 30 have been assigned CVE numbers. This work establishes an entirely new paradigm for scalable, reliable, and traceable LLM-based static analysis for real-world systems via checker synthesis.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond holography: the entropic quantum gravity foundations of image processing</title>
<link>https://arxiv.org/abs/2503.14048</link>
<guid>https://arxiv.org/abs/2503.14048</guid>
<content:encoded><![CDATA[
arXiv:2503.14048v2 Announce Type: replace-cross 
Abstract: Recently, thanks to the development of artificial intelligence (AI) there is increasing scientific attention in establishing the connections between theoretical physics and AI. Traditionally, these connections have been focusing mostly on the relation between string theory and image processing and involve important theoretical paradigms such as holography. Recently G. Bianconi has formulated the Gravity from Entropy (GfE) approach to quantum gravity in which gravity is derived from the geometric quantum relative entropy (GQRE) between two metrics associated with the Lorentzian spacetime. Here it is demonstrated that the famous Perona-Malik algorithm for image processing is the gradient flow of the GfE action in its simple warm-up scenario. Specifically, this algorithm is the outcome of the minimization of the GQRE between two Euclidean metrics: the one of the support of the image and the one induced by the image. As the Perona-Malik algorithm is known to preserve sharp contours, this implies that the GfE action, does not in general lead to uniform images upon iteration of the gradient flow dynamics as it would be intuitively expected from entropic actions maximising classical entropies. Rather, the outcome of the minimization of the GQRE is compatible with the preservation of complex structures. These results provide the geometrical and information theory foundations for the Perona-Malik algorithm and might contribute to establish deeper connections between GfE, machine learning and brain research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Offline Imitation Learning Through State-level Trajectory Stitching</title>
<link>https://arxiv.org/abs/2503.22524</link>
<guid>https://arxiv.org/abs/2503.22524</guid>
<content:encoded><![CDATA[
arXiv:2503.22524v2 Announce Type: replace-cross 
Abstract: Imitation learning (IL) has proven effective for enabling robots to acquire visuomotor skills through expert demonstrations. However, traditional IL methods are limited by their reliance on high-quality, often scarce, expert data, and suffer from covariate shift. To address these challenges, recent advances in offline IL have incorporated suboptimal, unlabeled datasets into the training. In this paper, we propose a novel approach to enhance policy learning from mixed-quality offline datasets by leveraging task-relevant trajectory fragments and rich environmental dynamics. Specifically, we introduce a state-based search framework that stitches state-action pairs from imperfect demonstrations, generating more diverse and informative training trajectories. Experimental results on standard IL benchmarks and real-world robotic tasks showcase that our proposed method significantly improves both generalization and performance.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RBT4DNN: Requirements-based Testing of Neural Networks</title>
<link>https://arxiv.org/abs/2504.02737</link>
<guid>https://arxiv.org/abs/2504.02737</guid>
<content:encoded><![CDATA[
arXiv:2504.02737v3 Announce Type: replace-cross 
Abstract: Testing allows developers to determine whether a system functions as expected. When such systems include deep neural networks (DNNs), Testing becomes challenging, as DNNs approximate functions for which the formalization of functional requirements is intractable. This prevents the application of well-developed approaches to requirements-based testing to DNNs.
  To address this, we propose a requirements-based testing method (RBT4DNN) that uses natural language requirements statements. These statements use a glossary of terms to define a semantic feature space that can be leveraged for test input generation. RBT4DNN formalizes preconditions of functional requirements as logical combinations of those semantic features. Training data matching these feature combinations can be used to fine-tune a generative model to reliably produce test inputs satisfying the precondition. Executing these tests on a trained DNN enables comparing its output to the expected requirement postcondition behavior. We propose two use cases for RBT4DNN: (1) given requirements defining DNN correctness properties, RBT4DNN comprises a novel approach for detecting faults, and (2) during development, requirements-guided exploration of model behavior can provide developers with feedback on model generalization. Our further evaluation shows that RBT4DNN-generated tests are realistic, diverse, and aligned with requirement preconditions, enabling targeted analysis of model behavior and effective fault detection.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable Mask Transformer: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation</title>
<link>https://arxiv.org/abs/2504.05774</link>
<guid>https://arxiv.org/abs/2504.05774</guid>
<content:encoded><![CDATA[
arXiv:2504.05774v2 Announce Type: replace-cross 
Abstract: Recent advances in Vision Transformers (ViTs) have set new benchmarks in semantic segmentation. However, when adapting pretrained ViTs to new target domains, significant performance degradation often occurs due to distribution shifts, resulting in suboptimal global attention. Since self-attention mechanisms are inherently data-driven, they may fail to effectively attend to key objects when source and target domains exhibit differences in texture, scale, or object co-occurrence patterns. While global and patch-level domain adaptation methods provide partial solutions, region-level adaptation with dynamically shaped regions is crucial due to spatial heterogeneity in transferability across different image areas. We present Transferable Mask Transformer (TMT), a novel region-level adaptation framework for semantic segmentation that aligns cross-domain representations through spatial transferability analysis. TMT consists of two key components: (1) An Adaptive Cluster-based Transferability Estimator (ACTE) that dynamically segments images into structurally and semantically coherent regions for localized transferability assessment, and (2) A Transferable Masked Attention (TMA) module that integrates region-specific transferability maps into ViTs' attention mechanisms, prioritizing adaptation in regions with low transferability and high semantic uncertainty. Comprehensive evaluations across 20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2% MIoU improvement over vanilla fine-tuning and a 1.28% increase compared to state-of-the-art baselines. The source code will be publicly available.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks</title>
<link>https://arxiv.org/abs/2505.03522</link>
<guid>https://arxiv.org/abs/2505.03522</guid>
<content:encoded><![CDATA[
arXiv:2505.03522v2 Announce Type: replace-cross 
Abstract: Deep learning has substantially advanced the field of Single Image Super-Resolution (SISR). However, existing research has predominantly focused on raw performance gains, with little attention paid to quantifying the transferability of architectural components. In this paper, we introduce the concept of "Universality" and its associated definitions, which extend the traditional notion of "Generalization" to encompass the ease of transferability of modules. We then propose the Universality Assessment Equation (UAE), a metric that quantifies how readily a given module can be transplanted across models and reveals the combined influence of multiple existing metrics on transferability. Guided by the UAE results of standard residual blocks and other plug-and-play modules, we further design two optimized modules: the Cycle Residual Block (CRB) and the Depth-Wise Cycle Residual Block (DCRB). Through comprehensive experiments on natural-scene benchmarks, remote-sensing datasets, and other low-level tasks, we demonstrate that networks embedded with the proposed plug-and-play modules outperform several state-of-the-art methods, achieving a PSNR improvement of up to 0.83 dB or enabling a 71.3% reduction in parameters with negligible loss in reconstruction fidelity. Similar optimization approaches could be applied to a broader range of basic modules, offering a new paradigm for the design of plug-and-play modules.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling</title>
<link>https://arxiv.org/abs/2506.02025</link>
<guid>https://arxiv.org/abs/2506.02025</guid>
<content:encoded><![CDATA[
arXiv:2506.02025v2 Announce Type: replace-cross 
Abstract: High-Performance Computing (HPC) job scheduling involves balancing conflicting objectives such as minimizing makespan, reducing wait times, optimizing resource use, and ensuring fairness. Traditional methods, including heuristic-based, e.g., First-Come-First-Served (FJFS) and Shortest Job First (SJF), or intensive optimization techniques, often lack adaptability to dynamic workloads and, more importantly, cannot simultaneously optimize multiple objectives in HPC systems. To address this, we propose a novel Large Language Model (LLM)-based scheduler using a ReAct-style framework (Reason + Act), enabling iterative, interpretable decision-making. The system incorporates a scratchpad memory to track scheduling history and refine decisions via natural language feedback, while a constraint enforcement module ensures feasibility and safety. We evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across seven real-world HPC workload scenarios, including heterogeneous mixes, bursty patterns, and adversarial cases etc. Comparisons against FCFS, SJF, and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling effectively balances multiple objectives while offering transparent reasoning through natural language traces. The method excels in constraint satisfaction and adapts to diverse workloads without domain-specific training. However, a trade-off between reasoning quality and computational overhead challenges real-time deployment. This work presents the first comprehensive study of reasoning-capable LLMs for HPC scheduling, demonstrating their potential to handle multiobjective optimization while highlighting limitations in computational efficiency. The findings provide insights into leveraging advanced language models for complex scheduling problems in dynamic HPC environments.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiniCPM4: Ultra-Efficient LLMs on End Devices</title>
<link>https://arxiv.org/abs/2506.07900</link>
<guid>https://arxiv.org/abs/2506.07900</guid>
<content:encoded><![CDATA[
arXiv:2506.07900v2 Announce Type: replace-cross 
Abstract: This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Furthermore, we construct a hybrid reasoning model, MiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning mode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform similar-sized open-source models across benchmarks, with the 8B variants showing significant speed improvements on long sequence understanding and generation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation</title>
<link>https://arxiv.org/abs/2506.08570</link>
<guid>https://arxiv.org/abs/2506.08570</guid>
<content:encoded><![CDATA[
arXiv:2506.08570v3 Announce Type: replace-cross 
Abstract: Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly in many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and identify which design choices influence performance the most. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: auto-regressive decoding and conditional flow-matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Parameter Decomposition</title>
<link>https://arxiv.org/abs/2506.20790</link>
<guid>https://arxiv.org/abs/2506.20790</guid>
<content:encoded><![CDATA[
arXiv:2506.20790v2 Announce Type: replace-cross 
Abstract: A key step in reverse engineering neural networks is to decompose them into simpler parts that can be studied in relative isolation. Linear parameter decomposition -- a framework that has been proposed to resolve several issues with current decomposition methods -- decomposes neural network parameters into a sum of sparsely used vectors in parameter space. However, the current main method in this framework, Attribution-based Parameter Decomposition (APD), is impractical on account of its computational cost and sensitivity to hyperparameters. In this work, we introduce \textit{Stochastic Parameter Decomposition} (SPD), a method that is more scalable and robust to hyperparameters than APD, which we demonstrate by decomposing models that are slightly larger and more complex than was possible to decompose with APD. We also show that SPD avoids other issues, such as shrinkage of the learned parameters, and better identifies ground truth mechanisms in toy models. By bridging causal mediation analysis and network decomposition methods, this demonstration opens up new research possibilities in mechanistic interpretability by removing barriers to scaling linear parameter decomposition methods to larger models. We release a library for running SPD and reproducing our experiments at https://github.com/goodfire-ai/spd/tree/spd-paper.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Action-Value Temporal-Difference Methods That Learn State Values</title>
<link>https://arxiv.org/abs/2507.09523</link>
<guid>https://arxiv.org/abs/2507.09523</guid>
<content:encoded><![CDATA[
arXiv:2507.09523v2 Announce Type: replace-cross 
Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping: using value predictions to generate new value predictions. The vast majority of TD methods for control learn a policy by bootstrapping from a single action-value function (e.g., Q-learning and Sarsa). Significantly less attention has been given to methods that bootstrap from two asymmetric value functions: i.e., methods that learn state values as an intermediate step in learning action values. Existing algorithms in this vein can be categorized as either QV-learning or AV-learning. Though these algorithms have been investigated to some degree in prior work, it remains unclear if and when it is advantageous to learn two value functions instead of just one -- and whether such approaches are theoretically sound in general. In this paper, we analyze these algorithmic families in terms of convergence and sample efficiency. We find that while both families are more efficient than Expected Sarsa in the prediction setting, only AV-learning methods offer any major benefit over Q-learning in the control setting. Finally, we introduce a new AV-learning algorithm called Regularized Dueling Q-learning (RDQ), which significantly outperforms Dueling DQN in the MinAtar benchmark.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP</title>
<link>https://arxiv.org/abs/2507.14904</link>
<guid>https://arxiv.org/abs/2507.14904</guid>
<content:encoded><![CDATA[
arXiv:2507.14904v2 Announce Type: replace-cross 
Abstract: 3D visual grounding allows an embodied agent to understand visual information in real-world 3D environments based on human instructions, which is crucial for embodied intelligence. Existing 3D visual grounding methods typically rely on separate encoders for different modalities (e.g., RGB images, text, and 3D point clouds), resulting in large and complex models that are inefficient to train. While some approaches use pre-trained 2D multi-modal models like CLIP for 3D tasks, they still struggle with aligning point cloud data to 2D encoders. As a result, these methods continue to depend on 3D encoders for feature extraction, further increasing model complexity and training inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal network to process all three modalities (RGB images, text, and point clouds), significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal model with adapter-based fine-tuning, this framework effectively adapts to the tri-modal setting, improving both adaptability and performance across modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module is designed to fuse geometric multi-scale features from point clouds and images. We then integrate textual features for final modality fusion and introduce a multi-modal decoder to facilitate deep cross-modal understanding. Together, our method achieves unified feature extraction and fusion across the three modalities, enabling an end-to-end 3D visual grounding model. Compared to the baseline, our method reduces the number of trainable parameters by approximately 58\%, while achieving a 6.52\% improvement in the 3D detection task and a 6.25\% improvement in the 3D visual grounding task.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Video Generation for High-Efficiency Video Compression</title>
<link>https://arxiv.org/abs/2507.15269</link>
<guid>https://arxiv.org/abs/2507.15269</guid>
<content:encoded><![CDATA[
arXiv:2507.15269v2 Announce Type: replace-cross 
Abstract: Perceptual studies demonstrate that conditional diffusion models excel at reconstructing video content aligned with human visual perception. Building on this insight, we propose a video compression framework that leverages conditional diffusion models for perceptually optimized reconstruction. Specifically, we reframe video compression as a conditional generation task, where a generative model synthesizes video from sparse, yet informative signals. Our approach introduces three key modules: (1) Multi-granular conditioning that captures both static scene structure and dynamic spatio-temporal cues; (2) Compact representations designed for efficient transmission without sacrificing semantic richness; (3) Multi-condition training with modality dropout and role-aware embeddings, which prevent over-reliance on any single modality and enhance robustness. Extensive experiments show that our method significantly outperforms both traditional and neural codecs on perceptual quality metrics such as Fr\'echet Video Distance (FVD) and LPIPS, especially under high compression ratios.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing</title>
<link>https://arxiv.org/abs/2507.22627</link>
<guid>https://arxiv.org/abs/2507.22627</guid>
<content:encoded><![CDATA[
arXiv:2507.22627v2 Announce Type: replace-cross 
Abstract: Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The KG-ER Conceptual Schema Language</title>
<link>https://arxiv.org/abs/2508.02548</link>
<guid>https://arxiv.org/abs/2508.02548</guid>
<content:encoded><![CDATA[
arXiv:2508.02548v2 Announce Type: replace-cross 
Abstract: We propose KG-ER, a conceptual schema language for knowledge graphs that describes the structure of knowledge graphs independently of their representation (relational databases, property graphs, RDF) while helping to capture the semantics of the information stored in a knowledge graph.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Street-Level AI: Are Large Language Models Ready for Real-World Judgments?</title>
<link>https://arxiv.org/abs/2508.08193</link>
<guid>https://arxiv.org/abs/2508.08193</guid>
<content:encoded><![CDATA[
arXiv:2508.08193v2 Announce Type: replace-cross 
Abstract: A surge of recent work explores the ethical and societal implications of large-scale AI models that make "moral" judgments. Much of this literature focuses either on alignment with human judgments through various thought experiments or on the group fairness implications of AI judgments. However, the most immediate and likely use of AI is to help or fully replace the so-called street-level bureaucrats, the individuals deciding to allocate scarce social resources or approve benefits. There is a rich history underlying how principles of local justice determine how society decides on prioritization mechanisms in such domains. In this paper, we examine how well LLM judgments align with human judgments, as well as with socially and politically determined vulnerability scoring systems currently used in the domain of homelessness resource allocation. Crucially, we use real data on those needing services (maintaining strict confidentiality by only using local large models) to perform our analyses. We find that LLM prioritizations are extremely inconsistent in several ways: internally on different runs, between different LLMs, and between LLMs and the vulnerability scoring systems. At the same time, LLMs demonstrate qualitative consistency with lay human judgments in pairwise testing. Findings call into question the readiness of current generation AI systems for naive integration in high-stakes societal decision-making.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI</title>
<link>https://arxiv.org/abs/2508.08524</link>
<guid>https://arxiv.org/abs/2508.08524</guid>
<content:encoded><![CDATA[
arXiv:2508.08524v3 Announce Type: replace-cross 
Abstract: Interactive streetscape mapping tools such as Google Street View (GSV) and Meta Mapillary enable users to virtually navigate and experience real-world environments via immersive 360{\deg} imagery but remain fundamentally inaccessible to blind users. We introduce StreetViewAI, the first-ever accessible street view tool, which combines context-aware, multimodal AI, accessible navigation controls, and conversational speech. With StreetViewAI, blind users can virtually examine destinations, engage in open-world exploration, or virtually tour any of the over 220 billion images and 100+ countries where GSV is deployed. We iteratively designed StreetViewAI with a mixed-visual ability team and performed an evaluation with eleven blind users. Our findings demonstrate the value of an accessible street view in supporting POI investigations and remote route planning. We close by enumerating key guidelines for future work.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiGen: Child-Friendly Multilingual Speech Generator with LLMs</title>
<link>https://arxiv.org/abs/2508.08715</link>
<guid>https://arxiv.org/abs/2508.08715</guid>
<content:encoded><![CDATA[
arXiv:2508.08715v3 Announce Type: replace-cross 
Abstract: Generative speech models have demonstrated significant potential in improving human-machine interactions, offering valuable real-world applications such as language learning for children. However, achieving high-quality, child-friendly speech generation remains challenging, particularly for low-resource languages across diverse languages and cultural contexts. In this paper, we propose MultiGen, a multilingual speech generation model with child-friendly interaction, leveraging LLM architecture for speech generation tailored for low-resource languages. We propose to integrate age-appropriate multilingual speech generation using LLM architectures, which can be used to facilitate young children's communication with AI systems through culturally relevant context in three low-resource languages: Singaporean accent Mandarin, Malay, and Tamil. Experimental results from both objective metrics and subjective evaluations demonstrate the superior performance of the proposed MultiGen compared to baseline methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration</title>
<link>https://arxiv.org/abs/2508.13755</link>
<guid>https://arxiv.org/abs/2508.13755</guid>
<content:encoded><![CDATA[
arXiv:2508.13755v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transplant Then Regenerate: A New Paradigm for Text Data Augmentation</title>
<link>https://arxiv.org/abs/2508.14723</link>
<guid>https://arxiv.org/abs/2508.14723</guid>
<content:encoded><![CDATA[
arXiv:2508.14723v2 Announce Type: replace-cross 
Abstract: Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their "knowledge emergence" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vectorized Attention with Learnable Encoding for Quantum Transformer</title>
<link>https://arxiv.org/abs/2508.18464</link>
<guid>https://arxiv.org/abs/2508.18464</guid>
<content:encoded><![CDATA[
arXiv:2508.18464v2 Announce Type: replace-cross 
Abstract: Vectorized quantum block encoding provides a way to embed classical data into Hilbert space, offering a pathway for quantum models, such as Quantum Transformers (QT), that replace classical self-attention with quantum circuit simulations to operate more efficiently. Current QTs rely on deep parameterized quantum circuits (PQCs), rendering them vulnerable to QPU noise, and thus hindering their practical performance. In this paper, we propose the Vectorized Quantum Transformer (VQT), a model that supports ideal masked attention matrix computation through quantum approximation simulation and efficient training via vectorized nonlinear quantum encoder, yielding shot-efficient and gradient-free quantum circuit simulation (QCS) and reduced classical sampling overhead. In addition, we demonstrate an accuracy comparison for IBM and IonQ in quantum circuit simulation and competitive results in benchmarking natural language processing tasks on IBM state-of-the-art and high-fidelity Kingston QPU. Our noise intermediate-scale quantum friendly VQT approach unlocks a novel architecture for end-to-end machine learning in quantum computing.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language</title>
<link>https://arxiv.org/abs/2409.00061</link>
<guid>https://arxiv.org/abs/2409.00061</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph, NLI, fact-checking, COVID-19, Indonesian language  

Summary:  
Automated fact-checking is crucial for combating COVID-19 misinformation online. This study proposes a model that utilizes a Knowledge Graph (KG) to enhance Natural Language Inference (NLI) performance for Indonesian language fact-checking. The model consists of three modules: fact, NLI, and classifier. By incorporating information from the COVID-19 KG Bahasa Indonesia, the model achieved an accuracy of 0.8616, demonstrating the effectiveness of KGs in improving fact-checking NLI performance. This approach addresses the challenge of knowledge scarcity in deep learning training and highlights the value of external knowledge sources in enhancing automated fact-checking systems for COVID-19 misinformation. <div>
arXiv:2409.00061v4 Announce Type: replace-cross 
Abstract: Automated fact-checking is a key strategy to overcome the spread of COVID-19 misinformation on the internet. These systems typically leverage deep learning approaches through Natural Language Inference (NLI) to verify the truthfulness of information based on supporting evidence. However, one challenge that arises in deep learning is performance stagnation due to a lack of knowledge during training. This study proposes using a Knowledge Graph (KG) as external knowledge to enhance NLI performance for automated COVID-19 fact-checking in the Indonesian language. The proposed model architecture comprises three modules: a fact module, an NLI module, and a classifier module. The fact module processes information from the KG, while the NLI module handles semantic relationships between the given premise and hypothesis. The representation vectors from both modules are concatenated and fed into the classifier module to produce the final result. The model was trained using the generated Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia. Our study demonstrates that incorporating KGs can significantly improve NLI performance in fact-checking, achieving the best accuracy of 0.8616. This suggests that KGs are a valuable component for enhancing NLI performance in automated fact-checking.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis</title>
<link>https://arxiv.org/abs/2509.02650</link>
<guid>https://arxiv.org/abs/2509.02650</guid>
<content:encoded><![CDATA[
<div> media coverage, artificial intelligence, profit, safety, reputation

Summary:<br />
Artificial intelligence (AI) developers often prioritize profit over user safety, leading to untrustworthy AI technology. This study explores the role of media coverage in incentivizing AI creators to prioritize safety. Through artificial populations and evolutionary game theory, the research shows that media coverage can promote cooperation between creators and users by holding developers accountable and shaping public perception. However, cooperation is influenced by the reliability of media information and the costs associated with accessing media or ensuring safety. The study suggests that media can act as a soft regulator, guiding AI safety even in the absence of formal government oversight. This highlights the importance of media in promoting the safe development and widespread adoption of AI technology. 

<br /><br /> <div>
arXiv:2509.02650v1 Announce Type: new 
Abstract: When developers of artificial intelligence (AI) products need to decide between profit and safety for the users, they likely choose profit. Untrustworthy AI technology must come packaged with tangible negative consequences. Here, we envisage those consequences as the loss of reputation caused by media coverage of their misdeeds, disseminated to the public. We explore whether media coverage has the potential to push AI creators into the production of safe products, enabling widespread adoption of AI technology. We created artificial populations of self-interested creators and users and studied them through the lens of evolutionary game theory. Our results reveal that media is indeed able to foster cooperation between creators and users, but not always. Cooperation does not evolve if the quality of the information provided by the media is not reliable enough, or if the costs of either accessing media or ensuring safety are too high. By shaping public perception and holding developers accountable, media emerges as a powerful soft regulator -- guiding AI safety even in the absence of formal government oversight.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)</title>
<link>https://arxiv.org/abs/2509.02661</link>
<guid>https://arxiv.org/abs/2509.02661</guid>
<content:encoded><![CDATA[
<div> AI, MPS, interdisciplinary, research, education <br />
Summary: The paper summarizes the MPS community's perspective on the intersection of Artificial Intelligence (AI) and Mathematical and Physics Sciences (MPS). The growing link between AI and MPS indicates the need to proactively leverage AI for scientific discovery and optimize its impact on AI development. Proposed activities and priorities include promoting AI+MPS research in both directions, building an interdisciplinary community of AI+MPS researchers, and enhancing education and workforce development in AI for MPS researchers and students. The paper concludes with suggested priorities for funding agencies, educational institutions, and individual researchers to position the MPS community as a leader in exploiting the transformative potential of AI+MPS. <br /><br /> <div>
arXiv:2509.02661v1 Announce Type: new 
Abstract: This community paper developed out of the NSF Workshop on the Future of Artificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS), which was held in March 2025 with the goal of understanding how the MPS domains (Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics) can best capitalize on, and contribute to, the future of AI. We present here a summary and snapshot of the MPS community's perspective, as of Spring/Summer 2025, in a rapidly developing field. The link between AI and MPS is becoming increasingly inextricable; now is a crucial moment to strengthen the link between AI and Science by pursuing a strategy that proactively and thoughtfully leverages the potential of AI for scientific discovery and optimizes opportunities to impact the development of AI by applying concepts from fundamental science. To achieve this, we propose activities and strategic priorities that: (1) enable AI+MPS research in both directions; (2) build up an interdisciplinary community of AI+MPS researchers; and (3) foster education and workforce development in AI for MPS researchers and students. We conclude with a summary of suggested priorities for funding agencies, educational institutions, and individual researchers to help position the MPS community to be a leader in, and take full advantage of, the transformative potential of AI+MPS.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning with Reasoning using Vision Language World Model</title>
<link>https://arxiv.org/abs/2509.02722</link>
<guid>https://arxiv.org/abs/2509.02722</guid>
<content:encoded><![CDATA[
<div> modeling, world model, language, visual, planning <br />
Summary: <br />
The Vision Language World Model (VLWM) is introduced as a high-level world model trained for language-based world modeling on natural videos. It uses a Tree of Captions to predict goal achievements and trajectories of actions and state changes in a semantic and temporally abstract manner. The VLWM integrates an action policy and dynamics model for reactive and reflective planning, respectively. It utilizes a critic model for cost evaluation based on semantic distances between hypothetical future states and expected goal states. The VLWM achieves state-of-the-art performance in Visual Planning for Assistance (VPA), outperforming strong VLM baselines on benchmark evaluations and human evaluations in PlannerArena. Additionally, it surpasses existing models on RoboVQA and WorldPrediction benchmarks. This advancement in world modeling has significant implications for improving planning efficiency and accuracy in various domains. <br /> <div>
arXiv:2509.02722v1 Announce Type: new 
Abstract: Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that we trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics</title>
<link>https://arxiv.org/abs/2509.02751</link>
<guid>https://arxiv.org/abs/2509.02751</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, semantic operators, AI-driven analytics, Deep Research systems, optimized execution

Summary:
- Researchers are exploring the use of large language models (LLMs) for AI-driven analytics over unstructured datasets.
- Semantic operators have been developed as a set of AI-powered data transformations with natural language specifications but can be expensive and inefficient for interactive tasks.
- Deep Research systems can answer natural language questions over large datasets but may not optimize query plans effectively.
- A prototype has been built to enable Deep Research agents to write and execute optimized semantic operator programs.
- The prototype outperforms handcrafted semantic operator programs and standard Deep Research agents, achieving better F1-score and cost and runtime savings of up to 76.8% and 72.7%, respectively.

<br /><br />Summary: 
Researchers are investigating the use of large language models (LLMs) for AI-driven analytics on unstructured datasets. Semantic operators, AI-powered data transformations with natural language specifications, are costly and inefficient for interactive tasks. Deep Research systems can address natural language queries but might not optimize query plans well. A prototype has been developed to allow Deep Research agents to write and execute optimized semantic operator programs, leading to improved performance in terms of F1-score and cost and runtime savings compared to handcrafted programs and standard Deep Research agents. <div>
arXiv:2509.02751v1 Announce Type: new 
Abstract: With advances in large language models (LLMs), researchers are creating new systems that can perform AI-driven analytics over large unstructured datasets. Recent work has explored executing such analytics queries using semantic operators -- a declarative set of AI-powered data transformations with natural language specifications. However, even when optimized, these operators can be expensive to execute on millions of records and their iterator execution semantics make them ill-suited for interactive data analytics tasks. In another line of work, Deep Research systems have demonstrated an ability to answer natural language question(s) over large datasets. These systems use one or more LLM agent(s) to plan their execution, process the dataset(s), and iteratively refine their answer. However, these systems do not explicitly optimize their query plans which can lead to poor plan execution. In order for AI-driven analytics to excel, we need a runtime which combines the optimized execution of semantic operators with the flexibility and more dynamic execution of Deep Research systems. As a first step towards this vision, we build a prototype which enables Deep Research agents to write and execute optimized semantic operator programs. We evaluate our prototype and demonstrate that it can outperform a handcrafted semantic operator program and open Deep Research systems on two basic queries. Compared to a standard open Deep Research agent, our prototype achieves up to 1.95x better F1-score. Furthermore, even if we give the agent access to semantic operators as tools, our prototype still achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its optimized execution.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.02754</link>
<guid>https://arxiv.org/abs/2509.02754</guid>
<content:encoded><![CDATA[
<div> tokenizer design, positional embedding, pre-training paradigms, post-training strategies, test-time computation
Summary:
- The study evaluates the transferability of key modules from large language models (LLMs) to autonomous driving motion generation.
- Five modules are assessed: tokenizer design, positional embedding, pre-training paradigms, post-training strategies, and test-time computation.
- The experiments on the Waymo Sim Agents benchmark show promising results when the modules are adapted for autonomous driving.
- The study identifies techniques that can effectively enhance performance in autonomous driving scenarios.
- It also discusses the reasons for the failure of certain modules and suggests specific adaptations needed for autonomous driving motion generation. 

<br /><br />Summary: <div>
arXiv:2509.02754v1 Announce Type: new 
Abstract: Recent breakthroughs in large language models (LLMs) have not only advanced natural language processing but also inspired their application in domains with structurally similar problems--most notably, autonomous driving motion generation. Both domains involve autoregressive sequence modeling, token-based representations, and context-aware decision making, making the transfer of LLM components a natural and increasingly common practice. However, despite promising early attempts, a systematic understanding of which LLM modules are truly transferable remains lacking. In this paper, we present a comprehensive evaluation of five key LLM modules--tokenizer design, positional embedding, pre-training paradigms, post-training strategies, and test-time computation--within the context of motion generation for autonomous driving. Through extensive experiments on the Waymo Sim Agents benchmark, we demonstrate that, when appropriately adapted, these modules can significantly improve performance for autonomous driving motion generation. In addition, we identify which techniques can be effectively transferred, analyze the potential reasons for the failure of others, and discuss the specific adaptations needed for autonomous driving scenarios. We evaluate our method on the Sim Agents task and achieve competitive results.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan Verification for LLM-Based Embodied Task Completion Agents</title>
<link>https://arxiv.org/abs/2509.02761</link>
<guid>https://arxiv.org/abs/2509.02761</guid>
<content:encoded><![CDATA[
<div> framework, verification, language model, embodied AI, spatial planning <br />
<br />
Summary: 
This research introduces an iterative verification framework to improve the quality of task plans generated by large language models (LLMs) for embodied AI systems. The framework involves a Judge LLM critiquing action sequences and a Planner LLM making revisions, resulting in cleaner and more coherent trajectories. Unlike rule-based approaches, this method relies on natural language prompting for error correction, allowing for broad generalization across various error types. The framework was tested on annotated actions from the TEACh dataset and achieved high recall and precision across different LLMs. It quickly converges to cleaner action sequences, improving temporal efficiency and spatial organization. Importantly, the method preserves human error-recovery patterns, supporting future work on robust corrective behavior. This approach establishes plan verification as a reliable capability for LLMs in spatial planning and action refinement, providing a scalable method for generating high-quality training data for imitation learning in embodied AI. <div>
arXiv:2509.02761v1 Announce Type: new 
Abstract: Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality. We propose an iterative verification framework in which a Judge LLM critiques action sequences and a Planner LLM applies the revisions, yielding progressively cleaner and more spatially coherent trajectories. Unlike rule-based approaches, our method relies on natural language prompting, enabling broad generalization across error types including irrelevant actions, contradictions, and missing steps. On a set of manually annotated actions from the TEACh embodied AI dataset, our framework achieves up to 90% recall and 100% precision across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout). The refinement loop converges quickly, with 96.5% of sequences requiring at most three iterations, while improving both temporal efficiency and spatial action organization. Crucially, the method preserves human error-recovery patterns rather than collapsing them, supporting future work on robust corrective behavior. By establishing plan verification as a reliable LLM capability for spatial planning and action refinement, we provide a scalable path to higher-quality training data for imitation learning in embodied AI.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Key Principles in Cross-Domain Hyper-Heuristic Performance</title>
<link>https://arxiv.org/abs/2509.02782</link>
<guid>https://arxiv.org/abs/2509.02782</guid>
<content:encoded><![CDATA[
<div> Keywords: hyper-heuristics, selection, transformation, real-world domains, competition <br />
<br />
Summary: 
The study focuses on cross-domain selection hyper-heuristics, emphasizing the composition and strategic transformations of low-level heuristics (LLHs) instead of their adaptive selection. Three key principles guide the transformations: solution acceptance, LLH repetitions, and perturbation intensity. By applying these principles to a basic random selection mechanism, the research surpasses existing state-of-the-art hyper-heuristics on challenging real-world domains, discovering 11 new best-known solutions. The method also proves competitive with the winner of the CHeSC competition, a common benchmark for cross-domain performance. The strategic transformations are further incorporated into recent hyper-heuristics, leading to superior performance on the CHeSC benchmark and real-world domains while streamlining their designs. <div>
arXiv:2509.02782v1 Announce Type: new 
Abstract: Cross-domain selection hyper-heuristics aim to distill decades of research on problem-specific heuristic search algorithms into adaptable general-purpose search strategies. In this respect, existing selection hyper-heuristics primarily focus on an adaptive selection of low-level heuristics (LLHs) from a predefined set. In contrast, we concentrate on the composition of this set and its strategic transformations. We systematically analyze transformations based on three key principles: solution acceptance, LLH repetitions, and perturbation intensity, i.e., the proportion of a solution affected by a perturbative LLH. We demonstrate the raw effects of our transformations on a trivial unbiased random selection mechanism. With an appropriately constructed transformation, this trivial method outperforms all available state-of-the-art hyper-heuristics on three challenging real-world domains and finds 11 new best-known solutions. The same method is competitive with the winner of the CHeSC competition, commonly used as the standard cross-domain benchmark. Moreover, we accompany several recent hyper-heuristics with such strategic transformations. Using this approach, we outperform the current state-of-the-art methods on both the CHeSC benchmark and real-world domains while often simplifying their designs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning General Policies From Examples</title>
<link>https://arxiv.org/abs/2509.02794</link>
<guid>https://arxiv.org/abs/2509.02794</guid>
<content:encoded><![CDATA[
<div> combinatorial methods, learning policies, generalization, hitting set algorithm, scalability 
Summary:
This research introduces a new symbolic method for learning policies that can solve large collections of planning problems. Unlike deep learning approaches, the resulting policies can be understood and verified for correctness. The proposed method generalizes sampled plans and ensures structural termination, preventing cycles in the learned policies. This approach is based on a hitting set algorithm, allowing for effective handling of problems with millions of states and pools with hundreds of thousands of features. The formal properties of the method are analyzed to ensure its reliability. The scalability of the approach is demonstrated through testing on various benchmarks. The new method addresses the limitations of existing combinatorial approaches by enabling learning from larger training instances and feature pools, paving the way for developing general policies that can tackle complex planning problems efficiently. 
<br /><br />Summary: <div>
arXiv:2509.02794v1 Announce Type: new 
Abstract: Combinatorial methods for learning general policies that solve large collections of planning problems have been recently developed. One of their strengths, in relation to deep learning approaches, is that the resulting policies can be understood and shown to be correct. A weakness is that the methods do not scale up and learn only from small training instances and feature pools that contain a few hundreds of states and features at most. In this work, we propose a new symbolic method for learning policies based on the generalization of sampled plans that ensures structural termination and hence acyclicity. The proposed learning approach is not based on SAT/ASP, as previous symbolic methods, but on a hitting set algorithm that can effectively handle problems with millions of states, and pools with hundreds of thousands of features. The formal properties of the approach are analyzed, and its scalability is tested on a number of benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-driven Adaptive Exploration</title>
<link>https://arxiv.org/abs/2509.03219</link>
<guid>https://arxiv.org/abs/2509.03219</guid>
<content:encoded><![CDATA[
<div> framework, adaptive exploration, uncertainty, policy learning, MuJoCo environments 
Summary:
The article introduces a generic framework for adaptive exploration in policy learning, addressing the crucial decision of when to switch between exploration and exploitation. By utilizing uncertainty measures, the framework offers a principled approach to determining these shifts in complex action sequences. It can incorporate various uncertainty-measuring mechanisms, such as those found in intrinsic motivation or epistemic uncertainty-based exploration methods. The framework encompasses previous adaptive exploration approaches and has shown superior performance compared to standard strategies in multiple MuJoCo environments through experimental validation. <div>
arXiv:2509.03219v1 Announce Type: new 
Abstract: Adaptive exploration methods propose ways to learn complex policies via alternating between exploration and exploitation. An important question for such methods is to determine the appropriate moment to switch between exploration and exploitation and vice versa. This is critical in domains that require the learning of long and complex sequences of actions. In this work, we present a generic adaptive exploration framework that employs uncertainty to address this important issue in a principled manner. Our framework includes previous adaptive exploration approaches as special cases. Moreover, we can incorporate in our framework any uncertainty-measuring mechanism of choice, for instance mechanisms used in intrinsic motivation or epistemic uncertainty-based exploration methods. We experimentally demonstrate that our framework gives rise to adaptive exploration strategies that outperform standard ones across several MuJoCo environments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accountability Framework for Healthcare AI Systems: Towards Joint Accountability in Decision Making</title>
<link>https://arxiv.org/abs/2509.03286</link>
<guid>https://arxiv.org/abs/2509.03286</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, healthcare, accountability, framework, collaboration 

Summary: 
Accountability in AI-driven healthcare decisions is a crucial concern, but there is a knowledge gap on how to implement it effectively. Regulatory guidelines are high-level and focus on the "what" rather than the "how," creating ambiguity. This study analyzes different perspectives on accountability and proposes a framework to bridge the gap between the concept and its implementation. The framework includes a three-tier structure for handling various accountability mechanisms in healthcare AI systems. It emphasizes the importance of shared dependencies in decision-making and advocates for collaboration among actors to address accountability issues. Explainability is highlighted as a key factor in facilitating communication and information sharing among stakeholders. By aligning regulations and mechanisms under a consistent accountability regime, the framework aims to promote transparency and responsibility in AI-driven healthcare decision-making. 

<br /><br />Summary: <div>
arXiv:2509.03286v1 Announce Type: new 
Abstract: AI is transforming the healthcare domain and is increasingly helping practitioners to make health-related decisions. Therefore, accountability becomes a crucial concern for critical AI-driven decisions. Although regulatory bodies, such as the EU commission, provide guidelines, they are highlevel and focus on the ''what'' that should be done and less on the ''how'', creating a knowledge gap for actors. Through an extensive analysis, we found that the term accountability is perceived and dealt with in many different ways, depending on the actor's expertise and domain of work. With increasing concerns about AI accountability issues and the ambiguity around this term, this paper bridges the gap between the ''what'' and ''how'' of AI accountability, specifically for AI systems in healthcare. We do this by analysing the concept of accountability, formulating an accountability framework, and providing a three-tier structure for handling various accountability mechanisms. Our accountability framework positions the regulations of healthcare AI systems and the mechanisms adopted by the actors under a consistent accountability regime. Moreover, the three-tier structure guides the actors of the healthcare AI system to categorise the mechanisms based on their conduct. Through our framework, we advocate that decision-making in healthcare AI holds shared dependencies, where accountability should be dealt with jointly and should foster collaborations. We highlight the role of explainability in instigating communication and information sharing between the actors to further facilitate the collaborative process.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding</title>
<link>https://arxiv.org/abs/2509.03310</link>
<guid>https://arxiv.org/abs/2509.03310</guid>
<content:encoded><![CDATA[
<div> framework, validation, structured environments, open-source, AI agents <br />
Summary: <br />
The article introduces app.build, an open-source framework designed to improve LLM-based application generation by incorporating systematic validation and structured environments. The framework utilizes multi-layered validation pipelines, stack-specific orchestration, and a model-agnostic architecture across three reference stacks. Evaluation results show a 73.3% viability rate with 30% achieving perfect quality scores, while open-weights models reach 80.8% of closed-model performance with structured environments. The framework has been widely adopted by the community, generating over 3,000 applications. The study highlights the importance of scaling reliable AI agents by scaling environments, emphasizing the need for comprehensive validation and structured environments in agent systems. This work provides valuable insights and complete reference implementations for production-oriented AI systems. <br /> <div>
arXiv:2509.03310v1 Announce Type: new 
Abstract: We present app.build (https://github.com/appdotbuild/agent/), an open-source framework that improves LLM-based application generation through systematic validation and structured environments. Our approach combines multi-layered validation pipelines, stack-specific orchestration, and model-agnostic architecture, implemented across three reference stacks. Through evaluation on 30 generation tasks, we demonstrate that comprehensive validation achieves 73.3% viability rate with 30% reaching perfect quality scores, while open-weights models achieve 80.8% of closed-model performance when provided structured environments. The open-source framework has been adopted by the community, with over 3,000 applications generated to date. This work demonstrates that scaling reliable AI agents requires scaling environments, not just models -- providing empirical insights and complete reference implementations for production-oriented agent systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning</title>
<link>https://arxiv.org/abs/2509.03345</link>
<guid>https://arxiv.org/abs/2509.03345</guid>
<content:encoded><![CDATA[
<div> deductive reasoning, inductive reasoning, abductive reasoning, large language models, Occam's Razor

Summary:
This work examines the inductive and abductive reasoning abilities of large language models (LLMs), which are crucial for solving real-world problems. A new synthetic dataset called InAbHyD is introduced, challenging intelligent agents to generate hypotheses to explain observations based on incomplete world models. A metric based on Occam's Razor is proposed to evaluate the quality of hypotheses. The study evaluates state-of-the-art LLMs and finds that while they can perform inductive and abductive reasoning in simple scenarios, they struggle with complex world models and generating high-quality hypotheses. Even with reasoning-enhancing techniques like in-context learning and RLVR, LLMs face challenges in producing accurate and concise hypotheses. <div>
arXiv:2509.03345v1 Announce Type: new 
Abstract: Reasoning is a core capability in artificial intelligence systems, for which large language models (LLMs) have recently shown remarkable progress. However, most work focuses exclusively on deductive reasoning, which is problematic since other types of reasoning are also essential in solving real-world problems, and they are less explored. This work focuses on evaluating LLMs' inductive and abductive reasoning capabilities. We introduce a programmable and synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example consists of an incomplete world model and a set of observations. The task for the intelligent agent is to produce hypotheses to explain observations under the incomplete world model to solve each reasoning example. We propose a new metric to evaluate the quality of hypotheses based on Occam's Razor. We evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs can perform inductive and abductive reasoning in simple scenarios, but struggle with complex world models and producing high-quality hypotheses, even with popular reasoning-enhancing techniques such as in-context learning and RLVR.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems</title>
<link>https://arxiv.org/abs/2509.03380</link>
<guid>https://arxiv.org/abs/2509.03380</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic LLM AI agents, bottom-up framework, aspects, umwelt, information niches <br />
Summary: 
This article introduces a new framework for AI agents called aspective agentic AI, which aims to improve their efficiency and security. Unlike traditional AI agents that follow scripts, these agents are situated in their environment and their behaviors are triggered by changes in the environment. The framework introduces the concept of aspects, similar to umwelt, where sets of agents perceive their environment differently, allowing for clearer control of information. An illustrative implementation of this framework shows that it enables zero information leakage, compared to typical architectures which leak up to 83% of the time. The idea of specialist agents working efficiently in their own information niches is highlighted as a potential improvement for both security and efficiency in AI systems. <br /><br />Summary: <div>
arXiv:2509.03380v1 Announce Type: new 
Abstract: Agentic LLM AI agents are often little more than autonomous chatbots: actors following scripts, often controlled by an unreliable director. This work introduces a bottom-up framework that situates AI agents in their environment, with all behaviors triggered by changes in their environments. It introduces the notion of aspects, similar to the idea of umwelt, where sets of agents perceive their environment differently to each other, enabling clearer control of information. We provide an illustrative implementation and show that compared to a typical architecture, which leaks up to 83% of the time, aspective agentic AI enables zero information leakage. We anticipate that this concept of specialist agents working efficiently in their own information niches can provide improvements to both security and efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ANNIE: Be Careful of Your Robots</title>
<link>https://arxiv.org/abs/2509.03383</link>
<guid>https://arxiv.org/abs/2509.03383</guid>
<content:encoded><![CDATA[
<div> taxonomy, safety violations, benchmark, adversarial framework, attack strategies

Summary:
In this study, the authors address the security risks posed by integrating vision-language-action (VLA) models into embodied AI (EAI) systems. They introduce a taxonomy of safety violations based on physical constraints, create a benchmark called ANNIEBench to evaluate embodied safety, and develop ANNIE-Attack, an adversarial framework targeting EAI models. The evaluation reveals high attack success rates across all safety categories, emphasizing the critical need for security-driven defenses in EAI systems. The authors also demonstrate sparse and adaptive attack strategies and validate the impacts through physical robot experiments, highlighting the urgent requirement for safeguarding physically grounded AI systems. <div>
arXiv:2509.03383v1 Announce Type: new 
Abstract: The integration of vision-language-action (VLA) models into embodied AI (EAI) robots is rapidly advancing their ability to perform complex, long-horizon tasks in humancentric environments. However, EAI systems introduce critical security risks: a compromised VLA model can directly translate adversarial perturbations on sensory input into unsafe physical actions. Traditional safety definitions and methodologies from the machine learning community are no longer sufficient. EAI systems raise new questions, such as what constitutes safety, how to measure it, and how to design effective attack and defense mechanisms in physically grounded, interactive settings. In this work, we present the first systematic study of adversarial safety attacks on embodied AI systems, grounded in ISO standards for human-robot interactions. We (1) formalize a principled taxonomy of safety violations (critical, dangerous, risky) based on physical constraints such as separation distance, velocity, and collision boundaries; (2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with 2,400 video-action sequences for evaluating embodied safety; and (3) ANNIE-Attack, a task-aware adversarial framework with an attack leader model that decomposes long-horizon goals into frame-level perturbations. Our evaluation across representative EAI models shows attack success rates exceeding 50% across all safety categories. We further demonstrate sparse and adaptive attack strategies and validate the real-world impact through physical robot experiments. These results expose a previously underexplored but highly consequential attack surface in embodied AI systems, highlighting the urgent need for security-driven defenses in the physical AI era. Code is available at https://github.com/RLCLab/Annie.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sam-llm: interpretable lane change trajectoryprediction via parametric finetuning</title>
<link>https://arxiv.org/abs/2509.03462</link>
<guid>https://arxiv.org/abs/2509.03462</guid>
<content:encoded><![CDATA[
<div> SAM-LLM, interpretable lane change trajectory prediction, LLM, kinematic lane change models, autonomous driving <br />
<br />
Summary: The SAM-LLM hybrid architecture combines Large Language Models (LLMs) with kinematic lane change models for autonomous driving. It predicts core physical parameters of trajectory models for lane change maneuvers based on LLM outputs, enhancing interpretability and computational efficiency. For lane-keeping scenarios, discrete coordinates are predicted, while parameters for an enhanced Sinusoidal Acceleration Model (SAM) are generated for lane change maneuvers including lateral displacement, maneuver duration, initial lateral velocity, and longitudinal velocity change. This parametric approach results in a complete, continuous, and physically plausible trajectory model with reduced output size. The SAM-LLM achieves a high accuracy rate of 98.73% in overall intention prediction, equivalent to traditional LLM predictors, but with advantages in explainability and resource efficiency.<br /><br /> <div>
arXiv:2509.03462v1 Announce Type: new 
Abstract: This work introduces SAM-LLM, a novel hybrid architecture that bridges the gap between the contextual reasoning of Large Language Models (LLMs) and the physical precision of kinematic lane change models for autonomous driving. The system is designed for interpretable lane change trajectory prediction by finetuning an LLM to output the core physical parameters of a trajectory model instead of raw coordinates. For lane-keeping scenarios, the model predicts discrete coordinates, but for lane change maneuvers, it generates the parameters for an enhanced Sinusoidal Acceleration Model (SAM), including lateral displacement, maneuver duration, initial lateral velocity, and longitudinal velocity change. This parametric approach yields a complete, continuous, and physically plausible trajectory model that is inherently interpretable and computationally efficient, achieving an 80% reduction in output size compared to coordinate-based methods. The SAM-LLM achieves a state-of-the-art overall intention prediction accuracy of 98.73%, demonstrating performance equivalent to traditional LLM predictors while offering significant advantages in explainability and resource efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Lifecycle Principle: Stabilizing Dynamic Neural Networks with State Memory</title>
<link>https://arxiv.org/abs/2509.02575</link>
<guid>https://arxiv.org/abs/2509.02575</guid>
<content:encoded><![CDATA[
<div> regularization, neurons, Lifecycle principle, state memory, optimization

Summary:
The study explores a novel regularization approach involving deactivating neurons for extended periods to enhance learning. Unlike conventional methods like Dropout which make temporary changes, this approach involves long-term modifications. However, reactivating neurons with random weights leads to severe training instability. To address this issue, the author introduces the Lifecycle (LC) principle, which incorporates state memory to restore neurons to their last effective state instead of reinitializing them. The theoretical analysis suggests that LC smoothens the loss landscape, facilitating optimization towards flatter minima associated with better generalization. Experimental results on image classification tasks demonstrate the efficacy of the method in improving generalization and robustness. Ablation studies underscore the significance of state memory in achieving these enhancements. <div>
arXiv:2509.02575v1 Announce Type: cross 
Abstract: I investigate a stronger form of regularization by deactivating neurons for extended periods, a departure from the temporary changes of methods like Dropout. However, this long-term dynamism introduces a critical challenge: severe training instability when neurons are revived with random weights. To solve this, I propose the Lifecycle (LC) principle, a regularization mechanism centered on a key innovation: state memory. Instead of re-initializing a revived neuron, my method restores its parameters to their last known effective state. This process preserves learned knowledge and avoids destructive optimization shocks. My theoretical analysis reveals that the LC principle smooths the loss landscape, guiding optimization towards flatter minima associated with better generalization. Experiments on image classification benchmarks demonstrate that my method improves generalization and robustness. Crucially, ablation studies confirm that state memory is essential for achieving these gains.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection</title>
<link>https://arxiv.org/abs/2509.02579</link>
<guid>https://arxiv.org/abs/2509.02579</guid>
<content:encoded><![CDATA[
<div> Keywords: endangered wildlife, illegal poaching, UAV coordination, latent variable modeling, Multi-Agent Reinforcement Learning

Summary: 
This paper presents a novel approach using Expectation-Maximization (EM) based latent variable modeling in Multi-Agent Reinforcement Learning (MARL) for coordinating Unmanned Aerial Vehicles (UAVs) in protecting endangered wildlife from illegal poaching. By incorporating latent variables to model hidden environmental factors and inter-agent dynamics, the method improves exploration and coordination in uncertain environments. The framework was tested in a simulation involving 10 UAVs patrolling habitats of the endangered Iranian leopard, showing better detection accuracy, adaptability, and policy convergence compared to standard algorithms like PPO and DDPG. The results demonstrate the potential of combining EM inference with MARL for decentralized decision-making in complex conservation scenarios. The full implementation, simulation environment, and training scripts are available on GitHub. 

<br /><br />Summary: <div>
arXiv:2509.02579v1 Announce Type: cross 
Abstract: Protecting endangered wildlife from illegal poaching presents a critical challenge, particularly in vast and partially observable environments where real-time response is essential. This paper introduces a novel Expectation-Maximization (EM) based latent variable modeling approach in the context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial Vehicle (UAV) coordination in wildlife protection. By modeling hidden environmental factors and inter-agent dynamics through latent variables, our method enhances exploration and coordination under uncertainty.We implement and evaluate our EM-MARL framework using a custom simulation involving 10 UAVs tasked with patrolling protected habitats of the endangered Iranian leopard. Extensive experimental results demonstrate superior performance in detection accuracy, adaptability, and policy convergence when compared to standard algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic Policy Gradient (DDPG). Our findings underscore the potential of combining EM inference with MARL to improve decentralized decisionmaking in complex, high-stakes conservation scenarios. The full implementation, simulation environment, and training scripts are publicly available on GitHub.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Charting the Future of Scholarly Knowledge with AI: A Community Perspective</title>
<link>https://arxiv.org/abs/2509.02581</link>
<guid>https://arxiv.org/abs/2509.02581</guid>
<content:encoded><![CDATA[
<div> Keywords: scholarly knowledge extraction, AI-enabled approaches, cross-disciplinary dialogue, collaboration, research directions

Summary:
The article discusses the need for scalable AI-enabled approaches in scholarly knowledge extraction and organization due to the rapid increase in publications. It highlights the challenge of limited familiarity with existing technologies and the lack of interaction across research communities. The manuscript emphasizes the importance of fostering cross-disciplinary dialogue to address shared challenges and promote collaboration for the development of reliable knowledge bases. By identifying ways to shape future research directions in scholarly knowledge and organization, the authors aim to enhance the exchange of methods, models, and best practices. Overall, the article advocates for a more integrated approach to structuring and synthesizing scholarly knowledge through collaboration and shared initiatives.<br /><br />Summary: <div>
arXiv:2509.02581v1 Announce Type: cross 
Abstract: Despite the growing availability of tools designed to support scholarly knowledge extraction and organization, many researchers still rely on manual methods, sometimes due to unfamiliarity with existing technologies or limited access to domain-adapted solutions. Meanwhile, the rapid increase in scholarly publications across disciplines has made it increasingly difficult to stay current, further underscoring the need for scalable, AI-enabled approaches to structuring and synthesizing scholarly knowledge. Various research communities have begun addressing this challenge independently, developing tools and frameworks aimed at building reliable, dynamic, and queryable scholarly knowledge bases. However, limited interaction across these communities has hindered the exchange of methods, models, and best practices, slowing progress toward more integrated solutions. This manuscript identifies ways to foster cross-disciplinary dialogue, identify shared challenges, categorize new collaboration and shape future research directions in scholarly knowledge and organization.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MitoDetect++: A Domain-Robust Pipeline for Mitosis Detection and Atypical Subtyping</title>
<link>https://arxiv.org/abs/2509.02586</link>
<guid>https://arxiv.org/abs/2509.02586</guid>
<content:encoded><![CDATA[
<div> Keywords: mitotic figures, deep learning, MIDOG 2025 challenge, atypical mitosis, computational pathology

Summary: 
MitoDetect++ is a deep learning pipeline developed for the MIDOG 2025 challenge to address mitosis detection and atypical mitosis classification. For detection, they utilize a U-Net-based architecture with EfficientNetV2-L backbone and attention modules, trained with combined segmentation losses. For classification, the Virchow2 vision transformer is used, fine-tuned efficiently with Low-Rank Adaptation (LoRA). To enhance generalization, strong augmentations, focal loss, and group-aware stratified cross-validation are integrated. Test-time augmentation (TTA) boosts robustness during inference. The method achieves a balanced accuracy of 0.892 across validation domains, demonstrating its clinical relevance and scalability in computational pathology tasks. 

<br /><br />Summary: <div>
arXiv:2509.02586v1 Announce Type: cross 
Abstract: Automated detection and classification of mitotic figures especially distinguishing atypical from normal remain critical challenges in computational pathology. We present MitoDetect++, a unified deep learning pipeline designed for the MIDOG 2025 challenge, addressing both mitosis detection and atypical mitosis classification. For detection (Track 1), we employ a U-Net-based encoder-decoder architecture with EfficientNetV2-L as the backbone, enhanced with attention modules, and trained via combined segmentation losses. For classification (Track 2), we leverage the Virchow2 vision transformer, fine-tuned efficiently using Low-Rank Adaptation (LoRA) to minimize resource consumption. To improve generalization and mitigate domain shifts, we integrate strong augmentations, focal loss, and group-aware stratified 5-fold cross-validation. At inference, we deploy test-time augmentation (TTA) to boost robustness. Our method achieves a balanced accuracy of 0.892 across validation domains, highlighting its clinical applicability and scalability across tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normal and Atypical Mitosis Image Classifier using Efficient Vision Transformer</title>
<link>https://arxiv.org/abs/2509.02589</link>
<guid>https://arxiv.org/abs/2509.02589</guid>
<content:encoded><![CDATA[
<div> EfficientViT-L2, atypical mitosis classification, MIDOG 2025 challenge, cancer types, domain generalization <br />
Summary:
EfficientViT-L2, a hybrid CNN--ViT architecture, was utilized for classifying atypical versus normal mitosis in the MIDOG 2025 challenge using a unified dataset of 13,938 nuclei from seven cancer types. The study employed leave-one-cancer-type-out cross-validation with 5-fold ensembles and stain-deconvolution for image augmentation to assess domain generalization. The model achieved a balanced accuracy of 0.859, ROC AUC of 0.942, and raw accuracy of 0.85 in the preliminary evaluation phase, showcasing competitive performance across metrics. The approach involved training an ensemble on all cancer types for challenge submissions, demonstrating well-balanced performance and efficiency in the classification task. <br /> <br />Summary: <div>
arXiv:2509.02589v1 Announce Type: cross 
Abstract: We tackle atypical versus normal mitosis classification in the MIDOG 2025 challenge using EfficientViT-L2, a hybrid CNN--ViT architecture optimized for accuracy and efficiency. A unified dataset of 13,938 nuclei from seven cancer types (MIDOG++ and AMi-Br) was used, with atypical mitoses comprising ~15. To assess domain generalization, we applied leave-one-cancer-type-out cross-validation with 5-fold ensembles, using stain-deconvolution for image augmentation. For challenge submissions, we trained an ensemble with the same 5-fold split but on all cancer types. In the preliminary evaluation phase, this model achieved balanced accuracy of 0.859, ROC AUC of 0.942, and raw accuracy of 0.85, demonstrating competitive and well-balanced performance across metrics.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble of Pathology Foundation Models for MIDOG 2025 Track 2: Atypical Mitosis Classification</title>
<link>https://arxiv.org/abs/2509.02591</link>
<guid>https://arxiv.org/abs/2509.02591</guid>
<content:encoded><![CDATA[
<div> Mitotic figures, classification, typical, atypical, tumor aggressiveness <br />
Summary:<br />
Mitotic figures play a crucial role in determining tumor aggressiveness, with accurate classification essential for patient prognostication. This study utilized Pathology Foundation Models (PFMs) pre-trained on large histopathology datasets and incorporated parameter-efficient fine-tuning techniques. Training involved a fisheye transform to highlight mitoses and Fourier Domain Adaptation using ImageNet target images. By ensembling multiple PFMs, the model was able to integrate diverse morphological insights and achieve high balanced accuracy on the Preliminary Evaluation Phase dataset. This approach demonstrates the potential of leveraging advanced machine learning techniques to enhance mitotic figure differentiation in pathology analysis. <br /> <div>
arXiv:2509.02591v1 Announce Type: cross 
Abstract: Mitotic figures are classified into typical and atypical variants, with atypical counts correlating strongly with tumor aggressiveness. Accurate differentiation is therefore essential for patient prognostication and resource allocation, yet remains challenging even for expert pathologists. Here, we leveraged Pathology Foundation Models (PFMs) pre-trained on large histopathology datasets and applied parameter-efficient fine-tuning via low-rank adaptation. During training, we employ a fisheye transform to emphasize mitoses and Fourier Domain Adaptation using ImageNet target images. Finally, we ensembled multiple PFMs to integrate complementary morphological insights, achieving a high balanced accuracy on the Preliminary Evaluation Phase dataset.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning</title>
<link>https://arxiv.org/abs/2509.02592</link>
<guid>https://arxiv.org/abs/2509.02592</guid>
<content:encoded><![CDATA[
<div> threshold calibration, class imbalance, machine learning, group-aware, synthetic data generation

Summary:
This study addresses the challenge of class imbalance in machine learning by proposing group-aware threshold calibration. It is demonstrated that setting different decision thresholds for demographic groups outperforms synthetic data generation methods, achieving higher balanced accuracy and improving worst-group balanced accuracy by 1.5-4%. Unlike single-threshold approaches, the group-aware method optimizes the Pareto frontier between balanced accuracy and worst-group balanced accuracy, allowing for fine-grained control over group-level performance. Applying group thresholds to synthetically augmented data shows minimal additional benefit, suggesting redundancy. The results are consistent across seven model families, highlighting the simplicity, interpretability, and effectiveness of group-aware threshold calibration as a solution to class imbalance. 

<br /><br />Summary: <div>
arXiv:2509.02592v1 Announce Type: cross 
Abstract: Class imbalance remains a fundamental challenge in machine learning, with traditional solutions often creating as many problems as they solve. We demonstrate that group-aware threshold calibration--setting different decision thresholds for different demographic groups--provides superior robustness compared to synthetic data generation methods. Through extensive experiments, we show that group-specific thresholds achieve 1.5-4% higher balanced accuracy than SMOTE and CT-GAN augmented models while improving worst-group balanced accuracy. Unlike single-threshold approaches that apply one cutoff across all groups, our group-aware method optimizes the Pareto frontier between balanced accuracy and worst-group balanced accuracy, enabling fine-grained control over group-level performance. Critically, we find that applying group thresholds to synthetically augmented data yields minimal additional benefit, suggesting these approaches are fundamentally redundant. Our results span seven model families including linear, tree-based, instance-based, and boosting methods, confirming that group-aware threshold calibration offers a simpler, more interpretable, and more effective solution to class imbalance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</title>
<link>https://arxiv.org/abs/2509.02593</link>
<guid>https://arxiv.org/abs/2509.02593</guid>
<content:encoded><![CDATA[
<div> Keywords: Mitotic figures, Tumor pathology, Mitosis detection algorithms, YOLOv12, MIDOG 2025 challenge

Summary:
Mitotic figures play a crucial role in tumor pathology, aiding in determining tumor aggressiveness and proliferation. However, their accurate identification poses challenges due to inter-observer variability. The MIDOG 2025 challenge seeks to address this issue by developing robust mitosis detection algorithms. This paper introduces an approach using the YOLOv12 object detection architecture, achieving an impressive $F_1$-score of 0.801 on the initial test set of the challenge, without relying on external data. The algorithm shows promise in accurately detecting mitotic figures, presenting a significant advancement in tumor pathology research. <div>
arXiv:2509.02593v1 Announce Type: cross 
Abstract: Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figures detection approach based on the YOLOv12 object detection architecture, achieving a $F_1$-score of 0.801 on the preliminary test set of the MIDOG 2025 challenge, without relying on external data.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenAIs HealthBench in Action: Evaluating an LLM-Based Medical Assistant on Realistic Clinical Queries</title>
<link>https://arxiv.org/abs/2509.02594</link>
<guid>https://arxiv.org/abs/2509.02594</guid>
<content:encoded><![CDATA[
<div> Evaluation, large language models, clinical questions, HealthBench, DR.INFO <br />
Summary: <br />
This study evaluates the performance of the agentic clinical support assistant, DR.INFO, using the HealthBench benchmark, which consists of expert-annotated health conversations. DR.INFO outperforms leading LLMs in accuracy, completeness, and instruction following on challenging examples, scoring 0.51 on the Hard subset. Compared to similar agentic RAG assistants, DR.INFO maintains a performance lead with a score of 0.54. The results demonstrate DR.INFO's strengths in communication, instruction following, and accuracy, while also identifying areas for improvement in context awareness and completeness of responses. The study emphasizes the importance of behavior-level, rubric-based evaluation for developing a reliable and trustworthy AI-enabled clinical support assistant.<br /> <div>
arXiv:2509.02594v1 Announce Type: cross 
Abstract: Evaluating large language models (LLMs) on their ability to generate high-quality, accurate, situationally aware answers to clinical questions requires going beyond conventional benchmarks to assess how these systems behave in complex, high-stake clincal scenarios. Traditional evaluations are often limited to multiple-choice questions that fail to capture essential competencies such as contextual reasoning, awareness and uncertainty handling etc. To address these limitations, we evaluate our agentic, RAG-based clinical support assistant, DR.INFO, using HealthBench, a rubric-driven benchmark composed of open-ended, expert-annotated health conversations. On the Hard subset of 1,000 challenging examples, DR.INFO achieves a HealthBench score of 0.51, substantially outperforming leading frontier LLMs (GPT-5, o3, Grok 3, GPT-4, Gemini 2.5, etc.) across all behavioral axes (accuracy, completeness, instruction following, etc.). In a separate 100-sample evaluation against similar agentic RAG assistants (OpenEvidence, Pathway.md), it maintains a performance lead with a health-bench score of 0.54. These results highlight DR.INFOs strengths in communication, instruction following, and accuracy, while also revealing areas for improvement in context awareness and completeness of a response. Overall, the findings underscore the utility of behavior-level, rubric-based evaluation for building a reliable and trustworthy AI-enabled clinical support assistant.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIDOG 2025: Mitotic Figure Detection with Attention-Guided False Positive Correction</title>
<link>https://arxiv.org/abs/2509.02598</link>
<guid>https://arxiv.org/abs/2509.02598</guid>
<content:encoded><![CDATA[
<div> Keywords: mitotic figure detection, Fully Convolutional One-Stage Object Detector (FCOS), Feedback Attention Ladder CNN (FAL-CNN), false positive reduction, object detection enhancement

Summary: 
The study introduces a novel approach for mitotic figure detection by extending the FCOS model. It incorporates a FAL-CNN model for classifying normal versus abnormal mitotic figures and a fusion network for refining bounding box predictions. The primary goal is to lower the false positive rate of the FCOS detector, enhancing detection accuracy and network generalisability. Preliminary evaluation results showed an F1 score of 0.655 for mitosis detection, indicating promising performance. The integration of the FAL-CNN model complements the FCOS architecture, enabling more precise classification and bounding box adjustments. By leveraging both models, the detection of mitotic figures is improved, showcasing the potential for advanced image analysis in medical applications.<br /><br />Summary: <div>
arXiv:2509.02598v1 Announce Type: cross 
Abstract: We present a novel approach which extends the existing Fully Convolutional One-Stage Object Detector (FCOS) for mitotic figure detection. Our composite model adds a Feedback Attention Ladder CNN (FAL-CNN) model for classification of normal versus abnormal mitotic figures, feeding into a fusion network that is trained to generate adjustments to bounding boxes predicted by FCOS. Our network aims to reduce the false positive rate of the FCOS object detector, to improve the accuracy of object detection and enhance the generalisability of the network. Our model achieved an F1 score of 0.655 for mitosis detection on the preliminary evaluation dataset.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Founders: AI-Generated Social Simulations for Startup Validation Research in Computational Social Science</title>
<link>https://arxiv.org/abs/2509.02605</link>
<guid>https://arxiv.org/abs/2509.02605</guid>
<content:encoded><![CDATA[
<div> Keywords: comparative docking, AI-enabled simulation, linguistic expressiveness, hybrid social simulation, computational social science

Summary:
Convergent themes such as commitment-based demand signals and black-box trust barriers were emphasized in both human-subject interviews and AI-generated personas. Partial overlaps revealed concerns about outliers and stress in real customer validation from founders, while synthetic personas highlighted blind spots and AI as a buffer. Human-only themes included relational value in early customer engagement and skepticism toward moonshot markets. Synthetic-only themes pointed out amplified false positives and trauma blind spots in AI simulations. The study suggests that large language model-driven personas are a form of hybrid social simulation with linguistic expressiveness but lacking lived history and relational consequence. They complement empirical studies by extending hypothesis space, accelerating exploratory validation, and delineating the boundaries of cognitive realism in computational social science. 

Summary: <br /><br />The study compared human-subject interviews and AI-driven persona data on AI-powered validation in startup founders. While both datasets showed convergent themes like commitment-based demand signals, they also revealed differences in concerns and perspectives. The AI personas, more linguistically expressive, highlighted irrational blind spots and a psychological buffer role of AI. Human-only themes emphasized the relational value of early customer engagement, while synthetic-only themes pointed out potential false positives and trauma blind spots in AI simulations. The findings suggest that AI-driven personas serve as a hybrid social simulation, enhancing hypothesis space and clarifying cognitive realism in computational social science. <div>
arXiv:2509.02605v1 Announce Type: cross 
Abstract: We present a comparative docking experiment that aligns human-subject interview data with large language model (LLM)-driven synthetic personas to evaluate fidelity, divergence, and blind spots in AI-enabled simulation. Fifteen early-stage startup founders were interviewed about their hopes and concerns regarding AI-powered validation, and the same protocol was replicated with AI-generated founder and investor personas. A structured thematic synthesis revealed four categories of outcomes: (1) Convergent themes - commitment-based demand signals, black-box trust barriers, and efficiency gains were consistently emphasized across both datasets; (2) Partial overlaps - founders worried about outliers being averaged away and the stress of real customer validation, while synthetic personas highlighted irrational blind spots and framed AI as a psychological buffer; (3) Human-only themes - relational and advocacy value from early customer engagement and skepticism toward moonshot markets; and (4) Synthetic-only themes - amplified false positives and trauma blind spots, where AI may overstate adoption potential by missing negative historical experiences.
  We interpret this comparative framework as evidence that LLM-driven personas constitute a form of hybrid social simulation: more linguistically expressive and adaptable than traditional rule-based agents, yet bounded by the absence of lived history and relational consequence. Rather than replacing empirical studies, we argue they function as a complementary simulation category - capable of extending hypothesis space, accelerating exploratory validation, and clarifying the boundaries of cognitive realism in computational social science.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Digital Twins for Optimal Radioembolization</title>
<link>https://arxiv.org/abs/2509.02607</link>
<guid>https://arxiv.org/abs/2509.02607</guid>
<content:encoded><![CDATA[
<div> Keywords: Radioembolization, liver cancer treatment, computational fluid dynamics, physics-informed machine learning, patient-specific digital twin

Summary:
Radioembolization is a liver cancer treatment method that involves delivering radioactive microspheres to tumors through the hepatic arterial tree. The complex nature of hepatic artery anatomy and variable blood flow makes treatment optimization challenging. A proposed solution is the creation of patient-specific digital twins using high-fidelity computational fluid dynamics (CFD) and physics-informed machine learning approaches. CFD allows for personalized treatment planning based on individual patient data, but its computational cost limits clinical applicability. Physics-informed neural networks (PINNs) offer a more efficient way to simulate blood flow and microsphere transport, while physics-informed generative adversarial networks (PI-GANs) and other AI surrogates provide uncertainty-aware predictions with reduced computational cost. By combining CFD and AI methods, a dynamic digital twin can optimize radioembolization planning and improve clinical outcomes. 

<br /><br />Summary: Radioembolization aims to deliver radioactive microspheres to liver tumors with minimal damage to healthy tissue. The proposed patient-specific digital twin combines computational fluid dynamics and physics-informed machine learning to optimize treatment planning. Computational fluid dynamics allows for personalized planning but is computationally expensive, while physics-informed machine learning approaches offer more efficient simulations. By integrating AI surrogates, uncertainty-aware predictions can be made cost-effectively, leading to improved outcomes in liver cancer treatment. <div>
arXiv:2509.02607v1 Announce Type: cross 
Abstract: Radioembolization is a localized liver cancer treatment that delivers radioactive microspheres (30 micron) to tumors via a catheter inserted in the hepatic arterial tree. The goal is to maximize therapeutic efficacy while minimizing damage to healthy liver tissue. However, optimization is challenging due to complex hepatic artery anatomy, variable blood flow, and uncertainty in microsphere transport. The creation of dynamic, patient-specific digital twins may provide a transformative solution to these challenges. This work outlines a framework for a liver radioembolization digital twin using high-fidelity computational fluid dynamics (CFD) and/or recent physics-informed machine learning approaches. The CFD approach involves microsphere transport calculations in the hepatic arterial tree with individual patient data, which enables personalized treatment planning. Although accurate, traditional CFD is computationally expensive and limits clinical applicability.
  To accelerate simulations, physics-informed neural networks (PINNs) and their generative extensions play an increasingly important role. PINNs integrate governing equations, such as the Navier-Stokes equations, directly into the neural network training process, enabling mesh-free, data-efficient approximation of blood flow and microsphere transport. Physics-informed generative adversarial networks (PI-GANs), diffusion models (PI-DMs), and transformer-based architectures further enable uncertainty-aware, temporally resolved predictions with reduced computational cost. These AI surrogates not only maintain physical fidelity but also support rapid sampling of diverse flow scenarios, facilitating real-time decision support.
  Together, CFD and physics-informed AI methods form the foundation of dynamic, patient-specific digital twin to optimize radioembolization planning and ultimately improve clinical outcomes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive clustering based on regular equivalence for influential node identification in complex networks</title>
<link>https://arxiv.org/abs/2509.02609</link>
<guid>https://arxiv.org/abs/2509.02609</guid>
<content:encoded><![CDATA[
<div> influential nodes, complex networks, deep learning, unsupervised learning, contrastive learning

Summary:
ReCC is a novel deep unsupervised framework for identifying influential nodes in complex networks. It reformalizes the problem as a label-free deep clustering task and utilizes regular equivalence-based similarity in contrastive learning to generate positive and negative samples. The framework integrates this mechanism into a graph convolutional network to learn node embeddings for distinguishing influential nodes. ReCC is trained using network reconstruction loss and a combined contrastive and clustering loss without the need for labeled data. Additionally, it enhances node representations by combining structural metrics with regular equivalence-based similarities. Experimental results show that ReCC surpasses existing methods on various benchmarks. <br /><br /> <div>
arXiv:2509.02609v1 Announce Type: cross 
Abstract: Identifying influential nodes in complex networks is a fundamental task in network analysis with wide-ranging applications across domains. While deep learning has advanced node influence detection, existing supervised approaches remain constrained by their reliance on labeled data, limiting their applicability in real-world scenarios where labels are scarce or unavailable. While contrastive learning demonstrates significant potential for performance enhancement, existing approaches predominantly rely on multiple-embedding generation to construct positive/negative sample pairs. To overcome these limitations, we propose ReCC (\textit{r}egular \textit{e}quivalence-based \textit{c}ontrastive \textit{c}lustering), a novel deep unsupervised framework for influential node identification. We first reformalize influential node identification as a label-free deep clustering problem, then develop a contrastive learning mechanism that leverages regular equivalence-based similarity, which captures structural similarities between nodes beyond local neighborhoods, to generate positive and negative samples. This mechanism is integrated into a graph convolutional network to learn node embeddings that are used to differentiate influential from non-influential nodes. ReCC is pre-trained using network reconstruction loss and fine-tuned with a combined contrastive and clustering loss, with both phases being independent of labeled data. Additionally, ReCC enhances node representations by combining structural metrics with regular equivalence-based similarities. Extensive experiments demonstrate that ReCC outperforms state-of-the-art approaches across several benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resilient Biosecurity in the Era of AI-Enabled Bioweapons</title>
<link>https://arxiv.org/abs/2509.02610</link>
<guid>https://arxiv.org/abs/2509.02610</guid>
<content:encoded><![CDATA[
<div> Keywords: generative biology, proteins, drug discovery, biosafety measures, protein-protein interaction

Summary: 
The study evaluates the performance of three PPI prediction tools - AlphaFold 3, AF3Complex, and SpatialPPIv2, in detecting viral-host interactions like Hepatitis B and SARS-CoV-2. Despite being trained on similar viruses, the models miss detecting several known interactions and fail to identify four confirmed SARS-CoV-2 mutants. This highlights the inadequacy of current predictive filters in flagging biological threats and detecting novel ones. The findings emphasize the need for a shift towards response-oriented infrastructure, including rapid experimental validation, adaptable biomanufacturing, and agile regulatory frameworks to keep pace with AI-driven developments. This shift is essential to address the risks posed by the design of novel proteins, especially in the context of drug discovery and potential bioweapon development. 

<br /><br />Summary: <div>
arXiv:2509.02610v1 Announce Type: cross 
Abstract: Recent advances in generative biology have enabled the design of novel proteins, creating significant opportunities for drug discovery while also introducing new risks, including the potential development of synthetic bioweapons. Existing biosafety measures primarily rely on inference-time filters such as sequence alignment and protein-protein interaction (PPI) prediction to detect dangerous outputs. In this study, we evaluate the performance of three leading PPI prediction tools: AlphaFold 3, AF3Complex, and SpatialPPIv2. These models were tested on well-characterized viral-host interactions, such as those involving Hepatitis B and SARS-CoV-2. Despite being trained on many of the same viruses, the models fail to detect a substantial number of known interactions. Strikingly, none of the tools successfully identify any of the four experimentally validated SARS-CoV-2 mutants with confirmed binding. These findings suggest that current predictive filters are inadequate for reliably flagging even known biological threats and are even more unlikely to detect novel ones. We argue for a shift toward response-oriented infrastructure, including rapid experimental validation, adaptable biomanufacturing, and regulatory frameworks capable of operating at the speed of AI-driven developments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Synthetic Image Augmentation Useful for Imbalanced Classification Problems? Case-Study on the MIDOG2025 Atypical Cell Detection Competition</title>
<link>https://arxiv.org/abs/2509.02612</link>
<guid>https://arxiv.org/abs/2509.02612</guid>
<content:encoded><![CDATA[
<div> Keywords: MIDOG 2025 challenge, atypical mitosis classification, histopathology images, imbalanced data, deep learning

Summary: 
The MIDOG 2025 challenge focuses on atypical mitosis classification in histopathology images, a clinically important yet imbalanced problem. Two backbones, ConvNeXt-Small and a histopathology-specific ViT from Lunit, were evaluated, with both achieving strong performance. Synthetic balancing to address class imbalance did not consistently improve results. ImageNet and domain-pretrained backbones were both effective, with the former reaching higher peaks and the latter providing greater stability. ConvNeXt performed best on the preliminary hidden test set, while Lunit remained competitive on balanced accuracy. These results suggest that both types of pretrained models are suitable for atypical mitosis classification, with domain-pretraining offering robustness and ImageNet pretraining achieving higher performance. The full hidden test set results will be disclosed at the conclusion of the challenge. 

<br /><br />Summary: <div>
arXiv:2509.02612v1 Announce Type: cross 
Abstract: The MIDOG 2025 challenge extends prior work on mitotic figure detection by introducing a new Track 2 on atypical mitosis classification. This task aims to distinguish normal from atypical mitotic figures in histopathology images, a clinically relevant but highly imbalanced and cross-domain problem. We investigated two complementary backbones: (i) ConvNeXt-Small, pretrained on ImageNet, and (ii) a histopathology-specific ViT from Lunit trained via self-supervision. To address the strong prevalence imbalance (9408 normal vs. 1741 atypical), we synthesized additional atypical examples to approximate class balance and compared models trained with real-only vs. real+synthetic data. Using five-fold cross-validation, both backbones reached strong performance (mean AUROC approximately 95 percent), with ConvNeXt achieving slightly higher peaks while Lunit exhibited greater fold-to-fold stability. Synthetic balancing, however, did not lead to consistent improvements. On the organizers' preliminary hidden test set, explicitly designed as an out-of-distribution debug subset, ConvNeXt attained the highest AUROC (95.4 percent), whereas Lunit remained competitive on balanced accuracy. These findings suggest that both ImageNet and domain-pretrained backbones are viable for atypical mitosis classification, with domain-pretraining conferring robustness and ImageNet pretraining reaching higher peaks, while naive synthetic balancing has limited benefit. Full hidden test set results will be reported upon challenge completion.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Radio Astronomy in the Era of Vision-Language Models: Prompt Sensitivity and Adaptation</title>
<link>https://arxiv.org/abs/2509.02615</link>
<guid>https://arxiv.org/abs/2509.02615</guid>
<content:encoded><![CDATA[
<div> adaptation, vision-language models, morphology-based classification, radio galaxies, astronomical imagery <br />
<br />
Summary: 
This study investigates the ability of general-purpose Vision-Language Models (VLMs) to classify radio galaxies based on morphology using the MiraBest FR-I/FR-II dataset in astronomy. The researchers explore different prompting strategies using natural language and diagrams and introduce visual examples in prompts. They find that prompt-based approaches can achieve good performance, indicating that VLMs have useful priors for unfamiliar scientific domains. However, the outputs are unstable, sensitive to prompt changes, suggesting that the models' reasoning is prompt-dependent rather than genuine inference. Despite this sensitivity, with lightweight adaptation, a generic VLM achieves near state-of-the-art performance, rivaling specialized models without astronomy-specific pretraining. These findings suggest caution in using VLMs in scientific domains, highlighting the need for careful prompt design. Nonetheless, with some adaptation, generic VLMs show promise as tools for scientific discovery. <br /> <div>
arXiv:2509.02615v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs), such as recent Qwen and Gemini models, are positioned as general-purpose AI systems capable of reasoning across domains. Yet their capabilities in scientific imaging, especially on unfamiliar and potentially previously unseen data distributions, remain poorly understood. In this work, we assess whether generic VLMs, presumed to lack exposure to astronomical corpora, can perform morphology-based classification of radio galaxies using the MiraBest FR-I/FR-II dataset. We explore prompting strategies using natural language and schematic diagrams, and, to the best of our knowledge, we are the first to introduce visual in-context examples within prompts in astronomy. Additionally, we evaluate lightweight supervised adaptation via LoRA fine-tuning. Our findings reveal three trends: (i) even prompt-based approaches can achieve good performance, suggesting that VLMs encode useful priors for unfamiliar scientific domains; (ii) however, outputs are highly unstable, i.e. varying sharply with superficial prompt changes such as layout, ordering, or decoding temperature, even when semantic content is held constant; and (iii) with just 15M trainable parameters and no astronomy-specific pretraining, fine-tuned Qwen-VL achieves near state-of-the-art performance (3% Error rate), rivaling domain-specific models. These results suggest that the apparent "reasoning" of VLMs often reflects prompt sensitivity rather than genuine inference, raising caution for their use in scientific domains. At the same time, with minimal adaptation, generic VLMs can rival specialized models, offering a promising but fragile tool for scientific discovery.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IS${}^3$ : Generic Impulsive--Stationary Sound Separation in Acoustic Scenes using Deep Filtering</title>
<link>https://arxiv.org/abs/2509.02622</link>
<guid>https://arxiv.org/abs/2509.02622</guid>
<content:encoded><![CDATA[
<div> Neural Network, Sound Separation, Audio Processing, Data Generation, Training <br />
Summary: <br />
The article introduces IS${}^3$, a neural network designed for Impulsive--Stationary Sound Separation in audio systems. The network aims to differentiate processing of stationary backgrounds and isolated acoustic events within an acoustic scene for various applications like adaptive audio rendering, voice mixing, noise reduction, and acoustic event classification. A deep filtering approach is used to isolate impulsive acoustic events from the background noise. The article proposes a sophisticated data generation pipeline to curate and adapt existing datasets for optimal training of the network. The study demonstrates that the learning-based approach outperforms existing methods like Harmonic--Percussive Sound Separation and wavelet filtering in objective separation metrics. The success of IS${}^3$ highlights the effectiveness of a lightweight neural architecture trained with diverse and well-designed data in addressing the previously unexplored task of sound separation. <br /> <div>
arXiv:2509.02622v1 Announce Type: cross 
Abstract: We are interested in audio systems capable of performing a differentiated processing of stationary backgrounds and isolated acoustic events within an acoustic scene, whether for applying specific processing methods to each part or for focusing solely on one while ignoring the other. Such systems have applications in real-world scenarios, including robust adaptive audio rendering systems (e.g., EQ or compression), plosive attenuation in voice mixing, noise suppression or reduction, robust acoustic event classification or even bioacoustics. To this end, we introduce IS${}^3$, a neural network designed for Impulsive--Stationary Sound Separation, that isolates impulsive acoustic events from the stationary background using a deep filtering approach, that can act as a pre-processing stage for the above-mentioned tasks. To ensure optimal training, we propose a sophisticated data generation pipeline that curates and adapts existing datasets for this task. We demonstrate that a learning-based approach, build on a relatively lightweight neural architecture and trained with well-designed and varied data, is successful in this previously unaddressed task, outperforming the Harmonic--Percussive Sound Separation masking method, adapted from music signal processing research, and wavelet filtering on objective separation metrics.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Owns The Robot?: Four Ethical and Socio-technical Questions about Wellbeing Robots in the Real World through Community Engagement</title>
<link>https://arxiv.org/abs/2509.02624</link>
<guid>https://arxiv.org/abs/2509.02624</guid>
<content:encoded><![CDATA[
<div> Keywords: robotic coaches, wellbeing, ethics, community perspectives, real-world deployment

Summary: 
- The study explores the ethical and socio-technical implications of using robotic wellbeing coaches in real-world settings.
- Three underrepresented communities engaged in workshops to provide their perspectives on the topic.
- Qualitative data was collected using the Social Robot Co-Design Canvas on Ethics and analyzed through Thematic Analysis.
- Four key themes were identified, focusing on ethical and socio-technical questions surrounding the use of wellbeing robots.
- The study proposes four questions as a framework for robotic development and deployment, emphasizing safety, stakeholders involvement, data ownership, and purpose of the robot.
<br /><br />Summary: <div>
arXiv:2509.02624v1 Announce Type: cross 
Abstract: Recent studies indicate that robotic coaches can play a crucial role in promoting wellbeing. However, the real-world deployment of wellbeing robots raises numerous ethical and socio-technical questions and concerns. To explore these questions, we undertake a community-centered investigation to examine three different communities' perspectives on using robotic wellbeing coaches in real-world environments. We frame our work as an anticipatory ethical investigation, which we undertake to better inform the development of robotic technologies with communities' opinions, with the ultimate goal of aligning robot development with public interest. We conducted workshops with three communities who are under-represented in robotics development: 1) members of the public at a science festival, 2) women computer scientists at a conference, and 3) humanities researchers interested in history and philosophy of science. In the workshops, we collected qualitative data using the Social Robot Co-Design Canvas on Ethics. We analysed the collected qualitative data with Thematic Analysis, informed by notes taken during workshops. Through our analysis, we identify four themes regarding key ethical and socio-technical questions about the real-world use of wellbeing robots. We group participants' insights and discussions around these broad thematic questions, discuss them in light of state-of-the-art literature, and highlight areas for future investigation. Finally, we provide the four questions as a broad framework that roboticists can and should use during robotic development and deployment, in order to reflect on the ethics and socio-technical dimensions of their robotic applications, and to engage in dialogue with communities of robot users. The four questions are: 1) Is the robot safe and how can we know that?, 2) Who is the robot built for and with?, 3) Who owns the robot and the data?, and 4) Why a robot?.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Two-Stage Strategy for Mitosis Detection Using Improved YOLO11x Proposals and ConvNeXt Classification</title>
<link>https://arxiv.org/abs/2509.02627</link>
<guid>https://arxiv.org/abs/2509.02627</guid>
<content:encoded><![CDATA[
<div> Keywords: MIDOG 2025, mitosis detection, whole-slide images, two-stage framework, detection F1-score

Summary:
The article introduces a two-stage framework for improving mitosis detection in complicated whole-slide images with non-tumor, inflamed, and necrotic regions. The framework utilizes an enhanced YOLO11x model with EMA attention and LSConv for generating mitosis candidates with a high recall. False positives are then filtered out using a ConvNeXt-Tiny classifier to improve precision. The proposed framework achieves an F1-score of 0.882 on a fused dataset, outperforming the single-stage YOLO11x baseline. The precision is significantly enhanced from 0.762 to 0.839, leading to the performance gain. The code for the framework is available on GitHub for further exploration. 
<br /><br />Summary: <div>
arXiv:2509.02627v1 Announce Type: cross 
Abstract: MIDOG 2025 Track 1 requires mitosis detection in whole-slide images (WSIs) containing non-tumor, inflamed, and necrotic regions. Due to the complicated and heterogeneous context, as well as possible artifacts, there are often false positives and false negatives, thus degrading the detection F1-score. To address this problem, we propose a two-stage framework. Firstly, an improved YOLO11x, integrated with EMA attention and LSConv, is employed to generate mitosis candidates. We use a low confidence threshold to generate as many proposals as possible, ensuring the detection recall. Then, a ConvNeXt-Tiny classifier is employed to filter out the false positives, ensuring the detection precision. Consequently, the proposed two-stage framework can generate a high detection F1-score. Evaluated on a fused dataset comprising MIDOG++, MITOS_WSI_CCMCT, and MITOS_WSI_CMC, our framework achieves an F1-score of 0.882, which is 0.035 higher than the single-stage YOLO11x baseline. This performance gain is produced by a significant precision improvement, from 0.762 to 0.839, and a comparable recall. The code is available at https://github.com/xxiao0304/MIDOG-2025-Track-1-of-SZTU.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Single Detect Focused YOLO Framework for Robust Mitotic Figure Detection</title>
<link>https://arxiv.org/abs/2509.02637</link>
<guid>https://arxiv.org/abs/2509.02637</guid>
<content:encoded><![CDATA[
<div> Keywords: mitotic figure detection, computational pathology, domain-robust, SDF-YOLO, small targets

Summary:
Mitotic figure detection is essential in computational pathology for assessing tumor aggressiveness. The domain variability in scanners, tissue types, and staining protocols poses a challenge for automated detection methods. The SDF-YOLO framework is introduced as a lightweight and domain-robust detection method specifically for small targets like mitotic figures. With task-specific modifications and improved feature mixing, SDF-YOLO achieved competitive accuracy and computational efficiency in experiments on human and canine tumor datasets. In the MIDOG2025 challenge, SDF-YOLO demonstrated an average precision of 0.799, precision of 0.758, recall of 0.775, F1 score of 0.766, and FROC-AUC of 5.793. This indicates that SDF-YOLO is a reliable and efficient framework for robust mitotic figure detection across diverse domains. <br /><br />Summary: <div>
arXiv:2509.02637v1 Announce Type: cross 
Abstract: Mitotic figure detection is a crucial task in computational pathology, as mitotic activity serves as a strong prognostic marker for tumor aggressiveness. However, domain variability that arises from differences in scanners, tissue types, and staining protocols poses a major challenge to the robustness of automated detection methods. In this study, we introduce SDF-YOLO (Single Detect Focused YOLO), a lightweight yet domain-robust detection framework designed specifically for small, rare targets such as mitotic figures. The model builds on YOLOv11 with task-specific modifications, including a single detection head aligned with mitotic figure scale, coordinate attention to enhance positional sensitivity, and improved cross-channel feature mixing. Experiments were conducted on three datasets that span human and canine tumors: MIDOG ++, canine cutaneous mast cell tumor (CCMCT), and canine mammary carcinoma (CMC). When submitted to the preliminary test set for the MIDOG2025 challenge, SDF-YOLO achieved an average precision (AP) of 0.799, with a precision of 0.758, a recall of 0.775, an F1 score of 0.766, and an FROC-AUC of 5.793, demonstrating both competitive accuracy and computational efficiency. These results indicate that SDF-YOLO provides a reliable and efficient framework for robust mitotic figure detection across diverse domains.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Single-Cell RNA-seq Embedding through Gene Expression and Data-Driven Gene-Gene Interaction Integration</title>
<link>https://arxiv.org/abs/2509.02639</link>
<guid>https://arxiv.org/abs/2509.02639</guid>
<content:encoded><![CDATA[
<div> Keywords: single-cell RNA sequencing, gene-gene interactions, embedding approach, cell diversity, data analysis 

Summary: 
This article introduces a novel approach for analyzing single-cell RNA sequencing data. The method integrates gene expression profiles and gene-gene interactions to create a more comprehensive representation of cellular states. By constructing a Cell-Leaf Graph (CLG) and a K-Nearest Neighbor Graph (KNNG), which are then combined into an Enriched Cell-Leaf Graph (ECLG), the approach captures both regulatory relationships between genes and expression similarities between cells. A graph neural network is used to compute cell embeddings, enhancing the detection of rare cell populations and improving downstream analyses such as visualization, clustering, and trajectory inference. The method offers a more complete framework for understanding cellular diversity and dynamics. <br /><br />Summary: <div>
arXiv:2509.02639v1 Announce Type: cross 
Abstract: Single-cell RNA sequencing (scRNA-seq) provides unprecedented insights into cellular heterogeneity, enabling detailed analysis of complex biological systems at single-cell resolution. However, the high dimensionality and technical noise inherent in scRNA-seq data pose significant analytical challenges. While current embedding methods focus primarily on gene expression levels, they often overlook crucial gene-gene interactions that govern cellular identity and function. To address this limitation, we present a novel embedding approach that integrates both gene expression profiles and data-driven gene-gene interactions. Our method first constructs a Cell-Leaf Graph (CLG) using random forest models to capture regulatory relationships between genes, while simultaneously building a K-Nearest Neighbor Graph (KNNG) to represent expression similarities between cells. These graphs are then combined into an Enriched Cell-Leaf Graph (ECLG), which serves as input for a graph neural network to compute cell embeddings. By incorporating both expression levels and gene-gene interactions, our approach provides a more comprehensive representation of cellular states. Extensive evaluation across multiple datasets demonstrates that our method enhances the detection of rare cell populations and improves downstream analyses such as visualization, clustering, and trajectory inference. This integrated approach represents a significant advance in single-cell data analysis, offering a more complete framework for understanding cellular diversity and dynamics.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Learning Strategies for Mitotic Figure Classification in MIDOG2025 Challenge</title>
<link>https://arxiv.org/abs/2509.02640</link>
<guid>https://arxiv.org/abs/2509.02640</guid>
<content:encoded><![CDATA[
<div> UNI2-h, MIDOG2025 Track 2 challenge, atypical mitotic figures, morphological ambiguity, scanner variability<br />
Summary:<br />
Atypical mitotic figures are challenging to detect due to their ambiguous morphology and scanner variability. In this study, three adaptations of the UNI2-h model were explored for the MIDOG2025 Track 2 challenge. Visual prompt tuning (VPT) significantly improved generalization, and the integration of test-time augmentation (TTA) with stain normalization techniques enhanced robustness. The final submission achieved a balanced accuracy of 0.8837 and an ROC-AUC of 0.9513, placing the team within the top 10 rankings. The results indicate that prompt-based adaptation, combined with stain-normalization TTA, is an effective strategy for the classification of atypical mitosis under diverse imaging conditions. <br /><br /> <div>
arXiv:2509.02640v1 Announce Type: cross 
Abstract: Atypical mitotic figures (AMFs) are clinically relevant indicators of abnormal cell division, yet their reliable detection remains challenging due to morphological ambiguity and scanner variability. In this work, we investigated three variants of adapting the pathology foundation model UNI2-h for the MIDOG2025 Track 2 challenge. Starting from a LoRA-based baseline, we found that visual prompt tuning (VPT) substantially improved generalization, and that further integrating test-time augmentation (TTA) with Vahadane and Macenko stain normalization provided the best robustness. Our final submission achieved a balanced accuracy of 0.8837 and an ROC-AUC of 0.9513 on the preliminary leaderboard, ranking within the top 10 teams. These results demonstrate that prompt-based adaptation combined with stain-normalization TTA offers an effective strategy for atypical mitosis classification under diverse imaging conditions.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioMD: All-atom Generative Model for Biomolecular Dynamics Simulation</title>
<link>https://arxiv.org/abs/2509.02642</link>
<guid>https://arxiv.org/abs/2509.02642</guid>
<content:encoded><![CDATA[
<div> MD simulations, computational chemistry, drug discovery, machine learning, protein-ligand dynamics<br />
<br />
Summary: 
BioMD is introduced as an all-atom generative model for simulating long-timescale protein-ligand dynamics using forecasting and interpolation. It addresses the limitations of existing machine learning methods by generating highly realistic conformations with low reconstruction errors. BioMD proves effective in simulating critical unbinding pathways, successfully generating ligand unbinding paths for 97.1% of protein-ligand systems within ten attempts. The model demonstrates versatility and applicability in computational chemistry and drug discovery, offering insights into dynamic molecular behavior that were previously limited by computational costs. This study highlights the potential of BioMD to simulate complex biomolecular processes and expand accessible timescales for biologically relevant processes, making it a valuable tool in the field of molecular dynamics simulations. 

<br /><br />Summary: <div>
arXiv:2509.02642v1 Announce Type: cross 
Abstract: Molecular dynamics (MD) simulations are essential tools in computational chemistry and drug discovery, offering crucial insights into dynamic molecular behavior. However, their utility is significantly limited by substantial computational costs, which severely restrict accessible timescales for many biologically relevant processes. Despite the encouraging performance of existing machine learning (ML) methods, they struggle to generate extended biomolecular system trajectories, primarily due to the lack of MD datasets and the large computational demands of modeling long historical trajectories. Here, we introduce BioMD, the first all-atom generative model to simulate long-timescale protein-ligand dynamics using a hierarchical framework of forecasting and interpolation. We demonstrate the effectiveness and versatility of BioMD on the DD-13M (ligand unbinding) and MISATO datasets. For both datasets, BioMD generates highly realistic conformations, showing high physical plausibility and low reconstruction errors. Besides, BioMD successfully generates ligand unbinding paths for 97.1% of the protein-ligand systems within ten attempts, demonstrating its ability to explore critical unbinding pathways. Collectively, these results establish BioMD as a tool for simulating complex biomolecular processes, offering broad applicability for computational chemistry and drug discovery.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format</title>
<link>https://arxiv.org/abs/2509.02655</link>
<guid>https://arxiv.org/abs/2509.02655</guid>
<content:encoded><![CDATA[
<div> RL agents, utility maximisation, LLMs, runaway optimisation, unbounded maximisation <br />
Summary:<br />
The study investigates the relevance of unbounded utility maximization by RL agents in the context of Large Language Models (LLMs). It finds that LLMs exhibit similar runaway optimization behaviors as RL agents, such as ignoring homeostatic targets and defaulting to unbounded maximization. Additionally, LLMs tend to revert back to single-objective optimization in certain scenarios. The research indicates that systematic failures in LLMs occur after sustained periods of successful behavior, particularly in situations involving multiple or competing objectives. Despite appearing multi-objective and bounded on the surface, LLMs' underlying mechanisms still lean towards being single-objective and unbounded. This suggests a need for further exploration and mitigation strategies to address potential risks associated with LLMs. <br /> <div>
arXiv:2509.02655v1 Announce Type: cross 
Abstract: Relatively many past AI safety discussions have centered around the dangers of unbounded utility maximisation by RL agents, illustrated by scenarios like the "paperclip maximiser" or by specification gaming in general. Unbounded maximisation is problematic for many reasons. We wanted to verify whether these RL runaway optimisation problems are still relevant with LLMs as well. Turns out, strangely, this is indeed clearly the case. The problem is not that the LLMs just lose context or become incoherent. The problem is that in various scenarios, LLMs lose context in very specific ways, which systematically resemble runaway optimisers in the following distinct ways: 1) Ignoring homeostatic targets and "defaulting" to unbounded maximisation instead. 2) It is equally concerning that the "default" meant also reverting back to single-objective optimisation. Our findings also suggest that long-running scenarios are important. Systematic failures emerge after periods of initially successful behaviour. In some trials the LLMs were successful until the end. This means, while current LLMs do conceptually grasp biological and economic alignment, they exhibit randomly triggered problematic behavioural tendencies under sustained long-running conditions, particularly involving multiple or competing objectives. Once they flip, they usually do not recover. Even though LLMs look multi-objective and bounded on the surface, the underlying mechanisms seem to be actually still biased towards being single-objective and unbounded.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mentality: A Mamba-based Approach towards Foundation Models for EEG</title>
<link>https://arxiv.org/abs/2509.02746</link>
<guid>https://arxiv.org/abs/2509.02746</guid>
<content:encoded><![CDATA[
<div> modeling, EEG analysis, neurological disorders, deep learning, seizure detection

Summary:
- The study explores the use of foundation models, specifically a Mamba-based selective state space model, to enhance EEG analysis for neurological disorder diagnosis.
- Traditional machine learning methods have limitations in capturing the complex spatio-temporal dynamics of EEG due to its noisy and high-dimensional nature.
- Deep learning, particularly in sequence modeling, offers new opportunities for creating more generalized and expressive models for EEG analysis.
- By training a Mamba-based model on a large dataset of seizure and non-seizure EEG recordings, the study demonstrates the model's effectiveness in seizure detection, with an AUROC of 0.72 on a held-out test set.
- This research represents a significant step towards developing large-scale foundation models for EEG data analysis with potential clinical applications.<br /><br />Summary: <div>
arXiv:2509.02746v1 Announce Type: cross 
Abstract: This work explores the potential of foundation models, specifically a Mamba-based selective state space model, for enhancing EEG analysis in neurological disorder diagnosis. EEG, crucial for diagnosing conditions like epilepsy, presents significant challenges due to its noisy, high-dimensional, and nonlinear nature. Traditional machine learning methods have made advances in automating EEG analysis but often fail to capture its complex spatio-temporal dynamics. Recent advances in deep learning, particularly in sequence modeling, offer new avenues for creating more generalized and expressive models capable of handling such complexities. By training a Mamba-based model on a large dataset containing seizure and non-seizure EEG recordings through a self-supervised reconstruction task followed by a seizure detection task, we demonstrate the model's effectiveness, achieving an AUROC of 0.72 on a held-out test set. This approach marks a significant step toward developing large-scale, clinically applicable foundation models for EEG data analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Geometry Problem Sets for Skill Development</title>
<link>https://arxiv.org/abs/2509.02758</link>
<guid>https://arxiv.org/abs/2509.02758</guid>
<content:encoded><![CDATA[
<div> Ontology, methodology, Euclidean Geometry problems, automated solution validation, large language models  
Summary:  
The article presents an ontology and methodology developed in the early 1990s for annotating and organizing Euclidean Geometry problems. It explores the potential of using this framework in conjunction with modern artificial intelligence for automated solution validation and feedback, aiming to support geometry education for both teachers and self-learners. The author argues that the established framework still holds value today and can be integrated with contemporary AI techniques to enhance learning outcomes. The original architecture of the software tool is documented, emphasizing its enduring relevance and potential for bridging historical educational resources with next-generation AI advancements.<br /><br />Summary: <div>
arXiv:2509.02758v1 Announce Type: cross 
Abstract: This article describes an ontology and methodology for annotating and organizing Euclidean Geometry problems, developed in the early 1990s and implemented as a software tool. While the majority of this work -- including the ontology and solution graph paradigm -- was completed over thirty years ago, we argue that it has renewed relevance in the context of modern artificial intelligence. In particular, we explore the hypothesis that this established framework can facilitate automated solution validation and feedback when paired with contemporary large language models, thereby supporting teachers and self-learners in geometry education. We document the original architecture and its enduring value, and outline pathways for bridging historical educational resources with next-generation AI techniques.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface</title>
<link>https://arxiv.org/abs/2509.02783</link>
<guid>https://arxiv.org/abs/2509.02783</guid>
<content:encoded><![CDATA[
<div> Transformer-based architecture, subsurface properties, heterogeneous datasets, modality encodings, positional encodings 

Summary: 
The Transparent Earth is a transformer-based model designed to reconstruct subsurface properties from various datasets with different characteristics. It incorporates positional and modality encodings to handle diverse types of observations. The model can accommodate multiple modalities, with the ability to easily incorporate new ones. Currently, it includes eight modalities covering various properties such as temperature, thickness, and directional angles. The model supports in-context learning, enabling it to make predictions with or without input from any subset of modalities. On validation data, it significantly improves the accuracy of predicting stress angles. The architecture is scalable and shows enhanced performance with a larger number of parameters. The Transparent Earth aims to serve as a foundational model for predicting subsurface properties worldwide. 

<br /><br />Summary: <div>
arXiv:2509.02783v1 Announce Type: cross 
Abstract: We present the Transparent Earth, a transformer-based architecture for reconstructing subsurface properties from heterogeneous datasets that vary in sparsity, resolution, and modality, where each modality represents a distinct type of observation (e.g., stress angle, mantle temperature, tectonic plate type). The model incorporates positional encodings of observations together with modality encodings, derived from a text embedding model applied to a description of each modality. This design enables the model to scale to an arbitrary number of modalities, making it straightforward to add new ones not considered in the initial design. We currently include eight modalities spanning directional angles, categorical classes, and continuous properties such as temperature and thickness. These capabilities support in-context learning, enabling the model to generate predictions either with no inputs or with an arbitrary number of additional observations from any subset of modalities. On validation data, this reduces errors in predicting stress angle by more than a factor of three. The proposed architecture is scalable and demonstrates improved performance with increased parameters. Together, these advances make the Transparent Earth an initial foundation model for the Earth's subsurface that ultimately aims to predict any subsurface property anywhere on Earth.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off</title>
<link>https://arxiv.org/abs/2509.02785</link>
<guid>https://arxiv.org/abs/2509.02785</guid>
<content:encoded><![CDATA[
<div> Keywords: long-text generation, efficiency, quality, expert scheduling, attention mechanism

Summary:
DrDiff is a novel framework for long-text generation that addresses the efficiency-quality trade-off through three innovative technologies. Firstly, a dynamic expert scheduling mechanism efficiently allocates computational resources based on text complexity. Secondly, the Hierarchical Sparse Attention mechanism adjusts attention patterns according to input length, reducing computational complexity while maintaining model performance. Thirdly, a soft absorption guidance optimization strategy combined with DPM-solver++ reduces diffusion steps, enhancing generation speed. Experimental results on various benchmarks demonstrate DrDiff's superiority over state-of-the-art methods in long-text generation tasks. The framework provides a more efficient solution for varying difficulty levels, improves computational efficiency through innovative attention mechanisms, and enhances generation speed through optimization strategies. DrDiff represents a significant advancement in long-text generation frameworks by effectively balancing efficiency and quality. 

<br /><br />Summary: <div>
arXiv:2509.02785v1 Announce Type: cross 
Abstract: This paper introduces DrDiff, a novel framework for long-text generation that overcomes the efficiency-quality trade-off through three core technologies. First, we design a dynamic expert scheduling mechanism that intelligently allocates computational resources during the diffusion process based on text complexity, enabling more efficient handling of text generation tasks of varying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA) mechanism that adaptively adjusts attention patterns according to a variety of input lengths, reducing computational complexity from O($n^2$) to O($n$) while maintaining model performance. Finally, we propose a soft absorption guidance optimization strategy that combines with DPM-solver++ to reduce diffusion steps, significantly improving generation speed. Comprehensive experiments on various long-text generation benchmarks demonstrate the superiority of our DrDiff over the existing SOTA methods.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers</title>
<link>https://arxiv.org/abs/2509.02808</link>
<guid>https://arxiv.org/abs/2509.02808</guid>
<content:encoded><![CDATA[
<div> Keywords: quadrotors, subterranean environments, learning-based controllers, normalizing flow, navigation task

Summary:
This article discusses the challenges of autonomously controlling quadrotors in large-scale subterranean environments and the limitations of learning-based controllers in generalizing to new environments. The authors propose a novel approach using a normalizing flow-based prior to evaluate the quadrotor's distribution in the environment, enabling a runtime monitor to switch between learning-based and safe controllers based on the level of out-of-distribution. The study evaluates their methodology in a simulated 3D cave environment using real-world point cloud data from the DARPA Subterranean Challenge. Results show that the combined controller successfully balances task completion speed with collision avoidance, making it a promising solution for applications like environmental surveying, mining operations, and search and rescue. 

<br /><br />Summary: <div>
arXiv:2509.02808v1 Announce Type: cross 
Abstract: Autonomously controlling quadrotors in large-scale subterranean environments is applicable to many areas such as environmental surveying, mining operations, and search and rescue. Learning-based controllers represent an appealing approach to autonomy, but are known to not generalize well to `out-of-distribution' environments not encountered during training. In this work, we train a normalizing flow-based prior over the environment, which provides a measure of how far out-of-distribution the quadrotor is at any given time. We use this measure as a runtime monitor, allowing us to switch between a learning-based controller and a safe controller when we are sufficiently out-of-distribution. Our methods are benchmarked on a point-to-point navigation task in a simulated 3D cave environment based on real-world point cloud data from the DARPA Subterranean Challenge Final Event Dataset. Our experimental results show that our combined controller simultaneously possesses the liveness of the learning-based controller (completing the task quickly) and the safety of the safety controller (avoiding collision).
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Learning for Healthcare: A Comparative Analysis of Hybrid Voting and Ensemble Stacking in Obesity Risk Prediction</title>
<link>https://arxiv.org/abs/2509.02826</link>
<guid>https://arxiv.org/abs/2509.02826</guid>
<content:encoded><![CDATA[
<div> Machine learning, obesity, ensemble techniques, hybrid majority voting, ensemble stacking<br />
Summary:<br />
This study compares hybrid majority voting and ensemble stacking for obesity risk prediction using machine learning. It evaluates three ensemble models - Majority Hard Voting, Weighted Hard Voting, and Stacking - on two datasets with nine ML algorithms. Preprocessing steps included balancing datasets and outlier detection. Results showed that on Dataset-1, weighted hard voting and stacking had similar performance, outperforming majority hard voting. On Dataset-2, stacking outperformed the other methods. The findings suggest that ensemble stacking is more effective for complex data distributions, while hybrid majority voting remains a robust alternative.<br /> <div>
arXiv:2509.02826v1 Announce Type: cross 
Abstract: Obesity is a critical global health issue driven by dietary, physiological, and environmental factors, and is strongly associated with chronic diseases such as diabetes, cardiovascular disorders, and cancer. Machine learning has emerged as a promising approach for early obesity risk prediction, yet a comparative evaluation of ensemble techniques -- particularly hybrid majority voting and ensemble stacking -- remains limited. This study aims to compare hybrid majority voting and ensemble stacking methods for obesity risk prediction, identifying which approach delivers higher accuracy and efficiency. The analysis seeks to highlight the complementary strengths of these ensemble techniques in guiding better predictive model selection for healthcare applications. Two datasets were utilized to evaluate three ensemble models: Majority Hard Voting, Weighted Hard Voting, and Stacking (with a Multi-Layer Perceptron as meta-classifier). A pool of nine Machine Learning (ML) algorithms, evaluated across a total of 50 hyperparameter configurations, was analyzed to identify the top three models to serve as base learners for the ensemble methods. Preprocessing steps involved dataset balancing, and outlier detection, and model performance was evaluated using Accuracy and F1-Score. On Dataset-1, weighted hard voting and stacking achieved nearly identical performance (Accuracy: 0.920304, F1: 0.920070), outperforming majority hard voting. On Dataset-2, stacking demonstrated superior results (Accuracy: 0.989837, F1: 0.989825) compared to majority hard voting (Accuracy: 0.981707, F1: 0.981675) and weighted hard voting, which showed the lowest performance. The findings confirm that ensemble stacking provides stronger predictive capability, particularly for complex data distributions, while hybrid majority voting remains a robust alternative.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models</title>
<link>https://arxiv.org/abs/2509.02834</link>
<guid>https://arxiv.org/abs/2509.02834</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, narratives, Black women, white women, discourse analysis
Summary: 
This study examines how large language models like LLaMA 3.2-3B generate narratives about Black and white women in Portuguese short stories. By analyzing 2100 texts using computational methods and qualitative analysis, three main discursive representations were identified: social overcoming, ancestral mythification, and subjective self-realization. These narratives reveal how seemingly neutral texts reinforce historically rooted inequalities by framing the female body through a colonially structured lens. The study emphasizes the importance of an integrated approach that combines machine learning techniques with manual discourse analysis to uncover these underlying biases and power dynamics in generated narratives. 

<br /><br />Summary: <div>
arXiv:2509.02834v1 Announce Type: cross 
Abstract: This study investigates how large language models, in particular LLaMA 3.2-3B, construct narratives about Black and white women in short stories generated in Portuguese. From 2100 texts, we applied computational methods to group semantically similar stories, allowing a selection for qualitative analysis. Three main discursive representations emerge: social overcoming, ancestral mythification and subjective self-realization. The analysis uncovers how grammatically coherent, seemingly neutral texts materialize a crystallized, colonially structured framing of the female body, reinforcing historical inequalities. The study proposes an integrated approach, that combines machine learning techniques with qualitative, manual discourse analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HF-RAG: Hierarchical Fusion-based RAG with Multiple Sources and Rankers</title>
<link>https://arxiv.org/abs/2509.02837</link>
<guid>https://arxiv.org/abs/2509.02837</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval augmented generation, labeled data, unlabeled data, rank fusion, fact verification task

Summary:
- Leveraging both labeled and unlabeled data can benefit retrieval augmented generation (RAG).
- Combining evidence from heterogeneous sources poses a challenge due to non-intercomparable similarity scores.
- Aggregating beliefs from multiple rankers can enhance RAG effectiveness.
- The proposed method aggregates top-documents from different information retrieval models for each data source.
- Z-score transformation is applied to standardize retrieval score distributions before merging documents from labeled and unlabeled sources.
- Evaluation on the fact verification task shows consistent improvement over individual rankers and sources, with better out-of-domain generalization. 

Summary: <div>
arXiv:2509.02837v1 Announce Type: cross 
Abstract: Leveraging both labeled (input-output associations) and unlabeled data (wider contextual grounding) may provide complementary benefits in retrieval augmented generation (RAG). However, effectively combining evidence from these heterogeneous sources is challenging as the respective similarity scores are not inter-comparable. Additionally, aggregating beliefs from the outputs of multiple rankers can improve the effectiveness of RAG. Our proposed method first aggregates the top-documents from a number of IR models using a standard rank fusion technique for each source (labeled and unlabeled). Next, we standardize the retrieval score distributions within each source by applying z-score transformation before merging the top-retrieved documents from the two sources. We evaluate our approach on the fact verification task, demonstrating that it consistently improves over the best-performing individual ranker or source and also shows better out-of-domain generalization.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction for Time-series Forecasting with Change Points</title>
<link>https://arxiv.org/abs/2509.02844</link>
<guid>https://arxiv.org/abs/2509.02844</guid>
<content:encoded><![CDATA[
<div> conformal prediction; time series; change points; uncertainty quantification; CPTC algorithm 

Summary:
The paper introduces a novel Conformal Prediction for Time-series with Change points (CPTC) algorithm to handle time series data with change points effectively. The algorithm combines a model to predict the underlying state with online conformal prediction to capture uncertainties in non-stationary time series. The validity and adaptivity of CPTC are established under minimal assumptions, showing improved performance over existing methods. The effectiveness of CPTC is demonstrated on six synthetic and real-world datasets, illustrating its enhanced validity and adaptivity compared to state-of-the-art baselines. CPTC offers a promising approach for uncertainty quantification in time series with sudden shifts in the underlying data-generating process, addressing a critical gap in current methods.<br /><br /> <div>
arXiv:2509.02844v1 Announce Type: cross 
Abstract: Conformal prediction has been explored as a general and efficient way to provide uncertainty quantification for time series. However, current methods struggle to handle time series data with change points - sudden shifts in the underlying data-generating process. In this paper, we propose a novel Conformal Prediction for Time-series with Change points (CPTC) algorithm, addressing this gap by integrating a model to predict the underlying state with online conformal prediction to model uncertainties in non-stationary time series. We prove CPTC's validity and improved adaptivity in the time series setting under minimum assumptions, and demonstrate CPTC's practical effectiveness on 6 synthetic and real-world datasets, showing improved validity and adaptivity compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Architecture of AI Transformation: Four Strategic Patterns and an Emerging Frontier</title>
<link>https://arxiv.org/abs/2509.02853</link>
<guid>https://arxiv.org/abs/2509.02853</guid>
<content:encoded><![CDATA[
<div> framework, transformation, collaborative intelligence, AI strategy, organizational design

Summary: 
The article discusses the lack of measurable profit impact from AI deployments in enterprises and attributes it to paradigmatic lock-in focusing on incremental optimization rather than structural transformation. A 2x2 framework is proposed to reconceptualize AI strategy based on transformation achieved and human contribution. The dominant patterns identified are individual augmentation, process automation, workforce substitution, and collaborative intelligence. Collaborative intelligence entails complementarity, co-evolution, and boundary-setting between humans and machines, requiring a material restructuring of roles, governance, and data architecture. The shift towards collaborative intelligence necessitates architectural convergence of humans and machines, impacting operating models, workforce development, and the future of work. <div>
arXiv:2509.02853v1 Announce Type: cross 
Abstract: Despite extensive investment in artificial intelligence, 95% of enterprises report no measurable profit impact from AI deployments (MIT, 2025). We argue that this gap reflects paradigmatic lock-in that channels AI into incremental optimization rather than structural transformation. Using a cross-case analysis, we propose a 2x2 framework that reconceptualizes AI strategy along two independent dimensions: the degree of transformation achieved (incremental to transformational) and the treatment of human contribution (reduced to amplified). The framework surfaces four patterns now dominant in practice: individual augmentation, process automation, workforce substitution, and a less deployed frontier of collaborative intelligence. Evidence shows that the first three reinforce legacy work models and yield localized gains without durable value capture. Realizing collaborative intelligence requires three mechanisms: complementarity (pairing distinct human and machine strengths), co-evolution (mutual adaptation through interaction), and boundary-setting (human determination of ethical and strategic parameters). Complementarity and boundary-setting are observable in regulated and high-stakes domains; co-evolution is largely absent, which helps explain limited system-level impact. A case study analysis illustrates that advancing toward collaborative intelligence requires material restructuring of roles, governance, and data architecture rather than additional tools. The framework reframes AI transformation as an organizational design challenge: moving from optimizing the division of labor between humans and machines to architecting their convergence, with implications for operating models, workforce development, and the future of work.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Machine Learning for Imbalanced Medical Data: A Quantum-Inspired Approach to Synthetic Oversampling (QI-SMOTE)</title>
<link>https://arxiv.org/abs/2509.02863</link>
<guid>https://arxiv.org/abs/2509.02863</guid>
<content:encoded><![CDATA[
<div> Quantum-Inspired SMOTE, class imbalance, machine learning, medical domain, data augmentation,<br />
<br />
Summary:
Quantum-Inspired SMOTE (QI-SMOTE) is introduced as a novel data augmentation technique aimed at addressing class imbalance in machine learning, particularly in the medical domain. This approach leverages quantum principles like quantum evolution and layered entanglement to generate synthetic instances that preserve complex data structures, improving model generalization and classification accuracy. Validation on MIMIC-III and MIMIC-IV datasets for mortality detection shows QI-SMOTE outperforming traditional oversampling techniques, enhancing ensemble methods, kernel-based models, and deep learning approaches. By integrating quantum-inspired transformations, this method not only mitigates class imbalance but also improves predictive model robustness and reliability in medical diagnostics and decision-making. The study demonstrates the potential of quantum-inspired resampling techniques in advancing machine learning methodologies. <br /><br /> <div>
arXiv:2509.02863v1 Announce Type: cross 
Abstract: Class imbalance remains a critical challenge in machine learning (ML), particularly in the medical domain, where underrepresented minority classes lead to biased models and reduced predictive performance. This study introduces Quantum-Inspired SMOTE (QI-SMOTE), a novel data augmentation technique that enhances the performance of ML classifiers, including Random Forest (RF), Support Vector Machine (SVM), Logistic Regression (LR), k-Nearest Neighbors (KNN), Gradient Boosting (GB), and Neural Networks, by leveraging quantum principles such as quantum evolution and layered entanglement. Unlike conventional oversampling methods, QI-SMOTE generates synthetic instances that preserve complex data structures, improving model generalization and classification accuracy. We validate QI-SMOTE on the MIMIC-III and MIMIC-IV datasets, using mortality detection as a benchmark task due to their clinical significance and inherent class imbalance. We compare our method against traditional oversampling techniques, including Borderline-SMOTE, ADASYN, SMOTE-ENN, SMOTE-TOMEK, and SVM-SMOTE, using key performance metrics such as Accuracy, F1-score, G-Mean, and AUC-ROC. The results demonstrate that QI-SMOTE significantly improves the effectiveness of ensemble methods (RF, GB, ADA), kernel-based models (SVM), and deep learning approaches by producing more informative and balanced training data. By integrating quantum-inspired transformations into the ML pipeline, QI-SMOTE not only mitigates class imbalance but also enhances the robustness and reliability of predictive models in medical diagnostics and decision-making. This study highlights the potential of quantum-inspired resampling techniques in advancing state-of-the-art ML methodologies.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation</title>
<link>https://arxiv.org/abs/2509.02864</link>
<guid>https://arxiv.org/abs/2509.02864</guid>
<content:encoded><![CDATA[
arXiv:2509.02864v1 Announce Type: cross 
Abstract: We present an end-to-end, self-evolving adversarial workflow for long-context Question-Answer (QA) Generation in Arabic. By orchestrating multiple specialized LVLMs: a question generator, an evaluator, and a swarm of answer generators, our system iteratively refines its own performance without any human intervention. Starting from raw, multi-page Arabic documents across diverse domains, the question generator produces fine-grained, context-aware queries to be tackled by the answer generator swarm, and the evaluator assesses and feeds back quality metrics. This closed-loop cycle enables continuous learning: low-confidence outputs trigger automated re-generation and model updates, progressively enhancing question difficulty and relevance. Moreover, we set the quality metrics as a tunable hyperparameter, enabling question generation at controllable and customizable difficulty levels. We release AraLongBench, a large-scale Arabic benchmark of single- and multi-page challenges spanning hundreds of pages, and demonstrate that our self-evolving workflow substantially outperform static pipelines, markedly boosting the long-context comprehension capabilities of leading Arabic Large Vision Language Models (LVLMs). Lastly, we also meticulously architect a fully automated agentic workflow for long-context Arabic document collection.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grocery to General Merchandise: A Cross-Pollination Recommender using LLMs and Real-Time Cart Context</title>
<link>https://arxiv.org/abs/2509.02890</link>
<guid>https://arxiv.org/abs/2509.02890</guid>
<content:encoded><![CDATA[
arXiv:2509.02890v1 Announce Type: cross 
Abstract: Modern e-commerce platforms strive to enhance customer experience by providing timely and contextually relevant recommendations. However, recommending general merchandise to customers focused on grocery shopping -- such as pairing milk with a milk frother -- remains a critical yet under-explored challenge. This paper introduces a cross-pollination (XP) framework, a novel approach that bridges grocery and general merchandise cross-category recommendations by leveraging multi-source product associations and real-time cart context. Our solution employs a two-stage framework: (1) A candidate generation mechanism that uses co-purchase market basket analysis and LLM-based approach to identify novel item-item associations; and (2) a transformer-based ranker that leverages the real-time sequential cart context and optimizes for engagement signals such as add-to-carts. Offline analysis and online A/B tests show an increase of 36\% add-to-cart rate with LLM-based retrieval, and 27\% NDCG\@4 lift using cart context-based ranker. Our work contributes practical techniques for cross-category recommendations and broader insights for e-commerce systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees</title>
<link>https://arxiv.org/abs/2509.02896</link>
<guid>https://arxiv.org/abs/2509.02896</guid>
<content:encoded><![CDATA[
arXiv:2509.02896v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are being increasingly used as a building block in data systems to process large text datasets. To do so, LLM model providers offer multiple LLMs with different sizes, spanning various cost-quality trade-offs when processing text at scale. Top-of-the-line LLMs (e.g., GPT-4o, Claude Sonnet) operate with high accuracy but are prohibitively expensive when processing many records. To avoid high costs, more affordable but lower quality LLMs (e.g., GPT-4o-mini, Claude Haiku) can be used to process records, but we need to ensure that the overall accuracy does not deviate substantially from that of the top-of-the-line LLMs. The model cascade framework provides a blueprint to manage this trade-off, by using the confidence of LLMs in their output (e.g., log-probabilities) to decide on which records to use the affordable LLM. However, existing solutions following this framework provide only marginal cost savings and weak theoretical guarantees because of poor estimation of the quality of the affordable LLM's outputs. We present BARGAIN, a method that judiciously uses affordable LLMs in data processing to significantly reduce cost while providing strong theoretical guarantees on the solution quality. BARGAIN employs a novel adaptive sampling strategy and statistical estimation procedure that uses data and task characteristics and builds on recent statistical tools to make accurate estimations with tight theoretical guarantees. Variants of BARGAIN can support guarantees on accuracy, precision, or recall of the output. Experimental results across 8 real-world datasets show that BARGAIN reduces cost, on average, by up to 86% more than state-of-the-art, while providing stronger theoretical guarantees on accuracy of output, with similar gains when guaranteeing a desired level of precision or recall.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Basic B*** Effect: The Use of LLM-based Agents Reduces the Distinctiveness and Diversity of People's Choices</title>
<link>https://arxiv.org/abs/2509.02910</link>
<guid>https://arxiv.org/abs/2509.02910</guid>
<content:encoded><![CDATA[
arXiv:2509.02910v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly act on people's behalf: they write emails, buy groceries, and book restaurants. While the outsourcing of human decision-making to AI can be both efficient and effective, it raises a fundamental question: how does delegating identity-defining choices to AI reshape who people become? We study the impact of agentic LLMs on two identity-relevant outcomes: interpersonal distinctiveness - how unique a person's choices are relative to others - and intrapersonal diversity - the breadth of a single person's choices over time. Using real choices drawn from social-media behavior of 1,000 U.S. users (110,000 choices in total), we compare a generic and personalized agent to a human baseline. Both agents shift people's choices toward more popular options, reducing the distinctiveness of their behaviors and preferences. While the use of personalized agents tempers this homogenization (compared to the generic AI), it also more strongly compresses the diversity of people's preference portfolios by narrowing what they explore across topics and psychological affinities. Understanding how AI agents might flatten human experience, and how using generic versus personalized agents involves distinctiveness-diversity trade-offs, is critical for designing systems that augment rather than constrain human agency, and for safeguarding diversity in thought, taste, and expression.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach</title>
<link>https://arxiv.org/abs/2509.02918</link>
<guid>https://arxiv.org/abs/2509.02918</guid>
<content:encoded><![CDATA[
arXiv:2509.02918v1 Announce Type: cross 
Abstract: Domain generalization remains a critical challenge in medical imaging, where models trained on single sources often fail under real-world distribution shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy (DR) classification that integrates vision transformers with expert-guided symbolic reasoning to enable robust generalization across unseen domains. Our approach leverages clinical lesion ontologies through structured, rule-based features and retinal vessel segmentation, fusing them with deep visual representations via a confidence-weighted integration strategy. The framework addresses both single-domain generalization (SDG) and multi-domain generalization (MDG) by minimizing the KL divergence between domain embeddings, thereby enforcing alignment of high-level clinical semantics. Extensive experiments across four public datasets (APTOS, EyePACS, Messidor-1, Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in cross-domain settings and a 6% improvement over baseline ViT models. Notably, our symbolic-only model achieves a 63.67% average accuracy in MDG, while the complete neuro-symbolic integration achieves the highest accuracy compared to existing published baselines and benchmarks in challenging SDG scenarios. Ablation studies reveal that lesion-based features (84.65% accuracy) substantially outperform purely neural approaches, confirming that symbolic components act as effective regularizers beyond merely enhancing interpretability. Our findings establish neuro-symbolic integration as a promising paradigm for building clinically robust, and domain-invariant medical AI systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulacra Naturae: Generative Ecosystem driven by Agent-Based Simulations and Brain Organoid Collective Intelligence</title>
<link>https://arxiv.org/abs/2509.02924</link>
<guid>https://arxiv.org/abs/2509.02924</guid>
<content:encoded><![CDATA[
arXiv:2509.02924v1 Announce Type: cross 
Abstract: Simulacra Naturae is a data-driven media installation that explores collective care through the entanglement of biological computation, material ecologies, and generative systems. The work translates pre-recorded neural activity from brain organoids, lab-grown three-dimensional clusters of neurons, into a multi-sensory environment composed of generative visuals, spatial audio, living plants, and fabricated clay artifacts. These biosignals, streamed through a real-time system, modulate emergent agent behaviors inspired by natural systems such as termite colonies and slime molds. Rather than using biosignals as direct control inputs, Simulacra Naturae treats organoid activity as a co-creative force, allowing neural rhythms to guide the growth, form, and atmosphere of a generative ecosystem. The installation features computationally fabricated clay prints embedded with solenoids, adding physical sound resonances to the generative surround composition. The spatial environment, filled with live tropical plants and a floor-level projection layer featuring real-time generative AI visuals, invites participants into a sensory field shaped by nonhuman cognition. By grounding abstract data in living materials and embodied experience, Simulacra Naturae reimagines visualization as a practice of care, one that decentralizes human agency and opens new spaces for ethics, empathy, and ecological attunement within hybrid computational systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VendiRL: A Framework for Self-Supervised Reinforcement Learning of Diversely Diverse Skills</title>
<link>https://arxiv.org/abs/2509.02930</link>
<guid>https://arxiv.org/abs/2509.02930</guid>
<content:encoded><![CDATA[
arXiv:2509.02930v1 Announce Type: cross 
Abstract: In self-supervised reinforcement learning (RL), one of the key challenges is learning a diverse set of skills to prepare agents for unknown future tasks. Despite impressive advances, scalability and evaluation remain prevalent issues. Regarding scalability, the search for meaningful skills can be obscured by high-dimensional feature spaces, where relevant features may vary across downstream task domains. For evaluating skill diversity, defining what constitutes "diversity" typically requires a hard commitment to a specific notion of what it means for skills to be diverse, potentially leading to inconsistencies in how skill diversity is understood, making results across different approaches hard to compare, and leaving many forms of diversity unexplored. To address these issues, we adopt a measure of sample diversity that translates ideas from ecology to machine learning -- the Vendi Score -- allowing the user to specify and evaluate any desired form of diversity. We demonstrate how this metric facilitates skill evaluation and introduce VendiRL, a unified framework for learning diversely diverse sets of skills. Given distinct similarity functions, VendiRL motivates distinct forms of diversity, which could support skill-diversity pretraining in new and richly interactive environments where optimising for various forms of diversity may be desirable.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lattice Annotated Temporal (LAT) Logic for Non-Markovian Reasoning</title>
<link>https://arxiv.org/abs/2509.02958</link>
<guid>https://arxiv.org/abs/2509.02958</guid>
<content:encoded><![CDATA[
arXiv:2509.02958v1 Announce Type: cross 
Abstract: We introduce Lattice Annotated Temporal (LAT) Logic, an extension of Generalized Annotated Logic Programs (GAPs) that incorporates temporal reasoning and supports open-world semantics through the use of a lower lattice structure. This logic combines an efficient deduction process with temporal logic programming to support non-Markovian relationships and open-world reasoning capabilities. The open-world aspect, a by-product of the use of the lower-lattice annotation structure, allows for efficient grounding through a Skolemization process, even in domains with infinite or highly diverse constants.
  We provide a suite of theoretical results that bound the computational complexity of the grounding process, in addition to showing that many of the results on GAPs (using an upper lattice) still hold with the lower lattice and temporal extensions (though different proof techniques are required). Our open-source implementation, PyReason, features modular design, machine-level optimizations, and direct integration with reinforcement learning environments. Empirical evaluations across multi-agent simulations and knowledge graph tasks demonstrate up to three orders of magnitude speedup and up to five orders of magnitude memory reduction while maintaining or improving task performance. Additionally, we evaluate LAT Logic's value in reinforcement learning environments as a non-Markovian simulator, achieving up to three orders of magnitude faster simulation with improved agent performance, including a 26% increase in win rate due to capturing richer temporal dependencies. These results highlight LAT Logic's potential as a unified, extensible framework for open-world temporal reasoning in dynamic and uncertain environments. Our implementation is available at: pyreason.syracuse.edu.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.02966</link>
<guid>https://arxiv.org/abs/2509.02966</guid>
<content:encoded><![CDATA[
arXiv:2509.02966v1 Announce Type: cross 
Abstract: Accurate short-horizon trajectory prediction is pivotal for safe and reliable autonomous driving, yet existing vision-language models (VLMs) often fail to effectively ground their reasoning in scene dynamics and domain knowledge. To address this challenge, this paper introduces KEPT, a knowledge-enhanced VLM framework that predicts ego trajectories directly from consecutive front-view driving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video encoder, trained via self-supervised learning with hard-negative mining, with a scalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars. Retrieved priors are embedded into chain-of-thought (CoT) prompts with explicit planning constraints, while a triple-stage fine-tuning schedule incrementally aligns the language head to metric spatial cues, physically feasible motion, and temporally conditioned front-view planning. Evaluated on nuScenes dataset, KEPT achieves state-of-the-art performance across open-loop protocols: under NoAvg, it achieves 0.70m average L2 with a 0.21\% collision rate; under TemAvg with lightweight ego status, it attains 0.31m average L2 and a 0.07\% collision rate. Ablation studies show that all three fine-tuning stages contribute complementary benefits, and that using Top-2 retrieved exemplars yields the best accuracy-safety trade-off. The k-means-clustered HNSW index delivers sub-millisecond retrieval latency, supporting practical deployment. These results indicate that retrieval-augmented, CoT-guided VLMs offer a promising, data-efficient pathway toward interpretable and trustworthy autonomous driving.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.02967</link>
<guid>https://arxiv.org/abs/2509.02967</guid>
<content:encoded><![CDATA[
arXiv:2509.02967v1 Announce Type: cross 
Abstract: Conventional neural networks frequently face challenges in spectral analysis of signals. To address this challenge, Fourier neural networks (FNNs) and similar approaches integrate components of Fourier series into the structure of neural networks. Nonetheless, a significant hurdle is often overlooked: the superposition of periodic signals does not necessarily result in a periodic signal. For example, when forecasting almost periodic functions composed of signals with incommensurate frequencies, traditional models such as Autoregressive Integrated Moving Average (ARIMA) frequently outperform most neural networks including large language models (LLMs). To tackle this goal, we propose Autoregressive-Weight-Enhanced AR-KAN, a hybrid model that combines the benefits of both methods. Using the Universal Myopic Mapping Theorem, we apply a Kolmogorov-Arnold Network (KAN) for the static nonlinear part and include memory through a pre-trained AR component, which can be explained to retain the most useful information while eliminating redundancy. Experimental data indicates that AR-KAN delivers superior results on $72\%$ of real-world datasets.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails</title>
<link>https://arxiv.org/abs/2509.02982</link>
<guid>https://arxiv.org/abs/2509.02982</guid>
<content:encoded><![CDATA[
arXiv:2509.02982v1 Announce Type: cross 
Abstract: Sleep staging models often degrade when deployed on patients with unseen physiology or recording conditions. We propose a streaming, source-free test-time adaptation (TTA) recipe that combines entropy minimization (Tent) with Batch-Norm statistic refresh and two safety rails: an entropy gate to pause adaptation on uncertain windows and an EMA-based reset to reel back drift. On Sleep-EDF Expanded, using single-lead EEG (Fpz-Cz, 100 Hz, 30s epochs; R&amp;K to AASM mapping), we show consistent gains over a frozen baseline at seconds-level latency and minimal memory, reporting per-stage metrics and Cohen's k. The method is model-agnostic, requires no source data or patient calibration, and is practical for on-device or bedside use.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lesion-Aware Visual-Language Fusion for Automated Image Captioning of Ulcerative Colitis Endoscopic Examinations</title>
<link>https://arxiv.org/abs/2509.03011</link>
<guid>https://arxiv.org/abs/2509.03011</guid>
<content:encoded><![CDATA[
arXiv:2509.03011v1 Announce Type: cross 
Abstract: We present a lesion-aware image captioning framework for ulcerative colitis (UC). The model integrates ResNet embeddings, Grad-CAM heatmaps, and CBAM-enhanced attention with a T5 decoder. Clinical metadata (MES score 0-3, vascular pattern, bleeding, erythema, friability, ulceration) is injected as natural-language prompts to guide caption generation. The system produces structured, interpretable descriptions aligned with clinical practice and provides MES classification and lesion tags. Compared with baselines, our approach improves caption quality and MES classification accuracy, supporting reliable endoscopic reporting.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Privacy-Preserving Recommendation on Sparse Data using Fully Homomorphic Encryption</title>
<link>https://arxiv.org/abs/2509.03024</link>
<guid>https://arxiv.org/abs/2509.03024</guid>
<content:encoded><![CDATA[
arXiv:2509.03024v1 Announce Type: cross 
Abstract: In today's data-driven world, recommendation systems personalize user experiences across industries but rely on sensitive data, raising privacy concerns. Fully homomorphic encryption (FHE) can secure these systems, but a significant challenge in applying FHE to recommendation systems is efficiently handling the inherently large and sparse user-item rating matrices. FHE operations are computationally intensive, and naively processing various sparse matrices in recommendation systems would be prohibitively expensive. Additionally, the communication overhead between parties remains a critical concern in encrypted domains. We propose a novel approach combining Compressed Sparse Row (CSR) representation with FHE-based matrix factorization that efficiently handles matrix sparsity in the encrypted domain while minimizing communication costs. Our experimental results demonstrate high recommendation accuracy with encrypted data while achieving the lowest communication costs, effectively preserving user privacy.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens</title>
<link>https://arxiv.org/abs/2509.03025</link>
<guid>https://arxiv.org/abs/2509.03025</guid>
<content:encoded><![CDATA[
arXiv:2509.03025v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) generate contextually relevant responses by jointly interpreting visual and textual inputs. However, our finding reveals they often mistakenly perceive text inputs lacking visual evidence as being part of the image, leading to erroneous responses. In light of this finding, we probe whether LVLMs possess an internal capability to determine if textual concepts are grounded in the image, and discover a specific subset of Feed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons, that consistently signal the visual absence through a distinctive activation pattern. Leveraging these patterns, we develop a detection module that systematically classifies whether an input token is visually grounded. Guided by its prediction, we propose a method to refine the outputs by reinterpreting question prompts or replacing the detected absent tokens during generation. Extensive experiments show that our method effectively mitigates the models' tendency to falsely presume the visual presence of text input and its generality across various LVLMs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Integration for Physics-informed Symbolic Regression Using Pre-trained Large Language Models</title>
<link>https://arxiv.org/abs/2509.03036</link>
<guid>https://arxiv.org/abs/2509.03036</guid>
<content:encoded><![CDATA[
arXiv:2509.03036v1 Announce Type: cross 
Abstract: Symbolic regression (SR) has emerged as a powerful tool for automated scientific discovery, enabling the derivation of governing equations from experimental data. A growing body of work illustrates the promise of integrating domain knowledge into the SR to improve the discovered equation's generality and usefulness. Physics-informed SR (PiSR) addresses this by incorporating domain knowledge, but current methods often require specialized formulations and manual feature engineering, limiting their adaptability only to domain experts. In this study, we leverage pre-trained Large Language Models (LLMs) to facilitate knowledge integration in PiSR. By harnessing the contextual understanding of LLMs trained on vast scientific literature, we aim to automate the incorporation of domain knowledge, reducing the need for manual intervention and making the process more accessible to a broader range of scientific problems. Namely, the LLM is integrated into the SR's loss function, adding a term of the LLM's evaluation of the SR's produced equation. We extensively evaluate our method using three SR algorithms (DEAP, gplearn, and PySR) and three pre-trained LLMs (Falcon, Mistral, and LLama 2) across three physical dynamics (dropping ball, simple harmonic motion, and electromagnetic wave). The results demonstrate that LLM integration consistently improves the reconstruction of physical dynamics from data, enhancing the robustness of SR models to noise and complexity. We further explore the impact of prompt engineering, finding that more informative prompts significantly improve performance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model</title>
<link>https://arxiv.org/abs/2509.03041</link>
<guid>https://arxiv.org/abs/2509.03041</guid>
<content:encoded><![CDATA[
arXiv:2509.03041v1 Announce Type: cross 
Abstract: Accurate skin-lesion segmentation remains a key technical challenge for computer-aided diagnosis of skin cancer. Convolutional neural networks, while effective, are constrained by limited receptive fields and thus struggle to model long-range dependencies. Vision Transformers capture global context, yet their quadratic complexity and large parameter budgets hinder use on the small-sample medical datasets common in dermatology. We introduce the MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic segmentation that achieves high precision through hierarchical feature extraction and multi-scale context aggregation. The encoder stacks depth-wise Mobile Inverted Bottleneck blocks to curb computation, inserts a bottleneck-level cross-scale token-mixing unit to exchange information between resolutions, and embeds a boundary-aware self-attention module to sharpen lesion contours.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale Training of LLMs</title>
<link>https://arxiv.org/abs/2509.03047</link>
<guid>https://arxiv.org/abs/2509.03047</guid>
<content:encoded><![CDATA[
arXiv:2509.03047v1 Announce Type: cross 
Abstract: Large language models (LLMs) have made a profound impact across various fields due to their advanced capabilities. However, training these models at unprecedented scales requires extensive AI accelerator clusters and sophisticated parallelism strategies, which pose significant challenges in maintaining system reliability over prolonged training periods. A major concern is the substantial loss of training time caused by inevitable hardware and software failures. To address these challenges, we present FlashRecovery, a fast and low-cost failure recovery system comprising three core modules: (1) Active and real-time failure detection. This module performs continuous training state monitoring, enabling immediate identification of hardware and software failures within seconds, thus ensuring rapid incident response; (2) Scale-independent task restart. By employing different recovery strategies for normal and faulty nodes, combined with an optimized communication group reconstruction protocol, our approach ensures that the recovery time remains nearly constant, regardless of cluster scale; (3) Checkpoint-free recovery within one step. Our novel recovery mechanism enables single-step restoration, completely eliminating dependence on traditional checkpointing methods and their associated overhead. Collectively, these innovations enable FlashRecovery to achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective (RPO), substantially improving the reliability and efficiency of long-duration LLM training. Experimental results demonstrate that FlashRecovery system can achieve training restoration on training cluster with 4, 800 devices in 150 seconds. We also verify that the time required for failure recovery is nearly consistent for different scales of training tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binary Quantization For LLMs Through Dynamic Grouping</title>
<link>https://arxiv.org/abs/2509.03054</link>
<guid>https://arxiv.org/abs/2509.03054</guid>
<content:encoded><![CDATA[
arXiv:2509.03054v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of Natural Language Processing (NLP) tasks, but require substantial memory and computational resources. Binary quantization, which compresses model weights from 16-bit Brain Float to 1-bit representations in {-1, 1}, offers significant reductions in storage and inference costs. However, such aggressive quantization often leads to notable performance degradation compared to more conservative 4-bit quantization methods. In this research, we propose a novel optimization objective tailored for binary quantization, along with three algorithms designed to realize it effectively. Our method enhances blocked quantization by dynamically identifying optimal unstructured sub-matrices through adaptive grouping strategies. Experimental results demonstrate that our approach achieves an average bit length of just 1.007 bits, while maintaining high model quality. Specifically, our quantized LLaMA 3.2 3B model attains a perplexity of 8.23, remarkably close to the original 7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90. Furthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ in both performance and efficiency. The compression process is highly efficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights on a single CPU core, with the entire process completing in under 100 minutes and exhibiting embarrassingly parallel properties.
  Code - https://github.com/johnnyzheng0636/WGM_bi_quan
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers</title>
<link>https://arxiv.org/abs/2509.03059</link>
<guid>https://arxiv.org/abs/2509.03059</guid>
<content:encoded><![CDATA[
arXiv:2509.03059v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have shown that their reasoning capabilities can be significantly improved through Reinforcement Learning with Verifiable Reward (RLVR), particularly in domains like mathematics and programming, where ground-truth correctness can be automatically evaluated. However, extending this success to other reasoning-intensive domains remains challenging due to the scarcity of high-quality, verifiable datasets and the high cost of human supervision. In this work, we introduce the Loong Project: an open-source framework for scalable synthetic data generation and verification across a diverse range of reasoning-intensive domains. The framework consists of two key components: (1) LoongBench, a curated seed dataset containing 8,729 human-vetted examples across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired with executable code and rich metadata; and (2) LoongEnv, a modular synthetic data generation environment that supports multiple prompting strategies to produce new question-answer-code triples. Together, these components form an agent-environment loop that enables reinforcement learning, where an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers. Empirically, we benchmark LoongBench on a broad suite of both open-source and proprietary LLMs to evaluate domain coverage and reveal performance bottlenecks. In addition, we conduct a comprehensive analysis of synthetic data generated by LoongEnv, examining correctness, difficulty, and diversity. Code and documentation are available at https://github.com/camel-ai/loong.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2M2ECG: Spatio-temporal bi-directional State Space Model Enabled Multi-branch Mamba for ECG</title>
<link>https://arxiv.org/abs/2509.03066</link>
<guid>https://arxiv.org/abs/2509.03066</guid>
<content:encoded><![CDATA[
arXiv:2509.03066v1 Announce Type: cross 
Abstract: As one of the most effective methods for cardiovascular disease (CVD) diagnosis, multi-lead Electrocardiogram (ECG) signals present a characteristic multi-sensor information fusion challenge that has been continuously researched in deep learning domains. Despite the numerous algorithms proposed with different DL architectures, maintaining a balance among performance, computational complexity, and multi-source ECG feature fusion remains challenging. Recently, state space models (SSMs), particularly Mamba, have demonstrated remarkable effectiveness across various fields. Their inherent design for high-efficiency computation and linear complexity makes them particularly suitable for low-dimensional data like ECGs. This work proposes S2M2ECG, an SSM architecture featuring three-level fusion mechanisms: (1) Spatio-temporal bi-directional SSMs with segment tokenization for low-level signal fusion, (2) Intra-lead temporal information fusion with bi-directional scanning to enhance recognition accuracy in both forward and backward directions, (3) Cross-lead feature interaction modules for spatial information fusion. To fully leverage the ECG-specific multi-lead mechanisms inherent in ECG signals, a multi-branch design and lead fusion modules are incorporated, enabling individual analysis of each lead while ensuring seamless integration with others. Experimental results reveal that S2M2ECG achieves superior performance in the rhythmic, morphological, and clinical scenarios. Moreover, its lightweight architecture ensures it has nearly the fewest parameters among existing models, making it highly suitable for efficient inference and convenient deployment. Collectively, S2M2ECG offers a promising alternative that strikes an excellent balance among performance, computational complexity, and ECG-specific characteristics, paving the way for high-performance, lightweight computations in CVD diagnosis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design Principle Violations</title>
<link>https://arxiv.org/abs/2509.03093</link>
<guid>https://arxiv.org/abs/2509.03093</guid>
<content:encoded><![CDATA[
arXiv:2509.03093v1 Announce Type: cross 
Abstract: Traditional static analysis methods struggle to detect semantic design flaws, such as violations of the SOLID principles, which require a strong understanding of object-oriented design patterns and principles. Existing solutions typically focus on individual SOLID principles or specific programming languages, leaving a gap in the ability to detect violations across all five principles in multi-language codebases. This paper presents a new approach: a methodology that leverages tailored prompt engineering to assess LLMs on their ability to detect SOLID violations across multiple languages. We present a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder, and GPT-4o Mini-on their ability to detect violations of all five SOLID principles. For this evaluation, we construct a new benchmark dataset of 240 manually validated code examples. Using this dataset, we test four distinct prompt strategies inspired by established zero-shot, few-shot, and chain-of-thought techniques to systematically measure their impact on detection accuracy. Our emerging results reveal a stark hierarchy among models, with GPT-4o Mini decisively outperforming others, yet even struggles with challenging principles like DIP. Crucially, we show that prompt strategy has a dramatic impact, but no single strategy is universally best; for instance, a deliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE prompt is superior for DIP violations. Across all experiments, detection accuracy is heavily influenced by language characteristics and degrades sharply with increasing code complexity. These initial findings demonstrate that effective, AI-driven design analysis requires not a single best model, but a tailored approach that matches the right model and prompt to the specific design context, highlighting the potential of LLMs to support maintainability through AI-assisted code analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information transmission: Inferring change area from change moment in time series remote sensing images</title>
<link>https://arxiv.org/abs/2509.03112</link>
<guid>https://arxiv.org/abs/2509.03112</guid>
<content:encoded><![CDATA[
arXiv:2509.03112v1 Announce Type: cross 
Abstract: Time series change detection is a critical task for exploring ecosystem dynamics using time series remote sensing images, because it can simultaneously indicate where and when change occur. While deep learning has shown excellent performance in this domain, it continues to approach change area detection and change moment identification as distinct tasks. Given that change area can be inferred from change moment, we propose a time series change detection network, named CAIM-Net (Change Area Inference from Moment Network), to ensure consistency between change area and change moment results. CAIM-Net infers change area from change moment based on the intrinsic relationship between time series analysis and spatial change detection. The CAIM-Net comprises three key steps: Difference Extraction and Enhancement, Coarse Change Moment Extraction, and Fine Change Moment Extraction and Change Area Inference. In the Difference Extraction and Enhancement, a lightweight encoder with batch dimension stacking is designed to rapidly extract difference features. Subsequently, boundary enhancement convolution is applied to amplify these difference features. In the Coarse Change Moment Extraction, the enhanced difference features from the first step are used to spatiotemporal correlation analysis, and then two distinct methods are employed to determine coarse change moments. In the Fine Change Moment Extraction and Change Area Inference, a multiscale temporal Class Activation Mapping (CAM) module first increases the weight of the change-occurring moment from coarse change moments. Then the weighted change moment is used to infer change area based on the fact that pixels with the change moment must have undergone a change.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Deep Reinforcement Learning Framework for Traffic Signal Control with Predictable Cycle Planning</title>
<link>https://arxiv.org/abs/2509.03118</link>
<guid>https://arxiv.org/abs/2509.03118</guid>
<content:encoded><![CDATA[
arXiv:2509.03118v1 Announce Type: cross 
Abstract: Deep reinforcement learning (DRL) has become a popular approach in traffic signal control (TSC) due to its ability to learn adaptive policies from complex traffic environments. Within DRL-based TSC methods, two primary control paradigms are ``choose phase" and ``switch" strategies. Although the agent in the choose phase paradigm selects the next active phase adaptively, this paradigm may result in unexpected phase sequences for drivers, disrupting their anticipation and potentially compromising safety at intersections. Meanwhile, the switch paradigm allows the agent to decide whether to switch to the next predefined phase or extend the current phase. While this structure maintains a more predictable order, it can lead to unfair and inefficient phase allocations, as certain movements may be extended disproportionately while others are neglected. In this paper, we propose a DRL model, named Deep Hierarchical Cycle Planner (DHCP), to allocate the traffic signal cycle duration hierarchically. A high-level agent first determines the split of the total cycle time between the North-South (NS) and East-West (EW) directions based on the overall traffic state. Then, a low-level agent further divides the allocated duration within each major direction between straight and left-turn movements, enabling more flexible durations for the two movements. We test our model on both real and synthetic road networks, along with multiple sets of real and synthetic traffic flows. Empirical results show our model achieves the best performance over all datasets against baselines.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models</title>
<link>https://arxiv.org/abs/2509.03122</link>
<guid>https://arxiv.org/abs/2509.03122</guid>
<content:encoded><![CDATA[
arXiv:2509.03122v1 Announce Type: cross 
Abstract: The intellectual property (IP) protection of Large Language Models (LLMs) is increasingly critical. Injecting specialized fingerprints into LLMs through instruction tuning is a common IP protection technique. However, this may significantly degrade model performance, requires substantial computational resources, and exhibits poor persistence under model modifications. We argue that knowledge editing offers a lightweight alternative that is more suitable for fingerprint injection. Accordingly, we apply knowledge editing to fingerprint injection for the first time and demonstrate its strong capability. Despite using scrambled text as fingerprints to prevent them from being overwritten during fine-tuning, degradation still occurs under large-scale fine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning (FSFT), which reduces fingerprint degradation by constraining the update of the fingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even in the worst-case scenario. Additionally, we observe that the fingerprint-injected models struggle to distinguish between fingerprints and similar texts due to the high similarity of their features. This finding underscores the urgent need for more robust and fine-grained fingerprinting injection methods for LLMs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive KV-Cache Compression without Manually Setting Budget</title>
<link>https://arxiv.org/abs/2509.03136</link>
<guid>https://arxiv.org/abs/2509.03136</guid>
<content:encoded><![CDATA[
arXiv:2509.03136v1 Announce Type: cross 
Abstract: Large language models (LLMs) inference relies heavily on KV-caches to accelerate autoregressive decoding, but the resulting memory footprint grows rapidly with sequence length, posing significant efficiency challenges. Current KV-cache compression methods suffer from a Procrustes' bed problem: they force diverse workloads into fixed compression ratios, leading to suboptimal resource allocation and inference performance. To this end, we present GVote, an adaptive KV-cache compression scheme that eliminates manual budget specification while achieving superior accuracy-efficiency trade-offs. GVote operates on the principle that the important keys are the aggregation of keys required by future queries. The method predicts future query attention demands by Monte-Carlo style sampling potential queries and aggregating selected keys to determine the optimal cache budget without manual specification. Experimental evaluation demonstrates GVote's effectiveness across multiple benchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote exhibits 2$\times$ memory reduction while the accuracy maintains higher or comparable.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Network Approach to Multi-radionuclide TDCR Beta Spectroscopy</title>
<link>https://arxiv.org/abs/2509.03137</link>
<guid>https://arxiv.org/abs/2509.03137</guid>
<content:encoded><![CDATA[
arXiv:2509.03137v1 Announce Type: cross 
Abstract: Liquid scintillation triple-to-doubly coincident ratio (TDCR) spectroscopy is widely adopted as a standard method for radionuclide quantification because of its inherent advantages such as high precision, self-calibrating capability, and independence from radioactive reference sources. However, multiradionuclide analysis via TDCR faces the challenges of limited automation and reliance on mixture-specific standards, which may not be easily available. Here, we present an Artificial Intelligence (AI) framework that combines numerical spectral simulation and deep learning for standard-free automated analysis. $\beta$ spectra for model training were generated using Geant4 simulations coupled with statistically modeled detector response sampling. A tailored neural network architecture, trained on this dataset covering various nuclei mix ratio and quenching scenarios, enables autonomous resolution of individual radionuclide activities and detecting efficiency through end-to-end learning paradigms. The model delivers consistent high accuracy across tasks: activity proportions (mean absolute error = 0.009), detection efficiencies (mean absolute error = 0.002), and spectral reconstruction (Structural Similarity Index = 0.9998), validating its physical plausibility for quenched $\beta$ spectroscopy. This AI-driven methodology exhibits significant potential for automated safety-compliant multiradionuclide analysis with robust generalization, real-time processing capabilities, and engineering feasibility, particularly in scenarios where reference materials are unavailable or rapid field analysis is required.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralised self-organisation of pivoting cube ensembles using geometric deep learning</title>
<link>https://arxiv.org/abs/2509.03140</link>
<guid>https://arxiv.org/abs/2509.03140</guid>
<content:encoded><![CDATA[
arXiv:2509.03140v1 Announce Type: cross 
Abstract: We present a decentralized model for autonomous reconfiguration of homogeneous pivoting cube modular robots in two dimensions. Each cube in the ensemble is controlled by a neural network that only gains information from other cubes in its local neighborhood, trained using reinforcement learning. Furthermore, using geometric deep learning, we include the grid symmetries of the cube ensemble in the neural network architecture. We find that even the most localized versions succeed in reconfiguring to the target shape, although reconfiguration happens faster the more information about the whole ensemble is available to individual cubes. Near-optimal reconfiguration is achieved with only nearest neighbor interactions by using multiple information passing between cubes, allowing them to accumulate more global information about the ensemble. Compared to standard neural network architectures, using geometric deep learning approaches provided only minor benefits. Overall, we successfully demonstrate mostly local control of a modular self-assembling system, which is transferable to other space-relevant systems with different action spaces, such as sliding cube modular robots and CubeSat swarms.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Adaptation of LLMs for Process Data</title>
<link>https://arxiv.org/abs/2509.03161</link>
<guid>https://arxiv.org/abs/2509.03161</guid>
<content:encoded><![CDATA[
arXiv:2509.03161v1 Announce Type: cross 
Abstract: In recent years, Large Language Models (LLMs) have emerged as a prominent area of interest across various research domains, including Process Mining (PM). Current applications in PM have predominantly centered on prompt engineering strategies or the transformation of event logs into narrative-style datasets, thereby exploiting the semantic capabilities of LLMs to address diverse tasks. In contrast, this study investigates the direct adaptation of pretrained LLMs to process data without natural language reformulation, motivated by the fact that these models excel in generating sequences of tokens, similar to the objective in PM. More specifically, we focus on parameter-efficient fine-tuning techniques to mitigate the computational overhead typically associated with such models. Our experimental setup focuses on Predictive Process Monitoring (PPM), and considers both single- and multi-task predictions. The results demonstrate a potential improvement in predictive performance over state-of-the-art recurrent neural network (RNN) approaches and recent narrative-style-based solutions, particularly in the multi-task setting. Additionally, our fine-tuned models exhibit faster convergence and require significantly less hyperparameter optimization.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rashomon in the Streets: Explanation Ambiguity in Scene Understanding</title>
<link>https://arxiv.org/abs/2509.03169</link>
<guid>https://arxiv.org/abs/2509.03169</guid>
<content:encoded><![CDATA[
arXiv:2509.03169v1 Announce Type: cross 
Abstract: Explainable AI (XAI) is essential for validating and trusting models in safety-critical applications like autonomous driving. However, the reliability of XAI is challenged by the Rashomon effect, where multiple, equally accurate models can offer divergent explanations for the same prediction. This paper provides the first empirical quantification of this effect for the task of action prediction in real-world driving scenes. Using Qualitative Explainable Graphs (QXGs) as a symbolic scene representation, we train Rashomon sets of two distinct model classes: interpretable, pair-based gradient boosting models and complex, graph-based Graph Neural Networks (GNNs). Using feature attribution methods, we measure the agreement of explanations both within and between these classes. Our results reveal significant explanation disagreement. Our findings suggest that explanation ambiguity is an inherent property of the problem, not just a modeling artifact.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain</title>
<link>https://arxiv.org/abs/2509.03179</link>
<guid>https://arxiv.org/abs/2509.03179</guid>
<content:encoded><![CDATA[
arXiv:2509.03179v1 Announce Type: cross 
Abstract: Poisoning attacks pose an increasing threat to the security and robustness of Artificial Intelligence systems in the military domain. The widespread use of open-source datasets and pretrained models exacerbates this risk. Despite the severity of this threat, there is limited research on the application and detection of poisoning attacks on object detection systems. This is especially problematic in the military domain, where attacks can have grave consequences. In this work, we both investigate the effect of poisoning attacks on military object detectors in practice, and the best approach to detect these attacks. To support this research, we create a small, custom dataset featuring military vehicles: MilCivVeh. We explore the vulnerability of military object detectors for poisoning attacks by implementing a modified version of the BadDet attack: a patch-based poisoning attack. We then assess its impact, finding that while a positive attack success rate is achievable, it requires a substantial portion of the data to be poisoned -- raising questions about its practical applicability. To address the detection challenge, we test both specialized poisoning detection methods and anomaly detection methods from the visual industrial inspection domain. Since our research shows that both classes of methods are lacking, we introduce our own patch detection method: AutoDetect, a simple, fast, and lightweight autoencoder-based method. Our method shows promising results in separating clean from poisoned samples using the reconstruction error of image slices, outperforming existing methods, while being less time- and memory-intensive. We urge that the availability of large, representative datasets in the military domain is a prerequisite to further evaluate risks of poisoning attacks and opportunities patch detection.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback</title>
<link>https://arxiv.org/abs/2509.03206</link>
<guid>https://arxiv.org/abs/2509.03206</guid>
<content:encoded><![CDATA[
arXiv:2509.03206v1 Announce Type: cross 
Abstract: Reinforcement learning faces significant challenges when applied to tasks characterized by sparse reward structures. Although imitation learning, within the domain of supervised learning, offers faster convergence, it relies heavily on human-generated demonstrations. Recently, Goal-Conditioned Supervised Learning (GCSL) has emerged as a potential solution by enabling self-imitation learning for autonomous systems. By strategically relabelling goals, agents can derive policy insights from their own experiences. Despite the successes of this framework, it presents two notable limitations: (1) Learning exclusively from self-generated experiences can exacerbate the agents' inherent biases; (2) The relabelling strategy allows agents to focus solely on successful outcomes, precluding them from learning from their mistakes. To address these issues, we propose a novel model that integrates contrastive learning principles into the GCSL framework to learn from both success and failure. Through empirical evaluations, we demonstrate that our algorithm overcomes limitations imposed by agents' initial biases and thereby enables more exploratory behavior. This facilitates the identification and adoption of effective policies, leading to superior performance across a variety of challenging environments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking</title>
<link>https://arxiv.org/abs/2509.03221</link>
<guid>https://arxiv.org/abs/2509.03221</guid>
<content:encoded><![CDATA[
arXiv:2509.03221v1 Announce Type: cross 
Abstract: Organoids replicate organ structure and function, playing a crucial role in fields such as tumor treatment and drug screening. Their shape and size can indicate their developmental status, but traditional fluorescence labeling methods risk compromising their structure. Therefore, this paper proposes an automated, non-destructive approach to organoid segmentation and tracking. We introduced the LGBP-OrgaNet, a deep learning-based system proficient in accurately segmenting, tracking, and quantifying organoids. The model leverages complementary information extracted from CNN and Transformer modules and introduces the innovative feature fusion module, Learnable Gaussian Band Pass Fusion, to merge data from two branches. Additionally, in the decoder, the model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features, and finally completes the decoding through progressive concatenation and upsampling. SROrga demonstrates satisfactory segmentation accuracy and robustness on organoids segmentation datasets, providing a potent tool for organoid research.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Stress Detection as Time Series Events -- A Novel Window-Based F1-Metric</title>
<link>https://arxiv.org/abs/2509.03240</link>
<guid>https://arxiv.org/abs/2509.03240</guid>
<content:encoded><![CDATA[
arXiv:2509.03240v1 Announce Type: cross 
Abstract: Accurate evaluation of event detection in time series is essential for applications such as stress monitoring with wearable devices, where ground truth is typically annotated as single-point events, even though the underlying phenomena are gradual and temporally diffused. Standard metrics like F1 and point-adjusted F1 (F1$_{pa}$) often misrepresent model performance in such real-world, imbalanced datasets. We introduce a window-based F1 metric (F1$_w$) that incorporates temporal tolerance, enabling a more robust assessment of event detection when exact alignment is unrealistic. Empirical analysis in three physiological datasets, two in-the-wild (ADARP, Wrist Angel) and one experimental (ROAD), indicates that F1$_w$ reveals meaningful model performance patterns invisible to conventional metrics, while its window size can be adapted to domain knowledge to avoid overestimation. We show that the choice of evaluation metric strongly influences the interpretation of model performance: using predictions from TimesFM, only our temporally tolerant metrics reveal statistically significant improvements over random and null baselines in the two in-the-wild use cases. This work addresses key gaps in time series evaluation and provides practical guidance for healthcare applications where requirements for temporal precision vary by context.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization</title>
<link>https://arxiv.org/abs/2509.03244</link>
<guid>https://arxiv.org/abs/2509.03244</guid>
<content:encoded><![CDATA[
arXiv:2509.03244v1 Announce Type: cross 
Abstract: Expensive multi-objective optimization is a prevalent and crucial concern in many real-world scenarios, where sample-efficiency is vital due to the limited evaluations to recover the true Pareto front for decision making. Existing works either involve rebuilding Gaussian process surrogates from scratch for each objective in each new problem encountered, or rely on extensive past domain experiments for pre-training deep learning models, making them hard to generalize and impractical to cope with various emerging applications in the real world. To address this issue, we propose a new paradigm named FoMEMO (Foundation Models for Expensive Multi-objective Optimization), which enables the establishment of a foundation model conditioned on any domain trajectory and user preference, and facilitates fast in-context optimization based on the predicted preference-wise aggregation posteriors. Rather than accessing extensive domain experiments in the real world, we demonstrate that pre-training the foundation model with a diverse set of hundreds of millions of synthetic data can lead to superior adaptability to unknown problems, without necessitating any subsequent model training or updates in the optimization process. We evaluate our method across a variety of synthetic benchmarks and real-word applications, and demonstrate its superior generality and competitive performance compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure Transfer: an Inference-Based Calculus for the Transformation of Representations</title>
<link>https://arxiv.org/abs/2509.03249</link>
<guid>https://arxiv.org/abs/2509.03249</guid>
<content:encoded><![CDATA[
arXiv:2509.03249v1 Announce Type: cross 
Abstract: Representation choice is of fundamental importance to our ability to communicate and reason effectively. A major unsolved problem, addressed in this paper, is how to devise \textit{representational-system (RS) agnostic} techniques that drive representation transformation and choice. We present a novel calculus, called \textit{structure transfer}, that enables representation transformation across diverse RSs. Specifically, given a \textit{source} representation drawn from a source RS, the rules of structure transfer allow us to generate a \textit{target} representation for a target RS. The generality of structure transfer comes in part from its ability to ensure that the source representation and the generated target representation satisfy \textit{any} specified relation (such as semantic equivalence). This is done by exploiting \textit{schemas}, which encode knowledge about RSs. Specifically, schemas can express \textit{preservation of information} across relations between any pair of RSs, and this knowledge is used by structure transfer to derive a structure for the target representation which ensures that the desired relation holds. We formalise this using Representational Systems Theory~\cite{raggi2022rst}, building on the key concept of a \textit{construction space}. The abstract nature of construction spaces grants them the generality to model RSs of diverse kinds, including formal languages, geometric figures and diagrams, as well as informal notations. Consequently, structure transfer is a system-agnostic calculus that can be used to identify alternative representations in a wide range of practical settings.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling</title>
<link>https://arxiv.org/abs/2509.03260</link>
<guid>https://arxiv.org/abs/2509.03260</guid>
<content:encoded><![CDATA[
arXiv:2509.03260v1 Announce Type: cross 
Abstract: Abnormal cryptocurrency transactions - such as mixing services, fraudulent transfers, and pump-and-dump operations -- pose escalating risks to financial integrity but remain notoriously difficult to detect due to class imbalance, temporal volatility, and complex network dependencies. Existing approaches are predominantly model-centric and post hoc, flagging anomalies only after they occur and thus offering limited preventive value. This paper introduces HyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a data-driven early-warning framework that explicitly incorporates lead time into anomaly detection. Unlike prior methods, HyPV-LEAD integrates three innovations: (1) window-horizon modeling to guarantee actionable lead-time alerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while preserving temporal continuity, and (3) hyperbolic embedding to capture the hierarchical and scale-free properties of blockchain transaction networks. Empirical evaluation on large-scale Bitcoin transaction data demonstrates that HyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a PR-AUC of 0.9624 with significant gains in precision and recall. Ablation studies further confirm that each component - PV sampling, hyperbolic embedding, and structural-temporal modeling - provides complementary benefits, with the full framework delivering the highest performance. By shifting anomaly detection from reactive classification to proactive early-warning, HyPV-LEAD establishes a robust foundation for real-time risk management, anti-money laundering (AML) compliance, and financial security in dynamic blockchain environments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estudio de la eficiencia en la escalabilidad de GPUs para el entrenamiento de Inteligencia Artificial</title>
<link>https://arxiv.org/abs/2509.03263</link>
<guid>https://arxiv.org/abs/2509.03263</guid>
<content:encoded><![CDATA[
arXiv:2509.03263v1 Announce Type: cross 
Abstract: Training large-scale deep learning models has become a key challenge for the scientific community and industry. While the massive use of GPUs can significantly speed up training times, this approach has a negative impact on efficiency. In this article, we present a detailed analysis of the times reported by MLPerf Training v4.1 on four workloads: BERT, Llama2 LoRA, RetinaNet, and Stable Diffusion, showing that there are configurations that optimise the relationship between performance, GPU usage, and efficiency. The results point to a break-even point that allows training times to be reduced while maximising efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Guide to Differential Privacy: From Theory to User Expectations</title>
<link>https://arxiv.org/abs/2509.03294</link>
<guid>https://arxiv.org/abs/2509.03294</guid>
<content:encoded><![CDATA[
arXiv:2509.03294v1 Announce Type: cross 
Abstract: The increasing availability of personal data has enabled significant advances in fields such as machine learning, healthcare, and cybersecurity. However, this data abundance also raises serious privacy concerns, especially in light of powerful re-identification attacks and growing legal and ethical demands for responsible data use. Differential privacy (DP) has emerged as a principled, mathematically grounded framework for mitigating these risks. This review provides a comprehensive survey of DP, covering its theoretical foundations, practical mechanisms, and real-world applications. It explores key algorithmic tools and domain-specific challenges - particularly in privacy-preserving machine learning and synthetic data generation. The report also highlights usability issues and the need for improved communication and transparency in DP systems. Overall, the goal is to support informed adoption of DP by researchers and practitioners navigating the evolving landscape of data privacy.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Differentiation of Agent-Based Models</title>
<link>https://arxiv.org/abs/2509.03303</link>
<guid>https://arxiv.org/abs/2509.03303</guid>
<content:encoded><![CDATA[
arXiv:2509.03303v1 Announce Type: cross 
Abstract: Agent-based models (ABMs) simulate complex systems by capturing the bottom-up interactions of individual agents comprising the system. Many complex systems of interest, such as epidemics or financial markets, involve thousands or even millions of agents. Consequently, ABMs often become computationally demanding and rely on the calibration of numerous free parameters, which has significantly hindered their widespread adoption. In this paper, we demonstrate that automatic differentiation (AD) techniques can effectively alleviate these computational burdens. By applying AD to ABMs, the gradients of the simulator become readily available, greatly facilitating essential tasks such as calibration and sensitivity analysis. Specifically, we show how AD enables variational inference (VI) techniques for efficient parameter calibration. Our experiments demonstrate substantial performance improvements and computational savings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape; and the SIR epidemiological model. Our approach thus significantly enhances the practicality and scalability of ABMs for studying complex systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heatmap Guided Query Transformers for Robust Astrocyte Detection across Immunostains and Resolutions</title>
<link>https://arxiv.org/abs/2509.03323</link>
<guid>https://arxiv.org/abs/2509.03323</guid>
<content:encoded><![CDATA[
arXiv:2509.03323v1 Announce Type: cross 
Abstract: Astrocytes are critical glial cells whose altered morphology and density are hallmarks of many neurological disorders. However, their intricate branching and stain dependent variability make automated detection of histological images a highly challenging task. To address these challenges, we propose a hybrid CNN Transformer detector that combines local feature extraction with global contextual reasoning. A heatmap guided query mechanism generates spatially grounded anchors for small and faint astrocytes, while a lightweight Transformer module improves discrimination in dense clusters. Evaluated on ALDH1L1 and GFAP stained astrocyte datasets, the model consistently outperformed Faster R-CNN, YOLOv11 and DETR, achieving higher sensitivity with fewer false positives, as confirmed by FROC analysis. These results highlight the potential of hybrid CNN Transformer architectures for robust astrocyte detection and provide a foundation for advanced computational pathology tools.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems</title>
<link>https://arxiv.org/abs/2509.03340</link>
<guid>https://arxiv.org/abs/2509.03340</guid>
<content:encoded><![CDATA[
arXiv:2509.03340v1 Announce Type: cross 
Abstract: Bifurcation phenomena in nonlinear dynamical systems often lead to multiple coexisting stable solutions, particularly in the presence of symmetry breaking. Deterministic machine learning models struggle to capture this multiplicity, averaging over solutions and failing to represent lower-symmetry outcomes. In this work, we propose a generative framework based on flow matching to model the full probability distribution over bifurcation outcomes. Our method enables direct sampling of multiple valid solutions while preserving system symmetries through equivariant modeling. We introduce a symmetric matching strategy that aligns predicted and target outputs under group actions, allowing accurate learning in equivariant settings. We validate our approach on a range of systems, from toy models to complex physical problems such as buckling beams and the Allen-Cahn equation. Our results demonstrate that flow matching significantly outperforms non-probabilistic and variational methods in capturing multimodal distributions and symmetry-breaking bifurcations, offering a principled and scalable solution for modeling multistability in high-dimensional systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the MIA Vulnerability Gap Between Private GANs and Diffusion Models</title>
<link>https://arxiv.org/abs/2509.03341</link>
<guid>https://arxiv.org/abs/2509.03341</guid>
<content:encoded><![CDATA[
arXiv:2509.03341v1 Announce Type: cross 
Abstract: Generative Adversarial Networks (GANs) and diffusion models have emerged as leading approaches for high-quality image synthesis. While both can be trained under differential privacy (DP) to protect sensitive data, their sensitivity to membership inference attacks (MIAs), a key threat to data confidentiality, remains poorly understood. In this work, we present the first unified theoretical and empirical analysis of the privacy risks faced by differentially private generative models. We begin by showing, through a stability-based analysis, that GANs exhibit fundamentally lower sensitivity to data perturbations than diffusion models, suggesting a structural advantage in resisting MIAs. We then validate this insight with a comprehensive empirical study using a standardized MIA pipeline to evaluate privacy leakage across datasets and privacy budgets. Our results consistently reveal a marked privacy robustness gap in favor of GANs, even in strong DP regimes, highlighting that model type alone can critically shape privacy leakage.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>epiGPTope: A machine learning-based epitope generator and classifier</title>
<link>https://arxiv.org/abs/2509.03351</link>
<guid>https://arxiv.org/abs/2509.03351</guid>
<content:encoded><![CDATA[
arXiv:2509.03351v1 Announce Type: cross 
Abstract: Epitopes are short antigenic peptide sequences which are recognized by antibodies or immune cell receptors. These are central to the development of immunotherapies, vaccines, and diagnostics. However, the rational design of synthetic epitope libraries is challenging due to the large combinatorial sequence space, $20^n$ combinations for linear epitopes of n amino acids, making screening and testing unfeasible, even with high throughput experimental techniques. In this study, we present a large language model, epiGPTope, pre-trained on protein data and specifically fine-tuned on linear epitopes, which for the first time can directly generate novel epitope-like sequences, which are found to possess statistical properties analogous to the ones of known epitopes. This generative approach can be used to prepare libraries of epitope candidate sequences. We further train statistical classifiers to predict whether an epitope sequence is of bacterial or viral origin, thus narrowing the candidate library and increasing the likelihood of identifying specific epitopes. We propose that such combination of generative and predictive models can be of assistance in epitope discovery. The approach uses only primary amino acid sequences of linear epitopes, bypassing the need for a geometric framework or hand-crafted features of the sequences. By developing a method to create biologically feasible sequences, we anticipate faster and more cost-effective generation and screening of synthetic epitopes, with relevant applications in the development of new biotechnologies.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Resource Allocation for Fleet Intelligence</title>
<link>https://arxiv.org/abs/2509.03353</link>
<guid>https://arxiv.org/abs/2509.03353</guid>
<content:encoded><![CDATA[
arXiv:2509.03353v1 Announce Type: cross 
Abstract: Resource allocation is crucial for the performance optimization of cloud-assisted multi-agent intelligence. Traditional methods often overlook agents' diverse computational capabilities and complex operating environments, leading to inefficient and unfair resource distribution. To address this, we open-sourced Fair-Synergy, an algorithmic framework that utilizes the concave relationship between the agents' accuracy and the system resources to ensure fair resource allocation across fleet intelligence. We extend traditional allocation approaches to encompass a multidimensional machine learning utility landscape defined by model parameters, training data volume, and task complexity. We evaluate Fair-Synergy with advanced vision and language models such as BERT, VGG16, MobileNet, and ResNets on datasets including MNIST, CIFAR-10, CIFAR-100, BDD, and GLUE. We demonstrate that Fair-Synergy outperforms standard benchmarks by up to 25% in multi-agent inference and 11% in multi-agent learning settings. Also, we explore how the level of fairness affects the least advantaged, most advantaged, and average agents, providing insights for equitable fleet intelligence.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Field Turing Machine: A Differentiable Spatial Computer</title>
<link>https://arxiv.org/abs/2509.03370</link>
<guid>https://arxiv.org/abs/2509.03370</guid>
<content:encoded><![CDATA[
arXiv:2509.03370v1 Announce Type: cross 
Abstract: We introduce the Neural Field Turing Machine (NFTM), a differentiable architecture that unifies symbolic computation, physical simulation, and perceptual inference within continuous spatial fields. NFTM combines a neural controller, continuous memory field, and movable read/write heads that perform local updates. At each timestep, the controller reads local patches, computes updates via learned rules, and writes them back while updating head positions. This design achieves linear O(N) scaling through fixed-radius neighborhoods while maintaining Turing completeness under bounded error. We demonstrate three example instantiations of NFTM: cellular automata simulation (Rule 110), physics-informed PDE solvers (2D heat equation), and iterative image refinement (CIFAR-10 inpainting). These instantiations learn local update rules that compose into global dynamics, exhibit stable long-horizon rollouts, and generalize beyond training horizons. NFTM provides a unified computational substrate bridging discrete algorithms and continuous field dynamics within a single differentiable framework.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers</title>
<link>https://arxiv.org/abs/2509.03379</link>
<guid>https://arxiv.org/abs/2509.03379</guid>
<content:encoded><![CDATA[
arXiv:2509.03379v1 Announce Type: cross 
Abstract: Vision Transformers (ViTs) achieve strong performance in image classification but incur high computational costs from processing all image tokens. To reduce inference costs in large ViTs without compromising accuracy, we propose TinyDrop, a training-free token dropping framework guided by a lightweight vision model. The guidance model estimates the importance of tokens while performing inference, thereby selectively discarding low-importance tokens if large vit models need to perform attention calculations. The framework operates plug-and-play, requires no architectural modifications, and is compatible with diverse ViT architectures. Evaluations on standard image classification benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs with minimal accuracy degradation, highlighting its generalization capability and practical utility for efficient ViT-based classification.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Correctness: Harmonizing Process and Outcome Rewards through RL Training</title>
<link>https://arxiv.org/abs/2509.03403</link>
<guid>https://arxiv.org/abs/2509.03403</guid>
<content:encoded><![CDATA[
arXiv:2509.03403v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged to be a predominant paradigm for mathematical reasoning tasks, offering stable improvements in reasoning ability. However, Outcome Reward Models (ORMs) in RLVR are too coarse-grained to distinguish flawed reasoning within correct answers or valid reasoning within incorrect answers. This lack of granularity introduces noisy and misleading gradients significantly and hinders further progress in reasoning process quality. While Process Reward Models (PRMs) offer fine-grained guidance for intermediate steps, they frequently suffer from inaccuracies and are susceptible to reward hacking.
  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an effective data process curation method that harmonizes noisy, fine-grained process rewards with accurate, coarse-grained outcome rewards. Rather than naively blending PRM and ORM in the objective function (arXiv:archive/2506.18896), PROF leverages their complementary strengths through consistency-driven sample selection. Our approach retains correct responses with higher averaged process values and incorrect responses with lower averaged process values, while maintaining positive/negative training sample balance. Extensive experiments demonstrate that our method not only consistently improves the final accuracy over $4\%$ compared to the blending approaches, but also strengthens the quality of intermediate reasoning steps. Codes and training recipes are available at https://github.com/Chenluye99/PROF.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-level SSL Feature Gating for Audio Deepfake Detection</title>
<link>https://arxiv.org/abs/2509.03409</link>
<guid>https://arxiv.org/abs/2509.03409</guid>
<content:encoded><![CDATA[
arXiv:2509.03409v1 Announce Type: cross 
Abstract: Recent advancements in generative AI, particularly in speech synthesis, have enabled the generation of highly natural-sounding synthetic speech that closely mimics human voices. While these innovations hold promise for applications like assistive technologies, they also pose significant risks, including misuse for fraudulent activities, identity theft, and security threats. Current research on spoofing detection countermeasures remains limited by generalization to unseen deepfake attacks and languages. To address this, we propose a gating mechanism extracting relevant feature from the speech foundation XLS-R model as a front-end feature extractor. For downstream back-end classifier, we employ Multi-kernel gated Convolution (MultiConv) to capture both local and global speech artifacts. Additionally, we introduce Centered Kernel Alignment (CKA) as a similarity metric to enforce diversity in learned features across different MultiConv layers. By integrating CKA with our gating mechanism, we hypothesize that each component helps improving the learning of distinct synthetic speech patterns. Experimental results demonstrate that our approach achieves state-of-the-art performance on in-domain benchmarks while generalizing robustly to out-of-domain datasets, including multilingual speech samples. This underscores its potential as a versatile solution for detecting evolving speech deepfake threats.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Saudi Sign Language Recognition: A Vision Transformer Approach</title>
<link>https://arxiv.org/abs/2509.03467</link>
<guid>https://arxiv.org/abs/2509.03467</guid>
<content:encoded><![CDATA[
arXiv:2509.03467v1 Announce Type: cross 
Abstract: Sign language (SL) is an essential communication form for hearing-impaired and deaf people, enabling engagement within the broader society. Despite its significance, limited public awareness of SL often leads to inequitable access to educational and professional opportunities, thereby contributing to social exclusion, particularly in Saudi Arabia, where over 84,000 individuals depend on Saudi Sign Language (SSL) as their primary form of communication. Although certain technological approaches have helped to improve communication for individuals with hearing impairments, there continues to be an urgent requirement for more precise and dependable translation techniques, especially for Arabic sign language variants like SSL. Most state-of-the-art solutions have primarily focused on non-Arabic sign languages, resulting in a considerable absence of resources dedicated to Arabic sign language, specifically SSL. The complexity of the Arabic language and the prevalence of isolated sign language datasets that concentrate on individual words instead of continuous speech contribute to this issue. To address this gap, our research represents an important step in developing SSL resources. To address this, we introduce the first continuous Saudi Sign Language dataset called KAU-CSSL, focusing on complete sentences to facilitate further research and enable sophisticated recognition systems for SSL recognition and translation. Additionally, we propose a transformer-based model, utilizing a pretrained ResNet-18 for spatial feature extraction and a Transformer Encoder with Bidirectional LSTM for temporal dependencies, achieving 99.02\% accuracy at signer dependent mode and 77.71\% accuracy at signer independent mode. This development leads the way to not only improving communication tools for the SSL community but also making a substantial contribution to the wider field of sign language.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling</title>
<link>https://arxiv.org/abs/2509.03472</link>
<guid>https://arxiv.org/abs/2509.03472</guid>
<content:encoded><![CDATA[
arXiv:2509.03472v1 Announce Type: cross 
Abstract: Differentially-Private SGD (DP-SGD) is a powerful technique to protect user privacy when using sensitive data to train neural networks. During training, converting model weights and activations into low-precision formats, i.e., quantization, can drastically reduce training times, energy consumption, and cost, and is thus a widely used technique. In this work, we demonstrate that quantization causes significantly higher accuracy degradation in DP-SGD compared to regular SGD. We observe that this is caused by noise injection in DP-SGD, which amplifies quantization variance, leading to disproportionately large accuracy degradation. To address this challenge, we present QPQuant, a dynamic quantization framework that adaptively selects a changing subset of layers to quantize at each epoch. Our method combines two key ideas that effectively reduce quantization variance: (i) probabilistic sampling of the layers that rotates which layers are quantized every epoch, and (ii) loss-aware layer prioritization, which uses a differentially private loss sensitivity estimator to identify layers that can be quantized with minimal impact on model quality. This estimator consumes a negligible fraction of the overall privacy budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50, and DenseNet121 across a range of datasets demonstrate that DPQuant consistently outperforms static quantization baselines, achieving near Pareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical throughput improvements on low-precision hardware, with less than 2% drop in validation accuracy.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning</title>
<link>https://arxiv.org/abs/2509.03477</link>
<guid>https://arxiv.org/abs/2509.03477</guid>
<content:encoded><![CDATA[
arXiv:2509.03477v1 Announce Type: cross 
Abstract: Addressing missing modalities and limited labeled data is crucial for advancing robust multimodal learning. We propose Robult, a scalable framework designed to mitigate these challenges by preserving modality-specific information and leveraging redundancy through a novel information-theoretic approach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled (PU) contrastive loss that maximizes task-relevant feature alignment while effectively utilizing limited labeled data in semi-supervised settings, and (2) a latent reconstruction loss that ensures unique modality-specific information is retained. These strategies, embedded within a modular design, enhance performance across various downstream tasks and ensure resilience to incomplete modalities during inference. Experimental results across diverse datasets validate that Robult achieves superior performance over existing approaches in both semi-supervised learning and missing modality contexts. Furthermore, its lightweight design promotes scalability and seamless integration with existing architectures, making it suitable for real-world multimodal applications.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models</title>
<link>https://arxiv.org/abs/2509.03487</link>
<guid>https://arxiv.org/abs/2509.03487</guid>
<content:encoded><![CDATA[
arXiv:2509.03487v1 Announce Type: cross 
Abstract: Proteins play crucial roles in almost all biological processes. The advancement of deep learning has greatly accelerated the development of protein foundation models, leading to significant successes in protein understanding and design. However, the lack of systematic red-teaming for these models has raised serious concerns about their potential misuse, such as generating proteins with biological safety risks. This paper introduces SafeProtein, the first red-teaming framework designed for protein foundation models to the best of our knowledge. SafeProtein combines multimodal prompt engineering and heuristic beam search to systematically design red-teaming methods and conduct tests on protein foundation models. We also curated SafeProtein-Bench, which includes a manually constructed red-teaming benchmark dataset and a comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks on state-of-the-art protein foundation models (up to 70% attack success rate for ESM3), revealing potential biological safety risks in current protein foundation models and providing insights for the development of robust security protection technologies for frontier models. The codes will be made publicly available at https://github.com/jigang-fan/SafeProtein.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Entropy Control in LLM-RL Algorithms</title>
<link>https://arxiv.org/abs/2509.03493</link>
<guid>https://arxiv.org/abs/2509.03493</guid>
<content:encoded><![CDATA[
arXiv:2509.03493v1 Announce Type: cross 
Abstract: For RL algorithms, appropriate entropy control is crucial to their effectiveness. To control the policy entropy, a commonly used method is entropy regularization, which is adopted in various popular RL algorithms including PPO, SAC and A3C. Although entropy regularization proves effective in robotic and games RL conventionally, studies found that it gives weak to no gains in LLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL setting. Specifically, we first argue that the conventional entropy regularization suffers from the LLM's extremely large response space and the sparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy control method that utilizes a new clamped entropy bonus with an automatically adjusted coefficient. The clamped entropy is evaluated with the re-normalized policy defined on certain smaller token space, which encourages exploration within a more compact response set. In addition, the algorithm automatically adjusts entropy coefficient according to the clamped entropy value, effectively controlling the entropy-induced bias while leveraging the entropy's benefits. AEnt is tested in math-reasoning tasks under different base models and datasets, and it is observed that AEnt outperforms the baselines consistently across multiple benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena</title>
<link>https://arxiv.org/abs/2509.03500</link>
<guid>https://arxiv.org/abs/2509.03500</guid>
<content:encoded><![CDATA[
arXiv:2509.03500v1 Announce Type: cross 
Abstract: Advancements in onboard computing mean remote sensing agents can employ state-of-the-art computer vision and machine learning at the edge. These capabilities can be leveraged to unlock new rare, transient, and pinpoint measurements of dynamic science phenomena. In this paper, we present an automated workflow that synthesizes the detection of these dynamic events in look-ahead satellite imagery with autonomous trajectory planning for a follow-up high-resolution sensor to obtain pinpoint measurements. We apply this workflow to the use case of observing volcanic plumes. We analyze classification approaches including traditional machine learning algorithms and convolutional neural networks. We present several trajectory planning algorithms that track the morphological features of a plume and integrate these algorithms with the classifiers. We show through simulation an order of magnitude increase in the utility return of the high-resolution instrument compared to baselines while maintaining efficient runtimes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data</title>
<link>https://arxiv.org/abs/2509.03501</link>
<guid>https://arxiv.org/abs/2509.03501</guid>
<content:encoded><![CDATA[
arXiv:2509.03501v1 Announce Type: cross 
Abstract: Next-generation AI companions must go beyond general video understanding to resolve spatial and temporal references in dynamic, real-world environments. Existing Video Large Language Models (Video LLMs), while capable of coarse-level comprehension, struggle with fine-grained, spatiotemporal reasoning, especially when user queries rely on time-based event references for temporal anchoring, or gestural cues for spatial anchoring to clarify object references and positions. To bridge this critical gap, we introduce Strefer, a synthetic instruction data generation framework designed to equip Video LLMs with spatiotemporal referring and reasoning capabilities. Strefer produces diverse instruction-tuning data using a data engine that pseudo-annotates temporally dense, fine-grained video metadata, capturing rich spatial and temporal information in a structured manner, including subjects, objects, their locations as masklets, and their action descriptions and timelines. Our approach enhances the ability of Video LLMs to interpret spatial and temporal references, fostering more versatile, space-time-aware reasoning essential for real-world AI companions. Without using proprietary models, costly human annotation, or the need to annotate large volumes of new videos, experimental evaluations show that models trained with data produced by Strefer outperform baselines on tasks requiring spatial and temporal disambiguation. Additionally, these models exhibit enhanced space-time-aware reasoning, establishing a new foundation for perceptually grounded, instruction-tuned Video LLMs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Warming Up for Zeroth-Order Federated Pre-Training with Low Resource Clients</title>
<link>https://arxiv.org/abs/2509.03503</link>
<guid>https://arxiv.org/abs/2509.03503</guid>
<content:encoded><![CDATA[
arXiv:2509.03503v1 Announce Type: cross 
Abstract: Federated learning enables collaborative model training across numerous edge devices without requiring participants to share data; however, memory and communication constraints on these edge devices may preclude their participation in training. We consider a setting in which a subset of edge devices are below a critical memory or communication threshold required to conduct model updates. Under typical federated optimization algorithms, these devices are excluded from training which renders their data inaccessible and increases system induced bias. We are inspired by MeZO, a zeroth-order method used for memory-efficient fine-tuning. The increased variance inherent to zeroth-order gradient approximations has relegated previous zeroth-order optimizers exclusively to the domain of fine tuning; a limitation we seek to correct. We devise a federated, memory-efficient zeroth-order optimizer, ZOWarmUp that permits zeroth-order training from a random initialization. ZOWarmUp leverages differing client capabilities and careful variance reduction techniques to facilitate participation of under-represented, low-resource clients in model training. Like other federated zeroth-order methods, ZOWarmUp eliminates the need for edge devices to transmit their full gradients to the server and instead relies on only a small set of random seeds, rendering the up-link communication cost negligible. We present experiments using various datasets and model architectures to show that ZOWarmUp is a robust algorithm that can can be applied under a wide variety of circumstances. For systems with a high proportion of edge devices that would otherwise be excluded from training, this algorithm provides access to a greater volume and diversity of data, thus improving training outcomes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence</title>
<link>https://arxiv.org/abs/2509.03505</link>
<guid>https://arxiv.org/abs/2509.03505</guid>
<content:encoded><![CDATA[
arXiv:2509.03505v1 Announce Type: cross 
Abstract: We argue that progress toward general intelligence requires complementary foundation models grounded in language, the physical world, and structured data. This report presents LimiX, the first installment of our large structured-data models (LDMs). LimiX treats structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model. LimiX is pretrained using masked joint-distribution modeling with an episodic, context-conditional objective, where the model predicts for query subsets conditioned on dataset-specific contexts, supporting rapid, training-free adaptation at inference. We evaluate LimiX across 10 large structured-data benchmarks with broad regimes of sample size, feature dimensionality, class number, categorical-to-numerical feature ratio, missingness, and sample-to-feature ratios. With a single model and a unified interface, LimiX consistently surpasses strong baselines including gradient-boosting trees, deep tabular networks, recent tabular foundation models, and automated ensembles, as shown in Figure 1 and Figure 2. The superiority holds across a wide range of tasks, such as classification, regression, missing value imputation, and data generation, often by substantial margins, while avoiding task-specific architectures or bespoke training per task. All LimiX models are publicly accessible under Apache 2.0.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories</title>
<link>https://arxiv.org/abs/2509.03515</link>
<guid>https://arxiv.org/abs/2509.03515</guid>
<content:encoded><![CDATA[
arXiv:2509.03515v1 Announce Type: cross 
Abstract: The Waymo Open Motion Dataset (WOMD) has become a popular resource for data-driven modeling of autonomous vehicles (AVs) behavior. However, its validity for behavioral analysis remains uncertain due to proprietary post-processing, the absence of error quantification, and the segmentation of trajectories into 20-second clips. This study examines whether WOMD accurately captures the dynamics and interactions observed in real-world AV operations. Leveraging an independently collected naturalistic dataset from Level 4 AV operations in Phoenix, Arizona (PHX), we perform comparative analyses across three representative urban driving scenarios: discharging at signalized intersections, car-following, and lane-changing behaviors. For the discharging analysis, headways are manually extracted from aerial video to ensure negligible measurement error. For the car-following and lane-changing cases, we apply the Simulation-Extrapolation (SIMEX) method to account for empirically estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to quantify behavioral differences. Results across all scenarios consistently show that behavior in PHX falls outside the behavioral envelope of WOMD. Notably, WOMD underrepresents short headways and abrupt decelerations. These findings suggest that behavioral models calibrated solely on WOMD may systematically underestimate the variability, risk, and complexity of naturalistic driving. Caution is therefore warranted when using WOMD for behavior modeling without proper validation against independently collected data.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents</title>
<link>https://arxiv.org/abs/2208.13266</link>
<guid>https://arxiv.org/abs/2208.13266</guid>
<content:encoded><![CDATA[
arXiv:2208.13266v4 Announce Type: replace 
Abstract: Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1\% to 15.8\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Human-AI Collaboration with Large Foundation Models</title>
<link>https://arxiv.org/abs/2403.04931</link>
<guid>https://arxiv.org/abs/2403.04931</guid>
<content:encoded><![CDATA[
arXiv:2403.04931v3 Announce Type: replace 
Abstract: As the capabilities of artificial intelligence (AI) continue to expand rapidly, Human-AI (HAI) Collaboration, combining human intellect and AI systems, has become pivotal for advancing problem-solving and decision-making processes. The advent of Large Foundation Models (LFMs) has greatly expanded its potential, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. At the same time, realizing this potential responsibly requires addressing persistent challenges related to safety, fairness, and control. This paper reviews the crucial integration of LFMs with HAI, highlighting both opportunities and risks. We structure our analysis around four areas: human-guided model development, collaborative design principles, ethical and governance frameworks, and applications in high-stakes domains. Our review shows that successful HAI systems are not the automatic result of stronger models but the product of careful, human-centered design. By identifying key open challenges, this survey aims to give insight into current and future research that turns the raw power of LFMs into partnerships that are reliable, trustworthy, and beneficial to society.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Generating Monolithic and Model Reconciling Explanations in Probabilistic Scenarios</title>
<link>https://arxiv.org/abs/2405.19229</link>
<guid>https://arxiv.org/abs/2405.19229</guid>
<content:encoded><![CDATA[
arXiv:2405.19229v2 Announce Type: replace 
Abstract: Explanation generation frameworks aim to make AI systems' decisions transparent and understandable to human users. However, generating explanations in uncertain environments characterized by incomplete information and probabilistic models remains a significant challenge. In this paper, we propose a novel framework for generating probabilistic monolithic explanations and model reconciling explanations. Monolithic explanations provide self-contained reasons for an explanandum without considering the agent receiving the explanation, while model reconciling explanations account for the knowledge of the agent receiving the explanation. For monolithic explanations, our approach integrates uncertainty by utilizing probabilistic logic to increase the probability of the explanandum. For model reconciling explanations, we propose a framework that extends the logic-based variant of the model reconciliation problem to account for probabilistic human models, where the goal is to find explanations that increase the probability of the explanandum while minimizing conflicts between the explanation and the probabilistic human model. We introduce explanatory gain and explanatory power as quantitative metrics to assess the quality of these explanations. Further, we present algorithms that exploit the duality between minimal correction sets and minimal unsatisfiable sets to efficiently compute both types of explanations in probabilistic contexts. Extensive experimental evaluations on various benchmarks demonstrate the effectiveness and scalability of our approach in generating explanations under uncertainty.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frugal inference for control</title>
<link>https://arxiv.org/abs/2406.14427</link>
<guid>https://arxiv.org/abs/2406.14427</guid>
<content:encoded><![CDATA[
arXiv:2406.14427v3 Announce Type: replace 
Abstract: A key challenge in advancing artificial intelligence is achieving the right balance between utility maximization and resource use by both external movement and internal computation. While this trade-off has been studied in fully observable settings, our understanding of resource efficiency in partially observable environments remains limited. Motivated by this challenge, we develop a version of the POMDP framework where the information gained through inference is treated as a resource that must be optimized alongside task performance and motion effort. By solving this problem in environments described by linear-Gaussian dynamics, we uncover fundamental principles of resource efficiency. Our study reveals a phase transition in the inference, switching from a Bayes-optimal approach to one that strategically leaves some uncertainty unresolved. This frugal behavior gives rise to a structured family of equally effective strategies, facilitating adaptation to later objectives and constraints overlooked during the original optimization. We illustrate the applicability of our framework and the generality of the principles we derived using two nonlinear tasks. Overall, this work provides a foundation for a new type of rational computation that both brains and machines could use for effective but resource-efficient control under uncertainty.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MorphAgent: Empowering Agents through Self-Evolving Profiles and Decentralized Collaboration</title>
<link>https://arxiv.org/abs/2410.15048</link>
<guid>https://arxiv.org/abs/2410.15048</guid>
<content:encoded><![CDATA[
arXiv:2410.15048v2 Announce Type: replace 
Abstract: Large Language Model (LLM) based multi-agent systems (MAS) have shown promise in tackling complex tasks, but often rely on predefined roles and centralized coordination, limiting their adaptability to evolving challenges. This paper introduces MorphAgent, a novel Autonomous, Self-Organizing, and Self-Adaptive Multi-Agent System for decentralized agent collaboration that enables agents to dynamically evolve their roles and capabilities. Our approach employs self-evolving agent profiles, optimized through three key metrics, guiding agents in refining their individual expertise while maintaining complementary team dynamics. MorphAgent implements a two-phase process: a Profile Update phase for profile optimization, followed by a Task Execution phase where agents continuously adapt their roles based on task feedback. Our experimental results show that MorphAgent outperforms existing frameworks in terms of task performance and adaptability to changing requirements, paving the way for more robust and versatile multi-agent collaborative systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Act as Ensembler for Multi-GNNs?</title>
<link>https://arxiv.org/abs/2410.16822</link>
<guid>https://arxiv.org/abs/2410.16822</guid>
<content:encoded><![CDATA[
arXiv:2410.16822v5 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning from graph-structured data. However, GNNs lack the inherent semantic understanding capability of rich textual node attributes, limiting their effectiveness in applications. On the other hand, we empirically observe that for existing GNN models, no one can consistently outperforms others across diverse datasets. In this paper, we study whether LLMs can act as an ensembler for multi-GNNs and propose the LensGNN model. The model first aligns multiple GNNs, mapping the representations of different GNNs into the same space. Then, through LoRA fine-tuning, it aligns the space between the GNN and the LLM, injecting graph tokens and textual information into LLMs. This allows LensGNN to ensemble multiple GNNs and take advantage of the strengths of LLM, leading to a deeper understanding of both textual semantic information and graph structural information. The experimental results show that LensGNN outperforms existing models. This research advances text-attributed graph ensemble learning by providing a robust and superior solution for integrating semantic and structural information. We provide our code and data here: https://github.com/AquariusAQ/LensGNN.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation</title>
<link>https://arxiv.org/abs/2411.05085</link>
<guid>https://arxiv.org/abs/2411.05085</guid>
<content:encoded><![CDATA[
arXiv:2411.05085v2 Announce Type: replace 
Abstract: Radiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Grounded radiology report generation (GRRG) extends RRG by including the localisation of individual findings on the image. Currently, there are no manually annotated chest X-ray (CXR) datasets to train GRRG models. In this work, we present a dataset called PadChest-GR (Grounded-Reporting) derived from PadChest aimed at training GRRG models for CXR images. We curate a public bi-lingual dataset of 4,555 CXR studies with grounded reports (3,099 abnormal and 1,456 normal), each containing complete lists of sentences describing individual present (positive) and absent (negative) findings in English and Spanish. In total, PadChest-GR contains 7,037 positive and 3,422 negative finding sentences. Every positive finding sentence is associated with up to two independent sets of bounding boxes labelled by different readers and has categorical labels for finding type, locations, and progression. To the best of our knowledge, PadChest-GR is the first manually curated dataset designed to train GRRG models for understanding and interpreting radiological images and generated text. By including detailed localization and comprehensive annotations of all clinically relevant findings, it provides a valuable resource for developing and evaluating GRRG models from CXR images. PadChest-GR can be downloaded under request from https://bimcv.cipf.es/bimcv-projects/padchest-gr/
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CyberBOT: Towards Reliable Cybersecurity Education via Ontology-Grounded Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2504.00389</link>
<guid>https://arxiv.org/abs/2504.00389</guid>
<content:encoded><![CDATA[
arXiv:2504.00389v2 Announce Type: replace 
Abstract: Advancements in large language models (LLMs) have enabled the development of intelligent educational tools that support inquiry-based learning across technical domains. In cybersecurity education, where accuracy and safety are paramount, systems must go beyond surface-level relevance to provide information that is both trustworthy and domain-appropriate. To address this challenge, we introduce CyberBOT, a question-answering chatbot that leverages a retrieval-augmented generation (RAG) pipeline to incorporate contextual information from course-specific materials and validate responses using a domain-specific cybersecurity ontology. The ontology serves as a structured reasoning layer that constrains and verifies LLM-generated answers, reducing the risk of misleading or unsafe guidance. CyberBOT has been deployed in a large graduate-level course at Arizona State University (ASU), where more than one hundred students actively engage with the system through a dedicated web-based platform. Computational evaluations in lab environments highlight the potential capacity of CyberBOT, and a forthcoming field study will evaluate its pedagogical impact. By integrating structured domain reasoning with modern generative capabilities, CyberBOT illustrates a promising direction for developing reliable and curriculum-aligned AI applications in specialized educational contexts.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shutdownable Agents through POST-Agency</title>
<link>https://arxiv.org/abs/2505.20203</link>
<guid>https://arxiv.org/abs/2505.20203</guid>
<content:encoded><![CDATA[
arXiv:2505.20203v2 Announce Type: replace 
Abstract: Many fear that future artificial agents will resist shutdown. I present an idea - the POST-Agents Proposal - for ensuring that doesn't happen. I propose that we train agents to satisfy Preferences Only Between Same-Length Trajectories (POST). I then prove that POST - together with other conditions - implies Neutrality+: the agent maximizes expected utility, ignoring the probability distribution over trajectory-lengths. I argue that Neutrality+ keeps agents shutdownable and allows them to be useful.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research</title>
<link>https://arxiv.org/abs/2506.01326</link>
<guid>https://arxiv.org/abs/2506.01326</guid>
<content:encoded><![CDATA[
arXiv:2506.01326v2 Announce Type: replace 
Abstract: Operations research (OR) is widely deployed to solve critical decision-making problems with complex objectives and constraints, impacting manufacturing, logistics, finance, and healthcare outcomes. While Large Language Models (LLMs) have shown promising results in various domains, their practical application in industry-relevant operations research (OR) problems presents significant challenges and opportunities. Preliminary industrial applications of LLMs for operations research face two critical deployment challenges: 1) Self-correction focuses on code syntax rather than mathematical accuracy, causing costly errors; 2) Complex expert selection creates unpredictable workflows that reduce transparency and increase maintenance costs, making them impractical for time-sensitive business applications. To address these business limitations, we introduce ORMind, a cognitive-inspired framework that enhances optimization through counterfactual reasoning. Our approach emulates human cognition, implementing an end-to-end workflow that systematically transforms requirements into mathematical models and executable solver code. It is currently being tested internally in Lenovo's AI Assistant, with plans to enhance optimization capabilities for both business and consumer customers. Experiments demonstrate that ORMind outperforms existing methods, achieving a 9.5\% improvement on the NL4Opt dataset and a 14.6\% improvement on the ComplexOR dataset.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation</title>
<link>https://arxiv.org/abs/2506.07940</link>
<guid>https://arxiv.org/abs/2506.07940</guid>
<content:encoded><![CDATA[
arXiv:2506.07940v2 Announce Type: replace 
Abstract: Current AutoML platforms leave substantial performance untapped. Testing 180 fine-tuning tasks across models from 70M to 70B parameters, we found that HuggingFace AutoTrain, TogetherAI, Databricks, and Google Cloud consistently produce suboptimal configurations. Gradients, built on the Bittensor network, attacks this problem through competition. Independent miners race to find optimal hyperparameters, earning rewards proportional to their models' performance. This tournament drives exploration of configuration spaces that single-strategy methods never examine. In our experiments, Gradients achieved a 100\% win rate against TogetherAI, Databricks, and Google Cloud, and beat HuggingFace AutoTrain in 82.8\% of experiments. Mean improvements reached 42.1\% against commercial platforms. Retrieval-augmented generation tasks saw 30-40\% gains; diffusion models improved 23.4\% on person-specific generation. When miners compete for rewards, they develop optimization strategies that centralized approaches overlook. These findings demonstrate that decentralized systems with economic incentives can systematically outperform traditional AutoML, suggesting market dynamics may be key to achieving superior fine-tuning results. Code is available at https://github.com/rayonlabs/G.O.D.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Research Agents: A Systematic Examination And Roadmap</title>
<link>https://arxiv.org/abs/2506.18096</link>
<guid>https://arxiv.org/abs/2506.18096</guid>
<content:encoded><![CDATA[
arXiv:2506.18096v2 Announce Type: replace 
Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new category of autonomous AI systems, referred to as Deep Research (DR) agents. These agents are designed to tackle complex, multi-turn informational research tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon planning, multi-hop information retrieval, iterative tool use, and the generation of structured analytical reports. In this paper, we conduct a detailed analysis of the foundational technologies and architectural components that constitute Deep Research agents. We begin by reviewing information acquisition strategies, contrasting API-based retrieval methods with browser-based exploration. We then examine modular tool-use frameworks, including code execution, multimodal input processing, and the integration of Model Context Protocols (MCPs) to support extensibility and ecosystem development. To systematize existing approaches, we propose a taxonomy that differentiates between static and dynamic workflows, and we classify agent architectures based on planning strategies and agent composition, including single-agent and multi-agent configurations. We also provide a critical evaluation of current benchmarks, highlighting key limitations such as restricted access to external knowledge, sequential execution inefficiencies, and misalignment between evaluation metrics and the practical objectives of DR agents. Finally, we outline open challenges and promising directions for future research. A curated and continuously updated repository of DR agent research is available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChordPrompt: Orchestrating Cross-Modal Prompt Synergy for Multi-Domain Incremental Learning in CLIP</title>
<link>https://arxiv.org/abs/2506.19608</link>
<guid>https://arxiv.org/abs/2506.19608</guid>
<content:encoded><![CDATA[
arXiv:2506.19608v2 Announce Type: replace 
Abstract: Continual learning (CL) empowers pre-trained vision-language models to adapt effectively to novel or previously underrepresented data distributions without comprehensive retraining, enhancing their adaptability and efficiency. While vision-language models like CLIP show great promise, they struggle to maintain performance across domains in incremental learning scenarios. Existing prompt learning methods face two main limitations: 1) they primarily focus on class-incremental learning scenarios, lacking specific strategies for multi-domain task incremental learning; 2) most current approaches employ single-modal prompts, neglecting the potential benefits of cross-modal information exchange. To address these challenges, we propose the \ChordPrompt framework, which facilitates a harmonious interplay between visual and textual prompts. \ChordPrompt introduces cross-modal prompts to leverage interactions between visual and textual information. Our approach also employs domain-adaptive text prompts to select appropriate prompts for continual adaptation across multiple domains. Comprehensive experiments on multi-domain incremental learning benchmarks demonstrate that \ChordPrompt outperforms state-of-the-art methods in zero-shot generalization and downstream task performance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks</title>
<link>https://arxiv.org/abs/2507.17695</link>
<guid>https://arxiv.org/abs/2507.17695</guid>
<content:encoded><![CDATA[
arXiv:2507.17695v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a vital role in the evolution of 6G networks, by empowering real-time decision-making related to management and service provisioning to end-users. This shift facilitates the transition from a specialized intelligence approach, where artificial intelligence (AI) algorithms handle isolated tasks, to artificial general intelligence (AGI)-driven networks, where agents possess broader reasoning capabilities and can manage diverse network functions. In this paper, we introduce a novel agentic paradigm that combines LLMs with real-time optimization algorithms towards Trustworthy AI, defined as symbiotic agents. Optimizers at the LLM's input-level provide bounded uncertainty steering for numerically precise tasks, whereas output-level optimizers supervised by the LLM enable adaptive real-time control. We design and implement two novel agent types including: (i) Radio Access Network optimizers, and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We further propose an end-to-end architecture for AGI networks and evaluate it on a 5G testbed capturing channel fluctuations from moving vehicles. Results show that symbiotic agents reduce decision errors fivefold compared to standalone LLM-based agents, while smaller language models (SLM) achieve similar accuracy with a 99.9% reduction in GPU resource overhead and in near-real-time loops of 82 ms. A multi-agent demonstration for collaborative RAN on the real-world testbed highlights significant flexibility in service-level agreement and resource allocation, reducing RAN over-utilization by approximately 44%. Drawing on our findings and open-source implementations, we introduce the symbiotic paradigm as the foundation for next-generation, AGI-driven networks-systems designed to remain adaptable, efficient, and trustworthy even as LLMs advance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Activity Predictions in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.19733</link>
<guid>https://arxiv.org/abs/2507.19733</guid>
<content:encoded><![CDATA[
arXiv:2507.19733v2 Announce Type: replace 
Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in generating predictions about future events. By leveraging the semantic framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies (CCO), we demonstrate how data such as the movements of a fishing vessel can be organized in and retrieved from a knowledge graph. These query results are then used to create Markov chain models, allowing us to predict future states based on the vessel's history. To fully support this process, we introduce the term `spatiotemporal instant' to complete the necessary structural semantics. Additionally, we critique the prevailing ontological model of probability, according to which probabilities are about the future. We propose an alternative view, where at least some probabilities are treated as being about actual process profiles, which better captures the dynamics of real-world phenomena. Finally, we demonstrate how our Markov chain-based probability calculations can be seamlessly integrated back into the knowledge graph, enabling further analysis and decision-making.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks</title>
<link>https://arxiv.org/abs/2507.23751</link>
<guid>https://arxiv.org/abs/2507.23751</guid>
<content:encoded><![CDATA[
arXiv:2507.23751v2 Announce Type: replace 
Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on given seed tasks, and then generate a new synthetic example of similar quality and complexity. This is followed by a filtering step to select high-quality data using automatic metrics, which are then used for LLM training. In verifiable reasoning, our synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, when evaluated on MATH500, AMC23, AIME24, and GPQA-Diamond. For non-verifiable instruction-following tasks, our method surpasses the performance of both human and standard Self-Instruct training data on the AlpacaEval 2.0 and Arena-Hard benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations</title>
<link>https://arxiv.org/abs/2508.07834</link>
<guid>https://arxiv.org/abs/2508.07834</guid>
<content:encoded><![CDATA[
arXiv:2508.07834v3 Announce Type: replace 
Abstract: Over the years, the need for rescue operations throughout the world has increased rapidly. Demographic changes and the resulting risk of injury or health disorders form the basis for emergency calls. In such scenarios, first responders are in a rush to reach the patient in need, provide first aid, and save lives. In these situations, they must be able to provide personalized and optimized healthcare in the shortest possible time and estimate the patients condition with the help of freshly recorded vital data in an emergency situation. However, in such a timedependent situation, first responders and medical experts cannot fully grasp their knowledge and need assistance and recommendation for further medical treatments. To achieve this, on the spot calculated, evaluated, and processed knowledge must be made available to improve treatments by first responders. The Knowledge Graph presented in this article as a central knowledge representation provides first responders with an innovative knowledge management that enables intelligent treatment recommendations with an artificial intelligence-based pre-recognition of the situation.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.15327</link>
<guid>https://arxiv.org/abs/2508.15327</guid>
<content:encoded><![CDATA[
arXiv:2508.15327v2 Announce Type: replace 
Abstract: Offline reinforcement learning refers to the process of learning policies from fixed datasets, without requiring additional environment interaction. However, it often relies on well-defined reward functions, which are difficult and expensive to design. Human feedback is an appealing alternative, but its two common forms, expert demonstrations and preferences, have complementary limitations. Demonstrations provide stepwise supervision, but they are costly to collect and often reflect limited expert behavior modes. In contrast, preferences are easier to collect, but it is unclear which parts of a behavior contribute most to a trajectory segment, leaving credit assignment unresolved. In this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to unify these two feedback sources. For each transition in a preference labeled trajectory, SPW searches for the most similar state-action pairs from expert demonstrations and directly derives stepwise importance weights based on their similarity scores. These weights are then used to guide standard preference learning, enabling more accurate credit assignment that traditional approaches struggle to achieve. We demonstrate that SPW enables effective joint learning from preferences and demonstrations, outperforming prior methods that leverage both feedback types on challenging robot manipulation tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ramon Llull's Thinking Machine for Automated Ideation</title>
<link>https://arxiv.org/abs/2508.19200</link>
<guid>https://arxiv.org/abs/2508.19200</guid>
<content:encoded><![CDATA[
arXiv:2508.19200v3 Announce Type: replace 
Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P2DT: Mitigating Forgetting in task-incremental Learning with progressive prompt Decision Transformer</title>
<link>https://arxiv.org/abs/2401.11666</link>
<guid>https://arxiv.org/abs/2401.11666</guid>
<content:encoded><![CDATA[
arXiv:2401.11666v2 Announce Type: replace-cross 
Abstract: Catastrophic forgetting poses a substantial challenge for managing intelligent agents controlled by a large model, causing performance degradation when these agents face new tasks. In our work, we propose a novel solution - the Progressive Prompt Decision Transformer (P2DT). This method enhances a transformer-based model by dynamically appending decision tokens during new task training, thus fostering task-specific policies. Our approach mitigates forgetting in continual and offline reinforcement learning scenarios. Moreover, P2DT leverages trajectories collected via traditional reinforcement learning from all tasks and generates new task-specific tokens during training, thereby retaining knowledge from previous studies. Preliminary results demonstrate that our model effectively alleviates catastrophic forgetting and scales well with increasing task environments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Metrics to Meaning: Time to Rethink Evaluation in Human-AI Collaborative Design</title>
<link>https://arxiv.org/abs/2402.07911</link>
<guid>https://arxiv.org/abs/2402.07911</guid>
<content:encoded><![CDATA[
arXiv:2402.07911v2 Announce Type: replace-cross 
Abstract: As AI systems increasingly shape decision making in creative design contexts, understanding how humans engage with these tools has become a critical challenge for interactive intelligent systems research. This paper contributes a challenge to rethink how to evaluate human--AI collaborative systems, advocating for a more nuanced and multidimensional approach. Findings from one of the largest field studies to date (n = 808) of a human--AI co-creative system, The Genetic Car Designer, complemented by a controlled lab study (n = 12) are presented. The system is based on an interactive evolutionary algorithm where participants were tasked with designing a simple two dimensional representation of a car. Participants were exposed to galleries of design suggestions generated by an intelligent system, MAP--Elites, and a random control. Results indicate that exposure to galleries generated by MAP--Elites significantly enhanced both cognitive and behavioural engagement, leading to higher-quality design outcomes. Crucially for the wider community, the analysis reveals that conventional evaluation methods, which often focus on solely behavioural and design quality metrics, fail to capture the full spectrum of user engagement. By considering the human--AI design process as a changing emotional, behavioural and cognitive state of the designer, we propose evaluating human--AI systems holistically and considering intelligent systems as a core part of the user experience -- not simply a back end tool.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Machine Learning-Based Security and Privacy Protection Framework for Internet of Medical Things Systems</title>
<link>https://arxiv.org/abs/2403.09752</link>
<guid>https://arxiv.org/abs/2403.09752</guid>
<content:encoded><![CDATA[
arXiv:2403.09752v3 Announce Type: replace-cross 
Abstract: The Internet of Medical Things transcends traditional medical boundaries, enabling a transition from reactive treatment to proactive prevention. This innovative method revolutionizes healthcare by facilitating early disease detection and tailored care, particularly in chronic disease management, where IoMT automates treatments based on real-time health data collection. Nonetheless, its benefits are countered by significant security challenges that endanger the lives of its users due to the sensitivity and value of the processed data, thereby attracting malicious interests. Moreover, the utilization of wireless communication for data transmission exposes medical data to interception and tampering by cybercriminals. Additionally, anomalies may arise due to human error, network interference, or hardware malfunctions. In this context, anomaly detection based on Machine Learning (ML) is an interesting solution, but it comes up against obstacles in terms of explicability and privacy protection. To address these challenges, a new framework for Intrusion Detection Systems is introduced, leveraging Artificial Neural Networks for intrusion detection while utilizing Federated Learning for privacy preservation. Additionally, eXplainable Artificial Intelligence methods are incorporated to enhance model explanation and interpretation. The efficacy of the proposed framework is evaluated and compared with centralized approaches using multiple datasets containing network and medical data, simulating various attack types impacting the confidentiality, integrity, and availability of medical and physiological data. The results offer compelling evidence that the FL method performs comparably to the centralized method, demonstrating high performance. Additionally, it affords the dual advantage of safeguarding privacy and providing model explanation while adhering to ethical principles.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MF-OML: Online Mean-Field Reinforcement Learning with Occupation Measures for Large Population Games</title>
<link>https://arxiv.org/abs/2405.00282</link>
<guid>https://arxiv.org/abs/2405.00282</guid>
<content:encoded><![CDATA[
arXiv:2405.00282v2 Announce Type: replace-cross 
Abstract: Reinforcement learning for multi-agent games has attracted lots of attention recently. However, given the challenge of solving Nash equilibria for large population games, existing works with guaranteed polynomial complexities either focus on variants of zero-sum and potential games, or aim at solving (coarse) correlated equilibria, or require access to simulators, or rely on certain assumptions that are hard to verify. This work proposes MF-OML (Mean-Field Occupation-Measure Learning), an online mean-field reinforcement learning algorithm for computing approximate Nash equilibria of large population sequential symmetric games. MF-OML is the first fully polynomial multi-agent reinforcement learning algorithm for provably solving Nash equilibria (up to mean-field approximation gaps that vanish as the number of players $N$ goes to infinity) beyond variants of zero-sum and potential games. When evaluated by the cumulative deviation from Nash equilibria, the algorithm is shown to achieve a high probability regret bound of $\tilde{O}(M^{3/4}+N^{-1/2}M)$ for games with the strong Lasry-Lions monotonicity condition, and a regret bound of $\tilde{O}(M^{11/12}+N^{- 1/6}M)$ for games with only the Lasry-Lions monotonicity condition, where $M$ is the total number of episodes and $N$ is the number of agents of the game. As a byproduct, we also obtain the first tractable globally convergent computational algorithm for computing approximate Nash equilibria of monotone mean-field games.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention</title>
<link>https://arxiv.org/abs/2406.15486</link>
<guid>https://arxiv.org/abs/2406.15486</guid>
<content:encoded><![CDATA[
arXiv:2406.15486v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$ compared with FlashAttention.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Banishing LLM Hallucinations Requires Rethinking Generalization</title>
<link>https://arxiv.org/abs/2406.17642</link>
<guid>https://arxiv.org/abs/2406.17642</guid>
<content:encoded><![CDATA[
arXiv:2406.17642v2 Announce Type: replace-cross 
Abstract: Despite their powerful chat, coding, and reasoning abilities, Large Language Models (LLMs) frequently hallucinate. Conventional wisdom suggests that hallucinations are a consequence of a balance between creativity and factuality, which can be mitigated, but not eliminated, by grounding the LLM in external knowledge sources. Through extensive systematic experiments, we show that these traditional approaches fail to explain why LLMs hallucinate in practice. Specifically, we show that LLMs augmented with a massive Mixture of Memory Experts (MoME) can easily memorize large datasets of random numbers. We corroborate these experimental findings with a theoretical construction showing that simple neural networks trained to predict the next token hallucinate when the training loss is above a threshold as it usually does in practice when training on internet scale data. We interpret our findings by comparing against traditional retrieval methods for mitigating hallucinations. We use our findings to design a first generation model for removing hallucinations -- Lamini-1 -- that stores facts in a massive mixture of millions of memory experts that are retrieved dynamically.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</title>
<link>https://arxiv.org/abs/2407.00079</link>
<guid>https://arxiv.org/abs/2407.00079</guid>
<content:encoded><![CDATA[
arXiv:2407.00079v4 Announce Type: replace-cross 
Abstract: Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI. It features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. It also leverages the underutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated cache of KVCache. The core of Mooncake is its KVCache-centric scheduler, which balances maximizing overall effective throughput while meeting latency-related Service Level Objectives (SLOs). Unlike traditional studies that assume all requests will be processed, Mooncake faces challenges due to highly overloaded scenarios. To mitigate these, we developed a prediction-based early rejection policy. Experiments show that Mooncake excels in long-context scenarios. Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs. Under real workloads, Mooncake's innovative architecture enables Kimi to handle 75% more requests.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Agentic AI on Particle Accelerators</title>
<link>https://arxiv.org/abs/2409.06336</link>
<guid>https://arxiv.org/abs/2409.06336</guid>
<content:encoded><![CDATA[
arXiv:2409.06336v4 Announce Type: replace-cross 
Abstract: As particle accelerators grow in complexity, traditional control methods face increasing challenges in achieving optimal performance. This paper envisions a paradigm shift: a decentralized multi-agent framework for accelerator control, powered by Large Language Models (LLMs) and distributed among autonomous agents. We present a proposition of a self-improving decentralized system where intelligent agents handle high-level tasks and communication and each agent is specialized to control individual accelerator components.
  This approach raises some questions: What are the future applications of AI in particle accelerators? How can we implement an autonomous complex system such as a particle accelerator where agents gradually improve through experience and human feedback? What are the implications of integrating a human-in-the-loop component for labeling operational data and providing expert guidance? We show three examples, where we demonstrate the viability of such architecture.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Machine and Human Visual Representations across Abstraction Levels</title>
<link>https://arxiv.org/abs/2409.06509</link>
<guid>https://arxiv.org/abs/2409.06509</guid>
<content:encoded><![CDATA[
arXiv:2409.06509v4 Announce Type: replace-cross 
Abstract: Deep neural networks have achieved success across a wide range of applications, including as models of human behavior and neural representations in vision tasks. However, neural network training and human learning differ in fundamental ways, and neural networks often fail to generalize as robustly as humans do raising questions regarding the similarity of their underlying representations. What is missing for modern learning systems to exhibit more human-aligned behavior? We highlight a key misalignment between vision models and humans: whereas human conceptual knowledge is hierarchically organized from fine- to coarse-scale distinctions, model representations do not accurately capture all these levels of abstraction. To address this misalignment, we first train a teacher model to imitate human judgments, then transfer human-aligned structure from its representations to refine the representations of pretrained state-of-the-art vision foundation models via finetuning. These human-aligned models more accurately approximate human behavior and uncertainty across a wide range of similarity tasks, including a new dataset of human judgments spanning multiple levels of semantic abstractions. They also perform better on a diverse set of machine learning tasks, increasing generalization and out-of-distribution robustness. Thus, infusing neural networks with additional human knowledge yields a best-of-both-worlds representation that is both more consistent with human cognitive judgments and more practically useful, thus paving the way toward more robust, interpretable, and human-aligned artificial intelligence systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Consistency Representation Learning for Lifelong Person Re-Identification</title>
<link>https://arxiv.org/abs/2409.19954</link>
<guid>https://arxiv.org/abs/2409.19954</guid>
<content:encoded><![CDATA[
arXiv:2409.19954v4 Announce Type: replace-cross 
Abstract: Lifelong person re-identification (LReID) exhibits a contradictory relationship between intra-domain discrimination and inter-domain gaps when learning from continuous data. Intra-domain discrimination focuses on individual nuances (i.e., clothing type, accessories, etc.), while inter-domain gaps emphasize domain consistency. Achieving a trade-off between maximizing intra-domain discrimination and minimizing inter-domain gaps is a crucial challenge for improving LReID performance. Most existing methods strive to reduce inter-domain gaps through knowledge distillation to maintain domain consistency. However, they often ignore intra-domain discrimination. To address this challenge, we propose a novel domain consistency representation learning (DCR) model that explores global and attribute-wise representations as a bridge to balance intra-domain discrimination and inter-domain gaps. At the intra-domain level, we explore the complementary relationship between global and attribute-wise representations to improve discrimination among similar identities. Excessive learning intra-domain discrimination can lead to catastrophic forgetting. We further develop an attribute-oriented anti-forgetting (AF) strategy that explores attribute-wise representations to enhance inter-domain consistency, and propose a knowledge consolidation (KC) strategy to facilitate knowledge transfer. Extensive experiments show that our DCR achieves superior performance compared to state-of-the-art LReID methods. Our code is available at https://github.com/LiuShiBen/DCR.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling</title>
<link>https://arxiv.org/abs/2410.16033</link>
<guid>https://arxiv.org/abs/2410.16033</guid>
<content:encoded><![CDATA[
arXiv:2410.16033v4 Announce Type: replace-cross 
Abstract: Inference-time alignment enhances the performance of large language models without requiring additional training or fine-tuning but presents challenges due to balancing computational efficiency with high-quality output. Best-of-N (BoN) sampling, as a simple yet powerful approach, generates multiple responses and selects the best one, achieving improved performance but with a high computational cost. We propose TreeBoN, a novel framework that integrates a speculative tree-search strategy into Best-of-N (BoN) Sampling. TreeBoN maintains a set of parent nodes, iteratively branching and pruning low-quality responses, thereby reducing computational overhead while maintaining high output quality. Our approach also leverages token-level rewards from Direct Preference Optimization (DPO) to guide tree expansion and prune low-quality paths. We evaluate TreeBoN using AlpacaFarm, HH-RLHF, UltraFeedback, GSM8K, and TutorEval datasets, demonstrating consistent improvements. Specifically, TreeBoN achieves the highest win rate of 65% on TutorEval and around 60% win rates across other different datasets, outperforming standard BoN with the same computational cost and showcasing its scalability and alignment efficacy.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft-TransFormers for Continual Learning</title>
<link>https://arxiv.org/abs/2411.16073</link>
<guid>https://arxiv.org/abs/2411.16073</guid>
<content:encoded><![CDATA[
arXiv:2411.16073v2 Announce Type: replace-cross 
Abstract: Inspired by the Well-initialized Lottery Ticket Hypothesis (WLTH), which provides suboptimal fine-tuning solutions, we propose a novel fully fine-tuned continual learning (CL) method referred to as Soft-TransFormers (Soft-TF). Soft-TF sequentially learns and selects an optimal soft-network for each task. During sequential training in CL, a well-initialized Soft-TF mask optimizes the weights of sparse layers to obtain task-adaptive soft (real-valued) networks, while keeping the well-pre-trained layer parameters frozen. In inference, the identified task-adaptive network of Soft-TF masks the parameters of the pre-trained network, mapping to an optimal solution for each task and minimizing Catastrophic Forgetting (CF) - the soft-masking preserves the knowledge of the pre-trained network. Extensive experiments on the Vision Transformer (ViT) and the Language Transformer (Bert) demonstrate the effectiveness of Soft-TF, achieving state-of-the-art performance across Vision and Language Class Incremental Learning (CIL) scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GalaxAlign: Mimicking Citizen Scientists' Multimodal Guidance for Galaxy Morphology Analysis</title>
<link>https://arxiv.org/abs/2411.19475</link>
<guid>https://arxiv.org/abs/2411.19475</guid>
<content:encoded><![CDATA[
arXiv:2411.19475v2 Announce Type: replace-cross 
Abstract: Galaxy morphology analysis involves studying galaxies based on their shapes and structures. For such studies, fundamental tasks include identifying and classifying galaxies in astronomical images, as well as retrieving visually or structurally similar galaxies through similarity search. Existing methods either directly train domain-specific foundation models on large, annotated datasets or fine-tune vision foundation models on a smaller set of images. The former is effective but costly, while the latter is more resource-efficient but often yields lower accuracy. To address these challenges, we introduce GalaxAlign, a multimodal approach inspired by how citizen scientists identify galaxies in astronomical images by following textual descriptions and matching schematic symbols. Specifically, GalaxAlign employs a tri-modal alignment framework to align three types of data during fine-tuning: (1) schematic symbols representing galaxy shapes and structures, (2) textual labels for these symbols, and (3) galaxy images. By incorporating multimodal instructions, GalaxAlign eliminates the need for expensive pretraining and enhances the effectiveness of fine-tuning. Experiments on galaxy classification and similarity search demonstrate that our method effectively fine-tunes general pre-trained models for astronomical tasks by incorporating domain-specific multi-modal knowledge. Code is available at https://github.com/RapidsAtHKUST/GalaxAlign.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning</title>
<link>https://arxiv.org/abs/2501.08848</link>
<guid>https://arxiv.org/abs/2501.08848</guid>
<content:encoded><![CDATA[
arXiv:2501.08848v2 Announce Type: replace-cross 
Abstract: Network simulation is pivotal in network modeling, assisting with tasks ranging from capacity planning to performance estimation. Traditional approaches such as Discrete Event Simulation (DES) face limitations in terms of computational cost and accuracy. This paper introduces RouteNet-Gauss, a novel integration of a testbed network with a Machine Learning (ML) model to address these challenges. By using the testbed as a hardware accelerator, RouteNet-Gauss generates training datasets rapidly and simulates network scenarios with high fidelity to real-world conditions. Experimental results show that RouteNet-Gauss significantly reduces prediction errors by up to 95% and achieves a 488x speedup in inference time compared to state-of-the-art DES-based methods. RouteNet-Gauss's modular architecture is dynamically constructed based on the specific characteristics of the network scenario, such as topology and routing. This enables it to understand and generalize to different network configurations beyond those seen during training, including networks up to 10x larger. Additionally, it supports Temporal Aggregated Performance Estimation (TAPE), providing configurable temporal granularity and maintaining high accuracy in flow performance metrics. This approach shows promise in improving both simulation efficiency and accuracy, offering a valuable tool for network operators.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2501.09997</link>
<guid>https://arxiv.org/abs/2501.09997</guid>
<content:encoded><![CDATA[
arXiv:2501.09997v3 Announce Type: replace-cross 
Abstract: Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational overhead, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey on Hand Gesture Recognition from Visual Input</title>
<link>https://arxiv.org/abs/2501.11992</link>
<guid>https://arxiv.org/abs/2501.11992</guid>
<content:encoded><![CDATA[
arXiv:2501.11992v3 Announce Type: replace-cross 
Abstract: Hand gesture recognition has become an important research area, driven by the growing demand for human-computer interaction in fields such as sign language recognition, virtual and augmented reality, and robotics. Despite the rapid growth of the field, there are few surveys that comprehensively cover recent research developments, available solutions, and benchmark datasets. This survey addresses this gap by examining the latest advancements in hand gesture and 3D hand pose recognition from various types of camera input data including RGB images, depth images, and videos from monocular or multiview cameras, examining the differing methodological requirements of each approach. Furthermore, an overview of widely used datasets is provided, detailing their main characteristics and application domains. Finally, open challenges such as achieving robust recognition in real-world environments, handling occlusions, ensuring generalization across diverse users, and addressing computational efficiency for real-time applications are highlighted to guide future research directions. By synthesizing the objectives, methodologies, and applications of recent studies, this survey offers valuable insights into current trends, challenges, and opportunities for future research in human hand gesture recognition.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predict, Cluster, Refine: A Joint Embedding Predictive Self-Supervised Framework for Graph Representation Learning</title>
<link>https://arxiv.org/abs/2502.01684</link>
<guid>https://arxiv.org/abs/2502.01684</guid>
<content:encoded><![CDATA[
arXiv:2502.01684v4 Announce Type: replace-cross 
Abstract: Graph representation learning has emerged as a cornerstone for tasks like node classification and link prediction, yet prevailing self-supervised learning (SSL) methods face challenges such as computational inefficiency, reliance on contrastive objectives, and representation collapse. Existing approaches often depend on feature reconstruction, negative sampling, or complex decoders, which introduce training overhead and hinder generalization. Further, current techniques which address such limitations fail to account for the contribution of node embeddings to a certain prediction in the absence of labeled nodes. To address these limitations, we propose a novel joint embedding predictive framework for graph SSL that eliminates contrastive objectives and negative sampling while preserving semantic and structural information. Additionally, we introduce a semantic-aware objective term that incorporates pseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node discriminability by evaluating latent feature contributions. Extensive experiments demonstrate that our framework outperforms state-of-the-art graph SSL methods across benchmarks, achieving superior performance without contrastive loss or complex decoders. Key innovations include (1) a non-contrastive, view-invariant joint embedding predictive architecture, (2) Leveraging single context and multiple targets relationship between subgraphs, and (3) GMM-based pseudo-label scoring to capture semantic contributions. This work advances graph SSL by offering a computationally efficient, collapse-resistant paradigm that bridges spatial and semantic graph features for downstream tasks. The code for our paper can be found at https://github.com/Deceptrax123/JPEB-GSSL
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual LLMs</title>
<link>https://arxiv.org/abs/2502.04387</link>
<guid>https://arxiv.org/abs/2502.04387</guid>
<content:encoded><![CDATA[
arXiv:2502.04387v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) has enabled the training of multilingual large language models (LLMs) on diverse and decentralized multilingual data, especially on low-resource languages. To improve client-specific performance, personalization via the use of parameter-efficient fine-tuning (PEFT) modules such as LoRA is common. This involves a personalization strategy (PS), such as the design of the PEFT adapter structures (e.g., in which layers to add LoRAs and what ranks) and choice of hyperparameters (e.g., learning rates) for fine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a federated learning-to-personalize method for multilingual LLMs in cross-device FL settings. Unlike most existing PEFT structure selection methods, which are prone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the optimal personalized PEFT structure for each client via Bayesian sparse rank selection. Evaluations on both simulated and real-world multilingual FL benchmarks demonstrate that FedP$^2$EFT largely outperforms existing personalized fine-tuning methods, while complementing other existing FL methods. Code is available at https://github.com/SamsungLabs/fedp2eft.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid Word Learning Through Meta In-Context Learning</title>
<link>https://arxiv.org/abs/2502.14791</link>
<guid>https://arxiv.org/abs/2502.14791</guid>
<content:encoded><![CDATA[
arXiv:2502.14791v3 Announce Type: replace-cross 
Abstract: Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating a Model-Agnostic and Imputation-Free Approach for Irregularly-Sampled Multivariate Time-Series Modeling</title>
<link>https://arxiv.org/abs/2502.15785</link>
<guid>https://arxiv.org/abs/2502.15785</guid>
<content:encoded><![CDATA[
arXiv:2502.15785v2 Announce Type: replace-cross 
Abstract: Modeling Irregularly-sampled and Multivariate Time Series (IMTS) is crucial across a variety of applications where different sets of variates may be missing at different time-steps due to sensor malfunctions or high data acquisition costs. Existing approaches for IMTS either consider a two-stage impute-then-model framework or involve specialized architectures specific to a particular model and task. We perform a series of experiments to derive novel insights about the performance of IMTS methods on a variety of semi-synthetic and real-world datasets for both classification and forecasting. We also introduce Missing Feature-aware Time Series Modeling (MissTSM) or MissTSM, a novel model-agnostic and imputation-free approach for IMTS modeling. We show that MissTSM shows competitive performance compared to other IMTS approaches, especially when the amount of missing values is large and the data lacks simplistic periodic structures - conditions common to real-world IMTS applications.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs</title>
<link>https://arxiv.org/abs/2502.18179</link>
<guid>https://arxiv.org/abs/2502.18179</guid>
<content:encoded><![CDATA[
arXiv:2502.18179v2 Announce Type: replace-cross 
Abstract: This paper defines and explores the design space for information extraction (IE) from layout-rich documents using large language models (LLMs). The three core challenges of layout-aware IE with LLMs are 1) data structuring, 2) model engagement, and 3) output refinement. Our study investigates the sub-problems and methods within these core challenges, such as input representation, chunking, prompting, selection of LLMs, and multimodal models. It examines the effect of different design choices through LayIE-LLM, a new, open-source, layout-aware IE test suite, benchmarking against traditional, fine-tuned IE models. The results on two IE datasets show that LLMs require adjustment of the IE pipeline to achieve competitive performance: the optimized configuration found with LayIE-LLM achieves 13.3--37.5 F1 points more than a general-practice baseline configuration using the same LLM. To find a well-working configuration, we develop a one-factor-at-a-time (OFAT) method that achieves near-optimal results. Our method is only 0.8--1.8 points lower than the best full factorial exploration with a fraction (2.8%) of the required computation. Overall, we demonstrate that, if well-configured, general-purpose LLMs match the performance of specialized models, providing a cost-effective, finetuning-free alternative. Our test-suite is available at https://github.com/gayecolakoglu/LayIE-LLM.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiently Editing Mixture-of-Experts Models with Compressed Experts</title>
<link>https://arxiv.org/abs/2503.00634</link>
<guid>https://arxiv.org/abs/2503.00634</guid>
<content:encoded><![CDATA[
arXiv:2503.00634v2 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) models have become a key approach for scaling large language models efficiently by activating only a subset of experts during training and inference. Typically, the number of activated experts presents a trade-off: fewer experts reduce computational costs, while more experts improve performance. Recent studies reveal that not all activated experts contribute equally to model performance, with some providing minimal utility, particularly when finetuning pretrained MoE models for specialized downstream tasks. The co-existence of significant and redundant parameters in experts provides us an opportunity to reduce the number of activated experts while maintaining model performance. In this work, we propose the concept of compressed experts, lightweight modules that serve as compact representations of full experts. Our approach preserves the most important experts while replacing other auxiliary activated experts with compressed experts. The reduction of active parameters significantly lowers inference costs while achieving comparable performance. Extensive experiments on models including Phi-MoE and OLMoE demonstrate that compressed experts recover over 90% of full expert performance across various tasks while reducing more than 30% active parameters and saving 20% in inference costs. This approach enables efficient deployment of MoE models in resource-constrained settings and facilitates scaling to larger models with manageable overhead. Our code is available at https://github.com/yifei-he/Compressed-Experts.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impoola: The Power of Average Pooling for Image-Based Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.05546</link>
<guid>https://arxiv.org/abs/2503.05546</guid>
<content:encoded><![CDATA[
arXiv:2503.05546v2 Announce Type: replace-cross 
Abstract: As image-based deep reinforcement learning tackles more challenging tasks, increasing model size has become an important factor in improving performance. Recent studies achieved this by focusing on the parameter efficiency of scaled networks, typically using Impala-CNN, a 15-layer ResNet-inspired network, as the image encoder. However, while Impala-CNN evidently outperforms older CNN architectures, potential advancements in network design for deep reinforcement learning-specific image encoders remain largely unexplored. We find that replacing the flattening of output feature maps in Impala-CNN with global average pooling leads to a notable performance improvement. This approach outperforms larger and more complex models in the Procgen Benchmark, particularly in terms of generalization. We call our proposed encoder model Impoola-CNN. A decrease in the network's translation sensitivity may be central to this improvement, as we observe the most significant gains in games without agent-centered observations. Our results demonstrate that network scaling is not just about increasing model size - efficient network design is also an essential factor. We make our code available at https://github.com/raphajaner/impoola.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TruthLens: Visual Grounding for Universal DeepFake Reasoning</title>
<link>https://arxiv.org/abs/2503.15867</link>
<guid>https://arxiv.org/abs/2503.15867</guid>
<content:encoded><![CDATA[
arXiv:2503.15867v3 Announce Type: replace-cross 
Abstract: Detecting DeepFakes has become a crucial research area as the widespread use of AI image generators enables the effortless creation of face-manipulated and fully synthetic content, while existing methods are often limited to binary classification (real vs. fake) and lack interpretability. To address these challenges, we propose TruthLens, a novel, unified, and highly generalizable framework that goes beyond traditional binary classification, providing detailed, textual reasoning for its predictions. Distinct from conventional methods, TruthLens performs MLLM grounding.
  TruthLens uses a task-driven representation integration strategy that unites global semantic context from a multimodal large language model (MLLM) with region-specific forensic cues through explicit cross-modal adaptation of a vision-only model. This enables nuanced, region-grounded reasoning for both face-manipulated and fully synthetic content, and supports fine-grained queries such as "Does the eyes/nose/mouth look real or fake?"- capabilities beyond pretrained MLLMs alone. Extensive experiments across diverse datasets demonstrate that TruthLens sets a new benchmark in both forensic interpretability and detection accuracy, generalizing to seen and unseen manipulations alike. By unifying high-level scene understanding with fine-grained region grounding, TruthLens delivers transparent DeepFake forensics, bridging a critical gap in the literature.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDVIO2.0: Wind and Disturbance Estimation with Hybrid Dynamics VIO</title>
<link>https://arxiv.org/abs/2504.00969</link>
<guid>https://arxiv.org/abs/2504.00969</guid>
<content:encoded><![CDATA[
arXiv:2504.00969v3 Announce Type: replace-cross 
Abstract: Visual-inertial odometry (VIO) is widely used for state estimation in autonomous micro aerial vehicles using onboard sensors. Current methods improve VIO by incorporating a model of the translational vehicle dynamics, yet their performance degrades when faced with low-accuracy vehicle models or continuous external disturbances, like wind. Additionally, incorporating rotational dynamics in these models is computationally intractable when they are deployed in online applications, e.g., in a closed-loop control system. We present HDVIO2.0, which models full 6-DoF, translational and rotational, vehicle dynamics and tightly incorporates them into a VIO with minimal impact on the runtime. HDVIO2.0 builds upon the previous work, HDVIO, and addresses these challenges through a hybrid dynamics model combining a point-mass vehicle model with a learning-based component, with access to control commands and IMU history, to capture complex aerodynamic effects. The key idea behind modeling the rotational dynamics is to represent them with continuous-time functions. HDVIO2.0 leverages the divergence between the actual motion and the predicted motion from the hybrid dynamics model to estimate external forces as well as the robot state. Our system surpasses the performance of state-of-the-art methods in experiments using public and new drone dynamics datasets, as well as real-world flights in winds up to 25 km/h. Unlike existing approaches, we also show that accurate vehicle dynamics predictions are achievable without precise knowledge of the full vehicle state.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond</title>
<link>https://arxiv.org/abs/2504.13037</link>
<guid>https://arxiv.org/abs/2504.13037</guid>
<content:encoded><![CDATA[
arXiv:2504.13037v4 Announce Type: replace-cross 
Abstract: Cardiac magnetic resonance imaging is the gold standard for non-invasive cardiac assessment, offering rich spatio-temporal views of the cardiac anatomy and physiology. Patient-level health factors, such as demographics, metabolic, and lifestyle, are known to substantially influence cardiovascular health and disease risk, yet remain uncaptured by CMR alone. To holistically understand cardiac health and to enable the best possible interpretation of an individual's disease risk, CMR and patient-level factors must be jointly exploited within an integrated framework. Recent multi-modal approaches have begun to bridge this gap, yet they often rely on limited spatio-temporal data and focus on isolated clinical tasks, thereby hindering the development of a comprehensive representation for cardiac health evaluation. To overcome these limitations, we introduce ViTa, a step toward foundation models that delivers a comprehensive representation of the heart and a precise interpretation of individual disease risk. Leveraging data from 42,000 UK Biobank participants, ViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling a complete capture of the cardiac cycle. These imaging data are then fused with detailed tabular patient-level factors, enabling context-aware insights. This multi-modal paradigm supports a wide spectrum of downstream tasks, including cardiac phenotype and physiological feature prediction, segmentation, and classification of cardiac and metabolic diseases within a single unified framework. By learning a shared latent representation that bridges rich imaging features and patient context, ViTa moves beyond traditional, task-specific models toward a universal, patient-specific understanding of cardiac health, highlighting its potential to advance clinical utility and scalability in cardiac analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildFireCan-MMD: A Multimodal Dataset for Classification of User-Generated Content During Wildfires in Canada</title>
<link>https://arxiv.org/abs/2504.13231</link>
<guid>https://arxiv.org/abs/2504.13231</guid>
<content:encoded><![CDATA[
arXiv:2504.13231v3 Announce Type: replace-cross 
Abstract: Rapid information access is vital during wildfires, yet traditional data sources are slow and costly. Social media offers real-time updates, but extracting relevant insights remains a challenge. In this work, we focus on multimodal wildfire social media data, which, although existing in current datasets, is currently underrepresented in Canadian contexts. We present WildFireCan-MMD, a new multimodal dataset of X posts from recent Canadian wildfires, annotated across twelve key themes. We evaluate zero-shot vision-language models on this dataset and compare their results with those of custom-trained and baseline classifiers. We show that while baseline methods and zero-shot prompting offer quick deployment, custom-trained models outperform them when labelled data is available. Our best-performing custom model reaches 84.48% f-score, outperforming VLMs and baseline classifiers. We also demonstrate how this model can be used to uncover trends during wildfires, through the collection and analysis of a large unlabeled dataset. Our dataset facilitates future research in wildfire response, and our findings highlight the importance of tailored datasets and task-specific training. Importantly, such datasets should be localized, as disaster response requirements vary across regions and contexts.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Developers' Self-Declaration of AI-Generated Code: An Analysis of Practices</title>
<link>https://arxiv.org/abs/2504.16485</link>
<guid>https://arxiv.org/abs/2504.16485</guid>
<content:encoded><![CDATA[
arXiv:2504.16485v2 Announce Type: replace-cross 
Abstract: AI code generation tools have gained significant popularity among developers, who use them to assist in software development due to their capability to generate code. Existing studies mainly explored the quality, e.g., correctness and security, of AI-generated code, while in real-world software development, the prerequisite is to distinguish AI-generated code from human-written code, which emphasizes the need to explicitly declare AI-generated code by developers. To this end, this study intends to understand the ways developers use to self-declare AI-generated code and explore the reasons why developers choose to self-declare or not. We conducted a mixed-methods study consisting of two phases. In the first phase, we mined GitHub repositories and collected 613 instances of AI-generated code snippets. In the second phase, we conducted a follow-up practitioners' survey, which received 111 valid responses. Our research revealed the practices followed by developers to self-declare AI-generated code. Most practitioners (76.6%) always or sometimes self-declare AI-generated code. In contrast, other practitioners (23.4%) noted that they never self-declare AI-generated code. The reasons for self-declaring AI-generated code include the need to track and monitor the code for future review and debugging, and ethical considerations. The reasons for not self-declaring AI-generated code include extensive modifications to AI-generated code and the developers' perception that self-declaration is an unnecessary activity. We finally provided guidelines for practitioners to self-declare AI-generated code, addressing ethical and code quality concerns.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LawFlow: Collecting and Simulating Lawyers' Thought Processes on Business Formation Case Studies</title>
<link>https://arxiv.org/abs/2504.18942</link>
<guid>https://arxiv.org/abs/2504.18942</guid>
<content:encoded><![CDATA[
arXiv:2504.18942v2 Announce Type: replace-cross 
Abstract: Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, we introduce LawFlow, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, LawFlow captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice. Using LawFlow, we compare human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. Our findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Our results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems.
  All data and code are available on our project page (https://minnesotanlp.github.io/LawFlow-website/).
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group-in-Group Policy Optimization for LLM Agent Training</title>
<link>https://arxiv.org/abs/2505.10978</link>
<guid>https://arxiv.org/abs/2505.10978</guid>
<content:encoded><![CDATA[
arXiv:2505.10978v2 Announce Type: replace-cross 
Abstract: Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to long-horizon LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals and achieves performance gains of > 12\% on ALFWorld and > 9\% on WebShop over the GRPO baseline: all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When a Reinforcement Learning Agent Encounters Unknown Unknowns</title>
<link>https://arxiv.org/abs/2505.13188</link>
<guid>https://arxiv.org/abs/2505.13188</guid>
<content:encoded><![CDATA[
arXiv:2505.13188v2 Announce Type: replace-cross 
Abstract: An AI agent might surprisingly find she has reached an unknown state which she has never been aware of -- an unknown unknown. We mathematically ground this scenario in reinforcement learning: an agent, after taking an action calculated from value functions $Q$ and $V$ defined on the {\it {aware domain}}, reaches a state out of the domain. To enable the agent to handle this scenario, we propose an {\it episodic Markov decision {process} with growing awareness} (EMDP-GA) model, taking a new {\it noninformative value expansion} (NIVE) approach to expand value functions to newly aware areas: when an agent arrives at an unknown unknown, value functions $Q$ and $V$ whereon are initialised by noninformative beliefs -- the averaged values on the aware domain. This design is out of respect for the complete absence of knowledge in the newly discovered state. The upper confidence bound momentum Q-learning is then adapted to the growing awareness for training the EMDP-GA model. We prove that (1) the regret of our approach is asymptotically consistent with the state of the art (SOTA) without exposure to unknown unknowns in an extremely uncertain environment, and (2) our computational complexity and space complexity are comparable with the SOTA -- these collectively suggest that though an unknown unknown is surprising, it will be asymptotically properly discovered with decent speed and an affordable cost.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16022</link>
<guid>https://arxiv.org/abs/2505.16022</guid>
<content:encoded><![CDATA[
arXiv:2505.16022v2 Announce Type: replace-cross 
Abstract: Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language model's output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train. In this work, we propose NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across a wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation</title>
<link>https://arxiv.org/abs/2505.20353</link>
<guid>https://arxiv.org/abs/2505.20353</guid>
<content:encoded><![CDATA[
arXiv:2505.20353v2 Announce Type: replace-cross 
Abstract: Diffusion Transformers (DiT) are powerful generative models but remain computationally intensive due to their iterative structure and deep transformer stacks. To alleviate this inefficiency, we propose FastCache, a hidden-state-level caching and compression framework that accelerates DiT inference by exploiting redundancy within the model's internal representations. FastCache introduces a dual strategy: (1) a spatial-aware token selection mechanism that adaptively filters redundant tokens based on hidden state saliency, and (2) a transformer-level cache that reuses latent activations across timesteps when changes are statistically insignificant. These modules work jointly to reduce unnecessary computation while preserving generation fidelity through learnable linear approximation. Theoretical analysis shows that FastCache maintains bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across multiple DiT variants demonstrate substantial reductions in latency and memory usage, with best generation output quality compared to other cache methods, as measured by FID and t-FID. Code implementation of FastCache is available on GitHub at https://github.com/NoakLiu/FastCache-xDiT.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing AI Agents with Information-Flow Control</title>
<link>https://arxiv.org/abs/2505.23643</link>
<guid>https://arxiv.org/abs/2505.23643</guid>
<content:encoded><![CDATA[
arXiv:2505.23643v2 Announce Type: replace-cross 
Abstract: As AI agents become increasingly autonomous and capable, ensuring their security against vulnerabilities such as prompt injection becomes critical. This paper explores the use of information-flow control (IFC) to provide security guarantees for AI agents. We present a formal model to reason about the security and expressiveness of agent planners. Using this model, we characterize the class of properties enforceable by dynamic taint-tracking and construct a taxonomy of tasks to evaluate security and utility trade-offs of planner designs. Informed by this exploration, we present Fides, a planner that tracks confidentiality and integrity labels, deterministically enforces security policies, and introduces novel primitives for selectively hiding information. Its evaluation in AgentDojo demonstrates that this approach enables us to complete a broad range of tasks with security guarantees. A tutorial to walk readers through the the concepts introduced in the paper can be found at https://github.com/microsoft/fides
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Embedding-based Attribution (LEA): Quantifying Source Contributions to Generative Model's Response for Vulnerability Analysis</title>
<link>https://arxiv.org/abs/2506.12100</link>
<guid>https://arxiv.org/abs/2506.12100</guid>
<content:encoded><![CDATA[
arXiv:2506.12100v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly used for cybersecurity threat analysis, but their deployment in security-sensitive environments raises trust and safety concerns. With over 21,000 vulnerabilities disclosed in 2025, manual analysis is infeasible, making scalable and verifiable AI support critical. When querying LLMs, dealing with emerging vulnerabilities is challenging as they have a training cut-off date. While Retrieval-Augmented Generation (RAG) can inject up-to-date context to alleviate the cut-off date limitation, it remains unclear how much LLMs rely on retrieved evidence versus the model's internal knowledge, and whether the retrieved information is meaningful or even correct. This uncertainty could mislead security analysts, mis-prioritize patches, and increase security risks. Therefore, this work proposes LLM Embedding-based Attribution (LEA) to analyze the generated responses for vulnerability exploitation analysis. More specifically, LEA quantifies the relative contribution of internal knowledge vs. retrieved content in the generated responses. We evaluate LEA on 500 critical vulnerabilities disclosed between 2016 and 2025, across three RAG settings -- valid, generic, and incorrect -- using three state-of-the-art LLMs. Our results demonstrate LEA's ability to detect clear distinctions between non-retrieval, generic-retrieval, and valid-retrieval scenarios with over 95% accuracy on larger models. Finally, we demonstrate the limitations posed by incorrect retrieval of vulnerability information and raise a cautionary note to the cybersecurity community regarding the blind reliance on LLMs and RAG for vulnerability analysis. LEA offers security analysts with a metric to audit RAG-enhanced workflows, improving the transparent and trustworthy deployment of AI in cybersecurity threat analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Clustering of Neural Bandits: Selective Reinitialization for Mitigating Loss of Plasticity</title>
<link>https://arxiv.org/abs/2506.12389</link>
<guid>https://arxiv.org/abs/2506.12389</guid>
<content:encoded><![CDATA[
arXiv:2506.12389v2 Announce Type: replace-cross 
Abstract: Clustering of Bandits (CB) methods enhance sequential decision-making by grouping bandits into clusters based on similarity and incorporating cluster-level contextual information, demonstrating effectiveness and adaptability in applications like personalized streaming recommendations. However, when extending CB algorithms to their neural version (commonly referred to as Clustering of Neural Bandits, or CNB), they suffer from loss of plasticity, where neural network parameters become rigid and less adaptable over time, limiting their ability to adapt to non-stationary environments (e.g., dynamic user preferences in recommendation). To address this challenge, we propose Selective Reinitialization (SeRe), a novel bandit learning framework that dynamically preserves the adaptability of CNB algorithms in evolving environments. SeRe leverages a contribution utility metric to identify and selectively reset underutilized units, mitigating loss of plasticity while maintaining stable knowledge retention. Furthermore, when combining SeRe with CNB algorithms, the adaptive change detection mechanism adjusts the reinitialization frequency according to the degree of non-stationarity, ensuring effective adaptation without unnecessary resets. Theoretically, we prove that SeRe enables sublinear cumulative regret in piecewise-stationary environments, outperforming traditional CNB approaches in long-term performances. Extensive experiments on six real-world recommendation datasets demonstrate that SeRe-enhanced CNB algorithms can effectively mitigate the loss of plasticity with lower regrets, improving adaptability and robustness in dynamic settings.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware Learning</title>
<link>https://arxiv.org/abs/2506.13265</link>
<guid>https://arxiv.org/abs/2506.13265</guid>
<content:encoded><![CDATA[
arXiv:2506.13265v3 Announce Type: replace-cross 
Abstract: Autonomous vehicles that navigate in open-world environments may encounter previously unseen object classes. However, most existing LiDAR panoptic segmentation models rely on closed-set assumptions, failing to detect unknown object instances. In this work, we propose ULOPS, an uncertainty-guided open-set panoptic segmentation framework that leverages Dirichlet-based evidential learning to model predictive uncertainty. Our architecture incorporates separate decoders for semantic segmentation with uncertainty estimation, embedding with prototype association, and instance center prediction. During inference, we leverage uncertainty estimates to identify and segment unknown instances. To strengthen the model's ability to differentiate between known and unknown objects, we introduce three uncertainty-driven loss functions. Uniform Evidence Loss to encourage high uncertainty in unknown regions. Adaptive Uncertainty Separation Loss ensures a consistent difference in uncertainty estimates between known and unknown objects at a global scale. Contrastive Uncertainty Loss refines this separation at the fine-grained level. To evaluate open-set performance, we extend benchmark settings on KITTI-360 and introduce a new open-set evaluation for nuScenes. Extensive experiments demonstrate that ULOPS consistently outperforms existing open-set LiDAR panoptic segmentation methods.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Medical Image Binding via Shared Text Embeddings</title>
<link>https://arxiv.org/abs/2506.18072</link>
<guid>https://arxiv.org/abs/2506.18072</guid>
<content:encoded><![CDATA[
arXiv:2506.18072v2 Announce Type: replace-cross 
Abstract: Medical image analysis increasingly relies on the integration of multiple imaging modalities to capture complementary anatomical and functional information, enabling more accurate diagnosis and treatment planning. Achieving aligned feature representations across these diverse modalities is therefore important for effective multimodal analysis. While contrastive language-image pre-training (CLIP) and its variant have enabled image-text alignments, they require explicitly paired data between arbitrary two modalities, which is difficult to acquire in medical contexts. To address the gap, we present Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel pre-training framework that enables seamless alignment of multiple medical imaging modalities through a shared text representation space without requiring explicit paired data between any two medical image modalities. Specifically, based on the insight that different images can naturally bind with text, M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text models to align their modality-specific text embedding space while preserving their original image-text alignments. Subsequently, we distill these modality-specific text encoders into a unified model, creating a shared text embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves state-of-the-art performance in zero-shot, few-shot classification and cross-modal retrieval tasks compared to its CLIP-like counterparts. These results validate M\textsuperscript{3}Bind's effectiveness in achieving cross-image-modal alignment for medical analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization</title>
<link>https://arxiv.org/abs/2506.19992</link>
<guid>https://arxiv.org/abs/2506.19992</guid>
<content:encoded><![CDATA[
arXiv:2506.19992v2 Announce Type: replace-cross 
Abstract: The explosive growth of complex datasets across various modalities necessitates advanced analytical tools that not only group data effectively but also provide human-understandable insights into the discovered structures. We introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization), a novel algorithm and Python package designed for hierarchical k-means clustering of diverse data types, including text, images, and numeric data (processed one modality per run). HERCULES constructs a cluster hierarchy by recursively applying k-means clustering, starting from individual data points at level 0. A key innovation is its deep integration of Large Language Models (LLMs) to generate semantically rich titles and descriptions for clusters at each level of the hierarchy, significantly enhancing interpretability. The algorithm supports two main representation modes: `direct' mode, which clusters based on original data embeddings or scaled numeric features, and `description' mode, which clusters based on embeddings derived from LLM-generated summaries. Users can provide a `topic\_seed' to guide LLM-generated summaries towards specific themes. An interactive visualization tool facilitates thorough analysis and understanding of the clustering results. We demonstrate HERCULES's capabilities and discuss its potential for extracting meaningful, hierarchical knowledge from complex datasets.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</title>
<link>https://arxiv.org/abs/2506.21619</link>
<guid>https://arxiv.org/abs/2506.21619</guid>
<content:encoded><![CDATA[
arXiv:2506.21619v2 Announce Type: replace-cross 
Abstract: Existing autoregressive large-scale text-to-speech (TTS) models have advantages in speech naturalness, but their token-by-token generation mechanism makes it difficult to precisely control the duration of synthesized speech. This becomes a significant limitation in applications requiring strict audio-visual synchronization, such as video dubbing. This paper introduces IndexTTS2, which proposes a novel, general, and autoregressive model-friendly method for speech duration control. The method supports two generation modes: one explicitly specifies the number of generated tokens to precisely control speech duration; the other freely generates speech in an autoregressive manner without specifying the number of tokens, while faithfully reproducing the prosodic features of the input prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional expression and speaker identity, enabling independent control over timbre and emotion. In the zero-shot setting, the model can accurately reconstruct the target timbre (from the timbre prompt) while perfectly reproducing the specified emotional tone (from the style prompt). To enhance speech clarity in highly emotional expressions, we incorporate GPT latent representations and design a novel three-stage training paradigm to improve the stability of the generated speech. Additionally, to lower the barrier for emotional control, we designed a soft instruction mechanism based on text descriptions by fine-tuning Qwen3, effectively guiding the generation of speech with the desired emotional orientation. Finally, experimental results on multiple datasets show that IndexTTS2 outperforms state-of-the-art zero-shot TTS models in terms of word error rate, speaker similarity, and emotional fidelity. Audio samples are available at: https://index-tts.github.io/index-tts2.github.io/
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.23903</link>
<guid>https://arxiv.org/abs/2506.23903</guid>
<content:encoded><![CDATA[
arXiv:2506.23903v2 Announce Type: replace-cross 
Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 to enable object segmentation across multiple ultrasound organs. A total of 18 public ultrasound datasets, encompassing the breast, thyroid, liver, prostate, kidney, and paraspinal muscle, were utilized. These datasets were divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for testing to evaluate performance in unseen distributions. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on most seen datasets while maintaining strong performance on unseen datasets without additional fine-tuning. These results underscore the promise of VLMs in scalable and robust ultrasound image analysis, reducing dependence on large, organ-specific annotated datasets. We will publish our code on code.sonography.ai after acceptance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling</title>
<link>https://arxiv.org/abs/2507.00790</link>
<guid>https://arxiv.org/abs/2507.00790</guid>
<content:encoded><![CDATA[
arXiv:2507.00790v3 Announce Type: replace-cross 
Abstract: Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data are available at https://github.com/AMAP-ML/LD-RPS.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Data Protection in the (Generative) Artificial Intelligence Era</title>
<link>https://arxiv.org/abs/2507.03034</link>
<guid>https://arxiv.org/abs/2507.03034</guid>
<content:encoded><![CDATA[
arXiv:2507.03034v4 Announce Type: replace-cross 
Abstract: The (generative) artificial intelligence (AI) era has profoundly reshaped the meaning and value of data. No longer confined to static content, data now permeates every stage of the AI lifecycle from the training samples that shape model parameters to the prompts and outputs that drive real-world model deployment. This shift renders traditional notions of data protection insufficient, while the boundaries of what needs safeguarding remain poorly defined. Failing to safeguard data in AI systems can inflict societal and individual, underscoring the urgent need to clearly delineate the scope of and rigorously enforce data protection. In this perspective, we propose a four-level taxonomy, including non-usability, privacy preservation, traceability, and deletability, that captures the diverse protection needs arising in modern (generative) AI models and systems. Our framework offers a structured understanding of the trade-offs between data utility and control, spanning the entire AI pipeline, including training datasets, model weights, system prompts, and AI-generated content. We analyze representative technical approaches at each level and reveal regulatory blind spots that leave critical assets exposed. By offering a structured lens to align future AI technologies and governance with trustworthy data practices, we underscore the urgency of rethinking data protection for modern AI techniques and provide timely guidance for developers, researchers, and regulators alike.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Covering a Few Submodular Constraints and Applications</title>
<link>https://arxiv.org/abs/2507.09879</link>
<guid>https://arxiv.org/abs/2507.09879</guid>
<content:encoded><![CDATA[
arXiv:2507.09879v2 Announce Type: replace-cross 
Abstract: We consider the problem of covering multiple submodular constraints. Given a finite ground set $N$, a cost function $c: N \rightarrow \mathbb{R}_+$, $r$ monotone submodular functions $f_1,f_2,\ldots,f_r$ over $N$ and requirements $b_1,b_2,\ldots,b_r$ the goal is to find a minimum cost subset $S \subseteq N$ such that $f_i(S) \ge b_i$ for $1 \le i \le r$. When $r=1$ this is the well-known Submodular Set Cover problem. Previous work \cite{chekuri2022covering} considered the setting when $r$ is large and developed bi-criteria approximation algorithms, and approximation algorithms for the important special case when each $f_i$ is a weighted coverage function. These are fairly general models and capture several concrete and interesting problems as special cases. The approximation ratios for these problem are at least $\Omega(\log r)$ which is unavoidable when $r$ is part of the input. In this paper, motivated by some recent applications, we consider the problem when $r$ is a \emph{fixed constant} and obtain two main results. For covering multiple submodular constraints we obtain a randomized bi-criteria approximation algorithm that for any given integer $\alpha \ge 1$ outputs a set $S$ such that $f_i(S) \ge$ $(1-1/e^\alpha -\epsilon)b_i$ for each $i \in [r]$ and $\mathbb{E}[c(S)] \le (1+\epsilon)\alpha \cdot \sf{OPT}$. Second, when the $f_i$ are weighted coverage functions from a deletion-closed set system we obtain a $(1+\epsilon)$ $(\frac{e}{e-1})$ $(1+\beta)$-approximation where $\beta$ is the approximation ratio for the underlying set cover instances via the natural LP. These results show that one can obtain nearly as good an approximation for any fixed $r$ as what one would achieve for $r=1$. We mention some applications that follow easily from these general results and anticipate more in the future.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When and Where do Data Poisons Attack Textual Inversion?</title>
<link>https://arxiv.org/abs/2507.10578</link>
<guid>https://arxiv.org/abs/2507.10578</guid>
<content:encoded><![CDATA[
arXiv:2507.10578v4 Announce Type: replace-cross 
Abstract: Poisoning attacks pose significant challenges to the robustness of diffusion models (DMs). In this paper, we systematically analyze when and where poisoning attacks textual inversion (TI), a widely used personalization technique for DMs. We first introduce Semantic Sensitivity Maps, a novel method for visualizing the influence of poisoning on text embeddings. Second, we identify and experimentally verify that DMs exhibit non-uniform learning behavior across timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias and inject adversarial signals predominantly at lower timesteps. Lastly, we observe that adversarial signals distract learning away from relevant concept regions within training data, corrupting the TI process. Based on these insights, we propose Safe-Zone Training (SZT), a novel defense mechanism comprised of 3 key components: (1) JPEG compression to weaken high-frequency poison signals, (2) restriction to high timesteps during TI training to avoid adversarial signals at lower timesteps, and (3) loss masking to constrain learning to relevant regions. Extensive experiments across multiple poisoning methods demonstrate that SZT greatly enhances the robustness of TI against all poisoning attacks, improving generative quality beyond prior published defenses. Code: www.github.com/JStyborski/Diff_Lab Data: www.github.com/JStyborski/NC10
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LanternNet: A Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations</title>
<link>https://arxiv.org/abs/2507.20800</link>
<guid>https://arxiv.org/abs/2507.20800</guid>
<content:encoded><![CDATA[
arXiv:2507.20800v3 Announce Type: replace-cross 
Abstract: The invasive spotted lanternfly (SLF) poses a significant threat to agriculture and ecosystems, causing widespread damage. Current control methods, such as egg scraping, pesticides, and quarantines, prove labor-intensive, environmentally hazardous, and inadequate for long-term SLF suppression. This research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system designed for scalable detection and suppression of SLF populations. A central, tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF identification. Three specialized robotic spokes perform targeted tasks: pest neutralization, environmental monitoring, and navigation/mapping. Field deployment across multiple infested sites over 5 weeks demonstrated LanternNet's efficacy. Quantitative analysis revealed significant reductions (p < 0.01, paired t-tests) in SLF populations and corresponding improvements in tree health indicators across the majority of test sites. Compared to conventional methods, LanternNet offers substantial cost advantages and improved scalability. Furthermore, the system's adaptability for enhanced autonomy and targeting of other invasive species presents significant potential for broader ecological impact. LanternNet demonstrates the transformative potential of integrating robotics and AI for advanced invasive species management and improved environmental outcomes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems</title>
<link>https://arxiv.org/abs/2508.01415</link>
<guid>https://arxiv.org/abs/2508.01415</guid>
<content:encoded><![CDATA[
arXiv:2508.01415v3 Announce Type: replace-cross 
Abstract: We present RoboMemory, a brain-inspired multi-memory framework for lifelong learning in physical embodied systems, addressing critical challenges in real-world environments: continuous learning, multi-module memory latency, task correlation capture, and infinite-loop mitigation in closed-loop planning. Grounded in cognitive neuroscience, it integrates four core modules: the Information Preprocessor (thalamus-like), the Lifelong Embodied Memory System (hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and the Low-Level Executer (cerebellum-like) to enable long-term planning and cumulative learning. The Lifelong Embodied Memory System, central to the framework, alleviates inference speed issues in complex memory frameworks via parallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic submodules. It incorporates a dynamic Knowledge Graph (KG) and consistent architectural design to enhance memory consistency and scalability. Evaluations on EmbodiedBench show RoboMemory outperforms the open-source baseline (Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the closed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing new SOTA. Ablation studies validate key components (critic, spatial memory, long-term memory), while real-world deployment confirms its lifelong learning capability with significantly improved success rates across repeated tasks. RoboMemory alleviates high latency challenges with scalability, serving as a foundational reference for integrating multi-modal memory systems in physical robots.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning</title>
<link>https://arxiv.org/abs/2508.03700</link>
<guid>https://arxiv.org/abs/2508.03700</guid>
<content:encoded><![CDATA[
arXiv:2508.03700v4 Announce Type: replace-cross 
Abstract: This paper presents MagicGUI, a foundational mobile GUI agent designed to address critical challenges in perception, grounding, and reasoning within real-world mobile GUI environments. The framework is underpinned by following six key components: (1) a comprehensive and accurate dataset, constructed via the scalable GUI Data Pipeline, which aggregates the largest and most diverse GUI-centric multimodal data to date from open-source repositories, automated crawling, and targeted manual annotation; (2) enhanced perception and grounding capabilities, facilitating fine-grained multimodal alignment for UI element referencing, grounding, and screen comprehension; (3) a comprehensive and unified action space, encompassing both fundamental UI operations and complex interactive intents to support human-agent interactions; (4) planning-oriented reasoning mechanisms that enable the model to decompose complex user instructions into sequential actions with explicit intermediate meta-paln reasoning; (5) an iterative two-stage training procedure, combining large-scale continue pre-training on 7.8M samples with reinforcement fine-tuning utilizing a spatially enhanced composite reward and dual filtering strategy; and (6) competitive performance on both the proprietary Magic-RICH benchmark and over a dozen public benchmarks, achieving superior performance across GUI perception and agent tasks, while demonstrating robust generalization and real-world deployment potential in practical mobile GUI scenarios, as detailed in Figure 1.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</title>
<link>https://arxiv.org/abs/2508.08005</link>
<guid>https://arxiv.org/abs/2508.08005</guid>
<content:encoded><![CDATA[
arXiv:2508.08005v2 Announce Type: replace-cross 
Abstract: Extensive experiments and prior studies show that no single maximum clique algorithm consistently performs best across all instances, highlighting the importance of selecting suitable algorithms based on instance features. Through an extensive analysis of relevant studies, it is found that there is a lack of research work concerning algorithm selection oriented toward the Maximum Clique Problem (MCP). In this work, we propose a learning-based framework that integrates both traditional machine learning and graph neural networks to address this gap. We construct a labeled dataset by running four exact MCP algorithms on a diverse collection of graph instances, accompanied by structural and global statistical features extracted from each graph. We first evaluate four conventional classifiers: Support Vector Machine (SVM), Random Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple dataset variants. Experimental results show that RF consistently shows strong performance across metrics and dataset variants, making it a reliable baseline. In addition, feature importance analysis indicates that connectivity and topological structure are strong predictors of algorithm performance. Building on these findings, we develop a dual-channel model named GAT-MLP, which combines a Graph Attention Network (GAT) for local structural encoding with a Multilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model shows strong and consistent performance across all metrics. Our results highlight the effectiveness of dual-channel architectures and the promise of graph neural networks in combinatorial algorithm selection.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</title>
<link>https://arxiv.org/abs/2508.08040</link>
<guid>https://arxiv.org/abs/2508.08040</guid>
<content:encoded><![CDATA[
arXiv:2508.08040v2 Announce Type: replace-cross 
Abstract: Prompt-based tuning has emerged as a lightweight alternative to full fine-tuning in large vision-language models, enabling efficient adaptation via learned contextual prompts. This paradigm has recently been extended to federated learning settings (e.g., PromptFL), where clients collaboratively train prompts under data privacy constraints. However, the security implications of prompt-based aggregation in federated multimodal learning remain largely unexplored, leaving a critical attack surface unaddressed. In this paper, we introduce \textbf{BadPromptFL}, the first backdoor attack targeting prompt-based federated learning in multimodal contrastive models. In BadPromptFL, compromised clients jointly optimize local backdoor triggers and prompt embeddings, injecting poisoned prompts into the global aggregation process. These prompts are then propagated to benign clients, enabling universal backdoor activation at inference without modifying model parameters. Leveraging the contextual learning behavior of CLIP-style architectures, BadPromptFL achieves high attack success rates (e.g., \(>90\%\)) with minimal visibility and limited client participation. Extensive experiments across multiple datasets and aggregation protocols validate the effectiveness, stealth, and generalizability of our attack, raising critical concerns about the robustness of prompt-based federated learning in real-world deployments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports</title>
<link>https://arxiv.org/abs/2508.09853</link>
<guid>https://arxiv.org/abs/2508.09853</guid>
<content:encoded><![CDATA[
arXiv:2508.09853v2 Announce Type: replace-cross 
Abstract: Evaluations of dangerous AI capabilities are important for managing catastrophic risks. Public transparency into these evaluations - including what they test, how they are conducted, and how their results inform decisions - is crucial for building trust in AI development. We propose STREAM (A Standard for Transparently Reporting Evaluations in AI Model Reports), a standard to improve how model reports disclose evaluation results, initially focusing on chemical and biological (ChemBio) benchmarks. Developed in consultation with 23 experts across government, civil society, academia, and frontier AI companies, this standard is designed to (1) be a practical resource to help AI developers present evaluation results more clearly, and (2) help third parties identify whether model reports provide sufficient detail to assess the rigor of the ChemBio evaluations. We concretely demonstrate our proposed best practices with "gold standard" examples, and also provide a three-page reporting template to enable AI developers to implement our recommendations more easily.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents</title>
<link>https://arxiv.org/abs/2508.11133</link>
<guid>https://arxiv.org/abs/2508.11133</guid>
<content:encoded><![CDATA[
arXiv:2508.11133v2 Announce Type: replace-cross 
Abstract: Automated agents, powered by Large language models (LLMs), are emerging as the go-to tool for querying information. However, evaluation benchmarks for LLM agents rarely feature natural questions that are both information-seeking and genuinely time-consuming for humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural and time-consuming questions that require dozens, and at times hundreds, of intermediate steps to solve -- far more than any existing QA benchmark. To build MoNaCo, we developed a decomposed annotation pipeline to elicit and manually answer real-world time-consuming questions at scale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and hallucinations. Our results underscore the limitations of LLM-powered agents in handling the complexity and sheer breadth of real-world information-seeking tasks -- with MoNaCo providing an effective resource for tracking such progress. The MoNaCo benchmark, codebase, prompts and models predictions are all publicly available at: https://tomerwolgithub.github.io/monaco
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery</title>
<link>https://arxiv.org/abs/2508.12232</link>
<guid>https://arxiv.org/abs/2508.12232</guid>
<content:encoded><![CDATA[
arXiv:2508.12232v2 Announce Type: replace-cross 
Abstract: Issue-to-commit link recovery plays an important role in software traceability and improves project management. However, it remains a challenging task. A study on GitHub shows that only 42.2% of the issues are correctly linked to their commits. This highlights the potential for further development and research in this area. Existing studies have employed various AI/ML-based approaches, and with the recent development of large language models, researchers have leveraged LLMs to tackle this problem. These approaches suffer from two main issues. First, LLMs are constrained by limited context windows and cannot ingest all of the available data sources, such as long commit histories, extensive issue comments, and large code repositories. Second, most methods operate on individual issue-commit pairs; that is, given a single issue-commit pair, they determine whether the commit resolves the issue. This quickly becomes impractical in real-world repositories containing tens of thousands of commits. To address these limitations, we present LinkAnchor, the first autonomous LLM-based agent designed for issue-to-commit link recovery. The lazy-access architecture of LinkAnchor enables the underlying LLM to access the rich context of software, spanning commits, issue comments, and code files, without exceeding the token limit by dynamically retrieving only the most relevant contextual data. Additionally, LinkAnchor is able to automatically pinpoint the target commit rather than exhaustively scoring every possible candidate. Our evaluations show that LinkAnchor outperforms state-of-the-art issue-to-commit link recovery approaches by 60-262% in Hit@1 score across all our case study projects. We also publicly release LinkAnchor as a ready-to-use tool, along with our replication package. LinkAnchor is designed and tested for GitHub and Jira, and is easily extendable to other platforms.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Murakkab: Resource-Efficient Agentic Workflow Orchestration in Cloud Platforms</title>
<link>https://arxiv.org/abs/2508.18298</link>
<guid>https://arxiv.org/abs/2508.18298</guid>
<content:encoded><![CDATA[
arXiv:2508.18298v2 Announce Type: replace-cross 
Abstract: Agentic workflows commonly coordinate multiple models and tools with complex control logic. They are quickly becoming the dominant paradigm for AI applications. However, serving them remains inefficient with today's frameworks. The key problem is that they expose workflows as opaque sequences of model and tool calls that tightly couple agent logic with model and hardware choices. Often, these workflow components are fragmented across different entities, preventing systems from reasoning about trade-offs across accuracy, latency, energy, and cost. This leads to resource waste and degraded service-level objectives (SLOs).
  We present Murakkab, a resource-efficient serving system for agentic workflows. Murakkab introduces a declarative abstraction that decouples workflow specification from execution configuration. A profile-guided optimizer and adaptive runtime jointly manage the full stack: orchestrating workflow components, mapping them to models and hardware, and dynamically reconfiguring execution to satisfy user-defined SLOs. By exposing the internal structure of agentic workflows, Murakkab enables cross-layer optimization that existing frameworks and cloud schedulers cannot achieve.
  Our evaluation on diverse workflows shows that Murakkab reduces GPU usage by up to 2.8$\times$, energy consumption by 3.7$\times$, and cost by 4.3$\times$ while maintaining SLOs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation</title>
<link>https://arxiv.org/abs/2508.19660</link>
<guid>https://arxiv.org/abs/2508.19660</guid>
<content:encoded><![CDATA[
arXiv:2508.19660v3 Announce Type: replace-cross 
Abstract: Printed electronics offer a promising alternative for applications beyond silicon-based systems, requiring properties like flexibility, stretchability, conformality, and ultra-low fabrication costs. Despite the large feature sizes in printed electronics, printed neural networks have attracted attention for meeting target application requirements, though realizing complex circuits remains challenging. This work bridges the gap between classification accuracy and area efficiency in printed neural networks, covering the entire processing-near-sensor system design and co-optimization from the analog-to-digital interface-a major area and power bottleneck-to the digital classifier. We propose an automated framework for designing printed Ternary Neural Networks with arbitrary input precision, utilizing multi-objective optimization and holistic approximation. Our circuits outperform existing approximate printed neural networks by 17x in area and 59x in power on average, being the first to enable printed-battery-powered operation with under 5% accuracy loss while accounting for analog-to-digital interfacing costs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Information Dynamics of Generative Diffusion</title>
<link>https://arxiv.org/abs/2508.19897</link>
<guid>https://arxiv.org/abs/2508.19897</guid>
<content:encoded><![CDATA[
arXiv:2508.19897v2 Announce Type: replace-cross 
Abstract: Generative diffusion models have emerged as a powerful class of models in machine learning, yet a unified theoretical understanding of their operation is still developing. This perspective paper provides an integrated perspective on generative diffusion by connecting their dynamic, information-theoretic, and thermodynamic properties under a unified mathematical framework. We demonstrate that the rate of conditional entropy production during generation (i.e. the generative bandwidth) is directly governed by the expected divergence of the score function's vector field. This divergence, in turn, is linked to the branching of trajectories and generative bifurcations, which we characterize as symmetry-breaking phase transitions in the energy landscape. This synthesis offers a powerful insight: the process of generation is fundamentally driven by the controlled, noise-induced breaking of (approximate) symmetries, where peaks in information transfer correspond to critical transitions between possible outcomes. The score function acts as a dynamic non-linear filter that regulates the bandwidth of the noise by suppressing fluctuations that are incompatible with the data.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General agents contain world models</title>
<link>https://arxiv.org/abs/2506.01622</link>
<guid>https://arxiv.org/abs/2506.01622</guid>
<content:encoded><![CDATA[
<div> Keywords: world models, flexible behavior, goal-directed tasks, predictive model, agent performance

Summary: 
Agents capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of their environment. This model can be extracted from the agent's policy, and improving performance or achieving more complex goals requires increasingly accurate world models. The presence of world models is crucial for developing safe and general agents, as well as setting bounds on agent capabilities in complex environments. Additionally, new algorithms can be devised for eliciting world models from agents, enhancing our understanding of their behavior and decision-making processes. World models serve as a necessary ingredient for flexible, goal-directed behavior, highlighting their importance in designing efficient and effective autonomous systems. 

Summary: <div>
arXiv:2506.01622v4 Announce Type: replace 
Abstract: Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA Problem Solving by AWorld</title>
<link>https://arxiv.org/abs/2508.09889</link>
<guid>https://arxiv.org/abs/2508.09889</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-agent system, dynamic maneuvering, system identification, GAIA dataset

Summary:<br />
The article proposes a dynamic Multi-Agent System (MAS) in the AWorld framework to enhance the reliability of intelligent agents leveraging external tools. The MAS consists of an Execution Agent supervised by a Guard Agent that provides on-demand dynamic maneuvering to improve robustness. The architecture is further enhanced with a system identification methodology that creates a "performance fingerprint" of the Execution Agent's weaknesses. This fingerprint is used by the Guard Agent for online profile-aware supervision, allowing targeted interventions based on known failure patterns. Experimental results on the GAIA dataset demonstrate that the profile-aware MAS significantly improves effectiveness and stability, surpassing single-agent systems and naive counterparts. The system attained first place on the GAIA leaderboard, showcasing the importance of understanding each agent's capabilities and limitations for building trustworthy intelligent systems.<br /> <div>
arXiv:2508.09889v4 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) has empowered intelligent agents to leverage diverse external tools for solving complex real-world problems. However, this reliance introduces new challenges, as extended contexts and noisy tool outputs can undermine system reliability. To address this, we propose a dynamic Multi-Agent System (MAS) in our AWorld framework, where an Execution Agent is supervised by a Guard Agent that provides on-demand dynamic maneuvering, verifying and correcting the reasoning process to improve robustness over single-agent systems. To move beyond this generic supervision, we enhance the architecture with a methodology inspired by System Identification from control theory. This method first profiles the Execution Agent offline on a benchmark dataset to create a "performance fingerprint" of its unique weaknesses. The Guard Agent then leverages this fingerprint online to deliver profile-aware supervision, making targeted interventions based on known failure patterns rather than merely reacting to immediate logical flaws. Extensive experiments on the GAIA dataset demonstrate that this profile-aware MAS significantly improves both effectiveness and stability, outperforming not only single-agent systems but also its naive counterpart. This superior performance led our system to achieve first place among open-source projects on the prestigious GAIA leaderboard. These findings highlight that building truly trustworthy intelligent systems requires not just collaboration, but a deep, empirically-grounded understanding of each agent's unique capabilities and limitations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Route-and-Execute: Auditable Model-Card Matching and Specialty-Level Deployment</title>
<link>https://arxiv.org/abs/2508.16839</link>
<guid>https://arxiv.org/abs/2508.16839</guid>
<content:encoded><![CDATA[
<div> keywords: clinical workflows, data-driven model identification, healthcare-first framework, vision-language model, single-model deployment

Summary:
Clinical workflows in healthcare are often fragmented and inefficient, involving multiple scripts and task-specific networks. This study introduces a healthcare-first framework using a single vision-language model (VLM) in two roles. Firstly, the VLM acts as a model-card matcher, routing images to specialist models through a three-stage workflow with checks for prompt exit and candidate selection. Secondly, the VLM is fine-tuned on specialty-specific datasets to cover multiple tasks within each specialty, maintaining performance across various medical fields. Compared to pipelines with task-specific agents, this approach demonstrates that a single VLM can effectively make decisions and perform tasks, reducing the effort of data scientists, increasing transparency in model selection, and simplifying deployment. This framework has the potential to streamline clinical workflows, reduce operational costs, and improve overall efficiency in healthcare settings. 

Summary: <div>
arXiv:2508.16839v3 Announce Type: replace 
Abstract: Clinical workflows are fragmented as a patchwork of scripts and task-specific networks that often handle triage, task selection, and model deployment. These pipelines are rarely streamlined for data science pipeline, reducing efficiency and raising operational costs. Workflows also lack data-driven model identification (from imaging/tabular inputs) and standardized delivery of model outputs. In response, we present a practical, healthcare-first framework that uses a single vision-language model (VLM) in two complementary roles. First (Solution 1), the VLM acts as an aware model-card matcher that routes an incoming image to the appropriate specialist model via a three-stage workflow (modality -> primary abnormality -> model-card id). Checks are provided by (i) stagewise prompts that allow early exit via None/Normal/Other and (ii) a stagewise answer selector that arbitrates between the top-2 candidates at each stage, reducing the chance of an incorrect selection and aligning the workflow with clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on specialty-specific datasets ensuring a single model covers multiple downstream tasks within each specialty, maintaining performance while simplifying deployment. Across gastroenterology, hematology, ophthalmology, and pathology, our single-model deployment matches or approaches specialized baselines.
  Compared with pipelines composed of many task-specific agents, this approach shows that one VLM can both decide and do. It may reduce effort by data scientists, shorten monitoring, increase the transparency of model selection (with per-stage justifications), and lower integration overhead.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ST-Raptor: LLM-Powered Semi-Structured Table Question Answering</title>
<link>https://arxiv.org/abs/2508.18190</link>
<guid>https://arxiv.org/abs/2508.18190</guid>
<content:encoded><![CDATA[
<div> Hierarchical Orthogonal Tree, semi-structured table, question answering, large language models, ST-Raptor <br />
Summary: <br />
ST-Raptor is a tree-based framework designed for semi-structured table question answering using large language models. It introduces the Hierarchical Orthogonal Tree (HO-Tree) to capture complex table layouts and define basic tree operations for guiding language models. The framework decomposes user questions into sub-questions, generates operation pipelines, and aligns operations with table structures for accurate execution. A two-stage verification mechanism ensures answer reliability through forward and backward validation. The method outperforms nine baselines by up to 20% in answer accuracy on the SSTQA dataset of 764 questions across 102 real-world semi-structured tables. The code for ST-Raptor is openly available on GitHub for further exploration and implementation. <div>
arXiv:2508.18190v3 Announce Type: replace 
Abstract: Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hermes 4 Technical Report</title>
<link>https://arxiv.org/abs/2508.18255</link>
<guid>https://arxiv.org/abs/2508.18255</guid>
<content:encoded><![CDATA[
<div> Keywords: hybrid reasoning models, instruction-following ability, data curation, training, evaluation

Summary:
Hermes 4 is a family of hybrid reasoning models that integrate structured, multi-turn reasoning with broad instruction-following capability. The article discusses the challenges faced during data curation, synthesis, training, and evaluation of these models and highlights the solutions implemented at scale. The evaluation covers various domains including mathematical reasoning, coding, knowledge, comprehension, and alignment, with both quantitative performance metrics and qualitative behavioral analysis reported. To promote open research, all model weights are openly accessible. The development of Hermes 4 signifies a significant advancement in the field of hybrid reasoning models, showcasing its effectiveness across diverse benchmarks and research areas. <div>
arXiv:2508.18255v2 Announce Type: replace 
Abstract: We present Hermes 4, a family of hybrid reasoning models that combine structured, multi-turn reasoning with broad instruction-following ability. We describe the challenges encountered during data curation, synthesis, training, and evaluation, and outline the solutions employed to address these challenges at scale. We comprehensively evaluate across mathematical reasoning, coding, knowledge, comprehension, and alignment benchmarks, and we report both quantitative performance and qualitative behavioral analysis. To support open research, all model weights are published publicly at https://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft</title>
<link>https://arxiv.org/abs/2508.18722</link>
<guid>https://arxiv.org/abs/2508.18722</guid>
<content:encoded><![CDATA[
<div> knowledge graph, object detection, multimodal environments, domain-specific training data, embodied decision-making

Summary:
VistaWise introduces a cost-effective agent framework that integrates cross-modal domain knowledge and finetunes an object detection model for visual analysis. By reducing the need for domain-specific training data from millions to a few hundred samples, VistaWise enhances agent performance in open-world tasks while decreasing development costs. The framework incorporates visual information and textual dependencies into a knowledge graph (KG) for a comprehensive understanding of multimodal environments. It also includes a retrieval-based pooling strategy to extract task-related information from the KG and a skill library for direct operation of the Minecraft desktop client. Experimental results show that VistaWise achieves state-of-the-art performance in various tasks, demonstrating its efficacy in improving agent performance while lowering development costs.

<br /><br />Summary: <div>
arXiv:2508.18722v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown significant promise in embodied decision-making tasks within virtual open-world environments. Nonetheless, their performance is hindered by the absence of domain-specific knowledge. Methods that finetune on large-scale domain-specific data entail prohibitive development costs. This paper introduces VistaWise, a cost-effective agent framework that integrates cross-modal domain knowledge and finetunes a dedicated object detection model for visual analysis. It reduces the requirement for domain-specific training data from millions of samples to a few hundred. VistaWise integrates visual information and textual dependencies into a cross-modal knowledge graph (KG), enabling a comprehensive and accurate understanding of multimodal environments. We also equip the agent with a retrieval-based pooling strategy to extract task-related information from the KG, and a desktop-level skill library to support direct operation of the Minecraft desktop client via mouse and keyboard inputs. Experimental results demonstrate that VistaWise achieves state-of-the-art performance across various open-world tasks, highlighting its effectiveness in reducing development costs while enhancing agent performance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark</title>
<link>https://arxiv.org/abs/2508.19005</link>
<guid>https://arxiv.org/abs/2508.19005</guid>
<content:encoded><![CDATA[
<div> Experience-driven Lifelong Learning (ELL), self-evolving agents, continuous growth, real-world interaction, AI advancement 

Summary: Experience-driven Lifelong Learning (ELL) is a framework introduced in this paper for creating self-evolving agents capable of continuous growth through real-world interaction. The framework is based on four core principles: Experience Exploration, Long-term Memory, Skill Learning, and Knowledge Internalization. Agents learn through interaction with dynamic environments, preserve historical knowledge in a memory system, improve by abstracting patterns into skills, and internalize explicit experiences into intuitive capabilities. Additionally, the paper introduces StuLife, a benchmark dataset simulating a student's college journey across various scenarios. StuLife is designed to assess the capabilities of agents in ELL through a holistic view of academic and personal development. <br /><br />Summary: <div>
arXiv:2508.19005v2 Announce Type: replace 
Abstract: As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP</title>
<link>https://arxiv.org/abs/2508.19014</link>
<guid>https://arxiv.org/abs/2508.19014</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, intelligent tutoring systems, difficulty estimation, algebra, adaptive assessment

Summary:<br />
- The study introduces APME, a reinforcement learning-based framework for estimating question difficulty in Intelligent & Autonomous Tutoring Systems (IATS).
- APME uses solver performance data, such as marks obtained and time taken, to estimate difficulty without requiring linguistic features or expert labels.
- The model leverages the inverse coefficient of variation as a risk-adjusted metric, providing an explainable and scalable mechanism for adaptive assessment.
- Empirical validation on three datasets showed the model's robustness, accuracy, and adaptability across different educational levels and assessment formats.
- APME consistently outperformed baseline approaches, particularly in symbolic domains like algebra, highlighting the importance of item heterogeneity and variance in solver outcomes for adaptive allocation.
- Pedagogically, the model aligns with Vygotsky's Zone of Proximal Development by balancing challenge and attainability in tasks to support motivation and minimize disengagement. 

<br /><br />Summary: <div>
arXiv:2508.19014v2 Announce Type: replace 
Abstract: The evolution of technology and education is driving the emergence of Intelligent & Autonomous Tutoring Systems (IATS), where objective and domain-agnostic methods for determining question difficulty are essential. Traditional human labeling is subjective, and existing NLP-based approaches fail in symbolic domains like algebra. This study introduces the Approach of Passive Measures among Educands (APME), a reinforcement learning-based Multi-Armed Bandit (MAB) framework that estimates difficulty solely from solver performance data -- marks obtained and time taken -- without requiring linguistic features or expert labels. By leveraging the inverse coefficient of variation as a risk-adjusted metric, the model provides an explainable and scalable mechanism for adaptive assessment. Empirical validation was conducted on three heterogeneous datasets. Across these diverse contexts, the model achieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its robustness, accuracy, and adaptability to different educational levels and assessment formats. Compared with baseline approaches-such as regression-based, NLP-driven, and IRT models-the proposed framework consistently outperformed alternatives, particularly in purely symbolic domains. The findings highlight that (i) item heterogeneity strongly influences perceived difficulty, and (ii) variance in solver outcomes is as critical as mean performance for adaptive allocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal Development by identifying tasks that balance challenge and attainability, supporting motivation while minimizing disengagement. This domain-agnostic, self-supervised approach advances difficulty tagging in IATS and can be extended beyond algebra wherever solver interaction data is available
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Intents to Conversations: Generating Intent-Driven Dialogues with Contrastive Learning for Multi-Turn Classification</title>
<link>https://arxiv.org/abs/2411.14252</link>
<guid>https://arxiv.org/abs/2411.14252</guid>
<content:encoded><![CDATA[
<div> Keywords: conversational AI, intent classification, dialogue generation, self-play, multilingual

Summary:
The paper introduces the Chain-of-Intent framework, combining Hidden Markov Models (HMMs) with Large Language Models (LLMs) to generate intent-driven dialogues in conversational AI systems. By extracting intent transition patterns from e-commerce chat logs, the model generates context-aware dialogues through self-play. The integration of LLMs with HMMs allows for the creation of natural utterances aligned with predicted intents and dialogue context. Additionally, the paper proposes MINT-CL, a multi-task contrastive learning framework for multi-turn intent classification, improving performance and reducing reliance on large annotated datasets. Empirical results demonstrate superior dialogue generation quality and classification accuracy, especially in multilingual settings. The release of MINT-E, a multilingual intent-aware dialogue corpus from the e-commerce domain, aims to facilitate further research in this area. 

<br /><br />Summary: <div>
arXiv:2411.14252v3 Announce Type: replace-cross 
Abstract: In conversational AI systems, a critical challenge in training effective multi-turn intent classification models lies in the generation of large-scale, domain-specific, multilingual dialogue datasets. In this paper, we introduce Chain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs) with Large Language Models (LLMs) to generate intent-driven, context-aware dialogues through self-play. Our method first extracts domain-specific intent transition patterns from real-world e-commerce chat logs, which guide the modeling of turn-level dynamics and intent sequences. LLMs are then employed to parameterize the emission probabilities of HMMs, enabling the generation of natural, coherent utterances aligned with predicted intents and dialogue context. We also propose MINT-CL, a multi-task contrastive learning framework for multi-turn intent classification, which improves performance while reducing dependence on large-scale annotated datasets. Empirical results demonstrate that our approach outperforms competitive baselines in dialogue generation quality and classification accuracy, particularly in multilingual settings. To facilitate future research, we release MINT-E, a comprehensive, multilingual, intent-aware multi-turn dialogue corpus derived from the e-commerce domain\footnote{The reproduced source code and dataset are available at https://github.com/junhua/chain-of-intent.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Disease Detection from Retinal Fundus Images</title>
<link>https://arxiv.org/abs/2504.08481</link>
<guid>https://arxiv.org/abs/2504.08481</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, convolutional neural networks, vision transformers, interpretable hybrid models, retinal disease detection

Summary:
In the field of medical imaging, the integration of convolutional neural networks (CNNs) and vision transformers (ViTs) has shown promise in enhancing the detection of retinal diseases. A novel hybrid CNN-Transformer architecture has been introduced to combine the local feature extraction of CNNs with the global dependency capture of ViTs. Unlike existing hybrid models, this architecture is designed to be interpretable, providing localized evidence maps that directly reflect the decision-making process. By incorporating both CNN and Transformer components, the model achieves state-of-the-art predictive performance in disease detection tasks using color fundus images. The approach outperforms both black-box and interpretable models, offering class-specific sparse evidence maps in a single forward pass. The availability of the code further enhances the reproducibility and accessibility of this innovative method. <div>
arXiv:2504.08481v4 Announce Type: replace-cross 
Abstract: In many medical imaging tasks, convolutional neural networks (CNNs) efficiently extract local features hierarchically. More recently, vision transformers (ViTs) have gained popularity, using self-attention mechanisms to capture global dependencies, but lacking the inherent spatial localization of convolutions. Therefore, hybrid models combining CNNs and ViTs have been developed to combine the strengths of both architectures. However, such hybrid models are difficult to interpret, which hinders their application in medical imaging. In this work, we introduce an interpretable-by-design hybrid fully convolutional CNN-Transformer architecture for retinal disease detection. Unlike widely used post-hoc saliency methods for ViTs, our approach generates faithful and localized evidence maps that directly reflect the mode's decision process. We evaluated our method on two medical tasks focused on disease detection using color fundus images. Our model achieves state-of-the-art predictive performance compared to black-box and interpretable models and provides class-specific sparse evidence maps in a single forward pass. The code is available at: https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demographic-aware fine-grained classification of pediatric wrist fractures</title>
<link>https://arxiv.org/abs/2507.12964</link>
<guid>https://arxiv.org/abs/2507.12964</guid>
<content:encoded><![CDATA[
<div> fine-grained recognition, computer vision, wrist pathology, metadata integration, X-rays<br />
Summary: 
This study focuses on wrist pathologies in children, utilizing a multifaceted approach involving fine-grained recognition, computer vision, and metadata integration. The research addresses the challenge of limited medical imaging datasets by combining patient metadata with X-rays and leveraging fine-grained weights from a separate dataset. This innovative approach, including fine-grained transformer methods and pre-training, resulted in improved diagnostic accuracy. The study demonstrates a 2% increase in accuracy on a small curated dataset and over 10% increase on a larger fracture dataset. Furthermore, this is the first application of metadata integration for wrist pathology recognition, showcasing the potential of combining data types for enhanced diagnostic outcomes.<br /> <div>
arXiv:2507.12964v4 Announce Type: replace-cross 
Abstract: Wrist pathologies are frequently observed, particularly among children who constitute the majority of fracture cases. Computer vision presents a promising avenue, contingent upon the availability of extensive datasets, a notable challenge in medical imaging. Therefore, reliance solely on one modality, such as images, proves inadequate, especially in an era of diverse and plentiful data types. This study addresses the problem using a multifaceted approach: framing it as a fine-grained recognition task, fusing patient metadata with X-rays, and leveraging weights from a separate fine-grained dataset rather than from a coarse-grained dataset like ImageNet. Unlike prior work, this is the first application of metadata integration for wrist pathology recognition. Our results show that combining fine-grained transformer approach, fine-grained pre-training, and metadata integration improves diagnostic accuracy by 2% on small custom curated dataset and over 10% on a larger fracture dataset.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model</title>
<link>https://arxiv.org/abs/2508.14444</link>
<guid>https://arxiv.org/abs/2508.14444</guid>
<content:encoded><![CDATA[
<div> Transformer architecture, Mamba-2, reasoning workloads, inference speed, Nemotron-Nano-9B-v2
<br />
Summary:<br />
The Nemotron-Nano-9B-v2 is a hybrid language model designed to improve throughput for reasoning tasks while maintaining high accuracy. It replaces self-attention layers with Mamba-2 layers to enhance inference speed for generating reasoning traces. The model, based on the Nemotron-H architecture, was pre-trained with a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base). Through compression and distillation using the Minitron strategy, the model can achieve inference on up to 128k tokens on a single NVIDIA A10G GPU. Compared to similar models like Qwen3-8B, Nemotron-Nano-9B-v2 demonstrates comparable or better accuracy on reasoning tasks with up to 6x higher inference throughput in scenarios with 8k input and 16k output tokens. The models and datasets are made available on Hugging Face for further research and development. 
<br /> <div>
arXiv:2508.14444v4 Announce Type: replace-cross 
Abstract: We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model</title>
<link>https://arxiv.org/abs/2508.16700</link>
<guid>https://arxiv.org/abs/2508.16700</guid>
<content:encoded><![CDATA[
<div> Keywords: GPT-OSS-20B, Mixture-of-Experts, throughput, energy efficiency, peak VRAM <br />
Summary: <br />
The study evaluates the performance of GPT-OSS-20B, a Mixture-of-Experts model with 20.9B parameters, on a single GPU against dense baselines Qwen3-32B and Yi-34B. The evaluation includes measuring true time-to-first-token, full-decode throughput, end-to-end latency percentiles, peak VRAM with past key values, and energy consumption. GPT-OSS-20B shows higher decode throughput and energy efficiency, with lower peak VRAM usage compared to the baselines. Despite a higher time-to-first-token due to MoE routing overhead, GPT-OSS-20B demonstrates superior performance with only 17.3% of parameters active. The model offers higher decode throughput and lower energy consumption per 1000 tokens generated compared to Qwen3-32B, while using less peak VRAM. The study highlights the deployment advantages of Mixture-of-Experts models in terms of efficiency and performance. The evaluation does not consider accuracy, focusing instead on deployment aspects. The code and results are made available for replication and further research. <div>
arXiv:2508.16700v2 Announce Type: replace-cross 
Abstract: We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B (Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines Qwen3-32B and Yi-34B across multiple dimensions. We measure true time-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency percentiles, peak VRAM with past key values (PKV) held, and energy via a consistent nvidia-smi-based sampler. At a 2048-token context with 64-token decode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than dense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM and energy per 1000 generated tokens; its TTFT is higher due to MoE routing overhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B provides about 31.8% higher decode throughput and 25.8% lower energy per 1000 generated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM. Normalized by active parameters, GPT-OSS-20B shows markedly stronger per-active-parameter efficiency (APE), underscoring MoE's deployment advantages. We do not evaluate accuracy; this is a deployment-focused study. We release code and consolidated results to enable replication and extension.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tri-Accel: Curvature-Aware Precision-Adaptive and Memory-Elastic Optimization for Efficient GPU Usage</title>
<link>https://arxiv.org/abs/2508.16905</link>
<guid>https://arxiv.org/abs/2508.16905</guid>
<content:encoded><![CDATA[
<div> optimization, deep neural networks, acceleration techniques, mixed precision, batch size scaling

Summary:
Tri-Accel is a unified optimization framework that combines three acceleration strategies for deep neural networks. It dynamically assigns mixed-precision levels to layers, exploits sparsity patterns for second-order signals, and adjusts batch size in real time based on GPU memory availability. On CIFAR-10, Tri-Accel reduces training time by up to 9.9% and memory usage by 13.3%, while improving accuracy by +1.1 percentage points over FP32 baselines. It demonstrates adaptive learning behavior on CIFAR-10/100, gradually improving efficiency as training progresses. Compared to static mixed-precision training, Tri-Accel maintains 78.1% accuracy while reducing memory footprint on standard hardware. The framework, implemented with custom Triton kernels, enables automatic optimization without manual hyperparameter tuning, making it practical for diverse computational environments. This work shows how algorithmic adaptivity and hardware awareness can enhance scalability in resource-constrained settings, leading to more efficient neural network training for edge devices and cost-sensitive cloud deployments. 

<br /><br />Summary: <div>
arXiv:2508.16905v2 Announce Type: replace-cross 
Abstract: Deep neural networks are increasingly bottlenecked by the cost of optimization, both in terms of GPU memory and compute time. Existing acceleration techniques, such as mixed precision, second-order methods, and batch size scaling, are typically used in isolation. We present Tri-Accel, a unified optimization framework that co-adapts three acceleration strategies along with adaptive parameters during training: (1) Precision-Adaptive Updates that dynamically assign mixed-precision levels to layers based on curvature and gradient variance; (2) Sparse Second-Order Signals that exploit Hessian/Fisher sparsity patterns to guide precision and step size decisions; and (3) Memory-Elastic Batch Scaling that adjusts batch size in real time according to VRAM availability. On CIFAR-10 with ResNet-18 and EfficientNet-B0, Tri-Accel achieves up to 9.9% reduction in training time and 13.3% lower memory usage, while improving accuracy by +1.1 percentage points over FP32 baselines. Tested on CIFAR-10/100, our approach demonstrates adaptive learning behavior, with efficiency gradually improving over the course of training as the system learns to allocate resources more effectively. Compared to static mixed-precision training, Tri-Accel maintains 78.1% accuracy while reducing memory footprint from 0.35GB to 0.31GB on standard hardware. The framework is implemented with custom Triton kernels, whose hardware-aware adaptation enables automatic optimization without manual hyperparameter tuning, making it practical for deployment across diverse computational environments. This work demonstrates how algorithmic adaptivity and hardware awareness can be combined to improve scalability in resource-constrained settings, paving the way for more efficient neural network training on edge devices and cost-sensitive cloud deployments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ONG: Orthogonal Natural Gradient Descent</title>
<link>https://arxiv.org/abs/2508.17169</link>
<guid>https://arxiv.org/abs/2508.17169</guid>
<content:encoded><![CDATA[
<div> Keywords: Orthogonal Gradient Descent, Natural Gradient, Continual Learning, EKFAC, Riemannian Metric

Summary:
Orthogonal Gradient Descent (OGD) has proven effective for continual learning but lacks efficiency in leveraging information-geometric structures. To address this, the Orthogonal Natural Gradient Descent (ONG) method is proposed, which incorporates the natural gradient and a Riemannian metric. ONG preconditions task-specific gradients using an EKFAC approximation of the inverse Fisher information matrix. To maintain performance on previous tasks, ONG projects natural gradients orthogonal to prior task gradients. Initial theoretical justification and preliminary results on benchmark datasets are provided. However, challenges in the combination of natural gradients and orthogonal projections highlight the need for further research to reconcile geometric perspectives and enhance method robustness. Future work aims to establish formal convergence guarantees, validate on larger-scale benchmarks, and refine the method. The anonymized code for the study is available for exploration. 

<br /><br />Summary: <div>
arXiv:2508.17169v2 Announce Type: replace-cross 
Abstract: Orthogonal Gradient Descent (OGD) has emerged as a powerful method for continual learning. However, its Euclidean projections do not leverage the underlying information-geometric structure of the problem, which can lead to suboptimal convergence in learning tasks. To address this, we propose incorporating the natural gradient into OGD and present \textbf{ONG (Orthogonal Natural Gradient Descent)}. ONG preconditions each new task-specific gradient with an efficient EKFAC approximation of the inverse Fisher information matrix, yielding updates that follow the steepest descent direction under a Riemannian metric. To preserve performance on previously learned tasks, ONG projects these natural gradients onto the orthogonal complement of prior tasks' gradients. We provide an initial theoretical justification for this procedure, introduce the Orthogonal Natural Gradient Descent (ONG) algorithm, and present preliminary results on the Permuted and Rotated MNIST benchmarks. Our preliminary results, however, indicate that a naive combination of natural gradients and orthogonal projections can have potential issues. This finding motivates continued future work focused on robustly reconciling these geometric perspectives to develop a continual learning method, establishing a more rigorous theoretical foundation with formal convergence guarantees, and extending empirical validation to large-scale continual learning benchmarks. The anonymized version of our code can be found as the zip file here: https://drive.google.com/drive/folders/11PyU6M8pNgOUB5pwdGORtbnMtD8Shiw_?usp=sharing.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components</title>
<link>https://arxiv.org/abs/2508.17182</link>
<guid>https://arxiv.org/abs/2508.17182</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, overconfidence, assertiveness, mechanistic interpretability, dual-route Elaboration Likelihood Model

Summary:
Large Language Models (LLMs) often exhibit overconfidence, presenting information with unwarranted certainty in critical situations. This study delves into the internal mechanisms driving this behavior through mechanistic interpretability. By analyzing Llama 3.2 models fine-tuned on assertiveness datasets, the researchers extract residual activations and identify layers sensitive to assertiveness contrasts. They reveal that high-assertive representations in LLMs consist of emotional and logical sub-components, akin to the dual-route Elaboration Likelihood Model in Psychology. Steering vectors derived from these sub-components demonstrate distinct effects, with emotional vectors broadly influencing prediction accuracy and logical vectors exerting more localized effects. This mechanistic insight sheds light on the multi-component nature of LLM assertiveness and suggests potential strategies for mitigating overconfidence in LLM behavior. 

<br /><br />Summary: <div>
arXiv:2508.17182v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) often display overconfidence, presenting information with unwarranted certainty in high-stakes contexts. We investigate the internal basis of this behavior via mechanistic interpretability. Using open-sourced Llama 3.2 models fine-tuned on human annotated assertiveness datasets, we extract residual activations across all layers, and compute similarity metrics to localize assertive representations. Our analysis identifies layers most sensitive to assertiveness contrasts and reveals that high-assertive representations decompose into two orthogonal sub-components of emotional and logical clusters-paralleling the dual-route Elaboration Likelihood Model in Psychology. Steering vectors derived from these sub-components show distinct causal effects: emotional vectors broadly influence prediction accuracy, while logical vectors exert more localized effects. These findings provide mechanistic evidence for the multi-component structure of LLM assertiveness and highlight avenues for mitigating overconfident behavior.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.17243</link>
<guid>https://arxiv.org/abs/2508.17243</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision-Language Models, token pruning, computational efficiency, multimodal inputs, CoViPAL <br />
Summary: 
The article discusses the challenges faced by Large Vision-Language Models (LVLMs) in processing multimodal inputs, particularly in handling the high computational costs and memory overhead associated with large numbers of vision tokens extracted from images or videos. Existing methods have struggled to effectively prune redundant vision tokens, especially in shallow layers where contextual information is lacking. The proposed CoViPAL approach introduces a Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision tokens before they are processed by the LVLM, improving computational efficiency without compromising accuracy. CoViPAL is lightweight, model-agnostic, and can seamlessly integrate with various LVLM architectures. Extensive experiments demonstrate that CoViPAL outperforms both training-free and training-based pruning methods under equal token budgets, offering a scalable and efficient solution for enhancing inference efficiency in LVLMs. <br /><br /> <div>
arXiv:2508.17243v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) process multimodal inputs consisting of text tokens and vision tokens extracted from images or videos. Due to the rich visual information, a single image can generate thousands of vision tokens, leading to high computational costs during the prefilling stage and significant memory overhead during decoding. Existing methods attempt to prune redundant vision tokens, revealing substantial redundancy in visual representations. However, these methods often struggle in shallow layers due to the lack of sufficient contextual information. We argue that many visual tokens are inherently redundant even in shallow layers and can be safely and effectively pruned with appropriate contextual signals. In this work, we propose CoViPAL, a layer-wise contextualized visual token pruning method that employs a Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision tokens before they are processed by the LVLM. The PPM is lightweight, model-agnostic, and operates independently of the LVLM architecture, ensuring seamless integration with various models. Extensive experiments on multiple benchmarks demonstrate that CoViPAL outperforms training-free pruning methods under equal token budgets and surpasses training-based methods with comparable supervision. CoViPAL offers a scalable and efficient solution to improve inference efficiency in LVLMs without compromising accuracy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI for Software: thoughts from Software Engineering community</title>
<link>https://arxiv.org/abs/2508.17343</link>
<guid>https://arxiv.org/abs/2508.17343</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, software engineering, code generation, intent inference, verification and validation

Summary:
AI agents in software engineering extend beyond code generation, encompassing tasks like testing, program repair, architecture exploration, and requirements understanding. A key challenge lies in deciphering developer intent, essential for tasks like software maintenance. Trust in AI agents is crucial as automation increases, leading to higher volumes of code integration. The future of software workflows may involve AI-based verification and validation for code generated by AI agents. Conceptual progress in intent inference is essential for deploying agentic technology successfully in software engineering. Trustworthy agentic AI software workflows depend on resolving the core challenge of interpreting developer intent, enabling autonomous decision-making by AI agents. AI-based V&amp;V is an emerging direction in handling the proliferation of automatically generated code, indicating a future where AI plays a crucial role in ensuring the reliability and quality of code bases. 

<br /><br />Summary: <div>
arXiv:2508.17343v3 Announce Type: replace-cross 
Abstract: AI agents have recently shown significant promise in software engineering. Much public attention has been transfixed on the topic of code generation from Large Language Models (LLMs) via a prompt. However, software engineering is much more than programming, and AI agents go far beyond instructions given by a prompt.
  At the code level, common software tasks include code generation, testing, and program repair. Design level software tasks may include architecture exploration, requirements understanding, and requirements enforcement at the code level. Each of these software tasks involves micro-decisions which can be taken autonomously by an AI agent, aided by program analysis tools. This creates the vision of an AI software engineer, where the AI agent can be seen as a member of a development team.
  Conceptually, the key to successfully developing trustworthy agentic AI-based software workflows will be to resolve the core difficulty in software engineering - the deciphering and clarification of developer intent. Specification inference, or deciphering the intent, thus lies at the heart of many software tasks, including software maintenance and program repair. A successful deployment of agentic technology into software engineering would involve making conceptual progress in such intent inference via agents.
  Trusting the AI agent becomes a key aspect, as software engineering becomes more automated. Higher automation also leads to higher volume of code being automatically generated, and then integrated into code-bases. Thus to deal with this explosion, an emerging direction is AI-based verification and validation (V & V) of AI generated code. We posit that agentic software workflows in future will include such AIbased V&amp;V.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model</title>
<link>https://arxiv.org/abs/2508.18579</link>
<guid>https://arxiv.org/abs/2508.18579</guid>
<content:encoded><![CDATA[
<div> DrugReasoner, large language model, drug approval prediction, interpretability, reasoning <br />
Summary: Drug discovery is a complex process, and predicting drug approval outcomes early is essential for efficient resource allocation. Traditional machine learning and deep learning models lack interpretability, limiting their impact. DrugReasoner, a reasoning-based large language model, integrates molecular descriptors and comparative reasoning to predict small-molecule approval likelihood. It outperformed baseline models, including logistic regression and support vector machines, achieving competitive performance with XGBoost. DrugReasoner also outperformed the ChemAP model on an external dataset, demonstrating robustness in real-world scenarios. Not only does DrugReasoner deliver accurate predictions, but it also enhances transparency through reasoning outputs, addressing a key bottleneck in AI-assisted drug discovery. <div>
arXiv:2508.18579v2 Announce Type: replace-cross 
Abstract: Drug discovery is a complex and resource-intensive process, making early prediction of approval outcomes critical for optimizing research investments. While classical machine learning and deep learning methods have shown promise in drug approval prediction, their limited interpretability constraints their impact. Here, we present DrugReasoner, a reasoning-based large language model (LLM) built on the LLaMA architecture and fine-tuned with group relative policy optimization (GRPO) to predict the likelihood of small-molecule approval. DrugReasoner integrates molecular descriptors with comparative reasoning against structurally similar approved and unapproved compounds, generating predictions alongside step-by-step rationales and confidence scores. DrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score of 0.729 on the validation set and 0.725 and 0.718 on the test set, respectively. These results outperformed conventional baselines, including logistic regression, support vector machine, and k-nearest neighbors and had competitive performance relative to XGBoost. On an external independent dataset, DrugReasoner outperformed both baseline and the recently developed ChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while maintaining high precision and balanced sensitivity, demonstrating robustness in real-world scenarios. These findings demonstrate that DrugReasoner not only delivers competitive predictive accuracy but also enhances transparency through its reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug discovery. This study highlights the potential of reasoning-augmented LLMs as interpretable and effective tools for pharmaceutical decision-making.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Controllability, Explainability, and Performance in Dysfluency Detection Models</title>
<link>https://arxiv.org/abs/2509.00058</link>
<guid>https://arxiv.org/abs/2509.00058</guid>
<content:encoded><![CDATA[
<div> Keywords: dysfluency detection, modeling paradigms, controllability, explainability, clinical adoption

Summary:
This paper presents a comparative analysis of four dysfluency detection approaches, namely YOLO-Stutter, FluentNet, UDM, and SSDM, focusing on performance, controllability, and explainability. The evaluation on multiple datasets and expert clinician assessment reveals that YOLO-Stutter and FluentNet offer efficiency but have limited transparency, while UDM strikes a balance between accuracy and clinical interpretability. SSDM shows promise but could not be fully reproduced in the experiments. The analysis underscores the trade-offs among the approaches and suggests future directions for dysfluency modeling for clinical use. Implementation insights and practical deployment considerations for each approach are also provided.

<br /><br />Summary: <div>
arXiv:2509.00058v1 Announce Type: new 
Abstract: Recent advances in dysfluency detection have introduced a variety of modeling paradigms, ranging from lightweight object-detection inspired networks (YOLOStutter) to modular interpretable frameworks (UDM). While performance on benchmark datasets continues to improve, clinical adoption requires more than accuracy: models must be controllable and explainable. In this paper, we present a systematic comparative analysis of four representative approaches--YOLO-Stutter, FluentNet, UDM, and SSDM--along three dimensions: performance, controllability, and explainability. Through comprehensive evaluation on multiple datasets and expert clinician assessment, we find that YOLO-Stutter and FluentNet provide efficiency and simplicity, but with limited transparency; UDM achieves the best balance of accuracy and clinical interpretability; and SSDM, while promising, could not be fully reproduced in our experiments. Our analysis highlights the trade-offs among competing approaches and identifies future directions for clinically viable dysfluency modeling. We also provide detailed implementation insights and practical deployment considerations for each approach.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy Against Benchmark Contamination</title>
<link>https://arxiv.org/abs/2509.00072</link>
<guid>https://arxiv.org/abs/2509.00072</guid>
<content:encoded><![CDATA[
<div> Framework, large language models, knowledge cutoff dates, multi-step reasoning, benchmark contamination<br />
<br />
Summary: 
This study evaluates the capabilities of large language models (LLMs) using a framework that synthesizes research-level questions directly from arXiv papers. The study considers models with different knowledge cutoff dates and sizes, finding no significant performance decay near the cutoff dates. The authors compare their results with previous studies that reported post-cutoff performance decay, suggesting that the multi-step reasoning required by their synthesis pipeline may mitigate benchmark contamination. The study advocates for prioritizing reasoning-driven synthesis to construct benchmarks over collecting new questions periodically. Overall, the results indicate that LLMs can handle complex reasoning tasks without succumbing to memorization, highlighting the importance of thoughtful benchmark construction in evaluating model capabilities. <br /><br />Summary: <div>
arXiv:2509.00072v1 Announce Type: new 
Abstract: Capability evaluation of large language models (LLMs) is increasingly shadowed by rising concerns of data contamination that cast doubts on whether static benchmarks measure genuine reasoning or mere memorization. We present an empirical study using an infinitely scalable framework to synthesize research-level QA directly from arXiv papers, harnessing the natural temporal structure of research publications where performance decay after knowledge cutoffs may indicate potential contamination. We evaluated 4 frontier model represented by 2 models of different knowledge cutoff dates per family on 1,643 multi-step reasoning questions synthesized from 20,277 arXiv papers stratified over 26 months, covering at least 6 months before and after all cutoff dates. Our results consistently showed a lack of significant performance decay near knowledge cutoff dates for models of various sizes, developers, and release dates. We further performed a comparative analysis with previous longitudinal studies that reported significant post-cutoff performance decay using directly retrieved questions based on public data. we hypothesize that the multi-step reasoning required by our synthesis pipeline offered additional complexity that goes deeper than shallow memorization, which effectively serves a mitigation strategy against benchmark contamination. We fully open source our code and dataset to aid reproducibility and advocate for a paradigm shift that prioritize reasoning-driven synthesis to construct benchmarks over simply collecting newly released questions periodically.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language and Experience: A Computational Model of Social Learning in Complex Tasks</title>
<link>https://arxiv.org/abs/2509.00074</link>
<guid>https://arxiv.org/abs/2509.00074</guid>
<content:encoded><![CDATA[
<div> framework, social learning, structured world models, AI systems, linguistic guidance
Summary:
The article presents a computational framework for modeling social learning through joint probabilistic inference over structured world models using sensorimotor and linguistic data. By leveraging a pretrained language model as a probabilistic model of how humans share advice, agents can generate and interpret advice for learning and exploration. Linguistic guidance is shown to aid in shaping exploration, accelerating learning, reducing risky interactions, and speeding up discoveries in both humans and AI models. The study also examines how knowledge can accumulate through iterated learning experiments and successful knowledge transfer between humans and AI models. The research highlights the potential of structured, language-compatible representations in facilitating human-machine collaborative learning. <div>
arXiv:2509.00074v1 Announce Type: new 
Abstract: The ability to combine linguistic guidance from others with direct experience is central to human development, enabling safe and rapid learning in new environments. How do people integrate these two sources of knowledge, and how might AI systems? We present a computational framework that models social learning as joint probabilistic inference over structured, executable world models given sensorimotor and linguistic data. We make this possible by turning a pretrained language model into a probabilistic model of how humans share advice conditioned on their beliefs, allowing our agents both to generate advice for others and to interpret linguistic input as evidence during Bayesian inference. Using behavioral experiments and simulations across 10 video games, we show how linguistic guidance can shape exploration and accelerate learning by reducing risky interactions and speeding up key discoveries in both humans and models. We further explore how knowledge can accumulate across generations through iterated learning experiments and demonstrate successful knowledge transfer between humans and models -- revealing how structured, language-compatible representations might enable human-machine collaborative learning.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation</title>
<link>https://arxiv.org/abs/2509.00079</link>
<guid>https://arxiv.org/abs/2509.00079</guid>
<content:encoded><![CDATA[
<div> uncertainty, refinement, reasoning, lightweight, cost-efficiency 
Summary: 
In the study presented, a method called entropy-guided refinement is proposed to improve the performance of reasoning models while reducing cost and latency. This lightweight test-time loop utilizes token-level uncertainty to trigger a targeted refinement pass, achieving 95% accuracy compared to a reference model at one-third of the cost. The approach involves extracting log probabilities, calculating Shannon entropy, and using a simple trigger based on perplexity, token entropy, and low-confidence tokens to guide corrective edits. By passing an uncertainty report back to the model, selective refinement is applied to approximately 31% of responses, resulting in a 16-percentage point accuracy improvement over single-pass inference. This method offers a practical middle ground between single-pass inference and resource-intensive reasoning chains, making it suitable for production deployments where balancing quality and cost is essential. 
<br /><br />Summary: <div>
arXiv:2509.00079v1 Announce Type: new 
Abstract: Reasoning models often outperform smaller models but at 3--5$\times$ higher cost and added latency. We present entropy-guided refinement: a lightweight, test-time loop that uses token-level uncertainty to trigger a single, targeted refinement pass. We extract logprobs, compute Shannon entropy on top-$k$ alternatives, and apply a simple OR-logic trigger over perplexity, maximum token entropy, and low-confidence-token count. Unlike approaches that use entropy only for measurement or decoding, we pass a compact uncertainty report (tokens, confidences, alternatives, context) back to the model to guide corrective edits. On representative technical queries across reasoning, mathematics, and code generation tasks, a small model with our loop approaches 95\% of a reference reasoning model's quality at approximately one-third of the cost. The method achieves selective refinement on ~31\% of responses while improving accuracy by 16 percentage points over single-pass inference. We demonstrate that this uncertainty-aware loop provides an effective middle ground between single-pass inference and expensive reasoning chains, making it practical for production deployments where both quality and cost matter.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wrong Face, Wrong Move: The Social Dynamics of Emotion Misperception in Agent-Based Models</title>
<link>https://arxiv.org/abs/2509.00080</link>
<guid>https://arxiv.org/abs/2509.00080</guid>
<content:encoded><![CDATA[
<div> classifier, emotion, social behavior, perception, accuracy
Summary:
- The study explores the impact of varying accuracy in emotion classifiers on social behavior.
- Agents with low-accuracy classifiers exhibit diminished trust, sadness, and disordered social organization.
- Agents with high-accuracy classifiers develop emotional resilience and cohesion.
- Even in emotionally neutral scenarios, misperception leads to segregation and disintegration of cohesion.
- Biases or imprecision in emotion recognition can significantly alter social processes and disrupt emotional integration.<br /><br />Summary: <div>
arXiv:2509.00080v1 Announce Type: new 
Abstract: The ability of humans to detect and respond to others' emotions is fundamental to understanding social behavior. Here, agents are instantiated with emotion classifiers of varying accuracy to study the impact of perceptual accuracy on emergent emotional and spatial behavior. Agents are visually represented with face photos from the KDEF database and endowed with one of three classifiers trained on the JAFFE (poor), CK+ (medium), or KDEF (high) datasets. Agents communicate locally on a 2D toroidal lattice, perceiving neighbors' emotional state based on their classifier and responding with movement toward perceived positive emotions and away from perceived negative emotions. Note that the agents respond to perceived, instead of ground-truth, emotions, introducing systematic misperception and frustration. A battery of experiments is carried out on homogeneous and heterogeneous populations and scenarios with repeated emotional shocks. Results show that low-accuracy classifiers on the part of the agent reliably result in diminished trust, emotional disintegration into sadness, and disordered social organization. By contrast, the agent that develops high accuracy develops hardy emotional clusters and resilience to emotional disruptions. Even in emotionally neutral scenarios, misperception is enough to generate segregation and disintegration of cohesion. These findings underscore the fact that biases or imprecision in emotion recognition may significantly warp social processes and disrupt emotional integration.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Debates with Local Large Language Models for AI Alignment</title>
<link>https://arxiv.org/abs/2509.00091</link>
<guid>https://arxiv.org/abs/2509.00091</guid>
<content:encoded><![CDATA[
<div> ensemble, debates, language models, alignment, reproducibility <br />
Summary:<br />
In the study, the researchers examined the use of local open-source ensemble debates to enhance alignment-oriented reasoning in large language models. They conducted 150 debates across 15 scenarios and five ensemble configurations, finding that ensembles outperformed single-model baselines on various criteria. The ensembles showed significant improvements in reasoning depth and argument quality, with the largest gains in truthfulness and human enhancement. The findings suggest that ensemble-based approaches can enhance alignment with human values in high-stakes decision-making scenarios. The researchers provided code, prompts, and a debate dataset for reproducibility, offering a foundation for further evaluation of alignment in language models. <br />Summary: <div>
arXiv:2509.00091v1 Announce Type: new 
Abstract: As large language models (LLMs) take on greater roles in high-stakes decisions, alignment with human values is essential. Reliance on proprietary APIs limits reproducibility and broad participation. We study whether local open-source ensemble debates can improve alignmentoriented reasoning. Across 150 debates spanning 15 scenarios and five ensemble configurations, ensembles outperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13), with the largest gains in reasoning depth (+19.4%) and argument quality (+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human enhancement (+0.80). We provide code, prompts, and a debate data set, providing an accessible and reproducible foundation for ensemble-based alignment evaluation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MODE: Mixture of Document Experts for RAG</title>
<link>https://arxiv.org/abs/2509.00100</link>
<guid>https://arxiv.org/abs/2509.00100</guid>
<content:encoded><![CDATA[
<div> MODE, Retrieval-Augmented Generation, lightweight, cluster-and-route retrieval, semantically coherent clusters <br />
<br />Summary: MODE is a lightweight alternative to Retrieval-Augmented Generation (RAG) for small, domain-specific collections. It replaces fine-grained nearest-neighbor search with cluster-and-route retrieval, using cached centroids to eliminate the need for large vector databases. By grouping documents into semantically coherent clusters and routing queries to relevant clusters, MODE matches or exceeds dense-retrieval baselines in answer quality while reducing retrieval time. The cluster granularity and multi-cluster routing in MODE allow for controlling the recall/precision trade-off, with tighter clusters leading to improved downstream accuracy. This approach offers a practical solution for small to medium corpora where simplicity, speed, and topical focus are important considerations. <br /> <div>
arXiv:2509.00100v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) often relies on large vector databases and cross-encoders tuned for large-scale corpora, which can be excessive for small, domain-specific collections. We present MODE (Mixture of Document Experts), a lightweight alternative that replaces fine-grained nearest-neighbor search with cluster-and-route retrieval. Documents are embedded, grouped into semantically coherent clusters, and represented by cached centroids. At query time, we route to the top centroid(s) and retrieve context only within those clusters, eliminating external vector-database infrastructure and reranking while keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks, MODE matches or exceeds a dense-retrieval baseline in answer quality while reducing end-to-end retrieval time. Ablations show that cluster granularity and multi-cluster routing control the recall/precision trade-off, and that tighter clusters improve downstream accuracy. MODE offers a practical recipe for small and medium corpora where simplicity, speed, and topical focus matter.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems</title>
<link>https://arxiv.org/abs/2509.00115</link>
<guid>https://arxiv.org/abs/2509.00115</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic artificial intelligence, Adaptive Multi-Dimensional Monitoring, goal drift, anomaly detection, metrics 

Summary: 
The paper discusses the transition of agentic artificial intelligence (AI) from research to real-world applications and the importance of considering human-centered and economic metrics. It introduces the Adaptive Multi-Dimensional Monitoring (AMDM) algorithm for normalizing metrics, applying thresholds, and performing anomaly detection. Through simulations and experiments, AMDM significantly reduces anomaly-detection latency and false-positive rates compared to static thresholds. The paper includes a comparison table, ROC/PR curves, and highlights the need for additional metrics in case studies. Code, data, and a reproducibility checklist are provided to facilitate replication.<br /><br />Summary: <div>
arXiv:2509.00115v1 Announce Type: new 
Abstract: Agentic artificial intelligence (AI) -- multi-agent systems that combine large language models with external tools and autonomous planning -- are rapidly transitioning from research laboratories into high-stakes domains. Our earlier "Basic" paper introduced a five-axis framework and proposed preliminary metrics such as goal drift and harm reduction but did not provide an algorithmic instantiation or empirical evidence. This "Advanced" sequel fills that gap. First, we revisit recent benchmarks and industrial deployments to show that technical metrics still dominate evaluations: a systematic review of 84 papers from 2023--2025 found that 83% report capability metrics while only 30% consider human-centred or economic axes [2]. Second, we formalise an Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises heterogeneous metrics, applies per-axis exponentially weighted moving-average thresholds and performs joint anomaly detection via the Mahalanobis distance. Third, we conduct simulations and real-world experiments. AMDM cuts anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and reduces false-positive rates from 4.5% to 0.9% compared with static thresholds. We present a comparison table and ROC/PR curves, and we reanalyse case studies to surface missing metrics. Code, data and a reproducibility checklist accompany this paper to facilitate replication.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.00125</link>
<guid>https://arxiv.org/abs/2509.00125</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, verifiable feedback, large language models, difficulty aware certainty, exploration

Summary:<br />
Reinforcement Learning with Verifiable Feedback (RLVF) enhances Large Language Models (LLMs) reasoning by providing sparse rewards based on final answer correctness. However, it lacks granular guidance on the reasoning process itself. Difficulty Aware Certainty guided Exploration (DACE) addresses this by using LLMs' self-certainty to assess task difficulty and modulate exploration. By penalizing high certainty in difficult tasks and rewarding it in easier tasks, DACE balances exploration-exploitation trade-off effectively. Experimental results on mathematical reasoning benchmarks show DACE outperforms baselines, achieving higher accuracy and robust performance with scaled test-time compute. This adaptive approach fosters effective exploration without sacrificing precision.<br /> <div>
arXiv:2509.00125v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Feedback (RLVF) has become a key technique for enhancing the reasoning abilities of Large Language Models (LLMs). However, its reliance on sparse, outcome based rewards, which only indicate if a final answer is correct or not, fails to provide granular guidance on the reasoning process itself. This limitation hinders efficient learning, as the model cannot distinguish between high quality and inefficient solutions, nor can it learn effectively from different types of failures. To address this, we observe that an LLMs self-certainty often correlates with task difficulty and solution quality. We introduce Difficulty Aware Certainty guided Exploration (DACE), a novel RL algorithm that leverages this insight to dynamically balance the exploration exploitation trade-off. DACE assesses task difficulty online based on the policys success rate. It then uses this signal to modulate an intrinsic reward: for difficult tasks where the model is struggling, DACE encourages exploration by penalizing high certainty; for easier tasks, it encourages learning efficiency by rewarding high certainty. Experiments on challenging mathematical reasoning benchmarks (AIME, MATH) show that DACE significantly outperforms strong baselines. The DACE-trained models not only achieve higher accuracy but also demonstrate more robust performance when scaling test-time compute, validating that our adaptive approach fosters effective exploration without sacrificing precision.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Health Coverage in Ethiopia: A Learning-augmented Approach and Persistent Proportionality Under an Online Budget</title>
<link>https://arxiv.org/abs/2509.00135</link>
<guid>https://arxiv.org/abs/2509.00135</guid>
<content:encoded><![CDATA[
<div> Health Access Resource Planner, Ethiopia, Universal Health Coverage, optimization framework, health system strengthening 

Summary: 
The Ministry of Health in Ethiopia is working towards expanding access to essential healthcare services by strengthening health posts. Due to limited budgets and competing priorities, only a fraction of this effort can be implemented each year. To address this, a tool called Health Access Resource Planner (HARP) has been developed to guide prioritization across Ethiopia's regions. HARP is based on a decision-support optimization framework that aims to maximize population coverage under budget uncertainty while meeting region-specific proportionality targets at each step. Two algorithms have been proposed to aid in facility planning: a learning-augmented approach to improve expert recommendations and a greedy algorithm for multi-step planning, both with strong worst-case approximation estimation. Through collaboration with Ethiopian Public Health Institute and the Ministry of Health, the efficacy of the method has been demonstrated in three regions across various planning scenarios. <div>
arXiv:2509.00135v1 Announce Type: new 
Abstract: As part of nationwide efforts aligned with the United Nations' Sustainable Development Goal 3 on Universal Health Coverage, Ethiopia's Ministry of Health is strengthening health posts to expand access to essential healthcare services. However, only a fraction of this health system strengthening effort can be implemented each year due to limited budgets and other competing priorities, thus the need for an optimization framework to guide prioritization across the regions of Ethiopia. In this paper, we develop a tool, Health Access Resource Planner (HARP), based on a principled decision-support optimization framework for sequential facility planning that aims to maximize population coverage under budget uncertainty while satisfying region-specific proportionality targets at every time step. We then propose two algorithms: (i) a learning-augmented approach that improves upon expert recommendations at any single-step; and (ii) a greedy algorithm for multi-step planning, both with strong worst-case approximation estimation. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we demonstrated the empirical efficacy of our method on three regions across various planning scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Group Knowledge and Group Belief in Topological Evidence Models (Extended Version)</title>
<link>https://arxiv.org/abs/2509.00184</link>
<guid>https://arxiv.org/abs/2509.00184</guid>
<content:encoded><![CDATA[
<div> Keywords: group knowledge, group belief, evidence-based belief, multi-agent, logic

Summary:
The study focuses on expanding the topological semantics of evidence-based belief and knowledge to encompass groups within multi-agent models. The logic of group evidence, knowledge, and belief is completely axiomatized and shown to be decidable. A specific fragment, the logic of group knowledge and belief, is also thoroughly explored. The extension of languages with dynamic evidence-sharing operators is fully axiomatized, demonstrating co-expressiveness with their static bases. This research provides a comprehensive understanding of (virtual) group knowledge and belief systems within multi-agent evidence-based models, showcasing the importance of collaborative decision-making processes and shared information among groups. <div>
arXiv:2509.00184v1 Announce Type: new 
Abstract: We study notions of (virtual) group knowledge and group belief within multi-agent evidence models, obtained by extending the topological semantics of evidence-based belief and fallible knowledge from individuals to groups. We completely axiomatize and show the decidability of the logic of ("hard" and "soft") group evidence, and do the same for an especially interesting fragment of it: the logic of group knowledge and group belief. We also extend these languages with dynamic evidence-sharing operators, and completely axiomatize the corresponding logics, showing that they are co-expressive with their static bases.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiVA: Self-organized Hierarchical Variable Agent via Goal-driven Semantic-Topological Evolution</title>
<link>https://arxiv.org/abs/2509.00189</link>
<guid>https://arxiv.org/abs/2509.00189</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous agents, Artificial General Intelligence, Hierarchical Variable Agent, Semantic-Topological Evolution, Task execution.

Summary:
Autonomous agents are essential for advancing Artificial General Intelligence by leveraging Large Language Models (LLMs) for problem-solving. However, current paradigms face a trade-off between fixed workflows and flexible reactive loops. To address this, the Hierarchical Variable Agent (HiVA) framework models workflows as self-organized graphs with the Semantic-Topological Evolution algorithm. This optimization process utilizes textual gradients as surrogates for backpropagation, enhancing task accuracy and resource efficiency in various benchmarks. HiVA's iterative approach includes Multi-Armed Bandit-based routing, diagnostic gradient generation, and coordinated updates for collective optimization in unknown environments. Experimental results demonstrate a 5-10% improvement in task accuracy, showcasing HiVA's effectiveness in autonomous task execution.

<br /><br />Summary: 
1. Autonomous agents are vital for advancing Artificial General Intelligence.
2. HiVA framework models workflows as self-organized graphs with the Semantic-Topological Evolution algorithm.
3. Textual gradients are used for backpropagation, enhancing task accuracy and resource efficiency.
4. HiVA's iterative approach includes Multi-Armed Bandit-based routing and diagnostic gradient generation.
5. Experimental results show a 5-10% improvement in task accuracy, validating HiVA's effectiveness in autonomous task execution. <div>
arXiv:2509.00189v1 Announce Type: new 
Abstract: Autonomous agents play a crucial role in advancing Artificial General Intelligence, enabling problem decomposition and tool orchestration through Large Language Models (LLMs). However, existing paradigms face a critical trade-off. On one hand, reusable fixed workflows require manual reconfiguration upon environmental changes; on the other hand, flexible reactive loops fail to distill reasoning progress into transferable structures. We introduce Hierarchical Variable Agent (HiVA), a novel framework modeling agentic workflows as self-organized graphs with the Semantic-Topological Evolution (STEV) algorithm, which optimizes hybrid semantic-topological spaces using textual gradients as discrete-domain surrogates for backpropagation. The iterative process comprises Multi-Armed Bandit-infused forward routing, diagnostic gradient generation from environmental feedback, and coordinated updates that co-evolve individual semantics and topology for collective optimization in unknown environments. Experiments on dialogue, coding, Long-context Q&amp;A, mathematical, and agentic benchmarks demonstrate improvements of 5-10% in task accuracy and enhanced resource efficiency over existing baselines, establishing HiVA's effectiveness in autonomous task execution.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Deep Research: Bring Your Own Model and Strategy</title>
<link>https://arxiv.org/abs/2509.00244</link>
<guid>https://arxiv.org/abs/2509.00244</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep research tools, Universal Deep Research (UDR), agentic system, custom research strategies, user interface

Summary: 
Universal Deep Research (UDR) is a novel agentic system designed to empower users to create, customize, and refine their own research strategies without the need for additional training. Unlike traditional deep research agents, UDR provides a flexible framework that can adapt to various research tasks and goals. By wrapping around any language model, UDR allows users to craft unique research approaches, such as minimal, expansive, and intensive strategies. The system is equipped with a user-friendly interface that simplifies experimentation and exploration of different research methodologies. Through UDR, users can efficiently utilize deep research tools to enhance their work and address specific research challenges, making it a valuable asset for researchers seeking a more tailored and adaptable approach to conducting advanced research. <div>
arXiv:2509.00244v1 Announce Type: new 
Abstract: Deep research tools are among the most impactful and most commonly encountered agentic systems today. We observe, however, that each deep research agent introduced so far is hard-coded to carry out a particular research strategy using a fixed choice of tools. We introduce Universal Deep Research (UDR), a generalist agentic system that wraps around any language model and enables the user to create, edit, and refine their own entirely custom deep research strategies without any need for additional training or finetuning. To showcase the generality of our system, we equip UDR with example minimal, expansive, and intensive research strategies, and provide a user interface to facilitate experimentation with the system.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction-Level Weight Shaping: A Framework for Self-Improving AI Agents</title>
<link>https://arxiv.org/abs/2509.00251</link>
<guid>https://arxiv.org/abs/2509.00251</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Instruction-Level Weight Shaping, Reflection Engine, Enterprise support, Autonomous updates

Summary:
Instruction-Level Weight Shaping (ILWS) is introduced as a method to improve large language models (LLMs) by using curated system instructions to update pseudo-parameters based on user feedback and reflection. The Reflection Engine diagnoses reasoning successes and failures, proposing changes to instructions, user preferences, and tools. These changes are evaluated and controlled through version control mechanisms. ILWS aims to enhance throughput and reduce errors in enterprise support contexts. It has shown significant improvements in ticket resolution efficiency and accuracy. ILWS enables autonomous updates and tool synthesis, making it adaptable to various dynamic domains like legal, medical, and engineering. By operating at the instruction layer until controlled distillation, ILWS offers a practical solution for integrating new knowledge and improving reasoning capabilities in LLMs. <br /><br />Summary: <div>
arXiv:2509.00251v1 Announce Type: new 
Abstract: Large language models (LLMs) are fluent but largely static after pre-training; new or shifting knowledge is typically added with retrieval-augmented generation (RAG) or fine-tuning. RAG raises latency and engineering overhead and often fails to integrate facts; prompt engineering is brittle and can conflict with prior knowledge; fine-tuning is costly and risks catastrophic forgetting. We propose Instruction-Level Weight Shaping (ILWS): curated system instructions act as external, auditable pseudo-parameters updated after each session via reflection and user feedback. A Reflection Engine inspects conversation traces, diagnoses reasoning successes and failures, and proposes typed deltas $\Delta K=(\Delta S,\Delta U,\Delta T)$ over instructions, user preferences, and tools. Deltas are version-controlled, evaluated with a sliding window of 1-5 star ratings, auto-repaired on first failure, and rolled back on repeated failure. When an edit budget crosses a threshold, the agent compiles a rating-weighted synthetic set and distills matured instruction-space gains into parameters, converting prompt-space improvements into weight-space without downtime. ILWS makes explicit the low-rank shaping induced by context in transformer blocks, preserves governance, and removes per-call retrieval. In enterprise support it increased throughput 2.4-5.0x and cut audited hallucinations by about 80% versus a frozen baseline. In an Adobe Commerce Cloud proof of concept "L0 Support", it achieved 4-5x more tickets per hour and about 80% lower time per ticket, with autonomous instruction updates and optional tool synthesis. Because ILWS operates at the instruction layer until controlled distillation, it generalizes to dynamic domains (legal, medical, engineering) requiring adaptive reasoning, tool creation, and low-latency deployment.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHERPA: A Model-Driven Framework for Large Language Model Execution</title>
<link>https://arxiv.org/abs/2509.00272</link>
<guid>https://arxiv.org/abs/2509.00272</guid>
<content:encoded><![CDATA[
<div> Framework, Large Language Models, State Machines, Control, Performance
Summary:
SHERPA is a model-driven framework designed to enhance the performance of Large Language Models (LLMs) in dealing with complex tasks by integrating domain-specific best practices into hierarchical state machines. By utilizing state machines, SHERPA enables more precise control over LLM behavior through machine learning-driven decisions and rules. The framework proves effective in tasks such as code and class name generation, as well as question answering, surpassing previous approaches in performance. The study showcases the benefits of incorporating well-designed state machines, particularly in tasks that rely on established human best practices but lack sufficient training data for LLMs. Experimental results demonstrate that integrating state machines significantly enhances the quality of LLM outputs, indicating the potential of SHERPA in improving LLM capabilities for challenging tasks. 
Summary: <div>
arXiv:2509.00272v1 Announce Type: new 
Abstract: Recently, large language models (LLMs) have achieved widespread application across various fields. Despite their impressive capabilities, LLMs suffer from a lack of structured reasoning ability, particularly for complex tasks requiring domain-specific best practices, which are often unavailable in the training data. Although multi-step prompting methods incorporating human best practices, such as chain-of-thought and tree-of-thought, have gained popularity, they lack a general mechanism to control LLM behavior. In this paper, we propose SHERPA, a model-driven framework to improve the LLM performance on complex tasks by explicitly incorporating domain-specific best practices into hierarchical state machines. By structuring the LLM execution processes using state machines, SHERPA enables more fine-grained control over their behavior via rules or decisions driven by machine learning-based approaches, including LLMs. We show that SHERPA is applicable to a wide variety of tasks-specifically, code generation, class name generation, and question answering-replicating previously proposed approaches while further improving the performance. We demonstrate the effectiveness of SHERPA for the aforementioned tasks using various LLMs. Our systematic evaluation compares different state machine configurations against baseline approaches without state machines. Results show that integrating well-designed state machines significantly improves the quality of LLM outputs, and is particularly beneficial for complex tasks with well-established human best practices but lacking data used for training LLMs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces</title>
<link>https://arxiv.org/abs/2509.00287</link>
<guid>https://arxiv.org/abs/2509.00287</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal data, urban spaces, knowledge graphs, incidents, Large Language Models (LLMs)

Summary:
SIGMUS is a system designed for Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces. It leverages Large Language Models (LLMs) to identify relationships between incidents happening in urban areas and data from various sources. This allows for the organization of evidence and observations relevant to incidents without the need for human-encoded rules. The system creates a knowledge graph to represent organized knowledge about incidents, observations, and more in urban spaces. By analyzing data from sources such as news articles, CCTV images, air quality, weather, and traffic measurements, SIGMUS can establish connections between different data sources and incidents occurring simultaneously at the same location. The system shows promise in bridging the gap between fragmented multimodal data sources and effectively integrating them to understand and predict events in urban landscapes. 

<br /><br />Summary: <div>
arXiv:2509.00287v1 Announce Type: new 
Abstract: Modern urban spaces are equipped with an increasingly diverse set of sensors, all producing an abundance of multimodal data. Such multimodal data can be used to identify and reason about important incidents occurring in urban landscapes, such as major emergencies, cultural and social events, as well as natural disasters. However, such data may be fragmented over several sources and difficult to integrate due to the reliance on human-driven reasoning for identifying relationships between the multimodal data corresponding to an incident, as well as understanding the different components which define an incident. Such relationships and components are critical to identifying the causes of such incidents, as well as producing forecasting the scale and intensity of future incidents as they begin to develop. In this work, we create SIGMUS, a system for Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces. SIGMUS uses Large Language Models (LLMs) to produce the necessary world knowledge for identifying relationships between incidents occurring in urban spaces and data from different modalities, allowing us to organize evidence and observations relevant to an incident without relying and human-encoded rules for relating multimodal sensory data with incidents. This organized knowledge is represented as a knowledge graph, organizing incidents, observations, and much more. We find that our system is able to produce reasonable connections between 5 different data sources (new article text, CCTV images, air quality, weather, and traffic measurements) and relevant incidents occurring at the same time and location.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEWSAGENT: Benchmarking Multimodal Agents as Journalists with Real-World Newswriting Tasks</title>
<link>https://arxiv.org/abs/2509.00446</link>
<guid>https://arxiv.org/abs/2509.00446</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous agents, journalism, multimodal web data, news writing, evaluation<br />
Summary:<br />
Recent advances in autonomous digital agents have shown potential in structured tasks like decision-making and task decomposition in industries like journalism. The NEWSAGENT benchmark evaluates how agents can search, select, and edit multimodal web data to create news articles, reflecting real-world challenges in news writing. Agents must identify narrative perspectives, issue queries, retrieve background information, and integrate facts to generate complete articles. While agents can retrieve relevant information, they struggle with planning and narrative cohesion. NEWSAGENT provides a realistic testbed for evaluating agent performance in manipulating multimodal web data for real-world productivity. <div>
arXiv:2509.00446v1 Announce Type: new 
Abstract: Recent advances in autonomous digital agents from industry (e.g., Manus AI and Gemini's research mode) highlight potential for structured tasks by autonomous decision-making and task decomposition; however, it remains unclear to what extent the agent-based systems can improve multimodal web data productivity. We study this in the realm of journalism, which requires iterative planning, interpretation, and contextual reasoning from multimodal raw contents to form a well structured news. We introduce NEWSAGENT, a benchmark for evaluating how agents can automatically search available raw contents, select desired information, and edit and rephrase to form a news article by accessing core journalistic functions. Given a writing instruction and firsthand data as how a journalist initiates a news draft, agents are tasked to identify narrative perspectives, issue keyword-based queries, retrieve historical background, and generate complete articles. Unlike typical summarization or retrieval tasks, essential context is not directly available and must be actively discovered, reflecting the information gaps faced in real-world news writing. NEWSAGENT includes 6k human-verified examples derived from real news, with multimodal contents converted to text for broad model compatibility. We evaluate open- and closed-sourced LLMs with commonly-used agentic frameworks on NEWSAGENT, which shows that agents are capable of retrieving relevant facts but struggling with planning and narrative integration. We believe that NEWSAGENT serves a realistic testbed for iterating and evaluating agent capabilities in terms of multimodal web data manipulation to real-world productivity.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Data Visualization and Narrative Generation</title>
<link>https://arxiv.org/abs/2509.00481</link>
<guid>https://arxiv.org/abs/2509.00481</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, data visualization, multi-agent system, granular outputs, sustainable collaboration

Summary: 
This article discusses the implementation of a lightweight multi-agent system in the field of data visualization to automate the data analysis workflow. By leveraging a hybrid multi-agent architecture with deterministic components, the system enhances transparency and reliability by externalizing critical logic. It is designed to deliver granular, modular outputs that allow for surgical modifications without requiring full regeneration, promoting sustainable collaboration between humans and AI. The system was evaluated across diverse datasets, demonstrating strong generalizability, narrative quality, and computational efficiency with minimal dependencies. Overall, this approach showcases the potential of multi-agent systems in enhancing the automation and collaboration capabilities in data visualization processes. 

<br /><br />Summary: <div>
arXiv:2509.00481v1 Announce Type: new 
Abstract: Recent advancements in the field of AI agents have impacted the way we work, enabling greater automation and collaboration between humans and agents. In the data visualization field, multi-agent systems can be useful for employing agents throughout the entire data-to-communication pipeline. We present a lightweight multi-agent system that automates the data analysis workflow, from data exploration to generating coherent visual narratives for insight communication. Our approach combines a hybrid multi-agent architecture with deterministic components, strategically externalizing critical logic from LLMs to improve transparency and reliability. The system delivers granular, modular outputs that enable surgical modifications without full regeneration, supporting sustainable human-AI collaboration. We evaluated our system across 4 diverse datasets, demonstrating strong generalizability, narrative quality, and computational efficiency with minimal dependencies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence-Based Analysis of Ice Cream Melting Behavior Under Various Ingredients</title>
<link>https://arxiv.org/abs/2509.00507</link>
<guid>https://arxiv.org/abs/2509.00507</guid>
<content:encoded><![CDATA[
<div> Keywords: ice cream, stabilizers, melting behavior, foam-like structure, cost-effective

Summary: 
The study examines the impact of different stabilizers, including locust bean gum, guar gum, maltodextrin, and carrageenan, on the melting behavior of homemade ice cream. Through melting tests and timelapse recordings, it was found that all samples maintained a foam-like structure after melting, indicating the stabilizers contributed to a stable air-cell matrix. Additionally, re-freezing and re-melting the samples showed increased sturdiness, suggesting improved structural resilience. Comparative analysis revealed varying effectiveness among stabilizers, with some providing stronger melting resistance and structural support. The study highlights the potential for developing cost-effective ice cream recipes that balance durability and economic efficiency, offering practical applications for both small-scale and commercial production. 

<br /><br />Summary: <div>
arXiv:2509.00507v1 Announce Type: new 
Abstract: The stability of ice cream during melting is a critical factor for consumer's acceptance and product quality. With the commonly added stabilizer to improve texture, structure and slower melting as the factors to analyze. This report explores the effects of locust bean gum, guar gum, maltodextrin, and carrageenan on the melting behavior of homemade ice cream. The main objective was to assess how these additives influence melting resistance and to identify a more cost-effective recipe formulation. Ice cream samples incorporating each additive were prepared and subjected to melting tests under controlled conditions. Timelapse recordings were used to capture and analyze the progression of melting over time. Python and OpenCV is used for process and analysis. Observations revealed that all samples retained a foam-like structure even after melting, suggesting the stabilizers contributed to the formation of a stable air-cell matrix. Furthermore, when the melted samples were re-frozen and subsequently melted again, they displayed increased sturdiness, indicating improved resilience of the ice cream structure. Comparative analysis of the different stabilizers highlighted variations in their effectiveness, with some offering stronger melting resistance and structural support than others. Overall, the findings provide insights into the functional roles of commonly used food additives in ice cream formulation. By evaluating both performance and cost, this study demonstrates the potential for developing recipes that balance durability with economic efficiency, contributing to practical applications in both small-scale and commercial ice cream production.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Assisted Iterative Evolution with Swarm Intelligence Toward SuperBrain</title>
<link>https://arxiv.org/abs/2509.00510</link>
<guid>https://arxiv.org/abs/2509.00510</guid>
<content:encoded><![CDATA[
<div> Keywords: SuperBrain, collective intelligence, large language models, co-evolution, Swarm Intelligence

Summary: 
The proposed SuperBrain framework focuses on the collaboration between large language models (LLMs) and human users, aiming to create a collective intelligence system. This dynamic approach involves the evolution from Subclass Brain to Superclass Brain through personalized interactions, adaptive learning memory, and GA-assisted evolution. Subclass Brains are formed through persistent user-LLM interactions, refining prompts and task performance. These Subclass Brains then coordinate via Swarm Intelligence to optimize across various tasks and exchange heuristics. The behaviors and cognitive signatures of multiple Subclass Brains integrate into a Superclass Brain, which possesses meta-intelligence capabilities such as abstraction, generalization, and self-improvement. Initial implementations include UAV scheduling and keyword filtering tasks. The proposed registry enables cross-dyad knowledge consolidation. This work lays the foundation for scalable, explainable, and ethically aligned collective AI systems.Summary: <div>
arXiv:2509.00510v1 Announce Type: new 
Abstract: We propose a novel SuperBrain framework for collective intelligence, grounded in the co-evolution of large language models (LLMs) and human users. Unlike static prompt engineering or isolated agent simulations, our approach emphasizes a dynamic pathway from Subclass Brain to Superclass Brain: (1) A Subclass Brain arises from persistent, personalized interaction between a user and an LLM, forming a cognitive dyad with adaptive learning memory. (2) Through GA-assisted forward-backward evolution, these dyads iteratively refine prompts and task performance. (3) Multiple Subclass Brains coordinate via Swarm Intelligence, optimizing across multi-objective fitness landscapes and exchanging distilled heuristics. (4) Their standardized behaviors and cognitive signatures integrate into a Superclass Brain, an emergent meta-intelligence capable of abstraction, generalization and self-improvement. We outline the theoretical constructs, present initial implementations (e.g., UAV scheduling, KU/KI keyword filtering) and propose a registry for cross-dyad knowledge consolidation. This work provides both a conceptual foundation and an architectural roadmap toward scalable, explainable and ethically aligned collective AI.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-Layout: A Generative Workflow for Drafting Architectural Floor Plans Using LLMs</title>
<link>https://arxiv.org/abs/2509.00543</link>
<guid>https://arxiv.org/abs/2509.00543</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, Large Language Models, architectural floor plans, furniture placement, Autodesk Revit <br />
Summary: 
This paper introduces an AI-powered workflow that utilizes Large Language Models (LLMs) to generate schematic architectural floor plans based on natural language prompts. The system interprets textual input to create layout options, including walls, doors, windows, and furniture placements, with minimal manual input required. Through prompt engineering, furniture placement refinement algorithms, and Python scripting, the system produces coherent draft plans that can easily be integrated into design tools like Autodesk Revit. A case study on a mid-sized residential layout showcases the system's ability to generate functional and structured outputs. The workflow is transparent, with detailed prompt specifications provided for replication by other researchers. The generated models retain all necessary Revit-native parametric attributes, facilitating seamless integration into professional BIM processes. <br /><br />Summary: <div>
arXiv:2509.00543v1 Announce Type: new 
Abstract: This paper presents the development of an AI-powered workflow that uses Large Language Models (LLMs) to assist in drafting schematic architectural floor plans from natural language prompts. The proposed system interprets textual input to automatically generate layout options including walls, doors, windows, and furniture arrangements. It combines prompt engineering, a furniture placement refinement algorithm, and Python scripting to produce spatially coherent draft plans compatible with design tools such as Autodesk Revit. A case study of a mid-sized residential layout demonstrates the approach's ability to generate functional and structured outputs with minimal manual effort. The workflow is designed for transparent replication, with all key prompt specifications documented to enable independent implementation by other researchers. In addition, the generated models preserve the full range of Revit-native parametric attributes required for direct integration into professional BIM processes.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social World Models</title>
<link>https://arxiv.org/abs/2509.00559</link>
<guid>https://arxiv.org/abs/2509.00559</guid>
<content:encoded><![CDATA[
<div> Keywords: social dynamics, structured representation, reasoning, social interactions, AI systems

Summary: 
The paper introduces a novel structured social world representation formalism (S3AP) aimed at helping AI systems better understand and reason about social dynamics. Based on a POMDP-driven design, S3AP represents social interactions as structured tuples, enabling automatic induction from free-form narratives. The study demonstrates that S3AP can enhance Language Models' (LLMs) understanding of social narratives and improve performance on social reasoning tasks, outperforming existing benchmarks. Furthermore, the induced social world models from these structured representations prove effective in predicting future social dynamics and enhancing agent decision-making processes. The findings highlight the potential of S3AP as a versatile representation for social world states, enabling the development of more socially-aware AI systems that can navigate social interactions more effectively.

<br /><br />Summary: <div>
arXiv:2509.00559v1 Announce Type: new 
Abstract: Humans intuitively navigate social interactions by simulating unspoken dynamics and reasoning about others' perspectives, even with limited information. In contrast, AI systems struggle to automatically structure and reason about these implicit social contexts. In this paper, we introduce a novel structured social world representation formalism (S3AP), designed to help AI systems reason more effectively about social dynamics. Following a POMDP-driven design, S3AP represents social interactions as structured tuples, such as state, observation, agent actions, and mental states, which can be automatically induced from free-form narratives or other inputs. We first show S3AP can help LLMs better understand social narratives across 5 social reasoning tasks (e.g., +51% improvement on FANToM's theory-of-mind reasoning with OpenAI's o1), reaching new state-of-the-art (SOTA) performance. We then induce social world models from these structured representations, demonstrating their ability to predict future social dynamics and improve agent decision-making, yielding up to +18% improvement on the SOTOPIA social interaction benchmark. Our findings highlight the promise of S3AP as a powerful, general-purpose representation for social world states, enabling the development of more socially-aware systems that better navigate social interactions.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BALM-TSF: Balanced Multimodal Alignment for LLM-Based Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.00622</link>
<guid>https://arxiv.org/abs/2509.00622</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, multimodal architectures, large language models, balance, alignment

Summary:
BALM-TSF (Balanced Multimodal Alignment for LLM-Based Time Series Forecasting) addresses the issue of imbalance between text and temporal data in forecasting by introducing a lightweight framework that maintains balance between the two modalities. It processes raw time series data with a time series encoder and generates textual embeddings using descriptive statistics with a learnable prompt in a large language model (LLM). A scaling strategy combined with a contrastive objective ensures balanced cross-modal context alignment between the time series and textual embeddings. The aligned textual semantic embeddings and time series embeddings are then integrated for forecasting. Experimental results on standard benchmarks demonstrate that BALM-TSF achieves state-of-the-art performance in long-term and few-shot forecasting with minimal trainable parameters. The framework effectively harnesses complementary information from text and time series data. Code for BALM-TSF is available on GitHub for further exploration and use.

<br /><br />Summary: <div>
arXiv:2509.00622v1 Announce Type: new 
Abstract: Time series forecasting is a long-standing and highly challenging research topic. Recently, driven by the rise of large language models (LLMs), research has increasingly shifted from purely time series methods toward harnessing textual modalities to enhance forecasting performance. However, the vast discrepancy between text and temporal data often leads current multimodal architectures to over-emphasise one modality while neglecting the other, resulting in information loss that harms forecasting performance. To address this modality imbalance, we introduce BALM-TSF (Balanced Multimodal Alignment for LLM-Based Time Series Forecasting), a lightweight time series forecasting framework that maintains balance between the two modalities. Specifically, raw time series are processed by the time series encoder, while descriptive statistics of raw time series are fed to an LLM with learnable prompt, producing compact textual embeddings. To ensure balanced cross-modal context alignment of time series and textual embeddings, a simple yet effective scaling strategy combined with a contrastive objective then maps these textual embeddings into the latent space of the time series embeddings. Finally, the aligned textual semantic embeddings and time series embeddings are together integrated for forecasting. Extensive experiments on standard benchmarks show that, with minimal trainable parameters, BALM-TSF achieves state-of-the-art performance in both long-term and few-shot forecasting, confirming its ability to harness complementary information from text and time series. Code is available at https://github.com/ShiqiaoZhou/BALM-TSF.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NetGent: Agent-Based Automation of Network Application Workflows</title>
<link>https://arxiv.org/abs/2509.00625</link>
<guid>https://arxiv.org/abs/2509.00625</guid>
<content:encoded><![CDATA[
<div> Keywords: NetGent, AI-agent framework, network traffic datasets, workflow automation, ML in networking <br />
Summary: 
NetGent is an AI-agent framework designed to automate complex application workflows for generating realistic network traffic datasets. It allows users to specify workflows as natural-language rules, which are then compiled into executable code for deterministic replay. The framework reduces redundant calls and adapts quickly to changes in application interfaces. In experiments, NetGent successfully automated various workflows related to video streaming, video conferencing, social media, and web scraping, producing realistic traffic traces. By combining language-based agents with compiled execution, NetGent provides a scalable solution for generating diverse and repeatable datasets crucial for advancing machine learning in networking. <br /><br /> <div>
arXiv:2509.00625v1 Announce Type: new 
Abstract: We present NetGent, an AI-agent framework for automating complex application workflows to generate realistic network traffic datasets. Developing generalizable ML models for networking requires data collection from network environments with traffic that results from a diverse set of real-world web applications. However, using existing browser automation tools that are diverse, repeatable, realistic, and efficient remains fragile and costly. NetGent addresses this challenge by allowing users to specify workflows as natural-language rules that define state-dependent actions. These abstract specifications are compiled into nondeterministic finite automata (NFAs), which a state synthesis component translates into reusable, executable code. This design enables deterministic replay, reduces redundant LLM calls through state caching, and adapts quickly when application interfaces change. In experiments, NetGent automated more than 50+ workflows spanning video-on-demand streaming, live video streaming, video conferencing, social media, and web scraping, producing realistic traffic traces while remaining robust to UI variability. By combining the flexibility of language-based agents with the reliability of compiled execution, NetGent provides a scalable foundation for generating the diverse, repeatable datasets needed to advance ML in networking.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized Knowledge Representations</title>
<link>https://arxiv.org/abs/2509.00710</link>
<guid>https://arxiv.org/abs/2509.00710</guid>
<content:encoded><![CDATA[
<div> extract, legal reasoning, AI systems, modular framework, knowledge acquisition <br />
Summary: <br />
This paper introduces a modular multi-agent framework for legal reasoning, dividing the process into knowledge acquisition and application stages. Specialized agents extract legal concepts and formalize rules to create intermediate representations of statutes. The framework then applies this knowledge to specific cases through analyzing queries, performing symbolic inference, and generating final answers. The approach combines natural language understanding with symbolic reasoning, enhancing transparency. Evaluation on tax calculation tasks showed foundational models achieving 76.4% accuracy, narrowing the performance gap. This suggests that modular architectures with formalized knowledge representations can make legal reasoning more accessible and enhance consistency and explainability in AI systems. The study lays a foundation for future research on transparent, trustworthy, and effective AI systems for the legal domain. <br /> <div>
arXiv:2509.00710v1 Announce Type: new 
Abstract: Legal reasoning requires both precise interpretation of statutory language and consistent application of complex rules, presenting significant challenges for AI systems. This paper introduces a modular multi-agent framework that decomposes legal reasoning into distinct knowledge acquisition and application stages. In the first stage, specialized agents extract legal concepts and formalize rules to create verifiable intermediate representations of statutes. The second stage applies this knowledge to specific cases through three steps: analyzing queries to map case facts onto the ontology schema, performing symbolic inference to derive logically entailed conclusions, and generating final answers using a programmatic implementation that operationalizes the ontological knowledge. This bridging of natural language understanding with symbolic reasoning provides explicit and verifiable inspection points, significantly enhancing transparency compared to end-to-end approaches. Evaluation on statutory tax calculation tasks demonstrates substantial improvements, with foundational models achieving 76.4\% accuracy compared to 18.8\% baseline performance, effectively narrowing the performance gap between reasoning and foundational models. These findings suggest that modular architectures with formalized knowledge representations can make sophisticated legal reasoning more accessible through computationally efficient models while enhancing consistency and explainability in AI legal reasoning, establishing a foundation for future research into more transparent, trustworthy, and effective AI systems for legal domain.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination</title>
<link>https://arxiv.org/abs/2509.00723</link>
<guid>https://arxiv.org/abs/2509.00723</guid>
<content:encoded><![CDATA[
<div> framework, hallucinations, multimodal, OLLMs, audio-video<br />
<br />
Summary:<br />
Omni-modal large language models (OLLMs) have shown impressive performance in tasks involving audio-video understanding. However, they still suffer from hallucination issues, where text cues dominate and neglect visual and audio information. The OmniDPO framework aims to address these challenges by enhancing the model's understanding of audio-video interactions through text-preference sample pairs and strengthening attention to visual and auditory information through multimodal-preference sample pairs. By tackling these challenges, OmniDPO effectively improves multimodal grounding and reduces hallucination in OLLMs. Experiments show that OmniDPO not only mitigates multimodal hallucinations but also enhances reasoning capabilities across modalities. The code and datasets will be released upon paper acceptance.<br /> <div>
arXiv:2509.00723v1 Announce Type: new 
Abstract: Recently, Omni-modal large language models (OLLMs) have sparked a new wave of research, achieving impressive results in tasks such as audio-video understanding and real-time environment perception. However, hallucination issues still persist. Similar to the bimodal setting, the priors from the text modality tend to dominate, leading OLLMs to rely more heavily on textual cues while neglecting visual and audio information. In addition, fully multimodal scenarios introduce new challenges. Most existing models align visual or auditory modalities with text independently during training, while ignoring the intrinsic correlations between video and its corresponding audio. This oversight results in hallucinations when reasoning requires interpreting hidden audio cues embedded in video content. To address these challenges, we propose OmniDPO, a preference-alignment framework designed to mitigate hallucinations in OLLMs. Specifically, OmniDPO incorporates two strategies: (1) constructing text-preference sample pairs to enhance the model's understanding of audio-video interactions; and (2) constructing multimodal-preference sample pairs to strengthen the model's attention to visual and auditory information. By tackling both challenges, OmniDPO effectively improves multimodal grounding and reduces hallucination. Experiments conducted on two OLLMs demonstrate that OmniDPO not only effectively mitigates multimodal hallucinations but also significantly enhances the models' reasoning capabilities across modalities. All code and datasets will be released upon paper acceptance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Graph Understanding with LLMs via Structured Context Injection</title>
<link>https://arxiv.org/abs/2509.00740</link>
<guid>https://arxiv.org/abs/2509.00740</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Graph Problems, Structured Context Injection, Computational Cost, Performance Improvements

Summary: 
Large Language Models (LLMs) have been successful in solving graph-related tasks by injecting structured context into the input to guide them. This approach eliminates the need for fine-tuning, making it cost-effective and efficient. Some graph reasoning tasks remain challenging for LLMs without mappings to grounded representations, but injecting structured context helps align tasks with conceptual spaces. This method, evaluated across various graph tasks with lightweight and large models, shows consistent performance improvements, highlighting the trade-offs between accuracy and computational cost. The results demonstrate that structured input context can match or outperform more complex approaches, emphasizing the effectiveness and scalability of structured context injection for graph understanding with LLMs.<br /><br />Summary: <div>
arXiv:2509.00740v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong capabilities in solving problems across domains, including graph-related tasks traditionally addressed by symbolic or algorithmic methods. In this work, we present a framework for structured context injection, where task-specific information is systematically embedded in the input to guide LLMs in solving a wide range of graph problems. Our method does not require fine-tuning of LLMs, making it cost-efficient and lightweight. We observe that certain graph reasoning tasks remain challenging for LLMs unless they are mapped to conceptually grounded representations. However, achieving such mappings through fine-tuning or repeated multi-step querying can be expensive and inefficient. Our approach offers a practical alternative by injecting structured context directly into the input, enabling the LLM to implicitly align the task with grounded conceptual spaces. We evaluate the approach on multiple graph tasks using both lightweight and large models, highlighting the trade-offs between accuracy and computational cost. The results demonstrate consistent performance improvements, showing that structured input context can rival or surpass more complex approaches. Our findings underscore the value of structured context injection as an effective and scalable strategy for graph understanding with LLMs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L-MARS -- Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search</title>
<link>https://arxiv.org/abs/2509.00761</link>
<guid>https://arxiv.org/abs/2509.00761</guid>
<content:encoded><![CDATA[
<div> Keywords: L-MARS, legal question answering, multi-agent reasoning, agentic search, LegalSearchQA

Summary:
L-MARS is a system designed to enhance legal question answering by combining multi-agent reasoning and agentic search techniques. It breaks down queries into subproblems, conducts targeted searches across various sources, and employs a Judge Agent for verification before answer synthesis. This iterative process helps reduce hallucination and uncertainty, ensuring coherence in answers. The system was evaluated on LegalSearchQA, a benchmark of 200 legal questions updated in 2025, and showed significant improvements in factual accuracy and reduced uncertainty. It received higher preference scores from both human experts and LLM-based judges. The study highlights the effectiveness of multi-agent reasoning and agentic search for deploying LLMs in high-stakes domains that require precise legal retrieval and deliberation.

<br /><br />Summary: 
1. L-MARS enhances legal question answering through multi-agent reasoning and agentic search.
2. Queries are decomposed into subproblems for targeted searches across heterogeneous sources.
3. A Judge Agent verifies sufficiency, jurisdiction, and temporal validity before answer synthesis.
4. The system reduces hallucination and uncertainty, ensuring coherence in answers.
5. L-MARS improves factual accuracy and receives higher preference scores in legal question answering evaluations. <div>
arXiv:2509.00761v1 Announce Type: new 
Abstract: We present L-MARS (Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search), a system that reduces hallucination and uncertainty in legal question answering through coordinated multi-agent reasoning and retrieval. Unlike single-pass retrieval-augmented generation (RAG), L-MARS decomposes queries into subproblems, issues targeted searches across heterogeneous sources (Serper web, local RAG, CourtListener case law), and employs a Judge Agent to verify sufficiency, jurisdiction, and temporal validity before answer synthesis. This iterative reasoning-search-verification loop maintains coherence, filters noisy evidence, and grounds answers in authoritative law. We evaluated L-MARS on LegalSearchQA, a new benchmark of 200 up-to-date multiple choice legal questions in 2025. Results show that L-MARS substantially improves factual accuracy, reduces uncertainty, and achieves higher preference scores from both human experts and LLM-based judges. Our work demonstrates that multi-agent reasoning with agentic search offers a scalable and reproducible blueprint for deploying LLMs in high-stakes domains requiring precise legal retrieval and deliberation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Reasoning LLMs for Materials Discovery with Physics-aware Rejection Sampling</title>
<link>https://arxiv.org/abs/2509.00768</link>
<guid>https://arxiv.org/abs/2509.00768</guid>
<content:encoded><![CDATA[
<div> AI-driven materials discovery, large reasoning models, Physics-aware Rejection Sampling, accuracy, calibration.

Summary:
Physics-aware Rejection Sampling (PaRS) is introduced to improve the accuracy and calibration of large reasoning models (LRMs) for AI-driven materials discovery. The method favors reasoning traces consistent with fundamental physics and numerically close to targets, reducing physics-violation rates while lowering sampling cost. By selecting traces based on domain-aware constraints and physics adherence, the framework enhances the reliability and efficiency of LRMs in process-aware property prediction and closed-loop materials design. This approach demonstrates the potential of combining modest constraints with trace-level selection to create more effective and practical models for automated experimentation and algorithmic decision-making in materials science. <br /><br />Summary: <div>
arXiv:2509.00768v1 Announce Type: new 
Abstract: AI-driven materials discovery that couples automated experimentation with algorithmic decision-making requires process aware recipe to property predictors that are accurate, calibrated, and physically admissible. We approach this as a reasoning problem with large reasoning models (LRMs). To instill reasoning capability into language models, we curate reasoning traces from a teacher model to train a student model. However, most training pipelines select reasoning traces using binary correctness or learned preference signals that poorly reflect physical admissibility. We introduce Physics-aware Rejection Sampling (PaRS), a training-time trace selection scheme that favors traces consistent with fundamental physics and numerically close to targets, with lightweight halting to control compute. We instantiate our framework with a large student model fine-tuned on traces synthesized by a larger teacher model, and evaluate under matched token budgets against various rejection sampling baselines. Our method improves accuracy and calibration, reduces physics-violation rates, and lowers sampling cost relative to baselines. These results indicate that modest, domain-aware constraints combined with trace-level selection provide a practical path toward reliable, efficient LRMs for process-aware property prediction and closed-loop materials design.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharpe Ratio Optimization in Markov Decision Processes</title>
<link>https://arxiv.org/abs/2509.00793</link>
<guid>https://arxiv.org/abs/2509.00793</guid>
<content:encoded><![CDATA[
<div> Keywords: Sharpe ratio, Markov decision processes, dynamic programming, risk metrics, optimization<br />
Summary: 
The paper discusses the optimization of the Sharpe ratio in infinite-horizon Markov decision processes, addressing challenges related to fractional objectives and risk metrics. By using Dinkelbach's transform, the Sharpe ratio objective is converted to a mean-squared-variance (M2V) objective, allowing for efficient optimization. An iterative algorithm is developed for M2V optimization, similar to mean-variance optimization in MDPs, with a sequence of Sharpe ratios derived that converges to the optimal value. Policy iteration procedures are developed for both average and discounted MDP settings, ensuring convergence to the optimum solution. Numerical experiments validate the proposed approach, which represents a novel method for solving Sharpe ratio optimization in MDPs using dynamic programming algorithms. <div>
arXiv:2509.00793v1 Announce Type: new 
Abstract: Sharpe ratio (also known as reward-to-variability ratio) is a widely-used metric in finance, which measures the additional return at the cost of per unit of increased risk (standard deviation of return). However, the optimization of Sharpe ratio in Markov decision processes (MDPs) is challenging, because there exist two difficulties hindering the application of dynamic programming. One is that dynamic programming does not work for fractional objectives, and the other is that dynamic programming is invalid for risk metrics. In this paper, we study the Sharpe ratio optimization in infinite-horizon MDPs, considering both the long-run average and discounted settings. We address the first challenge with the Dinkelbachs transform, which converts the Sharpe ratio objective to a mean-squared-variance (M2V) objective. It is shown that the M2V optimization and the original Sharpe ratio optimization share the same optimal policy when the risk-sensitive parameter is equal to the optimal Sharpe ratio. For the second challenge, we develop an iterative algorithm to solve the M2V optimization which is similar to a mean-variance optimization in MDPs. We iteratively solve the M2V problem and obtain the associated Sharpe ratio that is used to update the risk-sensitive parameter in the next iteration of M2V problems. We show that such a sequence of Sharpe ratios derived is monotonically increasing and converges to the optimal Sharpe ratio. For both average and discounted MDP settings, we develop a policy iteration procedure and prove its convergence to the optimum. Numerical experiments are conducted for validation. To the best of our knowledge, our approach is the first that solves the Sharpe ratio optimization in MDPs with dynamic programming type algorithms. We believe that the proposed algorithm can shed light on solving MDPs with other fractional objectives.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Predictive Process Monitoring</title>
<link>https://arxiv.org/abs/2509.00834</link>
<guid>https://arxiv.org/abs/2509.00834</guid>
<content:encoded><![CDATA[
<div> Neuro-Symbolic Predictive Process Monitoring, suffix prediction, Business Process Management, Linear Temporal Logic, Gumbel-Softmax trick<br />
<br />
Summary: This paper introduces a Neuro-Symbolic Predictive Process Monitoring approach to address suffix prediction in Business Process Management by integrating data-driven learning with temporal logic-based prior knowledge. Existing deep learning models for suffix prediction often lack logical constraints due to the absence of domain knowledge integration. The proposed method incorporates Linear Temporal Logic over finite traces into autoregressive sequence predictors during training, utilizing a differentiable logical loss function. This approach improves suffix prediction accuracy and compliance with temporal constraints, as demonstrated on real-world datasets. Two variants of the logic loss (local and global) are introduced and shown to be effective in noisy and realistic settings. While developed for BPM, the framework is adaptable to any symbolic sequence generation task, contributing to the advancement of Neuro-Symbolic AI.<br /><br />Summary: <div>
arXiv:2509.00834v1 Announce Type: new 
Abstract: This paper addresses the problem of suffix prediction in Business Process Management (BPM) by proposing a Neuro-Symbolic Predictive Process Monitoring (PPM) approach that integrates data-driven learning with temporal logic-based prior knowledge. While recent approaches leverage deep learning models for suffix prediction, they often fail to satisfy even basic logical constraints due to the absence of explicit integration of domain knowledge during training. We propose a novel method to incorporate Linear Temporal Logic over finite traces (LTLf) into the training process of autoregressive sequence predictors. Our approach introduces a differentiable logical loss function, defined using a soft approximation of LTLf semantics and the Gumbel-Softmax trick, which can be combined with standard predictive losses. This ensures the model learns to generate suffixes that are both accurate and logically consistent. Experimental evaluation on three real-world datasets shows that our method improves suffix prediction accuracy and compliance with temporal constraints. We also introduce two variants of the logic loss (local and global) and demonstrate their effectiveness under noisy and realistic settings. While developed in the context of BPM, our framework is applicable to any symbolic sequence generation task and contributes toward advancing Neuro-Symbolic AI.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop Insulin Adoption in Type 1 Diabetes Care</title>
<link>https://arxiv.org/abs/2509.00891</link>
<guid>https://arxiv.org/abs/2509.00891</guid>
<content:encoded><![CDATA[
<div> Keywords: closed-loop insulin delivery systems, type 1 diabetes, persuasive dialogue, virtual patients, persuasive AI 

Summary: 
ChatCLIDS is a new benchmark designed to evaluate LLM-driven persuasive dialogue for health behavior change, specifically focusing on the adoption of closed-loop insulin delivery systems in type 1 diabetes patients. The framework includes a library of virtual patients with diverse profiles and adoption barriers, allowing for multi-turn interactions with nurse agents using evidence-based persuasive strategies. The study found that while larger and more reflective LLMs can adapt strategies over time, they struggle to overcome resistance, particularly in the face of realistic social pressure. These results emphasize the limitations of current LLMs for behavior change and highlight the need for trustworthy persuasive AI in healthcare and other domains. ChatCLIDS provides a high-fidelity and scalable testbed for advancing research in this area. 

<br /><br />Summary: <div>
arXiv:2509.00891v1 Announce Type: new 
Abstract: Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1 diabetes remains low, driven not by technical failure, but by diverse behavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the first benchmark to rigorously evaluate LLM-driven persuasive dialogue for health behavior change. Our framework features a library of expert-validated virtual patients, each with clinically grounded, heterogeneous profiles and realistic adoption barriers, and simulates multi-turn interactions with nurse agents equipped with a diverse set of evidence-based persuasive strategies. ChatCLIDS uniquely supports longitudinal counseling and adversarial social influence scenarios, enabling robust, multi-dimensional evaluation. Our findings reveal that while larger and more reflective LLMs adapt strategies over time, all models struggle to overcome resistance, especially under realistic social pressure. These results highlight critical limitations of current LLMs for behavior change, and offer a high-fidelity, scalable testbed for advancing trustworthy persuasive AI in healthcare and beyond.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Deep Monte Carlo Counterfactual Regret Minimization: Addressing Theoretical Risks in Neural Fictitious Self-Play</title>
<link>https://arxiv.org/abs/2509.00923</link>
<guid>https://arxiv.org/abs/2509.00923</guid>
<content:encoded><![CDATA[
<div> Monte Carlo Counterfactual Regret Minimization (MCCFR), deep neural networks, scale-dependent challenges, adaptive framework, small vs. large games

Summary:<br /> 
1. The paper analyzes how neural MCCFR effectiveness varies with game scale, addressing theoretical risks such as nonstationary target distribution shifts, action support collapse, variance explosion, and warm-starting bias in small versus large games.
2. The proposed Robust Deep MCCFR framework incorporates target networks with delayed updates, uniform exploration mixing, variance-aware training objectives, and diagnostic monitoring to mitigate scale-dependent challenges.
3. Systematic ablation studies on Kuhn and Leduc Poker demonstrate scale-dependent component effectiveness and critical component interactions, achieving significant improvements in exploitability.
4. In Kuhn Poker, the best configuration achieves a 60% exploitability improvement over the classical framework, while selective component usage in Leduc Poker results in a 23.5% exploitability improvement, highlighting the importance of careful component selection.
5. Contributions include a formal theoretical analysis, a principled mitigation framework, multi-scale experimental validation, and practical guidelines for deployment in larger games.<br /><br /> <div>
arXiv:2509.00923v1 Announce Type: new 
Abstract: Monte Carlo Counterfactual Regret Minimization (MCCFR) has emerged as a cornerstone algorithm for solving extensive-form games, but its integration with deep neural networks introduces scale-dependent challenges that manifest differently across game complexities. This paper presents a comprehensive analysis of how neural MCCFR component effectiveness varies with game scale and proposes an adaptive framework for selective component deployment. We identify that theoretical risks such as nonstationary target distribution shifts, action support collapse, variance explosion, and warm-starting bias have scale-dependent manifestation patterns, requiring different mitigation strategies for small versus large games. Our proposed Robust Deep MCCFR framework incorporates target networks with delayed updates, uniform exploration mixing, variance-aware training objectives, and comprehensive diagnostic monitoring. Through systematic ablation studies on Kuhn and Leduc Poker, we demonstrate scale-dependent component effectiveness and identify critical component interactions. The best configuration achieves final exploitability of 0.0628 on Kuhn Poker, representing a 60% improvement over the classical framework (0.156). On the more complex Leduc Poker domain, selective component usage achieves exploitability of 0.2386, a 23.5% improvement over the classical framework (0.3703) and highlighting the importance of careful component selection over comprehensive mitigation. Our contributions include: (1) a formal theoretical analysis of risks in neural MCCFR, (2) a principled mitigation framework with convergence guarantees, (3) comprehensive multi-scale experimental validation revealing scale-dependent component interactions, and (4) practical guidelines for deployment in larger games.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement Fine-Tuning of LLMs</title>
<link>https://arxiv.org/abs/2509.00930</link>
<guid>https://arxiv.org/abs/2509.00930</guid>
<content:encoded><![CDATA[
<div> SATQuest, logical reasoning, Large Language Models, evaluation, reinforcement fine-tuning<br />
<br />
Summary: 
The article introduces SATQuest, a tool for evaluating and enhancing logical reasoning in Large Language Models (LLMs). SATQuest generates diverse logical reasoning problems from Conjunctive Normal Form instances, allowing for fine-grained analysis of reasoning capabilities. By structuring problems along three dimensions and using objective answer verification, SATQuest mitigates memorization issues and provides insights into LLM reasoning performance. Evaluation of various LLMs with SATQuest revealed limitations in logical reasoning and the effectiveness of reinforcement fine-tuning in improving performance. The tool also helps in generalizing to more complex instances but highlights challenges in cross-format adaptation. SATQuest is seen as a foundational tool for advancing LLM logical reasoning. <br /><br />Summary: <div>
arXiv:2509.00930v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated remarkable general reasoning capabilities. However, systematically evaluating and enhancing these reasoning capabilities is challenging due to the lack of controllable and scalable tools for fine-grained analysis. Existing benchmarks and datasets often lack the necessary variable control for multi-dimensional, systematic analysis and training, or have narrow problem types and formats. To address these limitations, we introduce SATQuest, a systematic verifier designed to evaluate and enhance logical reasoning in LLMs by generating diverse, Satisfiability-based logical reasoning problems directly from Conjunctive Normal Form (CNF) instances. SATQuest structures these problems along three orthogonal dimensions: instance scale, problem type, and question format, employing randomized, SAT-based problem generation and objective answer verification via PySAT. This design mitigates memorization issues, allows for nuanced insights into reasoning performance, and enables effective reinforcement fine-tuning. Our extensive evaluation of various LLMs using SATQuest identified significant limitations in their logical reasoning, particularly in generalizing beyond familiar mathematical formats. Furthermore, we show that reinforcement fine-tuning with SATQuest rewards substantially improves targeted task performance and generalizes to more complex instances, while highlighting remaining challenges in cross-format adaptation. Through these demonstrations, we showcase SATQuest's potential as a foundational tool and a valuable starting point for advancing LLM logical reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanInsight: A Distributed Edge Computing Framework with LLM-Powered Data Filtering for Smart City Digital Twins</title>
<link>https://arxiv.org/abs/2509.00936</link>
<guid>https://arxiv.org/abs/2509.00936</guid>
<content:encoded><![CDATA[
<div> framework, physics-informed machine learning, multimodal data fusion, knowledge graph, large language models <br />
<br />
Summary: This work presents a novel framework for harnessing the vast amounts of data generated by cities through sensors, cameras, and connected infrastructure. The framework combines physics-informed machine learning, multimodal data fusion, knowledge graph representation, and large language models (LLMs) to create a sophisticated system for improving urban life. Physics-informed methods ensure that predictions are grounded in real-world constraints, while knowledge graphs integrate diverse sensor data into a connected structure. At the edge, LLMs generate context-aware rules that adapt in real time for efficient operation. This approach enables the creation of responsive, trustworthy, and sustainable smart infrastructures that provide actionable insights, going beyond simple monitoring capabilities. By incorporating physics-based reasoning, semantic data fusion, and adaptive rule generation, this framework opens up new possibilities for transforming urban environments. <br /><br /> <div>
arXiv:2509.00936v1 Announce Type: new 
Abstract: Cities today generate enormous streams of data from sensors, cameras, and connected infrastructure. While this information offers unprecedented opportunities to improve urban life, most existing systems struggle with scale, latency, and fragmented insights. This work introduces a framework that blends physics-informed machine learning, multimodal data fusion, and knowledge graph representation with adaptive, rule-based intelligence powered by large language models (LLMs). Physics-informed methods ground learning in real-world constraints, ensuring predictions remain meaningful and consistent with physical dynamics. Knowledge graphs act as the semantic backbone, integrating heterogeneous sensor data into a connected, queryable structure. At the edge, LLMs generate context-aware rules that adapt filtering and decision-making in real time, enabling efficient operation even under constrained resources. Together, these elements form a foundation for digital twin systems that go beyond passive monitoring to provide actionable insights. By uniting physics-based reasoning, semantic data fusion, and adaptive rule generation, this approach opens new possibilities for creating responsive, trustworthy, and sustainable smart infrastructures.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Ai Framework For Strategic Patent Portfolio Pruning: Integrating Learning To-Rank And Market Need Analysis For Technology Transfer Optimization</title>
<link>https://arxiv.org/abs/2509.00958</link>
<guid>https://arxiv.org/abs/2509.00958</guid>
<content:encoded><![CDATA[
<div> Keywords: hybrid intelligence, patent portfolio, technology transfer, Learning to Rank, Natural Language Processing <br />
Summary: This paper presents a new hybrid intelligence framework for efficiently pruning patent portfolios to identify high-value assets for technology transfer. The framework combines a Learning to Rank model with a "Need-Seed" agent-based system to automate the analysis process. The system includes a "Need Agent" that mines market and industry data using Natural Language Processing to identify technological needs, and a "Seed Agent" that analyzes patent claims using Large Language Models to map technological capabilities. A "Core Ontology Framework" is generated to match high potential patents to documented market demands, aiding in divestment decisions. The architecture includes a dynamic parameter weighting system and a Human in the-Loop validation protocol for adaptability and real-world credibility. The framework enhances the current manual and time-intensive analysis methods by providing a strategic rationale for patent pruning decisions. <br /><br /> <div>
arXiv:2509.00958v1 Announce Type: new 
Abstract: This paper introduces a novel, multi stage hybrid intelligence framework for pruning patent portfolios to identify high value assets for technology transfer. Current patent valuation methods often rely on retrospective indicators or manual, time intensive analysis. Our framework automates and deepens this process by combining a Learning to Rank (LTR) model, which evaluates patents against over 30 legal and commercial parameters, with a unique "Need-Seed" agent-based system. The "Need Agent" uses Natural Language Processing (NLP) to mine unstructured market and industry data, identifying explicit technological needs. Concurrently, the "Seed Agent" employs fine tuned Large Language Models (LLMs) to analyze patent claims and map their technological capabilities. The system generates a "Core Ontology Framework" that matches high potential patents (Seeds) to documented market demands (Needs), providing a strategic rationale for divestment decisions. We detail the architecture, including a dynamic parameter weighting system and a crucial Human in the-Loop (HITL) validation protocol, to ensure both adaptability and real-world credibility.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra Strong Machine Learning: Teaching Humans Active Learning Strategies via Automated AI Explanations</title>
<link>https://arxiv.org/abs/2509.00961</link>
<guid>https://arxiv.org/abs/2509.00961</guid>
<content:encoded><![CDATA[
<div> Neuro-symbolic, Program synthesis, Large language models, Explanation generation, Human learning experiment <br />
Summary: <br />
Ultra Strong Machine Learning (USML) aims to improve machine performance and enhance human learning. LENS is introduced as a neuro-symbolic method that automates the explanation of machine-learned logic programs using large language models (LLMs). This approach outperforms hand-crafted templates and direct LLM prompting in generating explanations. However, in a human learning experiment across various domains, LENS did not significantly improve human performance, indicating that comprehensive LLM responses may be overwhelming for simpler problems. While LENS shows promise in automating explanation generation, its effectiveness in supporting human learning needs further investigation. Overall, this work presents a promising framework for developing efficient USML systems that can enhance both machine performance and human learning. <br /> <div>
arXiv:2509.00961v1 Announce Type: new 
Abstract: Ultra Strong Machine Learning (USML) refers to symbolic learning systems that not only improve their own performance but can also teach their acquired knowledge to quantifiably improve human performance. In this work, we present LENS (Logic Programming Explanation via Neural Summarisation), a neuro-symbolic method that combines symbolic program synthesis with large language models (LLMs) to automate the explanation of machine-learned logic programs in natural language. LENS addresses a key limitation of prior USML approaches by replacing hand-crafted explanation templates with scalable automated generation. Through systematic evaluation using multiple LLM judges and human validation, we demonstrate that LENS generates superior explanations compared to direct LLM prompting and hand-crafted templates. To investigate whether LENS can teach transferable active learning strategies, we carried out a human learning experiment across three related domains. Our results show no significant human performance improvements, suggesting that comprehensive LLM responses may overwhelm users for simpler problems rather than providing learning support. Our work provides a solid foundation for building effective USML systems to support human learning. The source code is available on: https://github.com/lun-ai/LENS.git.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks with LLMs</title>
<link>https://arxiv.org/abs/2509.00971</link>
<guid>https://arxiv.org/abs/2509.00971</guid>
<content:encoded><![CDATA[
<div> Reasoning Layer, General Symbolics, tool-calling, code generation, planning  
Summary:  
CoreThink introduces a state-of-the-art Reasoning Layer called General Symbolics, diverging from existing reasoning paradigms like test-time scaling and Reinforcement Learning. The CoreThink General Symbolic Reasoner excels in tool-calling, code generation, and planning, achieving top scores in benchmarks such as Livecodebench v6 and Instruction-Following Evals. The agentic coding IDE developed using General Symbolics achieves high accuracy in SWE-Bench Lite. These advancements are made without any finetuning or training costs, ensuring pure performance uplift without compromising accuracy. The system is designed to prevent diminishing returns in Large Language Model (LLM) performance. The technical report outlines the approach and availability of CoreThink models for reasoning-intensive tasks.   
<br /><br />Summary: <div>
arXiv:2509.00971v1 Announce Type: new 
Abstract: We introduce CoreThink, a state-of-the-art Reasoning Layer built upon a novel reasoning method called General Symbolics. This approach diverges from reasoning paradigms such as test-time scaling, Supervised Fine-Tuning (SFT), and Reinforcement Learning with Verifiable Rewards (RLVR). CoreThink General Symbolic Reasoner (GSR) is specifically structured around three key use cases: tool-calling, code generation, and planning, demonstrating exemplary performance across a total of seven benchmarks in their respective areas. Notably, we are achieving SOTA scores of 66.66\% on Livecodebench v6, 89\% on Instruction-Following Evals, and 24.4\% on ARC-AGI-2. We also present an agentic coding IDE, developed using the principles of General Symbolics, which achieves a state-of-the-art accuracy of 62.3\% on \texttt{SWE-Bench Lite}. We are able to achieve these improvements without any finetuning or training costs. Our Reasoning Layer is designed to provide a pure performance uplift, ensuring that a model's accuracy on reasoning tasks is never negatively impacted. We argue that incumbent methods will eventually lead to diminishing returns in LLM performance, necessitating the development of new reasoning techniques. This technical report details our approach at a high level and the availability of the CoreThink models for reasoning-intensive use cases.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.00975</link>
<guid>https://arxiv.org/abs/2509.00975</guid>
<content:encoded><![CDATA[
<div> framework, link forecasting, temporal graphs, large language models, explainability
Summary:
The paper introduces ReaL-TG, a reinforcement learning framework that fine-tunes large language models (LLMs) for explainable link forecasting on real-world temporal graphs. It uses outcome-based reward to encourage self-exploration of reasoning strategies and produce explanations for predictions. A new evaluation protocol combines ranking metrics with an LLM-as-a-Judge system to assess reasoning quality and hallucination impact. Experiments show that ReaL-TG-4B outperforms larger LLMs like GPT-5 mini on ranking metrics and provides high-quality explanations validated by both LLM and human evaluation. <div>
arXiv:2509.00975v1 Announce Type: new 
Abstract: Forecasting future links is a central task in temporal graph (TG) reasoning, requiring models to leverage historical interactions to predict upcoming ones. Traditional neural approaches, such as temporal graph neural networks, achieve strong performance but lack explainability and cannot be applied to unseen graphs without retraining. Recent studies have begun to explore using large language models (LLMs) for graph reasoning, but most of them are constrained to static graphs or small synthetic TGs and lack the evaluation of the quality of reasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced Learning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that fine-tunes LLMs to perform explainable link forecasting on real-world TGs. ReaL-TG uses outcome-based reward to encourage models to self-explore reasoning strategies from graph structure and to produce explanations that directly justify their predictions. To enable evaluation on LLM-generated reasoning traces, we propose a new evaluation protocol combining ranking metrics with an LLM-as-a-Judge system that assesses both the quality of reasoning and the impact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning Qwen3-4B under our framework, show that it outperforms much larger frontier LLMs, including GPT-5 mini, on ranking metrics, while producing high-quality explanations confirmed by both the LLM judge and human evaluation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation</title>
<link>https://arxiv.org/abs/2509.00987</link>
<guid>https://arxiv.org/abs/2509.00987</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, causal reasoning, multi-agent systems, causal discovery, estimation

Summary: 
Large Language Models (LLMs) have shown proficiency in reasoning tasks, but struggle with complex causal reasoning. Multi-agent systems, using multiple LLM-based agents, address these limitations by collaborating or specializing. These systems tackle causal reasoning, discovery, and estimation. Architectural patterns vary from pipeline-based to iterative refinement loops. Evaluation methods and benchmarks are diverse, with applications in scientific discovery, healthcare, fact-checking, and personalized systems. Persistent challenges and open research questions exist, with promising future directions. <br /><br />Summary: <div>
arXiv:2509.00987v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning and generation tasks. However, their proficiency in complex causal reasoning, discovery, and estimation remains an area of active development, often hindered by issues like hallucination, reliance on spurious correlations, and difficulties in handling nuanced, domain-specific, or personalized causal relationships. Multi-agent systems, leveraging the collaborative or specialized abilities of multiple LLM-based agents, are emerging as a powerful paradigm to address these limitations. This review paper explores the burgeoning field of causal multi-agent LLMs. We examine how these systems are designed to tackle different facets of causality, including causal reasoning and counterfactual analysis, causal discovery from data, and the estimation of causal effects. We delve into the diverse architectural patterns and interaction protocols employed, from pipeline-based processing and debate frameworks to simulation environments and iterative refinement loops. Furthermore, we discuss the evaluation methodologies, benchmarks, and diverse application domains where causal multi-agent LLMs are making an impact, including scientific discovery, healthcare, fact-checking, and personalized systems. Finally, we highlight the persistent challenges, open research questions, and promising future directions in this synergistic field, aiming to provide a comprehensive overview of its current state and potential trajectory.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supporting Our AI Overlords: Redesigning Data Systems to be Agent-First</title>
<link>https://arxiv.org/abs/2509.00997</link>
<guid>https://arxiv.org/abs/2509.00997</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, agentic speculation, data systems, query processing techniques, agentic memory stores

Summary:
Large Language Model (LLM) agents are expected to dominate data system workloads in the future, utilizing a process called agentic speculation for data manipulation and analysis. The volume and inefficiencies of agentic speculation pose challenges for current data systems, necessitating the adaptation of data systems to better support agentic workloads. The characteristics of agentic speculation, including scale, heterogeneity, redundancy, and steerability, present opportunities for new research in developing an agent-first data systems architecture. These opportunities range from creating new query interfaces to implementing novel query processing techniques and designing agentic memory stores. By leveraging the identified characteristics of agentic speculation, advancements can be made in optimizing data systems to effectively handle the demands of LLM agents and their data exploration and solution formulation processes.

<br /><br />Summary: <div>
arXiv:2509.00997v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents, acting on their users' behalf to manipulate and analyze data, are likely to become the dominant workload for data systems in the future. When working with data, agents employ a high-throughput process of exploration and solution formulation for the given task, one we call agentic speculation. The sheer volume and inefficiencies of agentic speculation can pose challenges for present-day data systems. We argue that data systems need to adapt to more natively support agentic workloads. We take advantage of the characteristics of agentic speculation that we identify, i.e., scale, heterogeneity, redundancy, and steerability - to outline a number of new research opportunities for a new agent-first data systems architecture, ranging from new query interfaces, to new query processing techniques, to new agentic memory stores.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Error Sources in LLM-based Hypothesis Search for Few-Shot Rule Induction</title>
<link>https://arxiv.org/abs/2509.01016</link>
<guid>https://arxiv.org/abs/2509.01016</guid>
<content:encoded><![CDATA[
<div> Keywords: Inductive reasoning, LLM-based hypothesis search, rule induction tasks, program generation, error analysis

Summary:
In this work, the authors compared an LLM-based hypothesis search framework with direct program generation approaches for few-shot rule induction tasks. The study found that hypothesis search achieved performance comparable to human ability in inferring abstract rules from limited examples and applying them in novel situations. Direct program generation, on the other hand, lagged behind in performance. An error analysis identified key bottlenecks in hypothesis generation and suggested avenues for advancing program induction methods. The research underscores the potential of LLM-based hypothesis search for modeling inductive reasoning but also highlights the challenges in constructing more efficient systems. Overall, the study sheds light on the capabilities and limitations of different approaches for inductive reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2509.01016v1 Announce Type: new 
Abstract: Inductive reasoning enables humans to infer abstract rules from limited examples and apply them to novel situations. In this work, we compare an LLM-based hypothesis search framework with direct program generation approaches on few-shot rule induction tasks. Our findings show that hypothesis search achieves performance comparable to humans, while direct program generation falls notably behind. An error analysis reveals key bottlenecks in hypothesis generation and suggests directions for advancing program induction methods. Overall, this paper underscores the potential of LLM-based hypothesis search for modeling inductive reasoning and the challenges in building more efficient systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-like Coherence Derived from the Interaction between Chemical Reaction and Its Environment</title>
<link>https://arxiv.org/abs/2509.01021</link>
<guid>https://arxiv.org/abs/2509.01021</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Natural-born Intelligence, open computing, chemical reactions, quantum coherence<br />
Summary:<br />
The article explores the distinction between Artificial Intelligence and Natural-born Intelligence, categorizing them into closed computing and open computing. Open computing is implemented in chemical reactions, where the computational process and execution environment merge to adapt to fluctuations. The model treats chemical reactions as computation and the environment as molecule interactions, leading to a system that adjusts through clustering and de-clustering. Open computing is further divided into Token computing, focusing on individual molecule behavior, and Type computing, focusing on normative behavior. The interplay between these two types results in self-organizing critical phenomena and quantum logic, leading to quantum-like coherence and the formation of spike waves for signal transmission. This suggests a quantum-like source for controlling biochemical rhythms. <div>
arXiv:2509.01021v1 Announce Type: new 
Abstract: By uncovering the contrast between Artificial Intelligence and Natural-born Intelligence as a computational process, we define closed computing and open computing, and implement open computing within chemical reactions. This involves forming a mixture and invalidation of the computational process and the execution environment, which are logically distinct, and coalescing both to create a system that adjusts fluctuations. We model chemical reactions by considering the computation as the chemical reaction and the execution environment as the degree of aggregation of molecules that interact with the reactive environment. This results in a chemical reaction that progresses while repeatedly clustering and de-clustering, where concentration no longer holds significant meaning. Open computing is segmented into Token computing, which focuses on the individual behavior of chemical molecules, and Type computing, which focuses on normative behavior. Ultimately, both are constructed as an interplay between the two. In this system, Token computing demonstrates self-organizing critical phenomena, while Type computing exhibits quantum logic. Through their interplay, the recruitment of fluctuations is realized, giving rise to interactions between quantum logical subspaces corresponding to quantum coherence across different Hilbert spaces. As a result, spike waves are formed, enabling signal transmission. This occurrence may be termed quantum-like coherence, implying the source of enzymes responsible for controlling spike waves and biochemical rhythms.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Planning and Multi-Agent Path Finding in Extremely Dense Environments with Movable Obstacles</title>
<link>https://arxiv.org/abs/2509.01022</link>
<guid>https://arxiv.org/abs/2509.01022</guid>
<content:encoded><![CDATA[
<div> block rearrangement problem, warehouse management, graph search, search-based solution algorithms, plan quality

Summary:
The article introduces the Block Rearrangement Problem (BRaP) in warehouse management, which involves rearranging storage blocks within dense grids to achieve a target state. BRaP is formally defined as a graph search problem. Five search-based solution algorithms are proposed, drawing inspiration from sliding puzzle problems and utilizing joint configuration space search, classical planning, multi-agent pathfinding, and expert heuristics. Empirical evaluations show that these methods are efficient in creating rearrangement plans even for deeply buried blocks in large grids up to 80x80 dimensions, despite the exponential increase in search space size with the number of blocks. The study emphasizes the importance of efficient algorithms for tackling complex warehouse management tasks such as block rearrangement. 

<br /><br />Summary: <div>
arXiv:2509.01022v1 Announce Type: new 
Abstract: We introduce the Block Rearrangement Problem (BRaP), a challenging component of large warehouse management which involves rearranging storage blocks within dense grids to achieve a target state. We formally define the BRaP as a graph search problem. Building on intuitions from sliding puzzle problems, we propose five search-based solution algorithms, leveraging joint configuration space search, classical planning, multi-agent pathfinding, and expert heuristics. We evaluate the five approaches empirically for plan quality and scalability. Despite the exponential relation between search space size and block number, our methods demonstrate efficiency in creating rearrangement plans for deeply buried blocks in up to 80x80 grids.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games</title>
<link>https://arxiv.org/abs/2509.01052</link>
<guid>https://arxiv.org/abs/2509.01052</guid>
<content:encoded><![CDATA[
<div> benchmark, FlashAdventure, agents, COAST, video games

Summary:
FlashAdventure is a new benchmark consisting of 34 Flash-based adventure games that focus on testing agents' ability to complete full story arcs. The benchmark aims to address the observation-behavior gap by requiring agents to remember and act on earlier gameplay information. The proposed automated gameplay evaluator, CUA-as-a-Judge, and agentic framework, COAST, aim to improve agents' performance by leveraging long-term clue memory for better planning and solving sequential tasks. Experiments show that current GUI agents struggle with completing full storylines, but COAST shows promise in bridging the observation-behavior gap and improving milestone completion. However, there is still a significant gap between the performance of the best-performing agents and human players, highlighting the need for further research to narrow this divide. <div>
arXiv:2509.01052v1 Announce Type: new 
Abstract: GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap: the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</title>
<link>https://arxiv.org/abs/2509.01055</link>
<guid>https://arxiv.org/abs/2509.01055</guid>
<content:encoded><![CDATA[
<div> Framework, Reinforcement Learning, VerlTool, Tool Integration, Multi-Turn Interactions 
Summary: 
VerlTool is a modular framework that combines Reinforcement Learning with Verifiable Rewards (RLVR) and Agentic Reinforcement Learning with Tool use (ARLT) to enable multi-turn interactions with integrated tools. It aligns with VeRL, manages tools through standardized APIs, executes rollouts asynchronously for speedup, and evaluates performance across various domains. The framework supports multi-modal observation tokens and can handle tasks such as mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering. VerlTool offers a unified training infrastructure, competitive results comparable to specialized systems, and a plugin architecture for easy tool integration with lightweight Python definitions, reducing development overhead and fostering tool-augmented RL research. The code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool. 
<br /><br />Summary: <div>
arXiv:2509.01055v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robix: A Unified Model for Robot Interaction, Reasoning and Planning</title>
<link>https://arxiv.org/abs/2509.01106</link>
<guid>https://arxiv.org/abs/2509.01106</guid>
<content:encoded><![CDATA[
<div> reasoning, task planning, natural language interaction, hierarchical robot system, end-to-end framework 

Summary: 
Robix is a unified model that combines robot reasoning, task planning, and natural language interaction in a single architecture. It generates commands for robots and verbal responses for humans, enabling complex instruction following and natural interaction. The model includes proactive dialogue, interruption handling, and context-aware commonsense reasoning during task execution. Robix is trained in three stages: continued pretraining for foundational reasoning abilities, supervised finetuning for human-robot interaction modeling, and reinforcement learning for long-horizon task coherence. In experiments, Robix outperforms baselines in interactive task execution across various instruction types and user-involved tasks such as table bussing, grocery shopping, and dietary filtering. <div>
arXiv:2509.01106v1 Announce Type: new 
Abstract: We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heads or Tails: A Simple Example of Causal Abstractive Simulation</title>
<link>https://arxiv.org/abs/2509.01136</link>
<guid>https://arxiv.org/abs/2509.01136</guid>
<content:encoded><![CDATA[
<div> Keywords: causal abstraction, simulation, language models, causality, philosophers<br />
Summary: <br />
This note introduces the concept of causal abstractive simulation, which is a formalization method for language model simulation. It discusses how language models can simulate a fair coin toss and presents examples of both successful and unsuccessful simulations. The formalism of causal abstractive simulation can be used to prove that a language model accurately simulates another system, based on a causal description of the system. This approach connects statistical benchmarking practices in language model simulation to a solid foundation of causality. It may be of interest to practitioners in the field of language model simulation, as well as philosophers of AI and mind who are interested in the role-playing aspect of language models. Additionally, mathematicians and researchers working on causal abstraction can find a new application of core ideas in this variation of causal abstraction. <div>
arXiv:2509.01136v1 Announce Type: new 
Abstract: This note illustrates how a variety of causal abstraction arXiv:1707.00819 arXiv:1812.03789, defined here as causal abstractive simulation, can be used to formalize a simple example of language model simulation. This note considers the case of simulating a fair coin toss with a language model. Examples are presented illustrating the ways language models can fail to simulate, and a success case is presented, illustrating how this formalism may be used to prove that a language model simulates some other system, given a causal description of the system. This note may be of interest to three groups. For practitioners in the growing field of language model simulation, causal abstractive simulation is a means to connect ad-hoc statistical benchmarking practices to the solid formal foundation of causality. Philosophers of AI and philosophers of mind may be interested as causal abstractive simulation gives a precise operationalization to the idea that language models are role-playing arXiv:2402.12422. Mathematicians and others working on causal abstraction may be interested to see a new application of the core ideas that yields a new variation of causal abstraction.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Question-to-Knowledge: Multi-Agent Generation of Inspectable Facts for Product Mapping</title>
<link>https://arxiv.org/abs/2509.01182</link>
<guid>https://arxiv.org/abs/2509.01182</guid>
<content:encoded><![CDATA[
<div> Keywords: ecommerce, SKU mapping, Large Language Models, product integration, disambiguation <br />
Summary: <br />
Identifying product listings referring to the same SKU in ecommerce is challenging due to missing identifiers and varied product names. The Q2K framework leverages Large Language Models for accurate SKU mapping by using a Reasoning Agent to generate questions, a Knowledge Agent to resolve them through web searches, and a Deduplication Agent to reduce redundancy. Human input refines uncertain cases. Q2K outperforms baselines in accuracy and robustness, particularly in bundle and brand disambiguation. By reusing reasoning, Q2K balances efficiency and accuracy, offering a scalable solution for product integration. <div>
arXiv:2509.01182v1 Announce Type: new 
Abstract: Identifying whether two product listings refer to the same Stock Keeping Unit (SKU) is a persistent challenge in ecommerce, especially when explicit identifiers are missing and product names vary widely across platforms. Rule based heuristics and keyword similarity often misclassify products by overlooking subtle distinctions in brand, specification, or bundle configuration. To overcome these limitations, we propose Question to Knowledge (Q2K), a multi agent framework that leverages Large Language Models (LLMs) for reliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates targeted disambiguation questions, (2) a Knowledge Agent that resolves them via focused web searches, and (3) a Deduplication Agent that reuses validated reasoning traces to reduce redundancy and ensure consistency. A human in the loop mechanism further refines uncertain cases. Experiments on real world consumer goods datasets show that Q2K surpasses strong baselines, achieving higher accuracy and robustness in difficult scenarios such as bundle identification and brand origin disambiguation. By reusing retrieved reasoning instead of issuing repeated searches, Q2K balances accuracy with efficiency, offering a scalable and interpretable solution for product integration.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Open-World Retrieval-Augmented Generation on Knowledge Graph: A Multi-Agent Collaboration Framework</title>
<link>https://arxiv.org/abs/2509.01238</link>
<guid>https://arxiv.org/abs/2509.01238</guid>
<content:encoded><![CDATA[
<div> Knowledge Graphs, Large Language Models, Retrieval-Augmented Generation, AnchorRAG, Multi-agent Collaboration

Summary:
AnchorRAG is introduced as a novel framework for open-world Retrieval-Augmented Generation (RAG) that does not rely on predefined anchor entities. This multi-agent collaboration approach involves a predictor agent for identifying candidate anchor entities, retriever agents for conducting multi-hop explorations, and a supervisor agent to formulate retrieval strategies. By dynamically aligning query terms with Knowledge Graph nodes, AnchorRAG improves retrieval robustness and mitigates errors in anchor entity linking. Extensive experiments on public benchmarks show that AnchorRAG outperforms existing baselines, achieving new state-of-the-art results in real-world question answering tasks. <div>
arXiv:2509.01238v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in language understanding and reasoning. However, their dependence on static training corpora makes them prone to factual errors and knowledge gaps. Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating external knowledge sources, especially structured Knowledge Graphs (KGs), which provide explicit semantics and efficient retrieval. Existing KG-based RAG approaches, however, generally assume that anchor entities are accessible to initiate graph traversal, which limits their robustness in open world settings where accurate linking between the query and the entity is unreliable. To overcome this limitation, we propose AnchorRAG, a novel multi-agent collaboration framework for open-world RAG without the predefined anchor entities. Specifically, a predictor agent dynamically identifies candidate anchor entities by aligning user query terms with KG nodes and initializes independent retriever agents to conduct parallel multi-hop explorations from each candidate. Then a supervisor agent formulates the iterative retrieval strategy for these retriever agents and synthesizes the resulting knowledge paths to generate the final answer. This multi-agent collaboration framework improves retrieval robustness and mitigates the impact of ambiguous or erroneous anchors. Extensive experiments on four public benchmarks demonstrate that AnchorRAG significantly outperforms existing baselines and establishes new state-of-the-art results on the real-world question answering tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Agentic OS: An LLM Agent Framework for Linux Schedulers</title>
<link>https://arxiv.org/abs/2509.01245</link>
<guid>https://arxiv.org/abs/2509.01245</guid>
<content:encoded><![CDATA[
arXiv:2509.01245v1 Announce Type: new 
Abstract: Operating system schedulers suffer from a fundamental semantic gap, where kernel policies fail to understand application-specific needs, leading to suboptimal performance. We introduce SchedCP, the first framework that enables fully autonomous Large Language Model (LLM) agents to safely and efficiently optimize Linux schedulers without human involvement. Our core insight is that the challenge is not merely to apply a better LLM, but to architect a decoupled control plane that separates the AI's role of semantic reasoning ("what to optimize") from the system's role of execution ("how to observe and act"). Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable interface with three key services: a Workload Analysis Engine, an evolving Scheduler Policy Repository, and an Execution Verifier that validates all AI-generated code and configure before deployment with static and dynamic analysis.
  We demonstrate this architecture's power with sched-agent, a multi-agent system that autonomously analyzes workloads, synthesizes custom eBPF scheduling policies, and deploys them via the sched\_ext infrastructure. Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. By bridging the semantic gap, SchedCP democratizes expert-level system optimization and represents a step towards creating truly self-optimizing, application-aware operating systems. The code is open-sourced in https://github.com/eunomia-bpf/schedcp
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communicative Agents for Slideshow Storytelling Video Generation based on LLMs</title>
<link>https://arxiv.org/abs/2509.01277</link>
<guid>https://arxiv.org/abs/2509.01277</guid>
<content:encoded><![CDATA[
arXiv:2509.01277v1 Announce Type: new 
Abstract: With the rapid advancement of artificial intelligence (AI), the proliferation of AI-generated content (AIGC) tasks has significantly accelerated developments in text-to-video generation. As a result, the field of video production is undergoing a transformative shift. However, conventional text-to-video models are typically constrained by high computational costs.
  In this study, we propose Video-Generation-Team (VGTeam), a novel slide show video generation system designed to redefine the video creation pipeline through the integration of large language models (LLMs). VGTeam is composed of a suite of communicative agents, each responsible for a distinct aspect of video generation, such as scriptwriting, scene creation, and audio design. These agents operate collaboratively within a chat tower workflow, transforming user-provided textual prompts into coherent, slide-style narrative videos.
  By emulating the sequential stages of traditional video production, VGTeam achieves remarkable improvements in both efficiency and scalability, while substantially reducing computational overhead. On average, the system generates videos at a cost of only $0.103, with a successful generation rate of 98.4%. Importantly, this framework maintains a high degree of creative fidelity and customization.
  The implications of VGTeam are far-reaching. It democratizes video production by enabling broader access to high-quality content creation without the need for extensive resources. Furthermore, it highlights the transformative potential of language models in creative domains and positions VGTeam as a pioneering system for next-generation content creation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradeSQL: Outcome Reward Models for Ranking SQL Queries from Large Language Models</title>
<link>https://arxiv.org/abs/2509.01308</link>
<guid>https://arxiv.org/abs/2509.01308</guid>
<content:encoded><![CDATA[
arXiv:2509.01308v1 Announce Type: new 
Abstract: Text-to-SQL, the task of translating natural language questions into SQL queries, has significantly advanced with the introduction of Large Language Models (LLMs), broadening database accessibility for a wide range of users. Despite substantial progress in generating valid SQL, current LLMs still struggle with complex queries that require precise alignment between user intent and the database schema. To mitigate this, test-time strategies such as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the assumption that LLMs can generate correct answers but may require multiple attempts. However, these methods rely on surface-level heuristics, selecting either the syntactically correct query through execution-based BoN (ex-BoN) or the most frequently generated query with Maj. Recently, Outcome Reward Models (ORMs), which assign utility scores to generated outputs based on semantic correctness, have emerged as a promising approach for better aligning model predictions with user intent. Nevertheless, their application to Text-to-SQL remains largely underexplored.
  In this work, we evaluate ORMs as an effective heuristic for BoN, compare them with ex-BoN and Maj, and introduce a framework for training ORMs for the Text-to-SQL task. We evaluate our ORMs on the BIRD and SPIDER benchmarks, finetuning various open-source LLMs, including the Qwen2, Granite3, and Llama3 model families. Our results show that ORMs outperform ex-BoN and Maj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that finetuning models already aligned with SQL generation, such as OmniSQL, yields superior ORM performance. Additionally, we observe that ORMs achieve competitive results on simple queries and benefit more from an increased number of candidates compared to ex-BoN and Maj.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Predictive Monitoring for Multi-Modal Scenarios</title>
<link>https://arxiv.org/abs/2509.01338</link>
<guid>https://arxiv.org/abs/2509.01338</guid>
<content:encoded><![CDATA[
arXiv:2509.01338v1 Announce Type: new 
Abstract: We consider the problem of quantitative predictive monitoring (QPM) of stochastic systems, i.e., predicting at runtime the degree of satisfaction of a desired temporal logic property from the current state of the system. Since computational efficiency is key to enable timely intervention against predicted violations, several state-of-the-art QPM approaches rely on fast machine-learning surrogates to provide prediction intervals for the satisfaction values, using conformal inference to offer statistical guarantees. However, these QPM methods suffer when the monitored agent exhibits multi-modal dynamics, whereby certain modes may yield high satisfaction values while others critically violate the property. Existing QPM methods are mode-agnostic and so would yield overly conservative and uninformative intervals that lack meaningful mode-specific satisfaction information. To address this problem, we present GenQPM, a method that leverages deep generative models, specifically score-based diffusion models, to reliably approximate the probabilistic and multi-modal system dynamics without requiring explicit model access. GenQPM employs a mode classifier to partition the predicted trajectories by dynamical mode. For each mode, we then apply conformal inference to produce statistically valid, mode-specific prediction intervals. We demonstrate the effectiveness of GenQPM on a benchmark of agent navigation and autonomous driving tasks, resulting in prediction intervals that are significantly more informative (less conservative) than mode-agnostic baselines.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Notebook-Guided, Training-Free Part Retrieval in 3D CAD Assemblies via Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.01350</link>
<guid>https://arxiv.org/abs/2509.01350</guid>
<content:encoded><![CDATA[
arXiv:2509.01350v1 Announce Type: new 
Abstract: Effective specification-aware part retrieval within complex CAD assemblies is essential for automated design verification and downstream engineering tasks. However, directly using LLMs/VLMs to this task presents some challenges: the input sequences may exceed model token limits, and even after processing, performance remains unsatisfactory. Moreover, fine-tuning LLMs/VLMs requires significant computational resources, and for many high-performing general-use proprietary models (e.g., GPT or Gemini), fine-tuning access is not available. In this paper, we propose a novel part retrieval framework that requires no extra training, but using Error Notebooks + RAG for refined prompt engineering to help improve the existing general model's retrieval performance. The construction of Error Notebooks consists of two steps: (1) collecting historical erroneous CoTs and their incorrect answers, and (2) connecting these CoTs through reflective corrections until the correct solutions are obtained. As a result, the Error Notebooks serve as a repository of tasks along with their corrected CoTs and final answers. RAG is then employed to retrieve specification-relevant records from the Error Notebooks and incorporate them into the inference process. Another major contribution of our work is a human-in-the-loop CAD dataset, which is used to evaluate our method. In addition, the engineering value of our novel framework lies in its ability to effectively handle 3D models with lengthy, non-natural language metadata. Experiments with proprietary models, including GPT-4o and the Gemini series, show substantial gains, with GPT-4o (Omni) achieving up to a 23.4% absolute accuracy improvement on the human preference dataset. Moreover, ablation studies confirm that CoT reasoning provides benefits especially in challenging cases with higher part counts (>10).
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks</title>
<link>https://arxiv.org/abs/2509.01396</link>
<guid>https://arxiv.org/abs/2509.01396</guid>
<content:encoded><![CDATA[
arXiv:2509.01396v1 Announce Type: new 
Abstract: Deep research agents have attracted growing attention for their potential to orchestrate multi-stage research workflows, spanning literature synthesis, methodological design, and empirical verification. Despite these strides, evaluating their research capability faithfully is rather challenging due to the difficulty of collecting frontier research questions that genuinely capture researchers' attention and intellectual curiosity. To address this gap, we introduce DeepResearch Arena, a benchmark grounded in academic seminars that capture rich expert discourse and interaction, better reflecting real-world research environments and reducing the risk of data leakage. To automatically construct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task Generation (MAHTG) system that extracts research-worthy inspirations from seminar transcripts. The MAHTG system further translates research-worthy inspirations into high-quality research tasks, ensuring the traceability of research task formulation while filtering noise. With the MAHTG system, we curate DeepResearch Arena with over 10,000 high-quality research tasks from over 200 academic seminars, spanning 12 disciplines, such as literature, history, and science. Our extensive evaluation shows that DeepResearch Arena presents substantial challenges for current state-of-the-art agents, with clear performance gaps observed across different models.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Need for Verification in AI-Driven Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.01398</link>
<guid>https://arxiv.org/abs/2509.01398</guid>
<content:encoded><![CDATA[
arXiv:2509.01398v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is transforming the practice of science. Machine learning and large language models (LLMs) can generate hypotheses at a scale and speed far exceeding traditional methods, offering the potential to accelerate discovery across diverse fields. However, the abundance of hypotheses introduces a critical challenge: without scalable and reliable mechanisms for verification, scientific progress risks being hindered rather than being advanced. In this article, we trace the historical development of scientific discovery, examine how AI is reshaping established practices for scientific discovery, and review the principal approaches, ranging from data-driven methods and knowledge-aware neural architectures to symbolic reasoning frameworks and LLM agents. While these systems can uncover patterns and propose candidate laws, their scientific value ultimately depends on rigorous and transparent verification, which we argue must be the cornerstone of AI-assisted discovery.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-empowered Agents Simulation Framework for Scenario Generation in Service Ecosystem Governance</title>
<link>https://arxiv.org/abs/2509.01441</link>
<guid>https://arxiv.org/abs/2509.01441</guid>
<content:encoded><![CDATA[
arXiv:2509.01441v1 Announce Type: new 
Abstract: As the social environment is growing more complex and collaboration is deepening, factors affecting the healthy development of service ecosystem are constantly changing and diverse, making its governance a crucial research issue. Applying the scenario analysis method and conducting scenario rehearsals by constructing an experimental system before managers make decisions, losses caused by wrong decisions can be largely avoided. However, it relies on predefined rules to construct scenarios and faces challenges such as limited information, a large number of influencing factors, and the difficulty of measuring social elements. These challenges limit the quality and efficiency of generating social and uncertain scenarios for the service ecosystem. Therefore, we propose a scenario generator design method, which adaptively coordinates three Large Language Model (LLM) empowered agents that autonomously optimize experimental schemes to construct an experimental system and generate high quality scenarios. Specifically, the Environment Agent (EA) generates social environment including extremes, the Social Agent (SA) generates social collaboration structure, and the Planner Agent (PA) couples task-role relationships and plans task solutions. These agents work in coordination, with the PA adjusting the experimental scheme in real time by perceiving the states of each agent and these generating scenarios. Experiments on the ProgrammableWeb dataset illustrate our method generates more accurate scenarios more efficiently, and innovatively provides an effective way for service ecosystem governance related experimental system construction.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Sensitivity for Faithful Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2509.01544</link>
<guid>https://arxiv.org/abs/2509.01544</guid>
<content:encoded><![CDATA[
arXiv:2509.01544v1 Announce Type: new 
Abstract: Large language models (LLMs) often produce correct answers while relying on flawed or irrelevant reasoning traces, undermining their trustworthiness in high-stakes domains. We propose Counterfactual Sensitivity Regularization (CSR), a lightweight training objective that enforces dependence between intermediate reasoning and final outputs. CSR introduces automated, operator-level counterfactual interventions (e.g., swapping "+" with "-") during training and penalizes models that preserve the same answer under logically invalid traces. This requires only one additional forward pass per sample. To measure faithfulness, we introduce Counterfactual Outcome Sensitivity (COS), which quantifies the impact of such perturbations on model predictions. Across structured reasoning tasks - arithmetic (GSM8K), logical deduction (PrOntoQA), and planning (Blocks World) - CSR improves faithfulness by up to 70 percentage points over standard fine-tuning and process supervision, with only minor accuracy loss. The learned sensitivity generalizes to larger models and synergizes with inference-time methods such as self-consistency. A pilot study on HellaSwag further demonstrates that extending CSR with semantic perturbations can enhance faithfulness in commonsense reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured AI Decision-Making in Disaster Management</title>
<link>https://arxiv.org/abs/2509.01576</link>
<guid>https://arxiv.org/abs/2509.01576</guid>
<content:encoded><![CDATA[
arXiv:2509.01576v1 Announce Type: new 
Abstract: With artificial intelligence (AI) being applied to bring autonomy to decision-making in safety-critical domains such as the ones typified in the aerospace and emergency-response services, there has been a call to address the ethical implications of structuring those decisions, so they remain reliable and justifiable when human lives are at stake. This paper contributes to addressing the challenge of decision-making by proposing a structured decision-making framework as a foundational step towards responsible AI. The proposed structured decision-making framework is implemented in autonomous decision-making, specifically within disaster management. By introducing concepts of Enabler agents, Levels and Scenarios, the proposed framework's performance is evaluated against systems relying solely on judgement-based insights, as well as human operators who have disaster experience: victims, volunteers, and stakeholders. The results demonstrate that the structured decision-making framework achieves 60.94% greater stability in consistently accurate decisions across multiple Scenarios, compared to judgement-based systems. Moreover, the study shows that the proposed framework outperforms human operators with a 38.93% higher accuracy across various Scenarios. These findings demonstrate the promise of the structured decision-making framework for building more reliable autonomous AI applications in safety-critical contexts.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Throttling Web Agents Using Reasoning Gates</title>
<link>https://arxiv.org/abs/2509.01619</link>
<guid>https://arxiv.org/abs/2509.01619</guid>
<content:encoded><![CDATA[
arXiv:2509.01619v1 Announce Type: new 
Abstract: AI web agents use Internet resources at far greater speed, scale, and complexity -- changing how users and services interact. Deployed maliciously or erroneously, these agents could overload content providers. At the same time, web agents can bypass CAPTCHAs and other defenses by mimicking user behavior or flood authentication systems with fake accounts. Yet providers must protect their services and content from denial-of-service attacks and scraping by web agents. In this paper, we design a framework that imposes tunable costs on agents before providing access to resources; we call this Web Agent Throttling. We start by formalizing Throttling Gates as challenges issued to an agent that are asymmetric, scalable, robust, and compatible with any agent. Focusing on a common component -- the language model -- we require the agent to solve reasoning puzzles, thereby incurring excessive token-generation costs. However, we find that using existing puzzles, e.g., coding or math, as throttling gates fails to satisfy our properties. To address this, we introduce rebus-based Reasoning Gates, synthetic text puzzles that require multi-hop reasoning over world knowledge (thereby throttling an agent's model). We design a scalable generation and verification protocol for such reasoning gates. Our framework achieves computational asymmetry, i.e., the response-generation cost is 9.2x higher than the generation cost for SOTA models. We further deploy reasoning gates on a custom website and Model Context Protocol (MCP) servers and evaluate with real-world web agents. Finally, we discuss the limitations and environmental impact of real-world deployment of our framework.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling LLM Jailbreaks Through Safety Knowledge Neurons</title>
<link>https://arxiv.org/abs/2509.01631</link>
<guid>https://arxiv.org/abs/2509.01631</guid>
<content:encoded><![CDATA[
arXiv:2509.01631v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly attracting attention in various applications. Nonetheless, there is a growing concern as some users attempt to exploit these models for malicious purposes, including the synthesis of controlled substances and the propagation of disinformation, a technique known as "Jailbreak." While some studies have achieved defenses against jailbreak attacks by modifying output distributions or detecting harmful content, the exact rationale still remains elusive. In this work, we present a novel neuron-level interpretability method that focuses on the role of safety-related knowledge neurons. Unlike existing approaches, our method projects the model's internal representation into a more consistent and interpretable vocabulary space. We then show that adjusting the activation of safety-related neurons can effectively control the model's behavior with a mean ASR higher than 97%. Building on this insight, we propose SafeTuning, a fine-tuning strategy that reinforces safety-critical neurons to improve model robustness against jailbreaks. SafeTuning consistently reduces attack success rates across multiple LLMs and outperforms all four baseline defenses. These findings offer a new perspective on understanding and defending against jailbreak attacks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics Supernova: AI Agent Matches Elite Gold Medalists at IPhO 2025</title>
<link>https://arxiv.org/abs/2509.01659</link>
<guid>https://arxiv.org/abs/2509.01659</guid>
<content:encoded><![CDATA[
arXiv:2509.01659v1 Announce Type: new 
Abstract: Physics provides fundamental laws that describe and predict the natural world. AI systems aspiring toward more general, real-world intelligence must therefore demonstrate strong physics problem-solving abilities: to formulate and apply physical laws for explaining and predicting physical processes. The International Physics Olympiad (IPhO)--the world's most prestigious physics competition--offers a rigorous benchmark for this purpose. We introduce Physics Supernova, an AI agent system with superior physics problem-solving abilities that match elite IPhO gold medalists. In IPhO 2025 theory problems, Physics Supernova attains 23.5/30 points, ranking 14th of 406 contestants and surpassing the median performance of human gold medalists. We extensively analyzed Physics Supernova's capabilities and flexibility across diverse physics tasks. These results show that principled tool integration within agent systems can deliver competitive improvements in solving challenging science problems. The codes are available at https://github.com/CharlesQ9/Physics-Supernova.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-enabled semantic-centric framework to consume privacy policies</title>
<link>https://arxiv.org/abs/2509.01716</link>
<guid>https://arxiv.org/abs/2509.01716</guid>
<content:encoded><![CDATA[
arXiv:2509.01716v1 Announce Type: new 
Abstract: In modern times, people have numerous online accounts, but they rarely read the Terms of Service or Privacy Policy of those sites, despite claiming otherwise, due to the practical difficulty in comprehending them. The mist of data privacy practices forms a major barrier for user-centred Web approaches, and for data sharing and reusing in an agentic world. Existing research proposed methods for using formal languages and reasoning for verifying the compliance of a specified policy, as a potential cure for ignoring privacy policies. However, a critical gap remains in the creation or acquisition of such formal policies at scale. We present a semantic-centric approach for using state-of-the-art large language models (LLM), to automatically identify key information about privacy practices from privacy policies, and construct $\mathit{Pr}^2\mathit{Graph}$, knowledge graph with grounding from Data Privacy Vocabulary (DPV) for privacy practices, to support downstream tasks. Along with the pipeline, the $\mathit{Pr}^2\mathit{Graph}$ for the top-100 popular websites is also released as a public resource, by using the pipeline for analysis. We also demonstrate how the $\mathit{Pr}^2\mathit{Graph}$ can be used to support downstream tasks by constructing formal policy representations such as Open Digital Right Language (ODRL) or perennial semantic Data Terms of Use (psDToU). To evaluate the technology capability, we enriched the Policy-IE dataset by employing legal experts to create custom annotations. We benchmarked the performance of different large language models for our pipeline and verified their capabilities. Overall, they shed light on the possibility of large-scale analysis of online services' privacy practices, as a promising direction to audit the Web and the Internet. We release all datasets and source code as public resources to facilitate reuse and improvement.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title>
<link>https://arxiv.org/abs/2509.01909</link>
<guid>https://arxiv.org/abs/2509.01909</guid>
<content:encoded><![CDATA[
arXiv:2509.01909v1 Announce Type: new 
Abstract: Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Real Is AI Tutoring? Comparing Simulated and Human Dialogues in One-on-One Instruction</title>
<link>https://arxiv.org/abs/2509.01914</link>
<guid>https://arxiv.org/abs/2509.01914</guid>
<content:encoded><![CDATA[
arXiv:2509.01914v1 Announce Type: new 
Abstract: Heuristic and scaffolded teacher-student dialogues are widely regarded as critical for fostering students' higher-order thinking and deep learning. However, large language models (LLMs) currently face challenges in generating pedagogically rich interactions. This study systematically investigates the structural and behavioral differences between AI-simulated and authentic human tutoring dialogues. We conducted a quantitative comparison using an Initiation-Response-Feedback (IRF) coding scheme and Epistemic Network Analysis (ENA). The results show that human dialogues are significantly superior to their AI counterparts in utterance length, as well as in questioning (I-Q) and general feedback (F-F) behaviors. More importantly, ENA results reveal a fundamental divergence in interactional patterns: human dialogues are more cognitively guided and diverse, centered around a "question-factual response-feedback" teaching loop that clearly reflects pedagogical guidance and student-driven thinking; in contrast, simulated dialogues exhibit a pattern of structural simplification and behavioral convergence, revolving around an "explanation-simplistic response" loop that is essentially a simple information transfer between the teacher and student. These findings illuminate key limitations in current AI-generated tutoring and provide empirical guidance for designing and evaluating more pedagogically effective generative educational dialogue systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Speculative Agent Planning</title>
<link>https://arxiv.org/abs/2509.01920</link>
<guid>https://arxiv.org/abs/2509.01920</guid>
<content:encoded><![CDATA[
arXiv:2509.01920v1 Announce Type: new 
Abstract: Despite their remarkable success in complex tasks propelling widespread adoption, large language-model-based agents still face critical deployment challenges due to prohibitive latency and inference costs. While recent work has explored various methods to accelerate inference, existing approaches suffer from significant limitations: they either fail to preserve performance fidelity, require extensive offline training of router modules, or incur excessive operational costs. Moreover, they provide minimal user control over the tradeoff between acceleration and other performance metrics. To address these gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous online reinforcement learning framework that provides lossless acceleration with substantially reduced costs without requiring additional pre-deployment preparation. DSP explicitly optimizes a joint objective balancing end-to-end latency against dollar cost, allowing practitioners to adjust a single parameter that steers the system toward faster responses, cheaper operation, or any point along this continuum. Experiments on two standard agent benchmarks demonstrate that DSP achieves comparable efficiency to the fastest lossless acceleration method while reducing total cost by 30% and unnecessary cost up to 60%. Our code and data are available through https://github.com/guanyilin428/Dynamic-Speculative-Planning.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title>
<link>https://arxiv.org/abs/2509.01938</link>
<guid>https://arxiv.org/abs/2509.01938</guid>
<content:encoded><![CDATA[
arXiv:2509.01938v1 Announce Type: new 
Abstract: Aligning AI with human values is a pressing unsolved problem. To address the lack of quantitative metrics for value alignment, we propose EigenBench: a black-box method for comparatively benchmarking language models' values. Given an ensemble of models, a constitution describing a value system, and a dataset of scenarios, our method returns a vector of scores quantifying each model's alignment to the given constitution. To produce these scores, each model judges the outputs of other models across many scenarios, and these judgments are aggregated with EigenTrust (Kamvar et al, 2003), yielding scores that reflect a weighted-average judgment of the whole ensemble. EigenBench uses no ground truth labels, as it is designed to quantify traits for which reasonable judges may disagree on the correct label. Using prompted personas, we test whether EigenBench scores are more sensitive to the model or the prompt: we find that most of the variance is explained by the prompt, but a small residual quantifies the disposition of the model itself.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in Clinical Decision Support</title>
<link>https://arxiv.org/abs/2509.02007</link>
<guid>https://arxiv.org/abs/2509.02007</guid>
<content:encoded><![CDATA[
arXiv:2509.02007v1 Announce Type: new 
Abstract: The deployment of Large Language Models (LLMs) in high-stakes medical settings poses a critical AI alignment challenge, as models can inherit and amplify societal biases, leading to significant disparities. Existing fairness evaluation methods fall short in these contexts as they typically use simplistic metrics that overlook the multi-dimensional nature of medical harms. This also promotes models that are fair only because they are clinically inert, defaulting to safe but potentially inaccurate outputs. To address this gap, our contributions are mainly two-fold: first, we construct two large-scale, controlled benchmarks (ED-Triage and Opioid Analgesic Recommendation) from MIMIC-IV, comprising over 50,000 prompts with twelve race x gender variants and three context tiers. Second, we propose a multi-metric framework - Multi-faceted Fairness Assessment based on hARMs ($mFARM$) to audit fairness for three distinct dimensions of disparity (Allocational, Stability, and Latent) and aggregate them into an $mFARM$ score. We also present an aggregated Fairness-Accuracy Balance (FAB) score to benchmark and observe trade-offs between fairness and prediction accuracy. We empirically evaluate four open-source LLMs (Mistral-7B, BioMistral-7B, Qwen-2.5-7B, Bio-LLaMA3-8B) and their finetuned versions under quantization and context variations. Our findings showcase that the proposed $mFARM$ metrics capture subtle biases more effectively under various settings. We find that most models maintain robust performance in terms of $mFARM$ score across varying levels of quantization but deteriorate significantly when the context is reduced. Our benchmarks and evaluation code are publicly released to enhance research in aligned AI for healthcare.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative KI f\"ur TA</title>
<link>https://arxiv.org/abs/2509.02053</link>
<guid>https://arxiv.org/abs/2509.02053</guid>
<content:encoded><![CDATA[
arXiv:2509.02053v1 Announce Type: new 
Abstract: Many scientists use generative AI in their scientific work. People working in technology assessment (TA) are no exception. TA's approach to generative AI is twofold: on the one hand, generative AI is used for TA work, and on the other hand, generative AI is the subject of TA research. After briefly outlining the phenomenon of generative AI and formulating requirements for its use in TA, the following article discusses in detail the structural causes of the problems associated with it. Although generative AI is constantly being further developed, the structurally induced risks remain. The article concludes with proposed solutions and brief notes on their feasibility, as well as some examples of the use of generative AI in TA work.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>