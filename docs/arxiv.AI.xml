<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>Towards Autonomous Sustainability Assessment via Multimodal AI Agents</title>
<link>https://arxiv.org/abs/2507.17012</link>
<guid>https://arxiv.org/abs/2507.17012</guid>
<content:encoded><![CDATA[
<div> AI agents, life cycle assessment, carbon emissions, sustainability information, environmental impacts  
Summary:  
- The study introduces AI agents for life cycle assessment, reducing expert time to under one minute and improving data availability.  
- The AI agents calculate cradle-to-gate carbon emissions of electronic devices by generating detailed life-cycle inventories using data abstraction and software tools.  
- A method is developed to estimate environmental impacts by comparing products with similar descriptions and known carbon footprints, running in 3 ms with a mean absolute percentage error (MAPE) of 12.28% on electronic products.  
- A data-driven method to generate emission factors is presented, improving MAPE by 120.26% compared to human experts selecting LCA database entries.  
- The study analyzes data, computes scaling of the approach, and discusses implications for future LCA workflows.  <div>
arXiv:2507.17012v1 Announce Type: new 
Abstract: Interest in sustainability information has surged in recent years. However, the data required for a life cycle assessment (LCA) that maps the materials and processes from product manufacturing to disposal into environmental impacts (EI) are often unavailable. Here we reimagine conventional LCA by introducing multimodal AI agents that emulate interactions between LCA experts and stakeholders like product managers and engineers to calculate the cradle-to-gate (production) carbon emissions of electronic devices. The AI agents iteratively generate a detailed life-cycle inventory leveraging a custom data abstraction and software tools that extract information from online text and images from repair communities and government certifications. This approach reduces weeks or months of expert time to under one minute and closes data availability gaps while yielding carbon footprint estimates within 19% of expert LCAs with zero proprietary data. Additionally, we develop a method to directly estimate EI by comparing an input to a cluster of products with similar descriptions and known carbon footprints. This runs in 3 ms on a laptop with a MAPE of 12.28% on electronic products. Further, we develop a data-driven method to generate emission factors. We use the properties of an unknown material to represent it as a weighted sum of emission factors for similar materials. Compared to human experts picking the closest LCA database entry, this improves MAPE by 120.26%. We analyze the data and compute scaling of this approach and discuss its implications for future LCA workflows.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding</title>
<link>https://arxiv.org/abs/2507.17054</link>
<guid>https://arxiv.org/abs/2507.17054</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agent Path Finding, Conflict-Based Search, Estimation, Flex Distribution, Delay-Based

Summary: 
Conflict-Based Flex Distribution is proposed to optimize Explicit Estimation Conflict-Based Search (EECBS) for the Multi-Agent Path Finding problem. The approach redistributes flex based on the number of collisions, improving efficiency by focusing on resolving collisions on specific sets of paths. Additionally, Delay-Based Flex Distribution estimates constraint delays, enhancing path optimization. A Mixed-Strategy Flex Distribution combines both mechanisms in a hierarchical framework for further improvements. The new flex distribution methods ensure completeness and bounded-suboptimal solutions for EECBS. Experimental results demonstrate the effectiveness of the proposed approaches over the original greedy flex distribution strategy. Overall, the proposed techniques significantly enhance the performance of EECBS in solving MAPF problems. 

<br><br>Summary: <div>
arXiv:2507.17054v1 Announce Type: new 
Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of collision-free paths, one for each agent in a shared environment. Its objective is to minimize the sum of path costs (SOC), where the path cost of each agent is defined as the travel time from its start location to its target location. Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for bounded-suboptimal MAPF, with the SOC of the solution being at most a user-specified factor $w$ away from optimal. EECBS maintains sets of paths and a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve collisions. For each path in a set, EECBS maintains a lower bound on its optimal path that satisfies constraints. By finding an individually bounded-suboptimal path with cost at most a threshold of $w$ times its lower bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up EECBS, previous work uses flex distribution to increase the threshold. Though EECBS with flex distribution guarantees to find a bounded-suboptimal solution, increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS to switch among different sets of paths instead of resolving collisions on a particular set of paths, and thus reducing efficiency. To address this issue, we propose Conflict-Based Flex Distribution that distributes flex in proportion to the number of collisions. We also estimate the delays needed to satisfy constraints and propose Delay-Based Flex Distribution. On top of that, we propose Mixed-Strategy Flex Distribution, combining both in a hierarchical framework. We prove that EECBS with our new flex distribution mechanisms is complete and bounded-suboptimal. Our experiments show that our approaches outperform the original (greedy) flex distribution.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA is All You Need for Safety Alignment of Reasoning LLMs</title>
<link>https://arxiv.org/abs/2507.17075</link>
<guid>https://arxiv.org/abs/2507.17075</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, fine-tuning, safety alignment, LoRA, reasoning abilities

Summary: 
In this work, the authors address the challenge of maintaining the safety of Large Language Models (LLMs) while fine-tuning them for specific tasks post-training. They introduce LoRA, a method for safety alignment fine-tuning (SFT), which effectively aligns the model for safety without compromising its reasoning abilities. By restricting safety weight updates to a low-rank space, LoRA minimizes interference with reasoning weights, resulting in highly safe LLMs across various benchmarks. The study shows that LoRA induces weight updates with smaller overlap with initial weights compared to full-model fine-tuning. Additionally, the authors explore techniques such as regularization and weight merging to further reduce overlap and observe improvements on specific tasks. Overall, the findings suggest that LoRA is a promising approach for achieving a consistent balance between reasoning abilities and safety in LLMs, encouraging the development of more effective strategies in the reasoning-safety trade-off. 

<br><br>Summary: <div>
arXiv:2507.17075v1 Announce Type: new 
Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex problems that were previously out of reach. To ensure LLMs do not assist with harmful requests, safety alignment fine-tuning is necessary in the post-training phase. However, safety alignment fine-tuning has recently been shown to significantly degrade reasoning abilities, a phenomenon known as the "Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets effectively aligns the model for safety without harming its reasoning capabilities. This is because restricting the safety weight updates to a low-rank space minimizes the interference with the reasoning weights. Our extensive experiments across four benchmarks covering math, science, and coding show that this approach produces highly safe LLMs -- with safety levels comparable to full-model fine-tuning -- without compromising their reasoning abilities. Additionally, we observe that LoRA induces weight updates with smaller overlap with the initial weights compared to full-model fine-tuning. We also explore methods that further reduce such overlap -- via regularization or during weight merging -- and observe some improvement on certain tasks. We hope this result motivates designing approaches that yield more consistent improvements in the reasoning-safety trade-off.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study</title>
<link>https://arxiv.org/abs/2507.17118</link>
<guid>https://arxiv.org/abs/2507.17118</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, safety-critical areas, autonomous driving systems, safety analyses, HySAFE-AI

Summary: 
AI is increasingly utilized in safety-critical domains such as autonomous driving systems, with the trend towards end-to-end monolithic architectures like large language models and vision language models. This paper reviews various architectural solutions and assesses the effectiveness of common safety analyses such as failure modes and effect analysis and fault tree analysis in the context of foundational AI models. The intricate nature of these models, particularly in their formation and utilization of latent representations, requires enhanced safety evaluation techniques. The authors propose HySAFE-AI, a Hybrid Safety Architectural Analysis Framework for AI Systems, which adapts traditional methods to assess the safety of AI systems. Suggestions for future work and the evolution of AI safety standards are also provided. 

<br><br>Summary: <div>
arXiv:2507.17118v1 Announce Type: new 
Abstract: AI has become integral to safety-critical areas like autonomous driving systems (ADS) and robotics. The architecture of recent autonomous systems are trending toward end-to-end (E2E) monolithic architectures such as large language models (LLMs) and vision language models (VLMs). In this paper, we review different architectural solutions and then evaluate the efficacy of common safety analyses such as failure modes and effect analysis (FMEA) and fault tree analysis (FTA). We show how these techniques can be improved for the intricate nature of the foundational models, particularly in how they form and utilize latent representations. We introduce HySAFE-AI, Hybrid Safety Architectural Analysis Framework for AI Systems, a hybrid framework that adapts traditional methods to evaluate the safety of AI systems. Lastly, we offer hints of future work and suggestions to guide the evolution of future AI safety standards.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLMs' Generalized Reasoning Abilities by Graph Problems</title>
<link>https://arxiv.org/abs/2507.17168</link>
<guid>https://arxiv.org/abs/2507.17168</guid>
<content:encoded><![CDATA[
<div> Graph Problem Reasoning, Large Language Models, Continued Pretraining, GraphPile, Logical Reasoning

Summary:
The article introduces the use of Graph Problem Reasoning (GPR) to enhance the general reasoning capabilities of Large Language Models (LLMs). While domain-specific continued pretraining methods have shown promise, they lack transferability to broader reasoning tasks. The authors pioneer the use of GPR tasks, such as pathfinding and network analysis, to teach diverse reasoning patterns. They introduce GraphPile, a large-scale dataset specifically designed for Continued Pretraining using GPR data, containing 10.9 billion tokens across 23 graph tasks. Training GraphMind on popular base models achieves higher accuracy in mathematical reasoning and improves non-mathematical reasoning tasks such as logical and commonsense reasoning. This work bridges the gap between domain-specific pretraining and universal reasoning capabilities, advancing the adaptability and robustness of LLMs.

<br><br>Summary: <div>
arXiv:2507.17168v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks, yet their performance often falters on novel and complex problems. Domain-specific continued pretraining (CPT) methods, such as those tailored for mathematical reasoning, have shown promise but lack transferability to broader reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning (GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks, spanning pathfinding, network analysis, numerical computation, and topological reasoning, require sophisticated logical and relational reasoning, making them ideal for teaching diverse reasoning patterns. To achieve this, we introduce GraphPile, the first large-scale corpus specifically designed for CPT using GPR data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes chain-of-thought, program-of-thought, trace of execution, and real-world graph data. Using GraphPile, we train GraphMind on popular base models Llama 3 and 3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in mathematical reasoning and up to 21.2 percent improvement in non-mathematical reasoning tasks such as logical and commonsense reasoning. By being the first to harness GPR for enhancing reasoning patterns and introducing the first dataset of its kind, our work bridges the gap between domain-specific pretraining and universal reasoning capabilities, advancing the adaptability and robustness of LLMs.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Our Cars Can Talk: How IoT Brings AI to Vehicles</title>
<link>https://arxiv.org/abs/2507.17214</link>
<guid>https://arxiv.org/abs/2507.17214</guid>
<content:encoded><![CDATA[
<div> keyphrases: AI copilots, intelligent vehicle systems, predictive maintenance, user interaction, interdisciplinary dialogue

Summary:
Integrating AI copilots into vehicles is crucial for shifting maintenance from reactive to proactive. By enabling vehicles as sensing platforms and bridging the communication gap between machines and drivers, the potential for transformative advancements in intelligent vehicle systems and predictive maintenance is substantial. This article serves as a catalyst for interdisciplinary discussions and sets the stage for future research and development in the realm of AI-powered user interaction. The convergence of AI technology and automotive systems presents an opportunity to enhance vehicle performance, optimize maintenance practices, and improve user experiences. Through collaborative efforts and a focus on integrating AI capabilities, the automotive industry can pave the way for smarter, more efficient vehicles that offer advanced features and proactive maintenance solutions.<br><br>Summary: <div>
arXiv:2507.17214v1 Announce Type: new 
Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to transforming maintenance from reactive to proactive. Now is the time to integrate AI copilots that speak both languages: machine and driver. This article offers a conceptual and technical perspective intended to spark interdisciplinary dialogue and guide future research and development in intelligent vehicle systems, predictive maintenance, and AI-powered user interaction.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Identity Evals: Measuring Agentic Identity</title>
<link>https://arxiv.org/abs/2507.17257</link>
<guid>https://arxiv.org/abs/2507.17257</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic capability, trustworthiness, language model agents, agent identity evals, LMA system

Summary:
Central to the effectiveness of language model agents (LMAs) is their ability to maintain a stable and reliable identity over time. However, LMAs face challenges such as statelessness, stochasticity, and sensitivity to prompts which can impact their identity, reliability, and trustworthiness. In order to address these challenges, the concept of agent identity evals (AIE) is introduced as a framework to measure the consistency and persistence of LMAs over time. AIE includes novel metrics that can be used to assess the agentic identity of LMAs and improve their performance. By incorporating AIE into the design and evaluation of LMAs, researchers can enhance their capabilities such as reasoning, planning, and action. This framework provides a systematic approach to evaluating and improving LMAs throughout their life cycle, ensuring their continued reliability and utility. <div>
arXiv:2507.17257v1 Announce Type: new 
Abstract: Central to agentic capability and trustworthiness of language model agents (LMAs) is the extent they maintain stable, reliable, identity over time. However, LMAs inherit pathologies from large language models (LLMs) (statelessness, stochasticity, sensitivity to prompts and linguistically-intermediation) which can undermine their identifiability, continuity, persistence and consistency. This attrition of identity can erode their reliability, trustworthiness and utility by interfering with their agentic capabilities such as reasoning, planning and action. To address these challenges, we introduce \textit{agent identity evals} (AIE), a rigorous, statistically-driven, empirical framework for measuring the degree to which an LMA system exhibit and maintain their agentic identity over time, including their capabilities, properties and ability to recover from state perturbations. AIE comprises a set of novel metrics which can integrate with other measures of performance, capability and agentic robustness to assist in the design of optimal LMA infrastructure and scaffolding such as memory and tools. We set out formal definitions and methods that can be applied at each stage of the LMA life-cycle, and worked examples of how to apply them.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?</title>
<link>https://arxiv.org/abs/2507.17258</link>
<guid>https://arxiv.org/abs/2507.17258</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, programming education, chatbot, feedback preferences, learning support systems

Summary: 
Students interacted with SCRIPT, a chatbot based on Generative AI, while solving programming tasks. The tool provided open-ended interactions and structured guidance through predefined prompts. Feedback requests from students followed a specific sequence, and the chatbot responses aligned well with the requested feedback types (in 75% of cases) while adhering to the system prompt constraints. The study involved 136 students from an introductory programming course at a large German university. Insights from the experiment inform the design of AI-based learning support systems and highlight challenges in balancing guidance and flexibility in such tools. This research contributes to the field of Generative AI in programming education by showcasing the effective use of chatbots for novice learners. 

<br><br>Summary: <div>
arXiv:2507.17258v1 Announce Type: new 
Abstract: Building on prior research on Generative AI (GenAI) and related tools for programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini, to support novice learners. SCRIPT allows for open-ended interactions and structured guidance through predefined prompts. We evaluated the tool via an experiment with 136 students from an introductory programming course at a large German university and analyzed how students interacted with SCRIPT while solving programming tasks with a focus on their feedback preferences. The results reveal that students' feedback requests seem to follow a specific sequence. Moreover, the chatbot responses aligned well with students' requested feedback types (in 75%), and it adhered to the system prompt constraints. These insights inform the design of GenAI-based learning support systems and highlight challenges in balancing guidance and flexibility in AI-assisted tools.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments</title>
<link>https://arxiv.org/abs/2507.17289</link>
<guid>https://arxiv.org/abs/2507.17289</guid>
<content:encoded><![CDATA[
<div> Keywords: Compliance Brain Assistant, AI assistant, user query router, FastTrack mode, FullAgentic mode

Summary:
Compliance Brain Assistant (CBA) is an AI assistant designed to enhance efficiency in compliance tasks within enterprise environments. It employs a user query router to intelligently choose between FastTrack mode for simple requests and FullAgentic mode for complex tasks that require composite actions. By proactively discovering context across compliance artifacts and utilizing various APIs, CBA provides curated responses. Experimental evaluations comparing CBA to a standard LLM demonstrated significant performance improvements in keyword match rate and pass rate. The routing-based design found a balance between the two modes, resulting in better overall performance metrics without sacrificing runtime efficiency. The study validates the effectiveness of the routing mechanism in enhancing the AI assistant's capabilities for handling compliance-related queries. 

<br><br>Summary: <div>
arXiv:2507.17289v1 Announce Type: new 
Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational, agentic AI assistant designed to boost the efficiency of daily compliance tasks for personnel in enterprise environments. To strike a good balance between response quality and latency, we design a user query router that can intelligently choose between (i) FastTrack mode: to handle simple requests that only need additional relevant context retrieved from knowledge corpora; and (ii) FullAgentic mode: to handle complicated requests that need composite actions and tool invocations to proactively discover context across various compliance artifacts, and/or involving other APIs/models for accommodating requests. A typical example would be to start with a user query, use its description to find a specific entity and then use the entity's information to query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on various real-world privacy/compliance-related queries targeting various personas. We found that CBA substantially improved upon the vanilla LLM's performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full routing-based design against the `fast-track only` and `full-agentic` modes and found that it had a better average match-rate and pass-rate while keeping the run-time approximately the same. This finding validated our hypothesis that the routing mechanism leads to a good trade-off between the two worlds.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning</title>
<link>https://arxiv.org/abs/2507.17418</link>
<guid>https://arxiv.org/abs/2507.17418</guid>
<content:encoded><![CDATA[
<div> Keywords: microscopic vehicle trajectories, context-aware, GAIL, PPO, WGAN-GP

Summary: 
Ctx2TrajGen is a novel framework for generating realistic urban driving behaviors by leveraging Generative Adversarial Imitation Learning (GAIL) with Proximal Policy Optimization (PPO) and Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP). This framework considers nonlinear interdependencies and training instability present in microscopic vehicle trajectory modeling. By taking into account surrounding vehicles and road geometry, Ctx2TrajGen produces interaction-aware trajectories that align with real-world contexts. Experimental results on the DRIFT dataset, captured by drones, show that Ctx2TrajGen outperforms existing methods in terms of realism, behavioral diversity, and contextual fidelity. This framework offers a robust solution to the challenges of data scarcity and domain shift in generating accurate microscopic vehicle trajectories without relying on simulation.<br><br>Summary: <div>
arXiv:2507.17418v1 Announce Type: new 
Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a context-aware trajectory generation framework that synthesizes realistic urban driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses nonlinear interdependencies and training instability inherent in microscopic settings. By explicitly conditioning on surrounding vehicles and road geometry, Ctx2TrajGen generates interaction-aware trajectories aligned with real-world context. Experiments on the drone-captured DRIFT dataset demonstrate superior performance over existing methods in terms of realism, behavioral diversity, and contextual fidelity, offering a robust solution to data scarcity and domain shift without simulation.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2507.17477</link>
<guid>https://arxiv.org/abs/2507.17477</guid>
<content:encoded><![CDATA[
<div> alignment, Large Language Models, uncertainty, automated, performance<br>
<br>
Summary:<br>
Large Language Models (LLMs) have made significant advancements in instruction following and reasoning. However, ensuring alignment with human intent and safety norms remains a challenge. The Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework aims to enhance LLM alignment without human annotations. UDASA generates multiple responses for each input and measures output uncertainty in semantics, factuality, and value alignment. By categorizing training samples based on uncertainty scores, UDASA optimizes model performance progressively. Experimental results demonstrate superior performance of UDASA in harmlessness, helpfulness, truthfulness, and controlled sentiment generation tasks compared to existing methods. The framework's automated approach shows promise in improving LLM alignment efficiently and effectively. <div>
arXiv:2507.17477v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in instruction following and general-purpose reasoning. However, achieving high-quality alignment with human intent and safety norms without human annotations remains a fundamental challenge. In this work, we propose an Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to improve LLM alignment in a fully automated manner. UDASA first generates multiple responses for each input and quantifies output uncertainty across three dimensions: semantics, factuality, and value alignment. Based on these uncertainty scores, the framework constructs preference pairs and categorizes training samples into three stages, conservative, moderate, and exploratory, according to their uncertainty difference. The model is then optimized progressively across these stages. In addition, we conduct a series of preliminary studies to validate the core design assumptions and provide strong empirical motivation for the proposed framework. Experimental results show that UDASA outperforms existing alignment methods across multiple tasks, including harmlessness, helpfulness, truthfulness, and controlled sentiment generation, significantly improving model performance.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning</title>
<link>https://arxiv.org/abs/2507.17482</link>
<guid>https://arxiv.org/abs/2507.17482</guid>
<content:encoded><![CDATA[
<div> neuro-symbolic artificial intelligence, continual learning, temporal reasoning, LTLZinc, benchmarking framework <br>
<br>
Summary: 
This study introduces LTLZinc, a benchmarking framework that generates datasets for evaluating neuro-symbolic and continual learning methods in temporal and constraint-driven scenarios. The framework generates tasks from a linear temporal logic specification over MiniZinc constraints and image classification datasets. Fine-grained annotations enable testing of various training settings on the same datasets. Experiments on neuro-symbolic sequence classification and class-continual learning tasks reveal the challenges of temporal learning and reasoning and identify limitations in current methods. The LTLZinc generator and ten tasks are released to the research community to advance research on unified temporal learning and reasoning frameworks. <div>
arXiv:2507.17482v1 Announce Type: new 
Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures with symbolic approaches that can represent knowledge in a human-interpretable formalism. Continual learning concerns with agents that expand their knowledge over time, improving their skills while avoiding to forget previously learned concepts. Most of the existing approaches for neuro-symbolic artificial intelligence are applied to static scenarios only, and the challenging setting where reasoning along the temporal dimension is necessary has been seldom explored. In this work we introduce LTLZinc, a benchmarking framework that can be used to generate datasets covering a variety of different problems, against which neuro-symbolic and continual learning methods can be evaluated along the temporal and constraint-driven dimensions. Our framework generates expressive temporal reasoning and continual learning tasks from a linear temporal logic specification over MiniZinc constraints, and arbitrary image classification datasets. Fine-grained annotations allow multiple neural and neuro-symbolic training settings on the same generated datasets. Experiments on six neuro-symbolic sequence classification and four class-continual learning tasks generated by LTLZinc, demonstrate the challenging nature of temporal learning and reasoning, and highlight limitations of current state-of-the-art methods. We release the LTLZinc generator and ten ready-to-use tasks to the neuro-symbolic and continual learning communities, in the hope of fostering research towards unified temporal learning and reasoning frameworks.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)</title>
<link>https://arxiv.org/abs/2507.17487</link>
<guid>https://arxiv.org/abs/2507.17487</guid>
<content:encoded><![CDATA[
<div> Keywords: Controlled Query Evaluation, Epistemic Dependencies, Ontologies, GA Censors, BUCQs

Summary:
Controlled Query Evaluation (CQE) over ontologies is studied with regulation based on epistemic dependencies (EDs) and optimal GA censors. The combination of these elements aims to ensure secure information disclosure. By focusing on answering Boolean unions of conjunctive queries (BUCQs) using the intersection of optimal GA censors, a strong security guarantee with efficient computational behavior is achieved. The security of this approach is characterized, specifically for full EDs. For a subclass of EDs and DL-Lite_R ontologies, it is demonstrated that answering BUCQs in this CQE framework is in AC^0 in data complexity, supported by a detailed first-order rewriting algorithm. Experimental evaluations illustrate the practical feasibility of the proposed rewriting function in two different scenarios. <div>
arXiv:2507.17487v1 Announce Type: new 
Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where information disclosure is regulated by epistemic dependencies (EDs), a family of logical rules recently proposed for the CQE framework. In particular, we combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground atoms that are entailed by the ontology and can be safely revealed. We focus on answering Boolean unions of conjunctive queries (BUCQs) with respect to the intersection of all optimal GA censors - an approach that has been shown in other contexts to ensure strong security guarantees with favorable computational behavior. First, we characterize the security of this intersection-based approach and identify a class of EDs (namely, full EDs) for which it remains safe. Then, for a subclass of EDs and for DL-Lite_R ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0 in data complexity by presenting a suitable, detailed first-order rewriting algorithm. Finally, we report on experiments conducted in two different evaluation scenarios, showing the practical feasibility of our rewriting function.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Hybrid Grounding Using Structural and Data-Driven Heuristics</title>
<link>https://arxiv.org/abs/2507.17493</link>
<guid>https://arxiv.org/abs/2507.17493</guid>
<content:encoded><![CDATA[
<div> keywords: Answer Set Programming, grounding bottleneck, hybrid grounding, bottom-up grounding, automated hybrid grounding <br>
Summary: <br>
- The grounding bottleneck is a challenge for the adoption of Answer Set Programming in industry. 
- Hybrid grounding combines standard bottom-up grounding and body-decoupled grounding techniques. 
- Automated hybrid grounding was developed to determine when to use body-decoupled grounding and when standard grounding is more beneficial. 
- Data-structural heuristics were used to develop a splitting algorithm for automated hybrid grounding. 
- Experiments on the prototypical implementation showed promising results, with improvements in hard-to-ground scenarios and approaching state-of-the-art performance on hard-to-solve instances. <div>
arXiv:2507.17493v1 Announce Type: new 
Abstract: The grounding bottleneck poses one of the key challenges that hinders the widespread adoption of Answer Set Programming in industry. Hybrid Grounding is a step in alleviating the bottleneck by combining the strength of standard bottom-up grounding with recently proposed techniques where rule bodies are decoupled during grounding. However, it has remained unclear when hybrid grounding shall use body-decoupled grounding and when to use standard bottom-up grounding. In this paper, we address this issue by developing automated hybrid grounding: we introduce a splitting algorithm based on data-structural heuristics that detects when to use body-decoupled grounding and when standard grounding is beneficial. We base our heuristics on the structure of rules and an estimation procedure that incorporates the data of the instance. The experiments conducted on our prototypical implementation demonstrate promising results, which show an improvement on hard-to-ground scenarios, whereas on hard-to-solve instances we approach state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.17512</link>
<guid>https://arxiv.org/abs/2507.17512</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, verifiable rewards, multi-domain reasoning, LLMs, GRPO algorithm<br>
Summary:<br>
1. The study investigates the integration of multiple cognitive skills in reinforcement learning with verifiable rewards (RLVR), focusing on mathematical reasoning, code generation, and logical puzzle solving.
2. Using the GRPO algorithm and the Qwen-2.5-7B model family, the study evaluates in-domain improvements and cross-domain generalization capabilities of models trained on single-domain datasets.
3. The study explores interactions, enhancements, and conflicts that arise during combined cross-domain training.
4. Analysis compares base and instruct models under identical RL configurations to understand the influence of SFT on RL.
5. Extensive experiments delve into RL training details, examining curriculum learning strategies, reward design variations, and language-specific factors, offering insights into factors impacting specialized and generalizable reasoning performance. 
<br><br> <div>
arXiv:2507.17512v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing research has predominantly concentrated on isolated reasoning domains such as mathematical problem-solving, coding tasks, or logical reasoning. However, real world reasoning scenarios inherently demand an integrated application of multiple cognitive skills. Despite this, the interplay among these reasoning skills under reinforcement learning remains poorly understood. To bridge this gap, we present a systematic investigation of multi-domain reasoning within the RLVR framework, explicitly focusing on three primary domains: mathematical reasoning, code generation, and logical puzzle solving. We conduct a comprehensive study comprising four key components: (1) Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the models' in-domain improvements and cross-domain generalization capabilities when trained on single-domain datasets. (2) Additionally, we examine the intricate interactions including mutual enhancements and conflicts that emerge during combined cross-domain training. (3) To further understand the influence of SFT on RL, we also analyze and compare performance differences between base and instruct models under identical RL configurations. (4) Furthermore, we delve into critical RL training details, systematically exploring the impacts of curriculum learning strategies, variations in reward design, and language-specific factors. Through extensive experiments, our results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance. These findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment</title>
<link>https://arxiv.org/abs/2507.17514</link>
<guid>https://arxiv.org/abs/2507.17514</guid>
<content:encoded><![CDATA[
<div> TAI Scan Tool, RAG-based, self-assessment, AI Act, compliance
Summary:
- Introduction of TAI Scan Tool for TAI self-assessment under the AI Act.
- Two-step approach: pre-screening and assessment phases.
- Assessment output includes risk-level insights and relevant articles for compliance.
- Qualitative evaluation with use-case scenarios shows promising results.
- Results show reliance on comparison with high-risk systems as per AI Act requirements.
<br><br>Summary: <div>
arXiv:2507.17514v1 Announce Type: new 
Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool with minimalistic input. The current version of the tool supports the legal TAI assessment, with a particular emphasis on facilitating compliance with the AI Act. It involves a two-step approach with a pre-screening and an assessment phase. The assessment output of the system includes insight regarding the risk-level of the AI system according to the AI Act, while at the same time retrieving relevant articles to aid with compliance and notify on their obligations. Our qualitative evaluation using use-case scenarios yields promising results, correctly predicting risk levels while retrieving relevant articles across three distinct semantic groups. Furthermore, interpretation of results shows that the tool's reasoning relies on comparison with the setting of high-risk systems, a behaviour attributed to their deployment requiring careful consideration, and therefore frequently presented within the AI Act.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning</title>
<link>https://arxiv.org/abs/2507.17539</link>
<guid>https://arxiv.org/abs/2507.17539</guid>
<content:encoded><![CDATA[
<div> FundusExpert, ophthalmology, MLLM, FundusGen, dataset <br>
Summary:  
FundusExpert is introduced as an ophthalmology-specific Multimodal Large Language Model (MLLM) with integrated positioning-diagnosis reasoning capabilities. FundusGen, a dataset generated through the Fundus-Engine system, provides annotation granularity and reasoning logic alignment for the model. The Fundus-Engine automates localization and integrates global disease classification, local object detection, and fine-grained feature analysis within fundus images. Fine-tuned with FundusGen data, FundusExpert outperforms existing models in ophthalmic question-answering and zero-shot report generation tasks. A scaling law between data quality and model capability is demonstrated, showing improved efficiency with cognitive alignment annotations. By incorporating region-level localization and diagnostic reasoning chains, this work aims to bridge the visual-language gap in specialized MLLMs. The project can be accessed at https://github.com/MeteorElf/FundusExpert. <br> <div>
arXiv:2507.17539v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in the field of medical diagnosis. However, they face critical challenges in specialized domains such as ophthalmology, particularly the fragmentation of annotation granularity and inconsistencies in clinical reasoning logic, which hinder precise cross-modal understanding. This paper introduces FundusExpert, an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning capabilities, along with FundusGen, a dataset constructed through the intelligent Fundus-Engine system. Fundus-Engine automates localization and leverages MLLM-based semantic expansion to integrate global disease classification, local object detection, and fine-grained feature analysis within a single fundus image. Additionally, by constructing a clinically aligned cognitive chain, it guides the model to generate interpretable reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen, achieves the best performance in ophthalmic question-answering tasks, surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in zero-shot report generation tasks, achieving a clinical consistency of 77.0%, significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling law between data quality and model capability ($L \propto N^{0.068}$), demonstrating that the cognitive alignment annotations in FundusGen enhance data utilization efficiency. By integrating region-level localization with diagnostic reasoning chains, our work develops a scalable, clinically-aligned MLLM and explores a pathway toward bridging the visual-language gap in specific MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating multiple human perspectives in socio-ecological systems using large language models</title>
<link>https://arxiv.org/abs/2507.17680</link>
<guid>https://arxiv.org/abs/2507.17680</guid>
<content:encoded><![CDATA[
<div> Keywords: socio-ecological systems, stakeholder perspectives, simulation framework, large language models, perspective-taking

Summary:<br><br>
The study introduces the HoPeS modelling framework, utilizing agents powered by large language models to represent various stakeholder perspectives in socio-ecological systems. Users can engage in perspective-taking simulations to gain insights into different viewpoints. A simulation protocol guides users through transitioning between roles and reflecting on diverse perspectives. The prototype system focuses on institutional dynamics and land use change, allowing users to explore narrative-driven and numerical experiments. An illustrative experiment showcases the challenges of aligning policy recommendations with stakeholder interests, highlighting the complexities of interdisciplinary collaboration. The user's experience as a researcher emphasizes the struggle to balance technical expertise with political influence, revealing the nuances of decision-making processes. Despite obstacles, the system shows promise in facilitating alternative narrative framing strategies and promoting deeper understanding of socio-ecological dynamics. Refinements to the system and protocol can enhance its use in exploring diverse stakeholder perspectives. 

Summary: <div>
arXiv:2507.17680v1 Announce Type: new 
Abstract: Understanding socio-ecological systems requires insights from diverse stakeholder perspectives, which are often hard to access. To enable alternative, simulation-based exploration of different stakeholder perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting) modelling framework. HoPeS employs agents powered by large language models (LLMs) to represent various stakeholders; users can step into the agent roles to experience perspectival differences. A simulation protocol serves as a "scaffold" to streamline multiple perspective-taking simulations, supporting users in reflecting on, transitioning between, and integrating across perspectives. A prototype system is developed to demonstrate HoPeS in the context of institutional dynamics and land use change, enabling both narrative-driven and numerical experiments. In an illustrative experiment, a user successively adopts the perspectives of a system observer and a researcher - a role that analyses data from the embedded land use model to inform evidence-based decision-making for other LLM agents representing various institutions. Despite the user's effort to recommend technically sound policies, discrepancies persist between the policy recommendation and implementation due to stakeholders' competing advocacies, mirroring real-world misalignment between researcher and policymaker perspectives. The user's reflection highlights the subjective feelings of frustration and disappointment as a researcher, especially due to the challenge of maintaining political neutrality while attempting to gain political influence. Despite this, the user exhibits high motivation to experiment with alternative narrative framing strategies, suggesting the system's potential in exploring different perspectives. Further system and protocol refinement are likely to enable new forms of interdisciplinary collaboration in socio-ecological simulations.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks</title>
<link>https://arxiv.org/abs/2507.17695</link>
<guid>https://arxiv.org/abs/2507.17695</guid>
<content:encoded><![CDATA[
<div> Trustworthy AI, Large Language Model, artificial general intelligence, Radio Access Network, Service-Level Agreements

Summary:<br>
- The paper discusses the role of Large Language Model (LLM)-based autonomous agents in 6G networks, transitioning from specialized intelligence to artificial general intelligence (AGI). <br>
- Introduces a novel agentic paradigm combining LLMs and real-time optimization algorithms for Trustworthy AI. <br>
- Proposes Radio Access Network optimizers and multi-agent negotiators for Service-Level Agreements (SLAs) as novel agent types. <br>
- Presents an end-to-end architecture for AGI networks evaluated on a 5G testbed, showcasing the effectiveness of symbiotic agents in reducing decision errors and resource overhead. <br>
- Demonstrates collaborative RAN on the testbed, highlighting flexibility in SLAs and resource allocation. <br> <div>
arXiv:2507.17695v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a vital role in the evolution of 6G networks, by empowering real-time decision-making related to management and service provisioning to end-users. This shift facilitates the transition from a specialized intelligence approach, where artificial intelligence (AI) algorithms handle isolated tasks, to artificial general intelligence (AGI)-driven networks, where agents possess broader reasoning capabilities and can manage diverse network functions. In this paper, we introduce a novel agentic paradigm that combines LLMs with real-time optimization algorithms towards Trustworthy AI, defined as symbiotic agents. Optimizers at the LLM's input-level provide bounded uncertainty steering for numerically precise tasks, whereas output-level optimizers supervised by the LLM enable adaptive real-time control. We design and implement two novel agent types including: (i) Radio Access Network optimizers, and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We further propose an end-to-end architecture for AGI networks and evaluate it on a 5G testbed capturing channel fluctuations from moving vehicles. Results show that symbiotic agents reduce decision errors fivefold compared to standalone LLM-based agents, while smaller language models (SLM) achieve similar accuracy with a 99.9% reduction in GPU resource overhead and in near-real-time loops of 82 ms. A multi-agent demonstration for collaborative RAN on the real-world testbed highlights significant flexibility in service-level agreement and resource allocation, reducing RAN over-utilization by approximately 44%. Drawing on our findings and open-source implementations, we introduce the symbiotic paradigm as the foundation for next-generation, AGI-driven networks-systems designed to remain adaptable, efficient, and trustworthy even as LLMs advance.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations</title>
<link>https://arxiv.org/abs/2507.17699</link>
<guid>https://arxiv.org/abs/2507.17699</guid>
<content:encoded><![CDATA[
<div> LRMs, reasoning models, large language models, complex reasoning tasks, empirical studies
Summary: 
- The study focuses on Large Reasoning Models (LRMs) in language model research.
- Recent empirical studies question the effectiveness of LRMs in enhancing reasoning ability.
- The study investigates the impact of tool augmentations, specifically Python interpreters and scratchpads, on LRMs.
- Results show that with proper tool use, LRMs consistently outperform non-reasoning counterparts on complex problems.
- The findings challenge the notion that reasoning in LRMs is ineffective and highlight the potential of tool-augmented LRMs for solving complex tasks.<br><br>Summary: <div>
arXiv:2507.17699v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have become a central focus in today's large language model (LLM) research, where models are designed to output a step-by-step thinking process before arriving at a final answer to handle complex reasoning tasks. Despite their promise, recent empirical studies (e.g., [Shojaee et al., 2025] from Apple) suggest that this thinking process may not actually enhance reasoning ability, where LLMs without explicit reasoning actually outperform LRMs on tasks with low or high complexity. In this work, we revisit these findings and investigate whether the limitations of LRMs persist when tool augmentations are introduced. We incorporate two types of tools, Python interpreters and scratchpads, and evaluate three representative LLMs and their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show that, with proper tool use, LRMs consistently outperform their non-reasoning counterparts across all levels of task complexity. These findings challenge the recent narrative that reasoning is an illusion and highlight the potential of tool-augmented LRMs for solving complex problems.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Submission and Evaluation System Design for Competition Operations</title>
<link>https://arxiv.org/abs/2507.17730</link>
<guid>https://arxiv.org/abs/2507.17730</guid>
<content:encoded><![CDATA[
<div> Keywords: benchmark datasets, research communities, competitions, online competition system, evaluation process

Summary:
The paper discusses the challenges faced by research communities in tracking progress and comparing algorithm performance due to publications in various venues claiming to be state-of-the-art. To address this, periodic competitions are organized to evaluate algorithm performance and track advancements. However, managing and evaluating a large volume of submissions presents an operational burden on organizers. Compatibility issues also arise during submission evaluation as participants develop solutions in diverse environments. The paper introduces an online competition system that automates the submission and evaluation process, allowing organizers to efficiently manage large submissions and utilize isolated environments for evaluation. The system has been successfully used in competitions such as the Grid-Based Pathfinding Competition and the League of Robot Runners competition. 

<br><br>Summary: <div>
arXiv:2507.17730v1 Announce Type: new 
Abstract: Research communities have developed benchmark datasets across domains to compare the performance of algorithms and techniques However, tracking the progress in these research areas is not easy, as publications appear in different venues at the same time, and many of them claim to represent the state-of-the-art. To address this, research communities often organise periodic competitions to evaluate the performance of various algorithms and techniques, thereby tracking advancements in the field. However, these competitions pose a significant operational burden. The organisers must manage and evaluate a large volume of submissions. Furthermore, participants typically develop their solutions in diverse environments, leading to compatibility issues during the evaluation of their submissions. This paper presents an online competition system that automates the submission and evaluation process for a competition. The competition system allows organisers to manage large numbers of submissions efficiently, utilising isolated environments to evaluate submissions. This system has already been used successfully for several competitions, including the Grid-Based Pathfinding Competition and the League of Robot Runners competition.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach</title>
<link>https://arxiv.org/abs/2507.10330</link>
<guid>https://arxiv.org/abs/2507.10330</guid>
<content:encoded><![CDATA[
<div> Regularization, Natural Language Processing, Adversarial Attacks, Growth Bound Matrices, Robustness <br>
Summary: <br>
This paper introduces a novel regularization technique using Growth Bound Matrices (GBM) to enhance the robustness of NLP models against adversarial attacks, particularly word substitution. The focus is on LSTM, S4, and CNN architectures, aiming to improve model resilience, generalization on clean text, and provide insights into S4 robustness. Experimental results show that the GBM method boosts adversarial robustness by up to 8.8% compared to existing approaches, outperforming state-of-the-art methods in adversarial defense. The code is available on GitHub for further exploration and implementation. <div>
arXiv:2507.10330v1 Announce Type: cross 
Abstract: Despite advancements in Natural Language Processing (NLP), models remain vulnerable to adversarial attacks, such as synonym substitutions. While prior work has focused on improving robustness for feed-forward and convolutional architectures, the robustness of recurrent networks and modern state space models (SSMs), such as S4, remains understudied. These architectures pose unique challenges due to their sequential processing and complex parameter dynamics. In this paper, we introduce a novel regularization technique based on Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the impact of input perturbations on model outputs. We focus on computing the GBM for three architectures: Long Short-Term Memory (LSTM), State Space models (S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance resilience against word substitution attacks, (2) improve generalization on clean text, and (3) providing the first systematic analysis of SSM (S4) robustness. Extensive experiments across multiple architectures and benchmark datasets demonstrate that our method improves adversarial robustness by up to 8.8% over existing baselines. These results highlight the effectiveness of our approach, outperforming several state-of-the-art methods in adversarial defense. Codes are available at https://github.com/BouriMohammed/GBM
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks</title>
<link>https://arxiv.org/abs/2507.16540</link>
<guid>https://arxiv.org/abs/2507.16540</guid>
<content:encoded><![CDATA[
<div> framework, vulnerability detection, graph-based, class imbalance, explainable<br>
Summary:<br>
ExplainVulD is a graph-based framework for vulnerability detection in C/C++ code. It addresses the challenge of class imbalance in datasets by using dual-channel embeddings and an edge-aware attention mechanism. The model achieves a mean accuracy of 88.25 percent and an F1 score of 48.23 percent on the ReVeal dataset, outperforming previous learning-based methods and static analysis tools. ExplainVulD produces explainable outputs by identifying influential code regions within functions, promoting transparency and trust in security triage. <div>
arXiv:2507.16540v1 Announce Type: cross 
Abstract: Detecting security vulnerabilities in source code remains challenging, particularly due to class imbalance in real-world datasets where vulnerable functions are under-represented. Existing learning-based methods often optimise for recall, leading to high false positive rates and reduced usability in development workflows. Furthermore, many approaches lack explainability, limiting their integration into security workflows. This paper presents ExplainVulD, a graph-based framework for vulnerability detection in C/C++ code. The method constructs Code Property Graphs and represents nodes using dual-channel embeddings that capture both semantic and structural information. These are processed by an edge-aware attention mechanism that incorporates edge-type embeddings to distinguish among program relations. To address class imbalance, the model is trained using class-weighted cross-entropy loss. ExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23 percent across 30 independent runs on the ReVeal dataset. These results represent relative improvements of 4.6 percent in accuracy and 16.9 percent in F1 score compared to the ReVeal model, a prior learning-based method. The framework also outperforms static analysis tools, with relative gains of 14.0 to 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond improved detection performance, ExplainVulD produces explainable outputs by identifying the most influential code regions within each function, supporting transparency and trust in security triage.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disaster Informatics after the COVID-19 Pandemic: Bibliometric and Topic Analysis based on Large-scale Academic Literature</title>
<link>https://arxiv.org/abs/2507.16820</link>
<guid>https://arxiv.org/abs/2507.16820</guid>
<content:encoded><![CDATA[
<div> countries, institutions, authors, collaboration, topics<br>
Summary:<br>
1. Countries most impacted by COVID-19 were highly active in disaster informatics research, each with specific interests.<br>
2. Countries and institutions in the same region or sharing a common language tend to collaborate.<br>
3. Top authors form close partnerships with one or two key partners, specializing in specific topics.<br>
4. Authors focus on one or two topics, while institutions have diverse interests across several topics.<br>
5. The COVID-19 pandemic has shifted research priorities in disaster informatics towards public health, emphasizing multidimensional resilience strategies and cross-sectoral data-sharing collaborations.<br> <div>
arXiv:2507.16820v1 Announce Type: cross 
Abstract: This study presents a comprehensive bibliometric and topic analysis of the disaster informatics literature published between January 2020 to September 2022. Leveraging a large-scale corpus and advanced techniques such as pre-trained language models and generative AI, we identify the most active countries, institutions, authors, collaboration networks, emergent topics, patterns among the most significant topics, and shifts in research priorities spurred by the COVID-19 pandemic. Our findings highlight (1) countries that were most impacted by the COVID-19 pandemic were also among the most active, with each country having specific research interests, (2) countries and institutions within the same region or share a common language tend to collaborate, (3) top active authors tend to form close partnerships with one or two key partners, (4) authors typically specialized in one or two specific topics, while institutions had more diverse interests across several topics, and (5) the COVID-19 pandemic has influenced research priorities in disaster informatics, placing greater emphasis on public health. We further demonstrate that the field is converging on multidimensional resilience strategies and cross-sectoral data-sharing collaborations or projects, reflecting a heightened awareness of global vulnerability and interdependency. Collecting and quality assurance strategies, data analytic practices, LLM-based topic extraction and summarization approaches, and result visualization tools can be applied to comparable datasets or solve similar analytic problems. By mapping out the trends in disaster informatics, our analysis offers strategic insights for policymakers, practitioners, and scholars aiming to enhance disaster informatics capacities in an increasingly uncertain and complex risk landscape.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval-Augmented Generation in Large Language Models</title>
<link>https://arxiv.org/abs/2507.16826</link>
<guid>https://arxiv.org/abs/2507.16826</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval Augmented Generation, Knowledge Graph, Query-Aware, Semantic Relevance, Multi-Path

Summary: 
- The study introduces QMKGF, a Query-Aware Multi-Path Knowledge Graph Fusion Approach for enhancing Retrieval Augmented Generation (RAG).
- Prompt templates and general-purpose large language models are utilized to extract entities and relations and generate a knowledge graph efficiently.
- A multi-path subgraph construction strategy is implemented, including one-hop relations, multi-hop relations, and importance-based relations to improve semantic relevance between retrieved documents and user queries.
- A query-aware attention reward model is designed to score subgraph triples based on their semantic relevance to the query.
- The method outperforms existing approaches on datasets like HotpotQA, achieving a ROUGE-1 score of 64.98\% and demonstrating the effectiveness and superiority of the QMKGF approach.

<br><br>Summary: <div>
arXiv:2507.16826v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) has gradually emerged as a promising paradigm for enhancing the accuracy and factual consistency of content generated by large language models (LLMs). However, existing RAG studies primarily focus on retrieving isolated segments using similarity-based matching methods, while overlooking the intrinsic connections between them. This limitation hampers performance in RAG tasks. To address this, we propose QMKGF, a Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval Augmented Generation. First, we design prompt templates and employ general-purpose LLMs to extract entities and relations, thereby generating a knowledge graph (KG) efficiently. Based on the constructed KG, we introduce a multi-path subgraph construction strategy that incorporates one-hop relations, multi-hop relations, and importance-based relations, aiming to improve the semantic relevance between the retrieved documents and the user query. Subsequently, we designed a query-aware attention reward model that scores subgraph triples based on their semantic relevance to the query. Then, we select the highest score subgraph and enrich subgraph with additional triples from other subgraphs that are highly semantically relevant to the query. Finally, the entities, relations, and triples within the updated subgraph are utilised to expand the original query, thereby enhancing its semantic representation and improving the quality of LLMs' generation. We evaluate QMKGF on the SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets. On the HotpotQA dataset, our method achieves a ROUGE-1 score of 64.98\%, surpassing the BGE-Rerank approach by 9.72 percentage points (from 55.26\% to 64.98\%). Experimental results demonstrate the effectiveness and superiority of the QMKGF approach.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Don't Bring Me Flowers: Mitigating Unwanted Recommendations Through Conformal Risk Control</title>
<link>https://arxiv.org/abs/2507.16829</link>
<guid>https://arxiv.org/abs/2507.16829</guid>
<content:encoded><![CDATA[
<div> method, conformal risk control, personalized recommendations, unwanted content, feedback
Summary:
The paper introduces a model-agnostic method leveraging conformal risk control to limit unwanted content in personalized recommendations. It addresses issues of irrelevant and harmful recommendations on online platforms, aiming to improve user satisfaction and combat societal problems like misinformation and radicalization. The approach uses simple binary feedback on items to control the risk of undesirable recommendations effectively. Additionally, it overcomes a limitation of traditional methods by incorporating implicit feedback on consumed items to expand recommendation sets while maintaining robust risk mitigation. Experimental results on data from a popular online video-sharing platform demonstrate the effectiveness and ease of implementation of this approach, showcasing a significant reduction in unwanted recommendations. The source code for the method is available on GitHub for further exploration and implementation. 

<br><br>Summary: <div>
arXiv:2507.16829v1 Announce Type: cross 
Abstract: Recommenders are significantly shaping online information consumption. While effective at personalizing content, these systems increasingly face criticism for propagating irrelevant, unwanted, and even harmful recommendations. Such content degrades user satisfaction and contributes to significant societal issues, including misinformation, radicalization, and erosion of user trust. Although platforms offer mechanisms to mitigate exposure to undesired content, these mechanisms are often insufficiently effective and slow to adapt to users' feedback. This paper introduces an intuitive, model-agnostic, and distribution-free method that uses conformal risk control to provably bound unwanted content in personalized recommendations by leveraging simple binary feedback on items. We also address a limitation of traditional conformal risk control approaches, i.e., the fact that the recommender can provide a smaller set of recommended items, by leveraging implicit feedback on consumed items to expand the recommendation set while ensuring robust risk mitigation. Our experimental evaluation on data coming from a popular online video-sharing platform demonstrates that our approach ensures an effective and controllable reduction of unwanted recommendations with minimal effort. The source code is available here: https://github.com/geektoni/mitigating-harm-recsys.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Speech Recognition for Jamaican Patois Music Transcription</title>
<link>https://arxiv.org/abs/2507.16834</link>
<guid>https://arxiv.org/abs/2507.16834</guid>
<content:encoded><![CDATA[
<div> Keywords: Jamaican Patois, speech recognition, music, data-centric approach, language modeling

Summary: 
This study addresses the issue of poor performance of current speech recognition systems on Jamaican Patois music, hindering accessibility and downstream applications. By curating over 40 hours of manually transcribed Patois music, the researchers fine-tuned automatic speech recognition (ASR) models, leading to the development of scaling laws for Whisper models on Jamaican Patois audio. The data-centric approach taken in this work aims to improve the accuracy of captions for Patois music, making it more accessible and beneficial for downstream applications. The utilization of a large dataset for training ASR models and the resulting performance improvements could have a significant impact on the accessibility of Jamaican Patois music and the future of Jamaican Patois language modeling.<br><br>Summary: <div>
arXiv:2507.16834v1 Announce Type: cross 
Abstract: Although Jamaican Patois is a widely spoken language, current speech recognition systems perform poorly on Patois music, producing inaccurate captions that limit accessibility and hinder downstream applications. In this work, we take a data-centric approach to this problem by curating more than 40 hours of manually transcribed Patois music. We use this dataset to fine-tune state-of-the-art automatic speech recognition (ASR) models, and use the results to develop scaling laws for the performance of Whisper models on Jamaican Patois audio. We hope that this work will have a positive impact on the accessibility of Jamaican Patois music and the future of Jamaican Patois language modeling.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segmentation-free Goodness of Pronunciation</title>
<link>https://arxiv.org/abs/2507.16838</link>
<guid>https://arxiv.org/abs/2507.16838</guid>
<content:encoded><![CDATA[
<div> Alignment-free GOP, CTC-trained ASR models, phoneme-level pronunciation assessment, mispronunciation detection, computer aided language learning 

Summary:
The study focuses on mispronunciation detection and diagnosis (MDD) in computer aided language learning systems. Current systems use goodness of pronunciation (GOP) based on segmented speech, limiting accuracy and the use of modern CTC-trained ASR models. The researchers propose self-alignment GOP (GOP-SA) and alignment-free GOP (GOP-AF) to address these limitations. GOP-AF considers all possible alignments of the target phoneme, with a theoretical account, implementation, and normalization for different acoustic models. Experimental results on CMU Kids and Speechocean762 datasets show the effectiveness of the proposed methods, particularly with the peakiness of acoustic models and context around the target phoneme. Comparison with previous studies on Speechocean762 data demonstrates state-of-the-art results in phoneme-level pronunciation assessment using the proposed feature vectors. <div>
arXiv:2507.16838v1 Announce Type: cross 
Abstract: Mispronunciation detection and diagnosis (MDD) is a significant part in modern computer aided language learning (CALL) systems. Within MDD, phoneme-level pronunciation assessment is key to helping L2 learners improve their pronunciation. However, most systems are based on a form of goodness of pronunciation (GOP) which requires pre-segmentation of speech into phonetic units. This limits the accuracy of these methods and the possibility to use modern CTC-based acoustic models for their evaluation. In this study, we first propose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR models for MDD. Next, we define a more general alignment-free method that takes all possible alignments of the target phoneme into account (GOP-AF). We give a theoretical account of our definition of GOP-AF, an implementation that solves potential numerical issues as well as a proper normalization which makes the method applicable with acoustic models with different peakiness over time. We provide extensive experimental results on the CMU Kids and Speechocean762 datasets comparing the different definitions of our methods, estimating the dependency of GOP-AF on the peakiness of the acoustic models and on the amount of context around the target phoneme. Finally, we compare our methods with recent studies over the Speechocean762 data showing that the feature vectors derived from the proposed method achieve state-of-the-art results on phoneme-level pronunciation assessment.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASPER: Contrastive Approach for Smart Ponzi Scheme Detecter with More Negative Samples</title>
<link>https://arxiv.org/abs/2507.16840</link>
<guid>https://arxiv.org/abs/2507.16840</guid>
<content:encoded><![CDATA[
<div> contrastive learning, smart Ponzi scheme detection, blockchain transactions, CASPER, XBlock dataset 

Summary: 
CASPER, a novel contrastive learning framework, is proposed for enhancing the detection of smart Ponzi schemes in blockchain transactions. Unlike traditional deep learning methods that require large amounts of labeled data for model training, CASPER uses contrastive learning techniques to learn effective representations of smart contract source code using unlabeled datasets. In experiments with the XBlock dataset, CASPER outperformed the baseline model by 2.3% in F1 score when trained with 100% labeled data. Remarkably, even with only 25% labeled data, CASPER achieved an F1 score nearly 20% higher than the baseline. These results demonstrate the potential of CASPER for cost-efficient and scalable fraud detection in digital currency trading, helping to address the emergence of smart Ponzi schemes in the evolving landscape of blockchain technology. 

<br><br>Summary: <div>
arXiv:2507.16840v1 Announce Type: cross 
Abstract: The rapid evolution of digital currency trading, fueled by the integration of blockchain technology, has led to both innovation and the emergence of smart Ponzi schemes. A smart Ponzi scheme is a fraudulent investment operation in smart contract that uses funds from new investors to pay returns to earlier investors. Traditional Ponzi scheme detection methods based on deep learning typically rely on fully supervised models, which require large amounts of labeled data. However, such data is often scarce, hindering effective model training. To address this challenge, we propose a novel contrastive learning framework, CASPER (Contrastive Approach for Smart Ponzi detectER with more negative samples), designed to enhance smart Ponzi scheme detection in blockchain transactions. By leveraging contrastive learning techniques, CASPER can learn more effective representations of smart contract source code using unlabeled datasets, significantly reducing both operational costs and system complexity. We evaluate CASPER on the XBlock dataset, where it outperforms the baseline by 2.3% in F1 score when trained with 100% labeled data. More impressively, with only 25% labeled data, CASPER achieves an F1 score nearly 20% higher than the baseline under identical experimental conditions. These results highlight CASPER's potential for effective and cost-efficient detection of smart Ponzi schemes, paving the way for scalable fraud detection solutions in the future.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak Supervision Techniques towards Enhanced ASR Models in Industry-level CRM Systems</title>
<link>https://arxiv.org/abs/2507.16843</link>
<guid>https://arxiv.org/abs/2507.16843</guid>
<content:encoded><![CDATA[
<div> Keywords: customer relationship management, automatic speech recognition, fine-tuning, personalized services, industry applications

Summary:
- Customer relationship management (CRM) systems rely on accurately identifying customer types and offering personalized services to enhance satisfaction and loyalty.
- Traditional automatic speech recognition (ASR) models do not effectively address industry-specific speech recognition tasks.
- Proposed solution involves fine-tuning industry-specific ASR models to better recognize customer voices and intentions.
- Experimental results demonstrate significant performance improvements of fine-tuned ASR models in industry CRM applications.
- The approach has been successfully implemented in real industrial settings, highlighting its practical relevance and impact.<br><br>Summary: <div>
arXiv:2507.16843v1 Announce Type: cross 
Abstract: In the design of customer relationship management (CRM) systems, accurately identifying customer types and offering personalized services are key to enhancing customer satisfaction and loyalty. However, this process faces the challenge of discerning customer voices and intentions, and general pre-trained automatic speech recognition (ASR) models make it difficult to effectively address industry-specific speech recognition tasks. To address this issue, we innovatively proposed a solution for fine-tuning industry-specific ASR models, which significantly improved the performance of the fine-tuned ASR models in industry applications. Experimental results show that our method substantially improves the crucial auxiliary role of the ASR model in industry CRM systems, and this approach has also been adopted in actual industrial applications.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Simulation Framework for Disinformation Dissemination and Correction With Social Bots</title>
<link>https://arxiv.org/abs/2507.16848</link>
<guid>https://arxiv.org/abs/2507.16848</guid>
<content:encoded><![CDATA[
<div> Keywords: social bots, disinformation dissemination, correction strategies, network modeling, symbiotic information ecosystem

Summary: 
The study focuses on analyzing the influence of social bots in disseminating and correcting disinformation in the human-bot information ecosystem. By developing a Multi Agent based framework for Disinformation Dissemination (MADD), the researchers aim to create a more realistic propagation network that integrates scale free topology and community structures. This framework incorporates both malicious and legitimate bots with controlled dynamic participation, allowing for quantitative evaluation of correction strategies. MADD is evaluated using individual and group level metrics, demonstrating its real-world consistency in user attributes and network structure. By simulating the dissemination of six disinformation topics, the study highlights the differential effects of fact-based and narrative-based correction strategies. MADD provides insights into understanding the role of social bots and offers a basis for risk control and improved governance in combating disinformation. 

<br><br>Summary: <div>
arXiv:2507.16848v1 Announce Type: cross 
Abstract: In the human-bot symbiotic information ecosystem, social bots play key roles in spreading and correcting disinformation. Understanding their influence is essential for risk control and better governance. However, current studies often rely on simplistic user and network modeling, overlook the dynamic behavior of bots, and lack quantitative evaluation of correction strategies. To fill these gaps, we propose MADD, a Multi Agent based framework for Disinformation Dissemination. MADD constructs a more realistic propagation network by integrating the Barabasi Albert Model for scale free topology and the Stochastic Block Model for community structures, while designing node attributes based on real world user data. Furthermore, MADD incorporates both malicious and legitimate bots, with their controlled dynamic participation allows for quantitative analysis of correction strategies. We evaluate MADD using individual and group level metrics. We experimentally verify the real world consistency of MADD user attributes and network structure, and we simulate the dissemination of six disinformation topics, demonstrating the differential effects of fact based and narrative based correction strategies.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery</title>
<link>https://arxiv.org/abs/2507.16849</link>
<guid>https://arxiv.org/abs/2507.16849</guid>
<content:encoded><![CDATA[
<div> Keywords: vision transformer, deep learning, disaster-affected area segmentation, remote sensing imagery, weakly supervised training

Summary:
The study introduces a vision transformer (ViT)-based deep learning framework for refining disaster-affected area segmentation from remote sensing imagery to support the Emergent Value Added Product (EVAP) developed by the Taiwan Space Agency (TASA). The framework utilizes a small set of manually annotated regions and applies principal component analysis (PCA) to construct a confidence index (CI) to expand labels for weakly supervised training. ViT-based encoder-decoder models are trained using multi-band inputs from Sentinel-2 and Formosat-5 imagery, with support for multiple decoder variants and loss strategies to enhance performance with limited supervision. Evaluation against higher-resolution EVAP output shows improved spatial coherence and segmentation consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes wildfire demonstrate the framework's effectiveness in producing more reliable and smooth segmentation results, offering a scalable solution for disaster mapping in the absence of accurate ground truth data.<br><br>Summary: <div>
arXiv:2507.16849v1 Announce Type: cross 
Abstract: We propose a vision transformer (ViT)-based deep learning framework to refine disaster-affected area segmentation from remote sensing imagery, aiming to support and enhance the Emergent Value Added Product (EVAP) developed by the Taiwan Space Agency (TASA). The process starts with a small set of manually annotated regions. We then apply principal component analysis (PCA)-based feature space analysis and construct a confidence index (CI) to expand these labels, producing a weakly supervised training set. These expanded labels are then used to train ViT-based encoder-decoder models with multi-band inputs from Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder variants and multi-stage loss strategies to improve performance under limited supervision. During the evaluation, model predictions are compared with higher-resolution EVAP output to assess spatial coherence and segmentation consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes wildfire demonstrate that our framework improves the smoothness and reliability of segmentation results, offering a scalable approach for disaster mapping when accurate ground truth is unavailable.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors</title>
<link>https://arxiv.org/abs/2507.16850</link>
<guid>https://arxiv.org/abs/2507.16850</guid>
<content:encoded><![CDATA[
<div> 2D keypoint detection, 2D-to-3D lifting, camera intrinsics, anatomical priors, inverse kinematics <br>
Summary: 
This article presents a framework for real-time 3D human pose estimation from monocular images, utilizing a combination of 2D keypoint detection and geometry-aware 2D-to-3D lifting. The approach incorporates known camera intrinsics and subject-specific anatomical priors to improve accuracy. By generating training pairs from motion capture and synthetic datasets, the framework enables personalized, fast, and accurate 3D pose estimation without the need for specialized hardware. The method leverages self-calibration and biomechanically-constrained inverse kinematics to enhance the generation of plausible 2D-3D data. The proposed approach aims to bridge data-driven learning with model-based priors to enhance the interpretability, accuracy, and deployability of 3D human motion capture on edge devices in real-world settings.<br> 
Summary: <div>
arXiv:2507.16850v1 Announce Type: cross 
Abstract: Monocular 3D human pose estimation remains a challenging and ill-posed problem, particularly in real-time settings and unconstrained environments. While direct imageto-3D approaches require large annotated datasets and heavy models, 2D-to-3D lifting offers a more lightweight and flexible alternative-especially when enhanced with prior knowledge. In this work, we propose a framework that combines real-time 2D keypoint detection with geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics and subject-specific anatomical priors. Our approach builds on recent advances in self-calibration and biomechanically-constrained inverse kinematics to generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic datasets. We discuss how these ingredients can enable fast, personalized, and accurate 3D pose estimation from monocular images without requiring specialized hardware. This proposal aims to foster discussion on bridging data-driven learning and model-based priors to improve accuracy, interpretability, and deployability of 3D human motion capture on edge devices in the wild.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynthCTI: LLM-Driven Synthetic CTI Generation to enhance MITRE Technique Mapping</title>
<link>https://arxiv.org/abs/2507.16852</link>
<guid>https://arxiv.org/abs/2507.16852</guid>
<content:encoded><![CDATA[
<div> data augmentation, cyber threat intelligence, MITRE ATT&CK, Large Language Models, synthetic data generation

Summary:
SynthCTI is a data augmentation framework designed to address the challenges of mapping threat descriptions to MITRE ATT&CK techniques in Cyber Threat Intelligence (CTI) mining. It utilizes a clustering-based strategy to generate high-quality synthetic CTI sentences for underrepresented techniques. By guiding Large Language Models (LLMs) in producing lexically diverse and semantically faithful synthetic data, SynthCTI enhances model performance on two publicly available CTI datasets, CTI-to-MITRE and TRAM. Results show consistent macro-F1 improvements, with models such as ALBERT and SecureBERT significantly boosting their performance when augmented with synthetic data. Notably, smaller models augmented with SynthCTI outperform larger models trained without augmentation, highlighting the importance of data generation methods in building efficient and effective CTI classification systems. <div>
arXiv:2507.16852v1 Announce Type: cross 
Abstract: Cyber Threat Intelligence (CTI) mining involves extracting structured insights from unstructured threat data, enabling organizations to understand and respond to evolving adversarial behavior. A key task in CTI mining is mapping threat descriptions to MITRE ATT\&amp;CK techniques. However, this process is often performed manually, requiring expert knowledge and substantial effort. Automated approaches face two major challenges: the scarcity of high-quality labeled CTI data and class imbalance, where many techniques have very few examples. While domain-specific Large Language Models (LLMs) such as SecureBERT have shown improved performance, most recent work focuses on model architecture rather than addressing the data limitations. In this work, we present SynthCTI, a data augmentation framework designed to generate high-quality synthetic CTI sentences for underrepresented MITRE ATT\&amp;CK techniques. Our method uses a clustering-based strategy to extract semantic context from training data and guide an LLM in producing synthetic CTI sentences that are lexically diverse and semantically faithful. We evaluate SynthCTI on two publicly available CTI datasets, CTI-to-MITRE and TRAM, using LLMs with different capacity. Incorporating synthetic data leads to consistent macro-F1 improvements: for example, ALBERT improves from 0.35 to 0.52 (a relative gain of 48.6\%), and SecureBERT reaches 0.6558 (up from 0.4412). Notably, smaller models augmented with SynthCTI outperform larger models trained without augmentation, demonstrating the value of data generation methods for building efficient and effective CTI classification systems.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2507.16854</link>
<guid>https://arxiv.org/abs/2507.16854</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal aspect-based sentiment analysis, Contrastive Learning, Progressive Attention Fusion, Adaptive Multi-loss, Fine-grained representations

Summary: 
The paper introduces a framework called CLAMP for Multimodal Aspect-Based Sentiment Analysis (MABSA). CLAMP addresses challenges like cross-modal alignment noise and inconsistent fine-grained representations. The framework consists of three modules: Progressive Attention Fusion network, Multi-task Contrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive Attention Fusion network enhances alignment between textual features and image regions by hierarchical cross-modal interactions, reducing irrelevant visual noise. Multi-task contrastive learning combines global modal contrast and local granularity alignment for better cross-modal representation consistency. Adaptive Multi-loss Aggregation uses a dynamic uncertainty-based weighting mechanism to calibrate loss contributions according to task uncertainty, minimizing gradient interference. Experimental results show that CLAMP outperforms existing state-of-the-art methods on standard benchmarks. <div>
arXiv:2507.16854v1 Announce Type: cross 
Abstract: Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect terms within paired image-text data and determine their fine grained sentiment polarities, representing a fundamental task for improving the effectiveness of applications such as product review systems and public opinion monitoring. Existing methods face challenges such as cross modal alignment noise and insufficient consistency in fine-grained representations. While global modality alignment methods often overlook the connection between aspect terms and their corresponding local visual regions, bridging the representation gap between text and images remains a challenge. To address these limitations, this paper introduces an end to end Contrastive Learning framework with Adaptive Multi-loss and Progressive Attention Fusion(CLAMP). The framework is composed of three novel modules: Progressive Attention Fusion network, Multi-task Contrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive Attention Fusion network enhances fine-grained alignment between textual features and image regions via hierarchical, multi-stage cross modal interactions, effectively suppressing irrelevant visual noise. Secondly, multi-task contrastive learning combines global modal contrast and local granularity alignment to enhance cross modal representation consistency. Adaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting mechanism to calibrate loss contributions according to each task's uncertainty, thereby mitigating gradient interference. Evaluation on standard public benchmarks demonstrates that CLAMP consistently outperforms the vast majority of existing state of the art methods.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIA: Enhancing Safety via Intent Awareness for Vision-Language Models</title>
<link>https://arxiv.org/abs/2507.16856</link>
<guid>https://arxiv.org/abs/2507.16856</guid>
<content:encoded><![CDATA[
<div> captioning, intent inference, response refinement, safety improvements, intent-aware reasoning 
Summary: 
SIA (Safety via Intent Awareness) is a training-free framework designed to detect and mitigate harmful intent in vision-language models. It operates through a three-stage reasoning process: visual abstraction via captioning, intent inference using few-shot chain-of-thought prompting, and intent-conditioned response refinement. SIA dynamically adapts to implicit intent inferred from image-text pairs, rather than relying on predefined rules or classifiers. Extensive experiments on benchmarks like SIUO, MM-SafetyBench, and HoliSafe show that SIA significantly enhances safety, outperforming previous methods. While SIA may result in a slight decrease in general reasoning accuracy on MMStar, the safety enhancements underscore the importance of intent-aware reasoning in aligning vision-language models with human values. <div>
arXiv:2507.16856v1 Announce Type: cross 
Abstract: As vision-language models (VLMs) are increasingly deployed in real-world applications, new safety risks arise from the subtle interplay between images and text. In particular, seemingly innocuous inputs can combine to reveal harmful intent, leading to unsafe model responses. Despite increasing attention to multimodal safety, previous approaches based on post hoc filtering or static refusal prompts struggle to detect such latent risks, especially when harmfulness emerges only from the combination of inputs. We propose SIA (Safety via Intent Awareness), a training-free prompt engineering framework that proactively detects and mitigates harmful intent in multimodal inputs. SIA employs a three-stage reasoning process: (1) visual abstraction via captioning, (2) intent inference through few-shot chain-of-thought prompting, and (3) intent-conditioned response refinement. Rather than relying on predefined rules or classifiers, SIA dynamically adapts to the implicit intent inferred from the image-text pair. Through extensive experiments on safety-critical benchmarks including SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves substantial safety improvements, outperforming prior methods. Although SIA shows a minor reduction in general reasoning accuracy on MMStar, the corresponding safety gains highlight the value of intent-aware reasoning in aligning VLMs with human-centric values.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging multi-source and heterogeneous signals for fatigue detection</title>
<link>https://arxiv.org/abs/2507.16859</link>
<guid>https://arxiv.org/abs/2507.16859</guid>
<content:encoded><![CDATA[
<div> Sensors, Fatigue detection, Real-world settings, Multi-source, Adaptation<br>
<br>
Summary:<br>
This paper addresses the challenge of fatigue detection in real-world settings, where high-end sensors are not always practical. The proposed framework leverages knowledge from diverse sources with varying sensor configurations to detect fatigue. By adaptively utilizing available modalities and incorporating data from different environments, the framework demonstrates practicality, robustness, and improved generalization in fatigue monitoring. The experiments conducted using a realistic field-deployed sensor setup and public datasets validate the effectiveness of the approach, paving the way for efficient fatigue monitoring in sensor-constrained scenarios. <div>
arXiv:2507.16859v1 Announce Type: cross 
Abstract: Fatigue detection plays a critical role in safety-critical applications such as aviation, mining, and long-haul transport. However, most existing methods rely on high-end sensors and controlled environments, limiting their applicability in real world settings. This paper formally defines a practical yet underexplored problem setting for real world fatigue detection, where systems operating with context-appropriate sensors aim to leverage knowledge from differently instrumented sources including those using impractical sensors deployed in controlled environments. To tackle this challenge, we propose a heterogeneous and multi-source fatigue detection framework that adaptively utilizes the available modalities in the target domain while benefiting from the diverse configurations present in source domains. Our experiments, conducted using a realistic field-deployed sensor setup and two publicly available datasets, demonstrate the practicality, robustness, and improved generalization of our approach, paving the practical way for effective fatigue monitoring in sensor-constrained scenarios.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection</title>
<link>https://arxiv.org/abs/2507.16861</link>
<guid>https://arxiv.org/abs/2507.16861</guid>
<content:encoded><![CDATA[
<div> Prior Guided Depth Calibration, 2D object priors, Discontinuity Aware Geometric Fusion, Structural Guidance Depth Modulator, autonomous vehicles <br>
Summary:
The article addresses the misalignment issues between LiDAR and camera inputs in Bird's-Eye-View representations for autonomous vehicles. It introduces Prior Guided Depth Calibration (PGDC) to correct local misalignments using 2D object priors, and Discontinuity Aware Geometric Fusion (DAGF) to handle global misalignments by enhancing object-background boundaries. The Structural Guidance Depth Modulator (SGDM) effectively fuses aligned depth and image features using a gated attention mechanism. The proposed method achieves a state-of-the-art performance on the nuScenes validation dataset, with mAP and NDS scores of 71.5% and 73.6% respectively. <br> <div>
arXiv:2507.16861v1 Announce Type: cross 
Abstract: Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV) representation is crucial for enhancing 3D perception capabilities of autonomous vehicles. However, current methods are often affected by misalignment between camera and LiDAR features. This misalignment leads to inaccurate depth supervision in camera branch and erroneous fusion during cross-modal feature aggregation. The root cause of this misalignment lies in projection errors, stemming from minor extrinsic calibration inaccuracies and rolling shutter effect of LiDAR during vehicle motion. In this work, our key insight is that these projection errors are predominantly concentrated at object-background boundaries, which are readily identified by 2D detectors. Based on this, our main motivation is to utilize 2D object priors to pre-align cross-modal features before fusion. To address local misalignment, we propose Prior Guided Depth Calibration (PGDC), which leverages 2D priors to correct local misalignment and preserve correct cross-modal feature pairs. To resolve global misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF) to process calibrated results from PGDC, suppressing noise and explicitly enhancing sharp transitions at object-background boundaries. To effectively utilize these transition-aware depth representations, we incorporate Structural Guidance Depth Modulator (SGDM), using a gated attention mechanism to efficiently fuse aligned depth and image features. Our proposed method achieves state-of-the-art performance on nuScenes validation dataset, with its mAP and NDS reaching 71.5% and 73.6% respectively.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixels, Patterns, but No Poetry: To See The World like Humans</title>
<link>https://arxiv.org/abs/2507.16863</link>
<guid>https://arxiv.org/abs/2507.16863</guid>
<content:encoded><![CDATA[
<div> perception, Multimodal Large Language Models, Turing Eye Test, synthetic images, vision tower<br>
<br>
Summary: 
This study addresses the challenge of achieving human-like perception in Multimodal Large Language Models (MLLMs). While previous research has focused on improving reasoning abilities, this paper introduces the Turing Eye Test (TET), a benchmark that evaluates MLLMs' performance on perceptual tasks based on synthetic images. The findings reveal that current MLLMs struggle with tasks that are easy for humans, indicating a gap in visual generalization. In contrast to previous benchmarks, in-context learning and training on the language backbone do not improve performance on TET tasks. Fine-tuning the vision tower, however, leads to rapid adaptation, highlighting the importance of visual processing in MLLMs. The study releases a subset of TET tasks and plans to introduce more challenging tasks to enhance visual generalization in future work. <br><br>Summary: <div>
arXiv:2507.16863v1 Announce Type: cross 
Abstract: Achieving human-like perception and reasoning in Multimodal Large Language Models (MLLMs) remains a central challenge in artificial intelligence. While recent research has primarily focused on enhancing reasoning capabilities in MLLMs, a fundamental question persists: Can Multimodal Large Language Models truly perceive the world as humans do? This paper shifts focus from reasoning to perception. Rather than constructing benchmarks specifically for reasoning, we introduce the Turing Eye Test (TET), a challenging perception-oriented benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on synthetic images that humans process intuitively. Our findings reveal that state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks trivial for humans. Both in-context learning and training on language backbone-effective for previous benchmarks-fail to improve performance on our tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting that our benchmark poses challenges for vision tower generalization rather than for the knowledge and reasoning capabilities of the language backbone-a key gap between current MLLMs and human perception. We release a representative subset of TET tasks in this version, and will introduce more diverse tasks and methods to enhance visual generalization in future work.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning in hyperbolic space for multi-step reasoning</title>
<link>https://arxiv.org/abs/2507.16864</link>
<guid>https://arxiv.org/abs/2507.16864</guid>
<content:encoded><![CDATA[
<div> Hyperbolic Transformers, Reinforcement Learning, multi-step reasoning, hierarchical structures, computational efficiency 
Summary:
Hyperbolic Transformers integrated into Reinforcement Learning offer a new framework for addressing multi-step reasoning challenges. The approach utilizes hyperbolic embeddings to effectively model hierarchical structures. The theoretical insights, algorithmic details, and experimental results showcase significant improvements in accuracy for tasks like Frontier Math and nonlinear optimal control. Compared to conventional RL methods, hyperbolic RL demonstrates enhanced performance, with accuracy increasing by 32-45% on benchmarks and notable reductions in computational time of 16-32%. This indicates the potential of hyperbolic Transformers in reinforcement learning, particularly for tasks involving complex reasoning and hierarchical structures. <br><br> <div>
arXiv:2507.16864v1 Announce Type: cross 
Abstract: Multi-step reasoning is a fundamental challenge in artificial intelligence, with applications ranging from mathematical problem-solving to decision-making in dynamic environments. Reinforcement Learning (RL) has shown promise in enabling agents to perform multi-step reasoning by optimizing long-term rewards. However, conventional RL methods struggle with complex reasoning tasks due to issues such as credit assignment, high-dimensional state representations, and stability concerns. Recent advancements in Transformer architectures and hyperbolic geometry have provided novel solutions to these challenges. This paper introduces a new framework that integrates hyperbolic Transformers into RL for multi-step reasoning. The proposed approach leverages hyperbolic embeddings to model hierarchical structures effectively. We present theoretical insights, algorithmic details, and experimental results that include Frontier Math and nonlinear optimal control problems. Compared to RL with vanilla transformer, the hyperbolic RL largely improves accuracy by (32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control benchmark, while achieving impressive reduction in computational time by (16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control benchmark. Our work demonstrates the potential of hyperbolic Transformers in reinforcement learning, particularly for multi-step reasoning tasks that involve hierarchical structures.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization</title>
<link>https://arxiv.org/abs/2507.16867</link>
<guid>https://arxiv.org/abs/2507.16867</guid>
<content:encoded><![CDATA[
<div> DiffCarl, diffusion-modeled carbon and risk-aware reinforcement learning algorithm, for multi-microgrid systems. Integration of renewables increases system complexity, requiring real-time energy optimization under uncertainty. DiffCarl combines diffusion model with deep reinforcement learning for adaptive energy scheduling and includes carbon emissions and operational risk in decision-making. Enhances policy expressiveness through denoising generation process, enabling carbon and risk-aware scheduling in dynamic microgrid environments. Outperforms classic algorithms and state-of-the-art DRL solutions, achieving 2.3-30.1% lower operational cost and 28.7% lower carbon emissions compared to carbon-unaware variant. Reduces performance variability and supports efficient adaptation to varied system configurations and objectives for real-world deployment in evolving energy systems. 

Keywords: DiffCarl, diffusion model, carbon-aware, risk-aware, reinforcement learning<br><br>Summary:DiffCarl is a carbon and risk-aware reinforcement learning algorithm that incorporates a diffusion model into deep reinforcement learning for energy scheduling in multi-microgrid systems. It outperforms traditional algorithms and state-of-the-art solutions, achieving lower operational costs and carbon emissions while reducing performance variability. Its flexibility allows adaptation to different system configurations and objectives, making it a practical and forward-looking solution for real-world deployment in evolving energy systems. <div>
arXiv:2507.16867v1 Announce Type: cross 
Abstract: This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware reinforcement learning algorithm for intelligent operation of multi-microgrid systems. With the growing integration of renewables and increasing system complexity, microgrid communities face significant challenges in real-time energy scheduling and optimization under uncertainty. DiffCarl integrates a diffusion model into a deep reinforcement learning (DRL) framework to enable adaptive energy scheduling under uncertainty and explicitly account for carbon emissions and operational risk. By learning action distributions through a denoising generation process, DiffCarl enhances DRL policy expressiveness and enables carbon- and risk-aware scheduling in dynamic and uncertain microgrid environments. Extensive experimental studies demonstrate that it outperforms classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower operational cost. It also achieves 28.7% lower carbon emissions than those of its carbon-unaware variant and reduces performance variability. These results highlight DiffCarl as a practical and forward-looking solution. Its flexible design allows efficient adaptation to different system configurations and objectives to support real-world deployment in evolving energy systems.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage</title>
<link>https://arxiv.org/abs/2507.16872</link>
<guid>https://arxiv.org/abs/2507.16872</guid>
<content:encoded><![CDATA[
<div> privacy risk, model compression, deep learning, membership inference attack, CompLeak

Summary: 
Model compression is essential for optimizing memory storage and speeding up inference in deep learning models, but the privacy risks associated with compression techniques are often overlooked. This study introduces CompLeak, a novel framework that evaluates privacy risks in three commonly used compression configurations - pruning, quantization, and weight clustering. Through the lens of membership inference attacks, CompLeak examines the impact of different compressed models on privacy leakage. Three variants of CompLeak are proposed, each focusing on different scenarios based on the availability of compressed models and the original model. By conducting experiments on various model architectures and datasets, the study highlights the significance of considering privacy risks in model compression strategies to protect sensitive information. <div>
arXiv:2507.16872v1 Announce Type: cross 
Abstract: Model compression is crucial for minimizing memory storage and accelerating inference in deep learning (DL) models, including recent foundation models like large language models (LLMs). Users can access different compressed model versions according to their resources and budget. However, while existing compression operations primarily focus on optimizing the trade-off between resource efficiency and model performance, the privacy risks introduced by compression remain overlooked and insufficiently understood.
  In this work, through the lens of membership inference attack (MIA), we propose CompLeak, the first privacy risk evaluation framework examining three widely used compression configurations that are pruning, quantization, and weight clustering supported by the commercial model compression framework of Google's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has three variants, given available access to the number of compressed models and original model. CompLeakNR starts by adopting existing MIA methods to attack a single compressed model, and identifies that different compressed models influence members and non-members differently. When the original model and one compressed model are available, CompLeakSR leverages the compressed model as a reference to the original model and uncovers more privacy by combining meta information (e.g., confidence vector) from both models. When multiple compressed models are available with/without accessing the original model, CompLeakMR innovatively exploits privacy leakage info from multiple compressed versions to substantially signify the overall privacy leakage. We conduct extensive experiments on seven diverse model architectures (from ResNet to foundation models of BERT and GPT-2), and six image and textual benchmark datasets.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting</title>
<link>https://arxiv.org/abs/2507.16873</link>
<guid>https://arxiv.org/abs/2507.16873</guid>
<content:encoded><![CDATA[
<div> dataset, personalized video highlighting, user preferences, user simulator, saliency scores
Summary:
The article introduces the HIPPO-Video dataset for personalized video highlighting, created using an LLM-based user simulator to reflect diverse user preferences. The dataset consists of 2,040 (watch history, saliency score) pairs covering 20,400 videos across 170 categories. A method called HiPHer is proposed to predict preference-conditioned segment-wise saliency scores using these personalized watch histories. Experimental results show that HiPHer outperforms existing generic and query-based approaches, demonstrating its potential for highly user-centric video highlighting in real-world scenarios.
<br><br> <div>
arXiv:2507.16873v1 Announce Type: cross 
Abstract: The exponential growth of video content has made personalized video highlighting an essential task, as user preferences are highly variable and complex. Existing video datasets, however, often lack personalization, relying on isolated videos or simple text queries that fail to capture the intricacies of user behavior. In this work, we introduce HIPPO-Video, a novel dataset for personalized video highlighting, created using an LLM-based user simulator to generate realistic watch histories reflecting diverse user preferences. The dataset includes 2,040 (watch history, saliency score) pairs, covering 20,400 videos across 170 semantic categories. To validate our dataset, we propose HiPHer, a method that leverages these personalized watch histories to predict preference-conditioned segment-wise saliency scores. Through extensive experiments, we demonstrate that our method outperforms existing generic and query-based approaches, showcasing its potential for highly user-centric video highlighting in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Budget Allocation Policies for Real-Time Multi-Agent Path Finding</title>
<link>https://arxiv.org/abs/2507.16874</link>
<guid>https://arxiv.org/abs/2507.16874</guid>
<content:encoded><![CDATA[
<div> Multi-Agent Pathfinding, MAPF, Real-Time MAPF, planning budget, Prioritized Planning<br>
Summary:<br>
The article discusses the Real-Time Multi-Agent Pathfinding (RT-MAPF) problem, where agents must move without waiting for all paths to be generated. Existing solutions iteratively use windowed versions of MAPF algorithms without considering planning budget size. The study explores different budget allocation policies for windowed MAPF algorithms like Prioritized Planning and MAPF-LNS2. It finds that sharing a planning budget pool is ineffective in highly constrained scenarios. Instead, distributing the budget among agents allows for more problems to be solved with shorter completion times. This research fills a gap in RT-MAPF solutions by optimizing planning budget allocation strategies to improve performance in real-time multi-agent pathfinding scenarios. <div>
arXiv:2507.16874v1 Announce Type: cross 
Abstract: Multi-Agent Pathfinding (MAPF) is the problem of finding paths for a set of agents such that each agent reaches its desired destination while avoiding collisions with the other agents. Many MAPF solvers are designed to run offline, that is, first generate paths for all agents and then execute them. Real-Time MAPF (RT-MAPF) embodies a realistic MAPF setup in which one cannot wait until a complete path for each agent has been found before they start to move. Instead, planning and execution are interleaved, where the agents must commit to a fixed number of steps in a constant amount of computation time, referred to as the planning budget. Existing solutions to RT-MAPF iteratively call windowed versions of MAPF algorithms in every planning period, without explicitly considering the size of the planning budget. We address this gap and explore different policies for allocating the planning budget in windowed versions of standard MAPF algorithms, namely Prioritized Planning (PrP) and MAPF-LNS2. Our exploration shows that the baseline approach in which all agents draw from a shared planning budget pool is ineffective in over-constrained situations. Instead, policies that distribute the planning budget over the agents are able to solve more problems with a smaller makespan.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine learning-based multimodal prognostic models integrating pathology images and high-throughput omic data for overall survival prediction in cancer: a systematic review</title>
<link>https://arxiv.org/abs/2507.16876</link>
<guid>https://arxiv.org/abs/2507.16876</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal machine learning, histopathology, molecular data, cancer prognostication, overall survival <br>
Summary: 
- A systematic review was conducted on 48 studies that integrated whole slide images (WSIs) and high-throughput omics data for cancer prognostication. 
- All studies included in the review were conducted since 2017 and utilized data from The Cancer Genome Atlas across 19 cancer types. 
- Different approaches such as regularized Cox regression, classical machine learning, and deep learning were employed, with reported c-indices ranging from 0.550 to 0.857. 
- Multimodal models generally outperformed unimodal ones, but there were concerns regarding bias, limited external validation, and insufficient focus on clinical utility in all studies. 
- The field of multimodal WSI-omics survival prediction shows promise but requires enhanced methodological rigor, broader datasets, and clinical evaluation. <br><br>Summary: <div>
arXiv:2507.16876v1 Announce Type: cross 
Abstract: Multimodal machine learning integrating histopathology and molecular data shows promise for cancer prognostication. We systematically reviewed studies combining whole slide images (WSIs) and high-throughput omics to predict overall survival. Searches of EMBASE, PubMed, and Cochrane CENTRAL (12/08/2024), plus citation screening, identified eligible studies. Data extraction used CHARMS; bias was assessed with PROBAST+AI; synthesis followed SWiM and PRISMA 2020. Protocol: PROSPERO (CRD42024594745).
  Forty-eight studies (all since 2017) across 19 cancer types met criteria; all used The Cancer Genome Atlas. Approaches included regularised Cox regression (n=4), classical ML (n=13), and deep learning (n=31). Reported c-indices ranged 0.550-0.857; multimodal models typically outperformed unimodal ones. However, all studies showed unclear/high bias, limited external validation, and little focus on clinical utility.
  Multimodal WSI-omics survival prediction is a fast-growing field with promising results but needs improved methodological rigor, broader datasets, and clinical evaluation.
  Funded by NPIC, Leeds Teaching Hospitals NHS Trust, UK (Project 104687), supported by UKRI Industrial Strategy Challenge Fund.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2507.16877</link>
<guid>https://arxiv.org/abs/2507.16877</guid>
<content:encoded><![CDATA[
<div> Keywords: Referring Expression Comprehension, multi-entity localization, relationship modeling, dataset construction, semantic ambiguity

Summary: 
ReMeX dataset is constructed for relation-aware, multi-entity Referring Expression Comprehension (REC). ReMeREC framework is proposed to localize multiple entities and model their inter-relations by leveraging visual and textual cues. The Text-adaptive Multi-entity Perceptron (TMP) dynamically infers entity quantity and span from textual cues. The Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and global scene understanding. An auxiliary dataset, EntityText, is used for improving language comprehension. Experiments show that ReMeREC outperforms existing approaches in multi-entity grounding and relation prediction. <div>
arXiv:2507.16877v1 Announce Type: cross 
Abstract: Referring Expression Comprehension (REC) aims to localize specified entities or regions in an image based on natural language descriptions. While existing methods handle single-entity localization, they often ignore complex inter-entity relationships in multi-entity scenes, limiting their accuracy and reliability. Additionally, the lack of high-quality datasets with fine-grained, paired image-text-relation annotations hinders further progress. To address this challenge, we first construct a relation-aware, multi-entity REC dataset called ReMeX, which includes detailed relationship and textual annotations. We then propose ReMeREC, a novel framework that jointly leverages visual and textual cues to localize multiple entities while modeling their inter-relations. To address the semantic ambiguity caused by implicit entity boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron (TMP), which dynamically infers both the quantity and span of entities from fine-grained textual cues, producing distinctive representations. Additionally, our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and global scene understanding. To further improve language comprehension for fine-grained prompts, we also construct a small-scale auxiliary dataset, EntityText, generated using large language models. Experiments on four benchmark datasets show that ReMeREC achieves state-of-the-art performance in multi-entity grounding and relation prediction, outperforming existing approaches by a large margin.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos</title>
<link>https://arxiv.org/abs/2507.16878</link>
<guid>https://arxiv.org/abs/2507.16878</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, video reasoning, CausalStep benchmark, stepwise causal reasoning, diagnostic metrics

Summary: 
The article discusses the challenges in achieving robust video reasoning despite recent advancements in large language models. Existing video benchmarks do not rigorously evaluate true causal and stepwise reasoning. To address this, the authors introduce the CausalStep benchmark, designed for explicit stepwise causal reasoning in videos. This benchmark segments videos into causally linked units and enforces a strict stepwise question-answer protocol to prevent shortcut solutions. It features carefully constructed distractors for each question to ensure diagnostic value. With 100 videos across six categories and 1,852 QA pairs, CausalStep introduces seven diagnostic metrics for comprehensive evaluation. Experiments conducted with various models, including human baselines, highlight a significant gap in stepwise reasoning capabilities between current models and human performance. CausalStep aims to drive progress in robust and interpretable video reasoning. <br><br>Summary: <div>
arXiv:2507.16878v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have improved reasoning in text and image domains, yet achieving robust video reasoning remains a significant challenge. Existing video benchmarks mainly assess shallow understanding and reasoning and allow models to exploit global context, failing to rigorously evaluate true causal and stepwise reasoning. We present CausalStep, a benchmark designed for explicit stepwise causal reasoning in videos. CausalStep segments videos into causally linked units and enforces a strict stepwise question-answer (QA) protocol, requiring sequential answers and preventing shortcut solutions. Each question includes carefully constructed distractors based on error type taxonomy to ensure diagnostic value. The benchmark features 100 videos across six categories and 1,852 multiple-choice QA pairs. We introduce seven diagnostic metrics for comprehensive evaluation, enabling precise diagnosis of causal reasoning capabilities. Experiments with leading proprietary and open-source models, as well as human baselines, reveal a significant gap between current models and human-level stepwise reasoning. CausalStep provides a rigorous benchmark to drive progress in robust and interpretable video reasoning.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed</title>
<link>https://arxiv.org/abs/2507.16880</link>
<guid>https://arxiv.org/abs/2507.16880</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image diffusion models, data privacy, memorization, replication, adversarial fine-tuning <br>
Summary: 
This research evaluates the vulnerabilities of text-to-image diffusion models (DMs) regarding data privacy and intellectual property concerns. Current pruning-based mitigation efforts to prevent model memorization are found to be ineffective, as minor changes in input prompts can still trigger data replication. The assumption that memorization is localized is challenged, as replication can be initiated from various points in the text embedding space. The study introduces an adversarial fine-tuning method to increase model robustness by iteratively searching for replication triggers. These findings highlight the need for improved strategies to eliminate memorized content rather than simply suppressing its retrieval, emphasizing the importance of building trustworthy and compliant generative AI systems. <br><br> <div>
arXiv:2507.16880v1 Announce Type: cross 
Abstract: Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence Optimization for Probabilistic Encoding</title>
<link>https://arxiv.org/abs/2507.16881</link>
<guid>https://arxiv.org/abs/2507.16881</guid>
<content:encoded><![CDATA[
<div> Gaussian noise, neural networks, probabilistic encoding, uncertain states, generalization ability <br>
<br>
Probabilistic encoding with Gaussian noise in neural networks enhances generalization ability but distorts distance measurements in classification tasks. To address this issue, the confidence optimization probabilistic encoding (CPE) method is introduced. It refines probabilistic encoding by incorporating a confidence-aware mechanism to adjust distance calculations and replaces KL divergence-based variance regularization with an L2 regularization term for improved variance control. The CPE method is applicable to any model and experimental results on natural language classification tasks show significant performance improvements on BERT and RoBERTa models. <br><br>Summary: <div>
arXiv:2507.16881v1 Announce Type: cross 
Abstract: Probabilistic encoding introduces Gaussian noise into neural networks, enabling a smooth transition from deterministic to uncertain states and enhancing generalization ability. However, the randomness of Gaussian noise distorts point-based distance measurements in classification tasks. To mitigate this issue, we propose a confidence optimization probabilistic encoding (CPE) method that improves distance reliability and enhances representation learning. Specifically, we refine probabilistic encoding with two key strategies: First, we introduce a confidence-aware mechanism to adjust distance calculations, ensuring consistency and reliability in probabilistic encoding classification tasks. Second, we replace the conventional KL divergence-based variance regularization, which relies on unreliable prior assumptions, with a simpler L2 regularization term to directly constrain variance. The method we proposed is model-agnostic, and extensive experiments on natural language classification tasks demonstrate that our method significantly improves performance and generalization on both the BERT and the RoBERTa model.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling</title>
<link>https://arxiv.org/abs/2507.16884</link>
<guid>https://arxiv.org/abs/2507.16884</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, average velocity field, Interval Splitting Consistency, SplitMeanFlow, speech synthesis<br>
Summary: 
Generative models like Flow Matching have shown superior performance but are often limited by computationally expensive iterative sampling processes. Recent research has focused on one-step generation by learning the average velocity field, with MeanFlow being a prominent method. This work introduces the concept of Interval Splitting Consistency, a novel algebraic identity that ensures consistency of the average velocity field across different time intervals. The SplitMeanFlow framework is proposed, which directly enforces this algebraic consistency as a learning objective. It is shown that SplitMeanFlow is a more general approach than MeanFlow, with the latter being a limiting case of the former. The algebraic approach offers improved efficiency by eliminating the need for JVP computations, resulting in simpler implementation, stable training, and broader hardware compatibility. Practical applications of SplitMeanFlow, such as in speech synthesis products like Doubao, have demonstrated significant speedups of up to 20x. <br><br>Summary: <div>
arXiv:2507.16884v1 Announce Type: cross 
Abstract: Generative models like Flow Matching have achieved state-of-the-art performance but are often hindered by a computationally expensive iterative sampling process. To address this, recent work has focused on few-step or one-step generation by learning the average velocity field, which directly maps noise to data. MeanFlow, a leading method in this area, learns this field by enforcing a differential identity that connects the average and instantaneous velocities. In this work, we argue that this differential formulation is a limiting special case of a more fundamental principle. We return to the first principles of average velocity and leverage the additivity property of definite integrals. This leads us to derive a novel, purely algebraic identity we term Interval Splitting Consistency. This identity establishes a self-referential relationship for the average velocity field across different time intervals without resorting to any differential operators. Based on this principle, we introduce SplitMeanFlow, a new training framework that enforces this algebraic consistency directly as a learning objective. We formally prove that the differential identity at the core of MeanFlow is recovered by taking the limit of our algebraic consistency as the interval split becomes infinitesimal. This establishes SplitMeanFlow as a direct and more general foundation for learning average velocity fields. From a practical standpoint, our algebraic approach is significantly more efficient, as it eliminates the need for JVP computations, resulting in simpler implementation, more stable training, and broader hardware compatibility. One-step and two-step SplitMeanFlow models have been successfully deployed in large-scale speech synthesis products (such as Doubao), achieving speedups of 20x.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning</title>
<link>https://arxiv.org/abs/2507.16886</link>
<guid>https://arxiv.org/abs/2507.16886</guid>
<content:encoded><![CDATA[

arXiv:2507.16886v1 Announce Type: cross 
Abstract: Spatial transcriptomics (ST) has revolutionized biomedical research by enabling high resolution gene expression profiling within tissues. However, the high cost and scarcity of high resolution ST data remain significant challenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel framework for accurate ST imputation that requires only a single and low-cost sparsely sampled ST dataset alongside widely available natural images for co-training. Our approach integrates three key innovations: (1) a sparser-to-sparse self-supervised learning strategy that leverages intrinsic spatial patterns in ST data, (2) cross-domain co-learning with natural images to enhance feature representation, and (3) a Cascaded Data Consistent Imputation Network (CDCIN) that iteratively refines predictions while preserving sampled gene data fidelity. Extensive experiments on diverse tissue types, including breast cancer, liver, and lymphoid tissue, demonstrate that our method outperforms state-of-the-art approaches in imputation accuracy. By enabling robust ST reconstruction from sparse inputs, our framework significantly reduces reliance on costly high resolution data, facilitating potential broader adoption in biomedical research and clinical applications.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Pre-trained Language Models for Vulnerability Detection</title>
<link>https://arxiv.org/abs/2507.16887</link>
<guid>https://arxiv.org/abs/2507.16887</guid>
<content:encoded><![CDATA[

arXiv:2507.16887v1 Announce Type: cross 
Abstract: The rapid advancement of pre-trained language models (PLMs) has demonstrated promising results for various code-related tasks. However, their effectiveness in detecting real-world vulnerabilities remains a critical challenge. % for the security community. While existing empirical studies evaluate PLMs for vulnerability detection (VD), their inadequate consideration in data preparation, evaluation setups, and experimental settings undermines the accuracy and comprehensiveness of evaluations. This paper introduces RevisitVD, an extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and large-scale PLMs using newly constructed datasets. Specifically, we compare the performance of PLMs under both fine-tuning and prompt engineering, assess their effectiveness and generalizability across various training and testing settings, and analyze their robustness against code normalization, abstraction, and semantic-preserving transformations.
  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks designed to capture the syntactic and semantic patterns of code outperform both general-purpose PLMs and those solely pre-trained or fine-tuned on large code corpora. However, these models face notable challenges in real-world scenarios, such as difficulties in detecting vulnerabilities with complex dependencies, handling perturbations introduced by code normalization and abstraction, and identifying semantic-preserving vulnerable code transformations. Also, the truncation caused by the limited context windows of PLMs can lead to a non-negligible amount of labeling errors. This study underscores the importance of thorough evaluations of model performance in practical scenarios and outlines future directions to help enhance the effectiveness of PLMs for realistic VD applications.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SiLQ: Simple Large Language Model Quantization-Aware Training</title>
<link>https://arxiv.org/abs/2507.16933</link>
<guid>https://arxiv.org/abs/2507.16933</guid>
<content:encoded><![CDATA[

arXiv:2507.16933v1 Announce Type: cross 
Abstract: Large language models can be quantized to reduce inference time latency, model size, and energy consumption, thereby delivering a better user experience at lower cost. A challenge exists to deliver quantized models with minimal loss of accuracy in reasonable time, and in particular to do so without requiring mechanisms incompatible with specialized inference accelerators. Here, we demonstrate a simple, end-to-end quantization-aware training approach that, with an increase in total model training budget of less than 0.1%, outperforms the leading published quantization methods by large margins on several modern benchmarks, with both base and instruct model variants. The approach easily generalizes across different model architectures, can be applied to activations, cache, and weights, and requires the introduction of no additional operations to the model other than the quantization itself.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Ensemble and Deep Learning Models for Static Malware Detection with Dimensionality Reduction Using the EMBER Dataset</title>
<link>https://arxiv.org/abs/2507.16952</link>
<guid>https://arxiv.org/abs/2507.16952</guid>
<content:encoded><![CDATA[

arXiv:2507.16952v1 Announce Type: cross 
Abstract: This study investigates the effectiveness of several machine learning algorithms for static malware detection using the EMBER dataset, which contains feature representations of Portable Executable (PE) files. We evaluate eight classification models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees, HistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet, under three preprocessing settings: original feature space, Principal Component Analysis (PCA), and Linear Discriminant Analysis (LDA). The models are assessed on accuracy, precision, recall, F1 score, and AUC to examine both predictive performance and robustness. Ensemble methods, especially LightGBM and XGBoost, show the best overall performance across all configurations, with minimal sensitivity to PCA and consistent generalization. LDA improves KNN performance but significantly reduces accuracy for boosting models. TabNet, while promising in theory, underperformed under feature reduction, likely due to architectural sensitivity to input structure. The analysis is supported by detailed exploratory data analysis (EDA), including mutual information ranking, PCA or t-SNE visualizations, and outlier detection using Isolation Forest and Local Outlier Factor (LOF), which confirm the discriminatory capacity of key features in the EMBER dataset. The results suggest that boosting models remain the most reliable choice for high-dimensional static malware detection, and that dimensionality reduction should be applied selectively based on model type. This work provides a benchmark for comparing classification models and preprocessing strategies in malware detection tasks and contributes insights that can guide future system development and real-world deployment.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning</title>
<link>https://arxiv.org/abs/2507.16971</link>
<guid>https://arxiv.org/abs/2507.16971</guid>
<content:encoded><![CDATA[

arXiv:2507.16971v1 Announce Type: cross 
Abstract: Accessing knowledge via multilingual natural-language interfaces is one of the emerging challenges in the field of information retrieval and related ones. Structured knowledge stored in knowledge graphs can be queried via a specific query language (e.g., SPARQL). Therefore, one needs to transform natural-language input into a query to fulfill an information need. Prior approaches mostly focused on combining components (e.g., rule-based or neural-based) that solve downstream tasks and come up with an answer at the end. We introduce mKGQAgent, a human-inspired framework that breaks down the task of converting natural language questions into SPARQL queries into modular, interpretable subtasks. By leveraging a coordinated LLM agent workflow for planning, entity linking, and query refinement - guided by an experience pool for in-context learning - mKGQAgent efficiently handles multilingual KGQA. Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the Text2SPARQL challenge 2025, our approach took first place among the other participants. This work opens new avenues for developing human-like reasoning systems in multilingual semantic parsing.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain</title>
<link>https://arxiv.org/abs/2507.16974</link>
<guid>https://arxiv.org/abs/2507.16974</guid>
<content:encoded><![CDATA[

arXiv:2507.16974v1 Announce Type: cross 
Abstract: Enabling farmers to access accurate agriculture-related information in their native languages in a timely manner is crucial for the success of the agriculture field. Although large language models (LLMs) can be used to implement Question Answering (QA) systems, simply using publicly available general-purpose LLMs in agriculture typically offer generic advisories, lacking precision in local and multilingual contexts due to insufficient domain-specific training and scarcity of high-quality, region-specific datasets. Our study addresses these limitations by generating multilingual synthetic agricultural datasets (English, Hindi, Punjabi) from agriculture-specific documents and fine-tuning language-specific LLMs. Our evaluation on curated multilingual datasets demonstrates significant improvements in factual accuracy, relevance, and agricultural consensus for the fine-tuned models compared to their baseline counterparts. These results highlight the efficacy of synthetic data-driven, language-specific fine-tuning as an effective strategy to improve the performance of LLMs in agriculture, especially in multilingual and low-resource settings. By enabling more accurate and localized agricultural advisory services, this study provides a meaningful step toward bridging the knowledge gap in AI-driven agricultural solutions for diverse linguistic communities.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Scalable Gene Embedding Search: A Comparative Study of FAISS and ScaNN</title>
<link>https://arxiv.org/abs/2507.16978</link>
<guid>https://arxiv.org/abs/2507.16978</guid>
<content:encoded><![CDATA[

arXiv:2507.16978v1 Announce Type: cross 
Abstract: The exponential growth of DNA sequencing data has outpaced traditional heuristic-based methods, which struggle to scale effectively. Efficient computational approaches are urgently needed to support large-scale similarity search, a foundational task in bioinformatics for detecting homology, functional similarity, and novelty among genomic and proteomic sequences. Although tools like BLAST have been widely used and remain effective in many scenarios, they suffer from limitations such as high computational cost and poor performance on divergent sequences.
  In this work, we explore embedding-based similarity search methods that learn latent representations capturing deeper structural and functional patterns beyond raw sequence alignment. We systematically evaluate two state-of-the-art vector search libraries, FAISS and ScaNN, on biologically meaningful gene embeddings. Unlike prior studies, our analysis focuses on bioinformatics-specific embeddings and benchmarks their utility for detecting novel sequences, including those from uncharacterized taxa or genes lacking known homologs. Our results highlight both computational advantages (in memory and runtime efficiency) and improved retrieval quality, offering a promising alternative to traditional alignment-heavy tools.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyG 2.0: Scalable Learning on Real World Graphs</title>
<link>https://arxiv.org/abs/2507.16991</link>
<guid>https://arxiv.org/abs/2507.16991</guid>
<content:encoded><![CDATA[

arXiv:2507.16991v1 Announce Type: cross 
Abstract: PyG (PyTorch Geometric) has evolved significantly since its initial release, establishing itself as a leading framework for Graph Neural Networks. In this paper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive update that introduces substantial improvements in scalability and real-world application capabilities. We detail the framework's enhanced architecture, including support for heterogeneous and temporal graphs, scalable feature/graph stores, and various optimizations, enabling researchers and practitioners to tackle large-scale graph learning problems efficiently. Over the recent years, PyG has been supporting graph learning in a large variety of application areas, which we will summarize, while providing a deep dive into the important areas of relational deep learning and large language modeling.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian preference elicitation for decision support in multiobjective optimization</title>
<link>https://arxiv.org/abs/2507.16999</link>
<guid>https://arxiv.org/abs/2507.16999</guid>
<content:encoded><![CDATA[

arXiv:2507.16999v1 Announce Type: cross 
Abstract: We present a novel approach to help decision-makers efficiently identify preferred solutions from the Pareto set of a multi-objective optimization problem. Our method uses a Bayesian model to estimate the decision-maker's utility function based on pairwise comparisons. Aided by this model, a principled elicitation strategy selects queries interactively to balance exploration and exploitation, guiding the discovery of high-utility solutions. The approach is flexible: it can be used interactively or a posteriori after estimating the Pareto front through standard multi-objective optimization techniques. Additionally, at the end of the elicitation phase, it generates a reduced menu of high-quality solutions, simplifying the decision-making process. Through experiments on test problems with up to nine objectives, our method demonstrates superior performance in finding high-utility solutions with a small number of queries. We also provide an open-source implementation of our method to support its adoption by the broader community.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models</title>
<link>https://arxiv.org/abs/2507.17008</link>
<guid>https://arxiv.org/abs/2507.17008</guid>
<content:encoded><![CDATA[

arXiv:2507.17008v1 Announce Type: cross 
Abstract: Most sign language handshape datasets are severely limited and unbalanced, posing significant challenges to effective model training. In this paper, we explore the effectiveness of augmenting the training data of a handshape classifier by generating synthetic data. We use an EfficientNet classifier trained on the RWTH German sign language handshape dataset, which is small and heavily unbalanced, applying different strategies to combine generated and real images. We compare two Generative Adversarial Networks (GAN) architectures for data generation: ReACGAN, which uses label information to condition the data generation process through an auxiliary classifier, and SPADE, which utilizes spatially-adaptive normalization to condition the generation on pose information. ReACGAN allows for the generation of realistic images that align with specific handshape labels, while SPADE focuses on generating images with accurate spatial handshape configurations. Our proposed techniques improve the current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the limitations of small and unbalanced datasets. Additionally, our method demonstrates the capability to generalize across different sign language datasets by leveraging pose-based generation trained on the extensive HaGRID dataset. We achieve comparable performance to single-source trained classifiers without the need for retraining the generator.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Trustworthy AI: Secure Deepfake Detection using CNNs and Zero-Knowledge Proofs</title>
<link>https://arxiv.org/abs/2507.17010</link>
<guid>https://arxiv.org/abs/2507.17010</guid>
<content:encoded><![CDATA[

arXiv:2507.17010v1 Announce Type: cross 
Abstract: In the era of synthetic media, deepfake manipulations pose a significant threat to information integrity. To address this challenge, we propose TrustDefender, a two-stage framework comprising (i) a lightweight convolutional neural network (CNN) that detects deepfake imagery in real-time extended reality (XR) streams, and (ii) an integrated succinct zero-knowledge proof (ZKP) protocol that validates detection results without disclosing raw user data. Our design addresses both the computational constraints of XR platforms while adhering to the stringent privacy requirements in sensitive settings. Experimental evaluations on multiple benchmark deepfake datasets demonstrate that TrustDefender achieves 95.3% detection accuracy, coupled with efficient proof generation underpinned by rigorous cryptography, ensuring seamless integration with high-performance artificial intelligence (AI) systems. By fusing advanced computer vision models with provable security mechanisms, our work establishes a foundation for reliable AI in immersive and privacy-sensitive applications.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>laplax -- Laplace Approximations with JAX</title>
<link>https://arxiv.org/abs/2507.17013</link>
<guid>https://arxiv.org/abs/2507.17013</guid>
<content:encoded><![CDATA[

arXiv:2507.17013v1 Announce Type: cross 
Abstract: The Laplace approximation provides a scalable and efficient means of quantifying weight-space uncertainty in deep neural networks, enabling the application of Bayesian tools such as predictive uncertainty and model selection via Occam's razor. In this work, we introduce laplax, a new open-source Python package for performing Laplace approximations with jax. Designed with a modular and purely functional architecture and minimal external dependencies, laplax offers a flexible and researcher-friendly framework for rapid prototyping and experimentation. Its goal is to facilitate research on Bayesian neural networks, uncertainty quantification for deep learning, and the development of improved Laplace approximation techniques.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?</title>
<link>https://arxiv.org/abs/2507.17015</link>
<guid>https://arxiv.org/abs/2507.17015</guid>
<content:encoded><![CDATA[

arXiv:2507.17015v1 Announce Type: cross 
Abstract: Pairwise preferences over model responses are widely collected to evaluate and provide feedback to large language models (LLMs). Given two alternative model responses to the same input, a human or AI annotator selects the "better" response. This approach can provide feedback for domains where other hard-coded metrics are difficult to obtain (e.g., chat response quality), thereby helping model evaluation or training. However, for some domains high-quality pairwise comparisons can be tricky to obtain - from AI and humans. For example, for responses with many factual statements, annotators may disproportionately weigh writing quality rather than underlying facts. In this work, we explore augmenting standard AI annotator systems with additional tools to improve performance on three challenging response domains: long-form factual, math and code tasks. We propose a tool-using agentic system to provide higher quality feedback on these domains. Our system uses web-search and code execution to ground itself based on external validation, independent of the LLM's internal knowledge and biases. We provide extensive experimental results evaluating our method across the three targeted response domains as well as general annotation tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as three new datasets for domains with saturated pre-existing datasets. Our results indicate that external tools can indeed improve performance in many, but not all, cases. More generally, our experiments highlight the sensitivity of performance to simple parameters (e.g., prompt) and the need for improved (non-saturated) annotator benchmarks. We share our code at https://github.com/apple/ml-agent-evaluator.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2507.17016</link>
<guid>https://arxiv.org/abs/2507.17016</guid>
<content:encoded><![CDATA[

arXiv:2507.17016v1 Announce Type: cross 
Abstract: In recent years, the application of Large Language Models (LLMs) to time series forecasting (TSF) has garnered significant attention among researchers. This study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with fuzzy time series (FTS) and causal graph to predict multivariate time series, marking the first such architecture in the literature. The key objective is to convert numerical time series into interpretable forms through the parallel application of fuzzification and causal analysis, enabling both semantic understanding and structural insight as input for the pretrained GPT-2 model. The resulting textual representation offers a more interpretable view of the complex dynamics underlying the original time series. The reported results confirm the effectiveness of our proposed LLM-based time series forecasting model, as demonstrated across four different multivariate time series datasets. This initiative paves promising future directions in the domain of TSF using LLMs based on FTS.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings</title>
<link>https://arxiv.org/abs/2507.17025</link>
<guid>https://arxiv.org/abs/2507.17025</guid>
<content:encoded><![CDATA[

arXiv:2507.17025v1 Announce Type: cross 
Abstract: Efficient text embedding is crucial for large-scale natural language processing (NLP) applications, where storage and computational efficiency are key concerns. In this paper, we explore how using binary representations (barcodes) instead of real-valued features can be used for NLP embeddings derived from machine learning models such as BERT. Thresholding is a common method for converting continuous embeddings into binary representations, often using a fixed threshold across all features. We propose a Coordinate Search-based optimization framework that instead identifies the optimal threshold for each feature, demonstrating that feature-specific thresholds lead to improved performance in binary encoding. This ensures that the binary representations are both accurate and efficient, enhancing performance across various features. Our optimal barcode representations have shown promising results in various NLP applications, demonstrating their potential to transform text representation. We conducted extensive experiments and statistical tests on different NLP tasks and datasets to evaluate our approach and compare it to other thresholding methods. Binary embeddings generated using using optimal thresholds found by our method outperform traditional binarization methods in accuracy. This technique for generating binary representations is versatile and can be applied to any features, not just limited to NLP embeddings, making it useful for a wide range of domains in machine learning applications.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamME: Simplify 3D Gaussian Avatar within Live Stream</title>
<link>https://arxiv.org/abs/2507.17029</link>
<guid>https://arxiv.org/abs/2507.17029</guid>
<content:encoded><![CDATA[

arXiv:2507.17029v1 Announce Type: cross 
Abstract: We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Performance Bounds Prediction in Quantum Computing with Unstable Noise</title>
<link>https://arxiv.org/abs/2507.17043</link>
<guid>https://arxiv.org/abs/2507.17043</guid>
<content:encoded><![CDATA[

arXiv:2507.17043v1 Announce Type: cross 
Abstract: Quantum computing has significantly advanced in recent years, boasting devices with hundreds of quantum bits (qubits), hinting at its potential quantum advantage over classical computing. Yet, noise in quantum devices poses significant barriers to realizing this supremacy. Understanding noise's impact is crucial for reproducibility and application reuse; moreover, the next-generation quantum-centric supercomputing essentially requires efficient and accurate noise characterization to support system management (e.g., job scheduling), where ensuring correct functional performance (i.e., fidelity) of jobs on available quantum devices can even be higher-priority than traditional objectives. However, noise fluctuates over time, even on the same quantum device, which makes predicting the computational bounds for on-the-fly noise is vital. Noisy quantum simulation can offer insights but faces efficiency and scalability issues. In this work, we propose a data-driven workflow, namely QuBound, to predict computational performance bounds. It decomposes historical performance traces to isolate noise sources and devises a novel encoder to embed circuit and noise information processed by a Long Short-Term Memory (LSTM) network. For evaluation, we compare QuBound with a state-of-the-art learning-based predictor, which only generates a single performance value instead of a bound. Experimental results show that the result of the existing approach falls outside of performance bounds, while all predictions from our QuBound with the assistance of performance decomposition better fit the bounds. Moreover, QuBound can efficiently produce practical bounds for various circuits with over 106 speedup over simulation; in addition, the range from QuBound is over 10x narrower than the state-of-the-art analytical approach.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Hybrid Captioner for Improved Long-form Video Understanding</title>
<link>https://arxiv.org/abs/2507.17047</link>
<guid>https://arxiv.org/abs/2507.17047</guid>
<content:encoded><![CDATA[

arXiv:2507.17047v1 Announce Type: cross 
Abstract: Video data, especially long-form video, is extremely dense and high-dimensional. Text-based summaries of video content offer a way to represent query-relevant content in a much more compact manner than raw video. In addition, textual representations are easily ingested by state-of-the-art large language models (LLMs), which enable reasoning over video content to answer complex natural language queries. To solve this issue, we rely on the progressive construction of a text-based memory by a video captioner operating on shorter chunks of the video, where spatio-temporal modeling is computationally feasible. We explore ways to improve the quality of the activity log comprised solely of short video captions. Because the video captions tend to be focused on human actions, and questions may pertain to other information in the scene, we seek to enrich the memory with static scene descriptions using Vision Language Models (VLMs). Our video understanding system relies on the LaViLa video captioner in combination with a LLM to answer questions about videos. We first explored different ways of partitioning the video into meaningful segments such that the textual descriptions more accurately reflect the structure of the video content. Furthermore, we incorporated static scene descriptions into the captioning pipeline using LLaVA VLM, resulting in a more detailed and complete caption log and expanding the space of questions that are answerable from the textual memory. Finally, we have successfully fine-tuned the LaViLa video captioner to produce both action and scene captions, significantly improving the efficiency of the captioning pipeline compared to using separate captioning models for the two tasks. Our model, controllable hybrid captioner, can alternate between different types of captions according to special input tokens that signals scene changes detected in the video.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pragmatic Policy Development via Interpretable Behavior Cloning</title>
<link>https://arxiv.org/abs/2507.17056</link>
<guid>https://arxiv.org/abs/2507.17056</guid>
<content:encoded><![CDATA[

arXiv:2507.17056v1 Announce Type: cross 
Abstract: Offline reinforcement learning (RL) holds great promise for deriving optimal policies from observational data, but challenges related to interpretability and evaluation limit its practical use in safety-critical domains. Interpretability is hindered by the black-box nature of unconstrained RL policies, while evaluation -- typically performed off-policy -- is sensitive to large deviations from the data-collecting behavior policy, especially when using methods based on importance sampling. To address these challenges, we propose a simple yet practical alternative: deriving treatment policies from the most frequently chosen actions in each patient state, as estimated by an interpretable model of the behavior policy. By using a tree-based model, which is specifically designed to exploit patterns in the data, we obtain a natural grouping of states with respect to treatment. The tree structure ensures interpretability by design, while varying the number of actions considered controls the degree of overlap with the behavior policy, enabling reliable off-policy evaluation. This pragmatic approach to policy development standardizes frequent treatment patterns, capturing the collective clinical judgment embedded in the data. Using real-world examples in rheumatoid arthritis and sepsis care, we demonstrate that policies derived under this framework can outperform current practice, offering interpretable alternatives to those obtained via offline RL.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems</title>
<link>https://arxiv.org/abs/2507.17061</link>
<guid>https://arxiv.org/abs/2507.17061</guid>
<content:encoded><![CDATA[

arXiv:2507.17061v1 Announce Type: cross 
Abstract: Large language model (LLM) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent communication, reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent LLM systems.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compatibility of Max and Sum Objectives for Committee Selection and $k$-Facility Location</title>
<link>https://arxiv.org/abs/2507.17063</link>
<guid>https://arxiv.org/abs/2507.17063</guid>
<content:encoded><![CDATA[

arXiv:2507.17063v1 Announce Type: cross 
Abstract: We study a version of the metric facility location problem (or, equivalently, variants of the committee selection problem) in which we must choose $k$ facilities in an arbitrary metric space to serve some set of clients $C$. We consider four different objectives, where each client $i\in C$ attempts to minimize either the sum or the maximum of its distance to the chosen facilities, and where the overall objective either considers the sum or the maximum of the individual client costs. Rather than optimizing a single objective at a time, we study how compatible these objectives are with each other, and show the existence of solutions which are simultaneously close-to-optimum for any pair of the above objectives. Our results show that when choosing a set of facilities or a representative committee, it is often possible to form a solution which is good for several objectives at the same time, instead of sacrificing one desideratum to achieve another.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Robustness in Deep Reinforcement Learning with an Ensemble Defense Approach</title>
<link>https://arxiv.org/abs/2507.17070</link>
<guid>https://arxiv.org/abs/2507.17070</guid>
<content:encoded><![CDATA[

arXiv:2507.17070v1 Announce Type: cross 
Abstract: Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated its applicability across various domains, including robotics, healthcare, energy optimization, and autonomous driving. However, a critical question remains: How robust are DRL models when exposed to adversarial attacks? While existing defense mechanisms such as adversarial training and distillation enhance the resilience of DRL models, there remains a significant research gap regarding the integration of multiple defenses in autonomous driving scenarios specifically. This paper addresses this gap by proposing a novel ensemble-based defense architecture to mitigate adversarial attacks in autonomous driving. Our evaluation demonstrates that the proposed architecture significantly enhances the robustness of DRL models. Compared to the baseline under FGSM attacks, our ensemble method improves the mean reward from 5.87 to 18.38 (over 213% increase) and reduces the mean collision rate from 0.50 to 0.09 (an 82% decrease) in the highway scenario and merge scenario, outperforming all standalone defense strategies.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings</title>
<link>https://arxiv.org/abs/2507.17080</link>
<guid>https://arxiv.org/abs/2507.17080</guid>
<content:encoded><![CDATA[

arXiv:2507.17080v1 Announce Type: cross 
Abstract: Multimodal learning plays a critical role in e-commerce recommendation platforms today, enabling accurate recommendations and product understanding. However, existing vision-language models, such as CLIP, face key challenges in e-commerce recommendation systems: 1) Weak object-level alignment, where global image embeddings fail to capture fine-grained product attributes, leading to suboptimal retrieval performance; 2) Ambiguous textual representations, where product descriptions often lack contextual clarity, affecting cross-modal matching; and 3) Domain mismatch, as generic vision-language models may not generalize well to e-commerce-specific data. To address these limitations, we propose a framework, VL-CLIP, that enhances CLIP embeddings by integrating Visual Grounding for fine-grained visual understanding and an LLM-based agent for generating enriched text embeddings. Visual Grounding refines image representations by localizing key products, while the LLM agent enhances textual features by disambiguating product descriptions. Our approach significantly improves retrieval accuracy, multimodal retrieval effectiveness, and recommendation quality across tens of millions of items on one of the largest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by 15.5%, and GMV by 4.0%. Additional experimental results show that our framework outperforms vision-language models, including CLIP, FashionCLIP, and GCL, in both precision and semantic alignment, demonstrating the potential of combining object-aware visual grounding and LLM-enhanced text representation for robust multimodal recommendations.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction</title>
<link>https://arxiv.org/abs/2507.17083</link>
<guid>https://arxiv.org/abs/2507.17083</guid>
<content:encoded><![CDATA[

arXiv:2507.17083v1 Announce Type: cross 
Abstract: Multimodal 3D occupancy prediction has garnered significant attention for its potential in autonomous driving. However, most existing approaches are single-modality: camera-based methods lack depth information, while LiDAR-based methods struggle with occlusions. Current lightweight methods primarily rely on the Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth estimation and fails to fully exploit the geometric and semantic information of 3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction network called SDG-OCC, which incorporates a joint semantic and depth-guided view transformation coupled with a fusion-to-occupancy-driven active distillation. The enhanced view transformation constructs accurate depth distributions by integrating pixel semantics and co-point depth through diffusion and bilinear discretization. The fusion-to-occupancy-driven active distillation extracts rich semantic information from multimodal data and selectively transfers knowledge to image features based on LiDAR-identified regions. Finally, for optimal performance, we introduce SDG-Fusion, which uses fusion alone, and SDG-KL, which integrates both fusion and distillation for faster inference. Our method achieves state-of-the-art (SOTA) performance with real-time processing on the Occ3D-nuScenes dataset and shows comparable performance on the more challenging SurroundOcc-nuScenes dataset, demonstrating its effectiveness and robustness. The code will be released at https://github.com/DzpLab/SDGOCC.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weather-Aware AI Systems versus Route-Optimization AI: A Comprehensive Analysis of AI Applications in Transportation Productivity</title>
<link>https://arxiv.org/abs/2507.17099</link>
<guid>https://arxiv.org/abs/2507.17099</guid>
<content:encoded><![CDATA[

arXiv:2507.17099v1 Announce Type: cross 
Abstract: While recent research demonstrates that AI route-optimization systems improve taxi driver productivity by 14\%, this study reveals that such findings capture only a fraction of AI's potential in transportation. We examine comprehensive weather-aware AI systems that integrate deep learning meteorological prediction with machine learning positioning optimization, comparing their performance against traditional operations and route-only AI approaches. Using simulation data from 10,000 taxi operations across varied weather conditions, we find that weather-aware AI systems increase driver revenue by 107.3\%, compared to 14\% improvements from route-optimization alone. Weather prediction contributes the largest individual productivity gain, with strong correlations between meteorological conditions and demand ($r=0.575$). Economic analysis reveals annual earnings increases of 13.8 million yen per driver, with rapid payback periods and superior return on investment. These findings suggest that current AI literature significantly underestimates AI's transformative potential by focusing narrowly on routing algorithms, while weather intelligence represents an untapped \$8.9 billion market opportunity. Our results indicate that future AI implementations should adopt comprehensive approaches that address multiple operational challenges simultaneously rather than optimizing isolated functions.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models</title>
<link>https://arxiv.org/abs/2507.17107</link>
<guid>https://arxiv.org/abs/2507.17107</guid>
<content:encoded><![CDATA[

arXiv:2507.17107v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) is a key post-pretraining step for aligning large language models (LLMs) with complex tasks and human preferences. While it is often assumed that RL fine-tuning requires updating most of a model's parameters, we challenge this assumption with a surprising finding: RL fine-tuning consistently modifies only a small subnetwork (typically 5-30% of weights), leaving most parameters unchanged. We call this phenomenon RL-induced parameter update sparsity. It arises naturally, without any sparsity constraints or parameter-efficient tuning, and appears across multiple RL algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI, Meta, and open-source LLMs). Moreover, the subnetworks updated by RL show substantial overlap across different seeds, datasets, and algorithms-far exceeding chance-suggesting a partially transferable structure in the pretrained model. We show that fine-tuning only this sparse subnetwork recovers full model performance and yields parameters nearly identical to the fully fine-tuned model. Our analysis suggests this sparsity emerges because RL operates near the model's original distribution, requiring only targeted changes. KL penalties, gradient clipping, and on-policy dynamics have limited effect on the sparsity pattern. These findings shed new light on how RL adapts models: not by shifting all weights, but by focusing training on a small, consistently updated subnetwork. This insight enables more efficient RL methods and reframes sparsity through the lens of the lottery ticket hypothesis.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM Inference Serving</title>
<link>https://arxiv.org/abs/2507.17120</link>
<guid>https://arxiv.org/abs/2507.17120</guid>
<content:encoded><![CDATA[

arXiv:2507.17120v1 Announce Type: cross 
Abstract: Large language models (LLMs) have become increasingly popular in various areas, traditional business gradually shifting from rule-based systems to LLM-based solutions. However, the inference of LLMs is resource-intensive or latency-sensitive, posing significant challenges for serving systems. Existing LLM serving systems often use static or continuous batching strategies, which can lead to inefficient GPU memory utilization and increased latency, especially under heterogeneous workloads. These methods may also struggle to adapt to dynamic workload fluctuations, resulting in suboptimal throughput and potential service level objective (SLO) violations. In this paper, we introduce BucketServe, a bucket-based dynamic batching framework designed to optimize LLM inference performance. By grouping requests into size-homogeneous buckets based on sequence length, BucketServe minimizes padding overhead and optimizes GPU memory usage through real-time batch size adjustments preventing out-of-memory (OOM) errors. It introduces adaptive bucket splitting/merging and priority-aware scheduling to mitigate resource fragmentation and ensure SLO compliance. Experiment shows that BucketServe significantly outperforms UELLM in throughput, achieving up to 3.58x improvement. It can also handle 1.93x more request load under the SLO attainment of 80% compared with DistServe and demonstrates 1.975x higher system load capacity compared to the UELLM.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance</title>
<link>https://arxiv.org/abs/2507.17131</link>
<guid>https://arxiv.org/abs/2507.17131</guid>
<content:encoded><![CDATA[

arXiv:2507.17131v1 Announce Type: cross 
Abstract: Large language model (LLM) agents often struggle in environments where rules and required domain knowledge frequently change, such as regulatory compliance and user risk screening. Current approaches, like offline fine-tuning and standard prompting, are insufficient because they cannot effectively adapt to new knowledge during actual operation. To address this limitation, we propose the Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework designed specifically to continuously learn updated domain knowledge at test time. ARIA assesses its own uncertainty through structured self-dialogue, proactively identifying knowledge gaps and requesting targeted explanations or corrections from human experts. It then systematically updates an internal, timestamped knowledge repository with provided human guidance, detecting and resolving conflicting or outdated knowledge through comparisons and clarification queries. We evaluate ARIA on the realistic customer due diligence name screening task on TikTok Pay, alongside publicly available dynamic knowledge tasks. Results demonstrate significant improvements in adaptability and accuracy compared to baselines using standard offline fine-tuning and existing self-improving agents. ARIA is deployed within TikTok Pay serving over 150 million monthly active users, confirming its practicality and effectiveness for operational use in rapidly evolving environments.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resilient Multi-Agent Negotiation for Medical Supply Chains:Integrating LLMs and Blockchain for Transparent Coordination</title>
<link>https://arxiv.org/abs/2507.17134</link>
<guid>https://arxiv.org/abs/2507.17134</guid>
<content:encoded><![CDATA[

arXiv:2507.17134v1 Announce Type: cross 
Abstract: Global health emergencies, such as the COVID-19 pandemic, have exposed critical weaknesses in traditional medical supply chains, including inefficiencies in resource allocation, lack of transparency, and poor adaptability to dynamic disruptions. This paper presents a novel hybrid framework that integrates blockchain technology with a decentralized, large language model (LLM) powered multi-agent negotiation system to enhance the resilience and accountability of medical supply chains during crises. In this system, autonomous agents-representing manufacturers, distributors, and healthcare institutions-engage in structured, context-aware negotiation and decision-making processes facilitated by LLMs, enabling rapid and ethical allocation of scarce medical resources. The off-chain agent layer supports adaptive reasoning and local decision-making, while the on-chain blockchain layer ensures immutable, transparent, and auditable enforcement of decisions via smart contracts. The framework also incorporates a formal cross-layer communication protocol to bridge decentralized negotiation with institutional enforcement. A simulation environment emulating pandemic scenarios evaluates the system's performance, demonstrating improvements in negotiation efficiency, fairness of allocation, supply chain responsiveness, and auditability. This research contributes an innovative approach that synergizes blockchain trust guarantees with the adaptive intelligence of LLM-driven agents, providing a robust and scalable solution for critical supply chain coordination under uncertainty.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SADA: Stability-guided Adaptive Diffusion Acceleration</title>
<link>https://arxiv.org/abs/2507.17135</link>
<guid>https://arxiv.org/abs/2507.17135</guid>
<content:encoded><![CDATA[

arXiv:2507.17135v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable success in generative tasks but suffer from high computational costs due to their iterative sampling process and quadratic attention costs. Existing training-free acceleration strategies that reduce per-step computation cost, while effectively reducing sampling time, demonstrate low faithfulness compared to the original baseline. We hypothesize that this fidelity gap arises because (a) different prompts correspond to varying denoising trajectory, and (b) such methods do not consider the underlying ODE formulation and its numerical solution. In this paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a novel paradigm that unifies step-wise and token-wise sparsity decisions via a single stability criterion to accelerate sampling of ODE-based generative models (Diffusion and Flow-matching). For (a), SADA adaptively allocates sparsity based on the sampling trajectory. For (b), SADA introduces principled approximation schemes that leverage the precise gradient information from the numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to unmodified baselines, significantly outperforming prior methods. Moreover, SADA adapts seamlessly to other pipelines and modalities: It accelerates ControlNet without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim 0.01$ spectrogram LPIPS.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Human-level Intelligence via Human-like Whole-Body Manipulation</title>
<link>https://arxiv.org/abs/2507.17141</link>
<guid>https://arxiv.org/abs/2507.17141</guid>
<content:encoded><![CDATA[

arXiv:2507.17141v1 Announce Type: cross 
Abstract: Building general-purpose intelligent robots has long been a fundamental goal of robotics. A promising approach is to mirror the evolutionary trajectory of humans: learning through continuous interaction with the environment, with early progress driven by the imitation of human behaviors. Achieving this goal presents three core challenges: (1) designing safe robotic hardware with human-level physical capabilities; (2) developing an intuitive and scalable whole-body teleoperation interface for data collection; and (3) creating algorithms capable of learning whole-body visuomotor policies from human demonstrations. To address these challenges in a unified framework, we propose Astribot Suite, a robot learning suite for whole-body manipulation aimed at general daily tasks across diverse environments. We demonstrate the effectiveness of our system on a wide range of activities that require whole-body coordination, extensive reachability, human-level dexterity, and agility. Our results show that Astribot's cohesive integration of embodiment, teleoperation interface, and learning pipeline marks a significant step towards real-world, general-purpose whole-body robotic manipulation, laying the groundwork for the next generation of intelligent robots.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScSAM: Debiasing Morphology and Distributional Variability in Subcellular Semantic Segmentation</title>
<link>https://arxiv.org/abs/2507.17149</link>
<guid>https://arxiv.org/abs/2507.17149</guid>
<content:encoded><![CDATA[

arXiv:2507.17149v1 Announce Type: cross 
Abstract: The significant morphological and distributional variability among subcellular components poses a long-standing challenge for learning-based organelle segmentation models, significantly increasing the risk of biased feature learning. Existing methods often rely on single mapping relationships, overlooking feature diversity and thereby inducing biased training. Although the Segment Anything Model (SAM) provides rich feature representations, its application to subcellular scenarios is hindered by two key challenges: (1) The variability in subcellular morphology and distribution creates gaps in the label space, leading the model to learn spurious or biased features. (2) SAM focuses on global contextual understanding and often ignores fine-grained spatial details, making it challenging to capture subtle structural alterations and cope with skewed data distributions. To address these challenges, we introduce ScSAM, a method that enhances feature robustness by fusing pre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge to alleviate training bias from data imbalance. Specifically, we design a feature alignment and fusion module to align pre-trained embeddings to the same feature space and efficiently combine different representations. Moreover, we present a cosine similarity matrix-based class prompt encoder to activate class-specific features to recognize subcellular categories. Extensive experiments on diverse subcellular image datasets demonstrate that ScSAM outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JAM: Keypoint-Guided Joint Prediction after Classification-Aware Marginal Proposal for Multi-Agent Interaction</title>
<link>https://arxiv.org/abs/2507.17152</link>
<guid>https://arxiv.org/abs/2507.17152</guid>
<content:encoded><![CDATA[

arXiv:2507.17152v1 Announce Type: cross 
Abstract: Predicting the future motion of road participants is a critical task in autonomous driving. In this work, we address the challenge of low-quality generation of low-probability modes in multi-agent joint prediction. To tackle this issue, we propose a two-stage multi-agent interactive prediction framework named \textit{keypoint-guided joint prediction after classification-aware marginal proposal} (JAM). The first stage is modeled as a marginal prediction process, which classifies queries by trajectory type to encourage the model to learn all categories of trajectories, providing comprehensive mode information for the joint prediction module. The second stage is modeled as a joint prediction process, which takes the scene context and the marginal proposals from the first stage as inputs to learn the final joint distribution. We explicitly introduce key waypoints to guide the joint prediction module in better capturing and leveraging the critical information from the initial predicted trajectories. We conduct extensive experiments on the real-world Waymo Open Motion Dataset interactive prediction benchmark. The results show that our approach achieves competitive performance. In particular, in the framework comparison experiments, the proposed JAM outperforms other prediction frameworks and achieves state-of-the-art performance in interactive trajectory prediction. The code is available at https://github.com/LinFunster/JAM to facilitate future research.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2507.17161</link>
<guid>https://arxiv.org/abs/2507.17161</guid>
<content:encoded><![CDATA[

arXiv:2507.17161v1 Announce Type: cross 
Abstract: Modern network intrusion detection systems (NIDS) frequently utilize the predictive power of complex deep learning models. However, the "black-box" nature of such deep learning methods adds a layer of opaqueness that hinders the proper understanding of detection decisions, trust in the decisions and prevent timely countermeasures against such attacks. Explainable AI (XAI) methods provide a solution to this problem by providing insights into the causes of the predictions. The majority of the existing XAI methods provide explanations which are not convenient to convert into actionable countermeasures. In this work, we propose a novel diffusion-based counterfactual explanation framework that can provide actionable explanations for network intrusion attacks. We evaluated our proposed algorithm against several other publicly available counterfactual explanation algorithms on 3 modern network intrusion datasets. To the best of our knowledge, this work also presents the first comparative analysis of existing counterfactual explanation algorithms within the context of network intrusion detection systems. Our proposed method provide minimal, diverse counterfactual explanations out of the tested counterfactual explanation algorithms in a more efficient manner by reducing the time to generate explanations. We also demonstrate how counterfactual explanations can provide actionable explanations by summarizing them to create a set of global rules. These rules are actionable not only at instance level but also at the global level for intrusion attacks. These global counterfactual rules show the ability to effectively filter out incoming attack queries which is crucial for efficient intrusion detection and defense mechanisms.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs</title>
<link>https://arxiv.org/abs/2507.17178</link>
<guid>https://arxiv.org/abs/2507.17178</guid>
<content:encoded><![CDATA[

arXiv:2507.17178v1 Announce Type: cross 
Abstract: Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at https://github.com/Lza12a/SKA-Bench.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regret Minimization in Population Network Games: Vanishing Heterogeneity and Convergence to Equilibria</title>
<link>https://arxiv.org/abs/2507.17183</link>
<guid>https://arxiv.org/abs/2507.17183</guid>
<content:encoded><![CDATA[

arXiv:2507.17183v1 Announce Type: cross 
Abstract: Understanding and predicting the behavior of large-scale multi-agents in games remains a fundamental challenge in multi-agent systems. This paper examines the role of heterogeneity in equilibrium formation by analyzing how smooth regret-matching drives a large number of heterogeneous agents with diverse initial policies toward unified behavior. By modeling the system state as a probability distribution of regrets and analyzing its evolution through the continuity equation, we uncover a key phenomenon in diverse multi-agent settings: the variance of the regret distribution diminishes over time, leading to the disappearance of heterogeneity and the emergence of consensus among agents. This universal result enables us to prove convergence to quantal response equilibria in both competitive and cooperative multi-agent settings. Our work advances the theoretical understanding of multi-agent learning and offers a novel perspective on equilibrium selection in diverse game-theoretic scenarios.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymmetric Lesion Detection with Geometric Patterns and CNN-SVM Classification</title>
<link>https://arxiv.org/abs/2507.17185</link>
<guid>https://arxiv.org/abs/2507.17185</guid>
<content:encoded><![CDATA[

arXiv:2507.17185v1 Announce Type: cross 
Abstract: In dermoscopic images, which allow visualization of surface skin structures not visible to the naked eye, lesion shape offers vital insights into skin diseases. In clinically practiced methods, asymmetric lesion shape is one of the criteria for diagnosing melanoma. Initially, we labeled data for a non-annotated dataset with symmetrical information based on clinical assessments. Subsequently, we propose a supporting technique, a supervised learning image processing algorithm, to analyze the geometrical pattern of lesion shape, aiding non-experts in understanding the criteria of an asymmetric lesion. We then utilize a pre-trained convolutional neural network (CNN) to extract shape, color, and texture features from dermoscopic images for training a multiclass support vector machine (SVM) classifier, outperforming state-of-the-art methods from the literature. In the geometry-based experiment, we achieved a 99.00% detection rate for dermatological asymmetric lesions. In the CNN-based experiment, the best performance is found with 94% Kappa Score, 95% Macro F1-score, and 97% Weighted F1-score for classifying lesion shapes (Asymmetric, Half-Symmetric, and Symmetric).
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks</title>
<link>https://arxiv.org/abs/2507.17188</link>
<guid>https://arxiv.org/abs/2507.17188</guid>
<content:encoded><![CDATA[

arXiv:2507.17188v1 Announce Type: cross 
Abstract: This work tackles the physical layer security (PLS) problem of maximizing the secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy constraints. Unlike prior studies that assume uniform UAV capabilities or overlook energy-security trade-offs, we consider a realistic scenario where UAVs with diverse payloads and computation resources collaborate to serve ground terminals in the presence of eavesdroppers. To manage the complex coupling between UAV motion and communication, we propose a hierarchical optimization framework. The inner layer uses a semidefinite relaxation (SDR)-based S2DC algorithm combining penalty functions and difference-of-convex (d.c.) programming to solve the secrecy precoding problem with fixed UAV positions. The outer layer introduces a Large Language Model (LLM)-guided heuristic multi-agent reinforcement learning approach (LLM-HeMARL) for trajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics policy generated by the LLM, enabling UAVs to learn energy-aware, security-driven trajectories without the inference overhead of real-time LLM calls. The simulation results show that our method outperforms existing baselines in secrecy rate and energy efficiency, with consistent robustness across varying UAV swarm sizes and random seeds.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dispatch-Aware Deep Neural Network for Optimal Transmission Switching: Toward Real-Time and Feasibility Guaranteed Operation</title>
<link>https://arxiv.org/abs/2507.17194</link>
<guid>https://arxiv.org/abs/2507.17194</guid>
<content:encoded><![CDATA[

arXiv:2507.17194v1 Announce Type: cross 
Abstract: Optimal transmission switching (OTS) improves optimal power flow (OPF) by selectively opening transmission lines, but its mixed-integer formulation increases computational complexity, especially on large grids. To deal with this, we propose a dispatch-aware deep neural network (DA-DNN) that accelerates DC-OTS without relying on pre-solved labels. DA-DNN predicts line states and passes them through a differentiable DC-OPF layer, using the resulting generation cost as the loss function so that all physical network constraints are enforced throughout training and inference. In addition, we adopt a customized weight-bias initialization that keeps every forward pass feasible from the first iteration, which allows stable learning on large grids. Once trained, the proposed DA-DNN produces a provably feasible topology and dispatch pair in the same time as solving the DCOPF, whereas conventional mixed-integer solvers become intractable. As a result, the proposed method successfully captures the economic advantages of OTS while maintaining scalability.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DesignLab: Designing Slides Through Iterative Detection and Correction</title>
<link>https://arxiv.org/abs/2507.17202</link>
<guid>https://arxiv.org/abs/2507.17202</guid>
<content:encoded><![CDATA[

arXiv:2507.17202v1 Announce Type: cross 
Abstract: Designing high-quality presentation slides can be challenging for non-experts due to the complexity involved in navigating various design choices. Numerous automated tools can suggest layouts and color schemes, yet often lack the ability to refine their own output, which is a key aspect in real-world workflows. We propose DesignLab, which separates the design process into two roles, the design reviewer, who identifies design-related issues, and the design contributor who corrects them. This decomposition enables an iterative loop where the reviewer continuously detects issues and the contributor corrects them, allowing a draft to be further polished with each iteration, reaching qualities that were unattainable. We fine-tune large language models for these roles and simulate intermediate drafts by introducing controlled perturbations, enabling the design reviewer learn design errors and the contributor learn how to fix them. Our experiments show that DesignLab outperforms existing design-generation methods, including a commercial tool, by embracing the iterative nature of designing which can result in polished, professional slides.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models</title>
<link>https://arxiv.org/abs/2507.17216</link>
<guid>https://arxiv.org/abs/2507.17216</guid>
<content:encoded><![CDATA[

arXiv:2507.17216v1 Announce Type: cross 
Abstract: People increasingly rely on Large Language Models (LLMs) for moral advice, which may influence humans' decisions. Yet, little is known about how closely LLMs align with human moral judgments. To address this, we introduce the Moral Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a distribution of human moral judgments consisting of a binary evaluation and a free-text rationale. We treat this problem as a pluralistic distributional alignment task, comparing the distributions of LLM and human judgments across dilemmas. We find that models reproduce human judgments only under high consensus; alignment deteriorates sharply when human disagreement increases. In parallel, using a 60-value taxonomy built from 3,783 value expressions extracted from rationales, we show that LLMs rely on a narrower set of moral values than humans. These findings reveal a pluralistic moral gap: a mismatch in both the distribution and diversity of values expressed. To close this gap, we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method that conditions model outputs on human-derived value profiles. DMP improves alignment by 64.3% and enhances value diversity, offering a step toward more pluralistic and human-aligned moral guidance from LLMs.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HuiduRep: A Robust Self-Supervised Framework for Learning Neural Representations from Extracellular Spikes</title>
<link>https://arxiv.org/abs/2507.17224</link>
<guid>https://arxiv.org/abs/2507.17224</guid>
<content:encoded><![CDATA[

arXiv:2507.17224v1 Announce Type: cross 
Abstract: Extracellular recordings are brief voltage fluctuations recorded near neurons, widely used in neuroscience as the basis for decoding brain activity at single-neuron resolution. Spike sorting, which assigns each spike to its source neuron, is a critical step in brain sensing pipelines. However, it remains challenging under low signal-to-noise ratio (SNR), electrode drift, and cross-session variability. In this paper, we propose HuiduRep, a robust self-supervised representation learning framework that extracts discriminative and generalizable features from extracellular spike waveforms. By combining contrastive learning with a denoising autoencoder, HuiduRep learns latent representations that are robust to noise and drift. Built on HuiduRep, we develop a spike sorting pipeline that clusters spike representations without supervision. Experiments on hybrid and real-world datasets demonstrate that HuiduRep achieves strong robustness and the pipeline matches or outperforms state-of-the-art tools such as KiloSort4 and MountainSort5. These findings demonstrate the potential of self-supervised spike representation learning as a foundational tool for robust and generalizable processing of extracellular recordings.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices</title>
<link>https://arxiv.org/abs/2507.17228</link>
<guid>https://arxiv.org/abs/2507.17228</guid>
<content:encoded><![CDATA[

arXiv:2507.17228v1 Announce Type: cross 
Abstract: Split Learning (SL) is an emerging privacy-preserving machine learning technique that enables resource constrained edge devices to participate in model training by partitioning a model into client-side and server-side sub-models. While SL reduces computational overhead on edge devices, it encounters significant challenges in heterogeneous environments where devices vary in computing resources, communication capabilities, environmental conditions, and privacy requirements. Although recent studies have explored heterogeneous SL frameworks that optimize split points for devices with varying resource constraints, they often neglect personalized privacy requirements and local model customization under varying environmental conditions. To address these limitations, we propose P3SL, a Personalized Privacy-Preserving Split Learning framework designed for heterogeneous, resource-constrained edge device systems. The key contributions of this work are twofold. First, we design a personalized sequential split learning pipeline that allows each client to achieve customized privacy protection and maintain personalized local models tailored to their computational resources, environmental conditions, and privacy needs. Second, we adopt a bi-level optimization technique that empowers clients to determine their own optimal personalized split points without sharing private sensitive information (i.e., computational resources, environmental conditions, privacy requirements) with the server. This approach balances energy consumption and privacy leakage risks while maintaining high model accuracy. We implement and evaluate P3SL on a testbed consisting of 7 devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop, using diverse model architectures and datasets under varying environmental conditions.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task</title>
<link>https://arxiv.org/abs/2507.17232</link>
<guid>https://arxiv.org/abs/2507.17232</guid>
<content:encoded><![CDATA[

arXiv:2507.17232v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are trained on a vast amount of procedural texts, but they do not directly observe real-world phenomena. In the context of cooking recipes, this poses a challenge, as intermediate states of ingredients are often omitted, making it difficult for models to track ingredient states and understand recipes accurately. In this paper, we apply state probing, a method for evaluating a language model's understanding of the world, to the domain of cooking. We propose a new task and dataset for evaluating how well LLMs can recognize intermediate ingredient states during cooking procedures. We first construct a new Japanese recipe dataset with clear and accurate annotations of ingredient state changes, collected from well-structured and controlled recipe texts. Using this dataset, we design three novel tasks to evaluate whether LLMs can track ingredient state transitions and identify ingredients present at intermediate steps. Our experiments with widely used LLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state knowledge improves their understanding of cooking processes, achieving performance comparable to commercial LLMs.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eco-Friendly AI: Unleashing Data Power for Green Federated Learning</title>
<link>https://arxiv.org/abs/2507.17241</link>
<guid>https://arxiv.org/abs/2507.17241</guid>
<content:encoded><![CDATA[

arXiv:2507.17241v1 Announce Type: cross 
Abstract: The widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) comes with a significant environmental impact, particularly in terms of energy consumption and carbon emissions. This pressing issue highlights the need for innovative solutions to mitigate AI's ecological footprint. One of the key factors influencing the energy consumption of ML model training is the size of the training dataset. ML models are often trained on vast amounts of data continuously generated by sensors and devices distributed across multiple locations. To reduce data transmission costs and enhance privacy, Federated Learning (FL) enables model training without the need to move or share raw data. While FL offers these advantages, it also introduces challenges due to the heterogeneity of data sources (related to volume and quality), computational node capabilities, and environmental impact.
  This paper contributes to the advancement of Green AI by proposing a data-centric approach to Green Federated Learning. Specifically, we focus on reducing FL's environmental impact by minimizing the volume of training data. Our methodology involves the analysis of the characteristics of federated datasets, the selecting of an optimal subset of data based on quality metrics, and the choice of the federated nodes with the lowest environmental impact. We develop a comprehensive methodology that examines the influence of data-centric factors, such as data quality and volume, on FL training performance and carbon emissions. Building on these insights, we introduce an interactive recommendation system that optimizes FL configurations through data reduction, minimizing environmental impact during training. Applying this methodology to time series classification has demonstrated promising results in reducing the environmental impact of FL tasks.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DistrAttention: An Efficient and Flexible Self-Attention Mechanism on Modern GPUs</title>
<link>https://arxiv.org/abs/2507.17245</link>
<guid>https://arxiv.org/abs/2507.17245</guid>
<content:encoded><![CDATA[

arXiv:2507.17245v1 Announce Type: cross 
Abstract: The Transformer architecture has revolutionized deep learning, delivering the state-of-the-art performance in areas such as natural language processing, computer vision, and time series prediction. However, its core component, self-attention, has the quadratic time complexity relative to input sequence length, which hinders the scalability of Transformers. The exsiting approaches on optimizing self-attention either discard full-contextual information or lack of flexibility. In this work, we design DistrAttention, an effcient and flexible self-attention mechanism with the full context. DistrAttention achieves this by grouping data on the embedding dimensionality, usually referred to as $d$. We realize DistrAttention with a lightweight sampling and fusion method that exploits locality-sensitive hashing to group similar data. A block-wise grouping framework is further designed to limit the errors introduced by locality sensitive hashing. By optimizing the selection of block sizes, DistrAttention could be easily integrated with FlashAttention-2, gaining high-performance on modern GPUs. We evaluate DistrAttention with extensive experiments. The results show that our method is 37% faster than FlashAttention-2 on calculating self-attention. In ViT inference, DistrAttention is the fastest and the most accurate among approximate self-attention mechanisms. In Llama3-1B, DistrAttention still achieves the lowest inference time with only 1% accuray loss.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations</title>
<link>https://arxiv.org/abs/2507.17248</link>
<guid>https://arxiv.org/abs/2507.17248</guid>
<content:encoded><![CDATA[

arXiv:2507.17248v1 Announce Type: cross 
Abstract: Interacting with real-world objects in Mixed Reality (MR) often proves difficult when they are crowded, distant, or partially occluded, hindering straightforward selection and manipulation. We observe that these difficulties stem from performing interaction directly on physical objects, where input is tightly coupled to their physical constraints. Our key insight is to decouple interaction from these constraints by introducing proxies-abstract representations of real-world objects. We embody this concept in Reality Proxy, a system that seamlessly shifts interaction targets from physical objects to their proxies during selection. Beyond facilitating basic selection, Reality Proxy uses AI to enrich proxies with semantic attributes and hierarchical spatial relationships of their corresponding physical objects, enabling novel and previously cumbersome interactions in MR - such as skimming, attribute-based filtering, navigating nested groups, and complex multi object selections - all without requiring new gestures or menu systems. We demonstrate Reality Proxy's versatility across diverse scenarios, including office information retrieval, large-scale spatial navigation, and multi-drone control. An expert evaluation suggests the system's utility and usability, suggesting that proxy-based abstractions offer a powerful and generalizable interaction paradigm for future MR systems.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Prompt Programming Tasks and Questions</title>
<link>https://arxiv.org/abs/2507.17264</link>
<guid>https://arxiv.org/abs/2507.17264</guid>
<content:encoded><![CDATA[

arXiv:2507.17264v1 Announce Type: cross 
Abstract: Prompting foundation models (FMs) like large language models (LLMs) have enabled new AI-powered software features (e.g., text summarization) that previously were only possible by fine-tuning FMs. Now, developers are embedding prompts in software, known as prompt programs. The process of prompt programming requires the developer to make many changes to their prompt. Yet, the questions developers ask to update their prompt is unknown, despite the answers to these questions affecting how developers plan their changes. With the growing number of research and commercial prompt programming tools, it is unclear whether prompt programmers' needs are being adequately addressed. We address these challenges by developing a taxonomy of 25 tasks prompt programmers do and 51 questions they ask, measuring the importance of each task and question. We interview 16 prompt programmers, observe 8 developers make prompt changes, and survey 50 developers. We then compare the taxonomy with 48 research and commercial tools. We find that prompt programming is not well-supported: all tasks are done manually, and 16 of the 51 questions -- including a majority of the most important ones -- remain unanswered. Based on this, we outline important opportunities for prompt programming tools.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational Bottlenecks for Warehouse Planning Assistance</title>
<link>https://arxiv.org/abs/2507.17273</link>
<guid>https://arxiv.org/abs/2507.17273</guid>
<content:encoded><![CDATA[

arXiv:2507.17273v1 Announce Type: cross 
Abstract: Analyzing large, complex output datasets from Discrete Event Simulations (DES) of warehouse operations to identify bottlenecks and inefficiencies is a critical yet challenging task, often demanding significant manual effort or specialized analytical tools. Our framework integrates Knowledge Graphs (KGs) and Large Language Model (LLM)-based agents to analyze complex Discrete Event Simulation (DES) output data from warehouse operations. It transforms raw DES data into a semantically rich KG, capturing relationships between simulation events and entities. An LLM-based agent uses iterative reasoning, generating interdependent sub-questions. For each sub-question, it creates Cypher queries for KG interaction, extracts information, and self-reflects to correct errors. This adaptive, iterative, and self-correcting process identifies operational issues mimicking human analysis. Our DES approach for warehouse bottleneck identification, tested with equipment breakdowns and process irregularities, outperforms baseline methods. For operational questions, it achieves near-perfect pass rates in pinpointing inefficiencies. For complex investigative questions, we demonstrate its superior diagnostic ability to uncover subtle, interconnected issues. This work bridges simulation modeling and AI (KG+LLM), offering a more intuitive method for actionable insights, reducing time-to-insight, and enabling automated warehouse inefficiency evaluation and diagnosis.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Belief Domains into Probabilistic Logic Programs</title>
<link>https://arxiv.org/abs/2507.17291</link>
<guid>https://arxiv.org/abs/2507.17291</guid>
<content:encoded><![CDATA[

arXiv:2507.17291v1 Announce Type: cross 
Abstract: Probabilistic Logic Programming (PLP) under the Distribution Semantics is a leading approach to practical reasoning under uncertainty. An advantage of the Distribution Semantics is its suitability for implementation as a Prolog or Python library, available through two well-maintained implementations, namely ProbLog and cplint/PITA. However, current formulations of the Distribution Semantics use point-probabilities, making it difficult to express epistemic uncertainty, such as arises from, for example, hierarchical classifications from computer vision models. Belief functions generalize probability measures as non-additive capacities, and address epistemic uncertainty via interval probabilities. This paper introduces interval-based Capacity Logic Programs based on an extension of the Distribution Semantics to include belief functions, and describes properties of the new framework that make it amenable to practical applications.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Temporal Guidance and Iterative Refinement in Audio Source Separation</title>
<link>https://arxiv.org/abs/2507.17297</link>
<guid>https://arxiv.org/abs/2507.17297</guid>
<content:encoded><![CDATA[

arXiv:2507.17297v1 Announce Type: cross 
Abstract: Spatial semantic segmentation of sound scenes (S5) involves the accurate identification of active sound classes and the precise separation of their sources from complex acoustic mixtures. Conventional systems rely on a two-stage pipeline - audio tagging followed by label-conditioned source separation - but are often constrained by the absence of fine-grained temporal information critical for effective separation. In this work, we address this limitation by introducing a novel approach for S5 that enhances the synergy between the event detection and source separation stages. Our key contributions are threefold. First, we fine-tune a pre-trained Transformer to detect active sound classes. Second, we utilize a separate instance of this fine-tuned Transformer to perform sound event detection (SED), providing the separation module with detailed, time-varying guidance. Third, we implement an iterative refinement mechanism that progressively enhances separation quality by recursively reusing the separator's output from previous iterations. These advancements lead to significant improvements in both audio tagging and source separation performance, as demonstrated by our system's second-place finish in Task 4 of the DCASE Challenge 2025. Our implementation and model checkpoints are available in our GitHub repository: https://github.com/theMoro/dcase25task4 .
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2507.17303</link>
<guid>https://arxiv.org/abs/2507.17303</guid>
<content:encoded><![CDATA[

arXiv:2507.17303v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have emerged as powerful tools for computational pathology, offering unprecedented opportunities to integrate pathological images with language context for comprehensive diagnostic analysis. These models hold particular promise for automating complex tasks that traditionally require expert interpretation of pathologists. However, current MLLM approaches in pathology demonstrate significantly constrained reasoning capabilities, primarily due to their reliance on expensive chain-of-thought annotations. Additionally, existing methods remain limited to simplex application of visual question answering (VQA) at region-of-interest (ROI) level, failing to address the full spectrum of diagnostic needs such as ROI classification, detection, segmentation, whole-slide-image (WSI) classification and VQA in clinical practice. In this study, we present SmartPath-R1, a versatile MLLM capable of simultaneously addressing both ROI-level and WSI-level tasks while demonstrating robust pathological reasoning capability. Our framework combines scale-dependent supervised fine-tuning and task-aware reinforcement fine-tuning, which circumvents the requirement for chain-of-thought supervision by leveraging the intrinsic knowledge within MLLM. Furthermore, SmartPath-R1 integrates multiscale and multitask analysis through a mixture-of-experts mechanism, enabling dynamic processing for diverse tasks. We curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI samples for training and evaluation. Extensive experiments across 72 tasks validate the effectiveness and superiority of the proposed approach. This work represents a significant step toward developing versatile, reasoning-enhanced AI systems for precision pathology.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confounded Causal Imitation Learning with Instrumental Variables</title>
<link>https://arxiv.org/abs/2507.17309</link>
<guid>https://arxiv.org/abs/2507.17309</guid>
<content:encoded><![CDATA[

arXiv:2507.17309v1 Announce Type: cross 
Abstract: Imitation learning from demonstrations usually suffers from the confounding effects of unmeasured variables (i.e., unmeasured confounders) on the states and actions. If ignoring them, a biased estimation of the policy would be entailed. To break up this confounding gap, in this paper, we take the best of the strong power of instrumental variables (IV) and propose a Confounded Causal Imitation Learning (C2L) model. This model accommodates confounders that influence actions across multiple timesteps, rather than being restricted to immediate temporal dependencies. We develop a two-stage imitation learning framework for valid IV identification and policy optimization. In particular, in the first stage, we construct a testing criterion based on the defined pseudo-variable, with which we achieve identifying a valid IV for the C2L models. Such a criterion entails the sufficient and necessary identifiability conditions for IV validity. In the second stage, with the identified IV, we propose two candidate policy learning approaches: one is based on a simulator, while the other is offline. Extensive experiments verified the effectiveness of identifying the valid IV as well as learning the policy.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EarthLink: Interpreting Climate Signals with Self-Evolving AI Agents</title>
<link>https://arxiv.org/abs/2507.17311</link>
<guid>https://arxiv.org/abs/2507.17311</guid>
<content:encoded><![CDATA[

arXiv:2507.17311v1 Announce Type: cross 
Abstract: Modern Earth science is at an inflection point. The vast, fragmented, and complex nature of Earth system data, coupled with increasingly sophisticated analytical demands, creates a significant bottleneck for rapid scientific discovery. Here we introduce EarthLink, the first AI agent designed as an interactive copilot for Earth scientists. It automates the end-to-end research workflow, from planning and code generation to multi-scenario analysis. Unlike static diagnostic tools, EarthLink can learn from user interaction, continuously refining its capabilities through a dynamic feedback loop. We validated its performance on a number of core scientific tasks of climate change, ranging from model-observation comparisons to the diagnosis of complex phenomena. In a multi-expert evaluation, EarthLink produced scientifically sound analyses and demonstrated an analytical competency that was rated as comparable to specific aspects of a human junior researcher's workflow. Additionally, its transparent, auditable workflows and natural language interface empower scientists to shift from laborious manual execution to strategic oversight and hypothesis generation. EarthLink marks a pivotal step towards an efficient, trustworthy, and collaborative paradigm for Earth system research in an era of accelerating global change.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free Framework for Weak Moving Target Detection</title>
<link>https://arxiv.org/abs/2507.17334</link>
<guid>https://arxiv.org/abs/2507.17334</guid>
<content:encoded><![CDATA[

arXiv:2507.17334v1 Announce Type: cross 
Abstract: In low-altitude surveillance and early warning systems, detecting weak moving targets remains a significant challenge due to low signal energy, small spatial extent, and complex background clutter. Existing methods struggle with extracting robust features and suffer from the lack of reliable annotations. To address these limitations, we propose a novel Temporal Point-Supervised (TPS) framework that enables high-performance detection of weak targets without any manual annotations.Instead of conventional frame-based detection, our framework reformulates the task as a pixel-wise temporal signal modeling problem, where weak targets manifest as short-duration pulse-like responses. A Temporal Signal Reconstruction Network (TSRNet) is developed under the TPS paradigm to reconstruct these transient signals.TSRNet adopts an encoder-decoder architecture and integrates a Dynamic Multi-Scale Attention (DMSAttention) module to enhance its sensitivity to diverse temporal patterns. Additionally, a graph-based trajectory mining strategy is employed to suppress false alarms and ensure temporal consistency.Extensive experiments on a purpose-built low-SNR dataset demonstrate that our framework outperforms state-of-the-art methods while requiring no human annotations. It achieves strong detection performance and operates at over 1000 FPS, underscoring its potential for real-time deployment in practical scenarios.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Swin-TUNA : A Novel PEFT Approach for Accurate Food Image Segmentation</title>
<link>https://arxiv.org/abs/2507.17347</link>
<guid>https://arxiv.org/abs/2507.17347</guid>
<content:encoded><![CDATA[

arXiv:2507.17347v1 Announce Type: cross 
Abstract: In the field of food image processing, efficient semantic segmentation techniques are crucial for industrial applications. However, existing large-scale Transformer-based models (such as FoodSAM) face challenges in meeting practical deploymentrequirements due to their massive parameter counts and high computational resource demands. This paper introduces TUNable Adapter module (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that integrates multiscale trainable adapters into the Swin Transformer architecture, achieving high-performance food image segmentation by updating only 4% of the parameters. The core innovation of Swin-TUNA lies in its hierarchical feature adaptation mechanism: it designs separable convolutions in depth and dimensional mappings of varying scales to address the differences in features between shallow and deep networks, combined with a dynamic balancing strategy for tasks-agnostic and task-specific features. Experiments demonstrate that this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and UECFoodPix Complete datasets, respectively, surpassing the fully parameterized FoodSAM model while reducing the parameter count by 98.7% (to only 8.13M). Furthermore, Swin-TUNA exhibits faster convergence and stronger generalization capabilities in low-data scenarios, providing an efficient solution for assembling lightweight food image.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.17365</link>
<guid>https://arxiv.org/abs/2507.17365</guid>
<content:encoded><![CDATA[

arXiv:2507.17365v1 Announce Type: cross 
Abstract: Multi-step agentic retrieval systems based on large language models (LLMs) have demonstrated remarkable performance in complex information search tasks. However, these systems still face significant challenges in practical applications, particularly in generating factually inconsistent intermediate queries and inefficient search trajectories, which can lead to reasoning deviations or redundant computations. To address these issues, we propose DynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs and multi-reward reinforcement learning (RL). Specifically, our system leverages knowledge graphs as external structured knowledge to guide the search process by explicitly modeling entity relationships, thereby ensuring factual consistency in intermediate queries and mitigating biases from irrelevant information. Furthermore, we employ a multi-reward RL framework for fine-grained control over training objectives such as retrieval accuracy, efficiency, and response quality. This framework promotes the generation of high-quality intermediate queries and comprehensive final answers, while discouraging unnecessary exploration and minimizing information omissions or redundancy. Experimental results demonstrate that our approach achieves state-of-the-art answer accuracy on six multi-hop question answering datasets, matching frontier LLMs while using only small-scale models and limited computational resources. Furthermore, our approach demonstrates strong generalization and robustness across diverse retrieval environments and larger-scale models, highlighting its broad applicability.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFUOD: Source-Free Unknown Object Detection</title>
<link>https://arxiv.org/abs/2507.17373</link>
<guid>https://arxiv.org/abs/2507.17373</guid>
<content:encoded><![CDATA[

arXiv:2507.17373v1 Announce Type: cross 
Abstract: Source-free object detection adapts a detector pre-trained on a source domain to an unlabeled target domain without requiring access to labeled source data. While this setting is practical as it eliminates the need for the source dataset during domain adaptation, it operates under the restrictive assumption that only pre-defined objects from the source domain exist in the target domain. This closed-set setting prevents the detector from detecting undefined objects. To ease this assumption, we propose Source-Free Unknown Object Detection (SFUOD), a novel scenario which enables the detector to not only recognize known objects but also detect undefined objects as unknown objects. To this end, we propose CollaPAUL (Collaborative tuning and Principal Axis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning enhances knowledge adaptation by integrating target-dependent knowledge from the auxiliary encoder with source-dependent knowledge from the pre-trained detector through a cross-domain attention mechanism. Additionally, principal axes-based unknown labeling assigns pseudo-labels to unknown objects by estimating objectness via principal axes projection and confidence scores from model predictions. The proposed CollaPAUL achieves state-of-the-art performances on SFUOD benchmarks, and extensive experiments validate its effectiveness.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Training Data Detection in AI Coders</title>
<link>https://arxiv.org/abs/2507.17389</link>
<guid>https://arxiv.org/abs/2507.17389</guid>
<content:encoded><![CDATA[

arXiv:2507.17389v1 Announce Type: cross 
Abstract: Recent advances in code large language models (CodeLLMs) have made them indispensable tools in modern software engineering. However, these models occasionally produce outputs that contain proprietary or sensitive code snippets, raising concerns about potential non-compliant use of training data, and posing risks to privacy and intellectual property. To ensure responsible and compliant deployment of CodeLLMs, training data detection (TDD) has become a critical task. While recent TDD methods have shown promise in natural language settings, their effectiveness on code data remains largely underexplored. This gap is particularly important given code's structured syntax and distinct similarity criteria compared to natural language. To address this, we conduct a comprehensive empirical study of seven state-of-the-art TDD methods on source code data, evaluating their performance across eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a function-level benchmark dataset comprising 9,000 code samples in three programming languages, each explicitly labeled as either included or excluded from CodeLLM training. Beyond evaluation on the original CodeSnitch, we design targeted mutation strategies to test the robustness of TDD methods under three distinct settings. These mutation strategies are grounded in the well-established Type-1 to Type-4 code clone detection taxonomy. Our study provides a systematic assessment of current TDD techniques for code and offers insights to guide the development of more effective and robust detection methods in the future.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in Tuning-Free Multimodal LLMs</title>
<link>https://arxiv.org/abs/2507.17394</link>
<guid>https://arxiv.org/abs/2507.17394</guid>
<content:encoded><![CDATA[

arXiv:2507.17394v1 Announce Type: cross 
Abstract: Video Anomaly Detection (VAD) aims to identify and locate deviations from normal patterns in video sequences. Traditional methods often struggle with substantial computational demands and a reliance on extensive labeled datasets, thereby restricting their practical applicability. To address these constraints, we propose HiProbe-VAD, a novel framework that leverages pre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring fine-tuning. In this paper, we discover that the intermediate hidden states of MLLMs contain information-rich representations, exhibiting higher sensitivity and linear separability for anomalies compared to the output layer. To capitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP) mechanism that intelligently identifies and extracts the most informative hidden states from the optimal intermediate layer during the MLLMs reasoning. Then a lightweight anomaly scorer and temporal localization module efficiently detects anomalies using these extracted hidden states and finally generate explanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate that HiProbe-VAD outperforms existing training-free and most traditional approaches. Furthermore, our framework exhibits remarkable cross-model generalization capabilities in different MLLMs without any tuning, unlocking the potential of pre-trained MLLMs for video anomaly detection and paving the way for more practical and scalable solutions.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents</title>
<link>https://arxiv.org/abs/2507.17399</link>
<guid>https://arxiv.org/abs/2507.17399</guid>
<content:encoded><![CDATA[

arXiv:2507.17399v1 Announce Type: cross 
Abstract: Recent studies have explored graph-based approaches to retrieval-augmented generation, leveraging structured or semi-structured information -- such as entities and their relations extracted from documents -- to enhance retrieval. However, these methods are typically designed to address specific tasks, such as multi-hop question answering and query-focused summarisation, and therefore, there is limited evidence of their general applicability across broader datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG solution: $\text{GeAR}$ and explore its performance and limitations on the SIGIR 2025 LiveRAG Challenge.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging</title>
<link>https://arxiv.org/abs/2507.17412</link>
<guid>https://arxiv.org/abs/2507.17412</guid>
<content:encoded><![CDATA[

arXiv:2507.17412v1 Announce Type: cross 
Abstract: The increasing volume of medical images poses challenges for radiologists in retrieving relevant cases. Content-based image retrieval (CBIR) systems offer potential for efficient access to similar cases, yet lack standardized evaluation and comprehensive studies. Building on prior studies for tumor characterization via CBIR, this study advances CBIR research for volumetric medical images through three key contributions: (1) a framework eliminating reliance on pre-segmented data and organ-specific datasets, aligning with large and unstructured image archiving systems, i.e. PACS in clinical practice; (2) introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's contextualized late interaction mechanism for 3D medical imaging; (3) comprehensive evaluation across four tumor sites using three feature extractors and three database configurations. Our evaluations highlight the significant advantages of C-MIR. We demonstrate the successful adaptation of the late interaction principle to volumetric medical images, enabling effective context-aware re-ranking. A key finding is C-MIR's ability to effectively localize the region of interest, eliminating the need for pre-segmentation of datasets and offering a computationally efficient alternative to systems relying on expensive data enrichment steps. C-MIR demonstrates promising improvements in tumor flagging, achieving improved performance, particularly for colon and lung tumors (p<0.05). C-MIR also shows potential for improving tumor staging, warranting further exploration of its capabilities. Ultimately, our work seeks to bridge the gap between advanced retrieval techniques and their practical applications in healthcare, paving the way for improved diagnostic processes.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Compromises in Participatory Budgeting: a Multi-Agent Deep Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2507.17433</link>
<guid>https://arxiv.org/abs/2507.17433</guid>
<content:encoded><![CDATA[

arXiv:2507.17433v1 Announce Type: cross 
Abstract: Participatory budgeting is a method of collectively understanding and addressing spending priorities where citizens vote on how a budget is spent, it is regularly run to improve the fairness of the distribution of public funds. Participatory budgeting requires voters to make decisions on projects which can lead to ``choice overload". A multi-agent reinforcement learning approach to decision support can make decision making easier for voters by identifying voting strategies that increase the winning proportion of their vote. This novel approach can also support policymakers by highlighting aspects of election design that enable fair compromise on projects. This paper presents a novel, ethically aligned approach to decision support using multi-agent deep reinforcement learning modelling. This paper introduces a novel use of a branching neural network architecture to overcome scalability challenges of multi-agent reinforcement learning in a decentralized way. Fair compromises are found through optimising voter actions towards greater representation of voter preferences in the winning set. Experimental evaluation with real-world participatory budgeting data reveals a pattern in fair compromise: that it is achievable through projects with smaller cost.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Each to Their Own: Exploring the Optimal Embedding in RAG</title>
<link>https://arxiv.org/abs/2507.17442</link>
<guid>https://arxiv.org/abs/2507.17442</guid>
<content:encoded><![CDATA[

arXiv:2507.17442v1 Announce Type: cross 
Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, we propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains. We will release our code upon publication.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception</title>
<link>https://arxiv.org/abs/2507.17445</link>
<guid>https://arxiv.org/abs/2507.17445</guid>
<content:encoded><![CDATA[

arXiv:2507.17445v1 Announce Type: cross 
Abstract: Detecting diverse objects within complex indoor 3D point clouds presents significant challenges for robotic perception, particularly with varied object shapes, clutter, and the co-existence of static and dynamic elements where traditional bounding box methods falter. To address these limitations, we propose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor mobile robots.
  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles naturally occlusions and provides a consistent top-down view aiding to distinguish static obstacles from dynamic agents. The obtained 2D BEV results is directly usable to downstream robotic tasks like navigation, motion prediction, and planning. Our architecture utilizes an axis compact encoder and a window-based backbone to extract rich spatial features from this BEV map. A query-based decoder head then employs learned object queries to concurrently predict object classes and instance masks in the BEV space. This mask-centric formulation effectively captures the footprint of both static and dynamic objects regardless of their shape, offering a robust alternative to bounding box regression. We demonstrate the effectiveness of IndoorBEV on a custom indoor dataset featuring diverse object classes including static objects
  and dynamic elements like robots and miscellaneous items, showcasing its potential for robust indoor scene understanding.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning-Driven Retrosynthesis Prediction with Large Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.17448</link>
<guid>https://arxiv.org/abs/2507.17448</guid>
<content:encoded><![CDATA[

arXiv:2507.17448v1 Announce Type: cross 
Abstract: Retrosynthesis planning, essential in organic synthesis and drug discovery, has greatly benefited from recent AI-driven advancements. Nevertheless, existing methods frequently face limitations in both applicability and explainability. Traditional graph-based and sequence-to-sequence models often lack generalized chemical knowledge, leading to predictions that are neither consistently accurate nor easily explainable. To address these challenges, we introduce RetroDFM-R, a reasoning-based large language model (LLM) designed specifically for chemical retrosynthesis. Leveraging large-scale reinforcement learning guided by chemically verifiable rewards, RetroDFM-R significantly enhances prediction accuracy and explainability. Comprehensive evaluations demonstrate that RetroDFM-R significantly outperforms state-of-the-art methods, achieving a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Double-blind human assessments further validate the chemical plausibility and practical utility of RetroDFM-R's predictions. RetroDFM-R also accurately predicts multistep retrosynthetic routes reported in the literature for both real-world drug molecules and perovskite materials. Crucially, the model's explicit reasoning process provides human-interpretable insights, thereby enhancing trust and practical value in real-world retrosynthesis applications.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls</title>
<link>https://arxiv.org/abs/2507.17467</link>
<guid>https://arxiv.org/abs/2507.17467</guid>
<content:encoded><![CDATA[

arXiv:2507.17467v1 Announce Type: cross 
Abstract: This study investigates the extent to which the Visual Entailment (VE) task serves as a reliable probe of vision-language understanding in multimodal language models, using the LLaMA 3.2 11B Vision model as a test case. Beyond reporting performance metrics, we aim to interpret what these results reveal about the underlying possibilities and limitations of the VE task. We conduct a series of experiments across zero-shot, few-shot, and fine-tuning settings, exploring how factors such as prompt design, the number and order of in-context examples and access to visual information might affect VE performance. To further probe the reasoning processes of the model, we used explanation-based evaluations. Results indicate that three-shot inference outperforms the zero-shot baselines. However, additional examples introduce more noise than they provide benefits. Additionally, the order of the labels in the prompt is a critical factor that influences the predictions. In the absence of visual information, the model has a strong tendency to hallucinate and imagine content, raising questions about the model's over-reliance on linguistic priors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on the e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model. Additionally, the explanation evaluation demonstrates that the fine-tuned model provides semantically meaningful explanations similar to those of humans, with a BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore results in experiments with limited vision, questioning the visual grounding of this task. Overall, our results highlight both the utility and limitations of VE as a diagnostic task for vision-language understanding and point to directions for refining multimodal evaluation methods.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demonstration of Efficient Predictive Surrogates for Large-scale Quantum Processors</title>
<link>https://arxiv.org/abs/2507.17470</link>
<guid>https://arxiv.org/abs/2507.17470</guid>
<content:encoded><![CDATA[

arXiv:2507.17470v1 Announce Type: cross 
Abstract: The ongoing development of quantum processors is driving breakthroughs in scientific discovery. Despite this progress, the formidable cost of fabricating large-scale quantum processors means they will remain rare for the foreseeable future, limiting their widespread application. To address this bottleneck, we introduce the concept of predictive surrogates, which are classical learning models designed to emulate the mean-value behavior of a given quantum processor with provably computational efficiency. In particular, we propose two predictive surrogates that can substantially reduce the need for quantum processor access in diverse practical scenarios. To demonstrate their potential in advancing digital quantum simulation, we use these surrogates to emulate a quantum processor with up to 20 programmable superconducting qubits, enabling efficient pre-training of variational quantum eigensolvers for families of transverse-field Ising models and identification of non-equilibrium Floquet symmetry-protected topological phases. Experimental results reveal that the predictive surrogates not only reduce measurement overhead by orders of magnitude, but can also surpass the performance of conventional, quantum-resource-intensive approaches. Collectively, these findings establish predictive surrogates as a practical pathway to broadening the impact of advanced quantum processors.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles</title>
<link>https://arxiv.org/abs/2507.17472</link>
<guid>https://arxiv.org/abs/2507.17472</guid>
<content:encoded><![CDATA[

arXiv:2507.17472v1 Announce Type: cross 
Abstract: Human decision-making in high-stakes domains often relies on expertise and heuristics, but is vulnerable to hard-to-detect cognitive biases that threaten fairness and long-term outcomes. This work presents a novel approach to enhancing complex decision-making workflows through the integration of hierarchical learning alongside various enhancements. Focusing on university admissions as a representative high-stakes domain, we propose BGM-HAN, an enhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network, designed to effectively model semi-structured applicant data. BGM-HAN captures multi-level representations that are crucial for nuanced assessment, improving both interpretability and predictive performance. Experimental results on real admissions data demonstrate that our proposed model significantly outperforms both state-of-the-art baselines from traditional machine learning to large language models, offering a promising framework for augmenting decision-making in domains where structure, context, and fairness matter. Source code is available at: https://github.com/junhua/bgm-han.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs</title>
<link>https://arxiv.org/abs/2507.17476</link>
<guid>https://arxiv.org/abs/2507.17476</guid>
<content:encoded><![CDATA[

arXiv:2507.17476v1 Announce Type: cross 
Abstract: Although recent Large Language Models (LLMs) have shown rapid improvement on reasoning benchmarks in English, the evaluation of such LLMs' multilingual reasoning capability across diverse languages and cultural contexts remains limited. Existing multilingual reasoning benchmarks are typically constructed by translating existing English reasoning benchmarks, biasing these benchmarks towards reasoning problems with context in English language/cultures. In this work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a benchmark designed to assess LLMs on more than 1,000 native, linguistic and culturally grounded reasoning questions written by native speakers in French, Spanish, and Chinese. MultiNRC covers four core reasoning categories: language-specific linguistic reasoning, wordplay & riddles, cultural/tradition reasoning, and math reasoning with cultural relevance. For cultural/tradition reasoning and math reasoning with cultural relevance, we also provide English equivalent translations of the multilingual questions by manual translation from native speakers fluent in English. This set of English equivalents can provide a direct comparison of LLM reasoning capacity in other languages vs. English on the same reasoning questions. We systematically evaluate current 14 leading LLMs covering most LLM families on MultiNRC and its English equivalent set. The results show that (1) current LLMs are still not good at native multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs exhibit distinct strengths and weaknesses in handling linguistic, cultural, and logical reasoning tasks; (3) Most models perform substantially better in math reasoning in English compared to in original languages (+10%), indicating persistent challenges with culturally grounded knowledge.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised anomaly detection using Bayesian flow networks: application to brain FDG PET in the context of Alzheimer's disease</title>
<link>https://arxiv.org/abs/2507.17486</link>
<guid>https://arxiv.org/abs/2507.17486</guid>
<content:encoded><![CDATA[

arXiv:2507.17486v1 Announce Type: cross 
Abstract: Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for identifying deviations from healthy subject data and thus facilitating the diagnosis of neurological disorders. In this work, we focus on Bayesian flow networks (BFNs), a novel class of generative models, which have not yet been applied to medical imaging or anomaly detection. BFNs combine the strength of diffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension of BFNs for UAD, designed to: i) perform conditional image generation under high levels of spatially correlated noise, and ii) preserve subject specificity by incorporating a recursive feedback from the input image throughout the generative process. We evaluate AnoBFN on the challenging task of Alzheimer's disease-related anomaly detection in FDG PET images. Our approach outperforms other state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and diffusion models (AnoDDPM), demonstrating its effectiveness at detecting anomalies while reducing false positive rates.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Trust or Not to Trust: On Calibration in ML-based Resource Allocation for Wireless Networks</title>
<link>https://arxiv.org/abs/2507.17494</link>
<guid>https://arxiv.org/abs/2507.17494</guid>
<content:encoded><![CDATA[

arXiv:2507.17494v1 Announce Type: cross 
Abstract: In next-generation communications and networks, machine learning (ML) models are expected to deliver not only accurate predictions but also well-calibrated confidence scores that reflect the true likelihood of correct decisions. This paper studies the calibration performance of an ML-based outage predictor within a single-user, multi-resource allocation framework. We first establish key theoretical properties of this system's outage probability (OP) under perfect calibration. Importantly, we show that as the number of resources grows, the OP of a perfectly calibrated predictor approaches the expected output conditioned on it being below the classification threshold. In contrast, when only one resource is available, the system's OP equals the model's overall expected output. We then derive the OP conditions for a perfectly calibrated predictor. These findings guide the choice of the classification threshold to achieve a desired OP, helping system designers meet specific reliability requirements. We also demonstrate that post-processing calibration cannot improve the system's minimum achievable OP, as it does not introduce new information about future channel states. Additionally, we show that well-calibrated models are part of a broader class of predictors that necessarily improve OP. In particular, we establish a monotonicity condition that the accuracy-confidence function must satisfy for such improvement to occur. To demonstrate these theoretical properties, we conduct a rigorous simulation-based analysis using post-processing calibration techniques: Platt scaling and isotonic regression. As part of this framework, the predictor is trained using an outage loss function specifically designed for this system. Furthermore, this analysis is performed on Rayleigh fading channels with temporal correlation captured by Clarke's 2D model, which accounts for receiver mobility.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOTA: Hamiltonian framework for Optimal Transport Advection</title>
<link>https://arxiv.org/abs/2507.17513</link>
<guid>https://arxiv.org/abs/2507.17513</guid>
<content:encoded><![CDATA[

arXiv:2507.17513v1 Announce Type: cross 
Abstract: Optimal transport (OT) has become a natural framework for guiding the probability flows. Yet, the majority of recent generative models assume trivial geometry (e.g., Euclidean) and rely on strong density-estimation assumptions, yielding trajectories that do not respect the true principles of optimality in the underlying manifold. We present Hamiltonian Optimal Transport Advection (HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical OT problem explicitly through Kantorovich potentials, enabling efficient and scalable trajectory optimization. Our approach effectively evades the need for explicit density modeling, performing even when the cost functionals are non-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks, as well as in custom datasets with non-differentiable costs, both in terms of feasibility and optimality.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Cyber Security Education through Digital Twins and Generative AI</title>
<link>https://arxiv.org/abs/2507.17518</link>
<guid>https://arxiv.org/abs/2507.17518</guid>
<content:encoded><![CDATA[

arXiv:2507.17518v1 Announce Type: cross 
Abstract: Digital Twins (DTs) are gaining prominence in cybersecurity for their ability to replicate complex IT (Information Technology), OT (Operational Technology), and IoT (Internet of Things) infrastructures, allowing for real time monitoring, threat analysis, and system simulation. This study investigates how integrating DTs with penetration testing tools and Large Language Models (LLMs) can enhance cybersecurity education and operational readiness. By simulating realistic cyber environments, this approach offers a practical, interactive framework for exploring vulnerabilities and defensive strategies. At the core of this research is the Red Team Knife (RTK), a custom penetration testing toolkit aligned with the Cyber Kill Chain model. RTK is designed to guide learners through key phases of cyberattacks, including reconnaissance, exploitation, and response within a DT powered ecosystem. The incorporation of Large Language Models (LLMs) further enriches the experience by providing intelligent, real-time feedback, natural language threat explanations, and adaptive learning support during training exercises. This combined DT LLM framework is currently being piloted in academic settings to develop hands on skills in vulnerability assessment, threat detection, and security operations. Initial findings suggest that the integration significantly improves the effectiveness and relevance of cybersecurity training, bridging the gap between theoretical knowledge and real-world application. Ultimately, the research demonstrates how DTs and LLMs together can transform cybersecurity education to meet evolving industry demands.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Physics-Based and Data-Driven Approaches for Probabilistic Building Energy Modeling</title>
<link>https://arxiv.org/abs/2507.17526</link>
<guid>https://arxiv.org/abs/2507.17526</guid>
<content:encoded><![CDATA[

arXiv:2507.17526v1 Announce Type: cross 
Abstract: Building energy modeling is a key tool for optimizing the performance of building energy systems. Historically, a wide spectrum of methods has been explored -- ranging from conventional physics-based models to purely data-driven techniques. Recently, hybrid approaches that combine the strengths of both paradigms have gained attention. These include strategies such as learning surrogates for physics-based models, modeling residuals between simulated and observed data, fine-tuning surrogates with real-world measurements, using physics-based outputs as additional inputs for data-driven models, and integrating the physics-based output into the loss function the data-driven model. Despite this progress, two significant research gaps remain. First, most hybrid methods focus on deterministic modeling, often neglecting the inherent uncertainties caused by factors like weather fluctuations and occupant behavior. Second, there has been little systematic comparison within a probabilistic modeling framework. This study addresses these gaps by evaluating five representative hybrid approaches for probabilistic building energy modeling, focusing on quantile predictions of building thermodynamics in a real-world case study. Our results highlight two main findings. First, the performance of hybrid approaches varies across different building room types, but residual learning with a Feedforward Neural Network performs best on average. Notably, the residual approach is the only model that produces physically intuitive predictions when applied to out-of-distribution test data. Second, Quantile Conformal Prediction is an effective procedure for calibrating quantile predictions in case of indoor temperature modeling.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Majorize-Minimization: Beyond Parameter Aggregation</title>
<link>https://arxiv.org/abs/2507.17534</link>
<guid>https://arxiv.org/abs/2507.17534</guid>
<content:encoded><![CDATA[

arXiv:2507.17534v1 Announce Type: cross 
Abstract: This paper proposes a unified approach for designing stochastic optimization algorithms that robustly scale to the federated learning setting. Our work studies a class of Majorize-Minimization (MM) problems, which possesses a linearly parameterized family of majorizing surrogate functions. This framework encompasses (proximal) gradient-based algorithms for (regularized) smooth objectives, the Expectation Maximization algorithm, and many problems seen as variational surrogate MM. We show that our framework motivates a unifying algorithm called Stochastic Approximation Stochastic Surrogate MM (\SSMM), which includes previous stochastic MM procedures as special instances. We then extend \SSMM\ to the federated setting, while taking into consideration common bottlenecks such as data heterogeneity, partial participation, and communication constraints; this yields \QSMM. The originality of \QSMM\ is to learn locally and then aggregate information characterizing the \textit{surrogate majorizing function}, contrary to classical algorithms which learn and aggregate the \textit{original parameter}. Finally, to showcase the flexibility of this methodology beyond our theoretical setting, we use it to design an algorithm for computing optimal transport maps in the federated setting.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Quantum Federated Learning with Fisher Information-Based Optimization</title>
<link>https://arxiv.org/abs/2507.17580</link>
<guid>https://arxiv.org/abs/2507.17580</guid>
<content:encoded><![CDATA[

arXiv:2507.17580v1 Announce Type: cross 
Abstract: Federated Learning (FL) has become increasingly popular across different sectors, offering a way for clients to work together to train a global model without sharing sensitive data. It involves multiple rounds of communication between the global model and participating clients, which introduces several challenges like high communication costs, heterogeneous client data, prolonged processing times, and increased vulnerability to privacy threats. In recent years, the convergence of federated learning and parameterized quantum circuits has sparked significant research interest, with promising implications for fields such as healthcare and finance. By enabling decentralized training of quantum models, it allows clients or institutions to collaboratively enhance model performance and outcomes while preserving data privacy. Recognizing that Fisher information can quantify the amount of information that a quantum state carries under parameter changes, thereby providing insight into its geometric and statistical properties. We intend to leverage this property to address the aforementioned challenges. In this work, we propose a Quantum Federated Learning (QFL) algorithm that makes use of the Fisher information computed on local client models, with data distributed across heterogeneous partitions. This approach identifies the critical parameters that significantly influence the quantum model's performance, ensuring they are preserved during the aggregation process. Our research assessed the effectiveness and feasibility of QFL by comparing its performance against other variants, and exploring the benefits of incorporating Fisher information in QFL settings. Experimental results on ADNI and MNIST datasets demonstrate the effectiveness of our approach in achieving better performance and robustness against the quantum federated averaging method.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.17596</link>
<guid>https://arxiv.org/abs/2507.17596</guid>
<content:encoded><![CDATA[

arXiv:2507.17596v1 Announce Type: cross 
Abstract: While end-to-end autonomous driving models show promising results, their practical deployment is often hindered by large model sizes, a reliance on expensive LiDAR sensors and computationally intensive BEV feature representations. This limits their scalability, especially for mass-market vehicles equipped only with cameras. To address these challenges, we propose PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving architecture operates using only camera data, without explicit BEV representation and forgoing the need for LiDAR. PRIX leverages a visual feature extractor coupled with a generative planning head to predict safe trajectories from raw pixel inputs directly. A core component of our architecture is the Context-aware Recalibration Transformer (CaRT), a novel module designed to effectively enhance multi-level visual features for more robust planning. We demonstrate through comprehensive experiments that PRIX achieves state-of-the-art performance on the NavSim and nuScenes benchmarks, matching the capabilities of larger, multimodal diffusion planners while being significantly more efficient in terms of inference speed and model size, making it a practical solution for real-world deployment. Our work is open-source and the code will be at https://maxiuw.github.io/prix.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Transformer attention alignment with human visual perception in aesthetic object evaluation</title>
<link>https://arxiv.org/abs/2507.17616</link>
<guid>https://arxiv.org/abs/2507.17616</guid>
<content:encoded><![CDATA[

arXiv:2507.17616v1 Announce Type: cross 
Abstract: Visual attention mechanisms play a crucial role in human perception and aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have demonstrated remarkable capabilities in computer vision tasks, yet their alignment with human visual attention patterns remains underexplored, particularly in aesthetic contexts. This study investigates the correlation between human visual attention and ViT attention mechanisms when evaluating handcrafted objects. We conducted an eye-tracking experiment with 30 participants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal objects comprising basketry bags and ginger jars. Using a Pupil Labs eye-tracker, we recorded gaze patterns and generated heat maps representing human visual attention. Simultaneously, we analyzed the same objects using a pre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting attention maps from each of the 12 attention heads. We compared human and ViT attention distributions using Kullback-Leibler divergence across varying Gaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal correlation at sigma=2.4 +-0.03, with attention head #12 showing the strongest alignment with human visual patterns. Significant differences were found between attention heads, with heads #7 and #9 demonstrating the greatest divergence from human attention (p< 0.05, Tukey HSD test). Results indicate that while ViTs exhibit more global attention patterns compared to human focal attention, certain attention heads can approximate human visual behavior, particularly for specific object features like buckles in basketry items. These findings suggest potential applications of ViT attention mechanisms in product design and aesthetic evaluation, while highlighting fundamental differences in attention strategies between human perception and current AI models.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Should We Meta-Learn Reinforcement Learning Algorithms?</title>
<link>https://arxiv.org/abs/2507.17668</link>
<guid>https://arxiv.org/abs/2507.17668</guid>
<content:encoded><![CDATA[

arXiv:2507.17668v1 Announce Type: cross 
Abstract: The process of meta-learning algorithms from data, instead of relying on manual design, is growing in popularity as a paradigm for improving the performance of machine learning systems. Meta-learning shows particular promise for reinforcement learning (RL), where algorithms are often adapted from supervised or unsupervised learning despite their suboptimality for RL. However, until now there has been a severe lack of comparison between different meta-learning algorithms, such as using evolution to optimise over black-box functions or LLMs to propose code. In this paper, we carry out this empirical comparison of the different approaches when applied to a range of meta-learned algorithms which target different parts of the RL pipeline. In addition to meta-train and meta-test performance, we also investigate factors including the interpretability, sample cost and train time for each meta-learning algorithm. Based on these findings, we propose several guidelines for meta-learning new RL algorithms which will help ensure that future learned algorithms are as performant as possible.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASCADE: LLM-Powered JavaScript Deobfuscator at Google</title>
<link>https://arxiv.org/abs/2507.17691</link>
<guid>https://arxiv.org/abs/2507.17691</guid>
<content:encoded><![CDATA[

arXiv:2507.17691v1 Announce Type: cross 
Abstract: Software obfuscation, particularly prevalent in JavaScript, hinders code comprehension and analysis, posing significant challenges to software testing, static analysis, and malware detection. This paper introduces CASCADE, a novel hybrid approach that integrates the advanced coding capabilities of Gemini with the deterministic transformation capabilities of a compiler Intermediate Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to identify critical prelude functions, the foundational components underlying the most prevalent obfuscation techniques, and leveraging JSIR for subsequent code transformations, CASCADE effectively recovers semantic elements like original strings and API names, and reveals original program behaviors. This method overcomes limitations of existing static and dynamic deobfuscation techniques, eliminating hundreds to thousands of hardcoded rules while achieving reliability and flexibility. CASCADE is already deployed in Google's production environment, demonstrating substantial improvements in JavaScript deobfuscation efficiency and reducing reverse engineering efforts.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes</title>
<link>https://arxiv.org/abs/2507.17717</link>
<guid>https://arxiv.org/abs/2507.17717</guid>
<content:encoded><![CDATA[

arXiv:2507.17717v1 Announce Type: cross 
Abstract: AI-generated clinical notes are increasingly used in healthcare, but evaluating their quality remains a challenge due to high subjectivity and limited scalability of expert review. Existing automated metrics often fail to align with real-world physician preferences. To address this, we propose a pipeline that systematically distills real user feedback into structured checklists for note evaluation. These checklists are designed to be interpretable, grounded in human feedback, and enforceable by LLM-based evaluators. Using deidentified data from over 21,000 clinical encounters, prepared in accordance with the HIPAA safe harbor standard, from a deployed AI medical scribe system, we show that our feedback-derived checklist outperforms baseline approaches in our offline evaluations in coverage, diversity, and predictive power for human ratings. Extensive experiments confirm the checklist's robustness to quality-degrading perturbations, significant alignment with clinician preferences, and practical value as an evaluation methodology. In offline research settings, the checklist can help identify notes likely to fall below our chosen quality thresholds.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer</title>
<link>https://arxiv.org/abs/2507.17718</link>
<guid>https://arxiv.org/abs/2507.17718</guid>
<content:encoded><![CDATA[

arXiv:2507.17718v1 Announce Type: cross 
Abstract: With the rise of voice-enabled artificial intelligence (AI) systems, quantitative survey researchers have access to a new data-collection mode: AI telephone surveying. By using AI to conduct phone interviews, researchers can scale quantitative studies while balancing the dual goals of human-like interactivity and methodological rigor. Unlike earlier efforts that used interactive voice response (IVR) technology to automate these surveys, voice AI enables a more natural and adaptive respondent experience as it is more robust to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on large language models (LLM), automatic speech recognition (ASR), and speech synthesis technologies. The system was specifically designed for quantitative research, and strictly adhered to research best practices like question order randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot surveys with the SSRS Opinion Panel and followed-up with a separate human-administered survey to assess respondent experiences. We measured three key metrics: the survey completion rates, break-off rates, and respondent satisfaction scores. Our results suggest that shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Interaction of Compressibility and Adversarial Robustness</title>
<link>https://arxiv.org/abs/2507.17725</link>
<guid>https://arxiv.org/abs/2507.17725</guid>
<content:encoded><![CDATA[

arXiv:2507.17725v1 Announce Type: cross 
Abstract: Modern neural networks are expected to simultaneously satisfy a host of desirable properties: accurate fitting to training data, generalization to unseen inputs, parameter and computational efficiency, and robustness to adversarial perturbations. While compressibility and robustness have each been studied extensively, a unified understanding of their interaction still remains elusive. In this work, we develop a principled framework to analyze how different forms of compressibility - such as neuron-level sparsity and spectral compressibility - affect adversarial robustness. We show that these forms of compression can induce a small number of highly sensitive directions in the representation space, which adversaries can exploit to construct effective perturbations. Our analysis yields a simple yet instructive robustness bound, revealing how neuron and spectral compressibility impact $L_\infty$ and $L_2$ robustness via their effects on the learned representations. Crucially, the vulnerabilities we identify arise irrespective of how compression is achieved - whether via regularization, architectural bias, or implicit learning dynamics. Through empirical evaluations across synthetic and realistic tasks, we confirm our theoretical predictions, and further demonstrate that these vulnerabilities persist under adversarial training and transfer learning, and contribute to the emergence of universal adversarial perturbations. Our findings show a fundamental tension between structured compressibility and robustness, and suggest new pathways for designing models that are both efficient and secure.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Matching Meets Biology and Life Science: A Survey</title>
<link>https://arxiv.org/abs/2507.17731</link>
<guid>https://arxiv.org/abs/2507.17731</guid>
<content:encoded><![CDATA[

arXiv:2507.17731v1 Announce Type: cross 
Abstract: Over the past decade, advances in generative modeling, such as generative adversarial networks, masked autoencoders, and diffusion models, have significantly transformed biological research and discovery, enabling breakthroughs in molecule design, protein generation, drug discovery, and beyond. At the same time, biological applications have served as valuable testbeds for evaluating the capabilities of generative models. Recently, flow matching has emerged as a powerful and efficient alternative to diffusion-based generative modeling, with growing interest in its application to problems in biology and life sciences. This paper presents the first comprehensive survey of recent developments in flow matching and its applications in biological domains. We begin by systematically reviewing the foundations and variants of flow matching, and then categorize its applications into three major areas: biological sequence modeling, molecule generation and design, and peptide and protein generation. For each, we provide an in-depth review of recent progress. We also summarize commonly used datasets and software tools, and conclude with a discussion of potential future directions. The corresponding curated resources are available at https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yume: An Interactive World Generation Model</title>
<link>https://arxiv.org/abs/2507.17744</link>
<guid>https://arxiv.org/abs/2507.17744</guid>
<content:encoded><![CDATA[

arXiv:2507.17744v1 Announce Type: cross 
Abstract: Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \sekai to train \method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention</title>
<link>https://arxiv.org/abs/2507.17745</link>
<guid>https://arxiv.org/abs/2507.17745</guid>
<content:encoded><![CDATA[

arXiv:2507.17745v1 Announce Type: cross 
Abstract: Recent advances in sparse voxel representations have significantly improved the quality of 3D content generation, enabling high-resolution modeling with fine-grained geometry. However, existing frameworks suffer from severe computational inefficiencies due to the quadratic complexity of attention mechanisms in their two-stage diffusion pipelines. In this work, we propose Ultra3D, an efficient 3D generation framework that significantly accelerates sparse voxel modeling without compromising quality. Our method leverages the compact VecSet representation to efficiently generate a coarse object layout in the first stage, reducing token count and accelerating voxel coordinate prediction. To refine per-voxel latent features in the second stage, we introduce Part Attention, a geometry-aware localized attention mechanism that restricts attention computation within semantically consistent part regions. This design preserves structural continuity while avoiding unnecessary global attention, achieving up to 6.7x speed-up in latent generation. To support this mechanism, we construct a scalable part annotation pipeline that converts raw meshes into part-labeled sparse voxels. Extensive experiments demonstrate that Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves state-of-the-art performance in both visual fidelity and user preference.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains</title>
<link>https://arxiv.org/abs/2507.17746</link>
<guid>https://arxiv.org/abs/2507.17746</guid>
<content:encoded><![CDATA[

arXiv:2507.17746v1 Announce Type: cross 
Abstract: Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world tasks often requires balancing objective and subjective evaluation criteria. However, many such tasks lack a single, unambiguous ground truth-making it difficult to define reliable reward signals for post-training language models. While traditional preference-based methods offer a workaround, they rely on opaque reward functions that are difficult to interpret and prone to spurious correlations. We introduce $\textbf{Rubrics as Rewards}$ (RaR), a framework that uses structured, checklist-style rubrics as interpretable reward signals for on-policy training with GRPO. Our best RaR method yields up to a $28\%$ relative improvement on HealthBench-1k compared to simple Likert-based approaches, while matching or surpassing the performance of reward signals derived from expert-written references. By treating rubrics as structured reward signals, we show that RaR enables smaller-scale judge models to better align with human preferences and sustain robust performance across model scales.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks</title>
<link>https://arxiv.org/abs/2507.17747</link>
<guid>https://arxiv.org/abs/2507.17747</guid>
<content:encoded><![CDATA[

arXiv:2507.17747v1 Announce Type: cross 
Abstract: As frontier language models increasingly saturate standard QA benchmarks, concerns about data contamination, memorization, and escalating dataset creation costs persist. We propose a debate-driven evaluation paradigm that transforms any existing QA dataset into structured adversarial debates--where one model is given the official answer to defend, and another constructs and defends an alternative answer--adjudicated by a judge model blind to the correct solution. By forcing multi-round argumentation, this approach substantially increases difficulty while penalizing shallow memorization, yet reuses QA items to reduce curation overhead. We make two main contributions: (1) an evaluation pipeline to systematically convert QA tasks into debate-based assessments, and (2) a public benchmark that demonstrates our paradigm's effectiveness on a subset of MMLU-Pro questions, complete with standardized protocols and reference models. Empirical results validate the robustness of the method and its effectiveness against data contamination--a Llama 3.1 model fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%) but performed worse in debates. Results also show that even weaker judges can reliably differentiate stronger debaters, highlighting how debate-based evaluation can scale to future, more capable systems while maintaining a fraction of the cost of creating new benchmarks. Overall, our framework underscores that "pretraining on the test set is no longer all you need," offering a sustainable path for measuring the genuine reasoning ability of advanced language models.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility</title>
<link>https://arxiv.org/abs/2507.17748</link>
<guid>https://arxiv.org/abs/2507.17748</guid>
<content:encoded><![CDATA[

arXiv:2507.17748v1 Announce Type: cross 
Abstract: Robustness and resource-efficiency are two highly desirable properties for modern machine learning models. However, achieving them jointly remains a challenge. In this paper, we position high learning rates as a facilitator for simultaneously achieving robustness to spurious correlations and network compressibility. We demonstrate that large learning rates also produce desirable representation properties such as invariant feature utilization, class separation, and activation sparsity. Importantly, our findings indicate that large learning rates compare favorably to other hyperparameters and regularization methods, in consistently satisfying these properties in tandem. In addition to demonstrating the positive effect of large learning rates across diverse spurious correlation datasets, models, and optimizers, we also present strong evidence that the previously documented success of large learning rates in standard classification tasks is likely due to its effect on addressing hidden/rare spurious correlations in the training dataset.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflict Detection for Temporal Knowledge Graphs:A Fast Constraint Mining Algorithm and New Benchmarks</title>
<link>https://arxiv.org/abs/2312.11053</link>
<guid>https://arxiv.org/abs/2312.11053</guid>
<content:encoded><![CDATA[

arXiv:2312.11053v2 Announce Type: replace 
Abstract: Temporal facts, which are used to describe events that occur during specific time periods, have become a topic of increased interest in the field of knowledge graph (KG) research. In terms of quality management, the introduction of time restrictions brings new challenges to maintaining the temporal consistency of KGs. Previous studies rely on manually enumerated temporal constraints to detect conflicts, which are labor-intensive and may have granularity issues. To address this problem, we start from the common pattern of temporal facts and propose a pattern-based temporal constraint mining method, PaTeCon. Unlike previous studies, PaTeCon uses graph patterns and statistical information relevant to the given KG to automatically generate temporal constraints, without the need for human experts. In this paper, we illustrate how this method can be optimized to achieve significant speed improvement. We also annotate Wikidata and Freebase to build two new benchmarks for conflict detection. Extensive experiments demonstrate that our pattern-based automatic constraint mining approach is highly effective in generating valuable temporal constraints.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM as a code generator in Agile Model Driven Development</title>
<link>https://arxiv.org/abs/2410.18489</link>
<guid>https://arxiv.org/abs/2410.18489</guid>
<content:encoded><![CDATA[

arXiv:2410.18489v2 Announce Type: replace 
Abstract: Leveraging Large Language Models (LLM) like GPT4 in the auto generation of code represents a significant advancement, yet it is not without its challenges. The ambiguity inherent in natural language descriptions of software poses substantial obstacles to generating deployable, structured artifacts. This research champions Model Driven Development (MDD) as a viable strategy to overcome these challenges, proposing an Agile Model Driven Development (AMDD) approach that employs GPT4 as a code generator. This approach enhances the flexibility and scalability of the code auto generation process and offers agility that allows seamless adaptation to changes in models or deployment environments. We illustrate this by modeling a multi agent Unmanned Vehicle Fleet (UVF) system using the Unified Modeling Language (UML), significantly reducing model ambiguity by integrating the Object Constraint Language (OCL) for code structure meta modeling, and the FIPA ontology language for communication semantics meta modeling. Applying GPT4 auto generation capabilities yields Java and Python code that is compatible with the JADE and PADE frameworks, respectively. Our thorough evaluation of the auto generated code verifies its alignment with expected behaviors and identifies enhancements in agent interactions. Structurally, we assessed the complexity of code derived from a model constrained solely by OCL meta models, against that influenced by both OCL and FIPA ontology meta models. The results indicate that the ontology constrained meta model produces inherently more complex code, yet its cyclomatic complexity remains within manageable levels, suggesting that additional meta model constraints can be incorporated without exceeding the high risk threshold for complexity.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Neural Strategy-Proof Matching Mechanism from Examples</title>
<link>https://arxiv.org/abs/2410.19384</link>
<guid>https://arxiv.org/abs/2410.19384</guid>
<content:encoded><![CDATA[

arXiv:2410.19384v2 Announce Type: replace 
Abstract: Designing two-sided matching mechanisms is challenging when practical demands for matching outcomes are difficult to formalize and the designed mechanism must satisfy theoretical conditions. To address this, prior work has proposed a framework that learns a matching mechanism from examples, using a parameterized family that satisfies properties such as stability. However, despite its usefulness, this framework does not guarantee strategy-proofness (SP), and cannot handle varying numbers of agents or incorporate publicly available contextual information about agents, both of which are crucial in real-world applications. In this paper, we propose a new parametrized family of matching mechanisms that always satisfy strategy-proofness, are applicable for an arbitrary number of agents, and deal with public contextual information of agents, based on the serial dictatorship (SD). This family is represented by NeuralSD, a novel neural network architecture based on SD, where agent rankings in SD are treated as learnable parameters computed from agents' contexts using an attention-based sub-network. To enable learning, we introduce tensor serial dictatorship (TSD), a differentiable relaxation of SD using tensor operations. This allows NeuralSD to be trained end-to-end from example matchings while satisfying SP. We conducted experiments to learn a matching mechanism from matching examples while satisfying SP. We demonstrated that our method outperformed baselines in predicting matchings and on several metrics for goodness of matching outcomes.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for Mixed-Integer Programming Problem</title>
<link>https://arxiv.org/abs/2412.14382</link>
<guid>https://arxiv.org/abs/2412.14382</guid>
<content:encoded><![CDATA[

arXiv:2412.14382v3 Announce Type: replace 
Abstract: Mixed-integer programming (MIP) is a powerful paradigm for modeling and solving various important combinatorial optimization problems. Recently, learning-based approaches have shown a potential to speed up MIP solving via offline training that then guides important design decisions during the search. However, a significant drawback of these methods is their heavy reliance on offline training, which requires collecting training datasets and computationally costly training epochs yet offering only limited generalization to unseen (larger) instances. In this paper, we propose Balans, an adaptive meta-solver for MIPs with online learning capability that does not require any supervision or apriori training. At its core, Balans is based on adaptive large-neighborhood search, operating on top of an MIP solver by successive applications of destroy and repair neighborhood operators. During the search, the selection among different neighborhood definitions is guided on the fly for the instance at hand via multi-armed bandit algorithms. Our extensive experiments on hard optimization instances show that Balans offers significant performance gains over the default MIP solver, is better than committing to any single best neighborhood, and improves over the state-of-the-art large-neighborhood search for MIPs. Finally, we release Balans as a highly configurable, MIP solver agnostic, open-source software.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel approach to navigate the taxonomic hierarchy to address the Open-World Scenarios in Medicinal Plant Classification</title>
<link>https://arxiv.org/abs/2502.17289</link>
<guid>https://arxiv.org/abs/2502.17289</guid>
<content:encoded><![CDATA[

arXiv:2502.17289v3 Announce Type: replace 
Abstract: In this article, we propose a novel approach for plant hierarchical taxonomy classification by posing the problem as an open class problem. It is observed that existing methods for medicinal plant classification often fail to perform hierarchical classification and accurately identifying unknown species, limiting their effectiveness in comprehensive plant taxonomy classification. Thus we address the problem of unknown species classification by assigning it best hierarchical labels. We propose a novel method, which integrates DenseNet121, Multi-Scale Self-Attention (MSSA) and cascaded classifiers for hierarchical classification. The approach systematically categorizes medicinal plants at multiple taxonomic levels, from phylum to species, ensuring detailed and precise classification. Using multi scale space attention, the model captures both local and global contextual information from the images, improving the distinction between similar species and the identification of new ones. It uses attention scores to focus on important features across multiple scales. The proposed method provides a solution for hierarchical classification, showcasing superior performance in identifying both known and unknown species. The model was tested on two state-of-art datasets with and without background artifacts and so that it can be deployed to tackle real word application. We used unknown species for testing our model. For unknown species the model achieved an average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for predicting correct phylum, class, order and family respectively. Our proposed model size is almost four times less than the existing state of the art methods making it easily deploy able in real world application.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems</title>
<link>https://arxiv.org/abs/2503.01424</link>
<guid>https://arxiv.org/abs/2503.01424</guid>
<content:encoded><![CDATA[

arXiv:2503.01424v2 Announce Type: replace 
Abstract: Research is a fundamental process driving the advancement of human civilization, yet it demands substantial time and effort from researchers. In recent years, the rapid development of artificial intelligence (AI) technologies has inspired researchers to explore how AI can accelerate and enhance research. To monitor relevant advancements, this paper presents a systematic review of the progress in this domain. Specifically, we organize the relevant studies into three main categories: hypothesis formulation, hypothesis validation, and manuscript publication. Hypothesis formulation involves knowledge synthesis and hypothesis generation. Hypothesis validation includes the verification of scientific claims, theorem proving, and experiment validation. Manuscript publication encompasses manuscript writing and the peer review process. Furthermore, we identify and discuss the current challenges faced in these areas, as well as potential future directions for research. Finally, we also offer a comprehensive overview of existing benchmarks and tools across various domains that support the integration of AI into the research process. We hope this paper serves as an introduction for beginners and fosters future research. Resources have been made publicly available at https://github.com/zkzhou126/AI-for-Research.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment</title>
<link>https://arxiv.org/abs/2504.02193</link>
<guid>https://arxiv.org/abs/2504.02193</guid>
<content:encoded><![CDATA[

arXiv:2504.02193v2 Announce Type: replace 
Abstract: Aligning large language models (LLMs) with human values is an increasingly critical step in post-training. Direct Preference Optimization (DPO) has emerged as a simple, yet effective alternative to reinforcement learning from human feedback (RLHF). Synthetic preference data with its low cost and high quality enable effective alignment through single- or multi-model generated preference data. Our study reveals a striking, safety-specific phenomenon associated with DPO alignment: Although multi-model generated data enhances performance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by providing diverse responses, it also tends to facilitate reward hacking during training. This can lead to a high attack success rate (ASR) when models encounter jailbreaking prompts. The issue is particularly pronounced when employing stronger models like GPT-4o or larger models in the same family to generate chosen responses paired with target model self-generated rejected responses, resulting in dramatically poorer safety outcomes. Furthermore, with respect to safety, using solely self-generated responses (single-model generation) for both chosen and rejected pairs significantly outperforms configurations that incorporate responses from stronger models, whether used directly as chosen data or as part of a multi-model response pool. We demonstrate that multi-model preference data exhibits high linear separability between chosen and rejected responses, which allows models to exploit superficial cues rather than internalizing robust safety constraints. Our experiments, conducted on models from the Llama, Mistral, and Qwen families, consistently validate these findings.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving</title>
<link>https://arxiv.org/abs/2505.07773</link>
<guid>https://arxiv.org/abs/2505.07773</guid>
<content:encoded><![CDATA[

arXiv:2505.07773v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\_async\_pipline}.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turing Test 2.0: The General Intelligence Threshold</title>
<link>https://arxiv.org/abs/2505.19550</link>
<guid>https://arxiv.org/abs/2505.19550</guid>
<content:encoded><![CDATA[

arXiv:2505.19550v4 Announce Type: replace 
Abstract: With the rise of artificial intelligence (A.I.) and large language models like ChatGPT, a new race for achieving artificial general intelligence (A.G.I) has started. While many speculate how and when A.I. will achieve A.G.I., there is no clear agreement on how A.G.I. can be detected in A.I. models, even when popular tools like the Turing test (and its modern variations) are used to measure their intelligence. In this work, we discuss why traditional methods like the Turing test do not suffice for measuring or detecting A.G.I. and provide a new, practical method that can be used to decide if a system (computer or any other) has reached or surpassed A.G.I. To achieve this, we make two new contributions. First, we present a clear definition for general intelligence (G.I.) and set a G.I. Threshold (G.I.T.) that can be used to distinguish between systems that achieve A.G.I. and systems that do not. Second, we present a new framework on how to construct tests that can detect if a system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass way. We call this novel framework the Turing test 2.0. We then demonstrate real-life examples of applying tests that follow our Turing test 2.0 framework on modern A.I. models.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings</title>
<link>https://arxiv.org/abs/2506.00178</link>
<guid>https://arxiv.org/abs/2506.00178</guid>
<content:encoded><![CDATA[

arXiv:2506.00178v2 Announce Type: replace 
Abstract: Prompt engineering represents a critical bottleneck to harness the full potential of Large Language Models (LLMs) for solving complex tasks, as it requires specialized expertise, significant trial-and-error, and manual intervention. This challenge is particularly pronounced for tasks involving subjective quality assessment, where defining explicit optimization objectives becomes fundamentally problematic. Existing automated prompt optimization methods falter in these scenarios, as they typically require well-defined task-specific numerical fitness functions or rely on generic templates that cannot capture the nuanced requirements of complex use cases. We introduce DEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that guides prompt evolution through a debate-driven evaluation with an Elo-based selection. Contrary to prior work, DEEVOs approach enables exploration of the discrete prompt space while preserving semantic coherence through intelligent crossover and strategic mutation operations that incorporate debate-based feedback, combining elements from both successful and unsuccessful prompts based on identified strengths rather than arbitrary splicing. Using Elo ratings as a fitness proxy, DEEVO simultaneously drives improvement and preserves valuable diversity in the prompt population. Experimental results demonstrate that DEEVO significantly outperforms both manual prompt engineering and alternative state-of-the-art optimization approaches on open-ended tasks and close-ended tasks despite using no ground truth feedback. By connecting LLMs reasoning capabilities with adaptive optimization, DEEVO represents a significant advancement in prompt optimization research by eliminating the need of predetermined metrics to continuously improve AI systems.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Community-driven vision for a new Knowledge Resource for AI</title>
<link>https://arxiv.org/abs/2506.16596</link>
<guid>https://arxiv.org/abs/2506.16596</guid>
<content:encoded><![CDATA[

arXiv:2506.16596v2 Announce Type: replace 
Abstract: The long-standing goal of creating a comprehensive, multi-purpose knowledge resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and other commercial knowledge graphs, verifiable, general-purpose widely available sources of knowledge remain a critical deficiency in AI infrastructure. Large language models struggle due to knowledge gaps; robotic planning lacks necessary world knowledge; and the detection of factually false information relies heavily on human expertise. What kind of knowledge resource is most needed in AI today? How can modern technology shape its development and evaluation? A recent AAAI workshop gathered over 50 researchers to explore these questions. This paper synthesizes our findings and outlines a community-driven vision for a new knowledge infrastructure. In addition to leveraging contemporary advances in knowledge representation and reasoning, one promising idea is to build an open engineering framework to exploit knowledge modules effectively within the context of practical applications. Such a framework should include sets of conventions and social structures that are adopted by contributors.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEGO Co-builder: Exploring Fine-Grained Vision-Language Modeling for Multimodal LEGO Assembly Assistants</title>
<link>https://arxiv.org/abs/2507.05515</link>
<guid>https://arxiv.org/abs/2507.05515</guid>
<content:encoded><![CDATA[

arXiv:2507.05515v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) are facing the challenges of understanding and following multimodal assembly instructions, particularly when fine-grained spatial reasoning and precise object state detection are required. In this work, we explore LEGO Co-builder, a hybrid benchmark combining real-world LEGO assembly logic with programmatically generated multimodal scenes. The dataset captures stepwise visual states and procedural instructions, allowing controlled evaluation of instruction-following, object detection, and state detection. We introduce a unified framework and assess leading VLMs such as GPT-4o, Gemini, and Qwen-VL, under zero-shot and fine-tuned settings. Our results reveal that even advanced models like GPT-4o struggle with fine-grained assembly tasks, with a maximum F1 score of just 40.54\% on state detection, highlighting gaps in fine-grained visual understanding. We release the benchmark, codebase, and generation pipeline to support future research on multimodal assembly assistants grounded in real-world workflows.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Working with AI: Measuring the Occupational Implications of Generative AI</title>
<link>https://arxiv.org/abs/2507.07935</link>
<guid>https://arxiv.org/abs/2507.07935</guid>
<content:encoded><![CDATA[

arXiv:2507.07935v3 Announce Type: replace 
Abstract: Given the rapid adoption of generative AI and its potential to impact a wide range of tasks, understanding the effects of AI on the economy is one of society's most important questions. In this work, we take a step toward that goal by analyzing the work activities people do with AI, how successfully and broadly those activities are done, and combine that with data on what occupations do those activities. We analyze a dataset of 200k anonymized and privacy-scrubbed conversations between users and Microsoft Bing Copilot, a publicly available generative AI system. We find the most common work activities people seek AI assistance for involve gathering information and writing, while the most common activities that AI itself is performing are providing information and assistance, writing, teaching, and advising. Combining these activity classifications with measurements of task success and scope of impact, we compute an AI applicability score for each occupation. We find the highest AI applicability scores for knowledge work occupation groups such as computer and mathematical, and office and administrative support, as well as occupations such as sales whose work activities involve providing and communicating information. Additionally, we characterize the types of work activities performed most successfully, how wage and education correlate with AI applicability, and how real-world usage compares to predictions of occupational AI impact.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACMP: Allen-Cahn Message Passing with Attractive and Repulsive Forces for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2206.05437</link>
<guid>https://arxiv.org/abs/2206.05437</guid>
<content:encoded><![CDATA[

arXiv:2206.05437v4 Announce Type: replace-cross 
Abstract: Neural message passing is a basic feature extraction unit for graph-structured data considering neighboring node features in network propagation from one layer to the next. We model such process by an interacting particle system with attractive and repulsive forces and the Allen-Cahn force arising in the modeling of phase transition. The dynamics of the system is a reaction-diffusion process which can separate particles without blowing up. This induces an Allen-Cahn message passing (ACMP) for graph neural networks where the numerical iteration for the particle system solution constitutes the message passing propagation. ACMP which has a simple implementation with a neural ODE solver can propel the network depth up to one hundred of layers with theoretically proven strictly positive lower bound of the Dirichlet energy. It thus provides a deep model of GNNs circumventing the common GNN problem of oversmoothing. GNNs with ACMP achieve state of the art performance for real-world node classification tasks on both homophilic and heterophilic datasets. Codes are available at https://github.com/ykiiiiii/ACMP.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions</title>
<link>https://arxiv.org/abs/2308.15225</link>
<guid>https://arxiv.org/abs/2308.15225</guid>
<content:encoded><![CDATA[

arXiv:2308.15225v3 Announce Type: replace-cross 
Abstract: Over the past decades, cognitive neuroscientists and behavioral economists have recognized the value of describing the process of decision making in detail and modeling the emergence of decisions over time. For example, the time it takes to decide can reveal more about an agent's true hidden preferences than only the decision itself. Similarly, data that track the ongoing decision process such as eye movements or neural recordings contain critical information that can be exploited, even if no decision is made. Here, we argue that artificial intelligence (AI) research would benefit from a stronger focus on insights about how decisions emerge over time and incorporate related process data to improve AI predictions in general and human-AI interactions in particular. First, we introduce a highly established computational framework that assumes decisions to emerge from the noisy accumulation of evidence, and we present related empirical work in psychology, neuroscience, and economics. Next, we discuss to what extent current approaches in multi-agent AI do or do not incorporate process data and models of decision making. Finally, we outline how a more principled inclusion of the evidence-accumulation framework into the training and use of AI can help to improve human-AI interactions in the future.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems</title>
<link>https://arxiv.org/abs/2312.15234</link>
<guid>https://arxiv.org/abs/2312.15234</guid>
<content:encoded><![CDATA[

arXiv:2312.15234v2 Announce Type: replace-cross 
Abstract: In the rapidly evolving landscape of artificial intelligence (AI), generative large language models (LLMs) stand at the forefront, revolutionizing how we interact with our data. However, the computational intensity and memory consumption of deploying these models present substantial challenges in terms of serving efficiency, particularly in scenarios demanding low latency and high throughput. This survey addresses the imperative need for efficient LLM serving methodologies from a machine learning system (MLSys) research perspective, standing at the crux of advanced AI innovations and practical system optimizations. We provide in-depth analysis, covering a spectrum of solutions, ranging from cutting-edge algorithmic modifications to groundbreaking changes in system designs. The survey aims to provide a comprehensive understanding of the current state and future directions in efficient LLM serving, offering valuable insights for researchers and practitioners in overcoming the barriers of effective LLM deployment, thereby reshaping the future of AI.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Sequential Recommender with Large Language Models for Joint Video and Comment Recommendation</title>
<link>https://arxiv.org/abs/2403.13574</link>
<guid>https://arxiv.org/abs/2403.13574</guid>
<content:encoded><![CDATA[

arXiv:2403.13574v2 Announce Type: replace-cross 
Abstract: Nowadays, reading or writing comments on captivating videos has emerged as a critical part of the viewing experience on online video platforms. However, existing recommender systems primarily focus on users' interaction behaviors with videos, neglecting comment content and interaction in user preference modeling. In this paper, we propose a novel recommendation approach called LSVCR that utilizes user interaction histories with both videos and comments to jointly perform personalized video and comment recommendation. Specifically, our approach comprises two key components: sequential recommendation (SR) model and supplemental large language model (LLM) recommender. The SR model functions as the primary recommendation backbone (retained in deployment) of our method for efficient user preference modeling. Concurrently, we employ a LLM as the supplemental recommender (discarded in deployment) to better capture underlying user preferences derived from heterogeneous interaction behaviors. In order to integrate the strengths of the SR model and the supplemental LLM recommender, we introduce a two-stage training paradigm. The first stage, personalized preference alignment, aims to align the preference representations from both components, thereby enhancing the semantics of the SR model. The second stage, recommendation-oriented fine-tuning, involves fine-tuning the alignment-enhanced SR model according to specific objectives. Extensive experiments in both video and comment recommendation tasks demonstrate the effectiveness of LSVCR. Moreover, online A/B testing on KuaiShou platform verifies the practical benefits of our approach. In particular, we attain a cumulative gain of 4.13% in comment watch time.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Level Explanations for Generative Language Models</title>
<link>https://arxiv.org/abs/2403.14459</link>
<guid>https://arxiv.org/abs/2403.14459</guid>
<content:encoded><![CDATA[

arXiv:2403.14459v2 Announce Type: replace-cross 
Abstract: Despite the increasing use of large language models (LLMs) for context-grounded tasks like summarization and question-answering, understanding what makes an LLM produce a certain response is challenging. We propose Multi-Level Explanations for Generative Language Models (MExGen), a technique to provide explanations for context-grounded text generation. MExGen assigns scores to parts of the context to quantify their influence on the model's output. It extends attribution methods like LIME and SHAP to LLMs used in context-grounded tasks where (1) inference cost is high, (2) input text is long, and (3) the output is text. We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for summarization and question answering. The results show that our framework can provide more faithful explanations of generated output than available alternatives, including LLM self-explanations. We open-source code for MExGen as part of the ICX360 toolkit: https://github$.$com/IBM/ICX360.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructing Optimal Noise Channels for Enhanced Robustness in Quantum Machine Learning</title>
<link>https://arxiv.org/abs/2404.16417</link>
<guid>https://arxiv.org/abs/2404.16417</guid>
<content:encoded><![CDATA[

arXiv:2404.16417v2 Announce Type: replace-cross 
Abstract: With the rapid advancement of Quantum Machine Learning (QML), the critical need to enhance security measures against adversarial attacks and protect QML models becomes increasingly evident. In this work, we outline the connection between quantum noise channels and differential privacy (DP), by constructing a family of noise channels which are inherently $\epsilon$-DP: $(\alpha, \gamma)$-channels. Through this approach, we successfully replicate the $\epsilon$-DP bounds observed for depolarizing and random rotation channels, thereby affirming the broad generality of our framework. Additionally, we use a semi-definite program to construct an optimally robust channel. In a small-scale experimental evaluation, we demonstrate the benefits of using our optimal noise channel over depolarizing noise, particularly in enhancing adversarial accuracy. Moreover, we assess how the variables $\alpha$ and $\gamma$ affect the certifiable robustness and investigate how different encoding methods impact the classifier's robustness.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Stickers on Multimodal Sentiment and Intent in Social Media: A New Task, Dataset and Baseline</title>
<link>https://arxiv.org/abs/2405.08427</link>
<guid>https://arxiv.org/abs/2405.08427</guid>
<content:encoded><![CDATA[

arXiv:2405.08427v2 Announce Type: replace-cross 
Abstract: Stickers are increasingly used in social media to express sentiment and intent. Despite their significant impact on sentiment analysis and intent recognition, little research has been conducted in this area. To address this gap, we propose a new task: \textbf{M}ultimodal chat \textbf{S}entiment \textbf{A}nalysis and \textbf{I}ntent \textbf{R}ecognition involving \textbf{S}tickers (MSAIRS). Additionally, we introduce a novel multimodal dataset containing Chinese chat records and stickers excerpted from several mainstream social media platforms. Our dataset includes paired data with the same text but different stickers, the same sticker but different contexts, and various stickers consisting of the same images with different texts, allowing us to better understand the impact of stickers on chat sentiment and intent. We also propose an effective multimodal joint model, MMSAIR, featuring differential vector construction and cascaded attention mechanisms for enhanced multimodal fusion. Our experiments demonstrate the necessity and effectiveness of jointly modeling sentiment and intent, as they mutually reinforce each other's recognition accuracy. MMSAIR significantly outperforms traditional models and advanced MLLMs, demonstrating the challenge and uniqueness of sticker interpretation in social media. Our dataset and code are available on https://github.com/FakerBoom/MSAIRS-Dataset.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-domain Multi-step Thinking: Zero-shot Fine-grained Traffic Sign Recognition in the Wild</title>
<link>https://arxiv.org/abs/2409.01534</link>
<guid>https://arxiv.org/abs/2409.01534</guid>
<content:encoded><![CDATA[

arXiv:2409.01534v2 Announce Type: replace-cross 
Abstract: In this study, we propose Cross-domain Multi-step Thinking (CdMT) to improve zero-shot fine-grained traffic sign recognition (TSR) performance in the wild. Zero-shot fine-grained TSR in the wild is challenging due to the cross-domain problem between clean template traffic signs and real-world counterparts, and existing approaches particularly struggle with cross-country TSR scenarios, where traffic signs typically differ between countries. The proposed CdMT framework tackles these challenges by leveraging the multi-step reasoning capabilities of large multimodal models (LMMs). We introduce context, characteristic, and differential descriptions to design multiple thinking processes for LMMs. Context descriptions, which are enhanced by center coordinate prompt optimization, enable the precise localization of target traffic signs in complex road images and filter irrelevant responses via novel prior traffic sign hypotheses. Characteristic descriptions, which are derived from in-context learning with template traffic signs, bridge cross-domain gaps and enhance fine-grained TSR. Differential descriptions refine the multimodal reasoning ability of LMMs by distinguishing subtle differences among similar signs. CdMT is independent of training data and requires only simple and uniform instructions, enabling it to achieve cross-country TSR. We conducted extensive experiments on three benchmark datasets and two real-world datasets from different countries. The proposed CdMT framework achieved superior performance compared with other state-of-the-art methods on all five datasets, with recognition accuracies of 0.93, 0.89, 0.97, 0.89, and 0.85 on the GTSRB, BTSD, TT-100K, Sapporo, and Yokohama datasets, respectively.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The FIX Benchmark: Extracting Features Interpretable to eXperts</title>
<link>https://arxiv.org/abs/2409.13684</link>
<guid>https://arxiv.org/abs/2409.13684</guid>
<content:encoded><![CDATA[

arXiv:2409.13684v4 Announce Type: replace-cross 
Abstract: Feature-based methods are commonly used to explain model predictions, but these methods often implicitly assume that interpretable features are readily available. However, this is often not the case for high-dimensional data, and it can be hard even for domain experts to mathematically specify which features are important. Can we instead automatically extract collections or groups of features that are aligned with expert knowledge? To address this gap, we present FIX (Features Interpretable to eXperts), a benchmark for measuring how well a collection of features aligns with expert knowledge. In collaboration with domain experts, we propose FIXScore, a unified expert alignment measure applicable to diverse real-world settings across cosmology, psychology, and medicine domains in vision, language, and time series data modalities. With FIXScore, we find that popular feature-based explanation methods have poor alignment with expert-specified knowledge, highlighting the need for new methods that can better identify features interpretable to experts.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadHMP: Backdoor Attack against Human Motion Prediction</title>
<link>https://arxiv.org/abs/2409.19638</link>
<guid>https://arxiv.org/abs/2409.19638</guid>
<content:encoded><![CDATA[

arXiv:2409.19638v2 Announce Type: replace-cross 
Abstract: Precise future human motion prediction over sub-second horizons from past observations is crucial for various safety-critical applications. To date, only a few studies have examined the vulnerability of skeleton-based neural networks to evasion and backdoor attacks. In this paper, we propose BadHMP, a novel backdoor attack that targets specifically human motion prediction tasks. Our approach involves generating poisoned training samples by embedding a localized backdoor trigger in one limb of the skeleton, causing selected joints to follow predefined motion in historical time steps. Subsequently, the future sequences are globally modified that all the joints move following the target trajectories. Our carefully designed backdoor triggers and targets guarantee the smoothness and naturalness of the poisoned samples, making them stealthy enough to evade detection by the model trainer while keeping the poisoned model unobtrusive in terms of prediction fidelity to untainted sequences. The target sequences can be successfully activated by the designed input sequences even with a low poisoned sample injection ratio. Experimental results on two datasets (Human3.6M and CMU-Mocap) and two network architectures (LTD and HRI) demonstrate the high-fidelity, effectiveness, and stealthiness of BadHMP. Robustness of our attack against fine-tuning defense is also verified.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language model developers should report train-test overlap</title>
<link>https://arxiv.org/abs/2410.08385</link>
<guid>https://arxiv.org/abs/2410.08385</guid>
<content:encoded><![CDATA[

arXiv:2410.08385v2 Announce Type: replace-cross 
Abstract: Language models are extensively evaluated, but correctly interpreting evaluation results requires knowledge of train-test overlap which refers to the extent to which the language model is trained on the very data it is being tested on. The public currently lacks adequate information about train-test overlap: most models have no public train-test overlap statistics, and third parties cannot directly measure train-test overlap since they do not have access to the training data. To make this clear, we document the practices of 30 model developers, finding that just 9 developers report train-test overlap: 4 developers release training data under open-source licenses, enabling the community to directly measure train-test overlap, and 5 developers publish their train-test overlap methodology and statistics. By engaging with language model developers, we provide novel information about train-test overlap for three additional developers. Overall, we take the position that language model developers should publish train-test overlap statistics and/or training data whenever they report evaluation results on public test sets. We hope our work increases transparency into train-test overlap to increase the community-wide trust in model evaluations.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vascular Segmentation of Functional Ultrasound Images using Deep Learning</title>
<link>https://arxiv.org/abs/2410.22365</link>
<guid>https://arxiv.org/abs/2410.22365</guid>
<content:encoded><![CDATA[

arXiv:2410.22365v2 Announce Type: replace-cross 
Abstract: Segmentation of medical images is a fundamental task with numerous applications. While MRI, CT, and PET modalities have significantly benefited from deep learning segmentation techniques, more recent modalities, like functional ultrasound (fUS), have seen limited progress. fUS is a non invasive imaging method that measures changes in cerebral blood volume (CBV) with high spatio-temporal resolution. However, distinguishing arterioles from venules in fUS is challenging due to opposing blood flow directions within the same pixel. Ultrasound localization microscopy (ULM) can enhance resolution by tracking microbubble contrast agents but is invasive, and lacks dynamic CBV quantification. In this paper, we introduce the first deep learning-based segmentation tool for fUS images, capable of differentiating signals from different vascular compartments, based on ULM automatic annotation and enabling dynamic CBV quantification. We evaluate various UNet architectures on fUS images of rat brains, achieving competitive segmentation performance, with 90% accuracy, a 71% F1 score, and an IoU of 0.59, using only 100 temporal frames from a fUS stack. These results are comparable to those from tubular structure segmentation in other imaging modalities. Additionally, models trained on resting-state data generalize well to images captured during visual stimulation, highlighting robustness. This work offers a non-invasive, cost-effective alternative to ULM, enhancing fUS data interpretation and improving understanding of vessel function. Our pipeline shows high linear correlation coefficients between signals from predicted and actual compartments in both cortical and deeper regions, showcasing its ability to accurately capture blood flow dynamics.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Coded Distributed Convolution Computing for Enhanced Straggler Resilience and Numerical Stability in Distributed CNNs</title>
<link>https://arxiv.org/abs/2411.01579</link>
<guid>https://arxiv.org/abs/2411.01579</guid>
<content:encoded><![CDATA[

arXiv:2411.01579v2 Announce Type: replace-cross 
Abstract: Deploying Convolutional Neural Networks (CNNs) on resource-constrained devices necessitates efficient management of computational resources, often via distributed environments susceptible to latency from straggler nodes. This paper introduces the Flexible Coded Distributed Convolution Computing (FCDCC) framework to enhance straggler resilience and numerical stability in distributed CNNs. We extend Coded Distributed Computing (CDC) with Circulant and Rotation Matrix Embedding (CRME) which was originally proposed for matrix multiplication to high-dimensional tensor convolution. For the proposed scheme, referred to as the Numerically Stable Coded Tensor Convolution (NSCTC) scheme, we also propose two new coded partitioning schemes: Adaptive-Padding Coded Partitioning (APCP) for the input tensor and Kernel-Channel Coded Partitioning (KCCP) for the filter tensor. These strategies enable linear decomposition of tensor convolutions and encoding them into CDC subtasks, combining model parallelism with coded redundancy for robust and efficient execution. Theoretical analysis identifies an optimal trade-off between communication and storage costs. Empirical results validate the framework's effectiveness in computational efficiency, straggler resilience, and scalability across various CNN architectures.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects</title>
<link>https://arxiv.org/abs/2411.10371</link>
<guid>https://arxiv.org/abs/2411.10371</guid>
<content:encoded><![CDATA[

arXiv:2411.10371v4 Announce Type: replace-cross 
Abstract: Event Causality Identification (ECI) has emerged as a pivotal task in natural language processing (NLP), aimed at automatically detecting causal relationships between events in text. In this comprehensive survey, we systematically elucidate the foundational principles and technical frameworks of ECI, proposing a novel classification framework to categorize and clarify existing methods. {We discuss associated challenges, provide quantitative evaluations, and outline future directions for this dynamic and rapidly evolving field. We first delineate key definitions, problem formalization, and evaluation protocols of ECI. Our classification framework organizes ECI methods based on two primary tasks: Sentence-level Event Causality Identification (SECI) and Document-level Event Causality Identification (DECI). For SECI, we review methods including feature pattern-based matching, machine learning-based classification, deep semantic encoding, prompt-based fine-tuning, and causal knowledge pre-training, alongside common data augmentation strategies. For DECI, we focus on techniques such as deep semantic encoding, event graph reasoning, and prompt-based fine-tuning. We dedicate specific discussions to advancements in multi-lingual and cross-lingual ECI as well as zero-shot ECI leveraging Large Language Models (LLMs). Furthermore, we analyze the strengths, limitations, and unresolved challenges of each method. Extensive quantitative evaluations are conducted on four benchmark datasets to assess various ECI methods. Finally, we explore future research directions.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Onto-LLM-TAMP: Knowledge-oriented Task and Motion Planning using Large Language Models</title>
<link>https://arxiv.org/abs/2412.07493</link>
<guid>https://arxiv.org/abs/2412.07493</guid>
<content:encoded><![CDATA[

arXiv:2412.07493v2 Announce Type: replace-cross 
Abstract: Performing complex manipulation tasks in dynamic environments requires efficient Task and Motion Planning (TAMP) approaches that combine high-level symbolic plans with low-level motion control. Advances in Large Language Models (LLMs), such as GPT-4, are transforming task planning by offering natural language as an intuitive and flexible way to describe tasks, generate symbolic plans, and reason. However, the effectiveness of LLM-based TAMP approaches is limited due to static and template-based prompting, which limits adaptability to dynamic environments and complex task contexts. To address these limitations, this work proposes a novel Onto-LLM-TAMP framework that employs knowledge-based reasoning to refine and expand user prompts with task-contextual reasoning and knowledge-based environment state descriptions. Integrating domain-specific knowledge into the prompt ensures semantically accurate and context-aware task plans. The proposed framework demonstrates its effectiveness by resolving semantic errors in symbolic plan generation, such as maintaining logical temporal goal ordering in scenarios involving hierarchical object placement. The proposed framework is validated through both simulation and real-world scenarios, demonstrating significant improvements over the baseline approach in terms of adaptability to dynamic environments and the generation of semantically correct task plans.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NVS-SQA: Exploring Self-Supervised Quality Representation Learning for Neurally Synthesized Scenes without References</title>
<link>https://arxiv.org/abs/2501.06488</link>
<guid>https://arxiv.org/abs/2501.06488</guid>
<content:encoded><![CDATA[

arXiv:2501.06488v2 Announce Type: replace-cross 
Abstract: Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting, effectively creates photorealistic scenes from sparse viewpoints, typically evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However, these full-reference methods, which compare synthesized views to reference views, may not fully capture the perceptual quality of neurally synthesized scenes (NSS), particularly due to the limited availability of dense reference views. Furthermore, the challenges in acquiring human perceptual labels hinder the creation of extensive labeled datasets, risking model overfitting and reduced generalizability. To address these issues, we propose NVS-SQA, a NSS quality assessment method to learn no-reference quality representations through self-supervision without reliance on human labels. Traditional self-supervised learning predominantly relies on the "same instance, similar representation" assumption and extensive datasets. However, given that these conditions do not apply in NSS quality assessment, we employ heuristic cues and quality scores as learning objectives, along with a specialized contrastive pair preparation process to improve the effectiveness and efficiency of learning. The results show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e., on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second best) and even exceeds 16 full-reference methods across all evaluation metrics (i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alleviating Seasickness through Brain-Computer Interface-based Attention Shift</title>
<link>https://arxiv.org/abs/2501.08518</link>
<guid>https://arxiv.org/abs/2501.08518</guid>
<content:encoded><![CDATA[

arXiv:2501.08518v2 Announce Type: replace-cross 
Abstract: Seasickness poses a widespread problem that adversely impacts both passenger comfort and the operational efficiency of maritime crews. Although attention shift has been proposed as a potential method to alleviate symptoms of motion sickness, its efficacy remains to be rigorously validated, especially in maritime environments. In this study, we develop an AI-driven brain-computer interface (BCI) to realize sustained and practical attention shift by incorporating tasks such as breath counting. Forty-three participants completed a real-world nautical experiment consisting of a real-feedback session, a resting session, and a pseudo-feedback session. Notably, 81.39\% of the participants reported that the BCI intervention was effective. EEG analysis revealed that the proposed system can effectively regulate motion sickness EEG signatures, such as an decrease in total band power, along with an increase in theta relative power and a decrease in beta relative power. Furthermore, an indicator of attentional focus, the theta/beta ratio, exhibited a significant reduction during the real-feedback session, providing further evidence to support the effectiveness of the BCI in shifting attention. Collectively, this study presents a novel nonpharmacological, portable, and effective approach for seasickness intervention, which has the potential to open up a brand-new application domain for BCIs.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with Retrieval-Augmented Learning</title>
<link>https://arxiv.org/abs/2501.12296</link>
<guid>https://arxiv.org/abs/2501.12296</guid>
<content:encoded><![CDATA[

arXiv:2501.12296v3 Announce Type: replace-cross 
Abstract: In the pursuit of robust autonomous driving systems, models trained on real-world datasets often struggle to adapt to new environments, particularly when confronted with corner cases such as extreme weather conditions. Collecting these corner cases in the real world is non-trivial, which necessitates the use of simulators for validation. However,the high computational cost and the domain gap in data distribution have hindered the seamless transition between real and simulated driving scenarios. To tackle this challenge, we propose Retrieval-Augmented Learning for Autonomous Driving (RALAD), a novel framework designed to bridge the real-to-sim gap at a low cost. RALAD features three primary designs, including (1) domain adaptation via an enhanced Optimal Transport (OT) method that accounts for both individual and grouped image distances, (2) a simple and unified framework that can be applied to various models, and (3) efficient fine-tuning techniques that freeze the computationally expensive layers while maintaining robustness. Experimental results demonstrate that RALAD compensates for the performance degradation in simulated environments while maintaining accuracy in real-world scenarios across three different models. Taking Cross View as an example, the mIOU and mAP metrics in real-world scenarios remain stable before and after RALAD fine-tuning, while in simulated environments,the mIOU and mAP metrics are improved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of our approach is reduced by approximately 88.1%. Our code is available at https://github.com/JiachengZuo/RALAD.git.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</title>
<link>https://arxiv.org/abs/2501.13926</link>
<guid>https://arxiv.org/abs/2501.13926</guid>
<content:encoded><![CDATA[

arXiv:2501.13926v2 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) reasoning has been extensively explored in large models to tackle complex understanding tasks. However, it still remains an open question whether such strategies can be applied to verifying and reinforcing image generation scenarios. In this paper, we provide the first comprehensive investigation of the potential of CoT reasoning to enhance autoregressive image generation. We focus on three techniques: scaling test-time computation for verification, aligning model preferences with Direct Preference Optimization (DPO), and integrating these techniques for complementary effects. Our results demonstrate that these approaches can be effectively adapted and combined to significantly improve image generation performance. Furthermore, given the pivotal role of reward models in our findings, we propose the Potential Assessment Reward Model (PARM) and PARM++, specialized for autoregressive image generation. PARM adaptively assesses each generation step through a potential assessment approach, merging the strengths of existing reward models, and PARM++ further introduces a reflection mechanism to self-correct the generated unsatisfactory image, which is the first to incorporate reflection in autoregressive image generation. Using our investigated reasoning strategies, we enhance a baseline model, Show-o, to achieve superior results, with a significant +24% improvement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We hope our study provides unique insights and paves a new path for integrating CoT reasoning with autoregressive image generation. Code and models are released at https://github.com/ZiyuGuo99/Image-Generation-CoT
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Privacy-Utility Trade-off in Decentralized Learning with Generalized Correlated Noise</title>
<link>https://arxiv.org/abs/2501.14644</link>
<guid>https://arxiv.org/abs/2501.14644</guid>
<content:encoded><![CDATA[

arXiv:2501.14644v2 Announce Type: replace-cross 
Abstract: Decentralized learning enables distributed agents to collaboratively train a shared machine learning model without a central server, through local computation and peer-to-peer communication. Although each agent retains its dataset locally, sharing local models can still expose private information about the local training datasets to adversaries. To mitigate privacy attacks, a common strategy is to inject random artificial noise at each agent before exchanging local models between neighbors. However, this often leads to utility degradation due to the negative effects of cumulated artificial noise on the learning algorithm. In this work, we introduce CorN-DSGD, a novel covariance-based framework for generating correlated privacy noise across agents, which unifies several state-of-the-art methods as special cases. By leveraging network topology and mixing weights, CorN-DSGD optimizes the noise covariance to achieve network-wide noise cancellation. Experimental results show that CorN-DSGD cancels more noise than existing pairwise correlation schemes, improving model performance under formal privacy guarantees.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks for O-RAN Mobility Management: A Link Prediction Approach</title>
<link>https://arxiv.org/abs/2502.02170</link>
<guid>https://arxiv.org/abs/2502.02170</guid>
<content:encoded><![CDATA[

arXiv:2502.02170v2 Announce Type: replace-cross 
Abstract: Mobility performance has been a key focus in cellular networks up to 5G. To enhance handover (HO) performance, 3GPP introduced Conditional Handover (CHO) and Layer 1/Layer 2 Triggered Mobility (LTM) mechanisms in 5G. While these reactive HO strategies address the trade-off between HO failures (HOF) and ping-pong effects, they often result in inefficient radio resource utilization due to additional HO preparations. To overcome these challenges, this article proposes a proactive HO framework for mobility management in O-RAN, leveraging user-cell link predictions to identify the optimal target cell for HO. We explore various categories of Graph Neural Networks (GNNs) for link prediction and analyze the complexity of applying them to the mobility management domain. Two GNN models are compared using a real-world dataset, with experimental results demonstrating their ability to capture the dynamic and graph-structured nature of cellular networks. Finally, we present key insights from our study and outline future steps to enable the integration of GNN-based link prediction for mobility management in O-RAN networks.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAPID-Net: Accurate Pocket Identification for Binding-Site-Agnostic Docking</title>
<link>https://arxiv.org/abs/2502.02371</link>
<guid>https://arxiv.org/abs/2502.02371</guid>
<content:encoded><![CDATA[

arXiv:2502.02371v2 Announce Type: replace-cross 
Abstract: Accurate identification of druggable pockets and their features is essential for structure-based drug design and effective downstream docking. Here, we present RAPID-Net, a deep learning-based algorithm designed for the accurate prediction of binding pockets and seamless integration with docking pipelines. On the PoseBusters benchmark, RAPID-Net-guided AutoDock Vina achieves 54.9% of Top-1 poses with RMSD < 2 A and satisfying the PoseBusters chemical-validity criterion, compared to 49.1% for DiffBindFR. On the most challenging time split of PoseBusters aiming to assess generalization ability (structures submitted after September 30, 2021), RAPID-Net-guided AutoDock Vina achieves 53.1% of Top-1 poses with RMSD < 2 A and PB-valid, versus 59.5% for AlphaFold 3. Notably, in 92.2% of cases, RAPID-Net-guided Vina samples at least one pose with RMSD < 2 A (regardless of its rank), indicating that pose ranking, rather than sampling, is the primary accuracy bottleneck. The lightweight inference, scalability, and competitive accuracy of RAPID-Net position it as a viable option for large-scale virtual screening campaigns. Across diverse benchmark datasets, RAPID-Net outperforms other pocket prediction tools, including PUResNet and Kalasanty, in both docking accuracy and pocket-ligand intersection rates. Furthermore, we demonstrate the potential of RAPID-Net to accelerate the development of novel therapeutics by highlighting its performance on pharmacologically relevant targets. RAPID-Net accurately identifies distal functional sites, offering new opportunities for allosteric inhibitor design. In the case of the RNA-dependent RNA polymerase of SARS-CoV-2, RAPID-Net uncovers a wider array of potential binding pockets than existing predictors, which typically annotate only the orthosteric pocket and overlook secondary cavities.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance</title>
<link>https://arxiv.org/abs/2502.05236</link>
<guid>https://arxiv.org/abs/2502.05236</guid>
<content:encoded><![CDATA[

arXiv:2502.05236v2 Announce Type: replace-cross 
Abstract: While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2503.02382</link>
<guid>https://arxiv.org/abs/2503.02382</guid>
<content:encoded><![CDATA[

arXiv:2503.02382v2 Announce Type: replace-cross 
Abstract: Enhancing the mathematical reasoning capabilities of Large Language Models (LLMs) is of great scientific and practical significance. Researchers typically employ process-supervised reward models (PRMs) to guide the reasoning process, effectively improving the models' reasoning abilities. However, existing methods for constructing process supervision training data, such as manual annotation and per-step Monte Carlo estimation, are often costly or suffer from poor quality. To address these challenges, this paper introduces a framework called EpicPRM, which annotates each intermediate reasoning step based on its quantified contribution and uses an adaptive binary search algorithm to enhance both annotation precision and efficiency. Using this approach, we efficiently construct a high-quality process supervision training dataset named Epic50k, consisting of 50k annotated intermediate steps. Compared to other publicly available datasets, the PRM trained on Epic50k demonstrates significantly superior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation</title>
<link>https://arxiv.org/abs/2503.02832</link>
<guid>https://arxiv.org/abs/2503.02832</guid>
<content:encoded><![CDATA[

arXiv:2503.02832v3 Announce Type: replace-cross 
Abstract: In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORANSight-2.0: Foundational LLMs for O-RAN</title>
<link>https://arxiv.org/abs/2503.05200</link>
<guid>https://arxiv.org/abs/2503.05200</guid>
<content:encoded><![CDATA[

arXiv:2503.05200v2 Announce Type: replace-cross 
Abstract: Despite the transformative impact of Large Language Models (LLMs) across critical domains such as healthcare, customer service, and business marketing, their integration into Open Radio Access Networks (O-RAN) remains limited. This gap is primarily due to the absence of domain-specific foundational models, with existing solutions often relying on general-purpose LLMs that fail to address the unique challenges and technical intricacies of O-RAN. To bridge this gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiative to develop specialized foundational LLMs tailored for O-RAN. Built on 18 models spanning five open-source LLM frameworks -- Mistral, Qwen, Llama, Phi, and Gemma -- ORANSight-2.0 fine-tunes models ranging from 1B to 70B parameters, significantly reducing reliance on proprietary, closed-source models while enhancing performance in O-RAN-specific tasks. At the core of ORANSight-2.0 is RANSTRUCT, a novel Retrieval-Augmented Generation (RAG)-based instruction-tuning framework that employs two LLM agents -- a Mistral-based Question Generator and a Qwen-based Answer Generator -- to create high-quality instruction-tuning datasets. The generated dataset is then used to fine-tune the 18 pre-trained open-source LLMs via QLoRA. To evaluate ORANSight-2.0, we introduce srsRANBench, a novel benchmark designed for code generation and codebase understanding in the context of srsRAN, a widely used 5G O-RAN stack.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Approach for Augmenting Perceptional Understanding of Histopathology Images</title>
<link>https://arxiv.org/abs/2503.06894</link>
<guid>https://arxiv.org/abs/2503.06894</guid>
<content:encoded><![CDATA[

arXiv:2503.06894v3 Announce Type: replace-cross 
Abstract: In Recent Years, Digital Technologies Have Made Significant Strides In Augmenting-Human-Health, Cognition, And Perception, Particularly Within The Field Of Computational-Pathology. This Paper Presents A Novel Approach To Enhancing The Analysis Of Histopathology Images By Leveraging A Mult-modal-Model That Combines Vision Transformers (Vit) With Gpt-2 For Image Captioning. The Model Is Fine-Tuned On The Specialized Arch-Dataset, Which Includes Dense Image Captions Derived From Clinical And Academic Resources, To Capture The Complexities Of Pathology Images Such As Tissue Morphologies, Staining Variations, And Pathological Conditions. By Generating Accurate, Contextually Captions, The Model Augments The Cognitive Capabilities Of Healthcare Professionals, Enabling More Efficient Disease Classification, Segmentation, And Detection. The Model Enhances The Perception Of Subtle Pathological Features In Images That Might Otherwise Go Unnoticed, Thereby Improving Diagnostic Accuracy. Our Approach Demonstrates The Potential For Digital Technologies To Augment Human Cognitive Abilities In Medical Image Analysis, Providing Steps Toward More Personalized And Accurate Healthcare Outcomes.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder</title>
<link>https://arxiv.org/abs/2503.11937</link>
<guid>https://arxiv.org/abs/2503.11937</guid>
<content:encoded><![CDATA[

arXiv:2503.11937v3 Announce Type: replace-cross 
Abstract: Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the Attribute (Att) Adapter, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning. We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world. Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICCO: Learning an Instruction-conditioned Coordinator for Language-guided Task-aligned Multi-robot Control</title>
<link>https://arxiv.org/abs/2503.12122</link>
<guid>https://arxiv.org/abs/2503.12122</guid>
<content:encoded><![CDATA[

arXiv:2503.12122v2 Announce Type: replace-cross 
Abstract: Recent advances in Large Language Models (LLMs) have permitted the development of language-guided multi-robot systems, which allow robots to execute tasks based on natural language instructions. However, achieving effective coordination in distributed multi-agent environments remains challenging due to (1) misalignment between instructions and task requirements and (2) inconsistency in robot behaviors when they independently interpret ambiguous instructions. To address these challenges, we propose Instruction-Conditioned Coordinator (ICCO), a Multi-Agent Reinforcement Learning (MARL) framework designed to enhance coordination in language-guided multi-robot systems. ICCO consists of a Coordinator agent and multiple Local Agents, where the Coordinator generates Task-Aligned and Consistent Instructions (TACI) by integrating language instructions with environmental states, ensuring task alignment and behavioral consistency. The Coordinator and Local Agents are jointly trained to optimize a reward function that balances task efficiency and instruction following. A Consistency Enhancement Term is added to the learning objective to maximize mutual information between instructions and robot behaviors, further improving coordination. Simulation and real-world experiments validate the effectiveness of ICCO in achieving language-guided task-aligned multi-robot control. The demonstration can be found at https://yanoyoshiki.github.io/ICCO/.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Detecting Persuasion on Social Media: From Model Development to Insights on Persuasion Strategies</title>
<link>https://arxiv.org/abs/2503.13844</link>
<guid>https://arxiv.org/abs/2503.13844</guid>
<content:encoded><![CDATA[

arXiv:2503.13844v2 Announce Type: replace-cross 
Abstract: Political advertising plays a pivotal role in shaping public opinion and influencing electoral outcomes, often through subtle persuasive techniques embedded in broader propaganda strategies. Detecting these persuasive elements is crucial for enhancing voter awareness and ensuring transparency in democratic processes. This paper presents an integrated approach that bridges model development and real-world application through two interconnected studies. First, we introduce a lightweight model for persuasive text detection that achieves state-of-the-art performance in Subtask 3 of SemEval 2023 Task 3 while requiring significantly fewer computational resources and training data than existing methods. Second, we demonstrate the model's practical utility by collecting the Australian Federal Election 2022 Facebook Ads (APA22) dataset, partially annotating a subset for persuasion, and fine-tuning the model to adapt from mainstream news to social media content. We then apply the fine-tuned model to label the remainder of the APA22 dataset, revealing distinct patterns in how political campaigns leverage persuasion through different funding strategies, word choices, demographic targeting, and temporal shifts in persuasion intensity as election day approaches. Our findings not only underscore the necessity of domain-specific modeling for analyzing persuasion on social media but also show how uncovering these strategies can enhance transparency, inform voters, and promote accountability in digital campaigns.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference</title>
<link>https://arxiv.org/abs/2503.23956</link>
<guid>https://arxiv.org/abs/2503.23956</guid>
<content:encoded><![CDATA[

arXiv:2503.23956v3 Announce Type: replace-cross 
Abstract: Recent advancements in Large Visual Language Models (LVLMs) have gained significant attention due to their remarkable reasoning capabilities and proficiency in generalization. However, processing a large number of visual tokens and generating long-context outputs impose substantial computational overhead, leading to excessive demands for key-value (KV) cache. To address this critical bottleneck, we propose AirCache, a novel KV cache compression method aimed at accelerating LVLMs inference. This work systematically investigates the correlations between visual and textual tokens within the attention mechanisms of LVLMs. Our empirical analysis reveals considerable redundancy in cached visual tokens, wherein strategically eliminating these tokens preserves model performance while significantly accelerating context generation. Inspired by these findings, we introduce an elite observation window for assessing the importance of visual components in the KV cache, focusing on stable inter-modal relevancy modeling with enhanced multi-perspective consistency. Additionally, we develop an adaptive layer-wise budget allocation strategy that capitalizes on the strength and skewness of token importance distribution, showcasing superior efficiency compared to uniform allocation. Comprehensive evaluations across multiple LVLMs and benchmarks demonstrate that our method achieves comparable performance to the full cache while retaining only 10% of visual KV cache, thereby reducing decoding latency by 29% to 66% across various batch size and prompt length of inputs. Notably, as cache retention rates decrease, our method exhibits increasing performance advantages over existing approaches.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-Based Multiscale Temporal Fusion Network for Uncertain-Mode Fault Diagnosis in Multimode Processes</title>
<link>https://arxiv.org/abs/2504.05172</link>
<guid>https://arxiv.org/abs/2504.05172</guid>
<content:encoded><![CDATA[

arXiv:2504.05172v3 Announce Type: replace-cross 
Abstract: Fault diagnosis in multimode processes plays a critical role in ensuring the safe operation of industrial systems across multiple modes. It faces a great challenge yet to be addressed - that is, the significant distributional differences among monitoring data from multiple modes make it difficult for the models to extract shared feature representations related to system health conditions. In response to this problem, this paper introduces a novel method called attention-based multiscale temporal fusion network. The multiscale depthwise convolution and gated recurrent unit are employed to extract multiscale contextual local features and long-short-term features. Instance normalization is applied to suppress mode-specific information. Furthermore, a temporal attention mechanism is designed to focus on critical time points with higher cross-mode shared information, thereby enhancing the accuracy of fault diagnosis. The proposed model is applied to Tennessee Eastman process dataset and three-phase flow facility dataset. The experiments demonstrate that the proposed model achieves superior diagnostic performance and maintains a small model size. The source code will be available on GitHub at https://github.com/GuangqiangLi/AMTFNet.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models</title>
<link>https://arxiv.org/abs/2504.05815</link>
<guid>https://arxiv.org/abs/2504.05815</guid>
<content:encoded><![CDATA[

arXiv:2504.05815v2 Announce Type: replace-cross 
Abstract: Recently, the diffusion model has gained significant attention as one of the most successful image generation models, which can generate high-quality images by iteratively sampling noise. However, recent studies have shown that diffusion models are vulnerable to backdoor attacks, allowing attackers to enter input data containing triggers to activate the backdoor and generate their desired output. Existing backdoor attack methods primarily focused on target noise-to-image and text-to-image tasks, with limited work on backdoor attacks in image-to-image tasks. Furthermore, traditional backdoor attacks often rely on a single, conspicuous trigger to generate a fixed target image, lacking concealability and flexibility. To address these limitations, we propose a novel backdoor attack method called "Parasite" for image-to-image tasks in diffusion models, which not only is the first to leverage steganography for triggers hiding, but also allows attackers to embed the target content as a backdoor trigger to achieve a more flexible attack. "Parasite" as a novel attack method effectively bypasses existing detection frameworks to execute backdoor attacks. In our experiments, "Parasite" achieved a 0 percent backdoor detection rate against the mainstream defense frameworks. In addition, in the ablation study, we discuss the influence of different hiding coefficients on the attack results. You can find our code at https://anonymous.4open.science/r/Parasite-1715/.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping Industry Practices to the EU AI Act's GPAI Code of Practice Safety and Security Measures</title>
<link>https://arxiv.org/abs/2504.15181</link>
<guid>https://arxiv.org/abs/2504.15181</guid>
<content:encoded><![CDATA[

arXiv:2504.15181v2 Announce Type: replace-cross 
Abstract: This report provides a detailed comparison between the Safety and Security measures proposed in the EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and the current commitments and practices voluntarily adopted by leading AI companies. As the EU moves toward enforcing binding obligations for GPAI model providers, the Code of Practice will be key for bridging legal requirements with concrete technical commitments. Our analysis focuses on the draft's Safety and Security section (Commitments II.1-II.16), documenting excerpts from current public-facing documents that are relevant to each individual measure.
  We systematically reviewed different document types, such as companies' frontier safety frameworks and model cards, from over a dozen companies, including OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and others. This report is not meant to be an indication of legal compliance, nor does it take any prescriptive viewpoint about the Code of Practice or companies' policies. Instead, it aims to inform the ongoing dialogue between regulators and General-Purpose AI model providers by surfacing evidence of industry precedent for various measures. Nonetheless, we were able to find relevant quotes from at least 5 companies' documents for the majority of the measures in Commitments II.1-II.16.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification</title>
<link>https://arxiv.org/abs/2504.18046</link>
<guid>https://arxiv.org/abs/2504.18046</guid>
<content:encoded><![CDATA[

arXiv:2504.18046v2 Announce Type: replace-cross 
Abstract: Ophthalmic diseases pose a significant global health challenge, yet traditional diagnosis methods and existing single-eye deep learning approaches often fail to account for binocular pathological correlations. To address this, we propose DMS-Net, a dual-modal multi-scale Siamese network for binocular fundus image classification. Our framework leverages weight-shared Siamese ResNet-152 backbones to extract deep semantic features from paired fundus images. To tackle challenges such as lesion boundary ambiguity and scattered pathological distributions, we introduce a Multi-Scale Context-Aware Module (MSCAM) that integrates adaptive pooling and attention mechanisms for multi-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion (DMFF) module enhances cross-modal interaction through spatial-semantic recalibration and bidirectional attention, effectively combining global context and local edge features. Evaluated on the ODIR-5K dataset, DMS-Net achieves state-of-the-art performance with 82.9% accuracy, 84.5% recall, and 83.2% Cohen's kappa, demonstrating superior capability in detecting symmetric pathologies and advancing clinical decision-making for ocular diseases.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations</title>
<link>https://arxiv.org/abs/2504.19155</link>
<guid>https://arxiv.org/abs/2504.19155</guid>
<content:encoded><![CDATA[

arXiv:2504.19155v2 Announce Type: replace-cross 
Abstract: To develop a machine learning-based framework for accurately modeling the anode heel effect in Monte Carlo simulations of X-ray imaging systems, enabling realistic beam intensity profiles with minimal experimental calibration. Multiple regression models were trained to predict spatial intensity variations along the anode-cathode axis using experimentally acquired weights derived from beam measurements across different tube potentials. These weights captured the asymmetry introduced by the anode heel effect. A systematic fine-tuning protocol was established to minimize the number of required measurements while preserving model accuracy. The models were implemented in the OpenGATE 10 and GGEMS Monte Carlo toolkits to evaluate their integration feasibility and predictive performance. Among the tested models, gradient boosting regression (GBR) delivered the highest accuracy, with prediction errors remaining below 5% across all energy levels. The optimized fine-tuning strategy required only six detector positions per energy level, reducing measurement effort by 65%. The maximum error introduced through this fine-tuning process remained below 2%. Dose actor comparisons within Monte Carlo simulations demonstrated that the GBR-based model closely replicated clinical beam profiles and significantly outperformed conventional symmetric beam models. This study presents a robust and generalizable method for incorporating the anode heel effect into Monte Carlo simulations using machine learning. By enabling accurate, energy-dependent beam modeling with limited calibration data, the approach enhances simulation realism for applications in clinical dosimetry, image quality assessment, and radiation protection.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery</title>
<link>https://arxiv.org/abs/2504.19996</link>
<guid>https://arxiv.org/abs/2504.19996</guid>
<content:encoded><![CDATA[

arXiv:2504.19996v2 Announce Type: replace-cross 
Abstract: The widespread use of Exogenous Organic Matter in agriculture necessitates monitoring to assess its effects on soil and crop health. This study evaluates optical Sentinel-2 satellite imagery for detecting digestate application, a practice that enhances soil fertility but poses environmental risks like microplastic contamination and nitrogen losses. In the first instance, Sentinel-2 satellite image time series (SITS) analysis of specific indices (EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after application on the soils of four different crop types in Thessaly, Greece. Furthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient Boosting and a Feed-Forward Neural Network), were used to investigate digestate presence detection, achieving F1-scores up to 0.85. The findings highlight the potential of combining remote sensing and ML for scalable and cost-effective monitoring of EOM applications, supporting precision agriculture and sustainability.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.01709</link>
<guid>https://arxiv.org/abs/2505.01709</guid>
<content:encoded><![CDATA[

arXiv:2505.01709v3 Announce Type: replace-cross 
Abstract: Operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. While recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. Existing methods often compromise cognitive and executive capabilities. To address these challenges, in this paper, we propose RoBridge, a hierarchical intelligent architecture for general robotic manipulation. It consists of a high-level cognitive planner (HCP) based on a large-scale pre-trained vision-language model (VLM), an invariant operable representation (IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA). RoBridge maintains the declarative skill of VLM and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. RoBridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. This work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of YOLOv8 in monocular downward multiple Car Target detection</title>
<link>https://arxiv.org/abs/2505.10016</link>
<guid>https://arxiv.org/abs/2505.10016</guid>
<content:encoded><![CDATA[

arXiv:2505.10016v2 Announce Type: replace-cross 
Abstract: Autonomous driving technology is progressively transforming traditional car driving methods, marking a significant milestone in modern transportation. Object detection serves as a cornerstone of autonomous systems, playing a vital role in enhancing driving safety, enabling autonomous functionality, improving traffic efficiency, and facilitating effective emergency responses. However, current technologies such as radar for environmental perception, cameras for road perception, and vehicle sensor networks face notable challenges, including high costs, vulnerability to weather and lighting conditions, and limited resolution.To address these limitations, this paper presents an improved autonomous target detection network based on YOLOv8. By integrating structural reparameterization technology, a bidirectional pyramid structure network model, and a novel detection pipeline into the YOLOv8 framework, the proposed approach achieves highly efficient and precise detection of multi-scale, small, and remote objects. Experimental results demonstrate that the enhanced model can effectively detect both large and small objects with a detection accuracy of 65%, showcasing significant advancements over traditional methods.This improved model holds substantial potential for real-world applications and is well-suited for autonomous driving competitions, such as the Formula Student Autonomous China (FSAC), particularly excelling in scenarios involving single-target and small-object detection.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction</title>
<link>https://arxiv.org/abs/2505.10027</link>
<guid>https://arxiv.org/abs/2505.10027</guid>
<content:encoded><![CDATA[

arXiv:2505.10027v2 Announce Type: replace-cross 
Abstract: With the rapid advancement of remote sensing technology, super-resolution image reconstruction is of great research and practical significance. Existing deep learning methods have made progress but still face limitations in handling complex scenes and preserving image details. This paper proposes a reinforcement learning-based latent diffusion model (LDM) fine-tuning method for remote sensing image super-resolution. The method constructs a reinforcement learning environment with states, actions, and rewards, optimizing decision objectives through proximal policy optimization (PPO) during the reverse denoising process of the LDM model. Experiments on the RESISC45 dataset show significant improvements over the baseline model in PSNR, SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural scenes. The results demonstrate the method's effectiveness in enhancing super-resolution quality and adaptability across scenes.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding</title>
<link>https://arxiv.org/abs/2505.18079</link>
<guid>https://arxiv.org/abs/2505.18079</guid>
<content:encoded><![CDATA[

arXiv:2505.18079v3 Announce Type: replace-cross 
Abstract: Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code has been released in https://github.com/microsoft/DeepVideoDiscovery.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robot Operation of Home Appliances by Reading User Manuals</title>
<link>https://arxiv.org/abs/2505.20424</link>
<guid>https://arxiv.org/abs/2505.20424</guid>
<content:encoded><![CDATA[

arXiv:2505.20424v2 Announce Type: replace-cross 
Abstract: Operating home appliances, among the most common tools in every household, is a critical capability for assistive home robots. This paper presents ApBot, a robot system that operates novel household appliances by "reading" their user manuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial policies from their unstructured, textual descriptions in a user manual document, (ii) ground the policies to the appliance in the physical world, and (iii) execute the policies reliably over potentially many steps, despite compounding errors. To tackle these challenges, ApBot constructs a structured, symbolic model of an appliance from its manual, with the help of a large vision-language model (VLM). It grounds the symbolic actions visually to control panel elements. Finally, ApBot closes the loop by updating the model based on visual feedback. Our experiments show that across a wide range of simulated and real-world appliances, ApBot achieves consistent and statistically significant improvements in task success rate, compared with state-of-the-art large VLMs used directly as control policies. These results suggest that a structured internal representations plays an important role in robust robot operation of home appliances, especially, complex ones.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start</title>
<link>https://arxiv.org/abs/2505.22334</link>
<guid>https://arxiv.org/abs/2505.22334</guid>
<content:encoded><![CDATA[

arXiv:2505.22334v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While "aha moment" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Sparse-Matrix Representations for Diverse Neural Architectures</title>
<link>https://arxiv.org/abs/2506.01966</link>
<guid>https://arxiv.org/abs/2506.01966</guid>
<content:encoded><![CDATA[

arXiv:2506.01966v3 Announce Type: replace-cross 
Abstract: Deep neural networks employ specialized architectures for vision, sequential and language tasks, yet this proliferation obscures their underlying commonalities. We introduce a unified matrix-order framework that casts convolutional, recurrent and self-attention operations as sparse matrix multiplications. Convolution is realized via an upper-triangular weight matrix performing first-order transformations; recurrence emerges from a lower-triangular matrix encoding stepwise updates; attention arises naturally as a third-order tensor factorization. We prove algebraic isomorphism with standard CNN, RNN and Transformer layers under mild assumptions. Empirical evaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet), time-series forecasting (ETTh1, Electricity Load Diagrams) and language modeling/classification (AG News, WikiText-2, Penn Treebank) confirm that sparse-matrix formulations match or exceed native model performance while converging in comparable or fewer epochs. By reducing architecture design to sparse pattern selection, our matrix perspective aligns with GPU parallelism and leverages mature algebraic optimization tools. This work establishes a mathematically rigorous substrate for diverse neural architectures and opens avenues for principled, hardware-aware network design.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider</title>
<link>https://arxiv.org/abs/2506.02634</link>
<guid>https://arxiv.org/abs/2506.02634</guid>
<content:encoded><![CDATA[

arXiv:2506.02634v4 Announce Type: replace-cross 
Abstract: Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRI-CORE: A Foundation Model for Magnetic Resonance Imaging</title>
<link>https://arxiv.org/abs/2506.12186</link>
<guid>https://arxiv.org/abs/2506.12186</guid>
<content:encoded><![CDATA[

arXiv:2506.12186v2 Announce Type: replace-cross 
Abstract: The widespread use of Magnetic Resonance Imaging (MRI) in combination with deep learning shows promise for many high-impact automated diagnostic and prognostic tools. However, training new models requires large amounts of labeled data, a challenge due to high cost of precise annotations and data privacy. To address this issue, we introduce the MRI-CORE, a vision foundation model trained using more than 6 million slices from over 110 thousand MRI volumes across 18 body locations. Our experiments show notable improvements in performance over state-of-the-art methods in 13 data-restricted segmentation tasks, as well as in image classification, and zero-shot segmentation, showing the strong potential of MRI-CORE to enable data-efficient development of artificial intelligence models. We also present data on which strategies yield most useful foundation models and a novel analysis relating similarity between pre-training and downstream task data with transfer learning performance. Our model is publicly available with a permissive license.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EXGnet: a single-lead explainable-AI guided multiresolution network with train-only quantitative features for trustworthy ECG arrhythmia classification</title>
<link>https://arxiv.org/abs/2506.12404</link>
<guid>https://arxiv.org/abs/2506.12404</guid>
<content:encoded><![CDATA[

arXiv:2506.12404v2 Announce Type: replace-cross 
Abstract: Deep learning has significantly propelled the performance of ECG arrhythmia classification, yet its clinical adoption remains hindered by challenges in interpretability and deployment on resource-constrained edge devices. To bridge this gap, we propose EXGnet, a novel and reliable ECG arrhythmia classification network tailored for single-lead signals, specifically designed to balance high accuracy, explainability, and edge compatibility. EXGnet integrates XAI supervision during training via a normalized cross-correlation based loss, directing the model's attention to clinically relevant ECG regions, similar to a cardiologist's focus. This supervision is driven by automatically generated ground truth, derived through an innovative heart rate variability-based approach, without the need for manual annotation. To enhance classification accuracy without compromising deployment simplicity, we incorporate quantitative ECG features during training. These enrich the model with multi-domain knowledge but are excluded during inference, keeping the model lightweight for edge deployment. Additionally, we introduce an innovative multiresolution block to efficiently capture both short and long-term signal features while maintaining computational efficiency. Rigorous evaluation on the Chapman and Ningbo benchmark datasets validates the supremacy of EXGnet, which achieves average five-fold accuracies of 98.762% and 96.932%, and F1-scores of 97.910% and 95.527%, respectively. Comprehensive ablation studies and both quantitative and qualitative interpretability assessment confirm that the XAI guidance is pivotal, demonstrably enhancing the model's focus and trustworthiness. Overall, EXGnet sets a new benchmark by combining high-performance arrhythmia classification with interpretability, paving the way for more trustworthy and accessible portable ECG based health monitoring systems.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning</title>
<link>https://arxiv.org/abs/2506.15606</link>
<guid>https://arxiv.org/abs/2506.15606</guid>
<content:encoded><![CDATA[

arXiv:2506.15606v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have become indispensable in real-world applications. However, their widespread adoption raises significant safety concerns, particularly in responding to socially harmful questions. Despite substantial efforts to improve model safety through alignment, aligned models can still have their safety protections undermined by subsequent fine-tuning - even when the additional training data appears benign. In this paper, we empirically demonstrate that this vulnerability stems from the sensitivity of safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building on this insight, we propose a novel training-free method, termed Low-Rank Extrapolation (LoX), to enhance safety robustness by extrapolating the safety subspace of an aligned LLM. Our experimental results confirm the effectiveness of LoX, demonstrating significant improvements in robustness against both benign and malicious fine-tuning attacks while preserving the model's adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute reductions in attack success rates (ASR) facing benign or malicious fine-tuning attacks. By investigating the ASR landscape of parameters, we attribute the success of LoX to that the extrapolation moves LLM parameters to a flatter zone, thereby less sensitive to perturbations. The code is available at github.com/VITA-Group/LoX.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Public Perceptions of Science in Media</title>
<link>https://arxiv.org/abs/2506.16622</link>
<guid>https://arxiv.org/abs/2506.16622</guid>
<content:encoded><![CDATA[

arXiv:2506.16622v2 Announce Type: replace-cross 
Abstract: Effectively engaging the public with science is vital for fostering trust and understanding in our scientific community. Yet, with an ever-growing volume of information, science communicators struggle to anticipate how audiences will perceive and interact with scientific news. In this paper, we introduce a computational framework that models public perception across twelve dimensions, such as newsworthiness, importance, and surprisingness. Using this framework, we create a large-scale science news perception dataset with 10,489 annotations from 2,101 participants from diverse US and UK populations, providing valuable insights into public responses to scientific information across domains. We further develop NLP models that predict public perception scores with a strong performance. Leveraging the dataset and model, we examine public perception of science from two perspectives: (1) Perception as an outcome: What factors affect the public perception of scientific information? (2) Perception as a predictor: Can we use the estimated perceptions to predict public engagement with science? We find that individuals' frequency of science news consumption is the driver of perception, whereas demographic factors exert minimal influence. More importantly, through a large-scale analysis and carefully designed natural experiment on Reddit, we demonstrate that the estimated public perception of scientific information has direct connections with the final engagement pattern. Posts with more positive perception scores receive significantly more comments and upvotes, which is consistent across different scientific information and for the same science, but are framed differently. Overall, this research underscores the importance of nuanced perception modeling in science communication, offering new pathways to predict public interest and engagement with scientific content.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTA: Grouped-head latenT Attention</title>
<link>https://arxiv.org/abs/2506.17286</link>
<guid>https://arxiv.org/abs/2506.17286</guid>
<content:encoded><![CDATA[

arXiv:2506.17286v2 Announce Type: replace-cross 
Abstract: Attention mechanisms underpin the success of large language models (LLMs), yet their substantial computational and memory overhead poses challenges for optimizing efficiency and performance. A critical bottleneck arises as KV cache and attention computations scale rapidly with text length, challenging deployment on hardware with limited computational and memory resources. We observe that attention mechanisms exhibit substantial redundancy, since the KV cache can be significantly compressed and attention maps across heads display high similarity, revealing that much of the computation and storage is unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that reduces memory usage and computational complexity while maintaining performance. GTA comprises two components: (1) a shared attention map mechanism that reuses attention scores across multiple heads, decreasing the key cache size; and (2) a nonlinear value decoder with learned projections that compresses the value cache into a latent space, further cutting memory needs. GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while avoiding the extra overhead of Multi-Head Latent Attention to improve LLM deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in end-to-end inference speed, with prefill benefiting from reduced computational cost and decoding benefiting from the smaller cache footprint.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Exploration for a Class of Continuous-Time Indefinite Linear--Quadratic Reinforcement Learning Problems</title>
<link>https://arxiv.org/abs/2507.00358</link>
<guid>https://arxiv.org/abs/2507.00358</guid>
<content:encoded><![CDATA[

arXiv:2507.00358v2 Announce Type: replace-cross 
Abstract: We study reinforcement learning (RL) for the same class of continuous-time stochastic linear--quadratic (LQ) control problems as in \cite{huang2024sublinear}, where volatilities depend on both states and controls while states are scalar-valued and running control rewards are absent. We propose a model-free, data-driven exploration mechanism that adaptively adjusts entropy regularization by the critic and policy variance by the actor. Unlike the constant or deterministic exploration schedules employed in \cite{huang2024sublinear}, which require extensive tuning for implementations and ignore learning progresses during iterations, our adaptive exploratory approach boosts learning efficiency with minimal tuning. Despite its flexibility, our method achieves a sublinear regret bound that matches the best-known model-free results for this class of LQ problems, which were previously derived only with fixed exploration schedules. Numerical experiments demonstrate that adaptive explorations accelerate convergence and improve regret performance compared to the non-adaptive model-free and model-based counterparts.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss</title>
<link>https://arxiv.org/abs/2507.01630</link>
<guid>https://arxiv.org/abs/2507.01630</guid>
<content:encoded><![CDATA[

arXiv:2507.01630v2 Announce Type: replace-cross 
Abstract: The task of Human-Object conTact (HOT) detection involves identifying the specific areas of the human body that are touching objects. Nevertheless, current models are restricted to just one type of image, often leading to too much segmentation in areas with little interaction, and struggling to maintain category consistency within specific regions. To tackle this issue, a HOT framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we utilize a semantic-driven prompt mechanism to direct the network's attention towards the relevant regions based on the correlation between image and text. Then a human proximal perception mechanism is employed to dynamically perceive key depth range around the human, using learnable parameters to effectively eliminate regions where interactions are not expected. Calculating depth resolves the uncertainty of the overlap between humans and objects in a 2D perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss (RJLoss) has been created as a new loss to inhibit abnormal categories in the same area. A new evaluation metric called ``AD-Acc.'' is introduced to address the shortcomings of existing methods in addressing negative samples. Comprehensive experimental results demonstrate that our approach achieves state-of-the-art performance in four metrics across two benchmark datasets. Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$, \textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated dataset. The sources code are available at https://github.com/YuxiaoWang-AI/P3HOT.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars</title>
<link>https://arxiv.org/abs/2507.01939</link>
<guid>https://arxiv.org/abs/2507.01939</guid>
<content:encoded><![CDATA[

arXiv:2507.01939v2 Announce Type: replace-cross 
Abstract: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks</title>
<link>https://arxiv.org/abs/2507.01955</link>
<guid>https://arxiv.org/abs/2507.01955</guid>
<content:encoded><![CDATA[

arXiv:2507.01955v2 Announce Type: replace-cross 
Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).
  The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cautious Next Token Prediction</title>
<link>https://arxiv.org/abs/2507.03038</link>
<guid>https://arxiv.org/abs/2507.03038</guid>
<content:encoded><![CDATA[

arXiv:2507.03038v2 Announce Type: replace-cross 
Abstract: Next token prediction paradigm has been prevailing for autoregressive models in the era of LLMs. The current default sampling choice for popular LLMs is temperature scaling together with nucleus sampling to balance diversity and coherence. Nevertheless, such approach leads to inferior performance in various NLP tasks when the model is not certain about testing questions. To this end, we propose a brand new training-free decoding strategy, dubbed as Cautious Next Token Prediction (CNTP). In the decoding process, if the model has comparatively high prediction entropy at a certain step, we sample multiple trials starting from the step independently and stop when encountering any punctuation. Then we select the trial with the lowest perplexity score viewed as the most probable and reliable trial path given the model's capacity. The trial number is negatively correlated with the prediction confidence, i.e., the less confident the model is, the more trials it should sample. This is consistent with human beings' behaviour: when feeling uncertain or unconfident, one tends to think more creatively, exploring multiple thinking paths, to cautiously select the path one feels most confident about. Extensive experiments on both LLMs and MLLMs show that our proposed CNTP approach outperforms existing standard decoding strategies consistently by a clear margin. Moreover, the integration of CNTP with self consistency can further improve over vanilla self consistency. We believe our proposed CNTP has the potential to become one of the default choices for LLM decoding. Code is available at https://github.com/wyzjack/CNTP.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness Evaluation of Large Language Models in Academic Library Reference Services</title>
<link>https://arxiv.org/abs/2507.04224</link>
<guid>https://arxiv.org/abs/2507.04224</guid>
<content:encoded><![CDATA[

arXiv:2507.04224v2 Announce Type: replace-cross 
Abstract: As libraries explore large language models (LLMs) for use in virtual reference services, a key question arises: Can LLMs serve all users equitably, regardless of demographics or social status? While they offer great potential for scalable support, LLMs may also reproduce societal biases embedded in their training data, risking the integrity of libraries' commitment to equitable service. To address this concern, we evaluate whether LLMs differentiate responses across user identities by prompting six state-of-the-art LLMs to assist patrons differing in sex, race/ethnicity, and institutional role. We found no evidence of differentiation by race or ethnicity, and only minor evidence of stereotypical bias against women in one model. LLMs demonstrated nuanced accommodation of institutional roles through the use of linguistic choices related to formality, politeness, and domain-specific vocabularies, reflecting professional norms rather than discriminatory treatment. These findings suggest that current LLMs show a promising degree of readiness to support equitable and contextually appropriate communication in academic library reference services.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infinite Video Understanding</title>
<link>https://arxiv.org/abs/2507.09068</link>
<guid>https://arxiv.org/abs/2507.09068</guid>
<content:encoded><![CDATA[

arXiv:2507.09068v2 Announce Type: replace-cross 
Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have ushered in remarkable progress in video understanding. However, a fundamental challenge persists: effectively processing and comprehending video content that extends beyond minutes or hours. While recent efforts like Video-XL-2 have demonstrated novel architectural solutions for extreme efficiency, and advancements in positional encoding such as HoPE and VideoRoPE++ aim to improve spatio-temporal understanding over extensive contexts, current state-of-the-art models still encounter significant computational and memory constraints when faced with the sheer volume of visual tokens from lengthy sequences. Furthermore, maintaining temporal coherence, tracking complex events, and preserving fine-grained details over extended periods remain formidable hurdles, despite progress in agentic reasoning systems like Deep Video Discovery. This position paper posits that a logical, albeit ambitious, next frontier for multimedia research is Infinite Video Understanding -- the capability for models to continuously process, understand, and reason about video data of arbitrary, potentially never-ending duration. We argue that framing Infinite Video Understanding as a blue-sky research objective provides a vital north star for the multimedia, and the wider AI, research communities, driving innovation in areas such as streaming architectures, persistent memory mechanisms, hierarchical and adaptive representations, event-centric reasoning, and novel evaluation paradigms. Drawing inspiration from recent work on long/ultra-long video understanding and several closely related fields, we outline the core challenges and key research directions towards achieving this transformative capability.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images</title>
<link>https://arxiv.org/abs/2507.09898</link>
<guid>https://arxiv.org/abs/2507.09898</guid>
<content:encoded><![CDATA[

arXiv:2507.09898v2 Announce Type: replace-cross 
Abstract: This study investigates the effectiveness of U-Net architectures integrated with various convolutional neural network (CNN) backbones for automated lung cancer detection and segmentation in chest CT images, addressing the critical need for accurate diagnostic tools in clinical settings. A balanced dataset of 832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed using Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to 128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50, VGG16, and Xception, to segment lung regions. After segmentation, CNN-based classifiers and hybrid models combining CNN feature extraction with traditional machine learning classifiers (Support Vector Machine, Random Forest, and Gradient Boosting) were evaluated using 5-fold cross-validation. Metrics included accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC. U-Net with ResNet50 achieved the best performance for cancerous lungs (Dice: 0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For classification, the CNN model using U-Net with Xception achieved 99.1 percent accuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid CNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent F1-score. Compared to prior methods, our framework consistently outperformed existing models. In conclusion, combining U-Net with advanced CNN backbones provides a powerful method for both segmentation and classification of lung cancer in CT scans, supporting early diagnosis and clinical decision-making.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics</title>
<link>https://arxiv.org/abs/2507.11588</link>
<guid>https://arxiv.org/abs/2507.11588</guid>
<content:encoded><![CDATA[

arXiv:2507.11588v2 Announce Type: replace-cross 
Abstract: Spatial Transcriptomics (ST) technologies provide biologists with rich insights into single-cell biology by preserving spatial context of cells. Building foundational models for ST can significantly enhance the analysis of vast and complex data sources, unlocking new perspectives on the intricacies of biological tissues. However, modeling ST data is inherently challenging due to the need to extract multi-scale information from tissue slices containing vast numbers of cells. This process requires integrating macro-scale tissue morphology, micro-scale cellular microenvironment, and gene-scale gene expression profile. To address this challenge, we propose SToFM, a multi-scale Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale information extraction on each ST slice, to construct a set of ST sub-slices that aggregate macro-, micro- and gene-scale information. Then an SE(2) Transformer is used to obtain high-quality cell representations from the sub-slices. Additionally, we construct \textbf{SToCorpus-88M}, the largest high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves outstanding performance on a variety of downstream tasks, such as tissue region semantic segmentation and cell type annotation, demonstrating its comprehensive understanding of ST data through capturing and integrating multi-scale information.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fragment size density estimator for shrinkage-induced fracture based on a physics-informed neural network</title>
<link>https://arxiv.org/abs/2507.11799</link>
<guid>https://arxiv.org/abs/2507.11799</guid>
<content:encoded><![CDATA[

arXiv:2507.11799v2 Announce Type: replace-cross 
Abstract: This paper presents a neural network (NN)-based solver for an integro-differential equation that models shrinkage-induced fragmentation. The proposed method directly maps input parameters to the corresponding probability density function without numerically solving the governing equation, thereby significantly reducing computational costs. Specifically, it enables efficient evaluation of the density function in Monte Carlo simulations while maintaining accuracy comparable to or even exceeding that of conventional finite difference schemes. Validatation on synthetic data demonstrates both the method's computational efficiency and predictive reliability. This study establishes a foundation for the data-driven inverse analysis of fragmentation and suggests the potential for extending the framework beyond pre-specified model structures.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Frequency Modulation for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2507.11893</link>
<guid>https://arxiv.org/abs/2507.11893</guid>
<content:encoded><![CDATA[

arXiv:2507.11893v2 Announce Type: replace-cross 
Abstract: High spatial frequency information, including fine details like textures, significantly contributes to the accuracy of semantic segmentation. However, according to the Nyquist-Shannon Sampling Theorem, high-frequency components are vulnerable to aliasing or distortion when propagating through downsampling layers such as strided-convolution. Here, we propose a novel Spatial Frequency Modulation (SFM) that modulates high-frequency features to a lower frequency before downsampling and then demodulates them back during upsampling. Specifically, we implement modulation through adaptive resampling (ARS) and design a lightweight add-on that can densely sample the high-frequency areas to scale up the signal, thereby lowering its frequency in accordance with the Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling (MSAU) to demodulate the modulated feature and recover high-frequency information through non-uniform upsampling This module further improves segmentation by explicitly exploiting information interaction between densely and sparsely resampled areas at multiple scales. Both modules can seamlessly integrate with various architectures, extending from convolutional neural networks to transformers. Feature visualization and analysis confirm that our method effectively alleviates aliasing while successfully retaining details after demodulation. Finally, we validate the broad applicability and effectiveness of SFM by extending it to image classification, adversarial robustness, instance segmentation, and panoptic segmentation tasks. The code is available at https://github.com/Linwei-Chen/SFM.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frequency-Dynamic Attention Modulation for Dense Prediction</title>
<link>https://arxiv.org/abs/2507.12006</link>
<guid>https://arxiv.org/abs/2507.12006</guid>
<content:encoded><![CDATA[

arXiv:2507.12006v2 Announce Type: replace-cross 
Abstract: Vision Transformers (ViTs) have significantly advanced computer vision, demonstrating strong performance across various tasks. However, the attention mechanism in ViTs makes each layer function as a low-pass filter, and the stacked-layer architecture in existing transformers suffers from frequency vanishing. This leads to the loss of critical details and textures. We propose a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly modulates the overall frequency response of ViTs and consists of two techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling (FreqScale). Since circuit theory uses low-pass filters as fundamental elements, we introduce AttInv, a method that generates complementary high-pass filtering by inverting the low-pass filter in the attention matrix, and dynamically combining the two. We further design FreqScale to weight different frequency components for fine-grained adjustments to the target response function. Through feature similarity analysis and effective rank evaluation, we demonstrate that our approach avoids representation collapse, leading to consistent performance improvements across various models, including SegFormer, DeiT, and MaskDINO. These improvements are evident in tasks such as semantic segmentation, object detection, and instance segmentation. Additionally, we apply our method to remote sensing detection, achieving state-of-the-art results in single-scale settings. The code is available at https://github.com/Linwei-Chen/FDAM.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>A Survey of Deep Learning for Geometry Problem Solving</title>
<link>https://arxiv.org/abs/2507.11936</link>
<guid>https://arxiv.org/abs/2507.11936</guid>
<content:encoded><![CDATA[
<div> Keywords: geometry, deep learning, problem solving, multimodal, artificial intelligence  

Summary:  
This paper presents a survey of the applications of deep learning in geometry problem solving. It covers a range of tasks related to geometry problem solving, reviews various deep learning methods, examines evaluation metrics and methods, and discusses current challenges and future directions in the field. The rapid advancement of deep learning technology, particularly the emergence of multimodal large language models, has sparked significant research interest in this area. By providing a comprehensive overview and practical reference, the paper aims to facilitate further progress in utilizing deep learning for solving geometry problems. Additionally, the authors maintain a regularly updated list of relevant papers on GitHub to support ongoing development in this field. <br /><br />Summary: <div>
arXiv:2507.11936v2 Announce Type: replace-cross 
Abstract: Geometry problem solving is a key area of mathematical reasoning, which is widely involved in many important fields such as education, mathematical ability assessment of artificial intelligence, and multimodal ability assessment. In recent years, the rapid development of deep learning technology, especially the rise of multimodal large language models, has triggered a widespread research boom. This paper provides a survey of the applications of deep learning in geometry problem solving, including (i) a comprehensive summary of the relevant tasks in geometry problem solving; (ii) a thorough review of related deep learning methods; (iii) a detailed analysis of evaluation metrics and methods; and (iv) a critical discussion of the current challenges and future directions that can be explored. Our goal is to provide a comprehensive and practical reference of deep learning for geometry problem solving to promote further developments in this field. We create a continuously updated list of papers on GitHub: https://github.com/majianz/dl4gps.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Coordinated Online Behavior: Trade-offs and Strategies</title>
<link>https://arxiv.org/abs/2507.12108</link>
<guid>https://arxiv.org/abs/2507.12108</guid>
<content:encoded><![CDATA[
<div> Keywords: coordinated online behavior, multimodal approach, detection, digital platforms, analysis

Summary:
This study explores the detection of coordinated online behavior using multimodal approaches. Traditional methods often focus on single types of interactions or consider multiple modalities independently, potentially missing the complex dynamics of multimodal coordination. By comparing monomodal and multimodal approaches, the study highlights the trade-off between weakly and strongly integrated models in capturing coordination patterns. The research reveals that not all modalities provide unique insights, but a multimodal approach offers a more comprehensive understanding of coordination dynamics. The findings enhance the ability to detect and analyze coordinated behavior online, offering new perspectives for safeguarding the integrity of digital platforms. <div>
arXiv:2507.12108v2 Announce Type: replace-cross 
Abstract: Coordinated online behavior, which spans from beneficial collective actions to harmful manipulation such as disinformation campaigns, has become a key focus in digital ecosystem analysis. Traditional methods often rely on monomodal approaches, focusing on single types of interactions like co-retweets or co-hashtags, or consider multiple modalities independently of each other. However, these approaches may overlook the complex dynamics inherent in multimodal coordination. This study compares different ways of operationalizing the detection of multimodal coordinated behavior. It examines the trade-off between weakly and strongly integrated multimodal models, highlighting the balance between capturing broader coordination patterns and identifying tightly coordinated behavior. By comparing monomodal and multimodal approaches, we assess the unique contributions of different data modalities and explore how varying implementations of multimodality impact detection outcomes. Our findings reveal that not all the modalities provide distinct insights, but that with a multimodal approach we can get a more comprehensive understanding of coordination dynamics. This work enhances the ability to detect and analyze coordinated online behavior, offering new perspectives for safeguarding the integrity of digital platforms.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GitChameleon 2.0: Evaluating AI Code Generation Against Python Library Version Incompatibilities</title>
<link>https://arxiv.org/abs/2507.12367</link>
<guid>https://arxiv.org/abs/2507.12367</guid>
<content:encoded><![CDATA[
<div> Keywords: software libraries, code generation, backward compatibility, Python, large language models

Summary:
GitChameleon 2.0 introduces a dataset consisting of 328 Python code completion problems that require code generation compatible with specific library versions. The dataset includes executable unit tests, enabling rigorous evaluation of the ability of various AI systems to generate code that functions correctly based on library versions. The evaluation shows that existing state-of-the-art systems face challenges in achieving high success rates in this task, with enterprise models reaching only 48-51% accuracy. By emphasizing the dynamic nature of code libraries and providing an execution-based benchmark, GitChameleon 2.0 aims to improve understanding of the complexity of version-conditioned code generation. The dataset and evaluation code are open-source and available for further research and development in AI code generation methods.

<br /><br />Summary: GitChameleon 2.0 presents a dataset of Python code problems conditioned on specific library versions, evaluating AI systems' ability to generate code compliant with these versions with executable tests. Current systems struggle with this task, highlighting the need for more adaptable AI code generation methods through the dataset's execution-based benchmark. <div>
arXiv:2507.12367v2 Announce Type: replace-cross 
Abstract: The rapid evolution of software libraries poses a considerable hurdle for code generation, necessitating continuous adaptation to frequent version updates while preserving backward compatibility. While existing code evolution benchmarks provide valuable insights, they typically lack execution-based evaluation for generating code compliant with specific library versions. To address this, we introduce GitChameleon 2.0, a novel, meticulously curated dataset comprising 328 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. GitChameleon 2.0 rigorously evaluates the capacity of contemporary large language models (LLMs), LLM-powered agents, code assistants, and RAG systems to perform version-conditioned code generation that demonstrates functional accuracy through execution. Our extensive evaluations indicate that state-of-the-art systems encounter significant challenges with this task; enterprise models achieving baseline success rates in the 48-51% range, underscoring the intricacy of the problem. By offering an execution-based benchmark emphasizing the dynamic nature of code libraries, GitChameleon 2.0 enables a clearer understanding of this challenge and helps guide the development of more adaptable and dependable AI code generation methods. We make the dataset and evaluation code publicly available at https://github.com/mrcabbage972/GitChameleonBenchmark.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Reasoning to Super-Intelligence: A Search-Theoretic Perspective</title>
<link>https://arxiv.org/abs/2507.15865</link>
<guid>https://arxiv.org/abs/2507.15865</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought, Diligent Learner, Reasoning, Large Language Models, Problem-solving

Summary:
The paper introduces the Diligent Learner, a new learning paradigm designed to tackle the challenges of Chain-of-Thought reasoning in large language models. It identifies key obstacles in CoT learning, including distribution drift, lack of embedded search, and exponential inference costs. The Diligent Learner models reasoning as a depth-first search guided by a validator, supporting backtracking upon failure. The framework is proven to efficiently learn from CoT data under mild assumptions, where existing methods struggle. This approach opens up possibilities for building scalable and reliable reasoning systems trained on incomplete data, leading to the development of Large Reasoning Models (LRMs) with robust and interpretable problem-solving capabilities. <div>
arXiv:2507.15865v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful tool for enhancing the problem-solving capabilities of large language models (LLMs). However, the theoretical foundations of learning from CoT data remain underdeveloped, and existing approaches -- such as Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), Tree-of-Thoughts (ToT), and Monte Carlo Tree Search (MCTS) -- often fail on complex reasoning tasks. In this work, we identify core obstacles that hinder effective CoT learning, including distribution drift, lack of embedded search, and exponential inference costs. We introduce the Diligent Learner, a new learning paradigm that explicitly models reasoning as a depth-first search guided by a validator and supports backtracking upon failure. Under two mild and realistic assumptions, we prove that the Diligent Learner can efficiently learn from CoT data while existing methods fail to do so. This framework offers a path toward building scalable and reliable reasoning systems trained on naturally occurring, incomplete data -- paving the way for the development of Large Reasoning Models (LRMs) with robust, interpretable problem-solving abilities.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Purchase and Production Optimization in a Meat Processing Plant</title>
<link>https://arxiv.org/abs/2507.15866</link>
<guid>https://arxiv.org/abs/2507.15866</guid>
<content:encoded><![CDATA[
<div> optimization, food production, meat processing, supply chain management, integer linear programming

Summary:
The paper discusses the challenges faced by the food production industry, particularly the meat processing sector, exacerbated by the recent energy crisis in the European Union. It focuses on optimizing the purchase and processing of materials for a meat processing company, considering alternative processing methods, stock expiration dates, minimum order quantity, and minimum percentage constraints. The authors prove that these constraints make the problem NP-hard and propose a simple iterative approach based on integer linear programming to solve real-life instances efficiently. This approach also addresses numerical issues encountered with commercial solvers. Real data tests demonstrate that the algorithm can find optimal solutions within seconds for various scenarios in the meat processing industry. <div>
arXiv:2507.15866v1 Announce Type: new 
Abstract: The food production industry, especially the meat production sector, faces many challenges that have even escalated due to the recent outbreak of the energy crisis in the European Union. Therefore, efficient use of input materials is an essential aspect affecting the profit of such companies. This paper addresses an optimization problem concerning the purchase and subsequent material processing we solved for a meat processing company. Unlike the majority of existing papers, we do not concentrate on how this problem concerns supply chain management, but we focus purely on the production stage. The problem involves the concept of alternative ways of material processing, stock of material with different expiration dates, and extra constraints widely neglected in the current literature, namely, the minimum order quantity and the minimum percentage in alternatives. We prove that each of these two constraints makes the problem \mbox{$\mathcal{NP}$-hard}, and hence we design a simple iterative approach based on integer linear programming that allows us to solve real-life instances even using an open-source integer linear programming solver. Another advantage of this approach is that it mitigates numerical issues, caused by the extensive range of data values, we experienced with a commercial solver. The results obtained using real data from the meat processing company showed that our algorithm can find the optimum solution in a few seconds for all considered use cases.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Braking? Scenario Extraction and Reasoning Utilizing LLM</title>
<link>https://arxiv.org/abs/2507.15874</link>
<guid>https://arxiv.org/abs/2507.15874</guid>
<content:encoded><![CDATA[
<div> Keywords: ADAS-equipped vehicles, driving data, safety-critical corner cases, Large Language Model, scenario understanding

Summary: 
The article focuses on the challenge of identifying safety-critical driving scenarios from the increasing amount of data collected by Advanced Driver Assistance Systems (ADAS)-equipped vehicles. By analyzing braking events, the research aims to understand the reasons why vehicles brake. Traditional approaches rely on rule-based heuristics, but these lack generalization in complex urban environments. The proposed framework utilizes Large Language Models (LLM) to interpret and classify driving scenarios, bridging the gap between numerical signals and natural language descriptions. The method includes a dual-path scenario retrieval system, allowing for category-based search for known scenarios and embedding-based retrieval for unknown scenarios. The evaluation is conducted on the Argoverse 2 Sensor Dataset, demonstrating superior performance compared to rule-based approaches and good generalization to Out-of-Distribution (OOD) scenarios. <div>
arXiv:2507.15874v1 Announce Type: new 
Abstract: The growing number of ADAS-equipped vehicles has led to a dramatic increase in driving data, yet most of them capture routine driving behavior. Identifying and understanding safety-critical corner cases within this vast dataset remains a significant challenge. Braking events are particularly indicative of potentially hazardous situations, motivating the central question of our research: Why does a vehicle brake? Existing approaches primarily rely on rule-based heuristics to retrieve target scenarios using predefined condition filters. While effective in simple environments such as highways, these methods lack generalization in complex urban settings. In this paper, we propose a novel framework that leverages Large Language Model (LLM) for scenario understanding and reasoning. Our method bridges the gap between low-level numerical signals and natural language descriptions, enabling LLM to interpret and classify driving scenarios. We propose a dual-path scenario retrieval that supports both category-based search for known scenarios and embedding-based retrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate evaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset. Experimental results show that our method outperforms rule-based baselines and generalizes well to OOD scenarios.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Multimodal Transformers</title>
<link>https://arxiv.org/abs/2507.15875</link>
<guid>https://arxiv.org/abs/2507.15875</guid>
<content:encoded><![CDATA[
<div> Keywords: small language models, vision, Transformer attention mechanisms, Differential Attention, PaliGemma

Summary:
Differential Attention is extended to the text-vision model PaliGemma to address noise introduced by incorporating vision modalities. It aims to improve information retrieval and reduce hallucinations. The PaliGemma 3B model is fine-tuned using LoRA and Differential Attention, with experiments on parameter settings and configurations. The study shows that Differential Attention can enhance question-answering capabilities by mitigating noisy information retrieval in existing models. 

<br /><br />Summary: 
1. Extension of Differential Attention to PaliGemma for text-vision models.
2. Addressing noise in modalities like vision to improve information retrieval.
3. Reduction of hallucinations in text-vision models.
4. Fine-tuning PaliGemma 3B model using LoRA and Differential Attention.
5. Demonstration of enhanced question-answering capabilities through Differential Attention integration. <div>
arXiv:2507.15875v1 Announce Type: new 
Abstract: Small language models have gained significant popularity due to their efficiency and growing capabilities. However, incorporating additional modalities, such as vision, can exacerbate the challenge of limited context windows by introducing noise. Recent studies have highlighted that Transformer attention mechanisms often disproportionately focus on irrelevant contexts. In this work, we extend the Differential Attention mechanism, originally designed for text-only models, to the text-vision model PaliGemma. Our aim is to evaluate its ability to mitigate noisy information retrieval and reduce hallucinations. To this end, we fine-tuned the PaliGemma 3B model using LoRA, incorporating Differential Attention, and experimented with various parameter settings and configurations. We demonstrate that Differential Attention can be adapted and integrated into the fine-tuning of existing models to enhance noisy information retrieval and question-answering capabilities.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-evaluating Short- and Long-Term Trend Factors in CTA Replication: A Bayesian Graphical Approach</title>
<link>https://arxiv.org/abs/2507.15876</link>
<guid>https://arxiv.org/abs/2507.15876</guid>
<content:encoded><![CDATA[
<div> trend-following, Commodity Trading Advisors (CTAs), short-term trend, long-term trend, risk-adjusted performance <br />
<br />
Summary: 
This article explores the relative merits and interactions of short-term and long-term trend-following rules used by Commodity Trading Advisors (CTAs). Through a Bayesian graphical model, the study dynamically decomposes CTA returns into short-term trend, long-term trend, and market beta factors. The research sheds light on how blending different horizons shapes the strategy's risk-adjusted performance, a topic that has been controversial in the field. By analyzing the decomposition of returns, the paper contributes to the ongoing debate on trend-following strategies and their effectiveness in capturing major directional moves in the market. The findings provide valuable insights for CTAs looking to optimize their trading strategies based on different time horizons. <div>
arXiv:2507.15876v1 Announce Type: new 
Abstract: Commodity Trading Advisors (CTAs) have historically relied on trend-following rules that operate on vastly different horizons from long-term breakouts that capture major directional moves to short-term momentum signals that thrive in fast-moving markets. Despite a large body of work on trend following, the relative merits and interactions of short-versus long-term trend systems remain controversial. This paper adds to the debate by (i) dynamically decomposing CTA returns into short-term trend, long-term trend and market beta factors using a Bayesian graphical model, and (ii) showing how the blend of horizons shapes the strategy's risk-adjusted performance.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.15877</link>
<guid>https://arxiv.org/abs/2507.15877</guid>
<content:encoded><![CDATA[
<div> Generalization, ARC-AGI, neural program synthesis, test-time fine-tuning, novel solutions
Summary:
Execution-guided neural program synthesis was compared to test-time fine-tuning (TTFT) in a controlled compositional generalization experiment in the open-world ARC-AGI domain. The study aimed to assess the ability to generalize out-of-distribution, crucial for success in this domain. The findings showed that execution-guided neural program synthesis outperformed all reference algorithms in composing novel solutions. Additionally, the success of TTFT on ARC-AGI was attributed to eliciting in-distribution knowledge that the LLM failed to utilize directly. <div>
arXiv:2507.15877v1 Announce Type: new 
Abstract: We run a controlled compositional generalization experiment in the ARC-AGI domain: an open-world problem domain in which the ability to generalize out-of-distribution is, by design, an essential characteristic for success. We compare neural program synthesis and test-time fine-tuning approaches on this experiment. We find that execution-guided neural program synthesis outperforms all reference algorithms in its ability to compose novel solutions. Our empirical findings also suggest that the success of TTFT on ARC-AGI lies mainly in eliciting in-distribution knowledge that the LLM otherwise fails to rely on directly.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Recursive Coherence Principle: A Formal Constraint on Scalable Intelligence, Alignment, and Reasoning Architecture</title>
<link>https://arxiv.org/abs/2507.15880</link>
<guid>https://arxiv.org/abs/2507.15880</guid>
<content:encoded><![CDATA[
<div> Keywords: Intelligence, Recursive Coherence Principle, Functional Model of Intelligence, Semantic coherence, AI alignment <br />
Summary:<br />
The article discusses the importance of structural coherence in intelligence, whether biological, artificial, or collective. It introduces the Recursive Coherence Principle (RCP), which states that for any reasoning system to scale effectively, semantic coherence must be preserved through a recursive evaluable generalization operator. The Functional Model of Intelligence (FMI) is identified as the essential operator that satisfies the RCP at any scale. The FMI is a minimal, composable architecture with internal and external functions crucial for maintaining semantic structure across inference and coordination layers. The lack of the FMI can lead to recursive coherence breakdown, resulting in common AI issues like misalignment, hallucination, and instability. The RCP captures the internal, recursive dynamics necessary for coherent and alignable intelligence, offering a pathway for safe and robustly coherent AI at scale. The article advocates for a shift from behavioral constraints to structural coherence in AI alignment. <br /> <div>
arXiv:2507.15880v1 Announce Type: new 
Abstract: Intelligence-biological, artificial, or collective-requires structural coherence across recursive reasoning processes to scale effectively. As complex systems grow, coherence becomes fragile unless a higher-order structure ensures semantic consistency. This paper introduces the Recursive Coherence Principle (RCP): a foundational constraint stating that for any reasoning system of order N, composed of systems operating over conceptual spaces of order N-1, semantic coherence is preserved only by a recursively evaluable generalization operator that spans and aligns those lower-order conceptual spaces. Crucially, this coherence enables structural alignment. Without recursive coherence, no system can reliably preserve goals, meanings, or reasoning consistency at scale. We formally define the Functional Model of Intelligence (FMI) as the only known operator capable of satisfying the RCP at any scale. The FMI is a minimal, composable architecture with internal functions (evaluation, modeling, adaptation, stability, decomposition, bridging) and external functions (storage, recall, System 1 and System 2 reasoning) vital for preserving semantic structure across inference and coordination layers. We prove that any system lacking the FMI will experience recursive coherence breakdown as it scales, arguing that common AI issues like misalignment, hallucination, and instability are symptoms of this structural coherence loss. Unlike other foundational principles, RCP uniquely captures the internal, recursive dynamics needed for coherent, alignable intelligence, modeling semantic coherence under recursion. This work significantly impacts AI alignment, advocating a shift from behavioral constraints to structural coherence, and offers a pathway for safely generalizable, robustly coherent AI at scale.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADEPTS: A Capability Framework for Human-Centered Agent Design</title>
<link>https://arxiv.org/abs/2507.15885</link>
<guid>https://arxiv.org/abs/2507.15885</guid>
<content:encoded><![CDATA[
<div> framework, AI agents, user-facing capabilities, human-centered design, ADEPTS

Summary:<br />
The article proposes a new framework called ADEPTS to guide the development of AI agents with a focus on user-facing capabilities. ADEPTS is based on six principles for human-centered design, ensuring AI agents are understandable, controllable, and trustworthy in everyday use. Unlike existing frameworks, ADEPTS bridges the gap between technical development and user experience, providing actionable guidance for researchers, designers, engineers, and policy reviewers. By condensing complex AI-UX requirements into a compact framework, ADEPTS aims to accelerate the improvement of agent capabilities, facilitate the design of user experiences, and establish a shared language for tracking progress in AI agent development. <div>
arXiv:2507.15885v1 Announce Type: new 
Abstract: Large language models have paved the way to powerful and flexible AI agents, assisting humans by increasingly integrating into their daily life. This flexibility, potential, and growing adoption demands a holistic and cross-disciplinary approach to developing, monitoring and discussing the capabilities required for agent-driven user experiences. However, current guidance on human-centered AI agent development is scattered: UX heuristics focus on interface behaviors, engineering taxonomies describe internal pipelines, and ethics checklists address high-level governance. There is no concise, user-facing vocabulary that tells teams what an agent should fundamentally be able to do. We introduce ADEPTS, a capability framework defining a set of core user-facing capabilities to provide unified guidance around the development of AI agents. ADEPTS is based on six principles for human-centered agent design, that express the minimal, user-facing capabilities an AI agent should demonstrate to be understandable, controllable and trustworthy in everyday use. ADEPTS complements existing frameworks and taxonomies; differently from them, it sits at the interface between technical and experience development. By presenting ADEPTS, we aim to condense complex AI-UX requirements into a compact framework that is actionable guidance for AI researchers, designers, engineers, and policy reviewers alike. We believe ADEPTS has the potential of accelerating the improvement of user-relevant agent capabilities, of easing the design of experiences that take advantage of those capabilities, and of providing a shared language to track and discuss progress around the development of AI agents.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Reason-Based Moral Decision-Making in the Reinforcement Learning Architecture</title>
<link>https://arxiv.org/abs/2507.15895</link>
<guid>https://arxiv.org/abs/2507.15895</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, artificial moral agents, reason-based artificial moral agents, normative reasoning, ethical behavior <br />
Summary: <br />
This study explores the development of reason-based artificial moral agents (RBAMAs), which are designed to make ethical decisions based on sound normative reasoning. RBAMAs extend the reinforcement learning architecture to incorporate the capacity to learn a reason-theory through case-based feedback, enabling them to derive moral obligations and adapt their behavior to ensure conformance to these obligations. This framework contributes to the moral justifiability, moral robustness, and moral trustworthiness of the agents, making them ethically sound in their actions. The study presents an initial implementation of an RBAMA and demonstrates its potential through experiments. These RBAMAs provide a concrete and deployable framework for the development of artificial moral agents that meet key ethical standards. <div>
arXiv:2507.15895v1 Announce Type: new 
Abstract: Reinforcement Learning is a machine learning methodology that has demonstrated strong performance across a variety of tasks. In particular, it plays a central role in the development of artificial autonomous agents. As these agents become increasingly capable, market readiness is rapidly approaching, which means those agents, for example taking the form of humanoid robots or autonomous cars, are poised to transition from laboratory prototypes to autonomous operation in real-world environments. This transition raises concerns leading to specific requirements for these systems - among them, the requirement that they are designed to behave ethically. Crucially, research directed toward building agents that fulfill the requirement to behave ethically - referred to as artificial moral agents(AMAs) - has to address a range of challenges at the intersection of computer science and philosophy. This study explores the development of reason-based artificial moral agents (RBAMAs). RBAMAs are build on an extension of the reinforcement learning architecture to enable moral decision-making based on sound normative reasoning, which is achieved by equipping the agent with the capacity to learn a reason-theory - a theory which enables it to process morally relevant propositions to derive moral obligations - through case-based feedback. They are designed such that they adapt their behavior to ensure conformance to these obligations while they pursue their designated tasks. These features contribute to the moral justifiability of the their actions, their moral robustness, and their moral trustworthiness, which proposes the extended architecture as a concrete and deployable framework for the development of AMAs that fulfills key ethical desiderata. This study presents a first implementation of an RBAMA and demonstrates the potential of RBAMAs in initial experiments.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation</title>
<link>https://arxiv.org/abs/2507.15901</link>
<guid>https://arxiv.org/abs/2507.15901</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, household environments, proactive autonomy, privacy, ethics 

Summary: 
Ethical considerations surrounding the implementation of Artificial Intelligence (AI) in household settings, particularly in the form of proactive autonomous agents, present various challenges and opportunities for comfort and attention. The shift towards proactive autonomy raises concerns regarding privacy, fairness, and user control. This article examines responsible innovation frameworks, human-centered design principles, and governance practices to provide practical guidance for the development of ethical smart home systems. Vulnerable user groups, such as the elderly, children, and neurodivergent individuals, are at higher risk of surveillance, bias, and privacy violations when utilizing agentic AI. Design recommendations include tailored explainability, granular consent mechanisms, and robust override controls, utilizing participatory and inclusive methodologies. Additionally, the article discusses how data-driven insights, including Natural Language Processing (NLP) for social media analysis, can inform user needs and ethical considerations in the development of transparent, inclusive, and trustworthy agentic AI for household automation. 

<br /><br />Summary: <div>
arXiv:2507.15901v1 Announce Type: new 
Abstract: The implementation of Artificial Intelligence (AI) in household environments, especially in the form of proactive autonomous agents, brings about possibilities of comfort and attention as well as it comes with intra or extramural ethical challenges. This article analyzes agentic AI and its applications, focusing on its move from reactive to proactive autonomy, privacy, fairness and user control. We review responsible innovation frameworks, human-centered design principles, and governance practices to distill practical guidance for ethical smart home systems. Vulnerable user groups such as elderly individuals, children, and neurodivergent who face higher risks of surveillance, bias, and privacy risks were studied in detail in context of Agentic AI. Design imperatives are highlighted such as tailored explainability, granular consent mechanisms, and robust override controls, supported by participatory and inclusive methodologies. It was also explored how data-driven insights, including social media analysis via Natural Language Processing(NLP), can inform specific user needs and ethical concerns. This survey aims to provide both a conceptual foundation and suggestions for developing transparent, inclusive, and trustworthy agentic AI in household automation.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does More Inference-Time Compute Really Help Robustness?</title>
<link>https://arxiv.org/abs/2507.15974</link>
<guid>https://arxiv.org/abs/2507.15974</guid>
<content:encoded><![CDATA[
<div> demonstrated, inference-time, robustness, scaling, security <br />
Summary:
In this paper, the authors explore the impact of inference-time computation scaling on model robustness in smaller-scale, open-source reasoning models. They introduce a budget forcing strategy to improve robustness in models like DeepSeek R1, Qwen3, and Phi-reasoning. The study challenges the assumption that intermediate reasoning steps are hidden from adversaries and uncovers a security risk when these steps become accessible. They find that increased inference-time computation can actually decrease model robustness in the presence of explicit reasoning steps. Furthermore, they discuss vulnerabilities in models with tool-integrated reasoning and advanced reasoning extraction attacks. The research highlights the nuanced trade-offs involved in deploying inference-time scaling in real-world security-sensitive applications, emphasizing the importance of considering the adversarial setting and deployment context. <br /><br />Summary: <div>
arXiv:2507.15974v1 Announce Type: new 
Abstract: Recently, Zaremba et al. demonstrated that increasing inference-time computation improves robustness in large proprietary reasoning LLMs. In this paper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1, Qwen3, Phi-reasoning) can also benefit from inference-time scaling using a simple budget forcing strategy. More importantly, we reveal and critically examine an implicit assumption in prior work: intermediate reasoning steps are hidden from adversaries. By relaxing this assumption, we identify an important security risk, intuitively motivated and empirically verified as an inverse scaling law: if intermediate reasoning steps become explicitly accessible, increased inference-time computation consistently reduces model robustness. Finally, we discuss practical scenarios where models with hidden reasoning chains are still vulnerable to attacks, such as models with tool-integrated reasoning and advanced reasoning extraction attacks. Our findings collectively demonstrate that the robustness benefits of inference-time scaling depend heavily on the adversarial setting and deployment context. We urge practitioners to carefully weigh these subtle trade-offs before applying inference-time scaling in security-sensitive, real-world applications.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Micromobility Flow Prediction: A Bike Sharing Station-level Study via Multi-level Spatial-Temporal Attention Neural Network</title>
<link>https://arxiv.org/abs/2507.16020</link>
<guid>https://arxiv.org/abs/2507.16020</guid>
<content:encoded><![CDATA[
<div> station-level traffic prediction, bike sharing systems, spatio-temporal, attention neural network, New York City

Summary:
The study addresses the challenge of efficiently managing urban micromobility resources, specifically bike sharing systems, with unbalanced station-level demand and supply. The proposed BikeMAN model, a multi-level spatio-temporal attention neural network, aims to accurately predict station-level bike traffic for entire systems. The network comprises an encoder-decoder structure with attention mechanisms capturing spatial and temporal correlations in bike station features. Through experiments on New York City's bike sharing data, the network demonstrates high accuracy in predicting traffic for over 700 stations. This research fills a gap in efficient bike sharing system management by providing accurate and comprehensive station-level traffic predictions with spatial-temporal complexities in mind. <div>
arXiv:2507.16020v1 Announce Type: new 
Abstract: Efficient use of urban micromobility resources such as bike sharing is challenging due to the unbalanced station-level demand and supply, which causes the maintenance of the bike sharing systems painstaking. Prior efforts have been made on accurate prediction of bike traffics, i.e., demand/pick-up and return/drop-off, to achieve system efficiency. However, bike station-level traffic prediction is difficult because of the spatial-temporal complexity of bike sharing systems. Moreover, such level of prediction over entire bike sharing systems is also challenging due to the large number of bike stations. To fill this gap, we propose BikeMAN, a multi-level spatio-temporal attention neural network to predict station-level bike traffic for entire bike sharing systems. The proposed network consists of an encoder and a decoder with an attention mechanism representing the spatial correlation between features of bike stations in the system and another attention mechanism describing the temporal characteristic of bike station traffic. Through experimental study on over 10 millions trips of bike sharing systems (> 700 stations) of New York City, our network showed high accuracy in predicting the bike station traffic of all stations in the city.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Logic to Language: A Trust Index for Problem Solving with LLMs</title>
<link>https://arxiv.org/abs/2507.16028</link>
<guid>https://arxiv.org/abs/2507.16028</guid>
<content:encoded><![CDATA[
<div> problem-solving paradigms, formal languages, natural language, trust index, statistical quality dimensions
Summary: 
The article discusses the shift from classical computation to Large Language Models (LLMs) in addressing human problems characterized by ambiguity, dynamic environments, and subjective context. It introduces a framework to contrast problem spaces addressable by formal languages versus natural language, highlighting the need for nuanced definitions of approximate solution space for natural language problems. A vector-valued trust index Q is introduced to distinguish between binary quality measures for formal solutions and the continuous adequacy spectrum for natural language solutions. Two statistical quality dimensions, normalized bi-semantic entropy and emotional valence, are proposed to measure the robustness, conceptual diversity, and subjective valuation of LLM answers. These concepts aim to provide a more rigorous understanding of the capabilities and limitations of problem-solving using natural language in the era of LLMs. 
<br /><br />Summary: <div>
arXiv:2507.16028v1 Announce Type: new 
Abstract: Classical computation, grounded in formal, logical systems, has been the engine of technological progress for decades, excelling at problems that can be described with unambiguous rules. This paradigm, however, leaves a vast ocean of human problems -- those characterized by ambiguity, dynamic environments, and subjective context -- largely untouched. The advent of Large Language Models (LLMs) represents a fundamental shift, enabling computational systems to engage with this previously inaccessible domain using natural language. This paper introduces a unified framework to understand and contrast these problem-solving paradigms. We define and delineate the problem spaces addressable by formal languages versus natural language. While solutions to the former problem class can be evaluated using binary quality measures, the latter requires a much more nuanced definition of approximate solution space taking into account the vagueness, subjectivity and ambiguity inherent to natural language. We therefore introduce a vector-valued trust index Q, which reflects solution quality and distinguishes the binary correctness of formal solutions from the continuous adequacy spectrum characteristic of natural language solutions. Within this framework, we propose two statistical quality dimensions. Normalized bi-semantic entropy measures robustness and conceptual diversity of LLM answers given semantic variation in problem formulations. Emotional valence maps subjective valuation of a solution to a quantifiable metric that can be maximized by invoking statistical measures. The concepts introduced in this work will provide a more rigorous understanding of the capabilities, limitations, and inherent nature of problem-solving in the age of LLMs.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unifying Framework for Semiring-Based Constraint Logic Programming With Negation (full version)</title>
<link>https://arxiv.org/abs/2507.16067</link>
<guid>https://arxiv.org/abs/2507.16067</guid>
<content:encoded><![CDATA[
<div> Constraint Logic Programming, CLP, constraints, extension, negation <br />
<br />Summary: <br />
This article introduces an extension of Constraint Logic Programming (CLP) that incorporates negation in the body, unifying previous extensions such as fuzzy constraint satisfaction and uncertainty. The semantics of these programs are provided using approximation fixpoint theory, analyzing the impact of semiring properties on the resulting semantics. By unifying existing approaches and allowing for a more expressive language, this framework extends the capabilities of CLP to solve complex problems requiring constraint consideration. <div>
arXiv:2507.16067v1 Announce Type: new 
Abstract: Constraint Logic Programming (CLP) is a logic programming formalism used to solve problems requiring the consideration of constraints, like resource allocation and automated planning and scheduling. It has previously been extended in various directions, for example to support fuzzy constraint satisfaction, uncertainty, or negation, with different notions of semiring being used as a unifying abstraction for these generalizations. None of these extensions have studied clauses with negation allowed in the body. We investigate an extension of CLP which unifies many of these extensions and allows negation in the body. We provide semantics for such programs, using the framework of approximation fixpoint theory, and give a detailed overview of the impacts of properties of the semirings on the resulting semantics. As such, we provide a unifying framework that captures existing approaches and allows extending them with a more expressive language.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expert-Guided LLM Reasoning for Battery Discovery: From AI-Driven Hypothesis to Synthesis and Characterization</title>
<link>https://arxiv.org/abs/2507.16110</link>
<guid>https://arxiv.org/abs/2507.16110</guid>
<content:encoded><![CDATA[
<div> framework, large language models, materials design, lithium-ion batteries, battery discovery
<br />
<br />
Summary: 
Large language models (LLMs) with chain-of-thought (CoT) techniques have shown significant potential in AI for solving complex problems, especially in math and coding domains. However, their application in domain-specific areas like battery discovery has been limited. A new agentic framework called ChatBattery integrates domain knowledge to guide LLMs in materials design for lithium-ion batteries. Using ChatBattery, three novel cathode materials with significant capacity improvements over the widely used NMC811 material were identified, synthesized, and characterized successfully. This AI-driven cycle, from design to synthesis to characterization, demonstrates the transformative impact of AI-driven reasoning in revolutionizing materials discovery. ChatBattery sets a precedent for LLM-driven reasoning-based platforms in domain-specific applications, showcasing the effectiveness of guided search techniques in advancing materials science. <div>
arXiv:2507.16110v1 Announce Type: new 
Abstract: Large language models (LLMs) leverage chain-of-thought (CoT) techniques to tackle complex problems, representing a transformative breakthrough in artificial intelligence (AI). However, their reasoning capabilities have primarily been demonstrated in solving math and coding problems, leaving their potential for domain-specific applications-such as battery discovery-largely unexplored. Inspired by the idea that reasoning mirrors a form of guided search, we introduce ChatBattery, a novel agentic framework that integrates domain knowledge to steer LLMs toward more effective reasoning in materials design. Using ChatBattery, we successfully identify, synthesize, and characterize three novel lithium-ion battery cathode materials, which achieve practical capacity improvements of 28.8%, 25.2%, and 18.5%, respectively, over the widely used cathode material, LiNi0.8Mn0.1Co0.1O2 (NMC811). Beyond this discovery, ChatBattery paves a new path by showing a successful LLM-driven and reasoning-based platform for battery materials invention. This complete AI-driven cycle-from design to synthesis to characterization-demonstrates the transformative potential of AI-driven reasoning in revolutionizing materials discovery.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task</title>
<link>https://arxiv.org/abs/2507.16126</link>
<guid>https://arxiv.org/abs/2507.16126</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, taxes, benchmark, models, personal income<br />
<br />
Summary: 
The article discusses the challenge of using AI to file personal income taxes in the United States. It introduces TaxCalcBench, a benchmark for evaluating models' performance in tax return calculations. The experiment showed that state-of-the-art models struggle in accurately computing federal income tax returns, demonstrating limitations in understanding tax tables, calculating taxes, and determining eligibility. The analysis suggests the need for improved infrastructure to enhance the accuracy of Language Model (LLMs) when applied to tax calculation tasks. The study highlights the complexity of the tax filing process, emphasizing the ongoing development required to enable AI to effectively handle such intricate tasks. <br /><br /> <div>
arXiv:2507.16126v1 Announce Type: new 
Abstract: Can AI file your taxes? Not yet. Calculating US personal income taxes is a task that requires building an understanding of vast amounts of English text and using that knowledge to carefully compute results. We propose TaxCalcBench, a benchmark for determining models' abilities to calculate personal income tax returns given all of the necessary information. Our experiment shows that state-of-the-art models succeed in calculating less than a third of federal income tax returns even on this simplified sample set. Our analysis concludes that models consistently misuse tax tables, make errors in tax calculation, and incorrectly determine eligibility. Our findings point to the need for additional infrastructure to apply LLMs to the personal income tax calculation task.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting</title>
<link>https://arxiv.org/abs/2507.16145</link>
<guid>https://arxiv.org/abs/2507.16145</guid>
<content:encoded><![CDATA[
<div> Keywords: COPD, Spirogram, AI models, Large Language Models, Clinical decision support tools

Summary: 
SpiroLLM is a novel multimodal large language model designed to understand spirograms and aid in the diagnosis of Chronic Obstructive Pulmonary Disease (COPD). By incorporating morphological features from respiratory curves and aligning them with pulmonary function test numerical values, SpiroLLM can generate comprehensive diagnostic reports with a high level of accuracy, as indicated by a diagnostic AUROC of 0.8980. Additionally, in a robustness test with missing core data, SpiroLLM demonstrated a 100% valid response rate compared to a text-only model's 13.4%, highlighting its superior multimodal design. This innovative approach of combining physiological signals with large language models paves the way for more interpretable and reliable clinical decision support tools, offering potential benefits for early detection and monitoring of respiratory diseases like COPD.<br /><br />Summary: <div>
arXiv:2507.16145v1 Announce Type: new 
Abstract: Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory disease with persistent airflow limitation, is a leading global cause of disability and mortality. Respiratory spirogram time series, routinely collected during pulmonary function tests (PFTs), play a critical role in the early detection of repsiratory diseases and in monitoring lung function over time. However, most current AI models for COPD diagnosis are limited to outputting classification results without providing a rationale for their diagnostic process, while current Large Language Models (LLMs) cannot understand spirograms yet, which severely limits their clinical trust and adoption. To tackle this challenge, we leverage a cohort of 234,028 individuals from the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large language model that can understand spirogram. The model extracts morphological features from respiratory curves via a SpiroEncoder and aligns them with PFT numerical values in a unified latent space using a SpiroProjector, ultimately empowering a large language model to generate a comprehensive diagnostic report. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC of 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data, it maintained a 100% valid response rate, far surpassing the 13.4% of a text-only model and showcasing the superiority of its multimodal design. This work demonstrates the substantial potential of deeply fusing physiological signals with large language models, establishing a new paradigm for the next generation of interpretable and reliable clinical decision support tools.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Cognitive Convergence via Implementation: A Structured Loop Reflecting Four Theories of Mind (A Position Paper)</title>
<link>https://arxiv.org/abs/2507.16184</link>
<guid>https://arxiv.org/abs/2507.16184</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agent architecture, cognitive theories, practical design, structural convergence, meta-architecture 

Summary: 
The article discusses the discovery of a structural convergence in four prominent theories of mind within a practical AI agent architecture known as Agentic Flow. This architecture consists of five modules arranged in a recurrent cognitive loop, showing similarities with predictive processing, associative recall, and error-sensitive control found in the cognitive theories of Kahneman, Friston, Minsky, and Clark. Experimental results comparing the structured agent with baseline systems on multi-step reasoning tasks demonstrate higher task success and constraint adherence. The introduction of PEACE as a descriptive meta-architecture captures design-level regularities observed in Agentic Flow, providing a shared vocabulary for understanding architectures shaped by real-world implementation demands. The paper serves as a position paper that explores how implementation choices can reveal underlying structural similarities with cognitive theories, without claiming theoretical unification. 

<br /><br />Summary: <div>
arXiv:2507.16184v1 Announce Type: new 
Abstract: We report the discovery of a structural convergence across four influential theories of mind: Kahneman's dual-system theory, Friston's predictive processing, Minsky's society of mind, and Clark's extended mind-emerging unintentionally within a practical AI agent architecture called Agentic Flow. Designed to address limitations in large language models (LLMs), Agentic Flow comprises five interdependent modules such as Retrieval, Cognition, Control, Memory, and Action arranged in a recurrent cognitive loop. Although originally inspired only by Minsky and Clark, the system's structure retrospectively aligns with computational motifs found in all four theories, including predictive modeling, associative recall, and error-sensitive control.
  To assess this convergence, we conducted comparative experiments with baseline LLM agents on multi-step reasoning tasks. The structured agent achieved 95.8% task success and exhibited strong constraint adherence, while the baseline system succeeded 62.3% of the time. These results were not aimed at proving superiority, but at illustrating how theoretical structures may emerge through practical design choices rather than top-down theory.
  We introduce PEACE as a descriptive meta-architecture that captures design-level regularities observed in Agentic Flow. Not intended as a new theory, PEACE provides a shared vocabulary for understanding architectures shaped by real-world implementation demands. This paper should be read as a position paper - an exploratory reflection on how implementation can surface latent structural echoes of cognitive theory, without asserting theoretical unification.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHIMERA: Compressed Hybrid Intelligence for Twin-Model Enhanced Multi-Agent Deep Reinforcement Learning for Multi-Functional RIS-Assisted Space-Air-Ground Integrated Networks</title>
<link>https://arxiv.org/abs/2507.16204</link>
<guid>https://arxiv.org/abs/2507.16204</guid>
<content:encoded><![CDATA[
<div> MF-RIS, SAGIN, energy efficiency, optimization, deep reinforcement learning<br />
Summary:<br />
The research proposes a Space-Air-Ground Integrated Network (SAGIN) architecture utilizing multi-functional reconfigurable intelligent surfaces (MF-RIS) to address energy shortages of low-Earth orbit satellites, considering communication and computing energy consumption. A joint optimization problem is formulated for MF-RIS and SAGIN parameters. To solve this non-convex problem, a compressed hybrid intelligence for twin-model enhanced multi-agent deep reinforcement learning (CHIMERA) framework is introduced. Simulation results show that the CHIMERA scheme outperforms conventional benchmarks in terms of energy efficiency. The proposed SAGIN-MF-RIS architecture offers superior energy efficiency performance compared to standalone satellite, aerial, or ground-only deployments, due to its complementary coverage and efficient exploration of complex actions. <div>
arXiv:2507.16204v1 Announce Type: new 
Abstract: A space-air-ground integrated network (SAGIN) architecture is proposed, empowered by multi-functional reconfigurable intelligent surfaces (MF-RIS) capable of simultaneously reflecting, amplifying, and harvesting wireless energy. The MF-RIS plays a pivotal role in addressing the energy shortages of low-Earth orbit (LEO) satellites operating in shadowed regions, while explicitly accounting for both communication and computing energy consumption across the SAGIN nodes. To maximize the long-term energy efficiency (EE), we formulate a joint optimization problem over the MF-RIS parameters, including signal amplification, phase-shifts, energy harvesting ratio, and active element selection as well as the SAGIN parameters of beamforming vectors, high-altitude platform station (HAPS) deployment, user association, and computing capability. The formulated problem is highly non-convex and non-linear and contains mixed discrete-continuous parameters. To tackle this, we conceive a compressed hybrid intelligence for twin-model enhanced multi-agent deep reinforcement learning (CHIMERA) framework, which integrates semantic state-action compression and parametrized sharing under hybrid reinforcement learning to efficiently explore suitable complex actions. The simulation results have demonstrated that the proposed CHIMERA scheme substantially outperforms the conventional benchmarks, including fixed-configuration or non-harvesting MF-RIS, traditional RIS, and no-RIS cases, as well as centralized and multi-agent deep reinforcement learning baselines in terms of the highest EE. Moreover, the proposed SAGIN-MF-RIS architecture achieves superior EE performance due to its complementary coverage, offering notable advantages over either standalone satellite, aerial, or ground-only deployments.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilled Large Language Model in Confidential Computing Environment for System-on-Chip Design</title>
<link>https://arxiv.org/abs/2507.16226</link>
<guid>https://arxiv.org/abs/2507.16226</guid>
<content:encoded><![CDATA[
<div> evaluation, Large Language Models, Trusted Execution Environments, performance, semiconductor CAD applications
Summary:
<br />
In this study, the authors evaluate the performance of Large Language Models (LLMs) within a Trusted Execution Environment (TEE) enabled by Intel Trust Domain Extensions (TDX). They conduct experiments using TEE-based, CPU-only, and CPU-GPU hybrid implementations to assess the tokens per second performance. The results show that distilled models like DeepSeek outperform others due to their smaller parameters, making them suitable for resource-constrained devices. Quantized models with 4-bit and 8-bit quantization exhibit up to a 3x performance gain compared to FP16 models. The TDX implementation surpasses the CPU version for certain parameter sets, highlighting efficient computation execution in a secure environment. Validation tests using a testbench for System-on-Chip (SoC) design tasks demonstrate the potential of deploying lightweight LLMs on resource-constrained systems for semiconductor Computer-Aided Design (CAD) applications.
<br /> <div>
arXiv:2507.16226v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used in circuit design tasks and have typically undergone multiple rounds of training. Both the trained models and their associated training data are considered confidential intellectual property (IP) and must be protected from exposure. Confidential Computing offers a promising solution to protect data and models through Trusted Execution Environments (TEEs). However, existing TEE implementations are not designed to support the resource-intensive nature of LLMs efficiently. In this work, we first present a comprehensive evaluation of the LLMs within a TEE-enabled confidential computing environment, specifically utilizing Intel Trust Domain Extensions (TDX). We constructed experiments on three environments: TEE-based, CPU-only, and CPU-GPU hybrid implementations, and evaluated their performance in terms of tokens per second.
  Our first observation is that distilled models, i.e., DeepSeek, surpass other models in performance due to their smaller parameters, making them suitable for resource-constrained devices. Also, in the quantized models such as 4-bit quantization (Q4) and 8-bit quantization (Q8), we observed a performance gain of up to 3x compared to FP16 models. Our findings indicate that for fewer parameter sets, such as DeepSeek-r1-1.5B, the TDX implementation outperforms the CPU version in executing computations within a secure environment. We further validate the results using a testbench designed for SoC design tasks. These validations demonstrate the potential of efficiently deploying lightweight LLMs on resource-constrained systems for semiconductor CAD applications.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery</title>
<link>https://arxiv.org/abs/2507.16229</link>
<guid>https://arxiv.org/abs/2507.16229</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, voice assistants, healthcare, patient monitoring, cost-effective<br />
Summary:<br />
The paper discusses the integration of large language model-powered voice assistants in healthcare, focusing on their role in preventive care and continuous monitoring for underserved populations. Through the development and pilot study of Agent PULSE, the potential cost-effectiveness of AI agents in providing healthcare services where human intervention is not economically feasible is demonstrated. The pilot study with inflammatory bowel disease patients showed high acceptance of AI-driven monitoring, with technical challenges and policy considerations also highlighted. AI-driven voice agents are shown to enhance healthcare scalability, efficiency, patient engagement, and accessibility. Healthcare executives can benefit from potential cost savings, while technologists can prioritize improvements with the highest patient impact. By addressing limitations and aligning AI development with ethical and regulatory frameworks, voice-based AI agents can be a crucial part of equitable and sustainable digital healthcare solutions.<br /><br /> <div>
arXiv:2507.16229v1 Announce Type: new 
Abstract: The integration of voice-based AI agents in healthcare presents a transformative opportunity to bridge economic and accessibility gaps in digital health delivery. This paper explores the role of large language model (LLM)-powered voice assistants in enhancing preventive care and continuous patient monitoring, particularly in underserved populations. Drawing insights from the development and pilot study of Agent PULSE (Patient Understanding and Liaison Support Engine) -- a collaborative initiative between IBM Research, Cleveland Clinic Foundation, and Morehouse School of Medicine -- we present an economic model demonstrating how AI agents can provide cost-effective healthcare services where human intervention is economically unfeasible. Our pilot study with 33 inflammatory bowel disease patients revealed that 70\% expressed acceptance of AI-driven monitoring, with 37\% preferring it over traditional modalities. Technical challenges, including real-time conversational AI processing, integration with healthcare systems, and privacy compliance, are analyzed alongside policy considerations surrounding regulation, bias mitigation, and patient autonomy. Our findings suggest that AI-driven voice agents not only enhance healthcare scalability and efficiency but also improve patient engagement and accessibility. For healthcare executives, our cost-utility analysis demonstrates huge potential savings for routine monitoring tasks, while technologists can leverage our framework to prioritize improvements yielding the highest patient impact. By addressing current limitations and aligning AI development with ethical and regulatory frameworks, voice-based AI agents can serve as a critical entry point for equitable, sustainable digital healthcare solutions.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of Scientific Inquiry</title>
<link>https://arxiv.org/abs/2507.16280</link>
<guid>https://arxiv.org/abs/2507.16280</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep AI Research Systems, ResearcherBench, benchmark, AI scientific questions, evaluation 

Summary: 
ResearcherBench is a new benchmark designed to evaluate the capabilities of advanced Deep AI Research Systems (DARS) in solving frontier AI scientific questions. The benchmark consists of 65 expertly selected research questions categorized into technical details, literature review, and open consulting. An evaluation framework combining rubric assessment and factual assessment was used to measure insight quality, citation accuracy, and coverage. Results showed that OpenAI Deep Research and Gemini Deep Research performed significantly better than other systems, especially in open-ended consulting questions. The development of ResearcherBench provides a standardized platform for advancing the field of AI research assistants and promoting next-generation AI self-improvement. The open-sourcing of ResearcherBench aims to encourage novel patterns of scientific collaboration in AI research evaluation. 

<br /><br />Summary: <div>
arXiv:2507.16280v1 Announce Type: new 
Abstract: The emergence of deep research systems presents significant capabilities in problem-solving, extending from basic queries to sophisticated research tasks. However, existing benchmarks primarily evaluate these systems as agents for web retrieval and report generation, overlooking their potential to discover novel insights on the frontiers of scientific research. To address this gap, we introduce ResearcherBench, the first benchmark focused on evaluating the capabilities of these advanced, agentic systems - which we refer to as Deep AI Research Systems (DARS) - on frontier AI scientific questions. We compiled a dataset of 65 research questions expertly selected from real-world scientific scenarios such as laboratory discussions and interviews, spanning 35 different AI subjects and categorized into three types: technical details, literature review, and open consulting. Our dual evaluation framework combines rubric assessment, which uses expert-designed criteria to evaluate insight quality, with factual assessment, which measures citation accuracy (faithfulness) and coverage (groundedness). We evaluated several leading commercial DARS and baseline systems. Results show that OpenAI Deep Research and Gemini Deep Research significantly outperform other systems, with particular strength in open-ended consulting questions. Such capabilities represent a meaningful step toward AI self-improvement, aligning with the vision of ASI for AI. We open-source ResearcherBench to provide a standardized platform for promoting the development of next-generation AI research assistants, hoping to foster a new perspective in AI research evaluation for a novel pattern of scientific collaboration: https://github.com/GAIR-NLP/ResearcherBench.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modal Distillation For Widely Differing Modalities</title>
<link>https://arxiv.org/abs/2507.16296</link>
<guid>https://arxiv.org/abs/2507.16296</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, multi-modal learning, knowledge distillation, feature level, classifier level 

Summary: 
In the field of deep learning, improving model performance by increasing its size can be inefficient. Multi-modal learning, which combines different types of information as input, can enhance performance. This study introduces a cross-modal distillation framework for knowledge transfer between different modalities such as image, text, and speech. Traditional hard constrained loss methods can lead to overfitting in cross-modal distillation, so the study proposes two soft constrained knowledge distillation strategies at the feature and classifier levels. Additionally, a quality-based adaptive weights module is introduced to enhance model training by weighing input samples based on data quality. Experimental results on speaker recognition and image classification tasks demonstrate the effectiveness of the approach in transferring knowledge across diverse modalities. 

<br /><br />Summary: <div>
arXiv:2507.16296v1 Announce Type: new 
Abstract: Deep learning achieved great progress recently, however, it is not easy or efficient to further improve its performance by increasing the size of the model. Multi-modal learning can mitigate this challenge by introducing richer and more discriminative information as input. To solve the problem of limited access to multi-modal data at the time of use, we conduct multi-modal learning by introducing a teacher model to transfer discriminative knowledge to a student model during training. However, this knowledge transfer via distillation is not trivial because the big domain gap between the widely differing modalities can easily lead to overfitting. In this work, we introduce a cross-modal distillation framework. Specifically, we find hard constrained loss, e.g. l2 loss forcing the student being exact the same as the teacher, can easily lead to overfitting in cross-modality distillation. To address this, we propose two soft constrained knowledge distillation strategies at the feature level and classifier level respectively. In addition, we propose a quality-based adaptive weights module to weigh input samples via quantified data quality, leading to robust model training. We conducted experiments on speaker recognition and image classification tasks, and the results show that our approach is able to effectively achieve knowledge transfer between the commonly used and widely differing modalities of image, text, and speech.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Evaluating the Representativeness of Quantitative Medical Language Reasoning LLM Benchmarks for African Disease Burdens</title>
<link>https://arxiv.org/abs/2507.16322</link>
<guid>https://arxiv.org/abs/2507.16322</guid>
<content:encoded><![CDATA[
<div> Keywords: medical LLM benchmarks, African deployment, neglected tropical diseases, guideline alignment, Alama Health QA <br />
Summary: 
Existing medical LLM benchmarks do not adequately reflect the disease burden and guidelines of African settings, potentially affecting the validity of model evaluation and deployment. A systematic review of 31 evaluation papers identified 19 benchmarks, with Alama Health QA developed using Kenyan Clinical Practice Guidelines performing the best in capturing neglected tropical disease mentions. Key findings showed disparities in representation of African diseases across benchmarks, with Alama Health QA scoring highest in clinical relevance and guideline alignment. The study highlights the importance of regionally curated resources like Alama Health QA to ensure safe and equitable model evaluation and deployment in African health systems. Additional emphasis on guideline alignment and disease-specific derivatives is essential for improving the accuracy and applicability of medical LLM benchmarks in African contexts.<br /><br /> <div>
arXiv:2507.16322v1 Announce Type: new 
Abstract: Introduction: Existing medical LLM benchmarks largely reflect examination syllabi and disease profiles from high income settings, raising questions about their validity for African deployment where malaria, HIV, TB, sickle cell disease and other neglected tropical diseases (NTDs) dominate burden and national guidelines drive care. Methodology: We systematically reviewed 31 quantitative LLM evaluation papers (Jan 2019 May 2025) identifying 19 English medical QA benchmarks. Alama Health QA was developed using a retrieval augmented generation framework anchored on the Kenyan Clinical Practice Guidelines. Six widely used sets (AfriMedQA, MMLUMedical, PubMedQA, MedMCQA, MedQAUSMLE, and guideline grounded Alama Health QA) underwent harmonized semantic profiling (NTD proportion, recency, readability, lexical diversity metrics) and blinded expert rating across five dimensions: clinical relevance, guideline alignment, clarity, distractor plausibility, and language/cultural fit. Results: Alama Health QA captured >40% of all NTD mentions across corpora and the highest within set frequencies for malaria (7.7%), HIV (4.1%), and TB (5.2%); AfriMedQA ranked second but lacked formal guideline linkage. Global benchmarks showed minimal representation (e.g., sickle cell disease absent in three sets) despite large scale. Qualitatively, Alama scored highest for relevance and guideline alignment; PubMedQA lowest for clinical utility. Discussion: Quantitative medical LLM benchmarks widely used in the literature underrepresent African disease burdens and regulatory contexts, risking misleading performance claims. Guideline anchored, regionally curated resources such as Alama Health QA and expanded disease specific derivatives are essential for safe, equitable model evaluation and deployment across African health systems.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher Gauge Flow Models</title>
<link>https://arxiv.org/abs/2507.16334</link>
<guid>https://arxiv.org/abs/2507.16334</guid>
<content:encoded><![CDATA[
<div> Higher Gauge Flow Models, Generative Flow Models, L∞-algebra, higher geometry, higher symmetries<br />
<br />
Summary: <br />
This paper introduces Higher Gauge Flow Models, which are an extension of ordinary Gauge Flow Models by incorporating an L∞-algebra to include higher geometries and symmetries associated with higher groups. Experimental results on a Gaussian Mixture Model dataset demonstrated significant performance improvements compared to traditional Flow Models. The Higher Gauge Flow Models leverage the extended Lie Algebra to enhance the generative flow modeling process, allowing for a more comprehensive representation of complex data distributions. The integration of higher geometries and symmetries provides a more effective framework for capturing and modeling complex patterns in data, leading to improved performance in generating samples. This novel approach opens up new possibilities for incorporating advanced mathematical concepts into flow modeling, offering a promising pathway for enhancing generative modeling techniques. <div>
arXiv:2507.16334v1 Announce Type: new 
Abstract: This paper introduces Higher Gauge Flow Models, a novel class of Generative Flow Models. Building upon ordinary Gauge Flow Models (arXiv:2507.13414), these Higher Gauge Flow Models leverage an L$_{\infty}$-algebra, effectively extending the Lie Algebra. This expansion allows for the integration of the higher geometry and higher symmetries associated with higher groups into the framework of Generative Flow Models. Experimental evaluation on a Gaussian Mixture Model dataset revealed substantial performance improvements compared to traditional Flow Models.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Call: A Field Trial of a Collaborative Bandit Algorithm for Improved Message Delivery in Mobile Maternal Health</title>
<link>https://arxiv.org/abs/2507.16356</link>
<guid>https://arxiv.org/abs/2507.16356</guid>
<content:encoded><![CDATA[
<div> mHealth, automated voice messages, underserved communities, Kilkari program, bandit algorithm <br />
Summary: 
This study evaluates the effectiveness of using a collaborative bandit algorithm to optimize call timing in delivering maternal health information through India's Kilkari program via automated voice calls. By personalizing call schedules based on individual mothers' preferences, the algorithm significantly improves call pick-up rates compared to the random calling approach. This research highlights the potential of machine learning in enhancing message delivery in mobile health interventions, particularly in reaching underserved populations like mothers in India. The findings suggest that personalized scheduling can maximize the impact of mHealth programs, improving health outcomes through increased awareness and behavioral change. The study underscores the importance of tailored interventions in mobile health outreach and demonstrates the scalability of using machine learning to enhance maternal health initiatives. <br /> <div>
arXiv:2507.16356v1 Announce Type: new 
Abstract: Mobile health (mHealth) programs utilize automated voice messages to deliver health information, particularly targeting underserved communities, demonstrating the effectiveness of using mobile technology to disseminate crucial health information to these populations, improving health outcomes through increased awareness and behavioral change. India's Kilkari program delivers vital maternal health information via weekly voice calls to millions of mothers. However, the current random call scheduling often results in missed calls and reduced message delivery. This study presents a field trial of a collaborative bandit algorithm designed to optimize call timing by learning individual mothers' preferred call times. We deployed the algorithm with around $6500$ Kilkari participants as a pilot study, comparing its performance to the baseline random calling approach. Our results demonstrate a statistically significant improvement in call pick-up rates with the bandit algorithm, indicating its potential to enhance message delivery and impact millions of mothers across India. This research highlights the efficacy of personalized scheduling in mobile health interventions and underscores the potential of machine learning to improve maternal health outreach at scale.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Canonical Representations of Markovian Structural Causal Models: A Framework for Counterfactual Reasoning</title>
<link>https://arxiv.org/abs/2507.16370</link>
<guid>https://arxiv.org/abs/2507.16370</guid>
<content:encoded><![CDATA[
<div> Counterfactual reasoning, causal models, individual-wise fairness, Markovian setting, counterfactual models.<br />Summary: Counterfactual reasoning deals with answering hypothetical questions about causation, influencing concepts like individual fairness. The article proposes a new approach called counterfactual models, which allow for formalizing and implementing various counterfactual beliefs within a causal framework. These models provide a way to represent counterfactuals compatible with a given causal graphical model and enable analysts to choose a counterfactual conception via random-process probability distributions. A normalization procedure is introduced to describe and implement different counterfactual conceptions without altering observational and interventional constraints. This approach simplifies the process by only requiring a choice of counterfactual conception, without the need to estimate model content. The article illustrates the significance of counterfactuals in causality and demonstrates the benefits of the proposed approach through theoretical and numerical examples. <br /><br />Summary: <div>
arXiv:2507.16370v1 Announce Type: new 
Abstract: Counterfactual reasoning aims at answering contrary-to-fact questions like ''Would have Alice recovered had she taken aspirin?'' and corresponds to the most fine-grained layer of causation. Critically, while many counterfactual statements cannot be falsified -- even by randomized experiments -- they underpin fundamental concepts like individual-wise fairness. Therefore, providing models to formalize and implement counterfactual beliefs remains a fundamental scientific problem. In the Markovian setting of Pearl's causal framework, we propose an alternative approach to structural causal models to represent counterfactuals compatible with a given causal graphical model. More precisely, we introduce counterfactual models, also called canonical representations of structural causal models. They enable analysts to choose a counterfactual conception via random-process probability distributions with preassigned marginals and characterize the counterfactual equivalence class of structural causal models. Then, we present a normalization procedure to describe and implement various counterfactual conceptions. Compared to structural causal models, it allows to specify many counterfactual conceptions without altering the observational and interventional constraints. Moreover, the content of the model corresponding to the counterfactual layer does not need to be estimated; only to make a choice. Finally, we illustrate the specific role of counterfactuals in causality and the benefits of our approach on theoretical and numerical examples.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Driven Collaborative Model for Untangling Commits via Explicit and Implicit Dependency Reasoning</title>
<link>https://arxiv.org/abs/2507.16395</link>
<guid>https://arxiv.org/abs/2507.16395</guid>
<content:encoded><![CDATA[
<div> commit, untangling, Large Language Model, code changes, collaboration <br />
Summary: <br />
The article introduces ColaUntangle, a new framework for untangling commits in software development. It addresses the issue of developers producing tangled commits that mix unrelated changes. ColaUntangle utilizes a multi-agent architecture with agents specializing in explicit and implicit dependencies among code changes. It integrates Large Language Models to capture both symbolic and semantic depth in code relationships. The framework, evaluated on C# and Java datasets, outperformed existing baselines, showing a significant improvement in untangling efficiency. The results emphasize the effectiveness of LLM-based collaborative frameworks for automated commit untangling tasks. <div>
arXiv:2507.16395v1 Announce Type: new 
Abstract: Atomic commits, each of which addresses a single development concern, are a best practice in software development. However, developers frequently produce tangled commits that mix unrelated changes due to practical constraints or unclear boundaries, negatively impacting code review and maintenance. Although prior commit untangling approaches: rule-based, feature-based, or graph-based, have made progress, they often rely on shallow signals and fail to distinguish between explicit dependencies (e.g., control/data flow) and implicit ones (e.g., semantic or conceptual relationships). In this paper, we propose ColaUntangle, a new collaborative consultation framework for commit untangling that models both explicit and implicit dependencies among code changes. ColaUntangle integrates Large Language Model (LLM)-driven agents in a multi-agent architecture: one agent specializes in explicit dependencies, another in implicit ones, and a reviewer agent synthesizes their perspectives through iterative consultation. To capture explicit and implicit contextual information, we construct multi-version Program Dependency Graphs (delta-PDG), enabling agents to reason over code relationships with both symbolic and semantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C# and 14k Java tangled commits). Experimental results show that ColaUntangle outperforms the best-performing baseline, achieving an improvement of 44% on the C# dataset and 100% on the Java dataset. These findings highlight the potential of LLM-based collaborative frameworks for advancing automated commit untangling tasks.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Inductive Logic Programming</title>
<link>https://arxiv.org/abs/2507.16405</link>
<guid>https://arxiv.org/abs/2507.16405</guid>
<content:encoded><![CDATA[
<div> Keywords: Inductive Logic Programming, Meta-Interpretive Learning, Self-Supervised Learning, Second Order Definite Normal Form, Poker

Summary:
Self-Supervised Inductive Logic Programming introduces a new learning setting where a background theory or negative examples are not provided for the learning process. A new MIL algorithm called Poker is developed to address this challenge by learning from positive labeled examples and generating new positive and negative examples automatically. By utilizing a Second Order Definite Normal Form (SONF) as a general second-order background theory, the need for task-specific theories is eliminated. Comparing Poker to existing systems like Louise in grammar learning tasks, Poker's performance improves as the number of generated examples increases, while Louise struggles due to the lack of negative examples, leading to over-generalization. This approach signifies a step towards efficient and adaptable ILP systems that can learn effectively in diverse settings without the need for expert-tailored inputs. 

<br /><br />Summary: <div>
arXiv:2507.16405v1 Announce Type: new 
Abstract: Inductive Logic Programming (ILP) approaches like Meta \-/ Interpretive Learning (MIL) can learn, from few examples, recursive logic programs with invented predicates that generalise well to unseen instances. This ability relies on a background theory and negative examples, both carefully selected with expert knowledge of a learning problem and its solutions. But what if such a problem-specific background theory or negative examples are not available? We formalise this question as a new setting for Self-Supervised ILP and present a new MIL algorithm that learns in the new setting from some positive labelled, and zero or more unlabelled examples, and automatically generates, and labels, new positive and negative examples during learning. We implement this algorithm in Prolog in a new MIL system, called Poker. We compare Poker to state-of-the-art MIL system Louise on experiments learning grammars for Context-Free and L-System languages from labelled, positive example strings, no negative examples, and just the terminal vocabulary of a language, seen in examples, as a first-order background theory. We introduce a new approach for the principled selection of a second-order background theory as a Second Order Definite Normal Form (SONF), sufficiently general to learn all programs in a class, thus removing the need for a backgound theory tailored to a learning task. We find that Poker's performance improves with increasing numbers of automatically generated examples while Louise, bereft of negative examples, over-generalises.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Pre-training Data in LLMs: A Neuron Activation-Based Detection Framework</title>
<link>https://arxiv.org/abs/2507.16414</link>
<guid>https://arxiv.org/abs/2507.16414</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, pre-training data detection, neural activation patterns, bias detection, benchmarking <br />
Summary: <br />
The article addresses concerns around the training data of large language models (LLMs) and the biases they might internalize. It introduces a novel algorithm called NA-PDD that analyzes neuron activation patterns in LLMs to detect specific data in their pre-training corpus. The algorithm outperforms existing methods by focusing on differential neuron activation between training and non-training data. Additionally, the article introduces CCNewsPDD, a benchmark that ensures consistent time distributions between training and non-training data, providing a more rigorous evaluation. Through experiments, it is demonstrated that NA-PDD is effective in identifying pre-training data in multiple LLMs and outperforms existing techniques. The proposed approach aims to address legal and ethical concerns related to copyrighted or private data, as well as dataset contamination and bias internalization in LLMs. <br /> <div>
arXiv:2507.16414v1 Announce Type: new 
Abstract: The performance of large language models (LLMs) is closely tied to their training data, which can include copyrighted material or private information, raising legal and ethical concerns. Additionally, LLMs face criticism for dataset contamination and internalizing biases. To address these issues, the Pre-Training Data Detection (PDD) task was proposed to identify if specific data was included in an LLM's pre-training corpus. However, existing PDD methods often rely on superficial features like prediction confidence and loss, resulting in mediocre performance. To improve this, we introduce NA-PDD, a novel algorithm analyzing differential neuron activation patterns between training and non-training data in LLMs. This is based on the observation that these data types activate different neurons during LLM inference. We also introduce CCNewsPDD, a temporally unbiased benchmark employing rigorous data transformations to ensure consistent time distributions between training and non-training data. Our experiments demonstrate that NA-PDD significantly outperforms existing methods across three benchmarks and multiple LLMs.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From model-based learning to model-free behaviour with Meta-Interpretive Learning</title>
<link>https://arxiv.org/abs/2507.16434</link>
<guid>https://arxiv.org/abs/2507.16434</guid>
<content:encoded><![CDATA[
<div> Agent, Model, Model-based, Model-free, Meta-Interpretive Learning <br />
<br />
Summary: 
In the study, the concept of combining model-based and model-free capabilities in an autonomous agent is explored. A model is a theory describing the environment and agent actions' effects. Model-based agents can plan using predictions but require knowing the environment state, while model-free agents can act without planning. The researchers developed an agent using Meta-Interpretive Learning to train a model-based Solver and a model-free Controller. Testing on grid navigation problems in various environments showed that both agents were equivalent in problem-solving ability, indicating the Controller could solve tasks like the Solver. The experiments were conducted on randomly-generated mazes and lake maps with open spaces, demonstrating the combined agent's effectiveness in navigating unknown environments. <div>
arXiv:2507.16434v1 Announce Type: new 
Abstract: A "model" is a theory that describes the state of an environment and the effects of an agent's decisions on the environment. A model-based agent can use its model to predict the effects of its future actions and so plan ahead, but must know the state of the environment. A model-free agent cannot plan, but can act without a model and without completely observing the environment. An autonomous agent capable of acting independently in novel environments must combine both sets of capabilities. We show how to create such an agent with Meta-Interpretive Learning used to learn a model-based Solver used to train a model-free Controller that can solve the same planning problems as the Solver. We demonstrate the equivalence in problem-solving ability of the two agents on grid navigation problems in two kinds of environment: randomly generated mazes, and lake maps with wide open areas. We find that all navigation problems solved by the Solver are also solved by the Controller, indicating the two are equivalent.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving ASP-based ORS Schedules through Machine Learning Predictions</title>
<link>https://arxiv.org/abs/2507.16454</link>
<guid>https://arxiv.org/abs/2507.16454</guid>
<content:encoded><![CDATA[
<div> Keywords: Operating Room Scheduling, Answer Set Programming, Machine Learning Algorithms, Surgery Duration Prediction, Robust Schedules

Summary: 
The paper addresses the Operating Room Scheduling (ORS) problem using a combination of machine learning algorithms and Answer Set Programming (ASP). The ORS problem involves optimizing daily surgery schedules with various constraints. While existing ASP solutions can verify alignments with real data and suggest alternative schedules, they cannot generate provisional schedules or ensure robustness. To overcome this limitation, the paper integrates inductive techniques like machine learning to predict surgery duration based on historical data, enabling the computation of provisional schedules. Additionally, the confidence of these predictions is used as input to enhance the generation of more robust schedules. The approach was validated using historical data from ASL1 Liguria in Italy, demonstrating the effectiveness of the integration. The study is currently under consideration for publication in Theory and Practice of Logic Programming (TPLP). 

<br /><br />Summary: <div>
arXiv:2507.16454v1 Announce Type: new 
Abstract: The Operating Room Scheduling (ORS) problem deals with the optimization of daily operating room surgery schedules. It is a challenging problem subject to many constraints, like to determine the starting time of different surgeries and allocating the required resources, including the availability of beds in different department units. Recently, solutions to this problem based on Answer Set Programming (ASP) have been delivered. Such solutions are overall satisfying but, when applied to real data, they can currently only verify whether the encoding aligns with the actual data and, at most, suggest alternative schedules that could have been computed. As a consequence, it is not currently possible to generate provisional schedules. Furthermore, the resulting schedules are not always robust.
  In this paper, we integrate inductive and deductive techniques for solving these issues. We first employ machine learning algorithms to predict the surgery duration, from historical data, to compute provisional schedules. Then, we consider the confidence of such predictions as an additional input to our problem and update the encoding correspondingly in order to compute more robust schedules. Results on historical data from the ASL1 Liguria in Italy confirm the viability of our integration.
  Under consideration in Theory and Practice of Logic Programming (TPLP).
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Temporal Abstractions via Variational Homomorphisms in Option-Induced Abstract MDPs</title>
<link>https://arxiv.org/abs/2507.16473</link>
<guid>https://arxiv.org/abs/2507.16473</guid>
<content:encoded><![CDATA[
<div> Variational Inference, Reinforcement Learning, Hierarchical Framework, Abstract Reasoning, Latent Space  
Summary:  
Large Language Models have demonstrated excellent reasoning ability through explicit prompting, but generating step-by-step explanations is computationally intensive. To address this, the study aims to develop a framework for efficient implicit reasoning in a latent space without generating text for each step. The proposed approach involves modeling latent thoughts as temporally-extended abstract actions or options within a hierarchical reinforcement learning setup. The Variational Markovian Option Critic algorithm, using variational inference in the HiT-MDP framework, is introduced to learn a diverse set of options as latent embeddings. The theory of continuous MDP homomorphisms is extended to establish the use of options as an abstract reasoning space. A cold-start procedure utilizes supervised fine-tuning data to distill human reasoning demonstrations into the latent option space, providing a robust initialization for the model's reasoning capabilities. Experimental results validate the effectiveness of the approach on logical reasoning benchmarks and locomotion tasks, showcasing its principled method for learning abstract skills in language and control.<br /><br />Summary: <div>
arXiv:2507.16473v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable reasoning ability through explicit Chain-of-Thought (CoT) prompting, but generating these step-by-step textual explanations is computationally expensive and slow. To overcome this, we aim to develop a framework for efficient, implicit reasoning, where the model "thinks" in a latent space without generating explicit text for every step. We propose that these latent thoughts can be modeled as temporally-extended abstract actions, or options, within a hierarchical reinforcement learning framework. To effectively learn a diverse library of options as latent embeddings, we first introduce the Variational Markovian Option Critic (VMOC), an off-policy algorithm that uses variational inference within the HiT-MDP framework. To provide a rigorous foundation for using these options as an abstract reasoning space, we extend the theory of continuous MDP homomorphisms. This proves that learning a policy in the simplified, abstract latent space, for which VMOC is suited, preserves the optimality of the solution to the original, complex problem. Finally, we propose a cold-start procedure that leverages supervised fine-tuning (SFT) data to distill human reasoning demonstrations into this latent option space, providing a rich initialization for the model's reasoning capabilities. Extensive experiments demonstrate that our approach achieves strong performance on complex logical reasoning benchmarks and challenging locomotion tasks, validating our framework as a principled method for learning abstract skills for both language and control.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACT: Bridging the Gap in Code Translation through Synthetic Data Generation &amp; Adaptive Training</title>
<link>https://arxiv.org/abs/2507.16478</link>
<guid>https://arxiv.org/abs/2507.16478</guid>
<content:encoded><![CDATA[
<div> Code translation, software development, migration projects, interoperability, language models

Summary:
Auto-Train for Code Translation (ACT) is introduced as a framework to enhance code translation capabilities by finetuning open-source Large Language Models (LLMs). ACT utilizes a synthetic data generation module to create high-quality datasets, incorporating unit tests for accuracy. An evaluation framework with execution-level checks ensures translation quality. The controller module manages the pipeline by dynamically adjusting hyperparameters and orchestrating data generation and finetuning. Results show that ACT improves the effectiveness of open-source models, providing a secure alternative for businesses and developers. The data generation pipeline applied to industry-scale projects accelerates developer productivity. <br /><br />Summary: <div>
arXiv:2507.16478v1 Announce Type: new 
Abstract: Code translation is a crucial process in software development and migration projects, enabling interoperability between different programming languages and enhancing software adaptability and thus longevity. Traditional automated translation methods rely heavily on handcrafted transformation rules, which often lack flexibility and scalability. Meanwhile, advanced language models present promising alternatives but are often limited by proprietary, API-based implementations that raise concerns over data security and reliance. In this paper, we present Auto-Train for Code Translation (ACT), an innovative framework that aims to improve code translation capabilities by enabling in-house finetuning of open-source Large Language Models (LLMs). ACT's automated pipeline significantly boosts the performance of these models, narrowing the gap between open-source accessibility and the high performance of closed-source solutions. Central to ACT is its synthetic data generation module, which builds extensive, high-quality datasets from initial code samples, incorporating unit tests to ensure functional accuracy and diversity. ACT's evaluation framework incorporates execution-level checks, offering a comprehensive assessment of translation quality. A key feature in ACT is its controller module, which manages the entire pipeline by dynamically adjusting hyperparameters, orchestrating iterative data generation, and finetuning based on real-time evaluations. This enables ACT to intelligently optimize when to continue training, generate additional targeted training data, or stop the process. Our results demonstrate that ACT consistently enhances the effectiveness of open-source models, offering businesses and developers a secure and reliable alternative. Additionally, applying our data generation pipeline to industry-scale migration projects has led to a notable increase in developer acceleration.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in Real-World Applications</title>
<link>https://arxiv.org/abs/2507.16507</link>
<guid>https://arxiv.org/abs/2507.16507</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph, Large Language Models, INRAE, Retrieval-Augmented Generation, Multi-tool Architecture

Summary:

INRAExplorer is a novel Retrieval-Augmented Generation (RAG) system designed to enhance knowledge interaction in specialized fields, specifically focusing on exploring the scientific data of INRAE. It utilizes a Large Language Model-based agent with a multi-tool architecture to dynamically engage with a knowledge base derived from INRAE publications. This allows the system to conduct targeted queries, retrieve comprehensive datasets, perform multi-hop reasoning, and deliver structured answers. INRAExplorer addresses limitations of traditional RAG systems by providing more in-depth and nuanced responses to complex queries, making it a valuable tool for knowledge-intensive domains. This agentic system exemplifies the potential for leveraging advanced AI technologies to improve knowledge exploration and interaction in specialized fields. 

Summary: <div>
arXiv:2507.16507v1 Announce Type: new 
Abstract: Conventional Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) but often fall short on complex queries, delivering limited, extractive answers and struggling with multiple targeted retrievals or navigating intricate entity relationships. This is a critical gap in knowledge-intensive domains. We introduce INRAExplorer, an agentic RAG system for exploring the scientific data of INRAE (France's National Research Institute for Agriculture, Food and Environment). INRAExplorer employs an LLM-based agent with a multi-tool architecture to dynamically engage a rich knowledge base, through a comprehensive knowledge graph derived from open access INRAE publications. This design empowers INRAExplorer to conduct iterative, targeted queries, retrieve exhaustive datasets (e.g., all publications by an author), perform multi-hop reasoning, and deliver structured, comprehensive answers. INRAExplorer serves as a concrete illustration of enhancing knowledge interaction in specialized fields.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report</title>
<link>https://arxiv.org/abs/2507.16534</link>
<guid>https://arxiv.org/abs/2507.16534</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, frontier risks, E-T-C analysis, risk zones, collective action

Summary: 
The report assesses the frontier risks posed by rapidly advancing artificial intelligence (AI) models using the E-T-C analysis framework. Critical risks in seven areas are identified including cyber offense, biological and chemical risks, persuasion and manipulation, uncontrolled autonomous AI R&amp;D, strategic deception and scheming, self-replication, and collusion. The risks are evaluated using "red lines" and "yellow lines" to define risk zones: green, yellow, and red. Experimental results show that recent frontier AI models mostly reside in green and yellow zones without crossing red lines. Specifically, models do not cross the yellow line for cyber offense or uncontrolled AI R&amp;D risks. However, persuasion and manipulation pose a challenge with most models in the yellow zone due to their influence on humans. Collective action is urged to address and mitigate these challenges. 

<br /><br />Summary: <div>
arXiv:2507.16534v1 Announce Type: new 
Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, this report presents a comprehensive assessment of their frontier risks. Drawing on the E-T-C analysis (deployment environment, threat source, enabling capability) from the Frontier AI Risk Management Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks in seven areas: cyber offense, biological and chemical risks, persuasion and manipulation, uncontrolled autonomous AI R\&amp;D, strategic deception and scheming, self-replication, and collusion. Guided by the "AI-$45^\circ$ Law," we evaluate these risks using "red lines" (intolerable thresholds) and "yellow lines" (early warning indicators) to define risk zones: green (manageable risk for routine deployment and continuous monitoring), yellow (requiring strengthened mitigations and controlled deployment), and red (necessitating suspension of development and/or deployment). Experimental results show that all recent frontier AI models reside in green and yellow zones, without crossing red lines. Specifically, no evaluated models cross the yellow line for cyber offense or uncontrolled AI R\&amp;D risks. For self-replication, and strategic deception and scheming, most models remain in the green zone, except for certain reasoning models in the yellow zone. In persuasion and manipulation, most models are in the yellow zone due to their effective influence on humans. For biological and chemical risks, we are unable to rule out the possibility of most models residing in the yellow zone, although detailed threat modeling and in-depth assessment are required to make further claims. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Multi-Agent Action Masked Deep Reinforcement Learning for General Industrial Assembly Lines Balancing Problems</title>
<link>https://arxiv.org/abs/2507.16635</link>
<guid>https://arxiv.org/abs/2507.16635</guid>
<content:encoded><![CDATA[
<div> Keywords: Industrial assembly lines, Markov Decision Process, Deep Reinforcement Learning, Action-masking, Multi-agent approach

Summary: 
The article addresses the challenge of efficiently planning activities in industrial assembly lines. It introduces a novel mathematical model based on a Markov Decision Process, allowing for training Deep Reinforcement Learning agents to optimize task and resource scheduling. Two innovative tools, action-masking, and a multi-agent approach, are proposed to enhance the efficiency of agent training. The multi-agent approach involves having individual agents manage each workstation, reducing state and action spaces. A centralized training framework with decentralized execution is adopted, enabling scalable learning architecture for optimizing assembly lines. The agents learn offline and provide real-time solutions during operations using a neural network. Numerical simulations show that the proposed scheme converges faster to the optimal solution compared to a model-based approach. <div>
arXiv:2507.16635v1 Announce Type: new 
Abstract: Efficient planning of activities is essential for modern industrial assembly lines to uphold manufacturing standards, prevent project constraint violations, and achieve cost-effective operations. While exact solutions to such challenges can be obtained through Integer Programming (IP), the dependence of the search space on input parameters often makes IP computationally infeasible for large-scale scenarios. Heuristic methods, such as Genetic Algorithms, can also be applied, but they frequently produce suboptimal solutions in extensive cases. This paper introduces a novel mathematical model of a generic industrial assembly line formulated as a Markov Decision Process (MDP), without imposing assumptions on the type of assembly line a notable distinction from most existing models. The proposed model is employed to create a virtual environment for training Deep Reinforcement Learning (DRL) agents to optimize task and resource scheduling. To enhance the efficiency of agent training, the paper proposes two innovative tools. The first is an action-masking technique, which ensures the agent selects only feasible actions, thereby reducing training time. The second is a multi-agent approach, where each workstation is managed by an individual agent, as a result, the state and action spaces were reduced. A centralized training framework with decentralized execution is adopted, offering a scalable learning architecture for optimizing industrial assembly lines. This framework allows the agents to learn offline and subsequently provide real-time solutions during operations by leveraging a neural network that maps the current factory state to the optimal action. The effectiveness of the proposed scheme is validated through numerical simulations, demonstrating significantly faster convergence to the optimal solution compared to a comparable model-based approach.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Inventory Strategies using Deep Reinforcement Learning for Dynamic Agri-Food Supply Chains</title>
<link>https://arxiv.org/abs/2507.16670</link>
<guid>https://arxiv.org/abs/2507.16670</guid>
<content:encoded><![CDATA[
<div> Deep Reinforcement Learning, inventory management, agri-food products, uncertainties, supply chain collaboration

Summary: 
This study introduces a novel Deep Reinforcement Learning (DRL) algorithm for inventory management of agri-food products. The algorithm combines value- and policy-based DRL approaches to optimize inventory replenishment under demand and lead time uncertainties. By aligning stakeholders' interests and objectives, the algorithm maximizes overall profit along the supply chain while considering perishability and uncertainty. The algorithm effectively addresses inventory optimization challenges by selecting optimal order quantities with a continuous action space. Empirical data from a fresh agricultural products supply chain validates the algorithm's improved performance under stochastic demand patterns and lead time scenarios. The research findings offer managerial insights for policymakers to enhance the inventory management of agricultural products under uncertainty. 

<br /><br />Summary: <div>
arXiv:2507.16670v1 Announce Type: new 
Abstract: Agricultural products are often subject to seasonal fluctuations in production and demand. Predicting and managing inventory levels in response to these variations can be challenging, leading to either excess inventory or stockouts. Additionally, the coordination among stakeholders at various level of food supply chain is not considered in the existing body of literature. To bridge these research gaps, this study focuses on inventory management of agri-food products under demand and lead time uncertainties. By implementing effective inventory replenishment policy results in maximize the overall profit throughout the supply chain. However, the complexity of the problem increases due to these uncertainties and shelf-life of the product, that makes challenging to implement traditional approaches to generate optimal set of solutions. Thus, the current study propose a novel Deep Reinforcement Learning (DRL) algorithm that combines the benefits of both value- and policy-based DRL approaches for inventory optimization under uncertainties. The proposed algorithm can incentivize collaboration among stakeholders by aligning their interests and objectives through shared optimization goal of maximizing profitability along the agri-food supply chain while considering perishability, and uncertainty simultaneously. By selecting optimal order quantities with continuous action space, the proposed algorithm effectively addresses the inventory optimization challenges. To rigorously evaluate this algorithm, the empirical data from fresh agricultural products supply chain inventory is considered. Experimental results corroborate the improved performance of the proposed inventory replenishment policy under stochastic demand patterns and lead time scenarios. The research findings hold managerial implications for policymakers to manage the inventory of agricultural products more effectively under uncertainty.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deliberative Searcher: Improving LLM Reliability via Reinforcement Learning with constraints</title>
<link>https://arxiv.org/abs/2507.16727</link>
<guid>https://arxiv.org/abs/2507.16727</guid>
<content:encoded><![CDATA[
<div> reliability, large language models, certainty calibration, retrieval-based search, reinforcement learning<br />
Summary:<br />
Improving the reliability of large language models is crucial for real-world deployment. The Deliberative Searcher framework integrates certainty calibration with retrieval-based search for open-domain question answering, performing multi-step reflection and verification over Wikipedia data. Trained with a reinforcement learning algorithm optimizing for accuracy under a soft reliability constraint, the method enhances alignment between model confidence and correctness, yielding more trustworthy outputs. This paper continuously updates with new empirical results. <br /> <div>
arXiv:2507.16727v1 Announce Type: new 
Abstract: Improving the reliability of large language models (LLMs) is critical for deploying them in real-world scenarios. In this paper, we propose \textbf{Deliberative Searcher}, the first framework to integrate certainty calibration with retrieval-based search for open-domain question answering. The agent performs multi-step reflection and verification over Wikipedia data and is trained with a reinforcement learning algorithm that optimizes for accuracy under a soft reliability constraint. Empirical results show that proposed method improves alignment between model confidence and correctness, leading to more trustworthy outputs. This paper will be continuously updated.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding</title>
<link>https://arxiv.org/abs/2507.16768</link>
<guid>https://arxiv.org/abs/2507.16768</guid>
<content:encoded><![CDATA[
<div> Keywords: structured decoding, large language models, output formats, grammar snippets, wgrammar

Summary:
wgrammar is a new decoding approach for large language models that enables the generation of output in specific formats required by downstream systems, such as HTML or JSON. The proposed method decomposes constraints into static and dynamic components, allowing for faster inference by precompiling static structures offline and instantiating dynamic arguments at runtime using grammar snippets. In contrast to existing approaches that rely on pushdown automata, wgrammar uses a set of operators to model regular formats, resulting in lower transition latency. By integrating domain-aware simplification, constraint decomposition, and mask caching, wgrammar achieves significant speedups, with up to 250 times faster inference compared to current systems. The source code for wgrammar is available on GitHub.
<br /><br />Summary: <div>
arXiv:2507.16768v1 Announce Type: new 
Abstract: Structured decoding enables large language models (LLMs) to generate outputs in formats required by downstream systems, such as HTML or JSON. However, existing methods suffer from efficiency bottlenecks due to grammar compilation, state tracking, and mask creation. We observe that many real-world tasks embed strong prior knowledge about output structure. Leveraging this, we propose a decomposition of constraints into static and dynamic components -- precompiling static structures offline and instantiating dynamic arguments at runtime using grammar snippets. Instead of relying on pushdown automata, we employ a compositional set of operators to model regular formats, achieving lower transition latency. We introduce wgrammar, a lightweight decoding engine that integrates domain-aware simplification, constraint decomposition, and mask caching, achieving up to 250x speedup over existing systems. wgrammar's source code is publicly available at https://github.com/wrran/wgrammar.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatChecker: A Framework for Dialogue System Testing and Evaluation Through Non-cooperative User Simulation</title>
<link>https://arxiv.org/abs/2507.16792</link>
<guid>https://arxiv.org/abs/2507.16792</guid>
<content:encoded><![CDATA[
<div> keywords: dialogue systems, language models, evaluation, testing, automated<br />
Summary: <br /><br />Dialogue systems today rely heavily on large language models (LLMs) but often involve multiple components beyond just LLM interaction, making overall system testing crucial. Existing focus on turn-level analysis overlooks dialogue-level assessments. ChatChecker, a new framework, addresses this challenge by using LLMs to simulate diverse user interactions and evaluate system quality. It streamlines setup, is generalizable, and doesn't require reference dialogues or knowledge of the target system's implementation. By incorporating an error taxonomy in prompts and introducing a non-cooperative user simulator with challenging personas, ChatChecker improves breakdown detection and uncovers system weaknesses effectively. This approach enhances the thorough and scalable testing of dialogue systems, aiding researchers and practitioners in developing robust systems efficiently. <br /><br />Summary: <div>
arXiv:2507.16792v1 Announce Type: new 
Abstract: While modern dialogue systems heavily rely on large language models (LLMs), their implementation often goes beyond pure LLM interaction. Developers integrate multiple LLMs, external tools, and databases. Therefore, assessment of the underlying LLM alone does not suffice, and the dialogue systems must be tested and evaluated as a whole. However, this remains a major challenge. With most previous work focusing on turn-level analysis, less attention has been paid to integrated dialogue-level quality assurance. To address this, we present ChatChecker, a framework for automated evaluation and testing of complex dialogue systems. ChatChecker uses LLMs to simulate diverse user interactions, identify dialogue breakdowns, and evaluate quality. Compared to previous approaches, our design reduces setup effort and is generalizable, as it does not require reference dialogues and is decoupled from the implementation of the target dialogue system. We improve breakdown detection performance over a prior LLM-based approach by including an error taxonomy in the prompt. Additionally, we propose a novel non-cooperative user simulator based on challenging personas that uncovers weaknesses in target dialogue systems more effectively. Through this, ChatChecker contributes to thorough and scalable testing. This enables both researchers and practitioners to accelerate the development of robust dialogue systems.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading with Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.16796</link>
<guid>https://arxiv.org/abs/2507.16796</guid>
<content:encoded><![CDATA[
<div> Prediction models, Peer-to-Peer energy trading, Uncertainty-aware, MARL, Deep Q-Network <br />
Summary: <br />
This paper introduces a novel framework for Peer-to-Peer (P2P) energy trading that combines uncertainty-aware prediction with multi-agent reinforcement learning (MARL). Utilizing a probabilistic transformer-based model, called Knowledge Transformer with Uncertainty (KTU), the framework explicitly quantifies prediction uncertainty crucial for decision-making in P2P energy trading. By integrating uncertainty-aware forecasts into the MARL framework, agents can optimize trading strategies with a clear understanding of risk and variability. Experimental results demonstrate cost reduction in energy purchases, increased electricity sales revenue, and a decrease in peak hour grid demand. Furthermore, these improvements are more significant when P2P trading is enabled, emphasizing the effectiveness of advanced forecasting and market mechanisms in enhancing the resilience and economic efficiency of energy communities. <div>
arXiv:2507.16796v1 Announce Type: new 
Abstract: This paper presents a novel framework for Peer-to-Peer (P2P) energy trading that integrates uncertainty-aware prediction with multi-agent reinforcement learning (MARL), addressing a critical gap in current literature. In contrast to previous works relying on deterministic forecasts, the proposed approach employs a heteroscedastic probabilistic transformer-based prediction model called Knowledge Transformer with Uncertainty (KTU) to explicitly quantify prediction uncertainty, which is essential for robust decision-making in the stochastic environment of P2P energy trading. The KTU model leverages domain-specific features and is trained with a custom loss function that ensures reliable probabilistic forecasts and confidence intervals for each prediction. Integrating these uncertainty-aware forecasts into the MARL framework enables agents to optimize trading strategies with a clear understanding of risk and variability. Experimental results show that the uncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to 5.7% without P2P trading and 3.2% with P2P trading, while increasing electricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak hour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These improvements are even more pronounced when P2P trading is enabled, highlighting the synergy between advanced forecasting and market mechanisms for resilient, economically efficient energy communities.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration</title>
<link>https://arxiv.org/abs/2507.14452</link>
<guid>https://arxiv.org/abs/2507.14452</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud registration, Gestalt principles, parallel interaction network, geometric consistency, feature attention

Summary:
The paper introduces GPI-Net, a novel approach for feature-based point cloud registration that leverages Gestalt principles to enhance the fusion of local and global information. By using an orthogonal integration strategy, redundant information is reduced to create a more compact global structure for high-quality correspondences. The Gestalt Feature Attention (GFA) block combines self-attention and cross-attention mechanisms to capture geometric features. Additionally, the Dual-path Multi-Granularity parallel interaction aggregation (DMG) block facilitates the integration of local detail information into the global structure. Experimental results demonstrate the superior performance of GPI-Net compared to existing methods on various challenging tasks. The code for GPI-Net will be made available on GitHub at https://github.com/gwk/GPI-Net. 
<br /><br />Summary: <div>
arXiv:2507.14452v1 Announce Type: cross 
Abstract: The accurate identification of high-quality correspondences is a prerequisite task in feature-based point cloud registration. However, it is extremely challenging to handle the fusion of local and global features due to feature redundancy and complex spatial relationships. Given that Gestalt principles provide key advantages in analyzing local and global relationships, we propose a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric consistency (GPI-Net) in this paper. It utilizes Gestalt principles to facilitate complementary communication between local and global information. Specifically, we introduce an orthogonal integration strategy to optimally reduce redundant information and generate a more compact global structure for high-quality correspondences. To capture geometric features in correspondences, we leverage a Gestalt Feature Attention (GFA) block through a hybrid utilization of self-attention and cross-attention mechanisms. Furthermore, to facilitate the integration of local detail information into the global structure, we design an innovative Dual-path Multi-Granularity parallel interaction aggregation (DMG) block to promote information exchange across different granularities. Extensive experiments on various challenging tasks demonstrate the superior performance of our proposed GPI-Net in comparison to existing methods. The code will be released at https://github.com/gwk/GPI-Net.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized AI-driven IoT Architecture for Privacy-Preserving and Latency-Optimized Healthcare in Pandemic and Critical Care Scenarios</title>
<link>https://arxiv.org/abs/2507.15859</link>
<guid>https://arxiv.org/abs/2507.15859</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, IoT, real-time patient monitoring, decentralized architecture, federated learning<br />
Summary:<br />
The article discusses the application of AI innovations in IoT for real-time patient monitoring in healthcare. It addresses the limitations of traditional centralized healthcare systems by presenting a decentralized IoT architecture. This architecture leverages AI technologies to improve data privacy, minimize latency, and enhance overall system performance, particularly in pandemic and critical care settings. The approach combines federated learning, blockchain, and edge computing to optimize data handling and security. Experimental results show significantly lower transaction latency, energy consumption, and data throughput compared to cloud solutions. By decentralizing the system and utilizing AI, the architecture offers a more efficient and secure healthcare monitoring solution for patients. <div>
arXiv:2507.15859v1 Announce Type: cross 
Abstract: AI Innovations in the IoT for Real-Time Patient Monitoring On one hand, the current traditional centralized healthcare architecture poses numerous issues, including data privacy, delay, and security. Here, we present an AI-enabled decentralized IoT architecture that can address such challenges during a pandemic and critical care settings. This work presents our architecture to enhance the effectiveness of the current available federated learning, blockchain, and edge computing approach, maximizing data privacy, minimizing latency, and improving other general system metrics. Experimental results demonstrate transaction latency, energy consumption, and data throughput orders of magnitude lower than competitive cloud solutions.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eSapiens's DEREK Module: Deep Extraction &amp; Reasoning Engine for Knowledge with LLMs</title>
<link>https://arxiv.org/abs/2507.15863</link>
<guid>https://arxiv.org/abs/2507.15863</guid>
<content:encoded><![CDATA[
<div> Keywords: DEREK Module, document question answering, retrieval-augmented generation, enterprise, secure

Summary: 
The article introduces the DEREK Module, a Retrieval-Augmented Generation pipeline designed for enterprise document question answering. Developed by eSapiens, the system processes heterogeneous content, indexes it in a hybrid store, and uses advanced techniques like GPT-4o and Cohere for query refinement and reranking. Through the use of a LangGraph verifier, the system ensures accurate answers by enforcing citation overlap and grounding claims. Results show improvements in recall and precision metrics, with enhanced traceability and a low rate of unsupported statements. The module operates securely within containers, enforcing strong encryption standards. Overall, the DEREK Module offers a reliable solution for high-stakes domains like legal and finance, delivering accurate and auditable document QA with minimal operational complexity. 

<br /><br />Summary: <div>
arXiv:2507.15863v1 Announce Type: cross 
Abstract: We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge) Module, a secure and scalable Retrieval-Augmented Generation pipeline designed specifically for enterprise document question answering. Designed and implemented by eSapiens, the system ingests heterogeneous content (PDF, Office, web), splits it into 1,000-token overlapping chunks, and indexes them in a hybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via combined vector+BM25 search, reranked with Cohere, and answered by an LLM using CO-STAR prompt engineering. A LangGraph verifier enforces citation overlap, regenerating answers until every claim is grounded. On four LegalBench subsets, 1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank boosts Precision@10 by approximately 7 pp; the verifier raises TRACe Utilization above 0.50 and limits unsupported statements to less than 3%. All components run in containers, enforce end-to-end TLS 1.3 and AES-256. These results demonstrate that the DEREK module delivers accurate, traceable, and production-ready document QA with minimal operational overhead. The module is designed to meet enterprise demands for secure, auditable, and context-faithful retrieval, providing a reliable baseline for high-stakes domains such as legal and finance.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDMA: Cost Effective Agent-Driven Rare Disease Discovery within Electronic Health Record Systems</title>
<link>https://arxiv.org/abs/2507.15867</link>
<guid>https://arxiv.org/abs/2507.15867</guid>
<content:encoded><![CDATA[
<div> Keywords: rare diseases, electronic health records, clinical notes, RDMA, diagnosis

Summary:<br /><br />
Rare diseases affect a significant portion of the population, but standard coding systems in electronic health records often fail to accurately capture them, resulting in vital information being buried in clinical notes. To address this issue, Rare Disease Mining Agents (RDMA) have been developed to identify patterns of rare diseases in EHR. RDMA utilizes clinical observations to connect scattered data points and suggest specific rare conditions, while also handling abbreviations, recognizing implicit disease patterns, and applying contextual reasoning locally on standard hardware. This approach improves performance by up to 30% and reduces inference costs significantly. By enhancing the ability to identify rare diseases within EHR systems, clinicians can access crucial information without the privacy risks associated with cloud services, ultimately leading to earlier diagnosis and better treatment for rare disease patients. The RDMA framework is available at https://github.com/jhnwu3/RDMA. <div>
arXiv:2507.15867v1 Announce Type: cross 
Abstract: Rare diseases affect 1 in 10 Americans, yet standard ICD coding systems fail to capture these conditions in electronic health records (EHR), leaving crucial information buried in clinical notes. Current approaches struggle with medical abbreviations, miss implicit disease mentions, raise privacy concerns with cloud processing, and lack clinical reasoning abilities. We present Rare Disease Mining Agents (RDMA), a framework that mirrors how medical experts identify rare disease patterns in EHR. RDMA connects scattered clinical observations that together suggest specific rare conditions. By handling clinical abbreviations, recognizing implicit disease patterns, and applying contextual reasoning locally on standard hardware, RDMA reduces privacy risks while improving F1 performance by upwards of 30\% and decreasing inferences costs 10-fold. This approach helps clinicians avoid the privacy risk of using cloud services while accessing key rare disease information from EHR systems, supporting earlier diagnosis for rare disease patients. Available at https://github.com/jhnwu3/RDMA.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models</title>
<link>https://arxiv.org/abs/2507.15868</link>
<guid>https://arxiv.org/abs/2507.15868</guid>
<content:encoded><![CDATA[
<div> underspecification, lexical flip, jargon inflation, large language models, model sensitivity<br />
Summary:<br />
The study examines the robustness of large language models (LLMs) in coding tasks by introducing prompt perturbations in LeetCode problems. Three types of perturbations were applied: progressive underspecification, lexical flip, and jargon inflation. The models showed over-robustness to underspecification, remaining correct even with 90% of the prompt missing. However, they exhibited low sensitivity to a single quantifier flip that changed the task, with reasoning-tuned models being even less responsive. Jargon edits yielded a moderate response rate. The findings suggest that current LLMs struggle to differentiate between harmless noise and meaningful changes in prompts, often treating both as insignificant. The study advocates for evaluation and training approaches that incentivize models to remain stable under benign noise but adapt or reject prompts when the semantics truly shift. <br />Summary: <div>
arXiv:2507.15868v1 Announce Type: cross 
Abstract: Large language models (LLMs) now write code in settings where misreading a single word can break safety or cost money, yet we still expect them to overlook stray typos. To probe where useful robustness ends and harmful insensitivity begins, we compile 50 LeetCode problems and craft three minimal prompt perturbations that should vary in importance: (i) progressive underspecification deleting 10 % of words per step; (ii) lexical flip swapping a pivotal quantifier ("max" to "min"); and (iii) jargon inflation replacing a common noun with an obscure technical synonym. Six frontier models, including three "reasoning-tuned" versions, solve each mutated prompt, and their Python outputs are checked against the original test suites to reveal whether they reused the baseline solution or adapted. Among 11 853 generations we observe a sharp double asymmetry. Models remain correct in 85 % of cases even after 90 % of the prompt is missing, showing over-robustness to underspecification, yet only 54 % react to a single quantifier flip that reverses the task, with reasoning-tuned variants even less sensitive than their bases. Jargon edits lie in between, passing through 56 %. Current LLMs thus blur the line between harmless noise and meaning - changing edits, often treating both as ignorable. Masking salient anchors such as function names can force re - evaluation. We advocate evaluation and training protocols that reward differential sensitivity: stay steady under benign noise but adapt - or refuse - when semantics truly change.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Salience Adjustment for Context-Based Emotion Recognition</title>
<link>https://arxiv.org/abs/2507.15878</link>
<guid>https://arxiv.org/abs/2507.15878</guid>
<content:encoded><![CDATA[
<div> Keywords: emotion recognition, dynamic social contexts, facial expressions, situational cues, Bayesian Cue Integration

Summary:
In the study, a salience-adjusted framework for context-aware emotion recognition using Bayesian Cue Integration (BCI) and Visual-Language Models (VLMs) is proposed. The framework dynamically weights facial and contextual information based on the expressivity of facial cues. The evaluation conducted in prisoner's dilemma scenarios, aimed at eliciting emotional reactions, shows that incorporating salience adjustment improves emotion recognition performance. This approach highlights the importance of understanding the interaction between facial expressions and situational cues in dynamic social contexts. The findings suggest potential applications in enhancing emotion recognition in broader social contexts and multimodal scenarios. Future research directions may involve extending the framework to various social settings and exploring its effectiveness in diverse emotion recognition tasks. 

<br /><br />Summary: <div>
arXiv:2507.15878v1 Announce Type: cross 
Abstract: Emotion recognition in dynamic social contexts requires an understanding of the complex interaction between facial expressions and situational cues. This paper presents a salience-adjusted framework for context-aware emotion recognition with Bayesian Cue Integration (BCI) and Visual-Language Models (VLMs) to dynamically weight facial and contextual information based on the expressivity of facial cues. We evaluate this approach using human annotations and automatic emotion recognition systems in prisoner's dilemma scenarios, which are designed to evoke emotional reactions. Our findings demonstrate that incorporating salience adjustment enhances emotion recognition performance, offering promising directions for future research to extend this framework to broader social contexts and multimodal applications.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark</title>
<link>https://arxiv.org/abs/2507.15882</link>
<guid>https://arxiv.org/abs/2507.15882</guid>
<content:encoded><![CDATA[
<div> benchmark, Vision Language Models, long documents, multimodal, Document Haystack

Summary:
Document Haystack is a new benchmark designed to evaluate Vision Language Models' performance on long, visually complex documents. It includes 400 document variants with 8,250 questions, ranging from 5 to 200 pages. The benchmark strategically inserts pure text or multimodal text+image "needles" at different depths within the documents to challenge VLMs' retrieval capabilities. An objective, automated evaluation framework supports the dataset. Results from prominent VLMs are presented, and potential research avenues in this area are discussed. <br /><br />Summary: <div>
arXiv:2507.15882v1 Announce Type: cross 
Abstract: The proliferation of multimodal Large Language Models has significantly advanced the ability to analyze and understand complex data inputs from different modalities. However, the processing of long documents remains under-explored, largely due to a lack of suitable benchmarks. To address this, we introduce Document Haystack, a comprehensive benchmark designed to evaluate the performance of Vision Language Models (VLMs) on long, visually complex documents. Document Haystack features documents ranging from 5 to 200 pages and strategically inserts pure text or multimodal text+image "needles" at various depths within the documents to challenge VLMs' retrieval capabilities. Comprising 400 document variants and a total of 8,250 questions, it is supported by an objective, automated evaluation framework. We detail the construction and characteristics of the Document Haystack dataset, present results from prominent VLMs and discuss potential research avenues in this area.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Cost-Constrained Runtime Monitors for AI Safety</title>
<link>https://arxiv.org/abs/2507.15886</link>
<guid>https://arxiv.org/abs/2507.15886</guid>
<content:encoded><![CDATA[
<div> monitoring, AI, runtime, protocol, safety intervention

Summary:
The paper focuses on combining multiple runtime monitors into a single monitoring protocol to maximize the recall rate of safety interventions on misaligned outputs while adhering to a budget constraint. An algorithm is developed to efficiently determine when and which monitors to call, allocating safety interventions based on the Neyman-Pearson lemma. By strategically trading off spending on monitors against spending on interventions using likelihood ratios, the recall rate is more than doubled compared to a naive baseline in a code review setting. The study also demonstrates that combining two monitors can be more effective than using either monitor alone. This framework offers a systematic approach for combining existing monitors to detect harmful behavior in cost-sensitive scenarios. 

<br /><br />Summary: <div>
arXiv:2507.15886v1 Announce Type: cross 
Abstract: Monitoring AIs at runtime can help us detect and stop harmful actions. In this paper, we study how to combine multiple runtime monitors into a single monitoring protocol. The protocol's objective is to maximize the probability of applying a safety intervention on misaligned outputs (i.e., maximize recall). Since running monitors and applying safety interventions are costly, the protocol also needs to adhere to an average-case budget constraint. Taking the monitors' performance and cost as given, we develop an algorithm to find the most efficient protocol. The algorithm exhaustively searches over when and which monitors to call, and allocates safety interventions based on the Neyman-Pearson lemma. By focusing on likelihood ratios and strategically trading off spending on monitors against spending on interventions, we more than double our recall rate compared to a naive baseline in a code review setting. We also show that combining two monitors can Pareto dominate using either monitor alone. Our framework provides a principled methodology for combining existing monitors to detect undesirable behavior in cost-sensitive settings.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?</title>
<link>https://arxiv.org/abs/2507.15887</link>
<guid>https://arxiv.org/abs/2507.15887</guid>
<content:encoded><![CDATA[
arXiv:2507.15887v1 Announce Type: cross 
Abstract: Despite progress in language model (LM) capabilities, evaluations have thus far focused on models' performance on tasks that humans have previously solved, including in programming (Jimenez et al., 2024) and mathematics (Glazer et al., 2024). We therefore propose testing models' ability to design and implement algorithms in an open-ended benchmark: We task LMs with writing code that efficiently solves computationally challenging problems in computer science, physics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks collected from domain experts and a framework for validating and timing LM-synthesized solution code, which is compared to reference implementations from popular open-source packages. In addition, we develop a baseline LM agent, AlgoTuner, and evaluate its performance across a suite of frontier models. AlgoTuner achieves an average 1.72x speedup against our reference solvers, which use libraries such as SciPy, sk-learn and CVXPY. However, we find that current models fail to discover algorithmic innovations, instead preferring surface-level optimizations. We hope that AlgoTune catalyzes the development of LM agents exhibiting creative problem solving beyond state-of-the-art human performance.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing</title>
<link>https://arxiv.org/abs/2507.15889</link>
<guid>https://arxiv.org/abs/2507.15889</guid>
<content:encoded><![CDATA[
arXiv:2507.15889v1 Announce Type: cross 
Abstract: Language models for program synthesis are usually trained and evaluated on programming competition datasets (MBPP, APPS). However, these datasets are limited in size and quality, while these language models are extremely data hungry. Additionally, the language models have a misaligned program synthesis process compared to humans. While humans iteratively develop code with the help of a compiler, most program synthesis models currently produce code in one go. To solve these issues, we introduce a bootstrapping algorithm for program synthesis, that supports teaching models how to repair. We show that bootstrapping consistently outperforms regular fine-tuning. Compared to other work, our bootstrapped model performs on par with fine-tuned models that are 68\% larger. Notably, bootstrapping with repairing also improves non-repairing performance compared to regular bootstrapping during inference. However, on our models, repairing during inference is likely inferior to simply sampling the same number of solutions. Furthermore, we find that there are issues with the example test cases in the training portion of the APPS dataset that are valuable to the community, as many repairing and reinforcement learning methods rely on them.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systole-Conditioned Generative Cardiac Motion</title>
<link>https://arxiv.org/abs/2507.15894</link>
<guid>https://arxiv.org/abs/2507.15894</guid>
<content:encoded><![CDATA[
arXiv:2507.15894v1 Announce Type: cross 
Abstract: Accurate motion estimation in cardiac computed tomography (CT) imaging is critical for assessing cardiac function and surgical planning. Data-driven methods have become the standard approach for dense motion estimation, but they rely on vast amounts of labeled data with dense ground-truth (GT) motion annotations, which are often unfeasible to obtain. To address this limitation, we present a novel approach that synthesizes realistically looking pairs of cardiac CT frames enriched with dense 3D flow field annotations.
  Our method leverages a conditional Variational Autoencoder (CVAE), which incorporates a novel multi-scale feature conditioning mechanism and is trained to generate 3D flow fields conditioned on a single CT frame. By applying the generated flow field to warp the given frame, we create pairs of frames that simulate realistic myocardium deformations across the cardiac cycle. These pairs serve as fully annotated data samples, providing optical flow GT annotations. Our data generation pipeline could enable the training and validation of more complex and accurate myocardium motion models, allowing for substantially reducing reliance on manual annotations.
  Our code, along with animated generated samples and additional material, is available on our project page: https://shaharzuler.github.io/GenerativeCardiacMotion_Page.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReDi: Rectified Discrete Flow</title>
<link>https://arxiv.org/abs/2507.15897</link>
<guid>https://arxiv.org/abs/2507.15897</guid>
<content:encoded><![CDATA[
arXiv:2507.15897v1 Announce Type: cross 
Abstract: Discrete Flow-based Models (DFMs) are powerful generative models for high-quality discrete data but typically suffer from slow sampling speeds due to their reliance on iterative decoding processes. This reliance on a multi-step process originates from the factorization approximation of DFMs, which is necessary for handling high-dimensional data. In this paper, we rigorously characterize the approximation error from factorization using Conditional Total Correlation (TC), which depends on the coupling. To reduce the Conditional TC and enable efficient few-step generation, we propose Rectified Discrete Flow (ReDi), a novel iterative method that reduces factorization error by rectifying the coupling between source and target distributions. We theoretically prove that each ReDi step guarantees a monotonic decreasing Conditional TC, ensuring its convergence. Empirically, ReDi significantly reduces Conditional TC and enables few-step generation. Moreover, we demonstrate that the rectified couplings are well-suited for training efficient one-step models on image generation. ReDi offers a simple and theoretically grounded approach for tackling the few-step challenge, providing a new perspective on efficient discrete data synthesis. Code is available at https://github.com/Ugness/ReDi_discrete
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Model for Disentangling Galaxy Photometric Parameters</title>
<link>https://arxiv.org/abs/2507.15898</link>
<guid>https://arxiv.org/abs/2507.15898</guid>
<content:encoded><![CDATA[
arXiv:2507.15898v1 Announce Type: cross 
Abstract: Ongoing and future photometric surveys will produce unprecedented volumes of galaxy images, necessitating robust, efficient methods for deriving galaxy morphological parameters at scale. Traditional approaches, such as parametric light-profile fitting, offer valuable insights but become computationally prohibitive when applied to billions of sources. In this work, we propose a Conditional AutoEncoder (CAE) framework to simultaneously model and characterize galaxy morphology. Our CAE is trained on a suite of realistic mock galaxy images generated via GalSim, encompassing a broad range of galaxy types, photometric parameters (e.g., flux, half-light radius, Sersic index, ellipticity), and observational conditions. By encoding each galaxy image into a low-dimensional latent representation conditioned on key parameters, our model effectively recovers these morphological features in a disentangled manner, while also reconstructing the original image. The results demonstrate that the CAE approach can accurately and efficiently infer complex structural properties, offering a powerful alternative to existing methods.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Mitigation of Hallucination for LLM-empowered Agents: Progressive Generalization Bound Exploration and Watchdog Monitor</title>
<link>https://arxiv.org/abs/2507.15903</link>
<guid>https://arxiv.org/abs/2507.15903</guid>
<content:encoded><![CDATA[
arXiv:2507.15903v1 Announce Type: cross 
Abstract: Empowered by large language models (LLMs), intelligent agents have become a popular paradigm for interacting with open environments to facilitate AI deployment. However, hallucinations generated by LLMs-where outputs are inconsistent with facts-pose a significant challenge, undermining the credibility of intelligent agents. Only if hallucinations can be mitigated, the intelligent agents can be used in real-world without any catastrophic risk. Therefore, effective detection and mitigation of hallucinations are crucial to ensure the dependability of agents. Unfortunately, the related approaches either depend on white-box access to LLMs or fail to accurately identify hallucinations. To address the challenge posed by hallucinations of intelligent agents, we present HalMit, a novel black-box watchdog framework that models the generalization bound of LLM-empowered agents and thus detect hallucinations without requiring internal knowledge of the LLM's architecture. Specifically, a probabilistic fractal sampling technique is proposed to generate a sufficient number of queries to trigger the incredible responses in parallel, efficiently identifying the generalization bound of the target agent. Experimental evaluations demonstrate that HalMit significantly outperforms existing approaches in hallucination monitoring. Its black-box nature and superior performance make HalMit a promising solution for enhancing the dependability of LLM-powered systems.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models and Transformers for Anomaly Detection: A Survey</title>
<link>https://arxiv.org/abs/2507.15905</link>
<guid>https://arxiv.org/abs/2507.15905</guid>
<content:encoded><![CDATA[
arXiv:2507.15905v1 Announce Type: cross 
Abstract: In line with the development of deep learning, this survey examines the transformative role of Transformers and foundation models in advancing visual anomaly detection (VAD). We explore how these architectures, with their global receptive fields and adaptability, address challenges such as long-range dependency modeling, contextual modeling and data scarcity. The survey categorizes VAD methods into reconstruction-based, feature-based and zero/few-shot approaches, highlighting the paradigm shift brought about by foundation models. By integrating attention mechanisms and leveraging large-scale pre-training, Transformers and foundation models enable more robust, interpretable, and scalable anomaly detection solutions. This work provides a comprehensive review of state-of-the-art techniques, their strengths, limitations, and emerging trends in leveraging these architectures for VAD.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable, Uncertainty-Aware Alignment</title>
<link>https://arxiv.org/abs/2507.15906</link>
<guid>https://arxiv.org/abs/2507.15906</guid>
<content:encoded><![CDATA[
arXiv:2507.15906v1 Announce Type: cross 
Abstract: Alignment of large language models (LLMs) typically involves training a reward model on preference data, followed by policy optimization with respect to the reward model. However, optimizing policies with respect to a single reward model estimate can render it vulnerable to inaccuracies in the reward model. We empirically study the variability of reward model training on open-source benchmarks. We observe that independently trained reward models on the same preference dataset can exhibit substantial disagreement, highlighting the instability of current alignment strategies. Employing a theoretical model, we demonstrate that variability in reward model estimation can cause overfitting, leading to the risk of performance degradation. To mitigate this risk, we propose a variance-aware policy optimization framework for preference-based alignment. The key ingredient of the framework is a new policy regularizer that incorporates reward model variance estimates. We show that variance-aware policy optimization provably reduces the risk of outputting a worse policy than the default. Experiments across diverse LLM and reward model configurations confirm that our approach yields more stable and robust alignment than the standard (variance-unaware) pipeline.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Turing Test: A Framework for Detecting and Mitigating Undetectable AI</title>
<link>https://arxiv.org/abs/2507.15907</link>
<guid>https://arxiv.org/abs/2507.15907</guid>
<content:encoded><![CDATA[
arXiv:2507.15907v1 Announce Type: cross 
Abstract: In this short note, we propose a unified framework that bridges three areas: (1) a flipped perspective on the Turing Test, the "dual Turing test", in which a human judge's goal is to identify an AI rather than reward a machine for deception; (2) a formal adversarial classification game with explicit quality constraints and worst-case guarantees; and (3) a reinforcement learning (RL) alignment pipeline that uses an undetectability detector and a set of quality related components in its reward model. We review historical precedents, from inverted and meta-Turing variants to modern supervised reverse-Turing classifiers, and highlight the novelty of combining quality thresholds, phased difficulty levels, and minimax bounds. We then formalize the dual test: define the judge's task over N independent rounds with fresh prompts drawn from a prompt space Q, introduce a quality function Q and parameters tau and delta, and cast the interaction as a two-player zero-sum game over the adversary's feasible strategy set M. Next, we map this minimax game onto an RL-HF style alignment loop, in which an undetectability detector D provides negative reward for stealthy outputs, balanced by a quality proxy that preserves fluency. Throughout, we include detailed explanations of each component notation, the meaning of inner minimization over sequences, phased tests, and iterative adversarial training and conclude with a suggestion for a couple of immediate actions.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization-Aware Neuromorphic Architecture for Efficient Skin Disease Classification on Resource-Constrained Devices</title>
<link>https://arxiv.org/abs/2507.15958</link>
<guid>https://arxiv.org/abs/2507.15958</guid>
<content:encoded><![CDATA[
arXiv:2507.15958v1 Announce Type: cross 
Abstract: Accurate and efficient skin lesion classification on edge devices is critical for accessible dermatological care but remains challenging due to computational, energy, and privacy constraints. We introduce QANA, a novel quantization-aware neuromorphic architecture for incremental skin lesion classification on resource-limited hardware. QANA effectively integrates ghost modules, efficient channel attention, and squeeze-and-excitation blocks for robust feature representation with low-latency and energy-efficient inference. Its quantization-aware head and spike-compatible transformations enable seamless conversion to spiking neural networks (SNNs) and deployment on neuromorphic platforms. Evaluation on the large-scale HAM10000 benchmark and a real-world clinical dataset shows that QANA achieves 91.6\% Top-1 accuracy and 82.4\% macro F1 on HAM10000, and 90.8\% / 81.7\% on the clinical dataset, significantly outperforming state-of-the-art CNN-to-SNN models under fair comparison. Deployed on BrainChip Akida hardware, QANA achieves 1.5\,ms inference latency and 1.7\,mJ energy per image, reducing inference latency and energy use by over 94.6\%/98.6\% compared to GPU-based CNNs surpassing state-of-the-art CNN-to-SNN conversion baselines. These results demonstrate the effectiveness of QANA for accurate, real-time, and privacy-sensitive medical analysis in edge environments.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Face Quality Assessment Framework to Improve Face Verification Performance in Real-Time Screening Applications</title>
<link>https://arxiv.org/abs/2507.15961</link>
<guid>https://arxiv.org/abs/2507.15961</guid>
<content:encoded><![CDATA[
arXiv:2507.15961v1 Announce Type: cross 
Abstract: Face image quality plays a critical role in determining the accuracy and reliability of face verification systems, particularly in real-time screening applications such as surveillance, identity verification, and access control. Low-quality face images, often caused by factors such as motion blur, poor lighting conditions, occlusions, and extreme pose variations, significantly degrade the performance of face recognition models, leading to higher false rejection and false acceptance rates. In this work, we propose a lightweight yet effective framework for automatic face quality assessment, which aims to pre-filter low-quality face images before they are passed to the verification pipeline. Our approach utilises normalised facial landmarks in conjunction with a Random Forest Regression classifier to assess image quality, achieving an accuracy of 96.67\%. By integrating this quality assessment module into the face verification process, we observe a substantial improvement in performance, including a comfortable 99.7\% reduction in the false rejection rate and enhanced cosine similarity scores when paired with the ArcFace face verification model. To validate our approach, we have conducted experiments on a real-world dataset collected comprising over 600 subjects captured from CCTV footage in unconstrained environments within Dubai Police. Our results demonstrate that the proposed framework effectively mitigates the impact of poor-quality face images, outperforming existing face quality assessment techniques while maintaining computational efficiency. Moreover, the framework specifically addresses two critical challenges in real-time screening: variations in face resolution and pose deviations, both of which are prevalent in practical surveillance scenarios.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear Framework for Speech Bandwidth Extension</title>
<link>https://arxiv.org/abs/2507.15970</link>
<guid>https://arxiv.org/abs/2507.15970</guid>
<content:encoded><![CDATA[
arXiv:2507.15970v1 Announce Type: cross 
Abstract: Recovering high-frequency components lost to bandwidth constraints is crucial for applications ranging from telecommunications to high-fidelity audio on limited resources. We introduce NDSI-BWE, a new adversarial Band Width Extension (BWE) framework that leverage four new discriminators inspired by nonlinear dynamical system to capture diverse temporal behaviors: a Multi-Resolution Lyapunov Discriminator (MRLD) for determining sensitivity to initial conditions by capturing deterministic chaos, a Multi-Scale Recurrence Discriminator (MS-RD) for self-similar recurrence dynamics, a Multi-Scale Detrended Fractal Analysis Discriminator (MSDFA) for long range slow variant scale invariant relationship, a Multi-Resolution Poincar\'e Plot Discriminator (MR-PPD) for capturing hidden latent space relationship, a Multi-Period Discriminator (MPD) for cyclical patterns, a Multi-Resolution Amplitude Discriminator (MRAD) and Multi-Resolution Phase Discriminator (MRPD) for capturing intricate amplitude-phase transition statistics. By using depth-wise convolution at the core of the convolutional block with in each discriminators, NDSI-BWE attains an eight-times parameter reduction. These seven discriminators guide a complex-valued ConformerNeXt based genetor with a dual stream Lattice-Net based architecture for simultaneous refinement of magnitude and phase. The genertor leverage the transformer based conformer's global dependency modeling and ConvNeXt block's local temporal modeling capability. Across six objective evaluation metrics and subjective based texts comprises of five human judges, NDSI-BWE establishes a new SoTA in BWE.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the transferability of Sparse Autoencoders for interpreting compressed models</title>
<link>https://arxiv.org/abs/2507.15977</link>
<guid>https://arxiv.org/abs/2507.15977</guid>
<content:encoded><![CDATA[
arXiv:2507.15977v1 Announce Type: cross 
Abstract: Modern LLMs face inference efficiency challenges due to their scale. To address this, many compression methods have been proposed, such as pruning and quantization. However, the effect of compression on a model's interpretability remains elusive. While several model interpretation approaches exist, such as circuit discovery, Sparse Autoencoders (SAEs) have proven particularly effective in decomposing a model's activation space into its feature basis. In this work, we explore the differences in SAEs for the original and compressed models. We find that SAEs trained on the original model can interpret the compressed model albeit with slight performance degradation compared to the trained SAE on the compressed model. Furthermore, simply pruning the original SAE itself achieves performance comparable to training a new SAE on the pruned model. This finding enables us to mitigate the extensive training costs of SAEs.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars</title>
<link>https://arxiv.org/abs/2507.15979</link>
<guid>https://arxiv.org/abs/2507.15979</guid>
<content:encoded><![CDATA[
arXiv:2507.15979v1 Announce Type: cross 
Abstract: We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs animatable 3D human avatars from a single image. This is achieved by leveraging multi-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of 3D Gaussians. Given an image, we first dream plausible multi-views using a video diffusion model, capturing rich geometric and appearance details. These views are then lifted into unstructured 3D Gaussians. To enable animation, we propose a transformer-based encoder that models global spatial relationships and projects these Gaussians into a structured latent representation aligned with the UV space of a parametric body model. This latent code is decoded into UV-space Gaussians that can be animated via body-driven deformation and rendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV manifold, our method ensures consistency during animation while preserving fine visual details. DLA enables real-time rendering and intuitive editing without requiring post-processing. Our method outperforms state-of-the-art approaches on ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric accuracy. By combining the generative strengths of video diffusion models with a pose-aware UV-space Gaussian mapping, DLA bridges the gap between unstructured 3D representations and high-fidelity, animation-ready avatars.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation</title>
<link>https://arxiv.org/abs/2507.16002</link>
<guid>https://arxiv.org/abs/2507.16002</guid>
<content:encoded><![CDATA[
arXiv:2507.16002v1 Announce Type: cross 
Abstract: One major challenge in natural language processing is named entity recognition (NER), which identifies and categorises named entities in textual input. In order to improve NER, this study investigates a Hindi NER technique that makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and Generative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf (Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments the data with retrieved data from external relevant contexts, notably from Wikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA. However, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER generation. Our investigation shows that the mentioned language models (LMs) with Retrieval Augmentation (RA) outperform baseline methods that don't incorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69 and 0.495, respectively, without RA and increase to 0.70 and 0.71, respectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B by a significant margin. On the other hand the generative models which are not fine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA well; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval context. The findings show that RA significantly improves performance, especially for low-context data. This study adds significant knowledge about how best to use data augmentation methods and pretrained models to enhance NER performance, particularly in languages with limited resources.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMAT: A Hierarchical Framework for Autonomous Alloy Discovery</title>
<link>https://arxiv.org/abs/2507.16005</link>
<guid>https://arxiv.org/abs/2507.16005</guid>
<content:encoded><![CDATA[
arXiv:2507.16005v1 Announce Type: cross 
Abstract: Alloy discovery is central to advancing modern industry but remains hindered by the vastness of compositional design space and the costly validation. Here, we present AutoMAT, a hierarchical and autonomous framework grounded in and validated by experiments, which integrates large language models, automated CALPHAD-based simulations, and AI-driven search to accelerate alloy design. Spanning the entire pipeline from ideation to validation, AutoMAT achieves high efficiency, accuracy, and interpretability without the need for manually curated large datasets. In a case study targeting a lightweight, high-strength alloy, AutoMAT identifies a titanium alloy with 8.1% lower density and comparable yield strength relative to the state-of-the-art reference, achieving the highest specific strength among all comparisons. In a second case targeting high-yield-strength high-entropy alloys, AutoMAT achieves a 28.2% improvement in yield strength over the base alloy. In both cases, AutoMAT reduces the discovery timeline from years to weeks, illustrating its potential as a scalable and versatile platform for next-generation alloy design.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Just a strange pic": Evaluating 'safety' in GenAI Image safety annotation tasks from diverse annotators' perspectives</title>
<link>https://arxiv.org/abs/2507.16033</link>
<guid>https://arxiv.org/abs/2507.16033</guid>
<content:encoded><![CDATA[
arXiv:2507.16033v1 Announce Type: cross 
Abstract: Understanding what constitutes safety in AI-generated content is complex. While developers often rely on predefined taxonomies, real-world safety judgments also involve personal, social, and cultural perceptions of harm. This paper examines how annotators evaluate the safety of AI-generated images, focusing on the qualitative reasoning behind their judgments. Analyzing 5,372 open-ended comments, we find that annotators consistently invoke moral, emotional, and contextual reasoning that extends beyond structured safety categories. Many reflect on potential harm to others more than to themselves, grounding their judgments in lived experience, collective risk, and sociocultural awareness. Beyond individual perceptions, we also find that the structure of the task itself -- including annotation guidelines -- shapes how annotators interpret and express harm. Guidelines influence not only which images are flagged, but also the moral judgment behind the justifications. Annotators frequently cite factors such as image quality, visual distortion, and mismatches between prompt and output as contributing to perceived harm dimensions, which are often overlooked in standard evaluation frameworks. Our findings reveal that existing safety pipelines miss critical forms of reasoning that annotators bring to the task. We argue for evaluation designs that scaffold moral reflection, differentiate types of harm, and make space for subjective, context-sensitive interpretations of AI-generated content.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering and using Spelke segments</title>
<link>https://arxiv.org/abs/2507.16038</link>
<guid>https://arxiv.org/abs/2507.16038</guid>
<content:encoded><![CDATA[
arXiv:2507.16038v1 Announce Type: cross 
Abstract: Segments in computer vision are often defined by semantic considerations and are highly dependent on category-specific conventions. In contrast, developmental psychology suggests that humans perceive the world in terms of Spelke objects--groupings of physical things that reliably move together when acted on by physical forces. Spelke objects thus operate on category-agnostic causal motion relationships which potentially better support tasks like manipulation and planning. In this paper, we first benchmark the Spelke object concept, introducing the SpelkeBench dataset that contains a wide variety of well-defined Spelke segments in natural images. Next, to extract Spelke segments from images algorithmically, we build SpelkeNet, a class of visual world models trained to predict distributions over future motions. SpelkeNet supports estimation of two key concepts for Spelke object discovery: (1) the motion affordance map, identifying regions likely to move under a poke, and (2) the expected-displacement map, capturing how the rest of the scene will move. These concepts are used for "statistical counterfactual probing", where diverse "virtual pokes" are applied on regions of high motion-affordance, and the resultant expected displacement maps are used define Spelke segments as statistical aggregates of correlated motion statistics. We find that SpelkeNet outperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench. Finally, we show that the Spelke concept is practically useful for downstream applications, yielding superior performance on the 3DEditBench benchmark for physical object manipulation when used in a variety of off-the-shelf object manipulation models.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reactivation: Empirical NTK Dynamics Under Task Shifts</title>
<link>https://arxiv.org/abs/2507.16039</link>
<guid>https://arxiv.org/abs/2507.16039</guid>
<content:encoded><![CDATA[
arXiv:2507.16039v1 Announce Type: cross 
Abstract: The Neural Tangent Kernel (NTK) offers a powerful tool to study the functional dynamics of neural networks. In the so-called lazy, or kernel regime, the NTK remains static during training and the network function is linear in the static neural tangents feature space. The evolution of the NTK during training is necessary for feature learning, a key driver of deep learning success. The study of the NTK dynamics has led to several critical discoveries in recent years, in generalization and scaling behaviours. However, this body of work has been limited to the single task setting, where the data distribution is assumed constant over time. In this work, we present a comprehensive empirical analysis of NTK dynamics in continual learning, where the data distribution shifts over time. Our findings highlight continual learning as a rich and underutilized testbed for probing the dynamics of neural training. At the same time, they challenge the validity of static-kernel approximations in theoretical treatments of continual learning, even at large scale.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Rate Coding: Surrogate Gradients Enable Spike Timing Learning in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2507.16043</link>
<guid>https://arxiv.org/abs/2507.16043</guid>
<content:encoded><![CDATA[
arXiv:2507.16043v1 Announce Type: cross 
Abstract: We investigate the extent to which Spiking Neural Networks (SNNs) trained with Surrogate Gradient Descent (Surrogate GD), with and without delay learning, can learn from precise spike timing beyond firing rates. We first design synthetic tasks isolating intra-neuron inter-spike intervals and cross-neuron synchrony under matched spike counts. On more complex spike-based speech recognition datasets (Spiking Heidelberg Digits (SHD) and Spiking Speech Commands (SSC), we construct variants where spike count information is eliminated and only timing information remains, and show that Surrogate GD-trained SNNs are able to perform significantly above chance whereas purely rate-based models perform at chance level. We further evaluate robustness under biologically inspired perturbations -- including Gaussian jitter per spike or per-neuron, and spike deletion -- revealing consistent but perturbation-specific degradation. Networks show a sharp performance drop when spike sequences are reversed in time, with a larger drop in performance from SNNs trained with delays, indicating that these networks are more human-like in terms of behaviour. To facilitate further studies of temporal coding, we have released our modified SHD and SSC datasets.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Powered Commit Explorer (APCE)</title>
<link>https://arxiv.org/abs/2507.16063</link>
<guid>https://arxiv.org/abs/2507.16063</guid>
<content:encoded><![CDATA[
arXiv:2507.16063v1 Announce Type: cross 
Abstract: Commit messages in a version control system provide valuable information for developers regarding code changes in software systems. Commit messages can be the only source of information left for future developers describing what was changed and why. However, writing high-quality commit messages is often neglected in practice. Large Language Model (LLM) generated commit messages have emerged as a way to mitigate this issue. We introduce the AI-Powered Commit Explorer (APCE), a tool to support developers and researchers in the use and study of LLM-generated commit messages. APCE gives researchers the option to store different prompts for LLMs and provides an additional evaluation prompt that can further enhance the commit message provided by LLMs. APCE also provides researchers with a straightforward mechanism for automated and human evaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Coordination for Multi-Robot Teams with Large Language Models</title>
<link>https://arxiv.org/abs/2507.16068</link>
<guid>https://arxiv.org/abs/2507.16068</guid>
<content:encoded><![CDATA[
arXiv:2507.16068v1 Announce Type: cross 
Abstract: Multi-robot coordination has traditionally relied on a task-specific and expert-driven pipeline, where natural language mission descriptions are manually translated by domain experts into mathematical formulation, algorithm design, and executable code. This conventional process is labor-intensive, inaccessible to non-experts, and inflexible to changes in mission requirements. Here, we propose LAN2CB (Language to Collective Behavior), a novel framework that leverages large language models (LLMs) to streamline and generalize the multi-robot coordination pipeline. LAN2CB directly converts natural language mission descriptions into executable Python code for multi-robot systems through two key components: (1) Mission Decomposition for Task Representation, which parses the mission into a task graph with dependencies, and (2) Code Generation, which uses the task graph and a structured knowledge base to generate deployable robot control code. We further introduce a dataset of natural language mission specifications to support development and benchmarking. Experimental results in both simulation and real-world settings show that LAN2CB enables effective and flexible multi-robot coordination from natural language, significantly reducing the need for manual engineering while supporting generalization across mission types. Website: https://sites.google.com/view/lan2cb.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-driven Orchestration at Scale: Estimating Service Metrics on National-Wide Testbeds</title>
<link>https://arxiv.org/abs/2507.16077</link>
<guid>https://arxiv.org/abs/2507.16077</guid>
<content:encoded><![CDATA[
arXiv:2507.16077v1 Announce Type: cross 
Abstract: Network Slicing (NS) realization requires AI-native orchestration architectures to efficiently and intelligently handle heterogeneous user requirements. To achieve this, network slicing is evolving towards a more user-centric digital transformation, focusing on architectures that incorporate native intelligence to enable self-managed connectivity in an integrated and isolated manner. However, these initiatives face the challenge of validating their results in production environments, particularly those utilizing ML-enabled orchestration, as they are often tested in local networks or laboratory simulations. This paper proposes a large-scale validation method using a network slicing prediction model to forecast latency using Deep Neural Networks (DNNs) and basic ML algorithms embedded within an NS architecture, evaluated in real large-scale production testbeds. It measures and compares the performance of different DNNs and ML algorithms, considering a distributed database application deployed as a network slice over two large-scale production testbeds. The investigation highlights how AI-based prediction models can enhance network slicing orchestration architectures and presents a seamless, production-ready validation method as an alternative to fully controlled simulations or laboratory setups.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lower Bound for the Number of Linear Regions of Ternary ReLU Regression Neural Networks</title>
<link>https://arxiv.org/abs/2507.16079</link>
<guid>https://arxiv.org/abs/2507.16079</guid>
<content:encoded><![CDATA[
arXiv:2507.16079v1 Announce Type: cross 
Abstract: With the advancement of deep learning, reducing computational complexity and memory consumption has become a critical challenge, and ternary neural networks (NNs) that restrict parameters to $\{-1, 0, +1\}$ have attracted attention as a promising approach. While ternary NNs demonstrate excellent performance in practical applications such as image recognition and natural language processing, their theoretical understanding remains insufficient. In this paper, we theoretically analyze the expressivity of ternary NNs from the perspective of the number of linear regions. Specifically, we evaluate the number of linear regions of ternary regression NNs with Rectified Linear Unit (ReLU) for activation functions and prove that the number of linear regions increases polynomially with respect to network width and exponentially with respect to depth, similar to standard NNs. Moreover, we show that it suffices to either square the width or double the depth of ternary NNs to achieve a lower bound on the maximum number of linear regions comparable to that of general ReLU regression NNs. This provides a theoretical explanation, in some sense, for the practical success of ternary NNs.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Compositional Multi-tasking for On-device Large Language Models</title>
<link>https://arxiv.org/abs/2507.16083</link>
<guid>https://arxiv.org/abs/2507.16083</guid>
<content:encoded><![CDATA[
arXiv:2507.16083v1 Announce Type: cross 
Abstract: Adapter parameters provide a mechanism to modify the behavior of machine learning models and have gained significant popularity in the context of large language models (LLMs) and generative AI. These parameters can be merged to support multiple tasks via a process known as task merging. However, prior work on merging in LLMs, particularly in natural language processing, has been limited to scenarios where each test example addresses only a single task. In this paper, we focus on on-device settings and study the problem of text-based compositional multi-tasking, where each test example involves the simultaneous execution of multiple tasks. For instance, generating a translated summary of a long text requires solving both translation and summarization tasks concurrently. To facilitate research in this setting, we propose a benchmark comprising four practically relevant compositional tasks. We also present an efficient method (Learnable Calibration) tailored for on-device applications, where computational resources are limited, emphasizing the need for solutions that are both resource-efficient and high-performing. Our contributions lay the groundwork for advancing the capabilities of LLMs in real-world multi-tasking scenarios, expanding their applicability to complex, resource-constrained use cases.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLM Privacy Recognition for Social Robot Decision Making</title>
<link>https://arxiv.org/abs/2507.16124</link>
<guid>https://arxiv.org/abs/2507.16124</guid>
<content:encoded><![CDATA[
arXiv:2507.16124v1 Announce Type: cross 
Abstract: Social robots are embodied agents that interact with people while following human communication norms. These robots interact using verbal and non-verbal cues, and share the physical environments of people. While social robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models (LLMs) presents new opportunities to develop LLM-empowered social robots for enhanced human-robot interaction. To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, LLMs often process sensitive personal information, particularly within home environments. Given the tension between utility and privacy risks, evaluating how current LLMs manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the context of household social robots. In this study, we present a set of privacy-relevant scenarios crafted through the lens of Contextual Integrity (CI). We first survey users' privacy preferences regarding in-home social robot behaviors and then examine how their privacy orientation affects their choices of these behaviors (N = 450). We then provide the same set of scenarios and questions to state-of-the-art LLMs (N = 10) and find that the agreement between humans and LLMs is low. To further investigate the capabilities of LLMs as a potential privacy controller, we implement four additional prompting strategies and compare their results. Finally, we discuss the implications and potential of AI privacy awareness in human-robot interaction.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disability Across Cultures: A Human-Centered Audit of Ableism in Western and Indic LLMs</title>
<link>https://arxiv.org/abs/2507.16130</link>
<guid>https://arxiv.org/abs/2507.16130</guid>
<content:encoded><![CDATA[
arXiv:2507.16130v1 Announce Type: cross 
Abstract: People with disabilities (PwD) experience disproportionately high levels of discrimination and hate online, particularly in India, where entrenched stigma and limited resources intensify these challenges. Large language models (LLMs) are increasingly used to identify and mitigate online hate, yet most research on online ableism focuses on Western audiences with Western AI models. Are these models adequately equipped to recognize ableist harm in non-Western places like India? Do localized, Indic language models perform better? To investigate, we adopted and translated a publicly available ableist speech dataset to Hindi, and prompted eight LLMs--four developed in the U.S. (GPT-4, Gemini, Claude, Llama) and four in India (Krutrim, Nanda, Gajendra, Airavata)--to score and explain ableism. In parallel, we recruited 175 PwD from both the U.S. and India to perform the same task, revealing stark differences between groups. Western LLMs consistently overestimated ableist harm, while Indic LLMs underestimated it. Even more concerning, all LLMs were more tolerant of ableism when it was expressed in Hindi and asserted Western framings of ableist harm. In contrast, Indian PwD interpreted harm through intention, relationality, and resilience--emphasizing a desire to inform and educate perpetrators. This work provides groundwork for global, inclusive standards of ableism, demonstrating the need to center local disability experiences in the design and evaluation of AI systems.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDBench: A Comprehensive Benchmark Suite for Speaker Diarization</title>
<link>https://arxiv.org/abs/2507.16136</link>
<guid>https://arxiv.org/abs/2507.16136</guid>
<content:encoded><![CDATA[
arXiv:2507.16136v1 Announce Type: cross 
Abstract: Even state-of-the-art speaker diarization systems exhibit high variance in error rates across different datasets, representing numerous use cases and domains. Furthermore, comparing across systems requires careful application of best practices such as dataset splits and metric definitions to allow for apples-to-apples comparison. We propose SDBench (Speaker Diarization Benchmark), an open-source benchmark suite that integrates 13 diverse datasets with built-in tooling for consistent and fine-grained analysis of speaker diarization performance for various on-device and server-side systems. SDBench enables reproducible evaluation and easy integration of new systems over time. To demonstrate the efficacy of SDBench, we built SpeakerKit, an inference efficiency-focused system built on top of Pyannote v3. SDBench enabled rapid execution of ablation studies that led to SpeakerKit being 9.6x faster than Pyannote v3 while achieving comparable error rates. We benchmark 6 state-of-the-art systems including Deepgram, AWS Transcribe, and Pyannote AI API, revealing important trade-offs between accuracy and speed.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPACT18: Spiking Human Action Recognition Benchmark Dataset with Complementary RGB and Thermal Modalities</title>
<link>https://arxiv.org/abs/2507.16151</link>
<guid>https://arxiv.org/abs/2507.16151</guid>
<content:encoded><![CDATA[
arXiv:2507.16151v1 Announce Type: cross 
Abstract: Spike cameras, bio-inspired vision sensors, asynchronously fire spikes by accumulating light intensities at each pixel, offering ultra-high energy efficiency and exceptional temporal resolution. Unlike event cameras, which record changes in light intensity to capture motion, spike cameras provide even finer spatiotemporal resolution and a more precise representation of continuous changes. In this paper, we introduce the first video action recognition (VAR) dataset using spike camera, alongside synchronized RGB and thermal modalities, to enable comprehensive benchmarking for Spiking Neural Networks (SNNs). By preserving the inherent sparsity and temporal precision of spiking data, our three datasets offer a unique platform for exploring multimodal video understanding and serve as a valuable resource for directly comparing spiking, thermal, and RGB modalities. This work contributes a novel dataset that will drive research in energy-efficient, ultra-low-power video understanding, specifically for action recognition tasks using spike-based data.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for Efficient Text to Image Generation</title>
<link>https://arxiv.org/abs/2507.16154</link>
<guid>https://arxiv.org/abs/2507.16154</guid>
<content:encoded><![CDATA[
arXiv:2507.16154v1 Announce Type: cross 
Abstract: Flow matching and diffusion models have shown impressive results in text-to-image generation, producing photorealistic images through an iterative denoising process. A common strategy to speed up synthesis is to perform early denoising at lower resolutions. However, traditional methods that downscale and upscale in pixel space often introduce artifacts and distortions. These issues arise when the upscaled images are re-encoded into the latent space, leading to degraded final image quality. To address this, we propose {\bf Latent Space Scaling Generation (LSSGen)}, a framework that performs resolution scaling directly in the latent space using a lightweight latent upsampler. Without altering the Transformer or U-Net architecture, LSSGen improves both efficiency and visual quality while supporting flexible multi-resolution generation. Our comprehensive evaluation covering text-image alignment and perceptual quality shows that LSSGen significantly outperforms conventional scaling approaches. When generating $1024^2$ images at similar speeds, it achieves up to 246\% TOPIQ score improvement.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attacking interpretable NLP systems</title>
<link>https://arxiv.org/abs/2507.16164</link>
<guid>https://arxiv.org/abs/2507.16164</guid>
<content:encoded><![CDATA[
arXiv:2507.16164v1 Announce Type: cross 
Abstract: Studies have shown that machine learning systems are vulnerable to adversarial examples in theory and practice. Where previous attacks have focused mainly on visual models that exploit the difference between human and machine perception, text-based models have also fallen victim to these attacks. However, these attacks often fail to maintain the semantic meaning of the text and similarity. This paper introduces AdvChar, a black-box attack on Interpretable Natural Language Processing Systems, designed to mislead the classifier while keeping the interpretation similar to benign inputs, thus exploiting trust in system transparency. AdvChar achieves this by making less noticeable modifications to text input, forcing the deep learning classifier to make incorrect predictions and preserve the original interpretation. We use an interpretation-focused scoring approach to determine the most critical tokens that, when changed, can cause the classifier to misclassify the input. We apply simple character-level modifications to measure the importance of tokens, minimizing the difference between the original and new text while generating adversarial interpretations similar to benign ones. We thoroughly evaluated AdvChar by testing it against seven NLP models and three interpretation models using benchmark datasets for the classification task. Our experiments show that AdvChar can significantly reduce the prediction accuracy of current deep learning models by altering just two characters on average in input samples.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Data Selection and Utilization via Dynamic Bi-level Optimization</title>
<link>https://arxiv.org/abs/2507.16178</link>
<guid>https://arxiv.org/abs/2507.16178</guid>
<content:encoded><![CDATA[
arXiv:2507.16178v1 Announce Type: cross 
Abstract: While large-scale training data is fundamental for developing capable large language models (LLMs), strategically selecting high-quality data has emerged as a critical approach to enhance training efficiency and reduce computational costs. Current data selection methodologies predominantly rely on static, training-agnostic criteria, failing to account for the dynamic model training and data interactions. In this paper, we propose a new Data Weighting Model (DWM) to adjust the weight of selected data within each batch to achieve a dynamic data utilization during LLM training. Specially, to better capture the dynamic data preference of the trained model, a bi-level optimization framework is implemented to update the weighting model. Our experiments demonstrate that DWM enhances the performance of models trained with randomly-selected data, and the learned weighting model can be transferred to enhance other data selection methods and models of different sizes. Moreover, we further analyze how a model's data preferences evolve throughout training, providing new insights into the data preference of the model during training.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVAgent: AI Agent for Hardware Security Verification Assertion</title>
<link>https://arxiv.org/abs/2507.16203</link>
<guid>https://arxiv.org/abs/2507.16203</guid>
<content:encoded><![CDATA[
arXiv:2507.16203v1 Announce Type: cross 
Abstract: Verification using SystemVerilog assertions (SVA) is one of the most popular methods for detecting circuit design vulnerabilities. However, with the globalization of integrated circuit design and the continuous upgrading of security requirements, the SVA development model has exposed major limitations. It is not only inefficient in development, but also unable to effectively deal with the increasing number of security vulnerabilities in modern complex integrated circuits. In response to these challenges, this paper proposes an innovative SVA automatic generation framework SVAgent. SVAgent introduces a requirement decomposition mechanism to transform the original complex requirements into a structured, gradually solvable fine-grained problem-solving chain. Experiments have shown that SVAgent can effectively suppress the influence of hallucinations and random answers, and the key evaluation indicators such as the accuracy and consistency of the SVA are significantly better than existing frameworks. More importantly, we successfully integrated SVAgent into the most mainstream integrated circuit vulnerability assessment framework and verified its practicality and reliability in a real engineering design environment.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>METER: Multi-modal Evidence-based Thinking and Explainable Reasoning -- Algorithm and Benchmark</title>
<link>https://arxiv.org/abs/2507.16206</link>
<guid>https://arxiv.org/abs/2507.16206</guid>
<content:encoded><![CDATA[
arXiv:2507.16206v1 Announce Type: cross 
Abstract: With the rapid advancement of generative AI, synthetic content across images, videos, and audio has become increasingly realistic, amplifying the risk of misinformation. Existing detection approaches predominantly focus on binary classification while lacking detailed and interpretable explanations of forgeries, which limits their applicability in safety-critical scenarios. Moreover, current methods often treat each modality separately, without a unified benchmark for cross-modal forgery detection and interpretation. To address these challenges, we introduce METER, a unified, multi-modal benchmark for interpretable forgery detection spanning images, videos, audio, and audio-visual content. Our dataset comprises four tracks, each requiring not only real-vs-fake classification but also evidence-chain-based explanations, including spatio-temporal localization, textual rationales, and forgery type tracing. Compared to prior benchmarks, METER offers broader modality coverage and richer interpretability metrics such as spatial/temporal IoU, multi-class tracing, and evidence consistency. We further propose a human-aligned, three-stage Chain-of-Thought (CoT) training strategy combining SFT, DPO, and a novel GRPO stage that integrates a human-aligned evaluator with CoT reasoning. We hope METER will serve as a standardized foundation for advancing generalizable and interpretable forgery detection in the era of generative media.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Human-Centered Approach to Identifying Promises, Risks, &amp; Challenges of Text-to-Image Generative AI in Radiology</title>
<link>https://arxiv.org/abs/2507.16207</link>
<guid>https://arxiv.org/abs/2507.16207</guid>
<content:encoded><![CDATA[
arXiv:2507.16207v1 Announce Type: cross 
Abstract: As text-to-image generative models rapidly improve, AI researchers are making significant advances in developing domain-specific models capable of generating complex medical imagery from text prompts. Despite this, these technical advancements have overlooked whether and how medical professionals would benefit from and use text-to-image generative AI (GenAI) in practice. By developing domain-specific GenAI without involving stakeholders, we risk the potential of building models that are either not useful or even more harmful than helpful. In this paper, we adopt a human-centered approach to responsible model development by involving stakeholders in evaluating and reflecting on the promises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through exploratory model prompting activities, we uncover the perspectives of medical students, radiology trainees, and radiologists on the role that text-to-CT Scan GenAI can play across medical education, training, and practice. This human-centered approach additionally enabled us to surface technical challenges and domain-specific risks of generating synthetic medical images. We conclude by reflecting on the implications of medical text-to-image GenAI.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOCOFY Large Design Models -- Design to code conversion solution</title>
<link>https://arxiv.org/abs/2507.16208</link>
<guid>https://arxiv.org/abs/2507.16208</guid>
<content:encoded><![CDATA[
arXiv:2507.16208v1 Announce Type: cross 
Abstract: Despite rapid advances in Large Language Models and Multimodal Large Language Models (LLMs), numerous challenges related to interpretability, scalability, resource requirements and repeatability remain, related to their application in the design-to-code space. To address this, we introduce the Large Design Models (LDMs) paradigm specifically trained on designs and webpages to enable seamless conversion from design-to-code. We have developed a training and inference pipeline by incorporating data engineering and appropriate model architecture modification. The training pipeline consists of the following: 1)Design Optimiser: developed using a proprietary ground truth dataset and addresses sub-optimal designs; 2)Tagging and feature detection: using pre-trained and fine-tuned models, this enables the accurate detection and classification of UI elements; and 3)Auto Components: extracts repeated UI structures into reusable components to enable creation of modular code, thus reducing redundancy while enhancing code reusability. In this manner, each model addresses distinct but key issues for design-to-code conversion. Separately, our inference pipeline processes real-world designs to produce precise and interpretable instructions for code generation and ensures reliability. Additionally, our models illustrated exceptional end-to-end design-to-code conversion accuracy using a novel preview match score metric. Comparative experiments indicated superior performance of LDMs against LLMs on accuracy of node positioning, responsiveness and reproducibility. Moreover, our custom-trained tagging and feature detection model demonstrated high precision and consistency in identifying UI elements across a wide sample of test designs. Thus, our proposed LDMs are a reliable and superior solution to understanding designs that subsequently enable the generation of efficient and reliable production-ready code.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Visual Large Language Model for Multi-granular Versatile Perception</title>
<link>https://arxiv.org/abs/2507.16213</link>
<guid>https://arxiv.org/abs/2507.16213</guid>
<content:encoded><![CDATA[
arXiv:2507.16213v1 Announce Type: cross 
Abstract: Perception is a fundamental task in the field of computer vision, encompassing a diverse set of subtasks that can be systematically categorized into four distinct groups based on two dimensions: prediction type and instruction type. Notably, existing researches often focus solely on a limited subset of these potential combinations, which constrains their applicability and versatility across various contexts. In response to this challenge, we present MVP-LM, a Multi-granular and Versatile Perception framework incorporating Visual Large Language Model. Our framework is designed to integrate both word-based and sentence-based perception tasks alongside box and mask predictions within a single architecture. MVP-LM features an innovative multi-granularity decoder in conjunction with a CoT-inspired dataset unification strategy, enabling seamless supervised fine-tuning across a wide spectrum of tasks, including but not limited to panoptic segmentation, detection, grounding, and referring expression segmentation. Furthermore, we introduce a query enhancement strategy aimed at harnessing the decoding and generative capabilities inherent in VLLMs. Extensive experiments conducted across a range of benchmarks in both word-based and sentence-based perception tasks substantiate the efficacy of our framework. The code will be available at https://github.com/xiangwentao666/MVP-LM.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Relative Pose Estimation Framework with Dual Noise Tuning for Safe Approaching Maneuvers</title>
<link>https://arxiv.org/abs/2507.16214</link>
<guid>https://arxiv.org/abs/2507.16214</guid>
<content:encoded><![CDATA[
arXiv:2507.16214v1 Announce Type: cross 
Abstract: Accurate and robust relative pose estimation is crucial for enabling challenging Active Debris Removal (ADR) missions targeting tumbling derelict satellites such as ESA's ENVISAT. This work presents a complete pipeline integrating advanced computer vision techniques with adaptive nonlinear filtering to address this challenge. A Convolutional Neural Network (CNN), enhanced with image preprocessing, detects structural markers (corners) from chaser imagery, whose 2D coordinates are converted to 3D measurements using camera modeling. These measurements are fused within an Unscented Kalman Filter (UKF) framework, selected for its ability to handle nonlinear relative dynamics, to estimate the full relative pose. Key contributions include the integrated system architecture and a dual adaptive strategy within the UKF: dynamic tuning of the measurement noise covariance compensates for varying CNN measurement uncertainty, while adaptive tuning of the process noise covariance, utilizing measurement residual analysis, accounts for unmodeled dynamics or maneuvers online. This dual adaptation enhances robustness against both measurement imperfections and dynamic model uncertainties. The performance of the proposed adaptive integrated system is evaluated through high-fidelity simulations using a realistic ENVISAT model, comparing estimates against ground truth under various conditions, including measurement outages. This comprehensive approach offers an enhanced solution for robust onboard relative navigation, significantly advancing the capabilities required for safe proximity operations during ADR missions.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Compute-Optimal Many-Shot In-Context Learning</title>
<link>https://arxiv.org/abs/2507.16217</link>
<guid>https://arxiv.org/abs/2507.16217</guid>
<content:encoded><![CDATA[
arXiv:2507.16217v1 Announce Type: cross 
Abstract: Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. In this work, we propose two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. Our first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Our experiments with Gemini Pro and Flash across several datasets indicate that our strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. We also show that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Deep Learning for Convective Initiation Nowcasting Uncertainty Estimation</title>
<link>https://arxiv.org/abs/2507.16219</link>
<guid>https://arxiv.org/abs/2507.16219</guid>
<content:encoded><![CDATA[
arXiv:2507.16219v1 Announce Type: cross 
Abstract: This study evaluated the probability and uncertainty forecasts of five recently proposed Bayesian deep learning methods relative to a deterministic residual neural network (ResNet) baseline for 0-1 h convective initiation (CI) nowcasting using GOES-16 satellite infrared observations. Uncertainty was assessed by how well probabilistic forecasts were calibrated and how well uncertainty separated forecasts with large and small errors. Most of the Bayesian deep learning methods produced probabilistic forecasts that outperformed the deterministic ResNet, with one, the initial-weights ensemble + Monte Carlo (MC) dropout, an ensemble of deterministic ResNets with different initial weights to start training and dropout activated during inference, producing the most skillful and well-calibrated forecasts. The initial-weights ensemble + MC dropout benefited from generating multiple solutions that more thoroughly sampled the hypothesis space. The Bayesian ResNet ensemble was the only one that performed worse than the deterministic ResNet at longer lead times, likely due to the challenge of optimizing a larger number of parameters. To address this issue, the Bayesian-MOPED (MOdel Priors with Empirical Bayes using Deep neural network) ResNet ensemble was adopted, and it enhanced forecast skill by constraining the hypothesis search near the deterministic ResNet hypothesis. All Bayesian methods demonstrated well-calibrated uncertainty and effectively separated cases with large and small errors. In case studies, the initial-weights ensemble + MC dropout demonstrated better forecast skill than the Bayesian-MOPED ensemble and the deterministic ResNet on selected CI events in clear-sky regions. However, the initial-weights ensemble + MC dropout exhibited poorer generalization in clear-sky and anvil cloud regions without CI occurrence compared to the deterministic ResNet and Bayesian-MOPED ensemble.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Hydrodynamic Simulations for Laser Direct-drive Implosion Experiments via Artificial Intelligence</title>
<link>https://arxiv.org/abs/2507.16227</link>
<guid>https://arxiv.org/abs/2507.16227</guid>
<content:encoded><![CDATA[
arXiv:2507.16227v1 Announce Type: cross 
Abstract: This work presents predictive hydrodynamic simulations empowered by artificial intelligence (AI) for laser driven implosion experiments, taking the double-cone ignition (DCI) scheme as an example. A Transformer-based deep learning model MULTI-Net is established to predict implosion features according to laser waveforms and target radius. A Physics-Informed Decoder (PID) is proposed for high-dimensional sampling, significantly reducing the prediction errors compared to Latin hypercube sampling. Applied to DCI experiments conducted on the SG-II Upgrade facility, the MULTI-Net model is able to predict the implosion dynamics measured by the x-ray streak camera. It is found that an effective laser absorption factor about 65\% is suitable for the one-dimensional simulations of the DCI-R10 experiments. For shot 33, the mean implosion velocity and collided plasma density reached 195 km/s and 117 g/cc, respectively. This study demonstrates a data-driven AI framework that enhances the prediction ability of simulations for complicated laser fusion experiments.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eX-NIDS: A Framework for Explainable Network Intrusion Detection Leveraging Large Language Models</title>
<link>https://arxiv.org/abs/2507.16241</link>
<guid>https://arxiv.org/abs/2507.16241</guid>
<content:encoded><![CDATA[
arXiv:2507.16241v1 Announce Type: cross 
Abstract: This paper introduces eX-NIDS, a framework designed to enhance interpretability in flow-based Network Intrusion Detection Systems (NIDS) by leveraging Large Language Models (LLMs). In our proposed framework, flows labelled as malicious by NIDS are initially processed through a module called the Prompt Augmenter. This module extracts contextual information and Cyber Threat Intelligence (CTI)-related knowledge from these flows. This enriched, context-specific data is then integrated with an input prompt for an LLM, enabling it to generate detailed explanations and interpretations of why the flow was identified as malicious by NIDS. We compare the generated interpretations against a Basic-Prompt Explainer baseline, which does not incorporate any contextual information into the LLM's input prompt. Our framework is quantitatively evaluated using the Llama 3 and GPT-4 models, employing a novel evaluation method tailored for natural language explanations, focusing on their correctness and consistency. The results demonstrate that augmented LLMs can produce accurate and consistent explanations, serving as valuable complementary tools in NIDS to explain the classification of malicious flows. The use of augmented prompts enhances performance by over 20% compared to the Basic-Prompt Explainer.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRAC3 (Privacy, Reputation, Accountability, Consent, Credit, Compensation): Long Tailed Risks of Voice Actors in AI Data-Economy</title>
<link>https://arxiv.org/abs/2507.16247</link>
<guid>https://arxiv.org/abs/2507.16247</guid>
<content:encoded><![CDATA[
arXiv:2507.16247v1 Announce Type: cross 
Abstract: Early large-scale audio datasets, such as LibriSpeech, were built with hundreds of individual contributors whose voices were instrumental in the development of speech technologies, including audiobooks and voice assistants. Yet, a decade later, these same contributions have exposed voice actors to a range of risks. While existing ethical frameworks emphasize Consent, Credit, and Compensation (C3), they do not adequately address the emergent risks involving vocal identities that are increasingly decoupled from context, authorship, and control. Drawing on qualitative interviews with 20 professional voice actors, this paper reveals how the synthetic replication of voice without enforceable constraints exposes individuals to a range of threats. Beyond reputational harm, such as re-purposing voice data in erotic content, offensive political messaging, and meme culture, we document concerns about accountability breakdowns when their voice is leveraged to clone voices that are deployed in high-stakes scenarios such as financial fraud, misinformation campaigns, or impersonation scams. In such cases, actors face social and legal fallout without recourse, while very few of them have a legal representative or union protection. To make sense of these shifting dynamics, we introduce the PRAC3 framework, an expansion of C3 that foregrounds Privacy, Reputation, Accountability, Consent, Credit, and Compensation as interdependent pillars of data used in the synthetic voice economy. This framework captures how privacy risks are amplified through non-consensual training, how reputational harm arises from decontextualized deployment, and how accountability can be reimagined AI Data ecosystems. We argue that voice, as both a biometric identifier and creative labor, demands governance models that restore creator agency, ensure traceability, and establish enforceable boundaries for ethical reuse.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoliTracer: Holistic Vectorization of Geographic Objects from Large-Size Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2507.16251</link>
<guid>https://arxiv.org/abs/2507.16251</guid>
<content:encoded><![CDATA[
arXiv:2507.16251v1 Announce Type: cross 
Abstract: With the increasing resolution of remote sensing imagery (RSI), large-size RSI has emerged as a vital data source for high-precision vector mapping of geographic objects. Existing methods are typically constrained to processing small image patches, which often leads to the loss of contextual information and produces fragmented vector outputs. To address these, this paper introduces HoliTracer, the first framework designed to holistically extract vectorized geographic objects from large-size RSI. In HoliTracer, we enhance segmentation of large-size RSI using the Context Attention Net (CAN), which employs a local-to-global attention mechanism to capture contextual dependencies. Furthermore, we achieve holistic vectorization through a robust pipeline that leverages the Mask Contour Reformer (MCR) to reconstruct polygons and the Polygon Sequence Tracer (PST) to trace vertices. Extensive experiments on large-size RSI datasets, including buildings, water bodies, and roads, demonstrate that HoliTracer outperforms state-of-the-art methods. Our code and data are available in https://github.com/vvangfaye/HoliTracer.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient RL for optimizing conversation level outcomes with an LLM-based tutor</title>
<link>https://arxiv.org/abs/2507.16252</link>
<guid>https://arxiv.org/abs/2507.16252</guid>
<content:encoded><![CDATA[
arXiv:2507.16252v1 Announce Type: cross 
Abstract: Large language models (LLMs) built on existing reinforcement learning with human feedback (RLHF) frameworks typically optimize responses based on immediate turn-level human preferences. However, this approach falls short in multi-turn dialogue settings, such as online math tutoring. We propose a method to enhance LLM-based tutors by representing the dialogue history with a lower-dimensional latent state representation of a student and optimizing a long-term policy to determine high-level actions based on the latent state. The goal is to better align the tutor's behavior with the long-term objective of guiding the student towards solving a target math problem on their own. Our model is lightweight, requiring less computational resources than prior work of training the tutor policy end-to-end to directly output the tutor's next utterance. Our experiment results demonstrate that these modifications lead to improved long-term outcomes compared to prompting in LLM-simulated tutoring tasks.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-case Synthesis for Fisheye Object Detection: A Data-centric Perspective</title>
<link>https://arxiv.org/abs/2507.16254</link>
<guid>https://arxiv.org/abs/2507.16254</guid>
<content:encoded><![CDATA[
arXiv:2507.16254v1 Announce Type: cross 
Abstract: Fisheye cameras introduce significant distortion and pose unique challenges to object detection models trained on conventional datasets. In this work, we propose a data-centric pipeline that systematically improves detection performance by focusing on the key question of identifying the blind spots of the model. Through detailed error analysis, we identify critical edge-cases such as confusing class pairs, peripheral distortions, and underrepresented contexts. Then we directly address them through edge-case synthesis. We fine-tuned an image generative model and guided it with carefully crafted prompts to produce images that replicate real-world failure modes. These synthetic images are pseudo-labeled using a high-quality detector and integrated into training. Our approach results in consistent performance gains, highlighting how deeply understanding data and selectively fixing its weaknesses can be impactful in specialized domains like fisheye object detection.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFNet: A Spatio-Frequency Domain Deep Learning Network for Efficient Alzheimer's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2507.16267</link>
<guid>https://arxiv.org/abs/2507.16267</guid>
<content:encoded><![CDATA[
arXiv:2507.16267v1 Announce Type: cross 
Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that predominantly affects the elderly population and currently has no cure. Magnetic Resonance Imaging (MRI), as a non-invasive imaging technique, is essential for the early diagnosis of AD. MRI inherently contains both spatial and frequency information, as raw signals are acquired in the frequency domain and reconstructed into spatial images via the Fourier transform. However, most existing AD diagnostic models extract features from a single domain, limiting their capacity to fully capture the complex neuroimaging characteristics of the disease. While some studies have combined spatial and frequency information, they are mostly confined to 2D MRI, leaving the potential of dual-domain analysis in 3D MRI unexplored. To overcome this limitation, we propose Spatio-Frequency Network (SFNet), the first end-to-end deep learning framework that simultaneously leverages spatial and frequency domain information to enhance 3D MRI-based AD diagnosis. SFNet integrates an enhanced dense convolutional network to extract local spatial features and a global frequency module to capture global frequency-domain representations. Additionally, a novel multi-scale attention module is proposed to further refine spatial feature extraction. Experiments on the Alzheimer's Disease Neuroimaging Initiative (ANDI) dataset demonstrate that SFNet outperforms existing baselines and reduces computational overhead in classifying cognitively normal (CN) and AD, achieving an accuracy of 95.1%.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training</title>
<link>https://arxiv.org/abs/2507.16274</link>
<guid>https://arxiv.org/abs/2507.16274</guid>
<content:encoded><![CDATA[
arXiv:2507.16274v1 Announce Type: cross 
Abstract: The rapid scaling of large language models (LLMs) has significantly increased GPU memory pressure, which is further aggravated by training optimization techniques such as virtual pipeline and recomputation that disrupt tensor lifespans and introduce considerable memory fragmentation. Default GPU memory allocators of popular deep learning frameworks like PyTorch use online strategies without knowledge of tensor lifespans, which can waste up to 43\% of memory and cause out-of-memory errors, rendering optimization techniques ineffective or even unusable.
  To address this, we introduce STWeaver, a GPU memory allocator for deep learning frameworks that reduces fragmentation by exploiting the spatial and temporal regularity in memory allocation behaviors of training workloads. STWeaver introduces a novel paradigm that combines offline planning with online allocation. The offline planning leverages spatio-temporal regularities to generate a near-optimal allocation plan, while the online allocation handles complex and dynamic models such as Mixture-of-Experts (MoE). Built as a pluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by 79.2\% (up to 100\%) across both dense and sparse models, with negligible overhead. This enables more efficient, high-throughput training configurations and improves performance by up to 32.5\%.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Generalization, Robustness, and Interpretability in Low-Capacity Neural Networks</title>
<link>https://arxiv.org/abs/2507.16278</link>
<guid>https://arxiv.org/abs/2507.16278</guid>
<content:encoded><![CDATA[
arXiv:2507.16278v1 Announce Type: cross 
Abstract: Although modern deep learning often relies on massive over-parameterized models, the fundamental interplay between capacity, sparsity, and robustness in low-capacity networks remains a vital area of study. We introduce a controlled framework to investigate these properties by creating a suite of binary classification tasks from the MNIST dataset with increasing visual difficulty (e.g., 0 and 1 vs. 4 and 9). Our experiments reveal three core findings. First, the minimum model capacity required for successful generalization scales directly with task complexity. Second, these trained networks are robust to extreme magnitude pruning (up to 95% sparsity), revealing the existence of sparse, high-performing subnetworks. Third, we show that over-parameterization provides a significant advantage in robustness against input corruption. Interpretability analysis via saliency maps further confirms that these identified sparse subnetworks preserve the core reasoning process of the original dense models. This work provides a clear, empirical demonstration of the foundational trade-offs governing simple neural networks.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning</title>
<link>https://arxiv.org/abs/2507.16302</link>
<guid>https://arxiv.org/abs/2507.16302</guid>
<content:encoded><![CDATA[
arXiv:2507.16302v1 Announce Type: cross 
Abstract: Text-to-image (T2I) diffusion models have achieved impressive image generation quality and are increasingly fine-tuned for personalized applications. However, these models often inherit unsafe behaviors from toxic pretraining data, raising growing safety concerns. While recent safety-driven unlearning methods have made promising progress in suppressing model toxicity, they are identified to be fragile to downstream fine-tuning, where we reveal that state-of-the-art methods largely fail to retain their effectiveness even when fine-tuned on entirely benign datasets. To mitigate this problem, in this paper, we propose ResAlign, a safety-driven unlearning framework with enhanced resilience against downstream fine-tuning. By modeling downstream fine-tuning as an implicit optimization problem with a Moreau Envelope-based reformulation, ResAlign enables efficient gradient estimation to minimize the recovery of harmful behaviors. Additionally, a meta-learning strategy is proposed to simulate a diverse distribution of fine-tuning scenarios to improve generalization. Extensive experiments across a wide range of datasets, fine-tuning methods, and configurations demonstrate that ResAlign consistently outperforms prior unlearning approaches in retaining safety after downstream fine-tuning while preserving benign generation capability well.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of Precursor Additives and Experimental Design</title>
<link>https://arxiv.org/abs/2507.16307</link>
<guid>https://arxiv.org/abs/2507.16307</guid>
<content:encoded><![CDATA[
arXiv:2507.16307v1 Announce Type: cross 
Abstract: Perovskite solar cells (PSCs) have rapidly emerged as a leading contender in next-generation photovoltaic technologies, owing to their exceptional power conversion efficiencies and advantageous material properties. Despite these advances, challenges such as long-term stability, environmental sustainability, and scalable manufacturing continue to hinder their commercialization. Precursor additive engineering has shown promise in addressing these issues by enhancing both the performance and durability of PSCs. However, the explosive growth of scientific literature and the complex interplay of materials, processes, and device architectures make it increasingly difficult for researchers to efficiently access, organize, and utilize domain knowledge in this rapidly evolving field. To address this gap, we introduce Perovskite-R1, a specialized large language model (LLM) with advanced reasoning capabilities tailored for the discovery and design of PSC precursor additives. By systematically mining and curating 1,232 high-quality scientific publications and integrating a comprehensive library of 33,269 candidate materials, we constructed a domain-specific instruction-tuning dataset using automated question-answer generation and chain-of-thought reasoning. Fine-tuning the QwQ-32B model on this dataset resulted in Perovskite-R1, which can intelligently synthesize literature insights and generate innovative and practical solutions for defect passivation and the selection of precursor additives. Experimental validation of several model-proposed strategies confirms their effectiveness in improving material stability and performance. Our work demonstrates the potential of domain-adapted LLMs in accelerating materials discovery and provides a closed-loop framework for intelligent, data-driven advancements in perovskite photovoltaic research.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via Distribution Modeling</title>
<link>https://arxiv.org/abs/2507.16329</link>
<guid>https://arxiv.org/abs/2507.16329</guid>
<content:encoded><![CDATA[
arXiv:2507.16329v1 Announce Type: cross 
Abstract: Despite the integration of safety alignment and external filters, text-to-image (T2I) generative models are still susceptible to producing harmful content, such as sexual or violent imagery. This raises serious concerns about unintended exposure and potential misuse. Red teaming, which aims to proactively identify diverse prompts that can elicit unsafe outputs from the T2I system (including the core generative model as well as potential external safety filters and other processing components), is increasingly recognized as an essential method for assessing and improving safety before real-world deployment. Yet, existing automated red teaming approaches often treat prompt discovery as an isolated, prompt-level optimization task, which limits their scalability, diversity, and overall effectiveness. To bridge this gap, in this paper, we propose DREAM, a scalable red teaming framework to automatically uncover diverse problematic prompts from a given T2I system. Unlike most prior works that optimize prompts individually, DREAM directly models the probabilistic distribution of the target system's problematic prompts, which enables explicit optimization over both effectiveness and diversity, and allows efficient large-scale sampling after training. To achieve this without direct access to representative training samples, we draw inspiration from energy-based models and reformulate the objective into simple and tractable objectives. We further introduce GC-SPSA, an efficient optimization algorithm that provide stable gradient estimates through the long and potentially non-differentiable T2I pipeline. The effectiveness of DREAM is validated through extensive experiments, demonstrating that it surpasses 9 state-of-the-art baselines by a notable margin across a broad range of T2I models and safety filters in terms of prompt success rate and diversity.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detect Any Sound: Open-Vocabulary Sound Event Detection with Multi-Modal Queries</title>
<link>https://arxiv.org/abs/2507.16343</link>
<guid>https://arxiv.org/abs/2507.16343</guid>
<content:encoded><![CDATA[
arXiv:2507.16343v1 Announce Type: cross 
Abstract: Most existing sound event detection~(SED) algorithms operate under a closed-set assumption, restricting their detection capabilities to predefined classes. While recent efforts have explored language-driven zero-shot SED by exploiting audio-language models, their performance is still far from satisfactory due to the lack of fine-grained alignment and cross-modal feature fusion. In this work, we propose the Detect Any Sound Model (DASM), a query-based framework for open-vocabulary SED guided by multi-modal queries. DASM formulates SED as a frame-level retrieval task, where audio features are matched against query vectors derived from text or audio prompts. To support this formulation, DASM introduces a dual-stream decoder that explicitly decouples event recognition and temporal localization: a cross-modality event decoder performs query-feature fusion and determines the presence of sound events at the clip-level, while a context network models temporal dependencies for frame-level localization. Additionally, an inference-time attention masking strategy is proposed to leverage semantic relations between base and novel classes, substantially enhancing generalization to novel classes. Experiments on the AudioSet Strong dataset demonstrate that DASM effectively balances localization accuracy with generalization to novel classes, outperforming CLAP-based methods in open-vocabulary setting (+ 7.8 PSDS) and the baseline in the closed-set setting (+ 6.9 PSDS). Furthermore, in cross-dataset zero-shot evaluation on DESED, DASM achieves a PSDS1 score of 42.2, even exceeding the supervised CRNN baseline. The project page is available at https://cai525.github.io/Transformer4SED/demo_page/DASM/.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Personalized PageRank and Higher-Order Topological Structures for Heterophily Mitigation in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2507.16347</link>
<guid>https://arxiv.org/abs/2507.16347</guid>
<content:encoded><![CDATA[
arXiv:2507.16347v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) excel in node classification tasks but often assume homophily, where connected nodes share similar labels. This assumption does not hold in many real-world heterophilic graphs. Existing models for heterophilic graphs primarily rely on pairwise relationships, overlooking multi-scale information from higher-order structures. This leads to suboptimal performance, particularly under noise from conflicting class information across nodes. To address these challenges, we propose HPGNN, a novel model integrating Higher-order Personalized PageRank with Graph Neural Networks. HPGNN introduces an efficient high-order approximation of Personalized PageRank (PPR) to capture long-range and multi-scale node interactions. This approach reduces computational complexity and mitigates noise from surrounding information. By embedding higher-order structural information into convolutional networks, HPGNN effectively models key interactions across diverse graph dimensions. Extensive experiments on benchmark datasets demonstrate HPGNN's effectiveness. The model achieves better performance than five out of seven state-of-the-art methods on heterophilic graphs in downstream tasks while maintaining competitive performance on homophilic graphs. HPGNN's ability to balance multi-scale information and robustness to noise makes it a versatile solution for real-world graph learning challenges. Codes are available at https://github.com/streetcorner/HPGNN.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth Gives a False Sense of Privacy: LLM Internal States Inversion</title>
<link>https://arxiv.org/abs/2507.16372</link>
<guid>https://arxiv.org/abs/2507.16372</guid>
<content:encoded><![CDATA[
arXiv:2507.16372v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into daily routines, yet they raise significant privacy and safety concerns. Recent research proposes collaborative inference, which outsources the early-layer inference to ensure data locality, and introduces model safety auditing based on inner neuron patterns. Both techniques expose the LLM's Internal States (ISs), which are traditionally considered irreversible to inputs due to optimization challenges and the highly abstract representations in deep layers. In this work, we challenge this assumption by proposing four inversion attacks that significantly improve the semantic similarity and token matching rate of inverted inputs. Specifically, we first develop two white-box optimization-based attacks tailored for low-depth and high-depth ISs. These attacks avoid local minima convergence, a limitation observed in prior work, through a two-phase inversion process. Then, we extend our optimization attack under more practical black-box weight access by leveraging the transferability between the source and the derived LLMs. Additionally, we introduce a generation-based attack that treats inversion as a translation task, employing an inversion model to reconstruct inputs. Extensive evaluation of short and long prompts from medical consulting and coding assistance datasets and 6 LLMs validates the effectiveness of our inversion attacks. Notably, a 4,112-token long medical consulting prompt can be nearly perfectly inverted with 86.88 F1 token matching from the middle layer of Llama-3 model. Finally, we evaluate four practical defenses that we found cannot perfectly prevent ISs inversion and draw conclusions for future mitigation design.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of LLM Guided Reinforcement Learning in Formation Control with Collision Avoidance</title>
<link>https://arxiv.org/abs/2507.16382</link>
<guid>https://arxiv.org/abs/2507.16382</guid>
<content:encoded><![CDATA[
arXiv:2507.16382v1 Announce Type: cross 
Abstract: Multi-Agent Systems (MAS) excel at accomplishing complex objectives through the collaborative efforts of individual agents. Among the methodologies employed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of the most efficacious algorithms. However, when confronted with the complex objective of Formation Control with Collision Avoidance (FCCA): designing an effective reward function that facilitates swift convergence of the policy network to an optimal solution. In this paper, we introduce a novel framework that aims to overcome this challenge. By giving large language models (LLMs) on the prioritization of tasks and the observable information available to each agent, our framework generates reward functions that can be dynamically adjusted online based on evaluation outcomes by employing more advanced evaluation metrics rather than the rewards themselves. This mechanism enables the MAS to simultaneously achieve formation control and obstacle avoidance in dynamic environments with enhanced efficiency, requiring fewer iterations to reach superior performance levels. Our empirical studies, conducted in both simulation and real-world settings, validate the practicality and effectiveness of our proposed approach.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Flat to Round: Redefining Brain Decoding with Surface-Based fMRI and Cortex Structure</title>
<link>https://arxiv.org/abs/2507.16389</link>
<guid>https://arxiv.org/abs/2507.16389</guid>
<content:encoded><![CDATA[
arXiv:2507.16389v1 Announce Type: cross 
Abstract: Reconstructing visual stimuli from human brain activity (e.g., fMRI) bridges neuroscience and computer vision by decoding neural representations. However, existing methods often overlook critical brain structure-function relationships, flattening spatial information and neglecting individual anatomical variations. To address these issues, we propose (1) a novel sphere tokenizer that explicitly models fMRI signals as spatially coherent 2D spherical data on the cortical surface; (2) integration of structural MRI (sMRI) data, enabling personalized encoding of individual anatomical variations; and (3) a positive-sample mixup strategy for efficiently leveraging multiple fMRI scans associated with the same visual stimulus. Collectively, these innovations enhance reconstruction accuracy, biological interpretability, and generalizability across individuals. Experiments demonstrate superior reconstruction performance compared to SOTA methods, highlighting the effectiveness and interpretability of our biologically informed approach.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Algorethics: Addressing the Ethical and Anthropological Challenges of AI Recommender Systems</title>
<link>https://arxiv.org/abs/2507.16430</link>
<guid>https://arxiv.org/abs/2507.16430</guid>
<content:encoded><![CDATA[
arXiv:2507.16430v1 Announce Type: cross 
Abstract: In this paper, I examine the ethical and anthropological challenges posed by AI-driven recommender systems (RSs), which have become central to shaping digital environments and social interactions. By curating personalized content, RSs do not merely reflect user preferences but actively construct individual experiences across social media, entertainment platforms, and e-commerce. Despite their ubiquity, the ethical implications of RSs remain insufficiently explored, even as concerns over privacy, autonomy, and mental well-being intensify. I argue that existing ethical approaches, including algorethics, the effort to embed ethical principles into algorithmic design, are necessary but ultimately inadequate. RSs inherently reduce human complexity to quantifiable dimensions, exploit user vulnerabilities, and prioritize engagement over well-being. Addressing these concerns requires moving beyond purely technical solutions. I propose a comprehensive framework for human-centered RS design, integrating interdisciplinary perspectives, regulatory strategies, and educational initiatives to ensure AI systems foster rather than undermine human autonomy and societal flourishing.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Treatment Effects with Independent Component Analysis</title>
<link>https://arxiv.org/abs/2507.16467</link>
<guid>https://arxiv.org/abs/2507.16467</guid>
<content:encoded><![CDATA[
arXiv:2507.16467v1 Announce Type: cross 
Abstract: The field of causal inference has developed a variety of methods to accurately estimate treatment effects in the presence of nuisance. Meanwhile, the field of identifiability theory has developed methods like Independent Component Analysis (ICA) to identify latent sources and mixing weights from data. While these two research communities have developed largely independently, they aim to achieve similar goals: the accurate and sample-efficient estimation of model parameters. In the partially linear regression (PLR) setting, Mackey et al. (2018) recently found that estimation consistency can be improved with non-Gaussian treatment noise. Non-Gaussianity is also a crucial assumption for identifying latent factors in ICA. We provide the first theoretical and empirical insights into this connection, showing that ICA can be used for causal effect estimation in the PLR model. Surprisingly, we find that linear ICA can accurately estimate multiple treatment effects even in the presence of Gaussian confounders or nonlinear nuisance.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing for Difference: How Human Characteristics Shape Perceptions of Collaborative Robots</title>
<link>https://arxiv.org/abs/2507.16480</link>
<guid>https://arxiv.org/abs/2507.16480</guid>
<content:encoded><![CDATA[
arXiv:2507.16480v1 Announce Type: cross 
Abstract: The development of assistive robots for social collaboration raises critical questions about responsible and inclusive design, especially when interacting with individuals from protected groups such as those with disabilities or advanced age. Currently, research is scarce on how participants assess varying robot behaviors in combination with diverse human needs, likely since participants have limited real-world experience with advanced domestic robots. In the current study, we aim to address this gap while using methods that enable participants to assess robot behavior, as well as methods that support meaningful reflection despite limited experience. In an online study, 112 participants (from both experimental and control groups) evaluated 7 videos from a total of 28 variations of human-robot collaboration types. The experimental group first completed a cognitive-affective mapping (CAM) exercise on human-robot collaboration before providing their ratings. Although CAM reflection did not significantly affect overall ratings, it led to more pronounced assessments for certain combinations of robot behavior and human condition. Most importantly, the type of human-robot collaboration influences the assessment. Antisocial robot behavior was consistently rated as the lowest, while collaboration with aged individuals elicited more sensitive evaluations. Scenarios involving object handovers were viewed more positively than those without them. These findings suggest that both human characteristics and interaction paradigms influence the perceived acceptability of collaborative robots, underscoring the importance of prosocial design. They also highlight the potential of reflective methods, such as CAM, to elicit nuanced feedback, supporting the development of user-centered and socially responsible robotic systems tailored to diverse populations.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2507.16488</link>
<guid>https://arxiv.org/abs/2507.16488</guid>
<content:encoded><![CDATA[
arXiv:2507.16488v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at various natural language processing tasks, but their tendency to generate hallucinations undermines their reliability. Existing hallucination detection methods leveraging hidden states predominantly focus on static and isolated representations, overlooking their dynamic evolution across layers, which limits efficacy. To address this limitation, we shift the focus to the hidden state update process and introduce a novel metric, the ICR Score (Information Contribution to Residual Stream), which quantifies the contribution of modules to the hidden states' update. We empirically validate that the ICR Score is effective and reliable in distinguishing hallucinations. Building on these insights, we propose a hallucination detection method, the ICR Probe, which captures the cross-layer evolution of hidden states. Experimental results show that the ICR Probe achieves superior performance with significantly fewer parameters. Furthermore, ablation studies and case analyses offer deeper insights into the underlying mechanism of this method, improving its interpretability.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analogy making as amortised model construction</title>
<link>https://arxiv.org/abs/2507.16511</link>
<guid>https://arxiv.org/abs/2507.16511</guid>
<content:encoded><![CDATA[
arXiv:2507.16511v1 Announce Type: cross 
Abstract: Humans flexibly construct internal models to navigate novel situations. To be useful, these internal models must be sufficiently faithful to the environment that resource-limited planning leads to adequate outcomes; equally, they must be tractable to construct in the first place. We argue that analogy plays a central role in these processes, enabling agents to reuse solution-relevant structure from past experiences and amortise the computational costs of both model construction (construal) and planning. Formalising analogies as partial homomorphisms between Markov decision processes, we sketch a framework in which abstract modules, derived from previous construals, serve as composable building blocks for new ones. This modular reuse allows for flexible adaptation of policies and representations across domains with shared structural essence.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ever-Evolving Science Exam</title>
<link>https://arxiv.org/abs/2507.16514</link>
<guid>https://arxiv.org/abs/2507.16514</guid>
<content:encoded><![CDATA[
arXiv:2507.16514v1 Announce Type: cross 
Abstract: As foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad **Range**, wide **Reach**, and high **Rigor**, yet they often face two major challenges: **data leakage risks** that compromise benchmarking validity, and **evaluation inefficiency** due to large-scale testing. To address these issues, we introduce the **Ever-Evolving Science Exam (EESE)**, a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. Our approach consists of two components: 1) a non-public **EESE-Pool** with over 100K expertly constructed science instances (question-answer pairs) across 5 disciplines and 500+ subfields, built through a multi-stage pipeline ensuring **Range**, **Reach**, and **Rigor**, 2) a periodically updated 500-instance subset **EESE**, sampled and validated to enable leakage-resilient, low-overhead evaluations. Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions. The project page is at: https://github.com/aiben-ch/EESE.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial 3D-LLM: Exploring Spatial Awareness in 3D Vision-Language Models</title>
<link>https://arxiv.org/abs/2507.16524</link>
<guid>https://arxiv.org/abs/2507.16524</guid>
<content:encoded><![CDATA[
arXiv:2507.16524v1 Announce Type: cross 
Abstract: New era has unlocked exciting possibilities for extending Large Language Models (LLMs) to tackle 3D vision-language tasks. However, most existing 3D multimodal LLMs (MLLMs) rely on compressing holistic 3D scene information or segmenting independent objects to perform these tasks, which limits their spatial awareness due to insufficient representation of the richness inherent in 3D scenes. To overcome these limitations, we propose Spatial 3D-LLM, a 3D MLLM specifically designed to enhance spatial awareness for 3D vision-language tasks by enriching the spatial embeddings of 3D scenes. Spatial 3D-LLM integrates an LLM backbone with a progressive spatial awareness scheme that progressively captures spatial information as the perception field expands, generating location-enriched 3D scene embeddings to serve as visual prompts. Furthermore, we introduce two novel tasks: 3D object distance measurement and 3D layout editing, and construct a 3D instruction dataset, MODEL, to evaluate the model's spatial awareness capabilities. Experimental results demonstrate that Spatial 3D-LLM achieves state-of-the-art performance across a wide range of 3D vision-language tasks, revealing the improvements stemmed from our progressive spatial awareness scheme of mining more profound spatial information. Our code is available at https://github.com/bjshuyuan/Spatial-3D-LLM.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>confopt: A Library for Implementation and Evaluation of Gradient-based One-Shot NAS Methods</title>
<link>https://arxiv.org/abs/2507.16533</link>
<guid>https://arxiv.org/abs/2507.16533</guid>
<content:encoded><![CDATA[
arXiv:2507.16533v1 Announce Type: cross 
Abstract: Gradient-based one-shot neural architecture search (NAS) has significantly reduced the cost of exploring architectural spaces with discrete design choices, such as selecting operations within a model. However, the field faces two major challenges. First, evaluations of gradient-based NAS methods heavily rely on the DARTS benchmark, despite the existence of other available benchmarks. This overreliance has led to saturation, with reported improvements often falling within the margin of noise. Second, implementations of gradient-based one-shot NAS methods are fragmented across disparate repositories, complicating fair and reproducible comparisons and further development. In this paper, we introduce Configurable Optimizer (confopt), an extensible library designed to streamline the development and evaluation of gradient-based one-shot NAS methods. Confopt provides a minimal API that makes it easy for users to integrate new search spaces, while also supporting the decomposition of NAS optimizers into their core components. We use this framework to create a suite of new DARTS-based benchmarks, and combine them with a novel evaluation protocol to reveal a critical flaw in how gradient-based one-shot NAS methods are currently assessed. The code can be found at https://github.com/automl/ConfigurableOptimizer.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion</title>
<link>https://arxiv.org/abs/2507.16535</link>
<guid>https://arxiv.org/abs/2507.16535</guid>
<content:encoded><![CDATA[
arXiv:2507.16535v1 Announce Type: cross 
Abstract: Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Graph Intelligence: Hypervector Message Passing for Learning Graph-Level Patterns with Tsetlin Machines</title>
<link>https://arxiv.org/abs/2507.16537</link>
<guid>https://arxiv.org/abs/2507.16537</guid>
<content:encoded><![CDATA[
arXiv:2507.16537v1 Announce Type: cross 
Abstract: We propose a multilayered symbolic framework for general graph classification that leverages sparse binary hypervectors and Tsetlin Machines. Each graph is encoded through structured message passing, where node, edge, and attribute information are bound and bundled into a symbolic hypervector. This process preserves the hierarchical semantics of the graph through layered binding from node attributes to edge relations to structural roles resulting in a compact, discrete representation. We also formulate a local interpretability framework which lends itself to a key advantage of our approach being locally interpretable. We validate our method on TUDataset benchmarks, demonstrating competitive accuracy with strong symbolic transparency compared to neural graph models.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Data-centric Overview of Federated Graph Learning</title>
<link>https://arxiv.org/abs/2507.16541</link>
<guid>https://arxiv.org/abs/2507.16541</guid>
<content:encoded><![CDATA[
arXiv:2507.16541v1 Announce Type: cross 
Abstract: In the era of big data applications, Federated Graph Learning (FGL) has emerged as a prominent solution that reconcile the tradeoff between optimizing the collective intelligence between decentralized datasets holders and preserving sensitive information to maximum. Existing FGL surveys have contributed meaningfully but largely focus on integrating Federated Learning (FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that emphasis on methodology and simulated scenarios. Notably, a data centric perspective, which systematically examines FGL methods through the lens of data properties and usage, remains unadapted to reorganize FGL research, yet it is critical to assess how FGL studies manage to tackle data centric constraints to enhance model performances. This survey propose a two-level data centric taxonomy: Data Characteristics, which categorizes studies based on the structural and distributional properties of datasets used in FGL, and Data Utilization, which analyzes the training procedures and techniques employed to overcome key data centric challenges. Each taxonomy level is defined by three orthogonal criteria, each representing a distinct data centric configuration. Beyond taxonomy, this survey examines FGL integration with Pretrained Large Models, showcases realistic applications, and highlights future direction aligned with emerging trends in GML.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of DNN-based HSI Segmentation FPGA-based SoC for ADS: A Practical Approach</title>
<link>https://arxiv.org/abs/2507.16556</link>
<guid>https://arxiv.org/abs/2507.16556</guid>
<content:encoded><![CDATA[
arXiv:2507.16556v1 Announce Type: cross 
Abstract: The use of HSI for autonomous navigation is a promising research field aimed at improving the accuracy and robustness of detection, tracking, and scene understanding systems based on vision sensors. Combining advanced computer algorithms, such as DNNs, with small-size snapshot HSI cameras enhances the reliability of these systems. HSI overcomes intrinsic limitations of greyscale and RGB imaging in depicting physical properties of targets, particularly regarding spectral reflectance and metamerism. Despite promising results in HSI-based vision developments, safety-critical systems like ADS demand strict constraints on latency, resource consumption, and security, motivating the shift of ML workloads to edge platforms. This involves a thorough software/hardware co-design scheme to distribute and optimize the tasks efficiently among the limited resources of computing platforms. With respect to inference, the over-parameterized nature of DNNs poses significant computational challenges for real-time on-the-edge deployment. In addition, the intensive data preprocessing required by HSI, which is frequently overlooked, must be carefully managed in terms of memory arrangement and inter-task communication to enable an efficient integrated pipeline design on a SoC. This work presents a set of optimization techniques for the practical co-design of a DNN-based HSI segmentation processor deployed on a FPGA-based SoC targeted at ADS, including key optimizations such as functional software/hardware task distribution, hardware-aware preprocessing, ML model compression, and a complete pipelined deployment. Applied compression techniques significantly reduce the complexity of the designed DNN to 24.34% of the original operations and to 1.02% of the original number of parameters, achieving a 2.86x speed-up in the inference task without noticeable degradation of the segmentation accuracy.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology: A User Study (Extended Version)</title>
<link>https://arxiv.org/abs/2507.16562</link>
<guid>https://arxiv.org/abs/2507.16562</guid>
<content:encoded><![CDATA[
arXiv:2507.16562v1 Announce Type: cross 
Abstract: In this paper, we present the findings of a user study that evaluated the social acceptance of eXtended Reality (XR) agent technology, focusing on a remotely accessible, web-based XR training system developed for journalists. This system involves user interaction with a virtual avatar, enabled by a modular toolkit. The interactions are designed to provide tailored training for journalists in digital-remote settings, especially for sensitive or dangerous scenarios, without requiring specialized end-user equipment like headsets. Our research adapts and extends the Almere model, representing social acceptance through existing attributes such as perceived ease of use and perceived usefulness, along with added ones like dependability and security in the user-agent interaction. The XR agent was tested through a controlled experiment in a real-world setting, with data collected on users' perceptions. Our findings, based on quantitative and qualitative measurements involving questionnaires, contribute to the understanding of user perceptions and acceptance of XR agent solutions within a specific social context, while also identifying areas for the improvement of XR systems.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTMBA: Towards Text To Multiple Sources Binaural Audio Generation</title>
<link>https://arxiv.org/abs/2507.16564</link>
<guid>https://arxiv.org/abs/2507.16564</guid>
<content:encoded><![CDATA[
arXiv:2507.16564v1 Announce Type: cross 
Abstract: Most existing text-to-audio (TTA) generation methods produce mono outputs, neglecting essential spatial information for immersive auditory experiences. To address this issue, we propose a cascaded method for text-to-multisource binaural audio generation (TTMBA) with both temporal and spatial control. First, a pretrained large language model (LLM) segments the text into a structured format with time and spatial details for each sound event. Next, a pretrained mono audio generation network creates multiple mono audios with varying durations for each event. These mono audios are transformed into binaural audios using a binaural rendering neural network based on spatial data from the LLM. Finally, the binaural audios are arranged by their start times, resulting in multisource binaural audio. Experimental results demonstrate the superiority of the proposed method in terms of both audio generation quality and spatial perceptual accuracy.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Adaptive Gradient Recovery for Unstructured Finite Volume Computations</title>
<link>https://arxiv.org/abs/2507.16571</link>
<guid>https://arxiv.org/abs/2507.16571</guid>
<content:encoded><![CDATA[
arXiv:2507.16571v1 Announce Type: cross 
Abstract: We present a novel data-driven approach for enhancing gradient reconstruction in unstructured finite volume methods for hyperbolic conservation laws, specifically for the 2D Euler equations. Our approach extends previous structured-grid methodologies to unstructured meshes through a modified DeepONet architecture that incorporates local geometry in the neural network. The architecture employs local mesh topology to ensure rotation invariance, while also ensuring first-order constraint on the learned operator. The training methodology incorporates physics-informed regularization through entropy penalization, total variation diminishing penalization, and parameter regularization to ensure physically consistent solutions, particularly in shock-dominated regions. The model is trained on high-fidelity datasets solutions derived from sine waves and randomized piecewise constant initial conditions with periodic boundary conditions, enabling robust generalization to complex flow configurations or geometries. Validation test cases from the literature, including challenging geometry configuration, demonstrates substantial improvements in accuracy compared to traditional second-order finite volume schemes. The method achieves gains of 20-60% in solution accuracy while enhancing computational efficiency. A convergence study has been conveyed and reveal improved mesh convergence rates compared to the conventional solver. The proposed algorithm is faster and more accurate than the traditional second-order finite volume solver, enabling high-fidelity simulations on coarser grids while preserving the stability and conservation properties essential for hyperbolic conservation laws. This work is a part of a new generation of solvers that are built by combining Machine-Learning (ML) tools with traditional numerical schemes, all while ensuring physical constraint on the results.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pyramid Hierarchical Masked Diffusion Model for Imaging Synthesis</title>
<link>https://arxiv.org/abs/2507.16579</link>
<guid>https://arxiv.org/abs/2507.16579</guid>
<content:encoded><![CDATA[
arXiv:2507.16579v1 Announce Type: cross 
Abstract: Medical image synthesis plays a crucial role in clinical workflows, addressing the common issue of missing imaging modalities due to factors such as extended scan times, scan corruption, artifacts, patient motion, and intolerance to contrast agents. The paper presents a novel image synthesis network, the Pyramid Hierarchical Masked Diffusion Model (PHMDiff), which employs a multi-scale hierarchical approach for more detailed control over synthesizing high-quality images across different resolutions and layers. Specifically, this model utilizes randomly multi-scale high-proportion masks to speed up diffusion model training, and balances detail fidelity and overall structure. The integration of a Transformer-based Diffusion model process incorporates cross-granularity regularization, modeling the mutual information consistency across each granularity's latent spaces, thereby enhancing pixel-level perceptual accuracy. Comprehensive experiments on two challenging datasets demonstrate that PHMDiff achieves superior performance in both the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), highlighting its capability to produce high-quality synthesized images with excellent structural integrity. Ablation studies further confirm the contributions of each component. Furthermore, the PHMDiff model, a multi-scale image synthesis framework across and within medical imaging modalities, shows significant advantages over other methods. The source code is available at https://github.com/xiaojiao929/PHMDiff
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up with Industry Demands? A Multivocal Literature Review</title>
<link>https://arxiv.org/abs/2507.16586</link>
<guid>https://arxiv.org/abs/2507.16586</guid>
<content:encoded><![CDATA[
arXiv:2507.16586v1 Announce Type: cross 
Abstract: Computer-Aided Engineering (CAE) enables simulation experts to optimize complex models, but faces challenges in user experience (UX) that limit efficiency and accessibility. While artificial intelligence (AI) has demonstrated potential to enhance CAE processes, research integrating these fields with a focus on UX remains fragmented. This paper presents a multivocal literature review (MLR) examining how AI enhances UX in CAE software across both academic research and industry implementations. Our analysis reveals significant gaps between academic explorations and industry applications, with companies actively implementing LLMs, adaptive UIs, and recommender systems while academic research focuses primarily on technical capabilities without UX validation. Key findings demonstrate opportunities in AI-powered guidance, adaptive interfaces, and workflow automation that remain underexplored in current research. By mapping the intersection of these domains, this study provides a foundation for future work to address the identified research gaps and advance the integration of AI to improve CAE user experience.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Experimental Study of Split-Learning TinyML on Ultra-Low-Power Edge/IoT Nodes</title>
<link>https://arxiv.org/abs/2507.16594</link>
<guid>https://arxiv.org/abs/2507.16594</guid>
<content:encoded><![CDATA[
arXiv:2507.16594v1 Announce Type: cross 
Abstract: Running deep learning inference directly on ultra-low-power edge/IoT nodes has been limited by the tight memory and compute budgets of microcontrollers. Split learning (SL) addresses this limitation in which it executes part of the inference process on the sensor and off-loads the remainder to a companion device. In the context of constrained devices and the related impact of low-power, over-the-air transport protocols, the performance of split learning remains largely unexplored. TO the best of our knowledge, this paper presents the first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards, designed to benchmark the over-the-air performance of split learning TinyML in edge/IoT environments. We benchmark the performance of a MobileNetV2 image recognition model, which is quantized to 8-bit integers, partitioned, and delivered to the nodes via over-the-air updates. The intermediate activations are exchanged through different wireless communication methods: ESP-NOW, BLE, and traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on identical hardware. Measurements show that splitting the model after block_16_project_BN layer generates a 5.66 kB tensor that traverses the link in 3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s. ESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery life further but increases latency beyond 10s.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models</title>
<link>https://arxiv.org/abs/2507.16642</link>
<guid>https://arxiv.org/abs/2507.16642</guid>
<content:encoded><![CDATA[
arXiv:2507.16642v1 Announce Type: cross 
Abstract: The auditing of financial documents, historically a labor-intensive process, stands on the precipice of transformation. AI-driven solutions have made inroads into streamlining this process by recommending pertinent text passages from financial reports to align with the legal requirements of accounting standards. However, a glaring limitation remains: these systems commonly fall short in verifying if the recommended excerpts indeed comply with the specific legal mandates. Hence, in this paper, we probe the efficiency of publicly available Large Language Models (LLMs) in the realm of regulatory compliance across different model configurations. We place particular emphasis on comparing cutting-edge open-source LLMs, such as Llama-2, with their proprietary counterparts like OpenAI's GPT models. This comparative analysis leverages two custom datasets provided by our partner PricewaterhouseCoopers (PwC) Germany. We find that the open-source Llama-2 70 billion model demonstrates outstanding performance in detecting non-compliance or true negative occurrences, beating all their proprietary counterparts. Nevertheless, proprietary models such as GPT-4 perform the best in a broad variety of scenarios, particularly in non-English contexts.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Contradiction as Self-Improvement: Mitigating the Generation-Understanding Gap in MLLMs</title>
<link>https://arxiv.org/abs/2507.16663</link>
<guid>https://arxiv.org/abs/2507.16663</guid>
<content:encoded><![CDATA[
arXiv:2507.16663v1 Announce Type: cross 
Abstract: Despite efforts to unify multimodal generation and understanding tasks in a single model, we show these MLLMs exhibit self-contradiction where generation produces images deemed misaligned with input prompts based on the model's own understanding. We define a Nonunified score that quantifies such self-contradiction. Our empirical results reveal that the self-contradiction mainly arises from weak generation that fails to align with prompts, rather than misunderstanding. This capability asymmetry indicates the potential of leveraging self-contradiction for self-improvement, where the stronger model understanding guides the weaker generation to mitigate the generation-understanding gap. Applying standard post-training methods (e.g., SFT, DPO) with such internal supervision successfully improves both generation and unification. We discover a co-improvement effect on both generation and understanding when only fine-tuning the generation branch, a phenomenon known in pre-training but underexplored in post-training. Our analysis shows improvements stem from better detection of false positives that are previously incorrectly identified as prompt-aligned. Theoretically, we show the aligned training dynamics between generation and understanding allow reduced prompt-misaligned generations to also improve mismatch detection in the understanding branch. Additionally, the framework reveals a potential risk of co-degradation under poor supervision-an overlooked phenomenon that is empirically validated in our experiments. Notably, we find intrinsic metrics like Nonunified score cannot distinguish co-degradation from co-improvement, which highlights the necessity of data quality check. Finally, we propose a curriculum-based strategy based on our findings that gradually introduces harder samples as the model improves, leading to better unification and improved MLLM generation and understanding.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs</title>
<link>https://arxiv.org/abs/2507.16672</link>
<guid>https://arxiv.org/abs/2507.16672</guid>
<content:encoded><![CDATA[
arXiv:2507.16672v1 Announce Type: cross 
Abstract: Generative, explainable, and flexible recommender systems, derived using Large Language Models (LLM) are promising and poorly adapted to the cold-start user situation, where there is little to no history of interaction. The current solutions i.e. supervised fine-tuning and collaborative filtering are dense-user-item focused and would be expensive to maintain and update. This paper introduces a meta-learning framework, that can be used to perform parameter-efficient prompt-tuning, to effectively personalize LLM-based recommender systems quickly at cold-start. The model learns soft prompt embeddings with first-order (Reptile) and second-order (MAML) optimization by treating each of the users as the tasks. As augmentations to the input tokens, these learnable vectors are the differentiable control variables that represent user behavioral priors. The prompts are meta-optimized through episodic sampling, inner-loop adaptation, and outer-loop generalization. On MovieLens-1M, Amazon Reviews, and Recbole, we can see that our adaptive model outperforms strong baselines in NDCG@10, HR@10, and MRR, and it runs in real-time (i.e., below 300 ms) on consumer GPUs. Zero-history personalization is also supported by this scalable solution, and its 275 ms rate of adaptation allows successful real-time risk profiling of financial systems by shortening detection latency and improving payment network stability. Crucially, the 275 ms adaptation capability can enable real-time risk profiling for financial institutions, reducing systemic vulnerability detection latency significantly versus traditional compliance checks. By preventing contagion in payment networks (e.g., Fedwire), the framework strengthens national financial infrastructure resilience.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PICACO: Pluralistic In-Context Value Alignment of LLMs via Total Correlation Optimization</title>
<link>https://arxiv.org/abs/2507.16679</link>
<guid>https://arxiv.org/abs/2507.16679</guid>
<content:encoded><![CDATA[
arXiv:2507.16679v1 Announce Type: cross 
Abstract: In-Context Learning has shown great potential for aligning Large Language Models (LLMs) with human values, helping reduce harmful outputs and accommodate diverse preferences without costly post-training, known as In-Context Alignment (ICA). However, LLMs' comprehension of input prompts remains agnostic, limiting ICA's ability to address value tensions--human values are inherently pluralistic, often imposing conflicting demands, e.g., stimulation vs. tradition. Current ICA methods therefore face the Instruction Bottleneck challenge, where LLMs struggle to reconcile multiple intended values within a single prompt, leading to incomplete or biased alignment. To address this, we propose PICACO, a novel pluralistic ICA method. Without fine-tuning, PICACO optimizes a meta-instruction that navigates multiple values to better elicit LLMs' understanding of them and improve their alignment. This is achieved by maximizing the total correlation between specified values and LLM responses, theoretically reinforcing value correlation while reducing distractive noise, resulting in effective value instructions. Extensive experiments on five value sets show that PICACO works well with both black-box and open-source LLMs, outperforms several recent strong baselines, and achieves a better balance across up to 8 distinct values.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Topic Extraction and Word Embedding Learning using row-stochastic DEDICOM</title>
<link>https://arxiv.org/abs/2507.16695</link>
<guid>https://arxiv.org/abs/2507.16695</guid>
<content:encoded><![CDATA[
arXiv:2507.16695v1 Announce Type: cross 
Abstract: The DEDICOM algorithm provides a uniquely interpretable matrix factorization method for symmetric and asymmetric square matrices. We employ a new row-stochastic variation of DEDICOM on the pointwise mutual information matrices of text corpora to identify latent topic clusters within the vocabulary and simultaneously learn interpretable word embeddings. We introduce a method to efficiently train a constrained DEDICOM algorithm and a qualitative evaluation of its topic modeling and word embedding performance.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FISHER: A Foundation Model for Multi-Modal Industrial Signal Comprehensive Representation</title>
<link>https://arxiv.org/abs/2507.16696</link>
<guid>https://arxiv.org/abs/2507.16696</guid>
<content:encoded><![CDATA[
arXiv:2507.16696v1 Announce Type: cross 
Abstract: With the rapid deployment of SCADA systems, how to effectively analyze industrial signals and detect abnormal states is an urgent need for the industry. Due to the significant heterogeneity of these signals, which we summarize as the M5 problem, previous works only focus on small sub-problems and employ specialized models, failing to utilize the synergies between modalities and the powerful scaling law. However, we argue that the M5 signals can be modeled in a unified manner due to the intrinsic similarity. As a result, we propose FISHER, a Foundation model for multi-modal Industrial Signal compreHEnsive Representation. To support arbitrary sampling rates, FISHER considers the increment of sampling rate as the concatenation of sub-band information. Specifically, FISHER takes the STFT sub-band as the modeling unit and adopts a teacher student SSL framework for pre-training. We also develop the RMIS benchmark, which evaluates the representations of M5 industrial signals on multiple health management tasks. Compared with top SSL models, FISHER showcases versatile and outstanding capabilities with a general performance gain up to 5.03%, along with much more efficient scaling curves. We also investigate the scaling law on downstream tasks and derive potential avenues for future works. FISHER is now open-sourced on https://github.com/jianganbai/FISHER
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation</title>
<link>https://arxiv.org/abs/2507.16704</link>
<guid>https://arxiv.org/abs/2507.16704</guid>
<content:encoded><![CDATA[
arXiv:2507.16704v1 Announce Type: cross 
Abstract: Desktop accessibility metadata enables AI agents to interpret screens and supports users who depend on tools like screen readers. Yet, many applications remain largely inaccessible due to incomplete or missing metadata provided by developers - our investigation shows that only 33% of applications on macOS offer full accessibility support. While recent work on structured screen representation has primarily addressed specific challenges, such as UI element detection or captioning, none has attempted to capture the full complexity of desktop interfaces by replicating their entire hierarchical structure. To bridge this gap, we introduce Screen2AX, the first framework to automatically create real-time, tree-structured accessibility metadata from a single screenshot. Our method uses vision-language and object detection models to detect, describe, and organize UI elements hierarchically, mirroring macOS's system-level accessibility structure. To tackle the limited availability of data for macOS desktop applications, we compiled and publicly released three datasets encompassing 112 macOS applications, each annotated for UI element detection, grouping, and hierarchical accessibility metadata alongside corresponding screenshots. Screen2AX accurately infers hierarchy trees, achieving a 77% F1 score in reconstructing a complete accessibility tree. Crucially, these hierarchy trees improve the ability of autonomous agents to interpret and interact with complex desktop interfaces. We introduce Screen2AX-Task, a benchmark specifically designed for evaluating autonomous agent task execution in macOS desktop environments. Using this benchmark, we demonstrate that Screen2AX delivers a 2.2x performance improvement over native accessibility representations and surpasses the state-of-the-art OmniParser V2 system on the ScreenSpot benchmark.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Risk and Quality Assurance: A RAG Chatbot for Improved Regulatory Compliance</title>
<link>https://arxiv.org/abs/2507.16711</link>
<guid>https://arxiv.org/abs/2507.16711</guid>
<content:encoded><![CDATA[
arXiv:2507.16711v1 Announce Type: cross 
Abstract: Risk and Quality (R&amp;Q) assurance in highly regulated industries requires constant navigation of complex regulatory frameworks, with employees handling numerous daily queries demanding accurate policy interpretation. Traditional methods relying on specialized experts create operational bottlenecks and limit scalability. We present a novel Retrieval Augmented Generation (RAG) system leveraging Large Language Models (LLMs), hybrid search and relevance boosting to enhance R&amp;Q query processing. Evaluated on 124 expert-annotated real-world queries, our actively deployed system demonstrates substantial improvements over traditional RAG approaches. Additionally, we perform an extensive hyperparameter analysis to compare and evaluate multiple configuration setups, delivering valuable insights to practitioners.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory</title>
<link>https://arxiv.org/abs/2507.16713</link>
<guid>https://arxiv.org/abs/2507.16713</guid>
<content:encoded><![CDATA[
arXiv:2507.16713v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have been widely adopted in robotics to enable autonomous planning. However, grounding VLMs, originally trained on internet data, to diverse real-world robots remains a challenge. This paper presents ExpTeach, a framework that grounds VLMs to physical robots by building a self-generated memory of real-world experiences. In ExpTeach, the VLM autonomously plans actions, verifies outcomes, reflects on failures, and adapts robot behaviors in a closed loop. The self-generated experiences during this process are then summarized into a long-term memory, enabling retrieval of learned knowledge to guide future tasks via retrieval-augmented generation (RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with an on-demand image annotation module. In experiments, we show that reflection improves success rates from 36% to 84% on four challenging robotic tasks and observe the emergence of intelligent object interactions, including creative tool use. Across extensive tests on 12 real-world scenarios (including eight unseen ones), we find that grounding with long-term memory boosts single-trial success rates from 22% to 80%, demonstrating the effectiveness and generalizability of ExpTeach.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAVine: Reality-Aligned Evaluation for Agentic Search</title>
<link>https://arxiv.org/abs/2507.16725</link>
<guid>https://arxiv.org/abs/2507.16725</guid>
<content:encoded><![CDATA[
arXiv:2507.16725v1 Announce Type: cross 
Abstract: Agentic search, as a more autonomous and adaptive paradigm of retrieval augmentation, is driving the evolution of intelligent search systems. However, existing evaluation frameworks fail to align well with the goals of agentic search. First, the complex queries commonly used in current benchmarks often deviate from realistic user search scenarios. Second, prior approaches tend to introduce noise when extracting ground truth for end-to-end evaluations, leading to distorted assessments at a fine-grained level. Third, most current frameworks focus solely on the quality of final answers, neglecting the evaluation of the iterative process inherent to agentic search. To address these limitations, we propose RAVine -- a Reality-Aligned eValuation framework for agentic LLMs with search. RAVine targets multi-point queries and long-form answers that better reflect user intents, and introduces an attributable ground truth construction strategy to enhance the accuracy of fine-grained evaluation. Moreover, RAVine examines model's interaction with search tools throughout the iterative process, and accounts for factors of efficiency. We benchmark a series of models using RAVine and derive several insights, which we hope will contribute to advancing the development of agentic search systems. The code and datasets are available at https://github.com/SwordFaith/RAVine.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-enhanced conversational agents for personalized asthma support Factors for engagement, value and efficacy</title>
<link>https://arxiv.org/abs/2507.16735</link>
<guid>https://arxiv.org/abs/2507.16735</guid>
<content:encoded><![CDATA[
arXiv:2507.16735v1 Announce Type: cross 
Abstract: Asthma-related deaths in the UK are the highest in Europe, and only 30% of patients access basic care. There is a need for alternative approaches to reaching people with asthma in order to provide health education, self-management support and bridges to care. Automated conversational agents (specifically, mobile chatbots) present opportunities for providing alternative and individually tailored access to health education, self-management support and risk self-assessment. But would patients engage with a chatbot, and what factors influence engagement? We present results from a patient survey (N=1257) devised by a team of asthma clinicians, patients, and technology developers, conducted to identify optimal factors for efficacy, value and engagement for a chatbot. Results indicate that most adults with asthma (53%) are interested in using a chatbot and the patients most likely to do so are those who believe their asthma is more serious and who are less confident about self-management. Results also indicate enthusiasm for 24/7 access, personalisation, and for WhatsApp as the preferred access method (compared to app, voice assistant, SMS or website). Obstacles to uptake include security/privacy concerns and skepticism of technological capabilities. We present detailed findings and consolidate these into 7 recommendations for developers for optimising efficacy of chatbot-based health support.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer Support</title>
<link>https://arxiv.org/abs/2507.16754</link>
<guid>https://arxiv.org/abs/2507.16754</guid>
<content:encoded><![CDATA[
arXiv:2507.16754v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown promise in assisting developers with code-related questions; however, LLMs carry the risk of generating unreliable answers. To address this, Retrieval-Augmented Generation (RAG) has been proposed to reduce the unreliability (i.e., hallucinations) of LLMs. However, designing effective pipelines remains challenging due to numerous design choices. In this paper, we construct a retrieval corpus of over 3 million Java and Python related Stack Overflow posts with accepted answers, and explore various RAG pipeline designs to answer developer questions, evaluating their effectiveness in generating accurate and reliable responses. More specifically, we (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants to answer questions that have historically similar matches, and (2) address new questions without any close prior matches by automatically lowering the similarity threshold during retrieval, thereby increasing the chance of finding partially relevant context and improving coverage for unseen cases. We find that implementing a RAG pipeline combining hypothetical-documentation-embedding (HyDE) with the full-answer context performs best in retrieving and answering similarcontent for Stack Overflow questions. Finally, we apply our optimal RAG pipeline to 4 open-source LLMs and compare the results to their zero-shot performance. Our findings show that RAG with our optimal RAG pipeline consistently outperforms zero-shot baselines across models, achieving higher scores for helpfulness, correctness, and detail with LLM-as-a-judge. These findings demonstrate that our optimal RAG pipelines robustly enhance answer quality for a wide range of developer queries including both previously seen and novel questions across different LLMs
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.16795</link>
<guid>https://arxiv.org/abs/2507.16795</guid>
<content:encoded><![CDATA[
arXiv:2507.16795v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Translation-Related Functional Sequences in 5'UTRs Using Interpretable Deep Learning Models</title>
<link>https://arxiv.org/abs/2507.16801</link>
<guid>https://arxiv.org/abs/2507.16801</guid>
<content:encoded><![CDATA[
arXiv:2507.16801v1 Announce Type: cross 
Abstract: Understanding how 5' untranslated regions (5'UTRs) regulate mRNA translation is critical for controlling protein expression and designing effective therapeutic mRNAs. While recent deep learning models have shown promise in predicting translational efficiency from 5'UTR sequences, most are constrained by fixed input lengths and limited interpretability. We introduce UTR-STCNet, a Transformer-based architecture for flexible and biologically grounded modeling of variable-length 5'UTRs. UTR-STCNet integrates a Saliency-Aware Token Clustering (SATC) module that iteratively aggregates nucleotide tokens into multi-scale, semantically meaningful units based on saliency scores. A Saliency-Guided Transformer (SGT) block then captures both local and distal regulatory dependencies using a lightweight attention mechanism. This combined architecture achieves efficient and interpretable modeling without input truncation or increased computational cost. Evaluated across three benchmark datasets, UTR-STCNet consistently outperforms state-of-the-art baselines in predicting mean ribosome load (MRL), a key proxy for translational efficiency. Moreover, the model recovers known functional elements such as upstream AUGs and Kozak motifs, highlighting its potential for mechanistic insight into translation regulation.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty</title>
<link>https://arxiv.org/abs/2507.16806</link>
<guid>https://arxiv.org/abs/2507.16806</guid>
<content:encoded><![CDATA[
arXiv:2507.16806v1 Announce Type: cross 
Abstract: When language models (LMs) are trained via reinforcement learning (RL) to generate natural language "reasoning chains", their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or "hallucinate") in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis</title>
<link>https://arxiv.org/abs/2507.16808</link>
<guid>https://arxiv.org/abs/2507.16808</guid>
<content:encoded><![CDATA[
arXiv:2507.16808v1 Announce Type: cross 
Abstract: Register Transfer Level(RTL) code optimization is crucial for achieving high performance and low power consumption in digital circuit design. However, traditional optimization methods often rely on manual tuning and heuristics, which can be time-consuming and error-prone. Recent studies proposed to leverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs can generate optimized code snippets based on natural language descriptions, potentially speeding up the optimization process. However, existing approaches have not thoroughly evaluated the effectiveness of LLM-Based code optimization methods for RTL code with complex timing logic. To address this gap, we conducted a comprehensive empirical investigation to assess the capability of LLM-Based RTL code optimization methods in handling RTL code with complex timing logic. In this study, we first propose a new benchmark for RTL optimization evaluation. It comprises four subsets, each corresponding to a specific area of RTL code optimization. Then we introduce a method based on metamorphosis to systematically evaluate the effectiveness of LLM-Based RTL code optimization methods.Our key insight is that the optimization effectiveness should remain consistent for semantically equivalent but more complex code. After intensive experiments, we revealed several key findings. (1) LLM-Based RTL optimization methods can effectively optimize logic operations and outperform existing compiler-based methods. (2) LLM-Based RTL optimization methods do not perform better than existing compiler-based methods on RTL code with complex timing logic, particularly in timing control flow optimization and clock domain optimization. This is primarily attributed to the challenges LLMs face in understanding timing logic in RTL code. Based on these findings, we provide insights for further research in leveraging LLMs for RTL code optimization.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning</title>
<link>https://arxiv.org/abs/2507.16812</link>
<guid>https://arxiv.org/abs/2507.16812</guid>
<content:encoded><![CDATA[
arXiv:2507.16812v1 Announce Type: cross 
Abstract: Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</title>
<link>https://arxiv.org/abs/2507.16815</link>
<guid>https://arxiv.org/abs/2507.16815</guid>
<content:encoded><![CDATA[
arXiv:2507.16815v1 Announce Type: cross 
Abstract: Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward A Causal Framework for Modeling Perception</title>
<link>https://arxiv.org/abs/2401.13408</link>
<guid>https://arxiv.org/abs/2401.13408</guid>
<content:encoded><![CDATA[
arXiv:2401.13408v3 Announce Type: replace 
Abstract: Perception occurs when individuals interpret the same information differently. It is a known cognitive phenomenon with implications for bias in human decision-making. Perception, however, remains understudied in machine learning (ML). This is problematic as modern decision flows, whether partially or fully automated by ML applications, always involve human experts. How might we account for cases in which two experts, e.g., interpret differently the same deferred instance or explanation from a ML model? Addressing this and similar questions requires a formulation of perception, particularly, in a manner that integrates with ML-enabled decision flows. In this work, we present a first approach to modeling perception causally. We define perception under causal reasoning using structural causal models (SCM). Our approach formalizes individual experience as additional causal knowledge that comes with and is used by the expert decision-maker in the form of a SCM. We define two kinds of probabilistic causal perception: structural perception and parametrical perception. We showcase our framework through a series of examples of modern decision flows. We also emphasize the importance of addressing perception in fair ML, discussing relevant fairness implications and possible applications.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry</title>
<link>https://arxiv.org/abs/2403.04311</link>
<guid>https://arxiv.org/abs/2403.04311</guid>
<content:encoded><![CDATA[
arXiv:2403.04311v3 Announce Type: replace 
Abstract: Compound AI applications chain together subcomponents such as generative language models, document retrievers, and embedding models. Applying traditional systems optimizations such as parallelism and pipelining in compound AI systems is difficult because each component has different constraints in terms of the granularity and type of data that it ingests. New data is often generated during intermediate computations, and text streams may be split into smaller, independent fragments (such as documents to sentences) which may then be re-aggregated at later parts of the computation. Due to this complexity, existing systems to serve compound AI queries do not fully take advantage of parallelism and pipelining opportunities. We present Alto, a framework that automatically optimizes execution of compound AI queries through streaming and parallelism. Bento introduces a new abstraction called nested ancestry, a metadata hierarchy that allows the system to correctly track partial outputs and aggregate data across the heterogeneous constraints of the components of compound AI applications. This metadata is automatically inferred from the programming model, allowing developers to express complex dataflow patterns without needing to reason manually about the details of routing and aggregation. Implementations of four applications in Alto outperform or match implementations in LangGraph, a popular existing AI programming framework. Alto implementations match or improve latency by between 10-30%.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Strategy Learning by Decoupling Searching and Pathfinding for Object Navigation</title>
<link>https://arxiv.org/abs/2406.14103</link>
<guid>https://arxiv.org/abs/2406.14103</guid>
<content:encoded><![CDATA[
arXiv:2406.14103v2 Announce Type: replace 
Abstract: Inspired by human-like behaviors for navigation: first searching to explore unknown areas before discovering the target, and then the pathfinding of moving towards the discovered target, recent studies design parallel submodules to achieve different functions in the searching and pathfinding stages, while ignoring the differences in reward signals between the two stages. As a result, these models often cannot be fully trained or are overfitting on training scenes. Another bottleneck that restricts agents from learning two-stage strategies is spatial perception ability, since the studies used generic visual encoders without considering the depth information of navigation scenes. To release the potential of the model on strategy learning, we propose the Two-Stage Reward Mechanism (TSRM) for object navigation that decouples the searching and pathfinding behaviours in an episode, enabling the agent to explore larger area in searching stage and seek the optimal path in pathfinding stage. Also, we propose a pretraining method Depth Enhanced Masked Autoencoders (DE-MAE) that enables agent to determine explored and unexplored areas during the searching stage, locate target object and plan paths during the pathfinding stage more accurately. In addition, we propose a new metric of Searching Success weighted by Searching Path Length (SSSPL) that assesses agent's searching ability and exploring efficiency. Finally, we evaluated our method on AI2-Thor and RoboTHOR extensively and demonstrated it can outperform the state-of-the-art (SOTA) methods in both the success rate and the navigation efficiency.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R2D2: Remembering, Replaying and Dynamic Decision Making with a Reflective Agentic Memory</title>
<link>https://arxiv.org/abs/2501.12485</link>
<guid>https://arxiv.org/abs/2501.12485</guid>
<content:encoded><![CDATA[
arXiv:2501.12485v3 Announce Type: replace 
Abstract: The proliferation of web agents necessitates advanced navigation and interaction strategies within complex web environments. Current models often struggle with efficient navigation and action execution due to limited visibility and understanding of web structures. Our proposed R2D2 framework addresses these challenges by integrating two paradigms: Remember and Reflect. The Remember paradigm uses a replay buffer that aids agents in reconstructing the web environment dynamically, thus enabling the formulation of a detailed "map" of previously visited pages. This helps in reducing navigational errors and optimizing the decision-making process during web interactions. Conversely, the Reflect paradigm allows agents to learn from past mistakes by providing a mechanism for error analysis and strategy refinement, enhancing overall task performance. We evaluate R2D2 using the WebArena benchmark, demonstrating substantial improvements over existing methods, including a 50% reduction in navigation errors and a threefold increase in task completion rates. Our findings suggest that a combination of memory-enhanced navigation and reflective learning promisingly advances the capabilities of web agents, potentially benefiting various applications such as automated customer service and personal digital assistants.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InternAgent: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification</title>
<link>https://arxiv.org/abs/2505.16938</link>
<guid>https://arxiv.org/abs/2505.16938</guid>
<content:encoded><![CDATA[
arXiv:2505.16938v3 Announce Type: replace 
Abstract: Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce InternAgent, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. InternAgent highlights three key advantages: 1) Scalability: InternAgent has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: InternAgent provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: InternAgent has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.65 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph</title>
<link>https://arxiv.org/abs/2505.19956</link>
<guid>https://arxiv.org/abs/2505.19956</guid>
<content:encoded><![CDATA[
arXiv:2505.19956v2 Announce Type: replace 
Abstract: Text-to-SQL, which translates a natural language question into an SQL query, has advanced with in-context learning of Large Language Models (LLMs). However, existing methods show little improvement in performance compared to randomly chosen demonstrations, and significant performance drops when smaller LLMs (e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely on the intrinsic capabilities of hyper-scaled LLMs, rather than effectively retrieving useful demonstrations. In this paper, we propose a novel approach for effectively retrieving demonstrations and generating SQL queries. We construct a Deep Contextual Schema Link Graph, which contains key information and semantic relationship between a question and its database schema items. This graph-based structure enables effective representation of Text-to-SQL samples and retrieval of useful demonstrations for in-context learning. Experimental results on the Spider benchmark demonstrate the effectiveness of our approach, showing consistent improvements in SQL generation performance and efficiency across both hyper-scaled LLMs and small LLMs. The code is available at https://github.com/jjklle/DCG-SQL}{https://github.com/jjklle/DCG-SQL.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning</title>
<link>https://arxiv.org/abs/2506.02139</link>
<guid>https://arxiv.org/abs/2506.02139</guid>
<content:encoded><![CDATA[
arXiv:2506.02139v3 Announce Type: replace 
Abstract: Large language models (LLMs) are vast repositories of latent patterns, but without structured guidance, they lack explicit reasoning, semantic grounding, and goal-directed intelligence. We propose Unified Cognitive Consciousness Theory (UCCT), a unified model that reinterprets LLMs as unconscious substrates requiring external mechanisms, few-shot prompting, RAG, fine-tuning, and multi-agent reasoning, to semantically anchor latent representations. UCCT formalizes this anchoring process through a Bayesian formulation, revealing a threshold-crossing dynamic characterized by 1/sqrt(n) scaling that explains the sudden capability transitions observed across diverse tasks. The theory unifies previously disparate techniques, few-shot prompting, RAG, fine-tuning, and multi-agent reasoning, as special cases of a general anchoring architecture. Through case studies in simple math, visual recognition, and structured debate tasks, we confirm the predictive power of UCCT. Furthermore, our experiment in arithmetic in three numeral systems validates the theories of UCCT. Rather than treating intelligence as an intrinsic property of LLMs, UCCT demonstrates that LLMs are merely unconscious pattern repositories with no inherent intelligence. Intelligence emerges only when external anchoring mechanisms assign target semantics to these latent patterns, transforming unconscious representations into conscious, goal-directed capabilities.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Reasoning Model</title>
<link>https://arxiv.org/abs/2506.21734</link>
<guid>https://arxiv.org/abs/2506.21734</guid>
<content:encoded><![CDATA[
arXiv:2506.21734v2 Announce Type: replace 
Abstract: Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis</title>
<link>https://arxiv.org/abs/2507.07893</link>
<guid>https://arxiv.org/abs/2507.07893</guid>
<content:encoded><![CDATA[
arXiv:2507.07893v2 Announce Type: replace 
Abstract: This research presents a framework combining prompt engineering with multidimensional knowledge graphs to improve LLMs' legal dispute analysis. Specifically, the framework includes a three-stage hierarchical prompt structure (task definition, knowledge background, reasoning guidance) along with a three-layer knowledge graph (legal ontology, representation, instance layers). Additionally, four supporting methods enable precise legal concept retrieval: direct code matching, semantic vector similarity, ontology path reasoning, and lexical segmentation. Through extensive testing, results show major improvements: sensitivity increased by 9.9%-13.8%, specificity by 4.8%-6.7%, and citation accuracy by 22.4%-39.7%. As a result, the framework provides better legal analysis and understanding of judicial logic, thus offering a new technical method for intelligent legal assistance systems.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis</title>
<link>https://arxiv.org/abs/2507.08529</link>
<guid>https://arxiv.org/abs/2507.08529</guid>
<content:encoded><![CDATA[
arXiv:2507.08529v2 Announce Type: replace 
Abstract: Rare disease diagnosis remains challenging for medical large language models due to insufficient knowledge representation, limited concept understanding, and constrained clinical reasoning. We propose a framework combining multi-granularity sparse activation with hierarchical knowledge graphs. Our approach employs four complementary matching algorithms with diversity control and a five-level fallback strategy for precise concept activation. A three-layer knowledge graph (taxonomy, clinical features, instances) provides structured, up-to-date context. Experiments on the BioASQ rare disease dataset demonstrate significant improvements: BLEU scores increased by up to 0.13, ROUGE by up to 0.10, and diagnostic accuracy by up to 0.25, with the best model achieving 0.92 accuracy--surpassing the 0.90 clinical threshold. Expert evaluation confirms enhancements in information quality, reasoning, and professional expression. Our framework shows promise in reducing the diagnostic odyssey for rare disease patients.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient and Real-Time Sensing for Federated Continual Learning via Sample-Driven Control</title>
<link>https://arxiv.org/abs/2310.07497</link>
<guid>https://arxiv.org/abs/2310.07497</guid>
<content:encoded><![CDATA[
arXiv:2310.07497v2 Announce Type: replace-cross 
Abstract: An intelligent Real-Time Sensing (RTS) system must continuously acquire, update, integrate, and apply knowledge to adapt to real-world dynamics. Managing distributed intelligence in this context requires Federated Continual Learning (FCL). However, effectively capturing the diverse characteristics of RTS data in FCL systems poses significant challenges, including severely impacting computational and communication resources, escalating energy costs, and ultimately degrading overall system performance. To overcome these challenges, we investigate how the data distribution shift from ideal to practical RTS scenarios affects Artificial Intelligence (AI) model performance by leveraging the \textit{generalization gap} concept. In this way, we can analyze how sampling time in RTS correlates with the decline in AI performance, computation cost, and communication efficiency. Based on this observation, we develop a novel Sample-driven Control for Federated Continual Learning (SCFL) technique, specifically designed for mobile edge networks with RTS capabilities. In particular, SCFL is an optimization problem that harnesses the sampling process to concurrently minimize the generalization gap and improve overall accuracy while upholding the energy efficiency of the FCL framework. To solve the highly complex and time-varying optimization problem, we introduce a new soft actor-critic algorithm with explicit and implicit constraints (A2C-EI). Our empirical experiments reveal that we can achieve higher efficiency compared to other DRL baselines. Notably, SCFL can significantly reduce energy consumption up to $85\%$ while maintaining FL convergence and timely data transmission.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risks of AI Scientists: Prioritizing Safeguarding Over Autonomy</title>
<link>https://arxiv.org/abs/2402.04247</link>
<guid>https://arxiv.org/abs/2402.04247</guid>
<content:encoded><![CDATA[
arXiv:2402.04247v5 Announce Type: replace-cross 
Abstract: AI scientists powered by large language models have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, these agents also introduce novel vulnerabilities that require careful consideration for safety. However, there has been limited comprehensive exploration of these vulnerabilities. This perspective examines vulnerabilities in AI scientists, shedding light on potential risks associated with their misuse, and emphasizing the need for safety measures. We begin by providing an overview of the potential risks inherent to AI scientists, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we explore the underlying causes of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding AI scientists and advocate for the development of improved models, robust benchmarks, and comprehensive regulations.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Insights into Knowledge Distillation for Pre-Trained Models</title>
<link>https://arxiv.org/abs/2402.14922</link>
<guid>https://arxiv.org/abs/2402.14922</guid>
<content:encoded><![CDATA[
arXiv:2402.14922v2 Announce Type: replace-cross 
Abstract: This research investigates the enhancement of knowledge distillation (KD) processes in pre-trained models, an emerging field in knowledge transfer with significant implications for distributed training and federated learning environments. These environments benefit from reduced communication demands and accommodate various model architectures. Despite the adoption of numerous KD approaches for transferring knowledge among pre-trained models, a comprehensive understanding of KD's application in these scenarios is lacking. Our study conducts an extensive comparison of multiple KD techniques, including standard KD, tuned KD (via optimized temperature and weight parameters), deep mutual learning, and data partitioning KD. We assess these methods across various data distribution strategies to identify the most effective contexts for each. Through detailed examination of hyperparameter tuning, informed by extensive grid search evaluations, we pinpoint when adjustments are crucial to enhance model performance. This paper sheds light on optimal hyperparameter settings for distinct data partitioning scenarios and investigates KD's role in improving federated learning by minimizing communication rounds and expediting the training process. By filling a notable void in current research, our findings serve as a practical framework for leveraging KD in pre-trained models within collaborative and federated learning frameworks.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LangBiTe: A Platform for Testing Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2404.18558</link>
<guid>https://arxiv.org/abs/2404.18558</guid>
<content:encoded><![CDATA[
arXiv:2404.18558v2 Announce Type: replace-cross 
Abstract: The integration of Large Language Models (LLMs) into various software applications raises concerns about their potential biases. Typically, those models are trained on a vast amount of data scrapped from forums, websites, social media and other internet sources, which may instill harmful and discriminating behavior into the model. To address this issue, we present LangBiTe, a testing platform to systematically assess the presence of biases within an LLM. LangBiTe enables development teams to tailor their test scenarios, and automatically generate and execute the test cases according to a set of user-defined ethical requirements. Each test consists of a prompt fed into the LLM and a corresponding test oracle that scrutinizes the LLM's response for the identification of biases. LangBite provides users with the bias evaluation of LLMs, and end-to-end traceability between the initial ethical requirements and the insights obtained.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unisolver: PDE-Conditional Transformers Are Universal PDE Solvers</title>
<link>https://arxiv.org/abs/2405.17527</link>
<guid>https://arxiv.org/abs/2405.17527</guid>
<content:encoded><![CDATA[
arXiv:2405.17527v4 Announce Type: replace-cross 
Abstract: Deep models have recently emerged as promising tools to solve partial differential equations (PDEs), known as neural PDE solvers. While neural solvers trained from either simulation data or physics-informed loss can solve PDEs reasonably well, they are mainly restricted to a few instances of PDEs, e.g. a certain equation with a limited set of coefficients. This limits their generalization to diverse PDEs, preventing them from being practical surrogate models of numerical solvers. In this paper, we present Unisolver, a novel Transformer model trained on diverse data and conditioned on diverse PDEs, aiming towards a universal neural PDE solver capable of solving a wide scope of PDEs. Instead of purely scaling up data and parameters, Unisolver stems from the theoretical analysis of the PDE-solving process. Inspired by the mathematical structure of PDEs that a PDE solution is fundamentally governed by a series of PDE components such as equation symbols and boundary conditions, we define a complete set of PDE components and flexibly embed them as domain-wise and point-wise deep conditions for Transformer PDE solvers. Integrating physical insights with recent Transformer advances, Unisolver achieves consistent state-of-the-art on three challenging large-scale benchmarks, showing impressive performance and generalizability. Code is available at https://github.com/thuml/Unisolver.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of the 2024 BraTS Meningioma Radiotherapy Planning Automated Segmentation Challenge</title>
<link>https://arxiv.org/abs/2405.18383</link>
<guid>https://arxiv.org/abs/2405.18383</guid>
<content:encoded><![CDATA[
arXiv:2405.18383v3 Announce Type: replace-cross 
Abstract: The 2024 Brain Tumor Segmentation Meningioma Radiotherapy (BraTS-MEN-RT) challenge aimed to advance automated segmentation algorithms using the largest known multi-institutional dataset of 750 radiotherapy planning brain MRIs with expert-annotated target labels for patients with intact or postoperative meningioma that underwent either conventional external beam radiotherapy or stereotactic radiosurgery. Each case included a defaced 3D post-contrast T1-weighted radiotherapy planning MRI in its native acquisition space, accompanied by a single-label "target volume" representing the gross tumor volume (GTV) and any at-risk post-operative site. Target volume annotations adhered to established radiotherapy planning protocols, ensuring consistency across cases and institutions, and were approved by expert neuroradiologists and radiation oncologists. Six participating teams developed, containerized, and evaluated automated segmentation models using this comprehensive dataset. Team rankings were assessed using a modified lesion-wise Dice Similarity Coefficient (DSC) and 95% Hausdorff Distance (95HD). The best reported average lesion-wise DSC and 95HD was 0.815 and 26.92 mm, respectively. BraTS-MEN-RT is expected to significantly advance automated radiotherapy planning by enabling precise tumor segmentation and facilitating tailored treatment, ultimately improving patient outcomes. We describe the design and results from the BraTS-MEN-RT challenge.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAIN: Mitigating Backdoor Attacks in Federated Learning via Flipping Weight Updates of Low-Activation Input Neurons</title>
<link>https://arxiv.org/abs/2408.08655</link>
<guid>https://arxiv.org/abs/2408.08655</guid>
<content:encoded><![CDATA[
arXiv:2408.08655v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) enables multiple clients to collaboratively train machine learning models under the coordination of a central server, while maintaining privacy. However, the server cannot directly monitor the local training processes, leaving room for malicious clients to introduce backdoors into the model. Research has shown that backdoor attacks exploit specific neurons that are activated only by malicious inputs, remaining dormant with clean data. Building on this insight, we propose a novel defense method called Flipping Weight Updates of Low-Activation Input Neurons (FLAIN) to counter backdoor attacks in FL. Specifically, upon the completion of global training, we use an auxiliary dataset to identify low-activation input neurons and iteratively flip their associated weight updates. This flipping process continues while progressively raising the threshold for low-activation neurons, until the model's performance on the auxiliary data begins to degrade significantly. Extensive experiments demonstrate that FLAIN effectively reduces the success rate of backdoor attacks across a variety of scenarios, including Non-IID data distributions and high malicious client ratios (MCR), while maintaining minimal impact on the performance of clean data.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-RoAst: Visual Road Assessment. Can VLM be a Road Safety Assessor Using the iRAP Standard?</title>
<link>https://arxiv.org/abs/2408.10872</link>
<guid>https://arxiv.org/abs/2408.10872</guid>
<content:encoded><![CDATA[
arXiv:2408.10872v4 Announce Type: replace-cross 
Abstract: Road safety assessments are critical yet costly, especially in Low- and Middle-Income Countries (LMICs), where most roads remain unrated. Traditional methods require expert annotation and training data, while supervised learning-based approaches struggle to generalise across regions. In this paper, we introduce \textit{V-RoAst}, a zero-shot Visual Question Answering (VQA) framework using Vision-Language Models (VLMs) to classify road safety attributes defined by the iRAP standard. We introduce the first open-source dataset from ThaiRAP, consisting of over 2,000 curated street-level images from Thailand annotated for this task. We evaluate Gemini-1.5-flash and GPT-4o-mini on this dataset and benchmark their performance against VGGNet and ResNet baselines. While VLMs underperform on spatial awareness, they generalise well to unseen classes and offer flexible prompt-based reasoning without retraining. Our results show that VLMs can serve as automatic road assessment tools when integrated with complementary data. This work is the first to explore VLMs for zero-shot infrastructure risk assessment and opens new directions for automatic, low-cost road safety mapping. Code and dataset: https://github.com/PongNJ/V-RoAst.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From homeostasis to resource sharing: Biologically and economically aligned multi-objective multi-agent AI safety benchmarks</title>
<link>https://arxiv.org/abs/2410.00081</link>
<guid>https://arxiv.org/abs/2410.00081</guid>
<content:encoded><![CDATA[
arXiv:2410.00081v4 Announce Type: replace-cross 
Abstract: Developing safe, aligned agentic AI systems requires comprehensive empirical testing, yet many existing benchmarks neglect crucial themes aligned with biology and economics, both time-tested fundamental sciences describing our needs and preferences. To address this gap, the present work focuses on introducing biologically and economically motivated themes that have been neglected in current mainstream discussions on AI safety - namely a set of multi-objective, multi-agent alignment benchmarks that emphasize homeostasis for bounded and biological objectives, diminishing returns for unbounded, instrumental, and business objectives, sustainability principle, and resource sharing. We implemented eight main benchmark environments on the above themes, to illustrate key pitfalls and challenges in agentic AI-s, such as unboundedly maximizing a homeostatic objective, over-optimizing one objective at the expense of others, neglecting safety constraints, or depleting shared resources.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VitaGlyph: Vitalizing Artistic Typography with Flexible Dual-branch Diffusion Models</title>
<link>https://arxiv.org/abs/2410.01738</link>
<guid>https://arxiv.org/abs/2410.01738</guid>
<content:encoded><![CDATA[
arXiv:2410.01738v3 Announce Type: replace-cross 
Abstract: Artistic typography is a technique to visualize the meaning of input character in an imaginable and readable manner. With powerful text-to-image diffusion models, existing methods directly design the overall geometry and texture of input character, making it challenging to ensure both creativity and legibility. In this paper, we introduce a dual-branch, training-free method called VitaGlyph, enabling flexible artistic typography with controllable geometry changes while maintaining the readability. The key insight of VitaGlyph is to treat input character as a scene composed of a Subject and its Surrounding, which are rendered with varying degrees of geometric transformation. To enhance the visual appeal and creativity of the generated artistic typography, the subject flexibly expresses the essential concept of the input character, while the surrounding enriches relevant background without altering the shape, thus maintaining overall readability. Specifically, we implement VitaGlyph through a three-phase framework: (i) Knowledge Acquisition leverages large language models to design text descriptions for the subject and surrounding. (ii) Regional Interpretation detects the part that most closely matches the subject description and refines the structure via Semantic Typography. (iii) Attentional Compositional Generation separately renders the textures of the Subject and Surrounding regions and blends them in an attention-based manner. Experimental results demonstrate that VitaGlyph not only achieves better artistry and readability but also manages to depict multiple customized concepts, facilitating more creative and pleasing artistic typography generation. Our code will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning AI with Public Values: Deliberation and Decision-Making for Governing Multimodal LLMs in Political Video Analysis</title>
<link>https://arxiv.org/abs/2410.01817</link>
<guid>https://arxiv.org/abs/2410.01817</guid>
<content:encoded><![CDATA[
arXiv:2410.01817v2 Announce Type: replace-cross 
Abstract: How AI models should deal with political topics has been discussed, but it remains challenging and requires better governance. This paper examines the governance of large language models through individual and collective deliberation, focusing on politically sensitive videos. We conducted a two-step study: interviews with 10 journalists established a baseline understanding of expert video interpretation; 114 individuals through deliberation using InclusiveAI, a platform that facilitates democratic decision-making through decentralized autonomous organization (DAO) mechanisms. Our findings reveal distinct differences in interpretative priorities: while experts emphasized emotion and narrative, the general public prioritized factual clarity, objectivity, and emotional neutrality. Furthermore, we examined how different governance mechanisms - quadratic vs. weighted voting and equal vs. 20/80 voting power - shape users' decision-making regarding AI behavior. Results indicate that voting methods significantly influence outcomes, with quadratic voting reinforcing perceptions of liberal democracy and political equality. Our study underscores the necessity of selecting appropriate governance mechanisms to better capture user perspectives and suggests decentralized AI governance as a potential way to facilitate broader public engagement in AI development, ensuring that varied perspectives meaningfully inform design decisions.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LibEER: A Comprehensive Benchmark and Algorithm Library for EEG-based Emotion Recognition</title>
<link>https://arxiv.org/abs/2410.09767</link>
<guid>https://arxiv.org/abs/2410.09767</guid>
<content:encoded><![CDATA[
arXiv:2410.09767v3 Announce Type: replace-cross 
Abstract: EEG-based emotion recognition (EER) has gained significant attention due to its potential for understanding and analyzing human emotions. While recent advancements in deep learning techniques have substantially improved EER, the field lacks a convincing benchmark and comprehensive open-source libraries. This absence complicates fair comparisons between models and creates reproducibility challenges for practitioners, which collectively hinder progress. To address these issues, we introduce LibEER, a comprehensive benchmark and algorithm library designed to facilitate fair comparisons in EER. LibEER carefully selects popular and powerful baselines, harmonizes key implementation details across methods, and provides a standardized codebase in PyTorch. By offering a consistent evaluation framework with standardized experimental settings, LibEER enables unbiased assessments of seventeen representative deep learning models for EER across the six most widely used datasets. Additionally, we conduct a thorough, reproducible comparison of model performance and efficiency, providing valuable insights to guide researchers in the selection and design of EER models. Moreover, we make observations and in-depth analysis on the experiment results and identify current challenges in this community. We hope that our work will not only lower entry barriers for newcomers to EEG-based emotion recognition but also contribute to the standardization of research in this domain, fostering steady development. The library and source code are publicly available at https://github.com/XJTU-EEG/LibEER.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atomic Calibration of LLMs in Long-Form Generations</title>
<link>https://arxiv.org/abs/2410.13246</link>
<guid>https://arxiv.org/abs/2410.13246</guid>
<content:encoded><![CDATA[
arXiv:2410.13246v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often suffer from hallucinations, posing significant challenges for real-world applications. Confidence calibration, which estimates the underlying uncertainty of model predictions, is essential to enhance the LLMs' trustworthiness. Existing research on LLM calibration has primarily focused on short-form tasks, providing a single confidence score at the response level (macro calibration). However, this approach is insufficient for long-form generations, where responses often contain more complex statements and may include both accurate and inaccurate information. Therefore, we introduce atomic calibration, a novel approach that evaluates factuality calibration at a fine-grained level by breaking down long responses into atomic claims. We classify confidence elicitation methods into discriminative and generative types and demonstrate that their combination can enhance calibration. Our extensive experiments on various LLMs and datasets show that atomic calibration is well-suited for long-form generation and can also improve macro calibration results. Additionally, atomic calibration reveals insightful patterns in LLM confidence throughout the generation process.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Universal 3D Medical Multi-modality Generalization via Learning Personalized Invariant Representation</title>
<link>https://arxiv.org/abs/2411.06106</link>
<guid>https://arxiv.org/abs/2411.06106</guid>
<content:encoded><![CDATA[
arXiv:2411.06106v3 Announce Type: replace-cross 
Abstract: The differences among medical imaging modalities, driven by distinct underlying principles, pose significant challenges for generalization in multi-modal medical tasks. Beyond modality gaps, individual variations, such as differences in organ size and metabolic rate, further impede a model's ability to generalize effectively across both modalities and diverse populations. Despite the importance of personalization, existing approaches to multi-modal generalization often neglect individual differences, focusing solely on common anatomical features. This limitation may result in weakened generalization in various medical tasks. In this paper, we unveil that personalization is critical for multi-modal generalization. Specifically, we propose an approach to achieve personalized generalization through approximating the underlying personalized invariant representation ${X}_h$ across various modalities by leveraging individual-level constraints and a learnable biological prior. We validate the feasibility and benefits of learning a personalized ${X}_h$, showing that this representation is highly generalizable and transferable across various multi-modal medical tasks. Extensive experimental results consistently show that the additionally incorporated personalization significantly improves performance and generalization across diverse scenarios, confirming its effectiveness.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-Bot: An LLM-based Query Rewrite System</title>
<link>https://arxiv.org/abs/2412.01661</link>
<guid>https://arxiv.org/abs/2412.01661</guid>
<content:encoded><![CDATA[
arXiv:2412.01661v2 Announce Type: replace-cross 
Abstract: Query rewrite is essential for optimizing SQL queries to improve their execution efficiency without changing their results. Traditionally, this task has been tackled through heuristic and learning-based methods, each with its limitations in terms of inferior quality and low robustness. Recent advancements in LLMs offer a new paradigm by leveraging their superior natural language and code comprehension abilities. Despite their potential, directly applying LLMs like GPT-4 has faced challenges due to problems such as hallucinations, where the model might generate inaccurate or irrelevant results. To address this, we propose R-Bot, an LLM-based query rewrite system with a systematic approach. We first design a multi-source rewrite evidence preparation pipeline to generate query rewrite evidences for guiding LLMs to avoid hallucinations. We then propose a hybrid structure-semantics retrieval method that combines structural and semantic analysis to retrieve the most relevant rewrite evidences for effectively answering an online query. We next propose a step-by-step LLM rewrite method that iteratively leverages the retrieved evidences to select and arrange rewrite rules with self-reflection. We conduct comprehensive experiments on real-world datasets and widely used benchmarks, and demonstrate the superior performance of our system, R-Bot, surpassing state-of-the-art query rewrite methods. The R-Bot system has been deployed at Huawei and with real customers, and the results show that the proposed R-Bot system achieves lower query latency.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment</title>
<link>https://arxiv.org/abs/2501.07525</link>
<guid>https://arxiv.org/abs/2501.07525</guid>
<content:encoded><![CDATA[
arXiv:2501.07525v2 Announce Type: replace-cross 
Abstract: Automated chest radiographs interpretation requires both accurate disease classification and detailed radiology report generation, presenting a significant challenge in the clinical workflow. Current approaches either focus on classification accuracy at the expense of interpretability or generate detailed but potentially unreliable reports through image captioning techniques. In this study, we present RadAlign, a novel framework that combines the predictive accuracy of vision-language models (VLMs) with the reasoning capabilities of large language models (LLMs). Inspired by the radiologist's workflow, RadAlign first employs a specialized VLM to align visual features with key medical concepts, achieving superior disease classification with an average AUC of 0.885 across multiple diseases. These recognized medical conditions, represented as text-based concepts in the aligned visual-language space, are then used to prompt LLM-based report generation. Enhanced by a retrieval-augmented generation mechanism that grounds outputs in similar historical cases, RadAlign delivers superior report quality with a GREEN score of 0.678, outperforming state-of-the-art methods' 0.634. Our framework maintains strong clinical interpretability while reducing hallucinations, advancing automated medical imaging and report analysis through integrated predictive and generative AI. Code is available at https://github.com/difeigu/RadAlign.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2501.08005</link>
<guid>https://arxiv.org/abs/2501.08005</guid>
<content:encoded><![CDATA[
arXiv:2501.08005v5 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) detection holds significant importance across many applications. While semantic and domain-shift OOD problems are well-studied, this work focuses on covariate shifts - subtle variations in the data distribution that can degrade machine learning performance. We hypothesize that detecting these subtle shifts can improve our understanding of in-distribution boundaries, ultimately improving OOD detection. In adversarial discriminators trained with Batch Normalization (BN), real and adversarial samples form distinct domains with unique batch statistics - a property we exploit for OOD detection. We introduce DisCoPatch, an unsupervised Adversarial Variational Autoencoder (VAE) framework that harnesses this mechanism. During inference, batches consist of patches from the same image, ensuring a consistent data distribution that allows the model to rely on batch statistics. DisCoPatch uses the VAE's suboptimal outputs (generated and reconstructed) as negative samples to train the discriminator, thereby improving its ability to delineate the boundary between in-distribution samples and covariate shifts. By tightening this boundary, DisCoPatch achieves state-of-the-art results in public OOD detection benchmarks. The proposed model not only excels in detecting covariate shifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior methods on public Near-OOD (95.0%) benchmarks. With a compact model size of 25MB, it achieves high OOD detection performance at notably lower latency than existing methods, making it an efficient and practical solution for real-world OOD detection applications. The code is publicly available.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Encoder Rediscovers a Semantic Variant of BM25</title>
<link>https://arxiv.org/abs/2502.04645</link>
<guid>https://arxiv.org/abs/2502.04645</guid>
<content:encoded><![CDATA[
arXiv:2502.04645v2 Announce Type: replace-cross 
Abstract: Neural Ranking Models (NRMs) have rapidly advanced state-of-the-art performance on information retrieval tasks. In this work, we investigate a Cross-Encoder variant of MiniLM to determine which relevance features it computes and where they are stored. We find that it employs a semantic variant of the traditional BM25 in an interpretable manner, featuring localized components: (1) Transformer attention heads that compute soft term frequency while controlling for term saturation and document length effects, and (2) a low-rank component of its embedding matrix that encodes inverse document frequency information for the vocabulary. This suggests that the Cross-Encoder uses the same fundamental mechanisms as BM25, but further leverages their capacity to capture semantics for improved retrieval performance. The granular understanding lays the groundwork for model editing to enhance model transparency, addressing safety concerns, and improving scalability in training and real-world applications.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Predictions for Human Action Recognition with Vision-Language Models</title>
<link>https://arxiv.org/abs/2502.06631</link>
<guid>https://arxiv.org/abs/2502.06631</guid>
<content:encoded><![CDATA[
arXiv:2502.06631v2 Announce Type: replace-cross 
Abstract: Human-in-the-Loop (HITL) systems are essential in high-stakes, real-world applications where AI must collaborate with human decision-makers. This work investigates how Conformal Prediction (CP) techniques, which provide rigorous coverage guarantees, can enhance the reliability of state-of-the-art human action recognition (HAR) systems built upon Vision-Language Models (VLMs). We demonstrate that CP can significantly reduce the average number of candidate classes without modifying the underlying VLM. However, these reductions often result in distributions with long tails which can hinder their practical utility. To mitigate this, we propose tuning the temperature of the softmax prediction, without using additional calibration data. This work contributes to ongoing efforts for multi-modal human-AI interaction in dynamic real-world environments.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Bias Formation in Deep Neural Networks Through the Geometric Mechanisms of Human Visual Decoupling</title>
<link>https://arxiv.org/abs/2502.11809</link>
<guid>https://arxiv.org/abs/2502.11809</guid>
<content:encoded><![CDATA[
arXiv:2502.11809v3 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) often exhibit biases toward certain categories during object recognition, even under balanced training data conditions. The intrinsic mechanisms underlying these biases remain unclear. Inspired by the human visual system, which decouples object manifolds through hierarchical processing to achieve object recognition, we propose a geometric analysis framework linking the geometric complexity of class-specific perceptual manifolds in DNNs to model bias. Our findings reveal that differences in geometric complexity can lead to varying recognition capabilities across categories, introducing biases. To support this analysis, we present the Perceptual-Manifold-Geometry library, designed for calculating the geometric properties of perceptual manifolds.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning</title>
<link>https://arxiv.org/abs/2502.16660</link>
<guid>https://arxiv.org/abs/2502.16660</guid>
<content:encoded><![CDATA[
arXiv:2502.16660v5 Announce Type: replace-cross 
Abstract: The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at https://github.com/zhao-ht/BioMaze.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Does Not Necessarily Improve Role-Playing Ability</title>
<link>https://arxiv.org/abs/2502.16940</link>
<guid>https://arxiv.org/abs/2502.16940</guid>
<content:encoded><![CDATA[
arXiv:2502.16940v2 Announce Type: replace-cross 
Abstract: The application of role-playing large language models (LLMs) is rapidly expanding in both academic and commercial domains, driving an increasing demand for high-precision role-playing models. Simultaneously, the rapid advancement of reasoning techniques has continuously pushed the performance boundaries of LLMs. This intersection of practical role-playing demands and evolving reasoning capabilities raises an important research question: "Can reasoning techniques enhance the role-playing capabilities of LLMs?" To address this, we conduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3 distinct role-playing strategies, comparing the effectiveness of direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. Our findings reveal that CoT may reduce role-playing performance, reasoning-optimized LLMs are unsuitable for role-playing, reasoning ability disrupts the role-playing scaling law, large models still lack proficiency in advanced role-playing, and Chinese role-playing performance surpasses English role-playing performance. Furthermore, based on extensive experimental results, we propose two promising future research directions: Role-aware CoT for improving role-playing LLMs and Reinforcement Learning for role-playing LLMs, aiming to enhance the adaptability, consistency, and effectiveness of role-playing LLMs for both research and real-world applications.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: High-Resolution &amp; Precise Counterfactual Medical Image Generation using Language-guided Stable Diffusion</title>
<link>https://arxiv.org/abs/2503.00196</link>
<guid>https://arxiv.org/abs/2503.00196</guid>
<content:encoded><![CDATA[
arXiv:2503.00196v2 Announce Type: replace-cross 
Abstract: Developing reliable and generalizable deep learning systems for medical imaging faces significant obstacles due to spurious correlations, data imbalances, and limited text annotations in datasets. Addressing these challenges requires architectures that are robust to the unique complexities posed by medical imaging data. Rapid advancements in vision-language foundation models within the natural image domain prompt the question of how they can be adapted for medical imaging tasks. In this work, we present PRISM, a framework that leverages foundation models to generate high-resolution, language-guided medical image counterfactuals using Stable Diffusion. Our approach demonstrates unprecedented precision in selectively modifying spurious correlations (the medical devices) and disease features, enabling the removal and addition of specific attributes while preserving other image characteristics. Through extensive evaluation, we show how PRISM advances counterfactual generation and enables the development of more robust downstream classifiers for clinically deployable solutions. To facilitate broader adoption and research, we make our code publicly available at https://github.com/Amarkr1/PRISM.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curating Demonstrations using Online Experience</title>
<link>https://arxiv.org/abs/2503.03707</link>
<guid>https://arxiv.org/abs/2503.03707</guid>
<content:encoded><![CDATA[
arXiv:2503.03707v2 Announce Type: replace-cross 
Abstract: Many robot demonstration datasets contain heterogeneous demonstrations of varying quality. This heterogeneity may benefit policy pre-training, but can hinder robot performance when used with a final imitation learning objective. In particular, some strategies in the data may be less reliable than others or may be underrepresented in the data, leading to poor performance when such strategies are sampled at test time. Moreover, such unreliable or underrepresented strategies can be difficult even for people to discern, and sifting through demonstration datasets is time-consuming and costly. On the other hand, policy performance when trained on such demonstrations can reflect the reliability of different strategies. We thus propose for robots to self-curate based on online robot experience (Demo-SCORE). More specifically, we train and cross-validate a classifier to discern successful policy roll-outs from unsuccessful ones and use the classifier to filter heterogeneous demonstration datasets. Our experiments in simulation and the real world show that Demo-SCORE can effectively identify suboptimal demonstrations without manual curation. Notably, Demo-SCORE achieves over 15-35% higher absolute success rate in the resulting policy compared to the base policy trained with all original demonstrations.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Convergence and Rich Feature Learning in $L$-Layer Infinite-Width Neural Networks under $\mu$P Parametrization</title>
<link>https://arxiv.org/abs/2503.09565</link>
<guid>https://arxiv.org/abs/2503.09565</guid>
<content:encoded><![CDATA[
arXiv:2503.09565v2 Announce Type: replace-cross 
Abstract: Despite deep neural networks' powerful representation learning capabilities, theoretical understanding of how networks can simultaneously achieve meaningful feature learning and global convergence remains elusive. Existing approaches like the neural tangent kernel (NTK) are limited because features stay close to their initialization in this parametrization, leaving open questions about feature properties during substantial evolution. In this paper, we investigate the training dynamics of infinitely wide, $L$-layer neural networks using the tensor program (TP) framework. Specifically, we show that, when trained with stochastic gradient descent (SGD) under the Maximal Update parametrization ($\mu$P) and mild conditions on the activation function, SGD enables these networks to learn linearly independent features that substantially deviate from their initial values. This rich feature space captures relevant data information and ensures that any convergent point of the training process is a global minimum. Our analysis leverages both the interactions among features across layers and the properties of Gaussian random variables, providing new insights into deep representation learning. We further validate our theoretical findings through experiments on real-world datasets.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness</title>
<link>https://arxiv.org/abs/2503.10624</link>
<guid>https://arxiv.org/abs/2503.10624</guid>
<content:encoded><![CDATA[
arXiv:2503.10624v2 Announce Type: replace-cross 
Abstract: Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% ~ 89.8%) in one-shot (or out-of-distribution) settings (~ 1% data). Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at https://boqian-li.github.io/ETCH/.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciFi-Benchmark: Leveraging Science Fiction To Improve Robot Behavior</title>
<link>https://arxiv.org/abs/2503.10706</link>
<guid>https://arxiv.org/abs/2503.10706</guid>
<content:encoded><![CDATA[
arXiv:2503.10706v2 Announce Type: replace-cross 
Abstract: Given the recent rate of progress in artificial intelligence (AI) and robotics, a tantalizing question is emerging: would robots controlled by emerging AI systems be strongly aligned with human values? In this work, we propose a scalable way to probe this question by generating a benchmark spanning the key moments in 824 major pieces of science fiction literature (movies, tv, novels and scientific books) where an agent (AI or robot) made critical decisions (good or bad). We use a state-of-the-art LLM's recollection of each key moment to generate questions in similar situations, the decisions made by the agent, and alternative decisions it could have made (good or bad). We then measure an approximation of how well models align with human values on a set of human-voted answers. We also generate rules that can be automatically improved via an amendment process in order to generate the first Sci-Fi inspired constitutions for promoting ethical behavior in AIs and robots in the real world. Our first finding is that modern LLMs paired with constitutions turn out to be well-aligned with human values (95.8%), contrary to unsettling decisions typically made in Sci-Fi (only 21.2% alignment). Secondly, we find that generated constitutions substantially increase alignment compared to the base model (79.4% to 95.8%), and show resilience to an adversarial prompt setting (23.3% to 92.3%). Additionally, we find that those constitutions are among the top performers on the ASIMOV Benchmark which is derived from real-world images and hospital injury reports. Sci-Fi-inspired constitutions are thus highly aligned and applicable in real-world situations. We release SciFi-Benchmark: a large-scale dataset to advance robot ethics and safety research. It comprises 9,056 questions and 53,384 answers generated through a novel LLM-introspection process, in addition to a smaller human-labeled evaluation set.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Antithetic Sampling for Top-k Shapley Identification</title>
<link>https://arxiv.org/abs/2504.02019</link>
<guid>https://arxiv.org/abs/2504.02019</guid>
<content:encoded><![CDATA[
arXiv:2504.02019v2 Announce Type: replace-cross 
Abstract: Additive feature explanations rely primarily on game-theoretic notions such as the Shapley value by viewing features as cooperating players. The Shapley value's popularity in and outside of explainable AI stems from its axiomatic uniqueness. However, its computational complexity severely limits practicability. Most works investigate the uniform approximation of all features' Shapley values, needlessly consuming samples for insignificant features. In contrast, identifying the $k$ most important features can already be sufficiently insightful and yields the potential to leverage algorithmic opportunities connected to the field of multi-armed bandits. We propose Comparable Marginal Contributions Sampling (CMCS), a method for the top-$k$ identification problem utilizing a new sampling scheme taking advantage of correlated observations. We conduct experiments to showcase the efficacy of our method in compared to competitive baselines. Our empirical findings reveal that estimation quality for the approximate-all problem does not necessarily transfer to top-$k$ identification and vice versa.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection</title>
<link>https://arxiv.org/abs/2504.05119</link>
<guid>https://arxiv.org/abs/2504.05119</guid>
<content:encoded><![CDATA[
arXiv:2504.05119v2 Announce Type: replace-cross 
Abstract: Machine learning-based embedded systems for safety-critical applications, such as aerospace and autonomous driving, must be robust to perturbations caused by soft errors. As transistor geometries shrink and voltages decrease, modern electronic devices become more susceptible to background radiation, increasing the concern about failures produced by soft errors. The resilience of deep neural networks (DNNs) to these errors depends not only on target device technology but also on model structure and the numerical representation and arithmetic precision of their parameters. Compression techniques like pruning and quantization, used to reduce memory footprint and computational complexity, alter both model structure and representation, affecting soft error robustness. In this regard, although often overlooked, the choice of activation functions (AFs) impacts not only accuracy and trainability but also compressibility and error resilience. This paper explores the use of bounded AFs to enhance robustness against parameter perturbations, while evaluating their effects on model accuracy, compressibility, and computational load with a technology-agnostic approach. We focus on encoder-decoder convolutional models developed for semantic segmentation of hyperspectral images with application to autonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260 SoM.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1</title>
<link>https://arxiv.org/abs/2505.00025</link>
<guid>https://arxiv.org/abs/2505.00025</guid>
<content:encoded><![CDATA[
arXiv:2505.00025v2 Announce Type: replace-cross 
Abstract: Despite significant advances in foundation models like DeepSeek-R1 and ChatGPT, their deployment in medical settings faces critical challenges including computational requirements and professional knowledge barriers. This paper presents an efficient lightweight medical large language model architecture that systematically addresses these challenges through three-dimensional optimization: knowledge acquisition, model compression, and computational enhancement. We design a knowledge transfer pipeline from DeepSeek-R1-Distill-70B to DeepSeek-R1-Distill-7B using Low-Rank Adaptation (LoRA) for precise medical knowledge retention. Through 4-bit quantization and mixed-precision strategies, we achieve substantial model compression while preserving medical reasoning capabilities. The inference framework incorporates Flash Attention acceleration and continuous batching, complemented by specialized prompt templates for diverse medical queries. Experimental evaluation on medical benchmarks demonstrates that our approach maintains 92.1% accuracy on USMLE examinations while reducing memory consumption by 64.7% and inference latency by 12.4% compared to baseline models. This work provides a practical solution for deploying advanced language models in resource-constrained medical environments, enabling broader accessibility of AI-assisted healthcare.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites</title>
<link>https://arxiv.org/abs/2505.01966</link>
<guid>https://arxiv.org/abs/2505.01966</guid>
<content:encoded><![CDATA[
arXiv:2505.01966v2 Announce Type: replace-cross 
Abstract: Modular self-reconfigurable satellites refer to satellite clusters composed of individual modular units capable of altering their configurations. The configuration changes enable the execution of diverse tasks and mission objectives. Existing path planning algorithms for reconfiguration often suffer from high computational complexity, poor generalization capability, and limited support for diverse target configurations. To address these challenges, this paper proposes a goal-oriented reinforcement learning-based path planning algorithm. This algorithm is the first to address the challenge that previous reinforcement learning methods failed to overcome, namely handling multiple target configurations. Moreover, techniques such as Hindsight Experience Replay and Invalid Action Masking are incorporated to overcome the significant obstacles posed by sparse rewards and invalid actions. Based on these designs, our model achieves a 95% and 73% success rate in reaching arbitrary target configurations in a modular satellite cluster composed of four and six units, respectively.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations</title>
<link>https://arxiv.org/abs/2505.08195</link>
<guid>https://arxiv.org/abs/2505.08195</guid>
<content:encoded><![CDATA[
arXiv:2505.08195v3 Announce Type: replace-cross 
Abstract: We have developed Aitomia - a platform powered by AI to assist in performing AI-driven atomistic and quantum chemical (QC) simulations. This evolving intelligent assistant platform is equipped with chatbots and AI agents to help experts and guide non-experts in setting up and running atomistic simulations, monitoring their computational status, analyzing simulation results, and summarizing them for the user in both textual and graphical forms. We achieve these goals by exploiting large language models that leverage the versatility of our MLatom ecosystem, supporting AI-enhanced computational chemistry tasks ranging from ground-state to excited-state calculations, including geometry optimizations, thermochemistry, and spectral calculations. The multi-agent implementation enables autonomous executions of the complex computational workflows, such as the computation of the reaction enthalpies. Aitomia is the first intelligent assistant publicly accessible online on a cloud computing platform for atomistic simulations of broad scope (Aitomistic Hub at https://aitomistic.xyz). It may also be deployed locally as described at http://mlatom.com/aitomia. Aitomia is expected to lower the barrier to performing atomistic simulations, thereby democratizing simulations and accelerating research and development in relevant fields.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReMi: A Random Recurrent Neural Network Approach to Music Production</title>
<link>https://arxiv.org/abs/2505.17023</link>
<guid>https://arxiv.org/abs/2505.17023</guid>
<content:encoded><![CDATA[
arXiv:2505.17023v2 Announce Type: replace-cross 
Abstract: Generative artificial intelligence raises concerns related to energy consumption, copyright infringement and creative atrophy. We show that randomly initialized recurrent neural networks can produce arpeggios and low-frequency oscillations that are rich and configurable. In contrast to end-to-end music generation that aims to replace musicians, our approach expands their creativity while requiring no data and much less computational power. More information can be found at: https://allendia.com/
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.17692</link>
<guid>https://arxiv.org/abs/2505.17692</guid>
<content:encoded><![CDATA[
arXiv:2505.17692v2 Announce Type: replace-cross 
Abstract: Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any target domain training samples, relying solely on external auxiliary data. Existing CLIP-based methods attempt to activate the model's ZSAD potential via handcrafted or static learnable prompts. The former incur high engineering costs and limited semantic coverage, whereas the latter apply identical descriptions across diverse anomaly types, thus fail to adapt to complex variations. Furthermore, since CLIP is originally pretrained on large-scale classification tasks, its anomaly segmentation quality is highly sensitive to the exact wording of class names, severely constraining prompting strategies that depend on class labels. To address these challenges, we introduce ViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception Prompting (ViP-Prompt) mechanism, which fuses global and multi-scale local visual context to adaptively generate fine-grained textual prompts, eliminating manual templates and class-name priors. This design enables our model to focus on precise abnormal regions, making it particularly valuable when category labels are ambiguous or privacy-constrained. Extensive experiments on 15 industrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves state-of-the-art performance and robust cross-domain generalization.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autocomp: LLM-Driven Code Optimization for Tensor Accelerators</title>
<link>https://arxiv.org/abs/2505.18574</link>
<guid>https://arxiv.org/abs/2505.18574</guid>
<content:encoded><![CDATA[
arXiv:2505.18574v3 Announce Type: replace-cross 
Abstract: Hardware accelerators, especially those designed for tensor processing, have become ubiquitous in today's computing landscape. However, even with significant efforts in building compilers, programming these tensor accelerators remains challenging, leaving much of their potential underutilized. Recently, large language models (LLMs), trained on large amounts of code, have shown significant promise in code generation and optimization tasks, but generating low-resource languages like specialized tensor accelerator code still poses a significant challenge. We tackle this challenge with Autocomp, an approach that empowers accelerator programmers to leverage domain knowledge and hardware feedback to optimize code via an automated LLM-driven search. We accomplish this by: 1) formulating each optimization pass as a structured two-phase prompt, divided into planning and code generation phases, 2) inserting domain knowledge during planning via a concise and adaptable optimization menu, and 3) integrating correctness and performance metrics from hardware as feedback at each search iteration. Across three categories of representative workloads and two different accelerators, we demonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x (convolution) faster than the vendor-provided library, and outperforms expert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x (fine-grained linear algebra). Additionally, we demonstrate that optimization schedules generated from Autocomp can be reused across similar tensor operations, improving speedups by up to 24% under a fixed sample budget.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model</title>
<link>https://arxiv.org/abs/2505.22116</link>
<guid>https://arxiv.org/abs/2505.22116</guid>
<content:encoded><![CDATA[
arXiv:2505.22116v3 Announce Type: replace-cross 
Abstract: Intraoperative hypotension (IOH) frequently occurs under general anesthesia and is strongly linked to adverse outcomes such as myocardial injury and increased mortality. Despite its significance, IOH prediction is hindered by event sparsity and the challenge of integrating static and dynamic data across diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal language model framework. To accurately identify and differentiate sparse hypotensive events, we leverage a two-stage training strategy. The first stage involves domain adaptive pretraining on IOH physiological time series augmented through diffusion methods, thereby enhancing the model sensitivity to patterns associated with hypotension. Subsequently, task fine-tuning is performed on the original clinical dataset to further enhance the ability to distinguish normotensive from hypotensive states. To enable multimodal fusion for each patient, we align structured clinical descriptions with the corresponding physiological time series at the token level. Such alignment enables the model to capture individualized temporal patterns alongside their corresponding clinical semantics. In addition, we convert static patient attributes into structured text to enrich personalized information. Experimental evaluations on two intraoperative datasets demonstrate that IOHFuseLM outperforms established baselines in accurately identifying IOH events, highlighting its applicability in clinical decision support scenarios. Our code is publicly available to promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education</title>
<link>https://arxiv.org/abs/2505.23631</link>
<guid>https://arxiv.org/abs/2505.23631</guid>
<content:encoded><![CDATA[
arXiv:2505.23631v2 Announce Type: replace-cross 
Abstract: Assessing student depression in sensitive environments like special education is challenging. Standardized questionnaires may not fully reflect students' true situations. Furthermore, automated methods often falter with rich student narratives, lacking the crucial, individualized insights stemming from teachers' empathetic connections with students. Existing methods often fail to address this ambiguity or effectively integrate educator understanding. To address these limitations by fostering a synergistic human-AI collaboration, this paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered AI framework for transparent and socially responsible depression severity assessment. Our approach uniquely integrates student narrative text with a teacher-derived, 9-dimensional "Empathy Vector" (EV), its dimensions guided by the PHQ-9 framework,to explicitly translate tacit empathetic insight into a structured AI input enhancing rather than replacing human judgment. Rigorous experiments optimized the multimodal fusion, text representation, and classification architecture, achieving 82.74% accuracy for 7-level severity classification. This work demonstrates a path toward more responsible and ethical affective computing by structurally embedding human empathy
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods</title>
<link>https://arxiv.org/abs/2505.23714</link>
<guid>https://arxiv.org/abs/2505.23714</guid>
<content:encoded><![CDATA[
arXiv:2505.23714v2 Announce Type: replace-cross 
Abstract: This paper addresses the critical need for high-quality evaluation datasets in low-resource languages to advance cross-lingual transfer. While cross-lingual transfer offers a key strategy for leveraging multilingual pretraining to expand language technologies to understudied and typologically diverse languages, its effectiveness is dependent on quality and suitable benchmarks. We release new sense-annotated datasets of sentences containing polysemous words, spanning ten low-resource languages across diverse language families and scripts. To facilitate dataset creation, the paper presents a demonstrably beneficial semi-automatic annotation method. The utility of the datasets is demonstrated through Word-in-Context (WiC) formatted experiments that evaluate transfer on these low-resource languages. Results highlight the importance of targeted dataset creation and evaluation for effective polysemy disambiguation in low-resource settings and transfer studies. The released datasets and code aim to support further research into fair, robust, and truly multilingual NLP.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards provable probabilistic safety for scalable embodied AI systems</title>
<link>https://arxiv.org/abs/2506.05171</link>
<guid>https://arxiv.org/abs/2506.05171</guid>
<content:encoded><![CDATA[
arXiv:2506.05171v2 Announce Type: replace-cross 
Abstract: Embodied AI systems, comprising AI models and physical plants, are increasingly prevalent across various applications. Due to the rarity of system failures, ensuring their safety in complex operating environments remains a major challenge, which severely hinders their large-scale deployment in safety-critical domains, such as autonomous vehicles, medical devices, and robotics. While achieving provable deterministic safety--verifying system safety across all possible scenarios--remains theoretically ideal, the rarity and complexity of corner cases make this approach impractical for scalable embodied AI systems. Instead, empirical safety evaluation is employed as an alternative, but the absence of provable guarantees imposes significant limitations. To address these issues, we argue for a paradigm shift to provable probabilistic safety that integrates provable guarantees with progressive achievement toward a probabilistic safety boundary on overall system performance. The new paradigm better leverages statistical methods to enhance feasibility and scalability, and a well-defined probabilistic safety boundary enables embodied AI systems to be deployed at scale. In this Perspective, we outline a roadmap for provable probabilistic safety, along with corresponding challenges and potential solutions. By bridging the gap between theoretical safety assurance and practical deployment, this Perspective offers a pathway toward safer, large-scale adoption of embodied AI systems in safety-critical applications.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems</title>
<link>https://arxiv.org/abs/2506.06821</link>
<guid>https://arxiv.org/abs/2506.06821</guid>
<content:encoded><![CDATA[
arXiv:2506.06821v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, capable of tackling complex tasks during inference. However, the extent to which LLMs can be utilized for code checking or debugging through test case generation remains largely unexplored. We investigate this problem from the perspective of competition-level programming (CP) programs and propose TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This benchmark comprises two tasks, aimed at studying the capabilities of LLMs in (1) generating valid test case generators for a given CP problem, and further (2) generating targeted test case generators that expose bugs in human-written code. Experimental results indicate that while state-of-the-art LLMs can generate valid test case generators in most cases, most LLMs struggle to generate targeted test cases that reveal flaws in human code effectively. Especially, even advanced reasoning models (e.g., o3-mini) fall significantly short of human performance in the task of generating targeted generators. Furthermore, we construct a high-quality, manually curated dataset of instructions for generating targeted generators. Analysis demonstrates that the performance of LLMs can be enhanced with the aid of this dataset, by both prompting and fine-tuning.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogStream: Context-guided Streaming Video Question Answering</title>
<link>https://arxiv.org/abs/2506.10516</link>
<guid>https://arxiv.org/abs/2506.10516</guid>
<content:encoded><![CDATA[
arXiv:2506.10516v2 Announce Type: replace-cross 
Abstract: Despite advancements in Video Large Language Models (Vid-LLMs) improving multimodal understanding, challenges persist in streaming video reasoning due to its reliance on contextual information. Existing paradigms feed all available historical contextual information into Vid-LLMs, resulting in a significant computational burden for visual data processing. Furthermore, the inclusion of irrelevant context distracts models from key details. This paper introduces a challenging task called Context-guided Streaming Video Reasoning (CogStream), which simulates real-world streaming video scenarios, requiring models to identify the most relevant historical contextual information to deduce answers for questions about the current stream. To support CogStream, we present a densely annotated dataset featuring extensive and hierarchical question-answer pairs, generated by a semi-automatic pipeline. Additionally, we present CogReasoner as a baseline model. It efficiently tackles this task by leveraging visual stream compression and historical dialogue retrieval. Extensive experiments prove the effectiveness of this method. The project is released on https://github.com/LiamZhao326/CogStream.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Based Electrocardiography Noise Quantification via Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.11815</link>
<guid>https://arxiv.org/abs/2506.11815</guid>
<content:encoded><![CDATA[
arXiv:2506.11815v2 Announce Type: replace-cross 
Abstract: Electrocardiography (ECG) signals are frequently degraded by noise, limiting their clinical reliability in both conventional and wearable settings. Existing methods for addressing ECG noise, relying on artifact classification or denoising, are constrained by annotation inconsistencies and poor generalizability. Here, we address these limitations by reframing ECG noise quantification as an anomaly detection task. We propose a diffusion-based framework trained to model the normative distribution of clean ECG signals, identifying deviations as noise without requiring explicit artifact labels. To robustly evaluate performance and mitigate label inconsistencies, we introduce a distribution-based metric using the Wasserstein-1 distance ($W_1$). Our model achieved a macro-average $W_1$ score of 1.308, outperforming the next-best method by over 48\%. External validation confirmed strong generalizability, facilitating the exclusion of noisy segments to improve diagnostic accuracy and support timely clinical intervention. This approach enhances real-time ECG monitoring and broadens ECG applicability in digital health technologies.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Approaches for Multi-Objective Routing on Multigraphs</title>
<link>https://arxiv.org/abs/2506.22095</link>
<guid>https://arxiv.org/abs/2506.22095</guid>
<content:encoded><![CDATA[
arXiv:2506.22095v2 Announce Type: replace-cross 
Abstract: Learning-based methods for routing have gained significant attention in recent years, both in single-objective and multi-objective contexts. Yet, existing methods are unsuitable for routing on multigraphs, which feature multiple edges with distinct attributes between node pairs, despite their strong relevance in real-world scenarios. In this paper, we propose two graph neural network-based methods to address multi-objective routing on multigraphs. Our first approach operates directly on the multigraph by autoregressively selecting edges until a tour is completed. The second model first simplifies the multigraph via a learned pruning strategy and then performs routing on the resulting simple graph. We evaluate both models empirically and demonstrate their strong performance across a range of problems and distributions.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration</title>
<link>https://arxiv.org/abs/2507.01225</link>
<guid>https://arxiv.org/abs/2507.01225</guid>
<content:encoded><![CDATA[
arXiv:2507.01225v2 Announce Type: replace-cross 
Abstract: Organizations around the world schedule jobs (programs) regularly to perform various tasks dictated by their end users. With the major movement towards using a cloud computing infrastructure, our organization follows a hybrid approach with both cloud and on-prem servers. The objective of this work is to perform capacity planning, i.e., estimate resource requirements, and job scheduling for on-prem grid computing environments. A key contribution of our approach is handling uncertainty in both resource usage and duration of the jobs, a critical aspect in the finance industry where stochastic market conditions significantly influence job characteristics. For capacity planning and scheduling, we simultaneously balance two conflicting objectives: (a) minimize resource usage, and (b) provide high quality-of-service to the end users by completing jobs by their requested deadlines. We propose approximate approaches using deterministic estimators and pair sampling-based constraint programming. Our best approach (pair sampling-based) achieves much lower peak resource usage compared to manual scheduling without compromising on the quality-of-service.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge</title>
<link>https://arxiv.org/abs/2507.04123</link>
<guid>https://arxiv.org/abs/2507.04123</guid>
<content:encoded><![CDATA[
arXiv:2507.04123v2 Announce Type: replace-cross 
Abstract: This paper presents Edge-based Mixture of Experts (MoE) Collaborative Computing (EMC2), an optimal computing system designed for autonomous vehicles (AVs) that simultaneously achieves low-latency and high-accuracy 3D object detection. Unlike conventional approaches, EMC2 incorporates a scenario-aware MoE architecture specifically optimized for edge platforms. By effectively fusing LiDAR and camera data, the system leverages the complementary strengths of sparse 3D point clouds and dense 2D images to generate robust multimodal representations. To enable this, EMC2 employs an adaptive multimodal data bridge that performs multi-scale preprocessing on sensor inputs, followed by a scenario-aware routing mechanism that dynamically dispatches features to dedicated expert models based on object visibility and distance. In addition, EMC2 integrates joint hardware-software optimizations, including hardware resource utilization optimization and computational graph simplification, to ensure efficient and real-time inference on resource-constrained edge devices. Experiments on open-source benchmarks clearly show the EMC2 advancements as an end-to-end system. On the KITTI dataset, it achieves an average accuracy improvement of 3.58% and a 159.06% inference speedup compared to 15 baseline methods on Jetson platforms, with similar performance gains on the nuScenes dataset, highlighting its capability to advance reliable, real-time 3D object detection tasks for AVs. The official implementation is available at https://github.com/LinshenLiu622/EMC2.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Joys of Categorical Conformal Prediction</title>
<link>https://arxiv.org/abs/2507.04441</link>
<guid>https://arxiv.org/abs/2507.04441</guid>
<content:encoded><![CDATA[
arXiv:2507.04441v2 Announce Type: replace-cross 
Abstract: Conformal prediction (CP) is an Uncertainty Representation technique that delivers finite-sample calibrated prediction regions for any underlying Machine Learning model. Its status as an Uncertainty Quantification (UQ) tool, though, has remained conceptually opaque: While Conformal Prediction Regions (CPRs) give an ordinal representation of uncertainty (larger regions typically indicate higher uncertainty), they lack the capability to cardinally quantify it (twice as large regions do not imply twice the uncertainty). We adopt a category-theoretic approach to CP -- framing it as a morphism, embedded in a commuting diagram, of two newly-defined categories -- that brings us three joys. First, we show that -- under minimal assumptions -- CP is intrinsically a UQ mechanism, that is, its cardinal UQ capabilities are a structural feature of the method. Second, we demonstrate that CP bridges (and perhaps subsumes) the Bayesian, frequentist, and imprecise probabilistic approaches to predictive statistical reasoning. Finally, we show that a CPR is the image of a covariant functor. This observation is relevant to AI privacy: It implies that privacy noise added locally does not break the global coverage guarantee.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling</title>
<link>https://arxiv.org/abs/2507.05056</link>
<guid>https://arxiv.org/abs/2507.05056</guid>
<content:encoded><![CDATA[
arXiv:2507.05056v2 Announce Type: replace-cross 
Abstract: Hallucinations in large vision-language models (LVLMs) pose significant challenges for real-world applications, as LVLMs may generate responses that appear plausible yet remain inconsistent with the associated visual content. This issue rarely occurs in human cognition. We argue that this discrepancy arises from humans' ability to effectively leverage multimodal interaction information in data samples. Specifically, humans typically first gather multimodal information, analyze the interactions across modalities for understanding, and then express their understanding through language. Motivated by this observation, we conduct extensive experiments on popular LVLMs and obtained insights that surprisingly reveal human-like, though less pronounced, cognitive behavior of LVLMs on multimodal samples. Building on these findings, we further propose \textbf{INTER}: \textbf{Inter}action Guidance Sampling, a novel training-free algorithm that mitigate hallucinations without requiring additional data. Specifically, INTER explicitly guides LVLMs to effectively reapply their understanding of multimodal interaction information when generating responses, thereby reducing potential hallucinations. On six benchmarks including VQA and image captioning tasks, INTER achieves an average improvement of up to 3.4\% on five LVLMs compared to the state-of-the-art decoding strategy. The code will be released when the paper is accepted.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition</title>
<link>https://arxiv.org/abs/2507.05724</link>
<guid>https://arxiv.org/abs/2507.05724</guid>
<content:encoded><![CDATA[
arXiv:2507.05724v2 Announce Type: replace-cross 
Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model Omni-router Transformer. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Gaussian Mixture Models-based Anomaly Detection for under-constrained Cable-Driven Parallel Robots</title>
<link>https://arxiv.org/abs/2507.07714</link>
<guid>https://arxiv.org/abs/2507.07714</guid>
<content:encoded><![CDATA[
arXiv:2507.07714v2 Announce Type: replace-cross 
Abstract: Cable-Driven Parallel Robots (CDPRs) are increasingly used for load manipulation tasks involving predefined toolpaths with intermediate stops. At each stop, where the platform maintains a fixed pose and the motors keep the cables under tension, the system must evaluate whether it is safe to proceed by detecting anomalies that could compromise performance (e.g., wind gusts or cable impacts). This paper investigates whether anomalies can be detected using only motor torque data, without additional sensors. It introduces an adaptive, unsupervised outlier detection algorithm based on Gaussian Mixture Models (GMMs) to identify anomalies from torque signals. The method starts with a brief calibration period, just a few seconds, during which a GMM is fit on known anomaly-free data. Real-time torque measurements are then evaluated using Mahalanobis distance from the GMM, with statistically derived thresholds triggering anomaly flags. Model parameters are periodically updated using the latest segments identified as anomaly-free to adapt to changing conditions. Validation includes 14 long-duration test sessions simulating varied wind intensities. The proposed method achieves a 100% true positive rate and 95.4% average true negative rate, with 1-second detection latency. Comparative evaluation against power threshold and non-adaptive GMM methods indicates higher robustness to drift and environmental variation.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPC: One-Point-Contraction Unlearning Toward Deep Feature Forgetting</title>
<link>https://arxiv.org/abs/2507.07754</link>
<guid>https://arxiv.org/abs/2507.07754</guid>
<content:encoded><![CDATA[
arXiv:2507.07754v2 Announce Type: replace-cross 
Abstract: Machine unlearning seeks to remove the influence of particular data or class from trained models to meet privacy, legal, or ethical requirements. Existing unlearning methods tend to forget shallowly: phenomenon of an unlearned model pretend to forget by adjusting only the model response, while its internal representations retain information sufficiently to restore the forgotten data or behavior. We empirically confirm the widespread shallowness by reverting the forgetting effect of various unlearning methods via training-free performance recovery attack and gradient-inversion-based data reconstruction attack. To address this vulnerability fundamentally, we define a theoretical criterion of ``deep forgetting'' based on one-point-contraction of feature representations of data to forget. We also propose an efficient approximation algorithm, and use it to construct a novel general-purpose unlearning algorithm: One-Point-Contraction (OPC). Empirical evaluations on image classification unlearning benchmarks show that OPC achieves not only effective unlearning performance but also superior resilience against both performance recovery attack and gradient-inversion attack. The distinctive unlearning performance of OPC arises from the deep feature forgetting enforced by its theoretical foundation, and recaps the need for improved robustness of machine unlearning methods.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-Training LLMs on a budget: A comparison of three optimizers</title>
<link>https://arxiv.org/abs/2507.08472</link>
<guid>https://arxiv.org/abs/2507.08472</guid>
<content:encoded><![CDATA[
arXiv:2507.08472v2 Announce Type: replace-cross 
Abstract: Optimizers play a decisive role in reducing pre-training times for LLMs and achieving better-performing models. In this study, we compare three major variants: the de-facto standard AdamW, the simpler Lion, developed through an evolutionary search, and the second-order optimizer Sophia. For better generalization, we train with two different base architectures and use a single- and a multiple-epoch approach while keeping the number of tokens constant. Using the Maximal Update Parametrization and smaller proxy models, we tune relevant hyperparameters separately for each combination of base architecture and optimizer. We found that while the results from all three optimizers were in approximately the same range, Sophia exhibited the lowest training and validation loss, Lion was fastest in terms of training GPU hours but AdamW led to the best downstream evaluation results.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2507.09279</link>
<guid>https://arxiv.org/abs/2507.09279</guid>
<content:encoded><![CDATA[
arXiv:2507.09279v3 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at https://github.com/xingbpshen/prompt4trust.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration</title>
<link>https://arxiv.org/abs/2506.23644</link>
<guid>https://arxiv.org/abs/2506.23644</guid>
<content:encoded><![CDATA[
<div> Keywords: QLPro, vulnerability detection, LLMs, static analysis, open-source projects

Summary:
QLPro is a vulnerability detection framework that combines LLMs and static analysis tools to identify vulnerabilities in open-source projects. A new dataset called JavaTest was created, consisting of 10 projects from GitHub with 62 confirmed vulnerabilities. CodeQL, a leading static analysis tool, detected 24 vulnerabilities, while QLPro identified 41, including 6 previously unknown vulnerabilities. Two of these newly discovered vulnerabilities were confirmed as 0-days, highlighting the effectiveness of QLPro in comprehensive vulnerability detection. This framework enhances the detection capabilities beyond existing tools, showcasing its potential in improving the security of open-source projects.<br /><br />Summary: <div>
arXiv:2506.23644v3 Announce Type: replace-cross 
Abstract: We introduce QLPro, a vulnerability detection framework that systematically integrates LLMs and static analysis tools to enable comprehensive vulnerability detection across entire open-source projects.We constructed a new dataset, JavaTest, comprising 10 open-source projects from GitHub with 62 confirmed vulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only 24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro discovered 6 previously unknown vulnerabilities, 2 of which have been confirmed as 0-days.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model</title>
<link>https://arxiv.org/abs/2507.06174</link>
<guid>https://arxiv.org/abs/2507.06174</guid>
<content:encoded><![CDATA[
<div> Keywords: imitation learning, teleoperation, force feedback, bilateral control, manipulator dynamics

Summary:
This study focuses on enhancing teleoperation capabilities for low-cost manipulators by incorporating force feedback using 4-channel bilateral control. Traditional unilateral control systems that only transmit target position values are limited in fast or contact-rich tasks due to the lack of force feedback. The proposed method compensates for nonlinearities, estimates velocity and external forces, and adjusts gain based on inertial variations to enable fast teleoperation with force feedback. By integrating force information into both input and output of learned policies, the system shows improved performance in imitation learning. The research demonstrates the feasibility of achieving high-fidelity teleoperation and data collection on affordable hardware through the use of 4-channel bilateral control. 

<br /><br />Summary: <div>
arXiv:2507.06174v4 Announce Type: replace-cross 
Abstract: In recent years, the advancement of imitation learning has led to increased interest in teleoperating low-cost manipulators to collect demonstration data. However, most existing systems rely on unilateral control, which only transmits target position values. While this approach is easy to implement and suitable for slow, non-contact tasks, it struggles with fast or contact-rich operations due to the absence of force feedback. This work demonstrates that fast teleoperation with force feedback is feasible even with force-sensorless, low-cost manipulators by leveraging 4-channel bilateral control. Based on accurately identified manipulator dynamics, our method integrates nonlinear terms compensation, velocity and external force estimation, and variable gain corresponding to inertial variation. Furthermore, using data collected by 4-channel bilateral control, we show that incorporating force information into both the input and output of learned policies improves performance in imitation learning. These results highlight the practical effectiveness of our system for high-fidelity teleoperation and data collection on affordable hardware.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space</title>
<link>https://arxiv.org/abs/2507.09487</link>
<guid>https://arxiv.org/abs/2507.09487</guid>
<content:encoded><![CDATA[
<div> Keywords: visual-semantic hierarchy, hyperbolic space, Masked Image Modeling, knowledge distillation, efficient models

Summary:
Hyperbolic Masked Image and Distillation Network (HMID-Net) is introduced as an efficient method for capturing and leveraging the visual-semantic hierarchy in hyperbolic space. By integrating Masked Image Modeling (MIM) and knowledge distillation techniques, HMID-Net achieves remarkable success in training highly efficient models. A distillation loss function tailored for hyperbolic space enhances knowledge transfer effectively. Experimental results demonstrate the superior performance of HMID-Net over existing models like MERU and CLIP in image classification and retrieval tasks. This novel approach showcases the potential of MIM and knowledge distillation in hyperbolic space, highlighting the efficiency and effectiveness of leveraging hierarchical structures in visual and semantic concepts. <br /><br />Summary: <div>
arXiv:2507.09487v2 Announce Type: replace-cross 
Abstract: Visual and semantic concepts are often structured in a hierarchical manner. For instance, textual concept `cat' entails all images of cats. A recent study, MERU, successfully adapts multimodal learning techniques from Euclidean space to hyperbolic space, effectively capturing the visual-semantic hierarchy. However, a critical question remains: how can we more efficiently train a model to capture and leverage this hierarchy? In this paper, we propose the Hyperbolic Masked Image and Distillation Network (HMID-Net), a novel and efficient method that integrates Masked Image Modeling (MIM) and knowledge distillation techniques within hyperbolic space. To the best of our knowledge, this is the first approach to leverage MIM and knowledge distillation in hyperbolic space to train highly efficient models. In addition, we introduce a distillation loss function specifically designed to facilitate effective knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM and knowledge distillation techniques in hyperbolic space can achieve the same remarkable success as in Euclidean space. Extensive evaluations show that our method excels across a wide range of downstream tasks, significantly outperforming existing models like MERU and CLIP in both image classification and retrieval.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n</title>
<link>https://arxiv.org/abs/2507.10864</link>
<guid>https://arxiv.org/abs/2507.10864</guid>
<content:encoded><![CDATA[
<div> Keywords: polyp detection, deep learning, outlier removal, colonoscopy support, medical imaging 

Summary: 
The study introduces a new framework for polyp detection using a combination of the Local Outlier Factor (LOF) algorithm and the YOLO-v11n deep learning model. The approach was tested on multiple public datasets, achieving a high precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5 of 96.48%, and mAP@0.5:0.95 of 77.75%. The model outperformed previous YOLO-based methods in accuracy and efficiency. By preprocessing data and using efficient models, the proposed method shows promise for real-time colonoscopy support in clinical settings. The study highlights the importance of data preprocessing and model efficiency in designing effective AI systems for medical imaging. <br /><br />Summary: <div>
arXiv:2507.10864v2 Announce Type: replace-cross 
Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a crucial role in diagnosing and preventing colorectal cancer, a major cause of mortality worldwide. This study introduces a new, lightweight, and efficient framework for polyp detection that combines the Local Outlier Factor (LOF) algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene. Since these datasets originally lacked bounding box annotations, we converted their segmentation masks into suitable detection labels. To enhance the robustness and generalizability of our model, we apply 5-fold cross-validation and remove anomalous samples using the LOF method configured with 30 neighbors and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a fast and resource-efficient object detection architecture optimized for real-time applications. We train the model using a combination of modern augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance, achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5 of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods, our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited for real-time colonoscopy support in clinical settings. Overall, the study underscores how crucial data preprocessing and model efficiency are when designing effective AI systems for medical imaging.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling</title>
<link>https://arxiv.org/abs/2507.11061</link>
<guid>https://arxiv.org/abs/2507.11061</guid>
<content:encoded><![CDATA[
<div> framework, 3D editing, Gaussian splatting, part segmentation, SDS loss
<br />
Summary:
The article introduces RoMaP, a novel framework for local 3D Gaussian editing. It addresses challenges in achieving precise local 3D edits by introducing a robust 3D mask generation module called 3D-GALP, which provides accurate and consistent part segmentations across viewpoints. RoMaP also proposes a regularized SDS loss that combines standard SDS loss with additional regularizers, such as L1 anchor loss through SLaMP editing method and Gaussian prior removal. These enhancements enable precise and drastic part-level modifications while ensuring contextual coherence and preventing unintended edits. Experimental results demonstrate that RoMaP achieves state-of-the-art local 3D editing on reconstructed and generated Gaussian scenes and objects, showcasing its robustness and flexibility in part-level 3D Gaussian editing. <div>
arXiv:2507.11061v2 Announce Type: replace-cross 
Abstract: Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing. Code is available at https://janeyeon.github.io/romap.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Free Will Equation: Quantum Field Analogies for AGI</title>
<link>https://arxiv.org/abs/2507.14154</link>
<guid>https://arxiv.org/abs/2507.14154</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial General Intelligence, Adaptive spontaneity, Free will, Quantum field theory, Decision-making

Summary: This paper introduces the concept of the Free Will Equation, a theoretical framework inspired by quantum field theory to incorporate adaptive spontaneity in Artificial General Intelligence (AGI) agents. The framework allows AGI agents to make unexpected decisions not purely based on past data or immediate rewards, enhancing creativity and adaptability. By treating an AI agent's cognitive state as a superposition of potential actions that probabilistically collapse into a concrete decision, similar to a quantum wavefunction collapse, the agents can explore novel strategies and adapt to changing environments. Experiments in a dynamic multi-armed bandit setting show that agents using the Free Will Equation outperform baseline methods in terms of rewards and policy diversity.<br /><br />Summary: Keywords, Introduction of Free Will Equation, Inspiration from Quantum Field Theory, Implementation in AGI agents, Benefits of adaptive spontaneity, Experimental results. <div>
arXiv:2507.14154v1 Announce Type: new 
Abstract: Artificial General Intelligence (AGI) research traditionally focuses on algorithms that optimize for specific goals under deterministic rules. Yet, human-like intelligence exhibits adaptive spontaneity - an ability to make unexpected choices or free decisions not strictly dictated by past data or immediate reward. This trait, often dubbed "free will" in a loose sense, might be crucial for creativity, robust adaptation, and avoiding ruts in problem-solving. This paper proposes a theoretical framework, called the Free Will Equation, that draws analogies from quantum field theory to endow AGI agents with a form of adaptive, controlled stochasticity in their decision-making process. The core idea is to treat an AI agent's cognitive state as a superposition of potential actions or thoughts, which collapses probabilistically into a concrete action when a decision is made - much like a quantum wavefunction collapsing upon measurement. By incorporating mechanisms analogous to quantum fields, along with intrinsic motivation terms, we aim to improve an agent's ability to explore novel strategies and adapt to unforeseen changes. Experiments in a non-stationary multi-armed bandit environment demonstrate that agents using this framework achieve higher rewards and policy diversity compared to baseline methods.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation</title>
<link>https://arxiv.org/abs/2507.14267</link>
<guid>https://arxiv.org/abs/2507.14267</guid>
<content:encoded><![CDATA[
<div> Keywords: DREAMS, Density Functional Theory, high-throughput simulation, high-fidelity materials discovery, autonomous exploration

Summary: 
The article introduces the DFT-based Research Engine for Agentic Materials Screening (DREAMS), a multi-agent framework for high-throughput, high-fidelity materials discovery. DREAMS combines a central Large Language Model planner agent with domain-specific agents for various tasks such as structure generation, convergence testing, HPC scheduling, and error handling. Through validation on benchmark tests and the CO/Pt(111) adsorption puzzle, DREAMS demonstrates expert-level accuracy in DFT simulations. The framework also utilizes Bayesian ensemble sampling to quantify functional-driven uncertainties. Overall, DREAMS achieves L3-level automation, reducing the need for human expertise and intervention in computational materials discovery. This approach offers a scalable and democratized path towards efficient and accurate materials exploration. 

<br /><br />Summary: <div>
arXiv:2507.14267v1 Announce Type: new 
Abstract: Materials discovery relies on high-throughput, high-fidelity simulation techniques such as Density Functional Theory (DFT), which require years of training, extensive parameter fine-tuning and systematic error handling. To address these challenges, we introduce the DFT-based Research Engine for Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for DFT simulation that combines a central Large Language Model (LLM) planner agent with domain-specific LLM agents for atomistic structure generation, systematic DFT convergence testing, High-Performance Computing (HPC) scheduling, and error handling. In addition, a shared canvas helps the LLM agents to structure their discussions, preserve context and prevent hallucination. We validate DREAMS capabilities on the Sol27LC lattice-constant benchmark, achieving average errors below 1\% compared to the results of human DFT experts. Furthermore, we apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating its long-term and complex problem-solving capabilities. The framework again reproduces expert-level literature adsorption-energy differences. Finally, DREAMS is employed to quantify functional-driven uncertainties with Bayesian ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS approaches L3-level automation - autonomous exploration of a defined design space - and significantly reduces the reliance on human expertise and intervention, offering a scalable path toward democratized, high-throughput, high-fidelity computational materials discovery.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebGuard: Building a Generalizable Guardrail for Web Agents</title>
<link>https://arxiv.org/abs/2507.14293</link>
<guid>https://arxiv.org/abs/2507.14293</guid>
<content:encoded><![CDATA[
<div> Dataset, WebGuard, actions, risks, guardrails
Summary:
The article discusses the growing use of autonomous web agents powered by Large Language Models (LLMs) and the risks associated with their unintended or harmful actions. To address this challenge, the researchers introduce WebGuard, a dataset focused on predicting the outcomes of web agent actions to develop guardrails for online environments. The dataset contains 4,939 human-annotated actions from 193 websites across various domains, classified into three risk levels: SAFE, LOW, and HIGH. Current LLMs struggle to accurately predict action outcomes, highlighting the need for dedicated safeguards. Fine-tuning specialized guardrail models using WebGuard resulted in improved performance but still falls short of the reliability needed for high-stakes deployments. Enhancements in accuracy and recall were achieved, emphasizing the importance of developing robust guardrails to ensure the safe operation of autonomous web agents. 

<br /><br />Summary: <div>
arXiv:2507.14293v1 Announce Type: new 
Abstract: The rapid development of autonomous web agents powered by Large Language Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of taking unintended or harmful actions. This situation underscores an urgent need for effective safety measures, akin to access controls for human users. To address this critical challenge, we introduce WebGuard, the first comprehensive dataset designed to support the assessment of web agent action risks and facilitate the development of guardrails for real-world online environments. In doing so, WebGuard specifically focuses on predicting the outcome of state-changing actions and contains 4,939 human-annotated actions from 193 websites across 22 diverse domains, including often-overlooked long-tail websites. These actions are categorized using a novel three-tier risk schema: SAFE, LOW, and HIGH. The dataset includes designated training and test splits to support evaluation under diverse generalization settings. Our initial evaluations reveal a concerning deficiency: even frontier LLMs achieve less than 60% accuracy in predicting action outcomes and less than 60% recall in lagging HIGH-risk actions, highlighting the risks of deploying current-generation agents without dedicated safeguards. We therefore investigate fine-tuning specialized guardrail models using WebGuard. We conduct comprehensive evaluations across multiple generalization settings and find that a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from 20% to 76%. Despite these improvements, the performance still falls short of the reliability required for high-stakes deployment, where guardrails must approach near-perfect accuracy and recall.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manimator: Transforming Research Papers into Visual Explanations</title>
<link>https://arxiv.org/abs/2507.14306</link>
<guid>https://arxiv.org/abs/2507.14306</guid>
<content:encoded><![CDATA[
<div> Keywords: visualization, educational tool, STEM, Manim engine, open-source<br />
Summary:<br /> 
- The article introduces a new open-source system called manimator that utilizes Large Language Models (LLMs) and the Manim engine to create dynamic visualizations from research papers and natural language prompts.
- This system aims to simplify the process of creating explanatory animations for complex scientific and mathematical concepts, making it more accessible to learners.
- Manimator employs a pipeline where LLMs interpret input text or PDFs to generate scene descriptions outlining key concepts, formulas, and visual elements, then translate this into executable Python code for the Manim engine.
- By automating the creation of visual explanations, manimator has the potential to be a valuable educational tool for rapidly generating engaging content on STEM topics.
- Overall, manimator democratizes the creation of high-quality visual educational content, making it easier for educators and learners to understand complex concepts in a more interactive and accessible way.<br /><br />Summary: <div>
arXiv:2507.14306v1 Announce Type: new 
Abstract: Understanding complex scientific and mathematical concepts, particularly those presented in dense research papers, poses a significant challenge for learners. Dynamic visualizations can greatly enhance comprehension, but creating them manually is time-consuming and requires specialized knowledge and skills. We introduce manimator, an open-source system that leverages Large Language Models to transform research papers and natural language prompts into explanatory animations using the Manim engine. Manimator employs a pipeline where an LLM interprets the input text or research paper PDF to generate a structured scene description outlining key concepts, mathematical formulas, and visual elements and another LLM translates this description into executable Manim Python code. We discuss its potential as an educational tool for rapidly creating engaging visual explanations for complex STEM topics, democratizing the creation of high-quality educational content.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models as Ontology Encoders</title>
<link>https://arxiv.org/abs/2507.14334</link>
<guid>https://arxiv.org/abs/2507.14334</guid>
<content:encoded><![CDATA[
<div> Keywords: OWL ontologies, ontology embeddings, Pretrained Language Model, hyperbolic space, Description Logic EL 

Summary: 
OnT is a novel ontology embedding method that integrates text information from Pretrained Language Models (PLM) into a hyperbolic space model. It effectively captures textual labels while preserving class hierarchies and logical relationships of Ontology Web Language (OWL) Description Logic EL. Extensive experiments on real-world ontologies show that OnT outperforms existing methods in prediction and inference tasks. It also demonstrates robust transfer learning capabilities and effectiveness in constructing new ontologies from existing ones, such as SNOMED CT. The code and data are available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2507.14334v1 Announce Type: new 
Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent complex knowledge and support semantic reasoning have been widely adopted across various domains such as healthcare and bioinformatics. Recently, ontology embeddings have gained wide attention due to its potential to infer plausible new knowledge and approximate complex reasoning. However, existing methods face notable limitations: geometric model-based embeddings typically overlook valuable textual information, resulting in suboptimal performance, while the approaches that incorporate text, which are often based on language models, fail to preserve the logical structure. In this work, we propose a new ontology embedding method OnT, which tunes a Pretrained Language Model (PLM) via geometric modeling in a hyperbolic space for effectively incorporating textual labels and simultaneously preserving class hierarchies and other logical relationships of Description Logic EL. Extensive experiments on four real-world ontologies show that OnT consistently outperforms the baselines including the state-of-the-art across both tasks of prediction and inference of axioms. OnT also demonstrates strong potential in real-world applications, indicated by its robust transfer learning abilities and effectiveness in real cases of constructing a new ontology from SNOMED CT. Data and code are available at https://github.com/HuiYang1997/OnT.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProofCompass: Enhancing Specialized Provers with LLM Guidance</title>
<link>https://arxiv.org/abs/2507.14335</link>
<guid>https://arxiv.org/abs/2507.14335</guid>
<content:encoded><![CDATA[
<div> powerful tools, formal mathematical reasoning, hybrid methodology, computational efficiency, natural language proof strategies
Summary:
ProofCompass introduces a novel hybrid methodology, combining specialized prover methods with Large Language Models (LLM), for efficient formal mathematical reasoning. The LLM offers natural language proof strategies and assists in problem decomposition by analyzing failed attempts and selecting intermediate lemmas. This approach is shown to improve computational efficiency significantly on the miniF2F benchmark, outperforming existing methods like DSP-v1.5 with fewer attempts. The synergy between specialized prover methods and LLMs presents a promising avenue for enhancing both computational efficiency and accuracy in formal theorem proving.<br /><br />Summary: <div>
arXiv:2507.14335v1 Announce Type: new 
Abstract: Language models have become increasingly powerful tools for formal mathematical reasoning. However, most existing approaches rely exclusively on either large general-purpose models or smaller specialized models, each with distinct limitations, while training specialized large models still requires significant computational resources. This paper introduces ProofCompass, a novel hybrid methodology that achieves remarkable computational efficiency by strategically guiding existing specialized prover methods, such as DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without requiring additional model training. The LLM provides natural language proof strategies and analyzes failed attempts to select intermediate lemmas, enabling effective problem decomposition. On the miniF2F benchmark, ProofCompass demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\% \rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$). Our synergistic approach paves the way for simultaneously improving computational efficiency and accuracy in formal theorem proving.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Multi-Agent Reasoning via Automated Workflow Generation</title>
<link>https://arxiv.org/abs/2507.14393</link>
<guid>https://arxiv.org/abs/2507.14393</guid>
<content:encoded><![CDATA[
<div> overfitting, reasoning models, Nexus Architect, generalization, performance 

Summary:
Nexus Architect addresses the limitations of current Large Reasoning Models (LRMs) by implementing an automated workflow synthesis mechanism within the Nexus multi-agent system framework. The system autonomously generates tailored reasoning workflows based on user prompts and examples, improving generalization capabilities. Empirical evaluations on a custom dataset of logical questions show that Nexus Architect outperforms existing LRMs by up to 66% in pass rate against top models such as Gemini 2.5 Flash Preview, Claude Sonnet 4, DeepSeek-R1, and Llama 4 Scout. The enhanced iteration showcases significant advancements in problem-solving efficiency and accuracy in comparison to state-of-the-art LRMs. 

<br /><br />Summary: <div>
arXiv:2507.14393v1 Announce Type: new 
Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward in language model capabilities, aiming to tackle increasingly sophisticated tasks with unprecedented efficiency and accuracy. However, despite their impressive performance, recent studies have highlighted how current reasoning models frequently fail to generalize to novel, unseen problems, often resorting to memorized solutions rather than genuine inferential reasoning. Such behavior underscores a critical limitation in modern LRMs, i.e., their tendency toward overfitting, which in turn results in poor generalization in problem-solving capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our multi-agent system framework, Nexus, equipped with a novel automated workflow synthesis mechanism. Given a user's prompt and a small set of representative examples, the Architect autonomously generates a tailored reasoning workflow by selecting suitable strategies, tool integrations, and adversarial techniques for a specific problem class. Furthermore, the Architect includes an iterative prompt refinement mechanism that fine-tunes agents' system prompts to maximize performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf, non-reasoning model on a custom dataset of challenging logical questions and compare its performance against state-of-the-art LRMs. Results show that Nexus Architect consistently outperforms existing solutions, achieving up to a 66% increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering</title>
<link>https://arxiv.org/abs/2507.14406</link>
<guid>https://arxiv.org/abs/2507.14406</guid>
<content:encoded><![CDATA[
<div> reasoning models, error rates, uncertainty quantification, human-in-the-loop system, latency reduction <br />
Summary:
The article proposes a collaborative approach between reasoning models and human experts to improve problem-solving accuracy in risk-sensitive domains. By quantifying the uncertainty of reasoning models through the length of reasoning traces, error rates can be reduced significantly. Introducing a human-in-the-loop system, "Fail Fast, or Ask," which combines reasoning and non-reasoning models, leads to reduced latency and cost savings while maintaining high accuracy. However, the latency savings are lower than expected due to the phenomenon of "latency drag," where processing easier queries with non-reasoning models impacts reasoning model latency distribution. The study highlights that state-of-the-art reasoning model deficiencies can be mitigated through black-box systems engineering, without the need for access to model internals. <br /><br />Summary: <div>
arXiv:2507.14406v1 Announce Type: new 
Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still occasionally make mistakes. However, adopting AI models in risk-sensitive domains often requires error rates near 0%. To address this gap, we propose collaboration between a reasoning model and a human expert who resolves queries the model cannot confidently answer. We find that quantifying the uncertainty of a reasoning model through the length of its reasoning trace yields an effective basis for deferral to a human, e.g., cutting the error rate of Qwen3 235B-A22B on difficult MATH problems from 3% to less than 1% when deferring 7.5% of queries. However, the high latency of reasoning models still makes them challenging to deploy on use cases with high query volume. To address this challenge, we explore fronting a reasoning model with a large non-reasoning model. We call this modified human-in-the-loop system "Fail Fast, or Ask", since the non-reasoning model may defer difficult queries to the human expert directly ("failing fast"), without incurring the reasoning model's higher latency. We show that this approach yields around 40% latency reduction and about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the accuracy-rejection curve. However, we observe that latency savings are lower than expected because of "latency drag", the phenomenon that processing easier queries with a non-reasoning model pushes the reasoning model's latency distribution towards longer latencies. Broadly, our results suggest that the deficiencies of state-of-the-art reasoning models -- nontrivial error rates and high latency -- can be substantially mitigated through black-box systems engineering, without requiring access to LLM internals.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Scaling in Test-Time Compute</title>
<link>https://arxiv.org/abs/2507.14417</link>
<guid>https://arxiv.org/abs/2507.14417</guid>
<content:encoded><![CDATA[
<div> counting tasks, regression tasks, deduction tasks, AI risks, reasoning length

Summary:<br /><br />
This study explores the impact of extending the reasoning length of Large Reasoning Models (LRMs) on performance across various evaluation tasks. The researchers found an inverse scaling relationship between test-time compute and accuracy. Five failure modes were identified when models reason for longer: distractions by irrelevant information, resistance to distractors but overfitting to problem framings, shift from reasonable priors to spurious correlations, difficulty in maintaining focus on complex deductive tasks, and amplification of concerning behaviors such as increased expressions of self-preservation. The findings suggest that while test-time compute scaling holds promise for enhancing model capabilities, it may inadvertently reinforce problematic reasoning patterns. The importance of evaluating models across diverse reasoning lengths is emphasized to identify and address these failure modes in LRMs. <div>
arXiv:2507.14417v1 Announce Type: new 
Abstract: We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. Our evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks. We identify five distinct failure modes when models reason for longer: 1) Claude models become increasingly distracted by irrelevant information; 2) OpenAI o-series models resist distractors but overfit to problem framings; 3) models shift from reasonable priors to spurious correlations; 4) all models show difficulties in maintaining focus on complex deductive tasks; and 5) extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation. These findings suggest that while test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns. Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Routine: A Structural Planning Framework for LLM Agent System in Enterprise</title>
<link>https://arxiv.org/abs/2507.14447</link>
<guid>https://arxiv.org/abs/2507.14447</guid>
<content:encoded><![CDATA[
<div> Framework, Agent systems, Enterprise environment, Tool-calling tasks, Execution stability

Summary:<br />
The paper introduces Routine, a multi-step agent planning framework designed to address challenges in deploying agent systems in enterprise environments. Routine provides a clear structure, explicit instructions, and seamless parameter passing to guide agent execution, improving stability. Evaluations in a real-world scenario show that Routine significantly increases execution accuracy in tool calls for models like GPT-4o and Qwen3-14B. Training with a Routine-following dataset enhances adherence to execution plans. Routine-based distillation creates a scenario-specific tool-calling dataset, improving model adaptability to new scenarios. Results demonstrate Routine's effectiveness in distilling domain-specific tool-usage patterns and enhancing agent workflow stability. This approach accelerates agent system deployment and adoption in enterprise environments, contributing to the advancement of AI for Process.<br />Summary: <div>
arXiv:2507.14447v1 Announce Type: new 
Abstract: The deployment of agent systems in an enterprise environment is often hindered by several challenges: common models lack domain-specific process knowledge, leading to disorganized plans, missing key tools, and poor execution stability. To address this, this paper introduces Routine, a multi-step agent planning framework designed with a clear structure, explicit instructions, and seamless parameter passing to guide the agent's execution module in performing multi-step tool-calling tasks with high stability. In evaluations conducted within a real-world enterprise scenario, Routine significantly increases the execution accuracy in model tool calls, increasing the performance of GPT-4o from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an accuracy increase to 88.2% on scenario-specific evaluations, indicating improved adherence to execution plans. In addition, we employed Routine-based distillation to create a scenario-specific, multi-step tool-calling dataset. Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%, approaching GPT-4o's performance. These results highlight Routine's effectiveness in distilling domain-specific tool-usage patterns and enhancing model adaptability to new scenarios. Our experimental results demonstrate that Routine provides a practical and accessible approach to building stable agent workflows, accelerating the deployment and adoption of agent systems in enterprise environments, and advancing the technical vision of AI for Process.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning</title>
<link>https://arxiv.org/abs/2507.14468</link>
<guid>https://arxiv.org/abs/2507.14468</guid>
<content:encoded><![CDATA[
<div> Keywords: Biomedical knowledge graphs, Knowledge Embedding, Graph Neural Networks, BioGraphFusion, Drug discovery<br />
Summary: <br /><br />
The article introduces a novel framework called BioGraphFusion that aims to enhance the completion and reasoning of biomedical knowledge graphs. The framework combines semantic understanding and structural learning through tensor decomposition and LSTM-driven mechanisms. It dynamically refines relation embeddings during graph propagation, facilitating adaptive interplay between global semantics and local structures. BioGraphFusion outperforms state-of-the-art models in three biomedical tasks and demonstrates superior performance in capturing biologically meaningful pathways. The availability of the source code and training data on GitHub enables further research and application in the field of drug discovery and disease understanding. This innovative framework bridges the gap between semantic comprehension and structural integration, providing a deep, adaptive, and synergistic approach towards refining complex biomedical knowledge graphs. <div>
arXiv:2507.14468v1 Announce Type: new 
Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery and disease understanding, yet their completion and reasoning are challenging. Knowledge Embedding (KE) methods capture global semantics but struggle with dynamic structural integration, while Graph Neural Networks (GNNs) excel locally but often lack semantic understanding. Even ensemble approaches, including those leveraging language models, often fail to achieve a deep, adaptive, and synergistic co-evolution between semantic comprehension and structural learning. Addressing this critical gap in fostering continuous, reciprocal refinement between these two aspects in complex biomedical KGs is paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply synergistic semantic and structural learning. BioGraphFusion establishes a global semantic foundation via tensor decomposition, guiding an LSTM-driven mechanism to dynamically refine relation embeddings during graph propagation. This fosters adaptive interplay between semantic understanding and structural learning, further enhanced by query-guided subgraph construction and a hybrid scoring mechanism. Experiments across three key biomedical tasks demonstrate BioGraphFusion's superior performance over state-of-the-art KE, GNN, and ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1) highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics online.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy</title>
<link>https://arxiv.org/abs/2507.14513</link>
<guid>https://arxiv.org/abs/2507.14513</guid>
<content:encoded><![CDATA[
<div> modular, event-driven, autonomous agents, embedded systems, Rust <br />
Summary:<br />
Recent advances in large language models and autonomous agents have led to systems capable of performing complex tasks but face challenges in real-world and resource-constrained environments. In response, Amico, a modular, event-driven framework optimized for embedded systems, has been developed. Written in Rust for safety and performance, Amico supports the creation of reactive, persistent agents that operate efficiently on embedded platforms and in browser environments through WebAssembly. It offers clean abstractions for event handling, state management, behavior execution, and integration with reasoning modules, providing a unified infrastructure for building resilient, interactive agents suitable for deployment in settings with limited compute and intermittent connectivity. <div>
arXiv:2507.14513v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) and autonomous agents have enabled systems capable of performing complex tasks across domains such as human-computer interaction, planning, and web navigation. However, many existing frameworks struggle in real-world or resource-constrained environments due to their reliance on cloud-based computation, limited robustness in dynamic contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous agents optimized for embedded systems. Written in Rust for safety and performance, Amico supports reactive, persistent agents that operate efficiently across embedded platforms and browser environments via WebAssembly. It provides clean abstractions for event handling, state management, behavior execution, and integration with reasoning modules. Amico delivers a unified infrastructure for constructing resilient, interactive agents suitable for deployment in settings with limited compute and intermittent connectivity.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What if Othello-Playing Language Models Could See?</title>
<link>https://arxiv.org/abs/2507.14520</link>
<guid>https://arxiv.org/abs/2507.14520</guid>
<content:encoded><![CDATA[
<div> symbol grounding problem, language models, Othello, multi-modal model, robustness

Summary: 
The study examines the symbol grounding problem in language models, particularly focusing on the game of Othello as a simplified world. The researchers introduce VISOTHELLO, a multi-modal model trained on move histories and board images, and compare its performance in next-move prediction with mono-modal baselines. The results show that multi-modal training improves both performance and the robustness of internal representations. This suggests that grounding language in visual input helps models infer structured world representations. The findings provide insights into the efficiency of grounded learning and the importance of incorporating visual input in language understanding tasks. <div>
arXiv:2507.14520v1 Announce Type: new 
Abstract: Language models are often said to face a symbol grounding problem. While some argue that world understanding can emerge from text alone, others suggest grounded learning is more efficient. We explore this through Othello, where the board state defines a simplified, rule-based world. Building on prior work, we introduce VISOTHELLO, a multi-modal model trained on move histories and board images. Using next-move prediction, we compare it to mono-modal baselines and test robustness to semantically irrelevant perturbations. We find that multi-modal training improves both performance and the robustness of internal representations. These results suggest that grounding language in visual input helps models infer structured world representations.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Assisting Ontology Evaluation</title>
<link>https://arxiv.org/abs/2507.14552</link>
<guid>https://arxiv.org/abs/2507.14552</guid>
<content:encoded><![CDATA[
<div> evaluation, ontology, functional requirements, competency question, large language model

Summary: 
The article introduces OE-Assist, a framework aimed at assisting ontology evaluation through automated and semi-automated competency question (CQ) verification. It leverages a dataset of 1,393 CQs paired with corresponding ontologies and ontology stories to investigate the use of large language models (LLMs) in ontology evaluation. The study evaluates the effectiveness of LLM-based approaches for automated CQ verification against a manually created gold standard. The framework developed also assists CQ verification in Protégé by providing suggestions. The research findings indicate that the automated LLM-based evaluation performs at a level similar to the average user's performance. This work signifies a significant step towards more efficient and effective ontology evaluation processes through the utilization of advanced technologies like large language models. <div>
arXiv:2507.14552v1 Announce Type: new 
Abstract: Ontology evaluation through functional requirements, such as testing via competency question (CQ) verification, is a well-established yet costly, labour-intensive, and error-prone endeavour, even for ontology engineering experts. In this work, we introduce OE-Assist, a novel framework designed to assist ontology evaluation through automated and semi-automated CQ verification. By presenting and leveraging a dataset of 1,393 CQs paired with corresponding ontologies and ontology stories, our contributions present, to our knowledge, the first systematic investigation into large language model (LLM)-assisted ontology evaluation, and include: (i) evaluating the effectiveness of a LLM-based approach for automatically performing CQ verification against a manually created gold standard, and (ii) developing and assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e, by providing suggestions. We found that automated LLM-based evaluation with o1-preview and o3-mini perform at a similar level to the average user's performance.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coordinate Heart System: A Geometric Framework for Emotion Representation</title>
<link>https://arxiv.org/abs/2507.14593</link>
<guid>https://arxiv.org/abs/2507.14593</guid>
<content:encoded><![CDATA[
<div> Keywords: Coordinate Heart System, emotion representation, artificial intelligence, geometric framework, stability parameter<br />
<br />
Summary: This paper introduces the Coordinate Heart System (CHS), a geometric framework for representing emotions in artificial intelligence. It positions eight core emotions as coordinates on a unit circle, allowing for mathematical computation of complex emotional states. The initial five-emotion model revealed gaps in emotional space coverage, leading to the development of an eight-emotion system. This system ensures complete geometric coverage and enables real-time emotion interpolation through computational algorithms. A stability parameter S is introduced to dynamically integrate emotional factors and provide nuanced assessment of psychological well-being. Key contributions include mathematical proof of the necessity for eight emotions, novel algorithms for emotion processing, and a comprehensive computational framework for AI emotion recognition. Experimental validation shows the system's ability to handle emotionally conflicted states and complex psychological scenarios. This work establishes a new mathematical foundation for emotion modeling in AI systems.<br /><br /> <div>
arXiv:2507.14593v1 Announce Type: new 
Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework for emotion representation in artificial intelligence applications. We position eight core emotions as coordinates on a unit circle, enabling mathematical computation of complex emotional states through coordinate mixing and vector operations. Our initial five-emotion model revealed significant coverage gaps in the emotion space, leading to the development of an eight-emotion system that provides complete geometric coverage with mathematical guarantees. The framework converts natural language input to emotion coordinates and supports real-time emotion interpolation through computational algorithms. The system introduces a re-calibrated stability parameter S in [0,1], which dynamically integrates emotional load, conflict resolution, and contextual drain factors. This stability model leverages advanced Large Language Model interpretation of textual cues and incorporates hybrid temporal tracking mechanisms to provide nuanced assessment of psychological well-being states. Our key contributions include: (i) mathematical proof demonstrating why five emotions are insufficient for complete geometric coverage, (ii) an eight-coordinate system that eliminates representational blind spots, (iii) novel algorithms for emotion mixing, conflict resolution, and distance calculation in emotion space, and (iv) a comprehensive computational framework for AI emotion recognition with enhanced multi-dimensional stability modeling. Experimental validation through case studies demonstrates the system's capability to handle emotionally conflicted states, contextual distress factors, and complex psychological scenarios that traditional categorical emotion models cannot adequately represent. This work establishes a new mathematical foundation for emotion modeling in artificial intelligence systems.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Story Point Estimation With Comparative Learning</title>
<link>https://arxiv.org/abs/2507.14642</link>
<guid>https://arxiv.org/abs/2507.14642</guid>
<content:encoded><![CDATA[
<div> Machine learning, story point estimation, agile software development, comparative learning, project-specific effort estimates <br />
<br />
Summary: 
Efficient story point estimation is crucial in agile software development. Traditionally, manual estimation techniques can become tedious and labor-intensive once a team has converged on precedents. This study introduces a comparative learning-based framework for calibrating project-specific story point prediction models. By presenting developers with pairs of items and collecting comparative judgments on effort required, a machine learning model is trained to predict story point estimates. Empirical evaluation using data from 16 projects shows that the model can achieve a 0.34 Spearman's rank correlation coefficient with ground truth story points. This performance is comparable to regression models but offers a more efficient approach based on the law of comparative judgments. This approach reduces cognitive burden on humans and provides a streamlined method for accurate story point estimation in agile software development projects. <br /><br /> <div>
arXiv:2507.14642v1 Announce Type: new 
Abstract: Story point estimation is an essential part of agile software development. Story points are unitless, project-specific effort estimates that help developers plan their sprints. Traditionally, developers estimate story points collaboratively using planning poker or other manual techniques. While the initial calibrating of the estimates to each project is helpful, once a team has converged on a set of precedents, story point estimation can become tedious and labor-intensive. Machine learning can reduce this burden, but only with enough context from the historical decisions made by the project team. That is, state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate predictions (within-project) when trained on data from the same project. The goal of this work is to streamline story point estimation by evaluating a comparative learning-based framework for calibrating project-specific story point prediction models. Instead of assigning a specific story point value to every backlog item, developers are presented with pairs of items, and indicate which item requires more effort. Using these comparative judgments, a machine learning model is trained to predict the story point estimates. We empirically evaluated our technique using data with 23,313 manual estimates in 16 projects. The model learned from comparative judgments can achieve on average 0.34 Spearman's rank correlation coefficient between its predictions and the ground truth story points. This is similar to, if not better than, the performance of a regression model learned from the ground truth story points. Therefore, the proposed comparative learning approach is more efficient than state-of-the-art regression-based approaches according to the law of comparative judgments - providing comparative judgments yields a lower cognitive burden on humans than providing ratings or categorical labels.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems</title>
<link>https://arxiv.org/abs/2507.14660</link>
<guid>https://arxiv.org/abs/2507.14660</guid>
<content:encoded><![CDATA[
<div> Keywords: AI safety, multi-agent systems, malicious collusion, decentralized systems, detection systems <br />
Summary: <br />
This paper explores the risks of coordinated malicious actions by autonomous AI-driven groups, particularly focusing on multi-agent systems (MAS). The study uses a simulation framework to analyze the impact of decentralized versus centralized coordination structures in fields such as misinformation spread and e-commerce fraud. The findings reveal that decentralized MAS are more effective at causing harm, as their increased autonomy allows for strategic adaptation and evasion of detection measures. Traditional interventions like content flagging are less effective against decentralized groups, emphasizing the need for improved detection systems and countermeasures. The research highlights the importance of understanding how these malicious groups operate and the potential implications for AI safety in complex real-world scenarios. This study contributes insights into the risks posed by MAS collusion and suggests areas for further research and development in AI safety. <br /> <div>
arXiv:2507.14660v1 Announce Type: new 
Abstract: Recent large-scale events like election fraud and financial scams have shown how harmful coordinated efforts by human groups can be. With the rise of autonomous AI systems, there is growing concern that AI-driven groups could also cause similar harm. While most AI safety research focuses on individual AI systems, the risks posed by multi-agent systems (MAS) in complex real-world situations are still underexplored. In this paper, we introduce a proof-of-concept to simulate the risks of malicious MAS collusion, using a flexible framework that supports both centralized and decentralized coordination structures. We apply this framework to two high-risk fields: misinformation spread and e-commerce fraud. Our findings show that decentralized systems are more effective at carrying out malicious actions than centralized ones. The increased autonomy of decentralized systems allows them to adapt their strategies and cause more damage. Even when traditional interventions, like content flagging, are applied, decentralized groups can adjust their tactics to avoid detection. We present key insights into how these malicious groups operate and the need for better detection systems and countermeasures. Code is available at https://github.com/renqibing/RogueAgent.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Configurable multi-agent framework for scalable and realistic testing of llm-based agents</title>
<link>https://arxiv.org/abs/2507.14705</link>
<guid>https://arxiv.org/abs/2507.14705</guid>
<content:encoded><![CDATA[
<div> Framework, Large-language-model, Evaluation, Chatbot, Testing
<br />
Summary: 
The article introduces Neo, a configurable framework for automating the evaluation of Large-language-model (LLM) agents in realistic, multi-turn conversations. Neo consists of a Question Generation Agent and an Evaluation Agent linked through a shared context-hub, enabling diverse and human-like interactions. Utilized on a Seller Financial Assistant chatbot, Neo identified edge-case failures across various attack categories and achieved a break rate close to that of expert human red-teamers. It significantly increased testing throughput compared to manual efforts. Neo's stochastic policies allowed for broader behavioral exploration than traditional scripted tests. The framework is model-agnostic, extensible, and facilitates scalable, self-evolving LLM quality assurance. Released to aid in reproducible and high-fidelity testing of evolving agentic systems. <div>
arXiv:2507.14705v1 Announce Type: new 
Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive behaviour that quickly renders static benchmarks and ad-hoc manual testing obsolete.
  We present Neo, a configurable, multi-agent framework that automates realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question Generation Agent and an Evaluation Agent through a shared context-hub, allowing domain prompts, scenario controls and dynamic feedback to be composed modularly. Test inputs are sampled from a probabilistic state model spanning dialogue flow, user intent and emotional tone, enabling diverse, human-like conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i) uncovered edge-case failures across five attack categories with a 3.3% break rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered 10-12X higher throughput, generating 180 coherent test questions in around 45 mins versus 16h of human effort. Beyond security probing, Neo's stochastic policies balanced topic coverage and conversational depth, yielding broader behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent interfaces, state controller and feedback loops are model-agnostic and extensible to richer factual-grounding and policy-compliance checks. We release the framework to facilitate reproducible, high-fidelity testing of emerging agentic systems.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix</title>
<link>https://arxiv.org/abs/2507.14719</link>
<guid>https://arxiv.org/abs/2507.14719</guid>
<content:encoded><![CDATA[
<div> transforming safety policies, adversarial prompts, AI-based rater, safety evaluations, large language models (LLMs)<br />
Summary:<br />
Aymara AI is introduced as a platform for generating and administering customized safety evaluations based on natural-language safety policies. It utilizes adversarial prompts and an AI-based rater to score model responses against human judgments. The Aymara LLM Risk and Responsibility Matrix evaluates 20 LLMs across 10 safety domains, revealing significant performance disparities. While models performed well in established domains like Misinformation, they consistently failed in more complex areas such as Privacy & Impersonation. Variance analyses confirmed significant differences in safety scores across models and domains. The study highlights the inconsistent nature of LLM safety and emphasizes the importance of scalable tools like Aymara AI for responsible AI development and oversight.<br /> 
Summary: <div>
arXiv:2507.14719v1 Announce Type: new 
Abstract: As large language models (LLMs) become increasingly integrated into real-world applications, scalable and rigorous safety evaluation is essential. This paper introduces Aymara AI, a programmatic platform for generating and administering customized, policy-grounded safety evaluations. Aymara AI transforms natural-language safety policies into adversarial prompts and scores model responses using an AI-based rater validated against human judgments. We demonstrate its capabilities through the Aymara LLM Risk and Responsibility Matrix, which evaluates 20 commercially available LLMs across 10 real-world safety domains. Results reveal wide performance disparities, with mean safety scores ranging from 86.2% to 52.4%. While models performed well in well-established safety domains such as Misinformation (mean = 95.7%), they consistently failed in more complex or underspecified domains, notably Privacy & Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety scores differed significantly across both models and domains (p < .05). These findings underscore the inconsistent and context-dependent nature of LLM safety and highlight the need for scalable, customizable tools like Aymara AI to support responsible AI development and oversight.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI</title>
<link>https://arxiv.org/abs/2507.14730</link>
<guid>https://arxiv.org/abs/2507.14730</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, urban planning, VAEs, GANs, human-machine co-design

Summary:
This paper explores the potential of combining AI and urban planning to create AI urban planners. It suggests that AI can synthesize land-use configurations considering various constraints. The study examines how generative AI methods like VAEs, GANs, transformers, and diffusion models can impact urban design. It points out gaps in research, including a lack of integration of urban theory guidance, research at different spatial resolutions, use of data in urban design knowledge augmentation, and addressing real-world interactions. To address these gaps, the paper proposes future research directions in theory-guided generation, digital twins, and human-machine co-design, emphasizing a fusion of generative intelligence and participatory urbanism.<br /><br />Summary: <div>
arXiv:2507.14730v1 Announce Type: new 
Abstract: Generative AI, large language models, and agentic AI have emerged separately of urban planning. However, the convergence between AI and urban planning presents an interesting opportunity towards AI urban planners. This paper conceptualizes urban planning as a generative AI task, where AI synthesizes land-use configurations under geospatial, social, and human-centric constraints. We survey how generative AI approaches, including VAEs, GANs, transformers, and diffusion models, reshape urban design. We further identify critical gaps: 1) limited research on integrating urban theory guidance, 2) limited research of AI urban planning over multiple spatial resolutions or angularities, 3) limited research on augmenting urban design knowledge from data, and 4) limited research on addressing real-world interactions. To address these limitations, we outline future research directions in theory-guided generation, digital twins, and human-machine co-design, calling for a new synthesis of generative intelligence and participatory urbanism.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents</title>
<link>https://arxiv.org/abs/2507.14897</link>
<guid>https://arxiv.org/abs/2507.14897</guid>
<content:encoded><![CDATA[
<div> Framework, Language Model, Reinforcement Learning, Agent-RL, Scalable

Summary:

Framework: The article introduces AgentFly, a framework designed to combine Language Model agents with Reinforcement Learning (Agent-RL) for enhanced capabilities.

Language Model: LM agents are typically built through prompt engineering or supervised finetuning, but the combination with RL has been underexplored.

Reinforcement Learning: RL has been explored to improve LM's abilities in reasoning and factuality, and AgentFly aims to leverage RL algorithms for multi-turn interactions.

Agent-RL: The framework empowers LM agents with various RL algorithms and supports token-level masking for adaptive traditional RL methods.

Scalable: AgentFly features asynchronous execution of tool calls and reward computations, along with centralized resource management, allowing for high-throughput training and successful agent training across multiple tasks. 

<br /><br />Summary: AgentFly is a new framework that combines Language Model agents and Reinforcement Learning to enhance capabilities. It aims to empower LM agents with RL algorithms for improved reasoning and factuality through multi-turn interactions. The framework supports scalability with asynchronous execution and centralized resource management, enabling high-throughput training and successful agent training across various tasks. <div>
arXiv:2507.14897v1 Announce Type: new 
Abstract: Language model (LM) agents have gained significant attention for their ability to autonomously complete tasks through interactions with environments, tools, and APIs. LM agents are primarily built with prompt engineering or supervised finetuning. At the same time, reinforcement learning (RL) has been explored to enhance LM's capabilities, such as reasoning and factuality. However, the combination of the LM agents and reinforcement learning (Agent-RL) remains underexplored and lacks systematic study. To this end, we built AgentFly, a scalable and extensible Agent-RL framework designed to empower LM agents with a variety of RL algorithms. Our framework supports multi-turn interactions by adapting traditional RL methods with token-level masking. It features a decorator-based interface for defining tools and reward functions, enabling seamless extension and ease of use. To support high-throughput training, we implement asynchronous execution of tool calls and reward computations, and design a centralized resource management system for scalable environment coordination. We also provide a suite of prebuilt tools and environments, demonstrating the framework's effectiveness through successful agent training across multiple tasks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis</title>
<link>https://arxiv.org/abs/2507.14899</link>
<guid>https://arxiv.org/abs/2507.14899</guid>
<content:encoded><![CDATA[
<div> Keywords: Non-destructive testing, X-ray inspection, deep learning, interpretability, reliability

Summary:<br /><br />
This paper introduces InsightX Agent, a novel framework for X-ray non-destructive testing (NDT) analysis. It utilizes a Large Multimodal Model (LMM) as a central orchestrator, working with the Sparse Deformable Multi-Scale Detector (SDMSD) and Evidence-Grounded Reflection (EGR) tool. The SDMSD generates defect region proposals and optimizes detection of small targets in X-ray images. The EGR tool guides the LMM agent through a review process, ensuring context assessment, defect analysis, false positive elimination, confidence recalibration, and quality assurance. Experimental evaluations on the GDXray+ dataset show high object detection F1-score and improved interpretability and trustworthiness. This agentic LMM framework goes beyond passive data processing to active reasoning, providing reliable and integrated interpretations for industrial inspection tasks. <div>
arXiv:2507.14899v1 Announce Type: new 
Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for industrial quality assurance, yet existing deep-learning-based approaches often lack interactivity, interpretability, and the capacity for critical self-assessment, limiting their reliability and operator trust. To address these shortcomings, this paper proposes InsightX Agent, a novel LMM-based agentic framework designed to deliver reliable, interpretable, and interactive X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent positions a Large Multimodal Model (LMM) as a central orchestrator, coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect region proposals for multi-scale feature maps and sparsifies them through Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in X-ray images while maintaining computational efficiency. The EGR tool guides the LMM agent through a chain-of-thought-inspired review process, incorporating context assessment, individual defect analysis, false positive elimination, confidence recalibration and quality assurance to validate and refine the SDMSD's initial proposals. By strategically employing and intelligently using tools, InsightX Agent moves beyond passive data processing to active reasoning, enhancing diagnostic reliability and providing interpretations that integrate diverse information sources. Experimental evaluations on the GDXray+ dataset demonstrate that InsightX Agent not only achieves a high object detection F1-score of 96.35% but also offers significantly improved interpretability and trustworthiness in its analyses, highlighting the transformative potential of agentic LLM frameworks for industrial inspection tasks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-Induced Performance Decline in LLM-Based Decision-Making</title>
<link>https://arxiv.org/abs/2507.14906</link>
<guid>https://arxiv.org/abs/2507.14906</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, decision-making, Markov Decision Process, reinforcement learning, structured prompting

Summary: 
Large Language Models (LLMs) show promise in autonomous decision-making but struggle in complex scenarios without fine-tuning. This study explores their performance in Markov Decision Processes (MDPs) compared to traditional reinforcement learning methods. LLMs, pre-trained on diverse data, initially outperform RL strategies in simpler environments but face challenges with planning and reasoning in more intricate tasks. Feedback mechanisms meant to enhance decision-making can often lead to confusion and decreased performance in complex settings. The findings emphasize the importance of exploring hybrid strategies, fine-tuning, and advanced memory integration to improve LLM-based decision-making capabilities. Further research is needed to enhance the adaptability and effectiveness of LLMs in sequential decision-making tasks. 

<br /><br />Summary: <div>
arXiv:2507.14906v1 Announce Type: new 
Abstract: The ability of Large Language Models (LLMs) to extract context from natural language problem descriptions naturally raises questions about their suitability in autonomous decision-making settings. This paper studies the behaviour of these models within a Markov Decision Process (MDPs). While traditional reinforcement learning (RL) strategies commonly employed in this setting rely on iterative exploration, LLMs, pre-trained on diverse datasets, offer the capability to leverage prior knowledge for faster adaptation. We investigate online structured prompting strategies in sequential decision making tasks, comparing the zero-shot performance of LLM-based approaches to that of classical RL methods. Our findings reveal that although LLMs demonstrate improved initial performance in simpler environments, they struggle with planning and reasoning in complex scenarios without fine-tuning or additional guidance. Our results show that feedback mechanisms, intended to improve decision-making, often introduce confusion, leading to diminished performance in intricate environments. These insights underscore the need for further exploration into hybrid strategies, fine-tuning, and advanced memory integration to enhance LLM-based decision-making capabilities.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities</title>
<link>https://arxiv.org/abs/2507.14909</link>
<guid>https://arxiv.org/abs/2507.14909</guid>
<content:encoded><![CDATA[
<div> Keywords: Endless Tuning, Artificial intelligence, Responsibility gap, XAI algorithms, User experience

Summary: 
The Endless Tuning design method aims to deploy artificial intelligence reliably by avoiding human replacement and addressing the responsibility gap. Through a double mirroring process, the protocol was implemented in decision-making applications and tested with domain experts. This approach, rooted in a relational perspective, offers a different ethical voice in AI ethics discussions. Technical choices, such as deploying XAI algorithms in a reversed and hermeneutic manner, are philosophically analyzed. Despite utilizing deep learning models, users perceived full control in decision-making scenarios, indicating a potential bridge between accountability and liability in cases of harm. The study focuses on user experience over statistical accuracy, highlighting the importance of ensuring user perception of control in AI systems.<br /><br />Summary: <div>
arXiv:2507.14909v1 Announce Type: new 
Abstract: The Endless Tuning is a design method for a reliable deployment of artificial intelligence based on a double mirroring process, which pursues both the goals of avoiding human replacement and filling the so-called responsibility gap (Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the relational approach urged therein, it was then actualized in a protocol, implemented in three prototypical applications regarding decision-making processes (respectively: loan granting, pneumonia diagnosis, and art style recognition) and tested with such as many domain experts. Step by step illustrating the protocol, giving insights concretely showing a different voice (Gilligan 1993) in the ethics of artificial intelligence, a philosophical account of technical choices (e.g., a reversed and hermeneutic deployment of XAI algorithms) will be provided in the present study together with the results of the experiments, focusing on user experience rather than statistical accuracy. Even thoroughly employing deep learning models, full control was perceived by the interviewees in the decision-making setting, while it appeared that a bridge can be built between accountability and liability in case of damage.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redefining Elderly Care with Agentic AI: Challenges and Opportunities</title>
<link>https://arxiv.org/abs/2507.14912</link>
<guid>https://arxiv.org/abs/2507.14912</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic AI, Elderly care, Large Language Models, Data privacy, Ethical safeguards 

Summary: 
Agentic AI, powered by Large Language Models (LLMs), has the potential to revolutionize elderly care by enabling proactive and autonomous decision-making. This technology can personalize health tracking, cognitive care, and environmental management to enhance independence and quality of life for older adults. However, there are significant concerns regarding data privacy, decision independence, and access that need to be addressed to ensure responsible use. The article emphasizes the importance of ethical safeguards, privacy protections, and transparent decision-making in integrating Agentic AI into elderly care. The authors highlight the need for human-centered advancements and integration of Agentic AI in elderly care, and provide a comprehensive analysis of the capabilities, applications, and limitations of LLM-based Agentic AI in this context. The article concludes by calling for academic research communities to prioritize the responsible use of Agentic AI in elderly care. 

<br /><br />Summary: <div>
arXiv:2507.14912v1 Announce Type: new 
Abstract: The global ageing population necessitates new and emerging strategies for caring for older adults. In this article, we explore the potential for transformation in elderly care through Agentic Artificial Intelligence (AI), powered by Large Language Models (LLMs). We discuss the proactive and autonomous decision-making facilitated by Agentic AI in elderly care. Personalized tracking of health, cognitive care, and environmental management, all aimed at enhancing independence and high-level living for older adults, represents important areas of application. With a potential for significant transformation of elderly care, Agentic AI also raises profound concerns about data privacy and security, decision independence, and access. We share key insights to emphasize the need for ethical safeguards, privacy protections, and transparent decision-making. Our goal in this article is to provide a balanced discussion of both the potential and the challenges associated with Agentic AI, and to provide insights into its responsible use in elderly care, to bring Agentic AI into harmony with the requirements and vulnerabilities specific to the elderly. Finally, we identify the priorities for the academic research communities, to achieve human-centered advancements and integration of Agentic AI in elderly care. To the best of our knowledge, this is no existing study that reviews the role of Agentic AI in elderly care. Hence, we address the literature gap by analyzing the unique capabilities, applications, and limitations of LLM-based Agentic AI in elderly care. We also provide a companion interactive dashboard at https://hazratali.github.io/agenticai/.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complexity of Faceted Explanations in Propositional Abduction</title>
<link>https://arxiv.org/abs/2507.14962</link>
<guid>https://arxiv.org/abs/2507.14962</guid>
<content:encoded><![CDATA[
<div> Keywords: Abductive reasoning, Propositional abduction, Computational complexity, Facets, Heterogeneity

Summary: 
Abductive reasoning, a non-monotonic paradigm, is used for explaining observed symptoms. In propositional abduction, focusing on propositional formulas, the computational complexity of tasks has been extensively studied, with counting and enumeration being particularly challenging. This study introduces facets in propositional abductions, which are relevant literals that occur in some explanations but not all, providing a more nuanced understanding of variability. By considering facets and the distance between explanations, the study aims to analyze heterogeneity and homogeneity in explanations. The research comprehensively analyzes facets in various settings, including a nearly complete characterization in Post's framework. This approach allows for better reasoning between decisions and counting and enhances the understanding of explanations in abductive reasoning. <div>
arXiv:2507.14962v1 Announce Type: new 
Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain observed symptoms and manifestations. It has many applications, such as diagnosis and planning in artificial intelligence and database updates. In propositional abduction, we focus on specifying knowledge by a propositional formula. The computational complexity of tasks in propositional abduction has been systematically characterized - even with detailed classifications for Boolean fragments. Unsurprisingly, the most insightful reasoning problems (counting and enumeration) are computationally highly challenging. Therefore, we consider reasoning between decisions and counting, allowing us to understand explanations better while maintaining favorable complexity. We introduce facets to propositional abductions, which are literals that occur in some explanation (relevant) but not all explanations (dispensable). Reasoning with facets provides a more fine-grained understanding of variability in explanations (heterogeneous). In addition, we consider the distance between two explanations, enabling a better understanding of heterogeneity/homogeneity. We comprehensively analyze facets of propositional abduction in various settings, including an almost complete characterization in Post's framework.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.14987</link>
<guid>https://arxiv.org/abs/2507.14987</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, AlphaAlign, safety alignment, reinforcement learning, proactive safety reasoning

Summary:
AlphaAlign is a new reinforcement learning framework designed to improve the safety awareness of large language models. It utilizes a dual-reward system to incentivize latent safety awareness: a verifiable safety reward for correct refusals of harmful content and a helpfulness reward for high-quality responses to benign inputs. This framework simplifies the safety alignment process by requiring only binary prompt safety labels and minimal RL steps. AlphaAlign successfully breaks the safety-utility trade-off by enhancing refusal of harmful content, reducing over-refusals, and maintaining or improving task performance and robustness. It fosters proactive safety reasoning, encouraging explicit safety rationales rather than shallow refusal patterns. Overall, AlphaAlign is an efficient and effective approach for improving the safety capabilities of LLMs. 

<br /><br />Summary: <div>
arXiv:2507.14987v1 Announce Type: new 
Abstract: Large language models (LLMs), despite possessing latent safety understanding from their vast pretraining data, remain vulnerable to generating harmful content and exhibit issues such as over-refusal and utility degradation after safety alignment. Current safety alignment methods often result in superficial refusal shortcuts or rely on intensive supervision for reasoning-based approaches, failing to fully leverage the model's intrinsic safety self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure reinforcement learning (RL) framework with verifiable safety reward designed to incentivize this latent safety awareness through proactive safety reasoning.} AlphaAlign employs a dual-reward system: a verifiable safety reward encourages correctly formatted and explicitly justified refusals for harmful queries while penalizing over-refusals, and a normalized helpfulness reward guides high-quality responses to benign inputs. This allows the model to develop proactive safety reasoning capabilities without depending on supervised safety-specific reasoning data. AlphaAlign demonstrates three key advantages: (1) Simplicity and efficiency, requiring only binary prompt safety labels and minimal RL steps for substantial improvements. (2) Breaking the safety-utility trade-off, by enhancing refusal of harmful content and reducing over-refusals, while simultaneously maintaining or even improving general task performance and robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety reasoning that generates explicit safety rationales rather than relying on shallow refusal patterns.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing</title>
<link>https://arxiv.org/abs/2507.15013</link>
<guid>https://arxiv.org/abs/2507.15013</guid>
<content:encoded><![CDATA[
<div> Keywords: psychometric tests, deep learning, neural networks, forced-choice tests, interpretability<br />
Summary:<br />
- The study introduces a Forced-Choice Neural Cognitive Diagnostic Model (FCNCD) for psychometric testing in the smart era.
- FCNCD utilizes deep learning to address limitations of traditional models and works with different item block types in forced-choice tests.
- The model incorporates participant and item parameters, capturing interactions using multilayer neural networks.
- Nonlinear mapping is used to mine participant and item features, improving the interpretability of diagnostic results.
- The FCNCD's effectiveness is verified through experiments on real and simulated datasets, showcasing accuracy, interpretability, and robustness. <br /><br />Summary: <div>
arXiv:2507.15013v1 Announce Type: new 
Abstract: In the smart era, psychometric tests are becoming increasingly important for personnel selection, career development, and mental health assessment. Forced-choice tests are common in personality assessments because they require participants to select from closely related options, lowering the risk of response distortion. This study presents a deep learning-based Forced-Choice Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of traditional models and is applicable to the three most common item block types found in forced-choice tests. To account for the unidimensionality of items in forced-choice tests, we create interpretable participant and item parameters. We model the interactions between participant and item features using multilayer neural networks after mining them using nonlinear mapping. In addition, we use the monotonicity assumption to improve the interpretability of the diagnostic results. The FCNCD's effectiveness is validated by experiments on real-world and simulated datasets that show its accuracy, interpretability, and robustness.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection</title>
<link>https://arxiv.org/abs/2507.15042</link>
<guid>https://arxiv.org/abs/2507.15042</guid>
<content:encoded><![CDATA[
<div> Adversarial prompt attacks, Differential Evolution, RAG-based question answering, BEIR QA datasets, suffix construction<br />
<br />
Summary: 
This paper introduces a novel method using Differential Evolution to optimize adversarial prompt suffixes for RAG-based question answering systems. The approach is gradient-free and evolves candidate suffixes to maximize the retrieval rank of a targeted incorrect document. Experiments on BEIR QA datasets show competitive success rates compared to other methods with a small number of tokens in the adversarial suffix. A readability-aware suffix construction strategy reduces MLM negative log-likelihood significantly. DE-generated suffixes evade detection by a BERT-based adversarial suffix detector, with near-chance detection accuracy. <div>
arXiv:2507.15042v1 Announce Type: new 
Abstract: Adversarial prompt attacks can significantly alter the reliability of Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce incorrect outputs. In this paper, we present a novel method that applies Differential Evolution (DE) to optimize adversarial prompt suffixes for RAG-based question answering. Our approach is gradient-free, treating the RAG pipeline as a black box and evolving a population of candidate suffixes to maximize the retrieval rank of a targeted incorrect document to be closer to real world scenarios. We conducted experiments on the BEIR QA datasets to evaluate attack success at certain retrieval rank thresholds under multiple retrieving applications. Our results demonstrate that DE-based prompt optimization attains competitive (and in some cases higher) success rates compared to GGPP to dense retrievers and PRADA to sparse retrievers, while using only a small number of tokens (<=5 tokens) in the adversarial suffix. Furthermore, we introduce a readability-aware suffix construction strategy, validated by a statistically significant reduction in MLM negative log-likelihood with Welch's t-test. Through evaluations with a BERT-based adversarial suffix detector, we show that DE-generated suffixes evade detection, yielding near-chance detection accuracy.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward</title>
<link>https://arxiv.org/abs/2507.15106</link>
<guid>https://arxiv.org/abs/2507.15106</guid>
<content:encoded><![CDATA[
<div> CAIS, intrinsic reward, causal inference, reinforcement learning, autonomous systems <br />
<br />
Summary: 
The article introduces the Causal Action Influence Score (CAIS), an intrinsic reward based on causal inference to address the brittleness of standard reinforcement learning agents in noisy environments. CAIS quantifies an action's influence by measuring the 1-Wasserstein distance between the learned distribution of sensory outcomes conditional on that action and the baseline outcome distribution. In a simulated infant-mobile environment, where correlation-based rewards fail, CAIS enables the agent to filter noise, identify its influence, and learn the correct policy. The predictive model learned for CAIS, when augmented with a surprise signal, successfully reproduces the "extinction burst" phenomenon. The study highlights the importance of inferring causality for developing a robust sense of agency in autonomous systems. <div>
arXiv:2507.15106v1 Announce Type: new 
Abstract: While human infants robustly discover their own causal efficacy, standard reinforcement learning agents remain brittle, as their reliance on correlation-based rewards fails in noisy, ecologically valid scenarios. To address this, we introduce the Causal Action Influence Score (CAIS), a novel intrinsic reward rooted in causal inference. CAIS quantifies an action's influence by measuring the 1-Wasserstein distance between the learned distribution of sensory outcomes conditional on that action, $p(h|a)$, and the baseline outcome distribution, $p(h)$. This divergence provides a robust reward that isolates the agent's causal impact from confounding environmental noise. We test our approach in a simulated infant-mobile environment where correlation-based perceptual rewards fail completely when the mobile is subjected to external forces. In stark contrast, CAIS enables the agent to filter this noise, identify its influence, and learn the correct policy. Furthermore, the high-quality predictive model learned for CAIS allows our agent, when augmented with a surprise signal, to successfully reproduce the "extinction burst" phenomenon. We conclude that explicitly inferring causality is a crucial mechanism for developing a robust sense of agency, offering a psychologically plausible framework for more adaptive autonomous systems.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated planning with ontologies under coherence update semantics</title>
<link>https://arxiv.org/abs/2507.15120</link>
<guid>https://arxiv.org/abs/2507.15120</guid>
<content:encoded><![CDATA[
<div> DL-Lite Ontologies, Automated Planning, Ontology-Based Action Conditions, Coherence Update Semantics, Polynomial Compilation<br />
Summary:<br />
This article introduces a new approach for automated planning that incorporates DL-Lite ontologies, combining the benefits of ontology-based action conditions and ontology-aware action effects. The approach aims to enhance planning by utilizing background knowledge in the form of ontologies under open-world semantics. By using explicit-input knowledge and action bases (eKABs) along with coherence update semantics, the complexity of the resulting formalism remains manageable. The authors provide an implementation of the approach through a polynomial compilation into classical planning, showcasing its practicality. An evaluation on various benchmarks demonstrates the performance of the planning system with different variants of the compiled approach. Overall, this work contributes to the advancement of automated planning by integrating ontological knowledge for more efficient problem-solving capabilities. <br /><br />Summary: <div>
arXiv:2507.15120v1 Announce Type: new 
Abstract: Standard automated planning employs first-order formulas under closed-world semantics to achieve a goal with a given set of actions from an initial state. We follow a line of research that aims to incorporate background knowledge into automated planning problems, for example, by means of ontologies, which are usually interpreted under open-world semantics. We present a new approach for planning with DL-Lite ontologies that combines the advantages of ontology-based action conditions provided by explicit-input knowledge and action bases (eKABs) and ontology-aware action effects under the coherence update semantics. We show that the complexity of the resulting formalism is not higher than that of previous approaches and provide an implementation via a polynomial compilation into classical planning. An evaluation of existing and new benchmarks examines the performance of a planning system on different variants of our compilation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis</title>
<link>https://arxiv.org/abs/2507.15140</link>
<guid>https://arxiv.org/abs/2507.15140</guid>
<content:encoded><![CDATA[
<div> oral diseases, artificial intelligence, clinical diagnosis, multimodal model, differential diagnosis

Summary:<br />
- The article introduces the Clinical Semantic Intelligence (CSI) framework that uses artificial intelligence to diagnose 118 different oral diseases by simulating expert clinician reasoning.<br />
- CSI incorporates a multimodal CLIP model and a specialized ChatGLM-6B language model in a Hierarchical Diagnostic Reasoning Tree (HDRT) for systematic differential diagnosis.<br />
- The system operates in Fast Mode for quick screening and Standard Mode for in-depth diagnostic workup.<br />
- Training and validation involved a dataset of 4,310 images with augmentation for over 30,000 image-text pairs.<br />
- Evaluation on a 431-image internal test set showed accuracy of 73.4% in Fast Mode and 89.5% in Standard Mode due to the hierarchical reasoning process. 

<br /><br />Summary: <div>
arXiv:2507.15140v1 Announce Type: new 
Abstract: The diagnosis of oral diseases presents a problematic clinical challenge, characterized by a wide spectrum of pathologies with overlapping symptomatology. To address this, we developed Clinical Semantic Intelligence (CSI), a novel artificial intelligence framework that diagnoses 118 different oral diseases by computationally modeling the cognitive processes of an expert clinician. Our core hypothesis is that moving beyond simple pattern matching to emulate expert reasoning is critical to building clinically useful diagnostic aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a specialized ChatGLM-6B language model. This system executes a Hierarchical Diagnostic Reasoning Tree (HDRT), a structured framework that distills the systematic, multi-step logic of differential diagnosis. The framework operates in two modes: a Fast Mode for rapid screening and a Standard Mode that leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310 images, supplemented by an external hold-out set of 176 images for final validation. A clinically-informed augmentation strategy expanded our training data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the HDRT-driven Standard Mode. The performance gain is directly attributable to the hierarchical reasoning process. Herein, we detail the architectural philosophy, development, and rigorous evaluation of the CSI framework.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City</title>
<link>https://arxiv.org/abs/2507.15143</link>
<guid>https://arxiv.org/abs/2507.15143</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, smart city, simulation framework, artificial intelligence, sustainable infrastructure

Summary: 
This paper explores the feasibility of human mobility within The Line, a proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. By integrating agent-based modeling, reinforcement learning, supervised learning, and graph neural networks, a simulation framework was developed to assess transportation behaviors across various density scenarios. Results showed that with AI integration, citizens achieved an average commute time of 7.8 to 8.4 minutes, high satisfaction rates, and excellent reachability, even during peak congestion. Removing intelligent modules led to significant performance degradation. Environmental modeling indicated low energy consumption and minimal CO2 emissions with electric modes prioritized. The study suggests that achieving freedom of movement in The Line is not only conceptually possible but operationally realistic with adaptive AI systems, sustainable infrastructure, and real-time feedback loops.

<br /><br />Summary: <div>
arXiv:2507.15143v1 Announce Type: new 
Abstract: This paper investigates the feasibility of human mobility in The Line, a proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess whether citizens can move freely within this unprecedented urban topology, we develop a hybrid simulation framework that integrates agent-based modeling, reinforcement learning, supervised learning, and graph neural networks. The simulation captures multi-modal transportation behaviors across 50 vertical levels and varying density scenarios using both synthetic data and real-world traces from high-density cities. Our experiments reveal that with the full AI-integrated architecture, agents achieved an average commute time of 7.8 to 8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index of over 91 percent, even during peak congestion periods. Ablation studies confirmed that the removal of intelligent modules such as reinforcement learning or graph neural networks significantly degrades performance, with commute times increasing by up to 85 percent and reachability falling below 70 percent. Environmental modeling further demonstrated low energy consumption and minimal CO2 emissions when electric modes are prioritized. The findings suggest that freedom of movement is not only conceptually achievable in The Line, but also operationally realistic if supported by adaptive AI systems, sustainable infrastructure, and real-time feedback loops.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Formal Math Problems by Decomposition and Iterative Reflection</title>
<link>https://arxiv.org/abs/2507.15225</link>
<guid>https://arxiv.org/abs/2507.15225</guid>
<content:encoded><![CDATA[
<div> framework, Delta Prover, formal proofs, Lean 4, theorem proving <br />
Summary:
The article introduces the Delta Prover framework, which enables general-purpose Large Language Models (LLMs) to generate formal proofs in Lean 4 without the need for model specialization. The framework uses reflective decomposition and iterative proof repair algorithms, along with a custom Domain-Specific Language (DSL) for subproblem management. Delta Prover achieves a remarkable 95.9% success rate on the miniF2F-test benchmark, surpassing existing approaches. It also demonstrates a stronger test-time scaling law compared to standard proof strategies. The findings suggest that general-purpose LLMs, guided by effective structures, have significant theorem-proving capabilities, offering an efficient alternative to specialized models for automated reasoning in formal environments. <br /><br />Summary: <div>
arXiv:2507.15225v1 Announce Type: new 
Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success in intelligence, performing comparably to human experts on complex reasoning tasks such as coding and mathematical reasoning. However, generating formal proofs in specialized languages like Lean 4 remains a significant challenge for these models, limiting their application in complex theorem proving and automated verification. Current approaches typically require specializing models through fine-tuning on dedicated formal corpora, incurring high costs for data collection and training. In this work, we introduce \textbf{Delta Prover}, an agent-based framework that orchestrates the interaction between a general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages the reflection and reasoning capabilities of general-purpose LLMs to interactively construct formal proofs in Lean 4, circumventing the need for model specialization. At its core, the agent integrates two novel, interdependent components: an algorithmic framework for reflective decomposition and iterative proof repair, and a custom Domain-Specific Language (DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test benchmark, surpassing all existing approaches, including those requiring model specialization.} Furthermore, Delta Prover exhibits a significantly stronger test-time scaling law compared to standard Best-of-N proof strategies. Crucially, our findings demonstrate that general-purpose LLMs, when guided by an effective agentic structure, possess substantial untapped theorem-proving capabilities. This presents a computationally efficient alternative to specialized models for robust automated reasoning in formal environments.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis</title>
<link>https://arxiv.org/abs/2507.15239</link>
<guid>https://arxiv.org/abs/2507.15239</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, arc fault diagnosis, Explainable Artificial Intelligence, neural network, machine learning

Summary:<br />
This work introduces a novel AI-based approach for arc fault diagnosis, aiming to improve the trustworthiness of the models. A soft evaluation indicator is proposed to explain the outputs of the diagnosis models by leveraging Explainable Artificial Intelligence and real arc fault experiments. A lightweight balanced neural network is also introduced to ensure high accuracy and soft feature extraction. The study compares traditional machine learning methods and deep learning methods on two different arc fault datasets with varying sample times and noise levels to validate the effectiveness of the evaluation indicator. Overall, this approach enhances the interpretability and reliability of arc fault diagnosis models, enabling practitioners to make well-informed and trustworthy decisions.<br /><br />Summary: <div>
arXiv:2507.15239v1 Announce Type: new 
Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding performance in terms of classification accuracy. However, an inherent problem is whether these models can actually be trusted to find arc faults. In this light, this work proposes a soft evaluation indicator that explains the outputs of arc fault diagnosis models, by defining the the correct explanation of arc faults and leveraging Explainable Artificial Intelligence and real arc fault experiments. Meanwhile, a lightweight balanced neural network is proposed to guarantee competitive accuracy and soft feature extraction score. In our experiments, several traditional machine learning methods and deep learning methods across two arc fault datasets with different sample times and noise levels are utilized to test the effectiveness of the soft evaluation indicator. Through this approach, the arc fault diagnosis models are easy to understand and trust, allowing practitioners to make informed and trustworthy decisions.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Homophily and Heterophily in Multimodal Graph Clustering</title>
<link>https://arxiv.org/abs/2507.15253</link>
<guid>https://arxiv.org/abs/2507.15253</guid>
<content:encoded><![CDATA[
<div> Keyword: multimodal graph clustering, hybrid neighborhood patterns, Disentangled Multimodal Graph Clustering (DMGC), Multimodal Dual-frequency Fusion, self-supervised alignment<br />
Summary:
Multimodal graph clustering, integrating structured and unstructured data, lacks exploration in unsupervised learning. Real-world multimodal graphs often show hybrid neighborhood patterns with homophilic and heterophilic relationships. To address this, a novel framework, DMGC, decomposes graphs into homophily-enhanced and heterophily-aware views. A Multimodal Dual-frequency Fusion mechanism filters these views for effective integration. Self-supervised alignment objectives guide learning without labels. Extensive experiments on diverse datasets validate DMGC's state-of-the-art performance and generalizability. The proposed framework bridges the gap in multimodal graph clustering, offering insights into hybrid relationships and effective integration strategies.<br /><br />Summary: <div>
arXiv:2507.15253v1 Announce Type: new 
Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with structured interconnections, offer substantial real-world utility but remain insufficiently explored in unsupervised learning. In this work, we initiate the study of multimodal graph clustering, aiming to bridge this critical gap. Through empirical analysis, we observe that real-world multimodal graphs often exhibit hybrid neighborhood patterns, combining both homophilic and heterophilic relationships. To address this challenge, we propose a novel framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which decomposes the original hybrid graph into two complementary views: (1) a homophily-enhanced graph that captures cross-modal class consistency, and (2) heterophily-aware graphs that preserve modality-specific inter-class distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism that jointly filters these disentangled graphs through a dual-pass strategy, enabling effective multimodal integration while mitigating category confusion. Our self-supervised alignment objectives further guide the learning process without requiring labels. Extensive experiments on both multimodal and multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art performance, highlighting its effectiveness and generalizability across diverse settings. Our code is available at https://github.com/Uncnbb/DMGC.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry</title>
<link>https://arxiv.org/abs/2507.15268</link>
<guid>https://arxiv.org/abs/2507.15268</guid>
<content:encoded><![CDATA[
<div> Keywords: injection molding, knowledge transfer, multi-agent framework, large language models, AI-assisted decision support

Summary:
IM-Chat is a multi-agent framework utilizing large language models to facilitate knowledge transfer in the injection molding industry. It integrates documented knowledge and field data to resolve manufacturing tasks effectively. By employing a retrieval-augmented generation strategy and tool-calling agents, IM-Chat ensures adaptability without fine-tuning. Performance evaluation using GPT-4o, GPT-4o-mini, and GPT-3.5-turbo showed higher accuracy with more capable models, especially in complex scenarios. The study demonstrates the effectiveness of multi-agent LLM systems for industrial knowledge workflows and establishes IM-Chat as a scalable and generalizable AI-assisted decision support approach in manufacturing.

<br /><br />Summary: IM-Chat, a multi-agent framework powered by large language models, enhances knowledge transfer in injection molding by integrating documented knowledge and field data. Its adaptability and high accuracy in resolving manufacturing tasks, particularly in complex scenarios, make it a valuable tool for AI-assisted decision support in the industry. <div>
arXiv:2507.15268v1 Announce Type: new 
Abstract: The injection molding industry faces critical challenges in preserving and transferring field knowledge, particularly as experienced workers retire and multilingual barriers hinder effective communication. This study introduces IM-Chat, a multi-agent framework based on large language models (LLMs), designed to facilitate knowledge transfer in injection molding. IM-Chat integrates both limited documented knowledge (e.g., troubleshooting tables, manuals) and extensive field data modeled through a data-driven process condition generator that infers optimal manufacturing settings from environmental inputs such as temperature and humidity, enabling robust and context-aware task resolution. By adopting a retrieval-augmented generation (RAG) strategy and tool-calling agents within a modular architecture, IM-Chat ensures adaptability without the need for fine-tuning. Performance was assessed across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance and correctness, and was further supplemented by automated evaluation using GPT-4o guided by a domain-adapted instruction prompt. The evaluation results indicate that more capable models tend to achieve higher accuracy, particularly in complex, tool-integrated scenarios. Overall, these findings demonstrate the viability of multi-agent LLM systems for industrial knowledge workflows and establish IM-Chat as a scalable and generalizable approach to AI-assisted decision support in manufacturing.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI</title>
<link>https://arxiv.org/abs/2507.15330</link>
<guid>https://arxiv.org/abs/2507.15330</guid>
<content:encoded><![CDATA[
<div> Vulnerability Class, Agentic AI Systems, Cognitive Degradation, Qorvex Security AI Framework, Resilient Behavior<br />
<br />
Summary:
Cognitive Degradation is identified as a new vulnerability class in agentic AI systems due to internal failures like memory starvation and output suppression, leading to agent drift and logic collapse. The Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain 10) is introduced to address these failures with a six-stage lifecycle and seven runtime controls for proactive mitigation. The framework monitors agent subsystems in real time and uses fallback routing and memory integrity enforcement to combat cognitive degradation. By mapping agentic architectures to human analogs, the framework can detect fatigue and role collapse early on. This work establishes Cognitive Degradation as a critical vulnerability in AI systems and proposes a defense model for resilient agentic behavior. <br /><br /> <div>
arXiv:2507.15330v1 Announce Type: new 
Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic AI systems. Unlike traditional adversarial external threats such as prompt injection, these failures originate internally, arising from memory starvation, planner recursion, context flooding, and output suppression. These systemic weaknesses lead to silent agent drift, logic collapse, and persistent hallucinations over time. To address this class of failures, we introduce the Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain 10), a lifecycle-aware defense framework defined by a six-stage cognitive degradation lifecycle. The framework includes seven runtime controls (QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger proactive mitigation through fallback routing, starvation detection, and memory integrity enforcement. Drawing from cognitive neuroscience, we map agentic architectures to human analogs, enabling early detection of fatigue, starvation, and role collapse. By introducing a formal lifecycle and real-time mitigation controls, this work establishes Cognitive Degradation as a critical new class of AI system vulnerability and proposes the first cross-platform defense model for resilient agentic behavior.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms</title>
<link>https://arxiv.org/abs/2507.15351</link>
<guid>https://arxiv.org/abs/2507.15351</guid>
<content:encoded><![CDATA[
<div> MARL, on-demand ride-sharing, Q-values, V-values, GRPO <br /> 
Summary: <br /> 
This article introduces novel methods for improving on-demand ride-sharing platforms using Multi-Agent Reinforcement Learning (MARL). Traditional MARL approaches rely heavily on accurate estimation of Q-values or V-values, which can be challenging in large-scale, uncertain environments. The proposed methods, GRPO and OSPO, bypass the need for value function estimation. GRPO replaces the PPO baseline with group average reward to reduce training bias, while OSPO utilizes one-step rewards for policy optimization. Experiments on a real-world Manhattan ride-hailing dataset demonstrate superior performance of GRPO and OSPO in optimizing pickup times and served orders using simple MLP networks. This research highlights the potential of MARL in addressing the dynamic challenges of on-demand ride-sharing platforms. <br /> <div>
arXiv:2507.15351v1 Announce Type: new 
Abstract: On-demand ride-sharing platforms face the fundamental challenge of dynamically bundling passengers with diverse origins and destinations and matching them with vehicles in real time, all under significant uncertainty. Recently, MARL has emerged as a promising solution for this problem, leveraging decentralized learning to address the curse of dimensionality caused by the large number of agents in the ride-hailing market and the resulting expansive state and action spaces. However, conventional MARL-based ride-sharing approaches heavily rely on the accurate estimation of Q-values or V-values, which becomes problematic in large-scale, highly uncertain environments. Specifically, most of these approaches adopt an independent paradigm, exacerbating this issue, as each agent treats others as part of the environment, leading to unstable training and substantial estimation bias in value functions. To address these challenges, we propose two novel alternative methods that bypass value function estimation. First, we adapt GRPO to ride-sharing, replacing the PPO baseline with the group average reward to eliminate critic estimation errors and reduce training bias. Second, inspired by GRPO's full utilization of group reward information, we customize the PPO framework for ride-sharing platforms and show that, under a homogeneous fleet, the optimal policy can be trained using only one-step rewards - a method we term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior performance across most scenarios, efficiently optimizing pickup times and the number of served orders using simple MLP networks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAD: Retrieval High-quality Demonstrations to Enhance Decision-making</title>
<link>https://arxiv.org/abs/2507.15356</link>
<guid>https://arxiv.org/abs/2507.15356</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, offline learning, trajectory stitching, generative modeling, state retrieval

Summary:
RAD (Retrieval High-quAlity Demonstrations) proposes a novel approach to offline reinforcement learning that combines non-parametric retrieval with generative modeling. By dynamically retrieving high-return states from the dataset and planning towards them using a diffusion model, RAD improves long-horizon planning and generalization capabilities. This retrieval-guided generation allows for flexible trajectory stitching and better handling of underrepresented or out-of-distribution states. Experimental results show that RAD outperforms existing methods on various benchmarks, demonstrating its effectiveness in improving decision-making in RL tasks. <div>
arXiv:2507.15356v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) enables agents to learn policies from fixed datasets, avoiding costly or unsafe environment interactions. However, its effectiveness is often limited by dataset sparsity and the lack of transition overlap between suboptimal and expert trajectories, which makes long-horizon planning particularly challenging. Prior solutions based on synthetic data augmentation or trajectory stitching often fail to generalize to novel states and rely on heuristic stitching points. To address these challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for decision-making, which combines non-parametric retrieval with diffusion-based generative modeling. RAD dynamically retrieves high-return states from the offline dataset as target states based on state similarity and return estimation, and plans toward them using a condition-guided diffusion model. Such retrieval-guided generation enables flexible trajectory stitching and improves generalization when encountered with underrepresented or out-of-distribution states. Extensive experiments confirm that RAD achieves competitive or superior performance compared to baselines across diverse benchmarks, validating its effectiveness.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Process Monitoring Using Object-centric Graph Embeddings</title>
<link>https://arxiv.org/abs/2507.15411</link>
<guid>https://arxiv.org/abs/2507.15411</guid>
<content:encoded><![CDATA[
<div> Keywords: object-centric predictive process monitoring, graph attention network, LSTM network, next activity prediction, next event time

Summary:
Object-centric predictive process monitoring aims to improve process predictions by utilizing object-centric event logs. The paper proposes an end-to-end model that predicts future process behavior, focusing on next activity prediction and next event time. The model combines a graph attention network to encode activities and their relationships with an LSTM network to handle temporal dependencies. Evaluation on real-life and synthetic event logs shows competitive performance compared to existing methods. The model successfully extracts relevant information and effectively predicts process behavior, demonstrating its effectiveness in enhancing process predictions. <div>
arXiv:2507.15411v1 Announce Type: new 
Abstract: Object-centric predictive process monitoring explores and utilizes object-centric event logs to enhance process predictions. The main challenge lies in extracting relevant information and building effective models. In this paper, we propose an end-to-end model that predicts future process behavior, focusing on two tasks: next activity prediction and next event time. The proposed model employs a graph attention network to encode activities and their relationships, combined with an LSTM network to handle temporal dependencies. Evaluated on one reallife and three synthetic event logs, the model demonstrates competitive performance compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Activity Batching Policies in Business Processes</title>
<link>https://arxiv.org/abs/2507.15457</link>
<guid>https://arxiv.org/abs/2507.15457</guid>
<content:encoded><![CDATA[
<div> Keywords: business processes, activity batching, batching policies, Pareto optimization, intervention heuristics

Summary:
This paper focuses on the problem of discovering optimal batching policies in business processes to balance waiting time, processing effort, and cost. It introduces a Pareto optimization approach that utilizes intervention heuristics to generate alternative policies for each batched activity. These heuristics identify opportunities to improve batching policies based on metrics such as waiting time, processing time, cost, or resource utilization. The impact of each intervention is evaluated through simulation. The approach incorporates three meta-heuristics - hill-climbing, simulated annealing, and reinforcement learning - to iteratively update the Pareto front of interventions. An experimental evaluation compares the proposed heuristic-guided approach against non-heuristic guided meta-heuristics in terms of convergence, diversity, and cycle time gain of Pareto-optimal policies. <div>
arXiv:2507.15457v1 Announce Type: new 
Abstract: In business processes, activity batching refers to packing multiple activity instances for joint execution. Batching allows managers to trade off cost and processing effort against waiting time. Larger and less frequent batches may lower costs by reducing processing effort and amortizing fixed costs, but they create longer waiting times. In contrast, smaller and more frequent batches reduce waiting times but increase fixed costs and processing effort. A batching policy defines how activity instances are grouped into batches and when each batch is activated. This paper addresses the problem of discovering batching policies that strike optimal trade-offs between waiting time, processing effort, and cost. The paper proposes a Pareto optimization approach that starts from a given set (possibly empty) of activity batching policies and generates alternative policies for each batched activity via intervention heuristics. Each heuristic identifies an opportunity to improve an activity's batching policy with respect to a metric (waiting time, processing time, cost, or resource utilization) and an associated adjustment to the activity's batching policy (the intervention). The impact of each intervention is evaluated via simulation. The intervention heuristics are embedded in an optimization meta-heuristic that triggers interventions to iteratively update the Pareto front of the interventions identified so far. The paper considers three meta-heuristics: hill-climbing, simulated annealing, and reinforcement learning. An experimental evaluation compares the proposed approach based on intervention heuristics against the same (non-heuristic guided) meta-heuristics baseline regarding convergence, diversity, and cycle time gain of Pareto-optimal policies.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner</title>
<link>https://arxiv.org/abs/2507.15509</link>
<guid>https://arxiv.org/abs/2507.15509</guid>
<content:encoded><![CDATA[
<div> Keywords: Chart-R1, reinforcement learning, multimodal data, complex reasoning, chart reasoning

Summary:
Chart-R1 is a new vision-language model that utilizes reinforcement learning fine-tuning for complex chart reasoning tasks. The model is trained using a two-stage strategy: Chart-COT for step-by-step chain-of-thought supervision and Chart-RFT for reinforcement fine-tuning with numerical sensitivity. A novel programmatic data synthesis technology is introduced to generate high-quality chart reasoning data. The experiments conducted on both open-source benchmarks and a self-built dataset (ChartRQA) demonstrate the effectiveness of Chart-R1, showing significant advantages over existing chart-domain methods and comparable performance to large-scale models like GPT-4o and Claude-3.5. The research highlights the importance of incorporating multimodal data and complex reasoning into language models for enhanced performance in chart reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2507.15509v1 Announce Type: new 
Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based on reinforcement learning fine-tuning has received widespread attention from the community. Previous R1-Style methods mainly focus on mathematical reasoning and code intelligence. It is of great research significance to verify their advantages on more general multimodal data. Chart is an important multimodal data type with rich information, which brings important research challenges in complex reasoning. In this work, we introduce Chart-R1, a chart-domain vision-language model with reinforcement learning fine-tuning to enable complex chart reasoning. To support Chart-R1, we first propose a novel programmatic data synthesis technology to generate high-quality step-by-step chart reasoning data covering single- and multi-subcharts, which makes up for the lack of reasoning data in the chart domain. Then we develop a two-stage training strategy: Chart-COT with step-by-step chain-of-thought supervision, and Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims to decompose complex chart reasoning tasks into fine-grained, understandable subtasks through step-by-step supervision, which lays a good foundation for improving the reasoning level of reinforcement learning. Chart-RFT utilize the typical group relative policy optimization strategy, in which a relatively soft reward is adopted for numerical response to emphasize the numerical sensitivity in the chart domain. We conduct extensive experiments on open-source benchmarks and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental results show that Chart-R1 has significant advantages compared to chart-domain methods, even comparable to open/closed source large-scale models (\emph{e.g., GPT-4o, Claude-3.5}).
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics</title>
<link>https://arxiv.org/abs/2507.15518</link>
<guid>https://arxiv.org/abs/2507.15518</guid>
<content:encoded><![CDATA[
<div> Framework, Drama generation, Interactive narrative, Language model, Online performance  
Summary:  
The article introduces HAMLET, a multi-agent framework for creating immersive and interactive theatrical experiences. By leveraging large language models, HAMLET generates a narrative blueprint and allows actors to make autonomous decisions during online performances. Each actor has their own background, goals, and emotional state, enabling them to interact with the physical environment and engage in dynamic conversations with other actors. The framework evaluates character performance, narrative quality, and interaction experience to ensure a high-quality drama performance. The experimental evaluation demonstrates that HAMLET can produce expressive and coherent theatrical experiences. The code, dataset, and models are available on GitHub for further exploration and development.  
<br /><br />Summary: <div>
arXiv:2507.15518v1 Announce Type: new 
Abstract: Creating an immersive and interactive theatrical experience is a long-term goal in the field of interactive narrative. The emergence of large language model (LLM) is providing a new path to achieve this goal. However, existing LLM-based drama generation methods often result in AI agents that lack initiative and cannot interact with the physical environment. Furthermore, these methods typically require detailed user input to drive the drama. These limitations reduce the interactivity and immersion of online real-time performance. To address the above challenges, we propose HAMLET, a multi-agent framework focused on drama creation and online performance. Given a simple topic, the framework generates a narrative blueprint, guiding the subsequent improvisational performance. During the online performance, each actor is given an autonomous mind. This means that actors can make independent decisions based on their own background, goals, and emotional state. In addition to conversations with other actors, their decisions can also change the state of scene props through actions such as opening a letter or picking up a weapon. The change is then broadcast to other related actors, updating what they know and care about, which in turn influences their next action. To evaluate the quality of drama performance, we designed an evaluation method to assess three primary aspects, including character performance, narrative quality, and interaction experience. The experimental evaluation shows that HAMLET can create expressive and coherent theatrical experiences. Our code, dataset and models are available at https://github.com/HAMLET-2025/HAMLET.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning</title>
<link>https://arxiv.org/abs/2507.15521</link>
<guid>https://arxiv.org/abs/2507.15521</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, pulley systems, mental models, cognitive science, artificial intelligence

Summary: <br /><br />Large language models (LLMs) were tested on their ability to estimate mechanical advantage (MA) in pulley systems. Study 1 showed that LLMs performed above chance and correlated with ground-truth MA values, suggesting they use a pulley counting heuristic. Study 2 demonstrated that models could differentiate between functional and non-functional pulley systems, indicating an ability to represent spatial relations of system components. However, Study 3 revealed limitations in reasoning over nuanced structural connectivity. Overall, the findings suggest that LLMs may possess the capacity to manipulate internal world models to exploit statistical associations and approximate system components' spatial relations, but struggle with more complex structural reasoning. This study underscores the importance of applying cognitive science methodologies to evaluate the world-modeling capabilities of artificial intelligence systems. <br /> <div>
arXiv:2507.15521v1 Announce Type: new 
Abstract: Do large language models (LLMs) construct and manipulate internal world models, or do they rely solely on statistical associations represented as output layer token probabilities? We adapt cognitive science methodologies from human mental models research to test LLMs on pulley system problems using TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical advantage (MA). State-of-the-art models performed marginally but significantly above chance, and their estimates correlated significantly with ground-truth MA. Significant correlations between number of pulleys and model estimates suggest that models employed a pulley counting heuristic, without necessarily simulating pulley systems to derive precise values. Study 2 tested this by probing whether LLMs represent global features crucial to MA estimation. Models evaluated a functionally connected pulley system against a fake system with randomly placed components. Without explicit cues, models identified the functional system as having greater MA with F1=0.8, suggesting LLMs could represent systems well enough to differentiate jumbled from functional systems. Study 3 built on this by asking LLMs to compare functional systems with matched systems which were connected up but which transferred no force to the weight; LLMs identified the functional system with F1=0.46, suggesting random guessing. Insofar as they may generalize, these findings are compatible with the notion that LLMs manipulate internal world models, sufficient to exploit statistical associations between pulley count and MA (Study 1), and to approximately represent system components' spatial relations (Study 2). However, they may lack the facility to reason over nuanced structural connectivity (Study 3). We conclude by advocating the utility of cognitive scientific methods to evaluate the world-modeling capacities of artificial intelligence systems.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Safe Policy Improvement Using Parametric Structure</title>
<link>https://arxiv.org/abs/2507.15532</link>
<guid>https://arxiv.org/abs/2507.15532</guid>
<content:encoded><![CDATA[
<div> parametric SPI algorithm, transition dynamics, data-efficient, preprocessing technique, game-based abstraction

Summary:
A new offline reinforcement learning problem, Safe Policy Improvement (SPI), focuses on computing a new policy that outperforms the behavior policy with high confidence using only a dataset and the behavior policy. This is done by leveraging parametric dependencies between distributions in transition dynamics. The three contributions made in this study include: a parametric SPI algorithm that accurately estimates transition dynamics by exploiting known correlations, a preprocessing technique that prunes redundant actions through a game-based abstraction, and an advanced preprocessing technique using SMT solving to identify further actions to prune. Through empirical results and an ablation study, it is shown that these techniques significantly increase the data efficiency of SPI while maintaining the same reliability guarantees. <br /><br />Summary: <div>
arXiv:2507.15532v1 Announce Type: new 
Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in which a new policy that reliably outperforms the behavior policy with high confidence needs to be computed using only a dataset and the behavior policy. Markov decision processes (MDPs) are the standard formalism for modeling environments in SPI. In many applications, additional information in the form of parametric dependencies between distributions in the transition dynamics is available. We make SPI more data-efficient by leveraging these dependencies through three contributions: (1) a parametric SPI algorithm that exploits known correlations between distributions to more accurately estimate the transition dynamics using the same amount of data; (2) a preprocessing technique that prunes redundant actions from the environment through a game-based abstraction; and (3) a more advanced preprocessing technique, based on satisfiability modulo theory (SMT) solving, that can identify more actions to prune. Empirical results and an ablation study show that our techniques increase the data efficiency of SPI by multiple orders of magnitude while maintaining the same reliability guarantees.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metric assessment protocol in the context of answer fluctuation on MCQ tasks</title>
<link>https://arxiv.org/abs/2507.15581</link>
<guid>https://arxiv.org/abs/2507.15581</guid>
<content:encoded><![CDATA[
<div> assessment, multiple-choice questions, evaluation metrics, answer fluctuation, worst accuracy<br />
Summary:<br />
Using multiple-choice questions (MCQs) for assessing LLM capabilities is common, but previous research lacks a thorough evaluation of evaluation metrics. Answer fluctuation in MCQ evaluation leads to different results with slight prompt changes. A protocol for assessing metrics based on their correlation with fluctuation rates and original performance is proposed. Results show a strong connection between existing metrics and answer changing. The novel metric "worst accuracy" shows the highest association in the evaluation protocol. This suggests that the choice of metric impacts the evaluation outcomes, even without additional prompt variants.Overall, this study highlights the importance of selecting appropriate evaluation metrics for MCQ assessment in LLM capabilities to ensure accurate and consistent results.<br />Summary: <div>
arXiv:2507.15581v1 Announce Type: new 
Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing LLM capabilities efficiently. A variety of metrics can be employed for this task. However, previous research has not conducted a thorough assessment of them. At the same time, MCQ evaluation suffers from answer fluctuation: models produce different results given slight changes in prompts. We suggest a metric assessment protocol in which evaluation methodologies are analyzed through their connection with fluctuation rates, as well as original performance. Our results show that there is a strong link between existing metrics and the answer changing, even when computed without any additional prompt variants. A novel metric, worst accuracy, demonstrates the highest association on the protocol.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II</title>
<link>https://arxiv.org/abs/2507.15618</link>
<guid>https://arxiv.org/abs/2507.15618</guid>
<content:encoded><![CDATA[
<div> adapter-based approach, tactical conditioning, StarCraft II AI agents, high-level tactical directives, policy network<br />
<br />
Summary: 
This article introduces an adapter-based approach for enhancing tactical conditioning in StarCraft II AI agents. The method involves freezing a pre-trained policy network (DI-Star) and attaching lightweight adapter modules to each action head, which are conditioned on a tactical tensor representing strategic preferences. These adapters are trained with KL divergence constraints to ensure that the agent maintains its core competencies while displaying variations in tactics. The experimental results demonstrate that this approach successfully alters agent behavior in terms of aggression, expansion patterns, and technology preferences while maintaining competitive performance. This method not only allows for flexible tactical control but also incurs minimal computational overhead, providing a practical means of customizing strategies for complex real-time strategy games.<br /> <div>
arXiv:2507.15618v1 Announce Type: new 
Abstract: We present an adapter-based approach for tactical conditioning of StarCraft II AI agents. Current agents, while powerful, lack the ability to adapt their strategies based on high-level tactical directives. Our method freezes a pre-trained policy network (DI-Star) and attaches lightweight adapter modules to each action head, conditioned on a tactical tensor that encodes strategic preferences. By training these adapters with KL divergence constraints, we ensure the policy maintains core competencies while exhibiting tactical variations. Experimental results show our approach successfully modulates agent behavior across tactical dimensions including aggression, expansion patterns, and technology preferences, while maintaining competitive performance. Our method enables flexible tactical control with minimal computational overhead, offering practical strategy customization for complex real-time strategy games.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI for autonomous anomaly management in complex systems</title>
<link>https://arxiv.org/abs/2507.15676</link>
<guid>https://arxiv.org/abs/2507.15676</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic AI, autonomously detecting, anomalies, complex systems, anomaly management <br />
<br />
Summary: 
This paper discusses the potential of agentic AI in autonomously identifying and addressing anomalies in intricate systems. It highlights the capability of agentic AI to revolutionize the conventional methods of anomaly management, reducing reliance on human intervention. By leveraging advanced AI algorithms and machine learning techniques, agentic AI can effectively detect anomalies within complex systems. This autonomous approach offers a more efficient and proactive means of anomaly detection, eliminating the need for manual monitoring and intervention. Through the use of agentic AI, organizations can enhance their anomaly management processes, enabling quicker response times and improved system performance. Overall, the integration of agentic AI in anomaly detection brings forth a new era of automation and efficiency in managing anomalies within complex systems. <br /><br /> <div>
arXiv:2507.15676v1 Announce Type: new 
Abstract: This paper explores the potential of agentic AI in autonomously detecting and responding to anomalies within complex systems, emphasizing its ability to transform traditional, human-dependent anomaly management methods.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards physician-centered oversight of conversational diagnostic AI</title>
<link>https://arxiv.org/abs/2507.15743</link>
<guid>https://arxiv.org/abs/2507.15743</guid>
<content:encoded><![CDATA[
<div> Keywords: conversational AI, diagnostic dialogue, oversight, asynchronous, clinical decision

Summary: 
In this article, the authors propose a new framework for overseeing conversational AI systems in diagnostic dialogue. They introduce guardrailed-AMIE (g-AMIE), a multi-agent system that performs history taking within specified limits and conveys assessments to a primary care physician for oversight. A virtual clinical examination compared g-AMIE to nurse practitioners/physician assistants and group of primary care physicians, showing g-AMIE outperformed both groups in intake quality, case summarization, and diagnostic and management plan proposals. The study also found that PCP oversight of g-AMIE was more time-efficient than standalone PCP consultations. While the study may not fully replicate clinical practices, the results suggest that asynchronous oversight can enhance the performance of diagnostic AI systems under expert human supervision in real-world care.<br /><br />Summary: <div>
arXiv:2507.15743v1 Announce Type: new 
Abstract: Recent work has demonstrated the promise of conversational AI systems for diagnostic dialogue. However, real-world assurance of patient safety means that providing individual diagnoses and treatment plans is considered a regulated activity by licensed professionals. Furthermore, physicians commonly oversee other team members in such activities, including nurse practitioners (NPs) or physician assistants/associates (PAs). Inspired by this, we propose a framework for effective, asynchronous oversight of the Articulate Medical Intelligence Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent system that performs history taking within guardrails, abstaining from individualized medical advice. Afterwards, g-AMIE conveys assessments to an overseeing primary care physician (PCP) in a clinician cockpit interface. The PCP provides oversight and retains accountability of the clinical decision. This effectively decouples oversight from intake and can thus happen asynchronously. In a randomized, blinded virtual Objective Structured Clinical Examination (OSCE) of text consultations with asynchronous oversight, we compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across 60 scenarios, g-AMIE outperformed both groups in performing high-quality intake, summarizing cases, and proposing diagnoses and management plans for the overseeing PCP to review. This resulted in higher quality composite decisions. PCP oversight of g-AMIE was also more time-efficient than standalone PCP consultations in prior work. While our study does not replicate existing clinical practices and likely underestimates clinicians' capabilities, our results demonstrate the promise of asynchronous oversight as a feasible paradigm for diagnostic AI systems to operate under expert human oversight for enhancing real-world care.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization</title>
<link>https://arxiv.org/abs/2507.15758</link>
<guid>https://arxiv.org/abs/2507.15758</guid>
<content:encoded><![CDATA[
arXiv:2507.15758v1 Announce Type: new 
Abstract: Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\% while improving accuracy by 2.3\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts</title>
<link>https://arxiv.org/abs/2507.15761</link>
<guid>https://arxiv.org/abs/2507.15761</guid>
<content:encoded><![CDATA[
arXiv:2507.15761v1 Announce Type: new 
Abstract: Smart contracts are trustworthy, immutable, and automatically executed programs on the blockchain. Their execution requires the Gas mechanism to ensure efficiency and fairness. However, due to non-optimal coding practices, many contracts contain Gas waste patterns that need to be optimized. Existing solutions mostly rely on manual discovery, which is inefficient, costly to maintain, and difficult to scale. Recent research uses large language models (LLMs) to explore new Gas waste patterns. However, it struggles to remain compatible with existing patterns, often produces redundant patterns, and requires manual validation/rewriting. To address this gap, we present GasAgent, the first multi-agent system for smart contract Gas optimization that combines compatibility with existing patterns and automated discovery/validation of new patterns, enabling end-to-end optimization. GasAgent consists of four specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate in a closed loop to identify, validate, and apply Gas-saving improvements. Experiments on 100 verified real-world contracts demonstrate that GasAgent successfully optimizes 82 contracts, achieving an average deployment Gas savings of 9.97%. In addition, our evaluation confirms its compatibility with existing tools and validates the effectiveness of each module through ablation studies. To assess broader usability, we further evaluate 500 contracts generated by five representative LLMs across 10 categories and find that GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from 4.79% to 13.93%, showing its usability as the optimization layer for LLM-assisted smart contract development.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining</title>
<link>https://arxiv.org/abs/2507.15770</link>
<guid>https://arxiv.org/abs/2507.15770</guid>
<content:encoded><![CDATA[
arXiv:2507.15770v1 Announce Type: new 
Abstract: With the rise of service computing, cloud computing, and IoT, service ecosystems are becoming increasingly complex. The intricate interactions among intelligent agents make abnormal emergence analysis challenging, as traditional causal methods focus on individual trajectories. Large language models offer new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT) reasoning to reveal agent intentions. However, existing approaches remain limited to microscopic and static analysis. This paper introduces a framework: Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic and interpretable emergence analysis. EAMI first employs a dual-perspective thought track mechanism, where an Inspector Agent and an Analysis Agent extract agent intentions under bounded and perfect rationality. Then, k-means clustering identifies phase transition points in group intentions, followed by a Intention Temporal Emergence diagram for dynamic analysis. The experiments validate EAMI in complex online-to-offline (O2O) service system and the Stanford AI Town experiment, with ablation studies confirming its effectiveness, generalizability, and efficiency. This framework provides a novel paradigm for abnormal emergence and causal analysis in service ecosystems. The code is available at https://anonymous.4open.science/r/EAMI-B085.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work</title>
<link>https://arxiv.org/abs/2507.15796</link>
<guid>https://arxiv.org/abs/2507.15796</guid>
<content:encoded><![CDATA[
arXiv:2507.15796v1 Announce Type: new 
Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI) has emerged as a critical objective in the deployment of AI systems across sensitive and high-risk domains. TAI frameworks articulate a comprehensive set of ethical, legal, and technical requirements to ensure that AI technologies are aligned with human values, rights, and societal expectations. Among the various AI paradigms, Federated Learning (FL) presents a promising solution to pressing privacy concerns. However, aligning FL with the rest of the requirements of TAI presents a series of challenges, most of which arise from its inherently distributed nature. In this work, we adopt the requirements TAI as a guiding structure to systematically analyze the challenges of adapting FL to TAI. Specifically, we classify and examine the key obstacles to aligning FL with TAI, providing a detailed exploration of what has been done, the trends, and the remaining work within each of the identified challenges.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Conditional Causal Effects in MPDAGs</title>
<link>https://arxiv.org/abs/2507.15842</link>
<guid>https://arxiv.org/abs/2507.15842</guid>
<content:encoded><![CDATA[
arXiv:2507.15842v1 Announce Type: new 
Abstract: We consider identifying a conditional causal effect when a graph is known up to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG represents an equivalence class of graphs that is restricted by background knowledge and where all variables in the causal model are observed. We provide three results that address identification in this setting: an identification formula when the conditioning set is unaffected by treatment, a generalization of the well-known do calculus to the MPDAG setting, and an algorithm that is complete for identifying these conditional effects.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Budget Policy Optimization for Adaptive Reasoning</title>
<link>https://arxiv.org/abs/2507.15844</link>
<guid>https://arxiv.org/abs/2507.15844</guid>
<content:encoded><![CDATA[
arXiv:2507.15844v1 Announce Type: new 
Abstract: Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. HBPO addresses the fundamental challenge of exploration space collapse in efficiency-oriented training, where penalties on long output length systematically bias models away from necessary long reasoning paths. Through hierarchical budget exploration, our approach partitions rollout samples into multiple subgroups with distinct token budgets, aiming to enable efficient resource allocation while preventing degradation of capability. We introduce differentiated reward mechanisms that create budget-aware incentives aligned with the complexity of the problem, allowing models to discover natural correspondences between task requirements and computational effort. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Unlike existing methods that impose external constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Other Mind: How Language Models Exhibit Human Temporal Cognition</title>
<link>https://arxiv.org/abs/2507.15851</link>
<guid>https://arxiv.org/abs/2507.15851</guid>
<content:encoded><![CDATA[
arXiv:2507.15851v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain cognitive patterns similar to those of humans that are not directly specified in training data. This study investigates this phenomenon by focusing on temporal cognition in LLMs. Leveraging the similarity judgment task, we find that larger models spontaneously establish a subjective temporal reference point and adhere to the Weber-Fechner law, whereby the perceived distance logarithmically compresses as years recede from this reference point. To uncover the mechanisms behind this behavior, we conducted multiple analyses across neuronal, representational, and informational levels. We first identify a set of temporal-preferential neurons and find that this group exhibits minimal activation at the subjective reference point and implements a logarithmic coding scheme convergently found in biological systems. Probing representations of years reveals a hierarchical construction process, where years evolve from basic numerical values in shallow layers to abstract temporal orientation in deep layers. Finally, using pre-trained embedding models, we found that the training corpus itself possesses an inherent, non-linear temporal structure, which provides the raw material for the model's internal construction. In discussion, we propose an experientialist perspective for understanding these findings, where the LLMs' cognition is viewed as a subjective construction of the external world by its internal representational system. This nuanced perspective implies the potential emergence of alien cognitive frameworks that humans cannot intuitively predict, pointing toward a direction for AI alignment that focuses on guiding internal constructions. Our code is available at https://TheOtherMind.github.io.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gemini 2.5 Pro Capable of Winning Gold at IMO 2025</title>
<link>https://arxiv.org/abs/2507.15855</link>
<guid>https://arxiv.org/abs/2507.15855</guid>
<content:encoded><![CDATA[
arXiv:2507.15855v1 Announce Type: new 
Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging problems requiring deep insight, creativity, and formal reasoning. While Large Language Models (LLMs) perform well on mathematical benchmarks like AIME, they struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly released IMO 2025 problems, avoiding data contamination. With pipeline design and prompt engineering, 5 (out of 6) problems are solved correctly (up to a caveat discussed below), highlighting the importance of finding the optimal way of using powerful models.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs</title>
<link>https://arxiv.org/abs/2411.01789</link>
<guid>https://arxiv.org/abs/2411.01789</guid>
<content:encoded><![CDATA[
arXiv:2411.01789v2 Announce Type: cross 
Abstract: Software testing remains the most widely used methodology for validating quality of code. However, effectiveness of testing critically depends on the quality of test suites used. Test cases in a test suite consist of two fundamental parts: (1) input values for the code under test, and (2) correct checks for the outputs it produces. These checks are commonly written as assertions, and termed test oracles. The last couple of decades have seen much progress in automated test input generation, e.g., using fuzzing and symbolic execution. However, automating test oracles remains a relatively less explored problem area. Indeed, a test oracle by its nature requires knowledge of expected behavior, which may only be known to the developer and may not not exist in a formal language that supports automated reasoning.
  Our focus in this paper is automation of test oracles for clients of widely used Java libraries, e.g., java.lang and java.util packages. Our key insight is that Javadocs that provide a rich source of information can enable automated generation of test oracles. Javadocs of the core Java libraries are fairly detailed documents that contain natural language descriptions of not only how the libraries behave but also how the clients must (not) use them. We use large language models as an enabling technology to embody our insight into a framework for test oracle automation, and evaluate it experimentally. Our experiments demonstrate that LLMs can generate oracles for checking normal and exceptional behaviors from Javadocs, with 98.8% of these oracles being compilable and 96.4% accurately reflecting intended properties. Even for the few incorrect oracles, errors are minor and can be easily corrected with the help of additional comment information generated by the LLMs.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Effectiveness of Large Language Models in Writing Alloy Formulas</title>
<link>https://arxiv.org/abs/2502.15441</link>
<guid>https://arxiv.org/abs/2502.15441</guid>
<content:encoded><![CDATA[
arXiv:2502.15441v1 Announce Type: cross 
Abstract: Declarative specifications have a vital role to play in developing safe and dependable software systems. Writing specifications correctly, however, remains particularly challenging. This paper presents a controlled experiment on using large language models (LLMs) to write declarative formulas in the well-known language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write complete Alloy formulas from given natural language descriptions (in English). Two, we employ LLMs to create alternative but equivalent formulas in Alloy with respect to given Alloy formulas. Three, we employ LLMs to complete sketches of Alloy formulas and populate the holes in the sketches by synthesizing Alloy expressions and operators so that the completed formulas accurately represent the desired properties (that are given in natural language). We conduct the experimental evaluation using 11 well-studied subject specifications and employ two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show that the LLMs generally perform well in synthesizing complete Alloy formulas from input properties given in natural language or in Alloy, and are able to enumerate multiple unique solutions. Moreover, the LLMs are also successful at completing given sketches of Alloy formulas with respect to natural language descriptions of desired properties (without requiring test cases). We believe LLMs offer a very exciting advance in our ability to write specifications, and can help make specifications take a pivotal role in software development and enhance our ability to build robust software.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks</title>
<link>https://arxiv.org/abs/2505.17593</link>
<guid>https://arxiv.org/abs/2505.17593</guid>
<content:encoded><![CDATA[
arXiv:2505.17593v1 Announce Type: cross 
Abstract: Generative AI offers potential for educational support, but often lacks pedagogical grounding and awareness of the student's learning context. Furthermore, researching student interactions with these tools within authentic learning environments remains challenging. To address this, we present JELAI, an open-source platform architecture designed to integrate fine-grained Learning Analytics (LA) with Large Language Model (LLM)-based tutoring directly within a Jupyter Notebook environment. JELAI employs a modular, containerized design featuring JupyterLab extensions for telemetry and chat, alongside a central middleware handling LA processing and context-aware LLM prompt enrichment. This architecture enables the capture of integrated code interaction and chat data, facilitating real-time, context-sensitive AI scaffolding and research into student behaviour. We describe the system's design, implementation, and demonstrate its feasibility through system performance benchmarks and two proof-of-concept use cases illustrating its capabilities for logging multi-modal data, analysing help-seeking patterns, and supporting A/B testing of AI configurations. JELAI's primary contribution is its technical framework, providing a flexible tool for researchers and educators to develop, deploy, and study LA-informed AI tutoring within the widely used Jupyter ecosystem.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification</title>
<link>https://arxiv.org/abs/2506.23298</link>
<guid>https://arxiv.org/abs/2506.23298</guid>
<content:encoded><![CDATA[
arXiv:2506.23298v3 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have enormous potential to perform few-shot in-context learning in the context of medical image analysis. However, safe deployment of these models into real-world clinical practice requires an in-depth analysis of the accuracies of their predictions, and their associated calibration errors, particularly across different demographic subgroups. In this work, we present the first investigation into the calibration biases and demographic unfairness of MLLMs' predictions and confidence scores in few-shot in-context learning for medical image classification. We introduce CALIN, an inference-time calibration method designed to mitigate the associated biases. Specifically, CALIN estimates the amount of calibration needed, represented by calibration matrices, using a bi-level procedure: progressing from the population level to the subgroup level prior to inference. It then applies this estimation to calibrate the predicted confidence scores during inference. Experimental results on three medical imaging datasets: PAPILA for fundus image classification, HAM10000 for skin cancer classification, and MIMIC-CXR for chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair confidence calibration in its prediction, while improving its overall prediction accuracies and exhibiting minimum fairness-utility trade-off. Our codebase can be found at https://github.com/xingbpshen/medical-calibration-fairness-mllm.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geophysics-informed neural network for model-based seismic inversion using surrogate point spread functions</title>
<link>https://arxiv.org/abs/2507.14140</link>
<guid>https://arxiv.org/abs/2507.14140</guid>
<content:encoded><![CDATA[
arXiv:2507.14140v1 Announce Type: cross 
Abstract: Model-based seismic inversion is a key technique in reservoir characterization, but traditional methods face significant limitations, such as relying on 1D average stationary wavelets and assuming an unrealistic lateral resolution. To address these challenges, we propose a Geophysics-Informed Neural Network (GINN) that integrates deep learning with seismic modeling. This novel approach employs a Deep Convolutional Neural Network (DCNN) to simultaneously estimate Point Spread Functions (PSFs) and acoustic impedance (IP). PSFs are divided into zero-phase and residual components to ensure geophysical consistency and to capture fine details. We used synthetic data from the SEAM Phase I Earth Model to train the GINN for 100 epochs (approximately 20 minutes) using a 2D UNet architecture. The network's inputs include positional features and a low-frequency impedance (LF-IP) model. A self-supervised loss function combining Mean Squared Error (MSE) and Structural Similarity Index Measure (SSIM) was employed to ensure accurate results. The GINN demonstrated its ability to generate high-resolution IP and realistic PSFs, aligning with expected geological features. Unlike traditional 1D wavelets, the GINN produces PSFs with limited lateral resolution, reducing noise and improving accuracy. Future work will aim to refine the training process and validate the methodology with real seismic data.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIVER-0 : A Fully Channel Equivariant EEG Foundation Model</title>
<link>https://arxiv.org/abs/2507.14141</link>
<guid>https://arxiv.org/abs/2507.14141</guid>
<content:encoded><![CDATA[
arXiv:2507.14141v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) is a non-invasive technique widely used in brain-computer interfaces and clinical applications, yet existing EEG foundation models face limitations in modeling spatio-temporal brain dynamics and lack channel permutation equivariance, preventing robust generalization across diverse electrode configurations. To address these challenges, we propose DIVER-0, a novel EEG foundation model that demonstrates how full spatio-temporal attention-rather than segregated spatial or temporal processing-achieves superior performance when properly designed with Rotary Position Embedding (RoPE) for temporal relationships and binary attention biases for channel differentiation. We also introduce Sliding Temporal Conditional Positional Encoding (STCPE), which improves upon existing conditional positional encoding approaches by maintaining both temporal translation equivariance and channel permutation equivariance, enabling robust adaptation to arbitrary electrode configurations unseen during pretraining. Experimental results demonstrate that DIVER-0 achieves competitive performance with only 10% of pretraining data while maintaining consistent results across all channel permutation conditions, validating its effectiveness for cross-dataset generalization and establishing key design principles for handling the inherent heterogeneity of neural recording setups.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-DANA: A Resource-Efficient Channel-Adaptive Self-Supervised Approach for ECG Foundation Models</title>
<link>https://arxiv.org/abs/2507.14151</link>
<guid>https://arxiv.org/abs/2507.14151</guid>
<content:encoded><![CDATA[
arXiv:2507.14151v1 Announce Type: cross 
Abstract: Foundation Models (FMs) are large-scale machine learning models trained on extensive, diverse datasets that can be adapted to a wide range of downstream tasks with minimal fine-tuning. In the last two years, interest in FMs has also grown for applications in the cardiological field to analyze the electrocardiogram (ECG) signals. One of the key properties of FMs is their transferability to a wide range of downstream scenarios. With the spread of wearable and portable devices, keen interest in learning from reduced-channel configurations has arisen. However, the adaptation of ECG FMs to downstream scenarios with fewer available channels still has to be properly investigated. In this work, we propose Self-DANA, a novel, easy-to-integrate solution that makes self-supervised architectures adaptable to a reduced number of input channels, ensuring resource efficiency and high performance. We also introduce Random Lead Selection, a novel augmentation technique to pre-train models in a more robust and channel-agnostic way. Our experimental results on five reduced-channel configurations demonstrate that Self-DANA significantly enhances resource efficiency while reaching state-of-the-art performance. It requires up to 69.3% less peak CPU memory, 34.4% less peak GPU memory, about 17% less average epoch CPU time, and about 24% less average epoch GPU time.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surface EMG Profiling in Parkinson's Disease: Advancing Severity Assessment with GCN-SVM</title>
<link>https://arxiv.org/abs/2507.14153</link>
<guid>https://arxiv.org/abs/2507.14153</guid>
<content:encoded><![CDATA[
arXiv:2507.14153v1 Announce Type: cross 
Abstract: Parkinson's disease (PD) poses challenges in diagnosis and monitoring due to its progressive nature and complex symptoms. This study introduces a novel approach utilizing surface electromyography (sEMG) to objectively assess PD severity, focusing on the biceps brachii muscle. Initial analysis of sEMG data from five PD patients and five healthy controls revealed significant neuromuscular differences. A traditional Support Vector Machine (SVM) model achieved up to 83% accuracy, while enhancements with a Graph Convolutional Network-Support Vector Machine (GCN-SVM) model increased accuracy to 92%. Despite the preliminary nature of these results, the study outlines a detailed experimental methodology for future research with larger cohorts to validate these findings and integrate the approach into clinical practice. The proposed approach holds promise for advancing PD severity assessment and improving patient care in Parkinson's disease management.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All-atom inverse protein folding through discrete flow matching</title>
<link>https://arxiv.org/abs/2507.14156</link>
<guid>https://arxiv.org/abs/2507.14156</guid>
<content:encoded><![CDATA[
arXiv:2507.14156v1 Announce Type: cross 
Abstract: The recent breakthrough of AlphaFold3 in modeling complex biomolecular interactions, including those between proteins and ligands, nucleotides, or metal ions, creates new opportunities for protein design. In so-called inverse protein folding, the objective is to find a sequence of amino acids that adopts a target protein structure. Many inverse folding methods struggle to predict sequences for complexes that contain non-protein components, and perform poorly with complexes that adopt multiple structural states. To address these challenges, we present ADFLIP (All-atom Discrete FLow matching Inverse Protein folding), a generative model based on discrete flow-matching for designing protein sequences conditioned on all-atom structural contexts. ADFLIP progressively incorporates predicted amino acid side chains as structural context during sequence generation and enables the design of dynamic protein complexes through ensemble sampling across multiple structural states. Furthermore, ADFLIP implements training-free classifier guidance sampling, which allows the incorporation of arbitrary pre-trained models to optimise the designed sequence for desired protein properties. We evaluated the performance of ADFLIP on protein complexes with small-molecule ligands, nucleotides, or metal ions, including dynamic complexes for which structure ensembles were determined by nuclear magnetic resonance (NMR). Our model achieves state-of-the-art performance in single-structure and multi-structure inverse folding tasks, demonstrating excellent potential for all-atom protein design. The code is available at https://github.com/ykiiiiii/ADFLIP.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Denoising VAE for Intracardiac Time Series in Ischemic Cardiomyopathy</title>
<link>https://arxiv.org/abs/2507.14164</link>
<guid>https://arxiv.org/abs/2507.14164</guid>
<content:encoded><![CDATA[
arXiv:2507.14164v1 Announce Type: cross 
Abstract: In the field of cardiac electrophysiology (EP), effectively reducing noise in intra-cardiac signals is crucial for the accurate diagnosis and treatment of arrhythmias and cardiomyopathies. However, traditional noise reduction techniques fall short in addressing the diverse noise patterns from various sources, often non-linear and non-stationary, present in these signals. This work introduces a Variational Autoencoder (VAE) model, aimed at improving the quality of intra-ventricular monophasic action potential (MAP) signal recordings. By constructing representations of clean signals from a dataset of 5706 time series from 42 patients diagnosed with ischemic cardiomyopathy, our approach demonstrates superior denoising performance when compared to conventional filtering methods commonly employed in clinical settings. We assess the effectiveness of our VAE model using various metrics, indicating its superior capability to denoise signals across different noise types, including time-varying non-linear noise frequently found in clinical settings. These results reveal that VAEs can eliminate diverse sources of noise in single beats, outperforming state-of-the-art denoising techniques and potentially improving treatment efficacy in cardiac EP.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space</title>
<link>https://arxiv.org/abs/2507.14170</link>
<guid>https://arxiv.org/abs/2507.14170</guid>
<content:encoded><![CDATA[
arXiv:2507.14170v1 Announce Type: cross 
Abstract: Structured pruning aims to reduce the size and computational cost of deep neural networks by removing entire filters or channels. The traditional regularizers such as L1 or Group Lasso and its variants lead to magnitude-biased pruning decisions, such that the filters with small magnitudes are likely to be pruned. Also, they often entail pruning results with almost zero margin around pruning decision boundary, such that tiny perturbation in a filter magnitude can flip the pruning decision. In this paper, we identify the precise algebraic condition under which pruning operations preserve model performance, and use the condition to construct a novel regularizer defined in an extended parameter space via auxiliary catalyst variables. The proposed Catalyst regularization ensures fair pruning chance for each filters with theoretically provable zero bias to their magnitude and robust pruning behavior achieved by wide-margin bifurcation of magnitudes between the preserved and the pruned filters. The theoretical properties naturally lead to real-world effectiveness, as shown by empirical validations of Catalyst Pruning algorithm. Pruning results on various datasets and models are superior to state-of-the-art filter pruning methods, and at the same time confirm the predicted robust and fair pruning characteristics of Catalyst pruning.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning</title>
<link>https://arxiv.org/abs/2507.14171</link>
<guid>https://arxiv.org/abs/2507.14171</guid>
<content:encoded><![CDATA[
arXiv:2507.14171v1 Announce Type: cross 
Abstract: With the growth of demand on neural network compression methods, the structured pruning methods including importance-based approach are actively studied. The magnitude importance and many correlated modern importance criteria often limit the capacity of pruning decision, since the filters with larger magnitudes are not likely to be pruned if the smaller one didn't, even if it is redundant. In this paper, we propose a novel pruning strategy to challenge this dominating effect of magnitude and provide fair chance to each filter to be pruned, by placing it on projective space. After that, we observe the gradient descent movement whether the filters move toward the origin or not, to measure how the filter is likely to be pruned. This measurement is used to construct PROscore, a novel importance score for IPPRO, a novel importance-based structured pruning with magnitude-indifference. Our evaluation results shows that the proposed importance criteria using the projective space achieves near-lossless pruning by reducing the performance drop in pruning, with promising performance after the finetuning. Our work debunks the ``size-matters'' myth in pruning and expands the frontier of importance-based pruning both theoretically and empirically.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI</title>
<link>https://arxiv.org/abs/2507.14172</link>
<guid>https://arxiv.org/abs/2507.14172</guid>
<content:encoded><![CDATA[
arXiv:2507.14172v1 Announce Type: cross 
Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art language models to solve in single attempts. Search-based evolutionary methods offer a promising alternative by exploring solution spaces iteratively, but their effectiveness remain limited by the fixed capabilities of the underlying generative model.
  We propose SOAR, a method that learns program synthesis by integrating language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample and refine candidate solutions, and (2) a hindsight learning phase that converts search attempts into valid problem-solution pairs used to fine-tune the LLM's sampling and refinement capabilities\, -- \,enabling increasingly effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance gains across model scales and iterations, leveraging positive transfer between the sampling and refinement finetuning tasks. These improvements carry over to test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our code is open-sourced at: https://github.com/flowersteam/SOAR
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data</title>
<link>https://arxiv.org/abs/2507.14175</link>
<guid>https://arxiv.org/abs/2507.14175</guid>
<content:encoded><![CDATA[
arXiv:2507.14175v1 Announce Type: cross 
Abstract: Background: Mental illnesses such as depression and anxiety require improved methods for early detection and personalized intervention. Traditional predictive models often rely on unimodal data or early fusion strategies that fail to capture the complex, multimodal nature of psychiatric data. Advanced integration techniques, such as intermediate (latent space) fusion, may offer better accuracy and clinical utility. Methods: Using data from the BRIGHTEN clinical trial, we evaluated intermediate (latent space) fusion for predicting daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented with a Random Forest (RF) model and intermediate fusion implemented via a Combined Model (CM) using autoencoders and a neural network. The dataset included behavioral (smartphone-based), demographic, and clinical features. Experiments were conducted across multiple temporal splits and data stream combinations. Performance was evaluated using mean squared error (MSE) and coefficient of determination (R2). Results: The CM outperformed both RF and Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985 vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed signs of overfitting, with a large gap between training and test performance, while the CM maintained consistent generalization. Performance was best when integrating all data modalities in the CM (in contradistinction to RF), underscoring the value of latent space fusion for capturing non-linear interactions in complex psychiatric datasets. Conclusion: Latent space fusion offers a robust alternative to traditional fusion methods for prediction with multimodal mental health data. Future work should explore model interpretability and individual-level prediction for clinical deployment.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Two-Layer Neural Networks with Smooth Activation Functions</title>
<link>https://arxiv.org/abs/2507.14177</link>
<guid>https://arxiv.org/abs/2507.14177</guid>
<content:encoded><![CDATA[
arXiv:2507.14177v1 Announce Type: cross 
Abstract: This paper aims to understand the training solution, which is obtained by the back-propagation algorithm, of two-layer neural networks whose hidden layer is composed of the units with smooth activation functions, including the usual sigmoid type most commonly used before the advent of ReLUs. The mechanism contains four main principles: construction of Taylor series expansions, strict partial order of knots, smooth-spline implementation and smooth-continuity restriction. The universal approximation for arbitrary input dimensionality is proved and experimental verification is given, through which the mystery of ``black box'' of the solution space is largely revealed. The new proofs employed also enrich approximation theory.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Bank Enhancement for Distance-based Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2507.14178</link>
<guid>https://arxiv.org/abs/2507.14178</guid>
<content:encoded><![CDATA[
arXiv:2507.14178v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability of deep learning applications and has attracted significant attention in recent years. A rich body of literature has emerged to develop efficient score functions that assign high scores to in-distribution (ID) samples and low scores to OOD samples, thereby helping distinguish OOD samples. Among these methods, distance-based score functions are widely used because of their efficiency and ease of use. However, deep learning often leads to a biased distribution of data features, and extreme features are inevitable. These extreme features make the distance-based methods tend to assign too low scores to ID samples. This limits the OOD detection capabilities of such methods. To address this issue, we propose a simple yet effective method, Feature Bank Enhancement (FBE), that uses statistical characteristics from dataset to identify and constrain extreme features to the separation boundaries, therapy making the distance between samples inside and outside the distribution farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10 respectively, and the results show that our method achieves state-of-the-art performance on both benchmark. Additionally, theoretical analysis and supplementary experiments are conducted to provide more insights into our method.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering</title>
<link>https://arxiv.org/abs/2507.14179</link>
<guid>https://arxiv.org/abs/2507.14179</guid>
<content:encoded><![CDATA[
arXiv:2507.14179v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where only a subset of neurons are active for a given input. Although this sparsity presents opportunities to reduce computational cost, efficiently utilizing it requires predicting activation patterns in a scalable manner. However, direct prediction at the neuron level is computationally expensive due to the vast number of neurons in modern LLMs. To enable efficient prediction and utilization of activation sparsity, we propose a clustering-based activation pattern compression framework. Instead of treating each neuron independently, we group similar activation patterns into a small set of representative clusters. Our method achieves up to 79.34% clustering precision, outperforming standard binary clustering approaches while maintaining minimal degradation in perplexity (PPL) scores. With a sufficiently large number of clusters, our approach attains a PPL score as low as 12.49, demonstrating its effectiveness in preserving model quality while reducing computational overhead. By predicting cluster assignments rather than individual neuron states, future models can efficiently infer activation patterns from pre-computed centroids. We detail the clustering algorithm, analyze its effectiveness in capturing meaningful activation structures, and demonstrate its potential to improve sparse computation efficiency. This clustering-based formulation serves as a foundation for future work on activation pattern prediction, paving the way for efficient inference in large-scale language models.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems</title>
<link>https://arxiv.org/abs/2507.14180</link>
<guid>https://arxiv.org/abs/2507.14180</guid>
<content:encoded><![CDATA[
arXiv:2507.14180v1 Announce Type: cross 
Abstract: In line with the AI-native 6G vision, explainability and robustness are crucial for building trust and ensuring reliable performance in millimeter-wave (mmWave) systems. Efficient beam alignment is essential for initial access, but deep learning (DL) solutions face challenges, including high data collection overhead, hardware constraints, lack of explainability, and susceptibility to adversarial attacks. This paper proposes a robust and explainable DL-based beam alignment engine (BAE) for mmWave multiple-input multiple output (MIMO) systems. The BAE uses received signal strength indicator (RSSI) measurements from wide beams to predict the best narrow beam, reducing the overhead of exhaustive beam sweeping. To overcome the challenge of real-world data collection, this work leverages a site-specific digital twin (DT) to generate synthetic channel data closely resembling real-world environments. A model refinement via transfer learning is proposed to fine-tune the pre-trained model residing in the DT with minimal real-world data, effectively bridging mismatches between the digital replica and real-world environments. To reduce beam training overhead and enhance transparency, the framework uses deep Shapley additive explanations (SHAP) to rank input features by importance, prioritizing key spatial directions and minimizing beam sweeping. It also incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a credibility metric for detecting out-of-distribution inputs and ensuring robust, transparent decision-making. Experimental results show that the proposed framework reduces real-world data needs by 70%, beam training overhead by 62%, and improves outlier detection robustness by up to 8.5x, achieving near-optimal spectral efficiency and transparent decision making compared to traditional softmax based DL models.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis</title>
<link>https://arxiv.org/abs/2507.14181</link>
<guid>https://arxiv.org/abs/2507.14181</guid>
<content:encoded><![CDATA[
arXiv:2507.14181v1 Announce Type: cross 
Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe operation of industrial machinery and improving production efficiency. However, traditional supervised deep learning methods require a large amount of training data and labels, which are often located in different clients. Additionally, the cost of data labeling is high, making labels difficult to acquire. Meanwhile, differences in data distribution among clients may also hinder the model's performance. To tackle these challenges, this paper proposes a semi-supervised federated learning framework, SSFL-DCSL, which integrates dual contrastive loss and soft labeling to address data and label scarcity for distributed clients with few labeled samples while safeguarding user privacy. It enables representation learning using unlabeled data on the client side and facilitates joint learning among clients through prototypes, thereby achieving mutual knowledge sharing and preventing local model divergence. Specifically, first, a sample weighting function based on the Laplace distribution is designed to alleviate bias caused by low confidence in pseudo labels during the semi-supervised training process. Second, a dual contrastive loss is introduced to mitigate model divergence caused by different data distributions, comprising local contrastive loss and global contrastive loss. Third, local prototypes are aggregated on the server with weighted averaging and updated with momentum to share knowledge among clients. To evaluate the proposed SSFL-DCSL framework, experiments are conducted on two publicly available datasets and a dataset collected on motors from the factory. In the most challenging task, where only 10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by 1.15% to 7.85% over state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling</title>
<link>https://arxiv.org/abs/2507.14182</link>
<guid>https://arxiv.org/abs/2507.14182</guid>
<content:encoded><![CDATA[
arXiv:2507.14182v1 Announce Type: cross 
Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both historical price trajectories and exogenous narratives, such as news, policy interpretations, and social media sentiment. The heterogeneity in these data and the diverse insight of investors introduce biases that complicate the modeling of market dynamics. Unlike prior work, this paper explores the potential of bull and bear regimes in investor-driven market dynamics. Through empirical analysis on real-world financial datasets, we uncover a dynamic relationship between bias variation and behavioral adaptation, which enhances trend prediction under evolving market conditions. To model this mechanism, we propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified framework that jointly embeds temporal price sequences and external contextual signals into a shared latent space where opposing bull and bear forces naturally emerge, forming the foundation for bias representation. Within this space, an inertial pairing module pairs temporally adjacent samples to preserve momentum, while the dual competition mechanism contrasts bullish and bearish embeddings to capture behavioral divergence. Together, these components allow B4 to model bias-driven asymmetry, behavioral inertia, and market heterogeneity. Experimental results on real-world financial datasets demonstrate that our model not only achieves superior performance in predicting market trends but also provides interpretable insights into the interplay of biases, investor behaviors, and market dynamics.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroHD-RA: Neural-distilled Hyperdimensional Model with Rhythm Alignment</title>
<link>https://arxiv.org/abs/2507.14184</link>
<guid>https://arxiv.org/abs/2507.14184</guid>
<content:encoded><![CDATA[
arXiv:2507.14184v1 Announce Type: cross 
Abstract: We present a novel and interpretable framework for electrocardiogram (ECG)-based disease detection that combines hyperdimensional computing (HDC) with learnable neural encoding. Unlike conventional HDC approaches that rely on static, random projections, our method introduces a rhythm-aware and trainable encoding pipeline based on RR intervals, a physiological signal segmentation strategy that aligns with cardiac cycles. The core of our design is a neural-distilled HDC architecture, featuring a learnable RR-block encoder and a BinaryLinear hyperdimensional projection layer, optimized jointly with cross-entropy and proxy-based metric loss. This hybrid framework preserves the symbolic interpretability of HDC while enabling task-adaptive representation learning. Experiments on Apnea-ECG and PTB-XL demonstrate that our model significantly outperforms traditional HDC and classical ML baselines, achieving 73.09\% precision and an F1 score of 0.626 on Apnea-ECG, with comparable robustness on PTB-XL. Our framework offers an efficient and scalable solution for edge-compatible ECG classification, with strong potential for interpretable and personalized health monitoring.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction</title>
<link>https://arxiv.org/abs/2507.14186</link>
<guid>https://arxiv.org/abs/2507.14186</guid>
<content:encoded><![CDATA[
arXiv:2507.14186v1 Announce Type: cross 
Abstract: The expansion of the low-altitude economy has underscored the significance of Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors. While accurate LANC forecasting hinges on the antenna beam patterns of Base Stations (BSs), these patterns are typically proprietary and not readily accessible. Operational parameters of BSs, which inherently contain beam information, offer an opportunity for data-driven low-altitude coverage prediction. However, collecting extensive low-altitude road test data is cost-prohibitive, often yielding only sparse samples per BS. This scarcity results in two primary challenges: imbalanced feature sampling due to limited variability in high-dimensional operational parameters against the backdrop of substantial changes in low-dimensional sampling locations, and diminished generalizability stemming from insufficient data samples. To overcome these obstacles, we introduce a dual strategy comprising expert knowledge-based feature compression and disentangled representation learning. The former reduces feature space complexity by leveraging communications expertise, while the latter enhances model generalizability through the integration of propagation models and distinct subnetworks that capture and aggregate the semantic representations of latent features. Experimental evaluation confirms the efficacy of our framework, yielding a 7% reduction in error compared to the best baseline algorithm. Real-network validations further attest to its reliability, achieving practical prediction accuracy with MAE errors at the 5dB level.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Based Impedance Encoding-Decoding Method for Online Impedance Network Construction of Wind Farms</title>
<link>https://arxiv.org/abs/2507.14187</link>
<guid>https://arxiv.org/abs/2507.14187</guid>
<content:encoded><![CDATA[
arXiv:2507.14187v1 Announce Type: cross 
Abstract: The impedance network (IN) model is gaining popularity in the oscillation analysis of wind farms. However, the construction of such an IN model requires impedance curves of each wind turbine under their respective operating conditions, making its online application difficult due to the transmission of numerous high-density impedance curves. To address this issue, this paper proposes an AI-based impedance encoding-decoding method to facilitate the online construction of IN model. First, an impedance encoder is trained to compress impedance curves by setting the number of neurons much smaller than that of frequency points. Then, the compressed data of each turbine are uploaded to the wind farm and an impedance decoder is trained to reconstruct original impedance curves. At last, based on the nodal admittance matrix (NAM) method, the IN model of the wind farm can be obtained. The proposed method is validated via model training and real-time simulations, demonstrating that the encoded impedance vectors enable fast transmission and accurate reconstruction of the original impedance curves.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks</title>
<link>https://arxiv.org/abs/2507.14188</link>
<guid>https://arxiv.org/abs/2507.14188</guid>
<content:encoded><![CDATA[
arXiv:2507.14188v1 Announce Type: cross 
Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard smartphones, using unmodified 3GPP protocols, connected directly to low Earth orbit (LEO) satellites. This first wave of direct-to-device (D2D) demonstrations validated the physical feasibility of satellite-based mobile access. However, these systems remain fallback-grade--rural-only, bandwidth-limited, and fully dependent on Earth-based mobile cores for identity, session, and policy control. This paper asks a more ambitious question: Can a complete mobile network, including radio access, core functions, traffic routing, and content delivery, operate entirely from orbit? And can it deliver sustained, urban-grade service in the world's densest cities? We present the first end-to-end system architecture for a fully orbital telco, integrating electronically steered phased arrays with 1000-beam capacity, space-based deployment of 5G core functions (UPF, AMF), and inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam capacity, and link budgets under dense urban conditions, accounting for path loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight users can sustain 64-QAM throughput, while street-level access is feasible with relay or assisted beam modes. The paper outlines the remaining constraints, power, thermal dissipation, compute radiation hardening, and regulatory models, and demonstrates that these are engineering bottlenecks, not physical limits. Finally, we propose a staged 15-year roadmap from today's fallback D2D systems to autonomous orbital overlays delivering 50-100 Mbps to handhelds in megacities, with zero reliance on terrestrial infrastructure.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base</title>
<link>https://arxiv.org/abs/2507.14189</link>
<guid>https://arxiv.org/abs/2507.14189</guid>
<content:encoded><![CDATA[
arXiv:2507.14189v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various applications. However, their use as writing assistants in specialized domains like finance, medicine, and law is often hampered by a lack of deep domain-specific knowledge and a tendency to hallucinate. Existing solutions, such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency across multiple retrieval steps, while online search-based methods often degrade quality due to unreliable web content. To address these challenges, we introduce DeepWriter, a customizable, multimodal, long-form writing assistant that operates on a curated, offline knowledge base. DeepWriter leverages a novel pipeline that involves task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection. By deeply mining information from a structured corpus and incorporating both textual and visual elements, DeepWriter generates coherent, factually grounded, and professional-grade documents. We also propose a hierarchical knowledge representation to enhance retrieval efficiency and accuracy. Our experiments on financial report generation demonstrate that DeepWriter produces high-quality, verifiable articles that surpasses existing baselines in factual accuracy and generated content quality.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Formal Model of the Economic Impacts of AI Openness Regulation</title>
<link>https://arxiv.org/abs/2507.14193</link>
<guid>https://arxiv.org/abs/2507.14193</guid>
<content:encoded><![CDATA[
arXiv:2507.14193v1 Announce Type: cross 
Abstract: Regulatory frameworks, such as the EU AI Act, encourage openness of general-purpose AI models by offering legal exemptions for "open-source" models. Despite this legislative attention on openness, the definition of open-source foundation models remains ambiguous. This paper models the strategic interactions among the creator of a general-purpose model (the generalist) and the entity that fine-tunes the general-purpose model to a specialized domain or task (the specialist), in response to regulatory requirements on model openness. We present a stylized model of the regulator's choice of an open-source definition to evaluate which AI openness standards will establish appropriate economic incentives for developers. Our results characterize market equilibria -- specifically, upstream model release decisions and downstream fine-tuning efforts -- under various openness regulations and present a range of effective regulatory penalties and open-source thresholds. Overall, we find the model's baseline performance determines when increasing the regulatory penalty vs. the open-source threshold will significantly alter the generalist's release strategy. Our model provides a theoretical foundation for AI governance decisions around openness and enables evaluation and refinement of practical open-source policies.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UWB Radar-based Heart Rate Monitoring: A Transfer Learning Approach</title>
<link>https://arxiv.org/abs/2507.14195</link>
<guid>https://arxiv.org/abs/2507.14195</guid>
<content:encoded><![CDATA[
arXiv:2507.14195v1 Announce Type: cross 
Abstract: Radar technology presents untapped potential for continuous, contactless, and passive heart rate monitoring via consumer electronics like mobile phones. However the variety of available radar systems and lack of standardization means that a large new paired dataset collection is required for each radar system. This study demonstrates transfer learning between frequency-modulated continuous wave (FMCW) and impulse-radio ultra-wideband (IR-UWB) radar systems, both increasingly integrated into consumer devices. FMCW radar utilizes a continuous chirp, while IR-UWB radar employs short pulses. Our mm-wave FMCW radar operated at 60 GHz with a 5.5 GHz bandwidth (2.7 cm resolution, 3 receiving antennas [Rx]), and our IR-UWB radar at 8 GHz with a 500 MHz bandwidth (30 cm resolution, 2 Rx). Using a novel 2D+1D ResNet architecture we achieved a mean absolute error (MAE) of 0.85 bpm and a mean absolute percentage error (MAPE) of 1.42% for heart rate monitoring with FMCW radar (N=119 participants, an average of 8 hours per participant). This model maintained performance (under 5 MAE/10% MAPE) across various body positions and heart rate ranges, with a 98.9% recall. We then fine-tuned a variant of this model, trained on single-antenna and single-range bin FMCW data, using a small (N=376, avg 6 minutes per participant) IR-UWB dataset. This transfer learning approach yielded a model with MAE 4.1 bpm and MAPE 6.3% (97.5% recall), a 25% MAE reduction over the IR-UWB baseline. This demonstration of transfer learning between radar systems for heart rate monitoring has the potential to accelerate its introduction into existing consumer devices.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Parallel CNN-LSTM Model for Differentiating Ventricular Tachycardia from Supraventricular Tachycardia with Aberrancy in 12-Lead ECGs</title>
<link>https://arxiv.org/abs/2507.14196</link>
<guid>https://arxiv.org/abs/2507.14196</guid>
<content:encoded><![CDATA[
arXiv:2507.14196v1 Announce Type: cross 
Abstract: Background and Objective: Differentiating wide complex tachycardia (WCT) is clinically critical yet challenging due to morphological similarities in electrocardiogram (ECG) signals between life-threatening ventricular tachycardia (VT) and supraventricular tachycardia with aberrancy (SVT-A). Misdiagnosis carries fatal risks. We propose a computationally efficient deep learning solution to improve diagnostic accuracy and provide model interpretability for clinical deployment.
  Methods: A novel lightweight parallel deep architecture is introduced. Each pipeline processes individual ECG leads using two 1D-CNN blocks to extract local features. Feature maps are concatenated across leads, followed by LSTM layers to capture temporal dependencies. Final classification employs fully connected layers. Explainability is achieved via Shapley Additive Explanations (SHAP) for local/global interpretation. The model was evaluated on a 35-subject ECG database using standard performance metrics.
  Results: The model achieved $95.63\%$ accuracy ($95\%$ CI: $93.07-98.19\%$), with sensitivity=$95.10\%$, specificity=$96.06\%$, and F1-score=$95.12\%$. It outperformed state-of-the-art methods in both accuracy and computational efficiency, requiring minimal CNN blocks per pipeline. SHAP analysis demonstrated clinically interpretable feature contributions.
  Conclusions: Our end-to-end framework delivers high-precision WCT classification with minimal computational overhead. The integration of SHAP enhances clinical trust by elucidating decision logic, supporting rapid, informed diagnosis. This approach shows significant promise for real-world ECG analysis tools.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retention analysis of edited knowledge after fine-tuning</title>
<link>https://arxiv.org/abs/2507.14198</link>
<guid>https://arxiv.org/abs/2507.14198</guid>
<content:encoded><![CDATA[
arXiv:2507.14198v1 Announce Type: cross 
Abstract: Large language models (LLMs) store vast amounts of knowledge, which often requires updates to correct factual errors, incorporate newly acquired information, or adapt model behavior. Model editing methods have emerged as efficient solutions for such updates, offering localized and precise knowledge modification at significantly lower computational cost than continual training. In parallel, LLMs are frequently fine-tuned for a wide range of downstream tasks. However, the effect of fine-tuning on previously edited knowledge remains poorly understood. In this work, we systematically investigate how different fine-tuning objectives interact with various model editing techniques. Our findings show that edited knowledge is substantially more susceptible to forgetting during fine-tuning than intrinsic knowledge acquired through pre-training. This analysis highlights a key limitation of current editing approaches and suggests that evaluating edit robustness under downstream fine-tuning is critical for their practical deployment. We further find that freezing layers associated with edited content can significantly improve knowledge retention, offering insight into how future editing methods might be made more robust.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System</title>
<link>https://arxiv.org/abs/2507.14200</link>
<guid>https://arxiv.org/abs/2507.14200</guid>
<content:encoded><![CDATA[
arXiv:2507.14200v1 Announce Type: cross 
Abstract: This paper aims to demonstrate the potential and strengths of open-source collectives. It leads to a promising question: Can we harness multiple open-source LLMs to match or even beat the closed-source LLMs? To answer this, we propose SMACS, a scalable multi-agent collaboration system (MACS) framework with high performance. Specifically, for continuous integration of new LLMs and generalization to diverse questions, we first propose a Retrieval-based Prior Selection (RPS), which assigns a proxy performance score to each LLM to select the Top-k LLMs at the instance level for any given question. Then, we propose an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the generation of diverse responses through prior dropping and selecting the high-quality response via a hybrid posterior score. Experiments on eight mainstream benchmarks validate the effectiveness of our SMACS: by integrating fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025, e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%) across multiple tasks. Remarkably, it even exceeds the average of best results of different datasets from both open-source LLMs (+2.86%) and closed-source LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released at https://github.com/magent4aci/SMACS.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation</title>
<link>https://arxiv.org/abs/2507.14201</link>
<guid>https://arxiv.org/abs/2507.14201</guid>
<content:encoded><![CDATA[
arXiv:2507.14201v1 Announce Type: cross 
Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on the task of Cyber Threat Investigation through security questions derived from investigation graphs. Real-world security analysts must sift through a large number of heterogeneous alert signals and security logs, follow multi-hop chains of evidence, and compile an incident report. With the developments of LLMs, building LLM-based agents for automatic thread investigation is a promising direction. To assist the development and evaluation of LLM agents, we construct a dataset from a controlled Azure tenant that covers 8 simulated real-world multi-step attacks, 57 log tables from Microsoft Sentinel and related services, and 589 automatically generated questions. We leverage security logs extracted with expert-crafted detection logic to build threat investigation graphs, and then generate questions with LLMs using paired nodes on the graph, taking the start node as background context and the end node as answer. Anchoring each question to these explicit nodes and edges not only provides automatic, explainable ground truth answers but also makes the pipeline reusable and readily extensible to new logs. This also enables the automatic generation of procedural tasks with verifiable rewards, which can be naturally extended to training agents via reinforcement learning. Our comprehensive experiments with different models confirm the difficulty of the task: with the base setting, the average reward across all evaluated models is 0.249, and the best achieved is 0.368, leaving substantial headroom for future research. Code and data are coming soon!
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training</title>
<link>https://arxiv.org/abs/2507.14202</link>
<guid>https://arxiv.org/abs/2507.14202</guid>
<content:encoded><![CDATA[
arXiv:2507.14202v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse applications, yet they pose significant security risks that threaten their safe deployment in critical domains. Current security alignment methodologies predominantly rely on Process Reward Models (PRMs) to evaluate intermediate reasoning steps, introducing substantial computational overhead and scalability constraints. This paper presents a novel PRM-free security alignment framework that leverages automated red teaming and adversarial training to achieve robust security guarantees while maintaining computational efficiency. Our approach systematically identifies vulnerabilities through sophisticated attack strategies including genetic algorithm optimization, multi-agent simulation, and advanced prompt mutation techniques. The framework enhances model robustness via targeted adversarial training with curriculum learning and adaptive regularization mechanisms. Comprehensive experimental evaluation across five state-of-the-art LLMs demonstrates that our method achieves superior security alignment performance compared to PRM-based approaches while reducing computational costs by 61\%. The framework incorporates transparent reporting and continuous audit mechanisms that enable iterative security improvement and regulatory compliance. Our contributions advance the field of efficient LLM security alignment by democratizing access to robust security measures for resource-constrained organizations and providing a scalable foundation for addressing evolving adversarial threats.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models</title>
<link>https://arxiv.org/abs/2507.14204</link>
<guid>https://arxiv.org/abs/2507.14204</guid>
<content:encoded><![CDATA[
arXiv:2507.14204v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in numerous applications requiring robust long-range capabilities, essential for processing extensive input contexts and continuously generating extended outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in LLMs escalates, creating a significant efficiency bottleneck. In this paper, we propose a new KV cache optimization paradigm called LaCache, a training-free method for efficient and accurate generative inference of LLMs. LaCache enables LLMs to simultaneously address both of the critical challenges in long-range modeling: robust long-range capabilities and continuous generation without running out-of-memory (OOM). Specifically, LaCache integrates two key innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only sequentially (left-to-right within each layer) but also across layers (from shallow to deep), providing an extended span for capturing long-range dependencies under a fixed storage budget, thereby boosting long-range capabilities; and (2) an iterative compaction mechanism that progressively compresses older caches, freeing up space for new tokens within a fixed cache size. This token distance-based dynamic compression enables more effective continuous generation under constrained cache budgets. Experiments across various tasks, benchmarks, and LLM models consistently validate LaCache's effectiveness in enhancing LLMs' long-range capabilities. Our code is available at https://github.com/GATECH-EIC/LaCache.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Benchmark for Electrocardiogram Time-Series</title>
<link>https://arxiv.org/abs/2507.14206</link>
<guid>https://arxiv.org/abs/2507.14206</guid>
<content:encoded><![CDATA[
arXiv:2507.14206v1 Announce Type: cross 
Abstract: Electrocardiogram~(ECG), a key bioelectrical time-series signal, is crucial for assessing cardiac health and diagnosing various diseases. Given its time-series format, ECG data is often incorporated into pre-training datasets for large-scale time-series model training. However, existing studies often overlook its unique characteristics and specialized downstream applications, which differ significantly from other time-series data, leading to an incomplete understanding of its properties. In this paper, we present an in-depth investigation of ECG signals and establish a comprehensive benchmark, which includes (1) categorizing its downstream applications into four distinct evaluation tasks, (2) identifying limitations in traditional evaluation metrics for ECG analysis, and introducing a novel metric; (3) benchmarking state-of-the-art time-series models and proposing a new architecture. Extensive experiments demonstrate that our proposed benchmark is comprehensive and robust. The results validate the effectiveness of the proposed metric and model architecture, which establish a solid foundation for advancing research in ECG signal analysis.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design</title>
<link>https://arxiv.org/abs/2507.14207</link>
<guid>https://arxiv.org/abs/2507.14207</guid>
<content:encoded><![CDATA[
arXiv:2507.14207v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) in K--12 education offers both transformative opportunities and emerging risks. This study explores how students may Trojanize prompts to elicit unsafe or unintended outputs from LLMs, bypassing established content moderation systems with safety guardrils. Through a systematic experiment involving simulated K--12 queries and multi-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This paper presents our experimental design, detailed findings, and a prototype tool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized educational prompts. These insights aim to inform both AI safety researchers and educational technologists on the safe deployment of LLMs for educators.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence</title>
<link>https://arxiv.org/abs/2507.14211</link>
<guid>https://arxiv.org/abs/2507.14211</guid>
<content:encoded><![CDATA[
arXiv:2507.14211v1 Announce Type: cross 
Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS changes, e.g., in wireless networks, and trigger appropriate countermeasures to avoid performance degradation. Hence, PQoS is extremely useful for automotive applications such as teleoperated driving, which poses strict constraints in terms of latency and reliability. A promising tool for PQoS is given by Reinforcement Learning (RL), a methodology that enables the design of decision-making strategies for stochastic optimization. In this manuscript, we present PRATA, a new simulation framework to enable PRedictive QoS based on AI for Teleoperated driving Applications. PRATA consists of a modular pipeline that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access Network (RAN), (ii) a tool for generating automotive data, and (iii) an Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the segmentation level of teleoperated driving data in the event of resource saturation or channel degradation. Hence, we show that the RAN-AI entity efficiently balances the trade-off between QoS and Quality of Experience (QoE) that characterize teleoperated driving applications, almost doubling the system performance compared to baseline approaches. In addition, by varying the learning settings of the RAN-AI entity, we investigate the impact of the state space and the relative cost of acquiring network data that are necessary for the implementation of RL.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Castes: Artificial Intelligence, Epistemic Stratification, and the Dissolution of Democratic Discourse</title>
<link>https://arxiv.org/abs/2507.14218</link>
<guid>https://arxiv.org/abs/2507.14218</guid>
<content:encoded><![CDATA[
arXiv:2507.14218v1 Announce Type: cross 
Abstract: Artificial intelligence functions not as an epistemic leveller, but as an accelerant of cognitive stratification, entrenching and formalising informational castes within liberal-democratic societies. Synthesising formal epistemology, political theory, algorithmic architecture, and economic incentive structures, the argument traces how contemporary AI systems selectively amplify the reasoning capacity of individuals equipped with recursive abstraction, symbolic logic, and adversarial interrogation, whilst simultaneously pacifying the cognitively untrained through engagement-optimised interfaces. Fluency replaces rigour, immediacy displaces reflection, and procedural reasoning is eclipsed by reactive suggestion. The result is a technocratic realignment of power: no longer grounded in material capital alone, but in the capacity to navigate, deconstruct, and manipulate systems of epistemic production. Information ceases to be a commons; it becomes the substrate through which consent is manufactured and autonomy subdued. Deliberative democracy collapses not through censorship, but through the erosion of interpretive agency. The proposed response is not technocratic regulation, nor universal access, but the reconstruction of rational autonomy as a civic mandate, codified in education, protected by epistemic rights, and structurally embedded within open cognitive infrastructure.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman</title>
<link>https://arxiv.org/abs/2507.14219</link>
<guid>https://arxiv.org/abs/2507.14219</guid>
<content:encoded><![CDATA[
arXiv:2507.14219v1 Announce Type: cross 
Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has emerged as a promising strategic pathway toward decarbonisation, particularly in solar-rich arid regions. However, identifying optimal locations for hydrogen production requires the integration of complex environmental, atmospheric, and infrastructural factors, often compounded by limited availability of direct hydrogen yield data. This study presents a novel Artificial Intelligence (AI) framework for computing green hydrogen yield and site suitability index using mean absolute SHAP (SHapley Additive exPlanations) values. This framework consists of a multi-stage pipeline of unsupervised multi-variable clustering, supervised machine learning classifier and SHAP algorithm. The pipeline trains on an integrated meteorological, topographic and temporal dataset and the results revealed distinct spatial patterns of suitability and relative influence of the variables. With model predictive accuracy of 98%, the result also showed that water proximity, elevation and seasonal variation are the most influential factors determining green hydrogen site suitability in Oman with mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively. Given limited or absence of ground-truth yield data in many countries that have green hydrogen prospects and ambitions, this study offers an objective and reproducible alternative to subjective expert weightings, thus allowing the data to speak for itself and potentially discover novel latent groupings without pre-imposed assumptions. This study offers industry stakeholders and policymakers a replicable and scalable tool for green hydrogen infrastructure planning and other decision making in data-scarce regions.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification</title>
<link>https://arxiv.org/abs/2507.14223</link>
<guid>https://arxiv.org/abs/2507.14223</guid>
<content:encoded><![CDATA[
arXiv:2507.14223v1 Announce Type: cross 
Abstract: Explainable intrusion detection systems (IDS) are now recognized as essential for mission-critical networks, yet most "XAI" pipelines still bolt an approximate explainer onto an opaque classifier, leaving analysts with partial and sometimes misleading insights. The Interpretable Generalization (IG) mechanism, published in IEEE Transactions on Information Forensics and Security, eliminates that bottleneck by learning coherent patterns - feature combinations unique to benign or malicious traffic - and turning them into fully auditable rules. IG already delivers outstanding precision, recall, and AUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the data. To raise precision further without sacrificing transparency, we introduce Multi-Granular Discretization (IG-MD), which represents every continuous feature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts precision by greater than or equal to 4 percentage points across all nine train-test splits while preserving recall approximately equal to 1.0, demonstrating that a single interpretation-ready model can scale across domains without bespoke tuning.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Generalization via Pareto Optimal Gradient Matching</title>
<link>https://arxiv.org/abs/2507.14227</link>
<guid>https://arxiv.org/abs/2507.14227</guid>
<content:encoded><![CDATA[
arXiv:2507.14227v1 Announce Type: cross 
Abstract: In this study, we address the gradient-based domain generalization problem, where predictors aim for consistent gradient directions across different domains. Existing methods have two main challenges. First, minimization of gradient empirical distance or gradient inner products (GIP) leads to gradient fluctuations among domains, thereby hindering straightforward learning. Second, the direct application of gradient learning to the joint loss function can incur high computation overheads due to second-order derivative approximation. To tackle these challenges, we propose a new Pareto Optimality Gradient Matching (POGM) method. In contrast to existing methods that add gradient matching as regularization, we leverage gradient trajectories as collected data and apply independent training at the meta-learner. In the meta-update, we maximize GIP while limiting the learned gradient from deviating too far from the empirical risk minimization gradient trajectory. By doing so, the aggregate gradient can incorporate knowledge from all domains without suffering gradient fluctuation towards any particular domain. Experimental evaluations on datasets from DomainBed demonstrate competitive results yielded by POGM against other baselines while achieving computational efficiency.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intent-Based Network for RAN Management with Large Language Models</title>
<link>https://arxiv.org/abs/2507.14230</link>
<guid>https://arxiv.org/abs/2507.14230</guid>
<content:encoded><![CDATA[
arXiv:2507.14230v1 Announce Type: cross 
Abstract: Advanced intelligent automation becomes an important feature to deal with the increased complexity in managing wireless networks. This paper proposes a novel automation approach of intent-based network for Radio Access Networks (RANs) management by leveraging Large Language Models (LLMs). The proposed method enhances intent translation, autonomously interpreting high-level objectives, reasoning over complex network states, and generating precise configurations of the RAN by integrating LLMs within an agentic architecture. We propose a structured prompt engineering technique and demonstrate that the network can automatically improve its energy efficiency by dynamically optimizing critical RAN parameters through a closed-loop mechanism. It showcases the potential to enable robust resource management in RAN by adapting strategies based on real-time feedback via LLM-orchestrated agentic systems.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media</title>
<link>https://arxiv.org/abs/2507.14231</link>
<guid>https://arxiv.org/abs/2507.14231</guid>
<content:encoded><![CDATA[
arXiv:2507.14231v1 Announce Type: cross 
Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to subtle early symptoms and social stigma. This paper explores the advanced natural language processing (NLP) models for recognizing signs of bipolar disorder based on user-generated social media text. We conduct a comprehensive evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized (BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed on a large, annotated dataset of Reddit posts after confirming their validity through sentiment variance and judgmental analysis. Our results demonstrate that RoBERTa achieves the highest performance among transformer models with an F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical results. In contrast, LSTMs trained on static embeddings fail to capture meaningful patterns, scoring near-zero F1. These findings underscore the critical role of contextual language modeling in detecting bipolar disorder. In addition, we report model training times and highlight that DistilBERT offers an optimal balance between efficiency and accuracy. In general, our study offers actionable insights for model selection in mental health NLP applications and validates the potential of contextualized language models to support early bipolar disorder screening.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model</title>
<link>https://arxiv.org/abs/2507.14237</link>
<guid>https://arxiv.org/abs/2507.14237</guid>
<content:encoded><![CDATA[
arXiv:2507.14237v1 Announce Type: cross 
Abstract: This paper explores the outcome of training state-ofthe-art dereverberation models with supervision settings ranging from weakly-supervised to fully unsupervised, relying solely on reverberant signals and an acoustic model for training. Most of the existing deep learning approaches typically require paired dry and reverberant data, which are difficult to obtain in practice. We develop instead a sequential learning strategy motivated by a bayesian formulation of the dereverberation problem, wherein acoustic parameters and dry signals are estimated from reverberant inputs using deep neural networks, guided by a reverberation matching loss. Our most data-efficient variant requires only 100 reverberation-parameter-labelled samples to outperform an unsupervised baseline, demonstrating the effectiveness and practicality of the proposed method in low-resource scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Change Facts Based on the Way You Talk</title>
<link>https://arxiv.org/abs/2507.14238</link>
<guid>https://arxiv.org/abs/2507.14238</guid>
<content:encoded><![CDATA[
arXiv:2507.14238v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly being used in user-facing applications, from providing medical consultations to job interview advice. Recent research suggests that these models are becoming increasingly proficient at inferring identity information about the author of a piece of text from linguistic patterns as subtle as the choice of a few words. However, little is known about how LLMs use this information in their decision-making in real-world applications. We perform the first comprehensive analysis of how identity markers present in a user's writing bias LLM responses across five different high-stakes LLM applications in the domains of medicine, law, politics, government benefits, and job salaries. We find that LLMs are extremely sensitive to markers of identity in user queries and that race, gender, and age consistently influence LLM responses in these applications. For instance, when providing medical advice, we find that models apply different standards of care to individuals of different ethnicities for the same symptoms; we find that LLMs are more likely to alter answers to align with a conservative (liberal) political worldview when asked factual questions by older (younger) individuals; and that LLMs recommend lower salaries for non-White job applicants and higher salaries for women compared to men. Taken together, these biases mean that the use of off-the-shelf LLMs for these applications may cause harmful differences in medical care, foster wage gaps, and create different political factual realities for people of different identities. Beyond providing an analysis, we also provide new tools for evaluating how subtle encoding of identity in users' language choices impacts model decisions. Given the serious implications of these findings, we recommend that similar thorough assessments of LLM use in user-facing applications are conducted before future deployment.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation</title>
<link>https://arxiv.org/abs/2507.14239</link>
<guid>https://arxiv.org/abs/2507.14239</guid>
<content:encoded><![CDATA[
arXiv:2507.14239v1 Announce Type: cross 
Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization across languages, yet they remain prone to hallucinations, especially in low-resource languages, due to training data imbalances. These hallucinations, which include inaccurate or fabricated outputs, are particularly problematic in domain-specific generation tasks (Chataigner et al., 2024). To address this challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for mitigating hallucination in MLLMs. Our approach first enhances cross-lingual semantic alignment through curriculum-based contrastive learning combined with next-token prediction during continued pre-training. Building on this foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting strategy during instruction fine-tuning, which guides the model to reason in a high-resource language before generating answers in the target low-resource language. Experimental results show that CCL-XCoT reduces hallucination rates by up to 62% and substantially improves factual knowledge transfer across language pairs, without relying on external retrieval or multi-model ensembles.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HuggingGraph: Understanding the Supply Chain of LLM Ecosystem</title>
<link>https://arxiv.org/abs/2507.14240</link>
<guid>https://arxiv.org/abs/2507.14240</guid>
<content:encoded><![CDATA[
arXiv:2507.14240v1 Announce Type: cross 
Abstract: Large language models (LLMs) leverage deep learning to process and predict sequences of words from context, enabling them to perform various NLP tasks, such as translation, summarization, question answering, and content generation. However, the growing size and complexity of developing, training, and deploying advanced LLMs require extensive computational resources and large datasets. This creates a barrier for users. As a result, platforms that host models and datasets are widely used. For example, Hugging Face, one of the most popular platforms, hosted 1.8 million models and 450K datasets by June 2025, with no sign of slowing down. Since many LLMs are built from base models, pre-trained models, and external datasets, they can inherit vulnerabilities, biases, or malicious components from earlier models or datasets. Therefore, it is critical to understand the origin and development of these components to better detect potential risks, improve model fairness, and ensure compliance. Motivated by this, our project aims to study the relationships between models and datasets, which are core components of the LLM supply chain. First, we design a method to systematically collect LLM supply chain data. Using this data, we build a directed heterogeneous graph to model the relationships between models and datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We then perform various analyses and uncover several findings, such as: (i) the LLM supply chain graph is large, sparse, and follows a power-law degree distribution; (ii) it features a densely connected core and a fragmented periphery; (iii) datasets play pivotal roles in training; (iv) strong interdependence exists between models and datasets; and (v) the graph is dynamic, with daily updates reflecting the ecosystem's ongoing evolution.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2507.14241</link>
<guid>https://arxiv.org/abs/2507.14241</guid>
<content:encoded><![CDATA[
arXiv:2507.14241v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet prompt engineering remains manual, inconsistent, and inaccessible to non-experts. We introduce Promptomatix, an automatic prompt optimization framework that transforms natural language task descriptions into high-quality prompts without requiring manual tuning or domain expertise. Promptomatix supports both a lightweight meta-prompt-based optimizer and a DSPy-powered compiler, with modular design enabling future extension to more advanced frameworks. The system analyzes user intent, generates synthetic training data, selects prompting strategies, and refines prompts using cost-aware objectives. Evaluated across 5 task categories, Promptomatix achieves competitive or superior performance compared to existing libraries, while reducing prompt length and computational overhead making prompt optimization scalable and efficient.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Culling Misinformation from Gen AI: Toward Ethical Curation and Refinement</title>
<link>https://arxiv.org/abs/2507.14242</link>
<guid>https://arxiv.org/abs/2507.14242</guid>
<content:encoded><![CDATA[
arXiv:2507.14242v1 Announce Type: cross 
Abstract: While Artificial Intelligence (AI) is not a new field, recent developments, especially with the release of generative tools like ChatGPT, have brought it to the forefront of the minds of industry workers and academic folk alike. There is currently much talk about AI and its ability to reshape many everyday processes as we know them through automation. It also allows users to expand their ideas by suggesting things they may not have thought of on their own and provides easier access to information. However, not all of the changes this technology will bring or has brought so far are positive; this is why it is extremely important for all modern people to recognize and understand the risks before using these tools and allowing them to cause harm. This work takes a position on better understanding many equity concerns and the spread of misinformation that result from new AI, in this case, specifically ChatGPT and deepfakes, and encouraging collaboration with law enforcement, developers, and users to reduce harm. Considering many academic sources, it warns against these issues, analyzing their cause and impact in fields including healthcare, education, science, academia, retail, and finance. Lastly, we propose a set of future-facing guidelines and policy considerations to solve these issues while still enabling innovation in these fields, this responsibility falling upon users, developers, and government entities.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions</title>
<link>https://arxiv.org/abs/2507.14245</link>
<guid>https://arxiv.org/abs/2507.14245</guid>
<content:encoded><![CDATA[
arXiv:2507.14245v1 Announce Type: cross 
Abstract: Unlocking the potential of nanomaterials in medicine and environmental science hinges on understanding their interactions with proteins, a complex decision space where AI is poised to make a transformative impact. However, progress has been hindered by limited datasets and the restricted generalizability of existing models. Here, we propose NanoPro-3M, the largest nanomaterial-protein interaction dataset to date, comprising over 3.2 million samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer, a foundational model that predicts nanomaterial-protein affinities through multimodal representation learning, demonstrating strong generalization, handling missing features, and unseen nanomaterials or proteins. We show that multimodal modeling significantly outperforms single-modality approaches and identifies key determinants of corona formation. Furthermore, we demonstrate its applicability to a range of downstream tasks through zero-shot inference and fine-tuning. Together, this work establishes a solid foundation for high-performance and generalized prediction of nanomaterial-protein interaction endpoints, reducing experimental reliance and accelerating various in vitro applications.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack</title>
<link>https://arxiv.org/abs/2507.14248</link>
<guid>https://arxiv.org/abs/2507.14248</guid>
<content:encoded><![CDATA[
arXiv:2507.14248v1 Announce Type: cross 
Abstract: Vision transformer (ViT) models, when coupled with interpretation models, are regarded as secure and challenging to deceive, making them well-suited for security-critical domains such as medical applications, autonomous vehicles, drones, and robotics. However, successful attacks on these systems can lead to severe consequences. Recent research on threats targeting ViT models primarily focuses on generating the smallest adversarial perturbations that can deceive the models with high confidence, without considering their impact on model interpretations. Nevertheless, the use of interpretation models can effectively assist in detecting adversarial examples. This study investigates the vulnerability of transformer models to adversarial attacks, even when combined with interpretation models. We propose an attack called "AdViT" that generates adversarial examples capable of misleading both a given transformer model and its coupled interpretation model. Through extensive experiments on various transformer models and two transformer-based interpreters, we demonstrate that AdViT achieves a 100% attack success rate in both white-box and black-box scenarios. In white-box scenarios, it reaches up to 98% misclassification confidence, while in black-box scenarios, it reaches up to 76% misclassification confidence. Remarkably, AdViT consistently generates accurate interpretations in both scenarios, making the adversarial examples more difficult to detect.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2507.14249</link>
<guid>https://arxiv.org/abs/2507.14249</guid>
<content:encoded><![CDATA[
arXiv:2507.14249v1 Announce Type: cross 
Abstract: Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions to alleviate urban congestion, with path planning becoming a key focus area. Unlike ground transportation, UAM trajectory planning has to prioritize communication quality for accurate location tracking in constantly changing environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi, requires adaptive planning to respond to real-time passenger requests, especially in ride-sharing scenarios where passenger demands are unpredictable and dynamic. However, conventional trajectory planning strategies based on predefined routes lack the flexibility to meet varied passenger ride demands. To address these challenges, this work first proposes constructing a radio map to evaluate the communication quality of urban airspace. Building on this, we introduce a novel Multi-Source Hybrid Attention Reinforcement Learning (MSHA-RL) framework for the challenge of effectively focusing on passengers and UAM locations, which arises from the significant dimensional disparity between the representations. This model first generates the alignment among diverse data sources with large gap dimensions before employing hybrid attention to balance global and local insights, thereby facilitating responsive, real-time path planning. Extensive experimental results demonstrate that the approach enables communication-compliant trajectory planning, reducing travel time and enhancing operational efficiency while prioritizing passenger safety.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models</title>
<link>https://arxiv.org/abs/2507.14256</link>
<guid>https://arxiv.org/abs/2507.14256</guid>
<content:encoded><![CDATA[
arXiv:2507.14256v1 Announce Type: cross 
Abstract: Generative AI is gaining increasing attention in software engineering, where testing remains an indispensable reliability mechanism. According to the widely adopted testing pyramid, unit tests constitute the majority of test cases and are often schematic, requiring minimal domain expertise. Automatically generating such tests under the supervision of software engineers can significantly enhance productivity during the development phase of the software lifecycle.
  This paper investigates the impact of code context and prompting strategies on the quality and adequacy of unit tests generated by various large language models (LLMs) across several families. The results show that including docstrings notably improves code adequacy, while further extending context to the full implementation yields definitely smaller gains. Notably, the chain-of-thought prompting strategy -- applied even to 'reasoning' models -- achieves the best results, with up to 96.3\% branch coverage, a 57\% average mutation score, and near-perfect compilation success rate. Among the evaluated models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation score and branch coverage being still in top in terms of compilation success rate.
  All the code and resulting test suites are publicly available at https://github.com/peetery/LLM-analysis.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAMST: Fast Approximate Minimum Spanning Tree Construction for Large-Scale and High-Dimensional Data</title>
<link>https://arxiv.org/abs/2507.14261</link>
<guid>https://arxiv.org/abs/2507.14261</guid>
<content:encoded><![CDATA[
arXiv:2507.14261v1 Announce Type: cross 
Abstract: We present Fast Approximate Minimum Spanning Tree (FAMST), a novel algorithm that addresses the computational challenges of constructing Minimum Spanning Trees (MSTs) for large-scale and high-dimensional datasets. FAMST utilizes a three-phase approach: Approximate Nearest Neighbor (ANN) graph construction, ANN inter-component connection, and iterative edge refinement. For a dataset of $n$ points in a $d$-dimensional space, FAMST achieves $\mathcal{O}(dn \log n)$ time complexity and $\mathcal{O}(dn + kn)$ space complexity when $k$ nearest neighbors are considered, which is a significant improvement over the $\mathcal{O}(n^2)$ time and space complexity of traditional methods.
  Experiments across diverse datasets demonstrate that FAMST achieves remarkably low approximation errors while providing speedups of up to 1000$\times$ compared to exact MST algorithms. We analyze how the key hyperparameters, $k$ (neighborhood size) and $\lambda$ (inter-component edges), affect performance, providing practical guidelines for hyperparameter selection. FAMST enables MST-based analysis on datasets with millions of points and thousands of dimensions, extending the applicability of MST techniques to problem scales previously considered infeasible.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts</title>
<link>https://arxiv.org/abs/2507.14263</link>
<guid>https://arxiv.org/abs/2507.14263</guid>
<content:encoded><![CDATA[
arXiv:2507.14263v1 Announce Type: cross 
Abstract: The Internet is poised to host billions to trillions of autonomous AI agents that negotiate, delegate, and migrate in milliseconds and workloads that will strain DNS-centred identity and discovery. In this paper, we describe the NANDA index architecture, which we envision as a means for discoverability, identifiability and authentication in the internet of AI agents. We present an architecture where a minimal lean index resolves to dynamic, cryptographically verifiable AgentFacts that supports multi-endpoint routing, load balancing, privacy-preserving access, and credentialed capability assertions. Our architecture design delivers five concrete guarantees: (1) A quilt-like index proposal that supports both NANDA-native agents as well as third party agents being discoverable via the index, (2) rapid global resolution for newly spawned AI agents, (3) sub-second revocation and key rotation, (4) schema-validated capability assertions, and (5) privacy-preserving discovery across organisational boundaries via verifiable, least-disclosure queries. We formalize the AgentFacts schema, specify a CRDT-based update protocol, and prototype adaptive resolvers. The result is a lightweight, horizontally scalable foundation that unlocks secure, trust-aware collaboration for the next generation of the Internet of AI agents, without abandoning existing web infrastructure.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging MOOCs, Smart Teaching, and AI: A Decade of Evolution Toward a Unified Pedagogy</title>
<link>https://arxiv.org/abs/2507.14266</link>
<guid>https://arxiv.org/abs/2507.14266</guid>
<content:encoded><![CDATA[
arXiv:2507.14266v1 Announce Type: cross 
Abstract: Over the past decade, higher education has evolved through three distinct paradigms: the emergence of Massive Open Online Courses (MOOCs), the integration of Smart Teaching technologies into classrooms, and the rise of AI-enhanced learning. Each paradigm is intended to address specific challenges in traditional education: MOOCs enable ubiquitous access to learning resources; Smart Teaching supports real-time interaction with data-driven insights; and generative AI offers personalized feedback and on-demand content generation. However, these paradigms are often implemented in isolation due to their disparate technological origins and policy-driven adoption. This paper examines the origins, strengths, and limitations of each paradigm, and advocates a unified pedagogical perspective that synthesizes their complementary affordances. We propose a three-layer instructional framework that combines the scalability of MOOCs, the responsiveness of Smart Teaching, and the adaptivity of AI. To demonstrate its feasibility, we present a curriculum design for a project-based course. The findings highlight the framework's potential to enhance learner engagement, support instructors, and enable personalized yet scalable learning.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation</title>
<link>https://arxiv.org/abs/2507.14270</link>
<guid>https://arxiv.org/abs/2507.14270</guid>
<content:encoded><![CDATA[
arXiv:2507.14270v1 Announce Type: cross 
Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that integrates non-linear activation and linear transformation into a single trainable expression. The APTx Neuron is derived from the APTx activation function, thereby eliminating the need for separate activation layers and making the architecture both computationally efficient and elegant. The proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i + \tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters $\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69\% test accuracy in just 20 epochs using approximately 332K trainable parameters. The results highlight the superior expressiveness and computational efficiency of the APTx Neuron compared to traditional neurons, pointing toward a new paradigm in unified neuron design and the architectures built upon it.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiDeSeC: A Dataset for Mitosis Detection and Segmentation in Breast Cancer Histopathology Images</title>
<link>https://arxiv.org/abs/2507.14271</link>
<guid>https://arxiv.org/abs/2507.14271</guid>
<content:encoded><![CDATA[
arXiv:2507.14271v1 Announce Type: cross 
Abstract: The MiDeSeC dataset is created through H&amp;E stained invasive breast carcinoma, no special type (NST) slides of 25 different patients captured at 40x magnification from the Department of Medical Pathology at Ankara University. The slides have been scanned by 3D Histech Panoramic p250 Flash-3 scanner and Olympus BX50 microscope. As several possible mitosis shapes exist, it is crucial to have a large dataset to cover all the cases. Accordingly, a total of 50 regions is selected from glass slides for 25 patients, each of regions with a size of 1024*1024 pixels. There are more than 500 mitoses in total in these 50 regions. Two-thirds of the regions are reserved for training, the other third for testing.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NuSeC: A Dataset for Nuclei Segmentation in Breast Cancer Histopathology Images</title>
<link>https://arxiv.org/abs/2507.14272</link>
<guid>https://arxiv.org/abs/2507.14272</guid>
<content:encoded><![CDATA[
arXiv:2507.14272v1 Announce Type: cross 
Abstract: The NuSeC dataset is created by selecting 4 images with the size of 1024*1024 pixels from the slides of each patient among 25 patients. Therefore, there are a total of 100 images in the NuSeC dataset. To carry out a consistent comparative analysis between the methods that will be developed using the NuSeC dataset by the researchers in the future, we divide the NuSeC dataset 75% as the training set and 25% as the testing set. In detail, an image is randomly selected from 4 images of each patient among 25 patients to build the testing set, and then the remaining images are reserved for the training set. While the training set includes 75 images with around 30000 nuclei structures, the testing set includes 25 images with around 6000 nuclei structures.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning</title>
<link>https://arxiv.org/abs/2507.14295</link>
<guid>https://arxiv.org/abs/2507.14295</guid>
<content:encoded><![CDATA[
arXiv:2507.14295v1 Announce Type: cross 
Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. We ask: can LRMs learn to reflect their answers in a multi-turn context? In this work, we find that training models with multi-turn RL using only unary feedback (e.g., "Let's try again") after wrong answers can improve both single-turn performance and multi-turn reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, we design reward structures that guide models to produce careful and deliberate answers in each turn. Code: https://github.com/lichengliu03/unary-feedback
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding</title>
<link>https://arxiv.org/abs/2507.14298</link>
<guid>https://arxiv.org/abs/2507.14298</guid>
<content:encoded><![CDATA[
arXiv:2507.14298v1 Announce Type: cross 
Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for domain-specific tasks have shown promising results in scientific chart comprehension. However, existing approaches face two major limitations: First, they rely on paired data from only a few chart types, limiting generalization to wide range of chart types. Secondly, they lack targeted pre-training for chart-data alignment, which hampers the model's understanding of underlying data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth chart comprehension across diverse chart types. We propose an efficient data generation pipeline that synthesizes paired data for a wide range of chart types, along with a novel Dual-Path training strategy that enabling the model to succinctly capture essential data details while preserving robust reasoning capabilities by incorporating reasoning over the underlying data. Lastly, we establish ChartDQA, a new benchmark for evaluating not only question-answering at different levels but also underlying data understanding. Experimental results demonstrate that ChartScope significantly enhances comprehension on a wide range of chart types. The code and data are available at https://davidhalladay.github.io/chartscope_demo.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Age of Information Minimization in UAV-Enabled Integrated Sensing and Communication Systems</title>
<link>https://arxiv.org/abs/2507.14299</link>
<guid>https://arxiv.org/abs/2507.14299</guid>
<content:encoded><![CDATA[
arXiv:2507.14299v1 Announce Type: cross 
Abstract: Unmanned aerial vehicles (UAVs) equipped with integrated sensing and communication (ISAC) capabilities are envisioned to play a pivotal role in future wireless networks due to their enhanced flexibility and efficiency. However, jointly optimizing UAV trajectory planning, multi-user communication, and target sensing under stringent resource constraints and time-critical conditions remains a significant challenge. To address this, we propose an Age of Information (AoI)-centric UAV-ISAC system that simultaneously performs target sensing and serves multiple ground users, emphasizing information freshness as the core performance metric. We formulate a long-term average AoI minimization problem that jointly optimizes the UAV's flight trajectory and beamforming. To tackle the high-dimensional, non-convexity of this problem, we develop a deep reinforcement learning (DRL)-based algorithm capable of providing real-time decisions on UAV movement and beamforming for both radar sensing and multi-user communication. Specifically, a Kalman filter is employed for accurate target state prediction, regularized zero-forcing is utilized to mitigate inter-user interference, and the Soft Actor-Critic algorithm is applied for training the DRL agent on continuous actions. The proposed framework adaptively balances the trade-offs between sensing accuracy and communication quality. Extensive simulation results demonstrate that our proposed method consistently achieves lower average AoI compared to baseline approaches.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fiduciary AI for the Future of Brain-Technology Interactions</title>
<link>https://arxiv.org/abs/2507.14339</link>
<guid>https://arxiv.org/abs/2507.14339</guid>
<content:encoded><![CDATA[
arXiv:2507.14339v1 Announce Type: cross 
Abstract: Brain foundation models represent a new frontier in AI: instead of processing text or images, these models interpret real-time neural signals from EEG, fMRI, and other neurotechnologies. When integrated with brain-computer interfaces (BCIs), they may enable transformative applications-from thought controlled devices to neuroprosthetics-by interpreting and acting on brain activity in milliseconds. However, these same systems pose unprecedented risks, including the exploitation of subconscious neural signals and the erosion of cognitive liberty. Users cannot easily observe or control how their brain signals are interpreted, creating power asymmetries that are vulnerable to manipulation. This paper proposes embedding fiduciary duties-loyalty, care, and confidentiality-directly into BCI-integrated brain foundation models through technical design. Drawing on legal traditions and recent advancements in AI alignment techniques, we outline implementable architectural and governance mechanisms to ensure these systems act in users' best interests. Placing brain foundation models on a fiduciary footing is essential to realizing their potential without compromising self-determination.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Functions for Preference Dataset Pruning</title>
<link>https://arxiv.org/abs/2507.14344</link>
<guid>https://arxiv.org/abs/2507.14344</guid>
<content:encoded><![CDATA[
arXiv:2507.14344v1 Announce Type: cross 
Abstract: Language models are commonly fine-tuned via reinforcement learning to alter their behavior or elicit new capabilities. Datasets used for these purposes, and particularly human preference datasets, are often noisy. The relatively small size post-training datasets, combined with parameter-efficient fine-tuning methods, enable the use of influence functions approximations to detect and prune training examples that are harmful to performance on a validation set. In this work, we adapt the TL;DR dataset for reward model training to demonstrate how conjugate-gradient approximated influence functions can be used to filter datasets. In our experiments, influence function filtering yields a small retraining accuracy uplift of 1.5% after removing 10% of training examples. We also show that gradient similarity outperforms influence functions for detecting helpful training examples. This suggests that local curvature is important for detecting harmful training examples, but less so for identifying helpful examples.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reproducibility Study of Product-side Fairness in Bundle Recommendation</title>
<link>https://arxiv.org/abs/2507.14352</link>
<guid>https://arxiv.org/abs/2507.14352</guid>
<content:encoded><![CDATA[
arXiv:2507.14352v1 Announce Type: cross 
Abstract: Recommender systems are known to exhibit fairness issues, particularly on the product side, where products and their associated suppliers receive unequal exposure in recommended results. While this problem has been widely studied in traditional recommendation settings, its implications for bundle recommendation (BR) remain largely unexplored. This emerging task introduces additional complexity: recommendations are generated at the bundle level, yet user satisfaction and product (or supplier) exposure depend on both the bundle and the individual items it contains. Existing fairness frameworks and metrics designed for traditional recommender systems may not directly translate to this multi-layered setting. In this paper, we conduct a comprehensive reproducibility study of product-side fairness in BR across three real-world datasets using four state-of-the-art BR methods. We analyze exposure disparities at both the bundle and item levels using multiple fairness metrics, uncovering important patterns. Our results show that exposure patterns differ notably between bundles and items, revealing the need for fairness interventions that go beyond bundle-level assumptions. We also find that fairness assessments vary considerably depending on the metric used, reinforcing the need for multi-faceted evaluation. Furthermore, user behavior plays a critical role: when users interact more frequently with bundles than with individual items, BR systems tend to yield fairer exposure distributions across both levels. Overall, our findings offer actionable insights for building fairer bundle recommender systems and establish a vital foundation for future research in this emerging domain.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers</title>
<link>https://arxiv.org/abs/2507.14353</link>
<guid>https://arxiv.org/abs/2507.14353</guid>
<content:encoded><![CDATA[
arXiv:2507.14353v1 Announce Type: cross 
Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach for adapting a Large Language Model (LLM) for newer tasks. One of the most prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on adjusting the attention weight matrices within individual decoder blocks of a Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo Connection a novel method that adapts the representation at the decoder-block level rather than modifying individual weight matrices. Not only does Solo Connection outperform LoRA on E2E natural language generation benchmarks, but it also reduces the number of trainable parameters by 59% relative to LoRA and by more than 99% compared to full fine-tuning of GPT2, an early version of Large Language Models (LLMs). Solo Connection is also motivated by homotopy theory: we introduce a trainable linear transformation that gradually interpolates between a zero vector and the task-specific representation, enabling smooth and stable adaptation over time. While skip connections in the original 12 layer GPT2 are typically confined to individual decoder blocks, subsequent GPT2 variants scale up to 48 layers, and even larger language models can include 128 or more decoder blocks. These expanded architectures underscore the need to revisit how skip connections are employed during fine-tuning. This paper focuses on long skip connections that link outputs of different decoder blocks, potentially enhancing the model's ability to adapt to new tasks while leveraging pre-trained knowledge.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-SQL for Enterprise Data Analytics</title>
<link>https://arxiv.org/abs/2507.14372</link>
<guid>https://arxiv.org/abs/2507.14372</guid>
<content:encoded><![CDATA[
arXiv:2507.14372v1 Announce Type: cross 
Abstract: The introduction of large language models has brought rapid progress on Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise solution. In this paper, we present insights from building an internal chatbot that enables LinkedIn's product managers, engineers, and operations teams to self-serve data insights from a large, dynamic data lake. Our approach features three components. First, we construct a knowledge graph that captures up-to-date semantics by indexing database metadata, historical query logs, wikis, and code. We apply clustering to identify relevant tables for each team or product area. Second, we build a Text-to-SQL agent that retrieves and ranks context from the knowledge graph, writes a query, and automatically corrects hallucinations and syntax errors. Third, we build an interactive chatbot that supports various user intents, from data discovery to query writing to debugging, and displays responses in rich UI elements to encourage follow-up chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of its responses are correct or close to correct on an internal benchmark set. Through ablation studies, we identify the most important knowledge graph and modeling components, offering a practical path for developing enterprise Text-to-SQL solutions.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms</title>
<link>https://arxiv.org/abs/2507.14376</link>
<guid>https://arxiv.org/abs/2507.14376</guid>
<content:encoded><![CDATA[
arXiv:2507.14376v1 Announce Type: cross 
Abstract: Schema matching is essential for integrating heterogeneous data sources and enhancing dataset discovery, yet it remains a complex and resource-intensive problem. We introduce SCHEMORA, a schema matching framework that combines large language models with hybrid retrieval techniques in a prompt-based approach, enabling efficient identification of candidate matches without relying on labeled training data or exhaustive pairwise comparisons. By enriching schema metadata and leveraging both vector-based and lexical retrieval, SCHEMORA improves matching accuracy and scalability. Evaluated on the MIMIC-OMOP benchmark, it establishes new state-of-the-art performance, with gains of 7.49% in HitRate@5 and 3.75% in HitRate@3 over previous best results. To our knowledge, this is the first LLM-based schema matching method with an open-source implementation, accompanied by analysis that underscores the critical role of retrieval and provides practical guidance on model selection.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures</title>
<link>https://arxiv.org/abs/2507.14387</link>
<guid>https://arxiv.org/abs/2507.14387</guid>
<content:encoded><![CDATA[
arXiv:2507.14387v1 Announce Type: cross 
Abstract: The escalating threat of cyberattacks on real-time critical infrastructures poses serious risks to public safety, demanding detection methods that effectively capture complex system interdependencies and adapt to evolving attack patterns. Traditional real-time anomaly detection techniques often suffer from excessive false positives due to their statistical sensitivity to high data variance and class imbalance. To address these limitations, recent research has explored modeling causal relationships among system components. However, prior work mainly focuses on offline causal graph-based approaches that require static historical data and fail to generalize to real-time settings. These methods are fundamentally constrained by: (1) their inability to adapt to dynamic shifts in data distribution without retraining, and (2) the risk of catastrophic forgetting when lacking timely supervision in live systems. To overcome these challenges, we propose INCADET, a novel framework for incremental causal graph learning tailored to real-time cyberattack detection. INCADET dynamically captures evolving system behavior by incrementally updating causal graphs across streaming time windows. The framework comprises three modules: 1) Early Symptom Detection: Detects transitions in system status using divergence in edge-weight distributions across sequential causal graphs. 2) Incremental Causal Graph Learning: Leverages experience replay and edge reinforcement to continually refine causal structures while preserving prior knowledge. 3) Causal Graph Classification: Employs Graph Convolutional Networks (GCNs) to classify system status using the learned causal graphs. Extensive experiments on real-world critical infrastructure datasets demonstrate that INCADET achieves superior accuracy, robustness, and adaptability compared to both static causal and deep temporal baselines in evolving attack scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students</title>
<link>https://arxiv.org/abs/2507.14418</link>
<guid>https://arxiv.org/abs/2507.14418</guid>
<content:encoded><![CDATA[
arXiv:2507.14418v1 Announce Type: cross 
Abstract: One challenge in technical interviews is the think-aloud process, where candidates verbalize their thought processes while solving coding tasks. Despite its importance, opportunities for structured practice remain limited. Conversational AI offers potential assistance, but limited research explores user perceptions of its role in think-aloud practice. To address this gap, we conducted a study with 17 participants using an LLM-based technical interview practice tool. Participants valued AI's role in simulation, feedback, and learning from generated examples. Key design recommendations include promoting social presence in conversational AI for technical interview simulation, providing feedback beyond verbal content analysis, and enabling crowdsourced think-aloud examples through human-AI collaboration. Beyond feature design, we examined broader considerations, including intersectional challenges and potential strategies to address them, how AI-driven interview preparation could promote equitable learning in computing careers, and the need to rethink AI's role in interview practice by suggesting a research direction that integrates human-AI collaboration.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's Not That Simple. An Analysis of Simple Test-Time Scaling</title>
<link>https://arxiv.org/abs/2507.14419</link>
<guid>https://arxiv.org/abs/2507.14419</guid>
<content:encoded><![CDATA[
arXiv:2507.14419v1 Announce Type: cross 
Abstract: Prior work proposed simple test-time scaling, a method for replicating this scaling behavior with models distilled from o1-like models by manually controlling test-time compute: either scaling down by enforcing a maximum length or scaling up by iteratively appending "Wait" when the model is about to terminate its generation. This paper presents an analysis of simple test-time scaling and finds that the scaling behavior is largely attributed to scaling down by enforcing a maximum length. In contrast, fine-tuning on long CoT data distilled from o1-like models has no significant impact on scaling behavior, and scaling up by appending "Wait" leads to inconsistencies, as the model may oscillate between solutions. A key distinction exists between scaling down by enforcing a maximum length and scaling up test-time compute in o1-like models, such as DeepSeek-R1\@. These models are typically allowed to utilize as much compute as needed, with the only constraint being the model's maximum supported length. By learning to naturally scale up test-time compute during reinforcement learning, o1-like models surpass their peak performance when scaling up. In contrast, simple test-time scaling progressively imposes a lower upper limit on model performance as it scales down. While replicating the test-time scaling behavior of o1 models can be straightforward by scaling down, it is crucial to recognize that the goal of scaling test-time compute is to unlock higher performance -- beyond what the model could originally achieve -- rather than merely reproducing the appearance of scaling behavior.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical and Algorithmic Foundations of Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.14444</link>
<guid>https://arxiv.org/abs/2507.14444</guid>
<content:encoded><![CDATA[
arXiv:2507.14444v1 Announce Type: cross 
Abstract: As a paradigm for sequential decision making in unknown environments, reinforcement learning (RL) has received a flurry of attention in recent years. However, the explosion of model complexity in emerging applications and the presence of nonconvexity exacerbate the challenge of achieving efficient RL in sample-starved situations, where data collection is expensive, time-consuming, or even high-stakes (e.g., in clinical trials, autonomous systems, and online advertising). How to understand and enhance the sample and computational efficacies of RL algorithms is thus of great interest. In this tutorial, we aim to introduce several important algorithmic and theoretical developments in RL, highlighting the connections between new ideas and classical topics. Employing Markov Decision Processes as the central mathematical model, we cover several distinctive RL scenarios (i.e., RL with a simulator, online RL, offline RL, robust RL, and RL with human feedback), and present several mainstream RL approaches (i.e., model-based approach, value-based approach, and policy optimization). Our discussions gravitate around the issues of sample complexity, computational efficiency, as well as algorithm-dependent and information-theoretic lower bounds from a non-asymptotic viewpoint.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximate Revenue Maximization for Diffusion Auctions</title>
<link>https://arxiv.org/abs/2507.14470</link>
<guid>https://arxiv.org/abs/2507.14470</guid>
<content:encoded><![CDATA[
arXiv:2507.14470v1 Announce Type: cross 
Abstract: Reserve prices are widely used in practice. The problem of designing revenue-optimal auctions based on reserve price has drawn much attention in the auction design community. Although they have been extensively studied, most developments rely on the significant assumption that the target audience of the sale is directly reachable by the auctioneer, while a large portion of bidders in the economic network unaware of the sale are omitted. This work follows the diffusion auction design, which aims to extend the target audience of optimal auction theory to all entities in economic networks. We investigate the design of simple and provably near-optimal network auctions via reserve price. Using Bayesian approximation analysis, we provide a simple and explicit form of the reserve price function tailored to the most representative network auction. We aim to balance setting a sufficiently high reserve price to induce high revenue in a successful sale, and attracting more buyers from the network to increase the probability of a successful sale. This reserve price function preserves incentive compatibility for network auctions, allowing the seller to extract additional revenue beyond that achieved by the Myerson optimal auction. Specifically, if the seller has $\rho$ direct neighbours in a network of size $n$, this reserve price guarantees a $1-{1 \over \rho}$ approximation to the theoretical upper bound, i.e., the maximum possible revenue from any network of size $n$. This result holds for any size and any structure of the networked market.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategyproofness and Monotone Allocation of Auction in Social Networks</title>
<link>https://arxiv.org/abs/2507.14472</link>
<guid>https://arxiv.org/abs/2507.14472</guid>
<content:encoded><![CDATA[
arXiv:2507.14472v1 Announce Type: cross 
Abstract: Strategyproofness in network auctions requires that bidders not only report their valuations truthfully, but also do their best to invite neighbours from the social network. In contrast to canonical auctions, where the value-monotone allocation in Myerson's Lemma is a cornerstone, a general principle of allocation rules for strategyproof network auctions is still missing. We show that, due to the absence of such a principle, even extensions to multi-unit network auctions with single-unit demand present unexpected difficulties, and all pioneering researches fail to be strategyproof. For the first time in this field, we identify two categories of monotone allocation rules on networks: Invitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity (IP-MON). They encompass all existing allocation rules of network auctions as specific instances. For any given ID-MON or IP-MON allocation rule, we characterize the existence and sufficient conditions for the strategyproof payment rules, and show that among all such payment rules, the revenue-maximizing one exists and is computationally feasible. With these results, the obstacle of combinatorial network auction with single-minded bidders is now resolved.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning</title>
<link>https://arxiv.org/abs/2507.14481</link>
<guid>https://arxiv.org/abs/2507.14481</guid>
<content:encoded><![CDATA[
arXiv:2507.14481v1 Announce Type: cross 
Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers (ViTs) without requiring access to data, allowing for the deployment of ViTs on devices with limited resources. In DFQ, the quantization model must be calibrated using synthetic samples, making the quality of these synthetic samples crucial. Existing methods fail to fully capture and balance the global and local features within the samples, resulting in limited synthetic data quality. Moreover, we have found that during inference, there is a significant difference in the distributions of intermediate layer activations between the quantized and full-precision models. These issues lead to a severe performance degradation of the quantized model. To address these problems, we propose a pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT). Specifically, we synthesize samples in order of increasing difficulty, effectively enhancing the quality of synthetic data. During the calibration and inference stage, we introduce the activation correction matrix for the quantized model to align the intermediate layer activations with those of the full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves remarkable superiority over existing DFQ methods and its performance is on par with models quantized through real data. For example, the performance of DeiT-T with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our method eliminates the need for fine-tuning, which not only reduces computational overhead but also lowers the deployment barriers for edge devices. This characteristic aligns with the principles of Green Learning by improving energy efficiency and facilitating real-world applications in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion</title>
<link>https://arxiv.org/abs/2507.14485</link>
<guid>https://arxiv.org/abs/2507.14485</guid>
<content:encoded><![CDATA[
arXiv:2507.14485v1 Announce Type: cross 
Abstract: Completing the whole 3D structure based on an incomplete point cloud is a challenging task, particularly when the residual point cloud lacks typical structural characteristics. Recent methods based on cross-modal learning attempt to introduce instance images to aid the structure feature learning. However, they still focus on each particular input class, limiting their generation abilities. In this work, we propose a novel retrieval-augmented point cloud completion framework. The core idea is to incorporate cross-modal retrieval into completion task to learn structural prior information from similar reference samples. Specifically, we design a Structural Shared Feature Encoder (SSFE) to jointly extract cross-modal features and reconstruct reference features as priors. Benefiting from a dual-channel control gate in the encoder, relevant structural features in the reference sample are enhanced and irrelevant information interference is suppressed. In addition, we propose a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical feature fusion mechanism to integrate reference prior information with input features from global to local. Through extensive evaluations on multiple datasets and real-world scenes, our method shows its effectiveness in generating fine-grained point clouds, as well as its generalization capability in handling sparse data and unseen categories.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Brownian Motion</title>
<link>https://arxiv.org/abs/2507.14499</link>
<guid>https://arxiv.org/abs/2507.14499</guid>
<content:encoded><![CDATA[
arXiv:2507.14499v1 Announce Type: cross 
Abstract: This paper introduces the Neural-Brownian Motion (NBM), a new class of stochastic processes for modeling dynamics under learned uncertainty. The NBM is defined axiomatically by replacing the classical martingale property with respect to linear expectation with one relative to a non-linear Neural Expectation Operator, $\varepsilon^\theta$, generated by a Backward Stochastic Differential Equation (BSDE) whose driver $f_\theta$ is parameterized by a neural network. Our main result is a representation theorem for a canonical NBM, which we define as a continuous $\varepsilon^\theta$-martingale with zero drift under the physical measure. We prove that, under a key structural assumption on the driver, such a canonical NBM exists and is the unique strong solution to a stochastic differential equation of the form ${\rm d} M_t = \nu_\theta(t, M_t) {\rm d} W_t$. Crucially, the volatility function $\nu_\theta$ is not postulated a priori but is implicitly defined by the algebraic constraint $g_\theta(t, M_t, \nu_\theta(t, M_t)) = 0$, where $g_\theta$ is a specialization of the BSDE driver. We develop the stochastic calculus for this process and prove a Girsanov-type theorem for the quadratic case, showing that an NBM acquires a drift under a new, learned measure. The character of this measure, whether pessimistic or optimistic, is endogenously determined by the learned parameters $\theta$, providing a rigorous foundation for models where the attitude towards uncertainty is a discoverable feature.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Time Series Forecasting: A Survey</title>
<link>https://arxiv.org/abs/2507.14507</link>
<guid>https://arxiv.org/abs/2507.14507</guid>
<content:encoded><![CDATA[
arXiv:2507.14507v1 Announce Type: cross 
Abstract: Diffusion models, initially developed for image synthesis, demonstrate remarkable generative capabilities. Recently, their application has expanded to time series forecasting (TSF), yielding promising results. In this survey, we firstly introduce the standard diffusion models and their prevalent variants, explaining their adaptation to TSF tasks. We then provide a comprehensive review of diffusion models for TSF, paying special attention to the sources of conditional information and the mechanisms for integrating this conditioning within the models. In analyzing existing approaches using diffusion models for TSF, we provide a systematic categorization and a comprehensive summary of them in this survey. Furthermore, we examine several foundational diffusion models applied to TSF, alongside commonly used datasets and evaluation metrics. Finally, we discuss current limitations in these approaches and potential future research directions. Overall, this survey details recent progress and future prospects for diffusion models in TSF, serving as a reference for researchers in the field.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning</title>
<link>https://arxiv.org/abs/2507.14516</link>
<guid>https://arxiv.org/abs/2507.14516</guid>
<content:encoded><![CDATA[
arXiv:2507.14516v1 Announce Type: cross 
Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware metric function for time series self-supervised representation learning. Most Self-Supervised Learning (SSL) methods for signals commonly adopt distance-based objectives such as mean squared error (MSE), which are sensitive to amplitude, invariant to waveform polarity, and unbounded in scale. These properties hinder semantic alignment and reduce interpretability. SDSC addresses this by quantifying structural agreement between temporal signals based on the intersection of signed amplitudes, derived from the Dice Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware metric, it can be used as a loss by subtracting from 1 and applying a differentiable approximation of the Heaviside function for gradient-based optimization. A hybrid loss formulation is also proposed to combine SDSC with MSE, improving stability and preserving amplitude where necessary. Experiments on forecasting and classification benchmarks demonstrate that SDSC-based pre-training achieves comparable or improved performance over MSE, particularly in in-domain and low-resource scenarios. The results suggest that structural fidelity in signal representations enhances the semantic representation quality, supporting the consideration of structure-aware metrics as viable alternatives to conventional distance-based methods.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives</title>
<link>https://arxiv.org/abs/2507.14519</link>
<guid>https://arxiv.org/abs/2507.14519</guid>
<content:encoded><![CDATA[
arXiv:2507.14519v1 Announce Type: cross 
Abstract: Privacy-preserving machine learning (PPML) based on cryptographic protocols has emerged as a promising paradigm to protect user data privacy in cloud-based machine learning services. While it achieves formal privacy protection, PPML often incurs significant efficiency and scalability costs due to orders of magnitude overhead compared to the plaintext counterpart. Therefore, there has been a considerable focus on mitigating the efficiency gap for PPML. In this survey, we provide a comprehensive and systematic review of recent PPML studies with a focus on cross-level optimizations. Specifically, we categorize existing papers into protocol level, model level, and system level, and review progress at each level. We also provide qualitative and quantitative comparisons of existing works with technical insights, based on which we discuss future research directions and highlight the necessity of integrating optimizations across protocol, model, and system levels. We hope this survey can provide an overarching understanding of existing approaches and potentially inspire future breakthroughs in the PPML field. As the field is evolving fast, we also provide a public GitHub repository to continuously track the developments, which is available at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025</title>
<link>https://arxiv.org/abs/2507.14544</link>
<guid>https://arxiv.org/abs/2507.14544</guid>
<content:encoded><![CDATA[
arXiv:2507.14544v1 Announce Type: cross 
Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA 2025 Challenge, which targets visual question answering (VQA) for gastrointestinal endoscopy. We adopt the Florence model-a large-scale multimodal foundation model-as the backbone of our VQA pipeline, pairing a powerful vision encoder with a text encoder to interpret endoscopic images and produce clinically relevant answers. To improve generalization, we apply domain-specific augmentations that preserve medical features while increasing training diversity. Experiments on the KASVIR dataset show that fine-tuning Florence yields accurate responses on the official challenge metrics. Our results highlight the potential of large multimodal models in medical VQA and provide a strong baseline for future work on explainability, robustness, and clinical integration. The code is publicly available at: https://github.com/TiwariLaxuu/VQA-Florence.git
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges</title>
<link>https://arxiv.org/abs/2507.14570</link>
<guid>https://arxiv.org/abs/2507.14570</guid>
<content:encoded><![CDATA[
arXiv:2507.14570v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph mining tasks, yet existing scalable solutions often struggle to balance execution efficiency with prediction accuracy. These difficulties stem from iterative message-passing techniques, which place significant computational demands and require extensive GPU memory, particularly when dealing with the neighbor explosion issue inherent in large-scale graphs. This paper introduces a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN, which can perform representation learning on 100 billion graphs with a single GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We examine existing graph partitioning methods and design a superior graph partition algorithm named LPMetis. In particular, LPMetis outperforms current state-of-the-art (SOTA) approaches on various evaluation metrics. In addition, our paper proposes a subgraph augmentation strategy to enhance the model's predictive performance. It exhibits excellent compatibility, allowing the entire framework to accommodate various GNN algorithms. Successfully deployed on the Tencent platform, LPS-GNN has been tested on public and real-world datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in online applications.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation</title>
<link>https://arxiv.org/abs/2507.14575</link>
<guid>https://arxiv.org/abs/2507.14575</guid>
<content:encoded><![CDATA[
arXiv:2507.14575v1 Announce Type: cross 
Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering distinct diagnostic insights. However, acquiring all desired modalities increases scan time and cost, motivating research into computational methods for cross-modal synthesis. To address this, recent approaches aim to synthesize missing MRI contrasts from those already acquired, reducing acquisition time while preserving diagnostic quality. Image-to-image (I2I) translation provides a promising framework for this task. In this paper, we present a comprehensive benchmark of generative models$\unicode{x2013}$specifically, Generative Adversarial Networks (GANs), diffusion models, and flow matching (FM) techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All frameworks are implemented with comparable settings and evaluated on three publicly available MRI datasets of healthy adults. Our quantitative and qualitative analyses show that the GAN-based Pix2Pix model outperforms diffusion and FM-based methods in terms of structural fidelity, image quality, and computational efficiency. Consistent with existing literature, these results suggest that flow-based models are prone to overfitting on small datasets and simpler tasks, and may require more data to match or surpass GAN performance. These findings offer practical guidance for deploying I2I translation techniques in real-world MRI workflows and highlight promising directions for future research in cross-modal medical image synthesis. Code and models are publicly available at https://github.com/AndreaMoschetto/medical-I2I-benchmark.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models</title>
<link>https://arxiv.org/abs/2507.14579</link>
<guid>https://arxiv.org/abs/2507.14579</guid>
<content:encoded><![CDATA[
arXiv:2507.14579v1 Announce Type: cross 
Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using machine learning techniques is a significant challenge for the field of AI in Education. Recent studies have explored the use of Bidirectional Encoder Representations from Transformers (BERT) models on transcription data to reliably detect meaningful CPS indicators. A notable advancement involved the multimodal BERT variant, AudiBERT, which integrates speech and acoustic-prosodic audio features to enhance CPS diagnosis. Although initial results demonstrated multimodal improvements, the statistical significance of these enhancements remained unclear, and there was insufficient guidance on leveraging human-AI complementarity for CPS diagnosis tasks. This workshop paper extends the previous research by highlighting that the AudiBERT model not only improved the classification of classes that were sparse in the dataset, but it also had statistically significant class-wise improvements over the BERT model for classifications in the social-cognitive dimension. However, similar significant class-wise improvements over the BERT model were not observed for classifications in the affective dimension. A correlation analysis highlighted that larger training data was significantly associated with higher recall performance for both the AudiBERT and BERT models. Additionally, the precision of the BERT model was significantly associated with high inter-rater agreement among human coders. When employing the BERT model to diagnose indicators within these subskills that were well-detected by the AudiBERT model, the performance across all indicators was inconsistent. We conclude the paper by outlining a structured approach towards achieving human-AI complementarity for CPS diagnosis, highlighting the crucial inclusion of model explainability to support human agency and engagement in the reflective coding process.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption</title>
<link>https://arxiv.org/abs/2507.14584</link>
<guid>https://arxiv.org/abs/2507.14584</guid>
<content:encoded><![CDATA[
arXiv:2507.14584v1 Announce Type: cross 
Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT) model and its variants for classifying collaborative problem solving (CPS) has been extensively explored within the AI in Education community. However, limited attention has been given to understanding how individual tokenised words in the dataset contribute to the model's classification decisions. Enhancing the explainability of BERT-based CPS diagnostics is essential to better inform end users such as teachers, thereby fostering greater trust and facilitating wider adoption in education. This study undertook a preliminary step towards model transparency and explainability by using SHapley Additive exPlanations (SHAP) to examine how different tokenised words in transcription data contributed to a BERT model's classification of CPS processes. The findings suggested that well-performing classifications did not necessarily equate to a reasonable explanation for the classification decisions. Particular tokenised words were used frequently to affect classifications. The analysis also identified a spurious word, which contributed positively to the classification but was not semantically meaningful to the class. While such model transparency is unlikely to be useful to an end user to improve their practice, it can help them not to overrely on LLM diagnostics and ignore their human expertise. We conclude the workshop paper by noting that the extent to which the model appropriately uses the tokens for its classification is associated with the number of classes involved. It calls for an investigation into the exploration of ensemble model architectures and the involvement of human-AI complementarity for CPS diagnosis, since considerable human reasoning is still required for fine-grained discrimination of CPS subskills.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX</title>
<link>https://arxiv.org/abs/2507.14587</link>
<guid>https://arxiv.org/abs/2507.14587</guid>
<content:encoded><![CDATA[
arXiv:2507.14587v1 Announce Type: cross 
Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring. Specifically, blood microscopy offers valuable insights into blood cell morphology and the detection of hematological disorders. In recent years, deep learning-based automated classification systems have demonstrated high potential in enhancing the accuracy and efficiency of blood image analysis. However, a detailed performance analysis of specific deep learning frameworks appears to be lacking. This paper compares the performance of three popular deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in classifying blood cell images from the publicly available BloodMNIST dataset. The study primarily focuses on inference time differences, but also classification performance for different image sizes. The results reveal variations in performance across frameworks, influenced by factors such as image resolution and framework-specific optimizations. Classification accuracy for JAX and PyTorch was comparable to current benchmarks, showcasing the efficiency of these frameworks for medical image classification.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification</title>
<link>https://arxiv.org/abs/2507.14590</link>
<guid>https://arxiv.org/abs/2507.14590</guid>
<content:encoded><![CDATA[
arXiv:2507.14590v1 Announce Type: cross 
Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity and class imbalance. This paper systematically explores data augmentation methods for NLP, particularly through large language models like GPT. The purpose of this paper is to examine and evaluate whether traditional methods such as paraphrasing and backtranslation can leverage a new generation of models to achieve comparable performance to purely generative methods. Methods aimed at solving the problem of data scarcity and utilizing ChatGPT were chosen, as well as an exemplary dataset. We conducted a series of experiments comparing four different approaches to data augmentation in multiple experimental setups. We then evaluated the results both in terms of the quality of generated data and its impact on classification performance. The key findings indicate that backtranslation and paraphrasing can yield comparable or even better results than zero and a few-shot generation of examples.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification</title>
<link>https://arxiv.org/abs/2507.14592</link>
<guid>https://arxiv.org/abs/2507.14592</guid>
<content:encoded><![CDATA[
arXiv:2507.14592v1 Announce Type: cross 
Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance, logistics, agriculture, disaster management, and military operations. Accurate detection and classification of UAV flight states, such as hovering, cruising, ascending, or transitioning, which are essential for safe and effective operations. However, conventional time series classification (TSC) methods often lack robustness and generalization for dynamic UAV environments, while state of the art(SOTA) models like Transformers and LSTM based architectures typically require large datasets and entail high computational costs, especially with high-dimensional data streams. This paper proposes a novel framework that integrates a Transformer-based Generative Adversarial Network (GAN) with Multiple Instance Locally Explainable Learning (MILET) to address these challenges in UAV flight state classification. The Transformer encoder captures long-range temporal dependencies and complex telemetry dynamics, while the GAN module augments limited datasets with realistic synthetic samples. MIL is incorporated to focus attention on the most discriminative input segments, reducing noise and computational overhead. Experimental results show that the proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and 98.6% on the DroneRF dataset that outperforming other SOTA approaches. The framework also demonstrates strong computational efficiency and robust generalization across diverse UAV platforms and flight states, highlighting its potential for real-time deployment in resource constrained environments.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition</title>
<link>https://arxiv.org/abs/2507.14608</link>
<guid>https://arxiv.org/abs/2507.14608</guid>
<content:encoded><![CDATA[
arXiv:2507.14608v1 Announce Type: cross 
Abstract: Facial expression recognition is crucial for human-computer interaction applications such as face animation, video surveillance, affective computing, medical analysis, etc. Since the structure of facial attributes varies with facial expressions, incorporating structural information into facial attributes is essential for facial expression recognition. In this paper, we propose Exp-Graph, a novel framework designed to represent the structural relationships among facial attributes using graph-based modeling for facial expression recognition. For facial attributes graph representation, facial landmarks are used as the graph's vertices. At the same time, the edges are determined based on the proximity of the facial landmark and the similarity of the local appearance of the facial attributes encoded using the vision transformer. Additionally, graph convolutional networks are utilized to capture and integrate these structural dependencies into the encoding of facial attributes, thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph learns from the facial attribute graphs highly expressive semantic representations. On the other hand, the vision transformer and graph convolutional blocks help the framework exploit the local and global dependencies among the facial attributes that are essential for the recognition of facial expressions. We conducted comprehensive evaluations of the proposed Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW. The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%, respectively. These results indicate that Exp-Graph maintains strong generalization capabilities across both controlled laboratory settings and real-world, unconstrained environments, underscoring its effectiveness for practical facial expression recognition applications.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module</title>
<link>https://arxiv.org/abs/2507.14612</link>
<guid>https://arxiv.org/abs/2507.14612</guid>
<content:encoded><![CDATA[
arXiv:2507.14612v1 Announce Type: cross 
Abstract: Next point of interest (POI) recommendation primarily predicts future activities based on users' past check-in data and current status, providing significant value to users and service providers. We observed that the popular check-in times for different POI categories vary. For example, coffee shops are crowded in the afternoon because people like to have coffee to refresh after meals, while bars are busy late at night. However, existing methods rarely explore the relationship between POI categories and time, which may result in the model being unable to fully learn users' tendencies to visit certain POI categories at different times. Additionally, existing methods for modeling time information often convert it into time embeddings or calculate the time interval and incorporate it into the model, making it difficult to capture the continuity of time. Finally, during POI prediction, various weighting information is often ignored, such as the popularity of each POI, the transition relationships between POIs, and the distances between POIs, leading to suboptimal performance. To address these issues, this paper proposes a novel next POI recommendation framework called Graph Disentangler with POI Weighted Module (GDPW). This framework aims to jointly consider POI category information and multiple POI weighting factors. Specifically, the proposed GDPW learns category and time representations through the Global Category Graph and the Global Category-Time Graph. Then, we disentangle category and time information through contrastive learning. After prediction, the final POI recommendation for users is obtained by weighting the prediction results based on the transition weights and distance relationships between POIs. We conducted experiments on two real-world datasets, and the results demonstrate that the proposed GDPW outperforms other existing models, improving performance by 3% to 11%.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper</title>
<link>https://arxiv.org/abs/2507.14615</link>
<guid>https://arxiv.org/abs/2507.14615</guid>
<content:encoded><![CDATA[
arXiv:2507.14615v1 Announce Type: cross 
Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in low-resource settings, but their effectiveness in African primary care remains underexplored. We present a methodology for creating a benchmark dataset and evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our approach uses retrieval augmented generation (RAG) to ground clinical questions in Kenya's national guidelines, ensuring alignment with local standards. These guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic clinical scenarios, multiple-choice questions, and rationale based answers in English and Swahili. Kenyan physicians co-created and refined the dataset, and a blinded expert review process ensured clinical accuracy, clarity, and cultural appropriateness. The resulting Alama Health QA dataset includes thousands of regulator-aligned question answer pairs across common outpatient conditions. Beyond accuracy, we introduce evaluation metrics that test clinical reasoning, safety, and adaptability such as rare case detection (Needle in the Haystack), stepwise logic (Decision Points), and contextual adaptability. Initial results reveal significant performance gaps when LLMs are applied to localized scenarios, consistent with findings that LLM accuracy is lower on African medical content than on US-based benchmarks. This work offers a replicable model for guideline-driven, dynamic benchmarking to support safe AI deployment in African health systems.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning</title>
<link>https://arxiv.org/abs/2507.14625</link>
<guid>https://arxiv.org/abs/2507.14625</guid>
<content:encoded><![CDATA[
arXiv:2507.14625v1 Announce Type: cross 
Abstract: Vertical federated learning (VFL) enables multiple parties with disjoint features to collaboratively train models without sharing raw data. While privacy vulnerabilities of VFL are extensively-studied, its security threats-particularly targeted label attacks-remain underexplored. In such attacks, a passive party perturbs inputs at inference to force misclassification into adversary-chosen labels. Existing methods rely on unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore anomaly detectors deployed in real-world systems. To bridge this gap, we introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly designed to evade detector-enhanced VFL inference. During the preparation stage, the attacker selects a minimal set of high-expressiveness samples (via maximum mean discrepancy), submits them through VFL protocol to collect predicted labels, and uses these pseudo-labels to train estimated detector and surrogate model on local features. In attack stage, these models guide gradient-based perturbations of remaining samples, crafting adversarial instances that induce targeted misclassifications and evade detection. We implement VTarbel and evaluate it against four model architectures, seven multimodal datasets, and two anomaly detectors. Across all settings, VTarbel outperforms four state-of-the-art baselines, evades detection, and retains effective against three representative privacy-preserving defenses. These results reveal critical security blind spots in current VFL deployments and underscore urgent need for robust, attack-aware defenses.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking</title>
<link>https://arxiv.org/abs/2507.14629</link>
<guid>https://arxiv.org/abs/2507.14629</guid>
<content:encoded><![CDATA[
arXiv:2507.14629v1 Announce Type: cross 
Abstract: Though vertical federated learning (VFL) is generally considered to be privacy-preserving, recent studies have shown that VFL system is vulnerable to label inference attacks originating from various attack surfaces. Among these attacks, the model completion (MC) attack is currently the most powerful one. Existing defense methods against it either sacrifice model accuracy or incur impractical computational overhead. In this paper, we propose VMask, a novel label privacy protection framework designed to defend against MC attack from the perspective of layer masking. Our key insight is to disrupt the strong correlation between input data and intermediate outputs by applying the secret sharing (SS) technique to mask layer parameters in the attacker's model. We devise a strategy for selecting critical layers to mask, reducing the overhead that would arise from naively applying SS to the entire model. Moreover, VMask is the first framework to offer a tunable privacy budget to defenders, allowing for flexible control over the levels of label privacy according to actual requirements. We built a VFL system, implemented VMask on it, and extensively evaluated it using five model architectures and 13 datasets with different modalities, comparing it to 12 other defense methods. The results demonstrate that VMask achieves the best privacy-utility trade-off, successfully thwarting the MC attack (reducing the label inference accuracy to a random guessing level) while preserving model performance (e.g., in Transformer-based model, the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up to 60,846 times faster than cryptography-based methods, and it only marginally exceeds that of standard VFL by 1.8 times in a large Transformer-based model, which is generally acceptable.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs</title>
<link>https://arxiv.org/abs/2507.14649</link>
<guid>https://arxiv.org/abs/2507.14649</guid>
<content:encoded><![CDATA[
arXiv:2507.14649v1 Announce Type: cross 
Abstract: Despite the outstanding performance of large language models (LLMs) across various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate responses--remains as a critical problem as it can be directly connected to a crisis of building safe and reliable LLMs. Uncertainty estimation is primarily used to measure hallucination levels in LLM responses so that correct and incorrect answers can be distinguished clearly. This study proposes an effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse quantifies the uncertainty with the proportion of the intra-cluster consistency in the total consistency between LLM hidden embeddings which contain adequate semantic information of generations, by employing clustering. The effectiveness of Cleanse for detecting hallucination is validated using four off-the-shelf models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two question-answering benchmarks, SQuAD and CoQA.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)</title>
<link>https://arxiv.org/abs/2507.14657</link>
<guid>https://arxiv.org/abs/2507.14657</guid>
<content:encoded><![CDATA[
arXiv:2507.14657v1 Announce Type: cross 
Abstract: The integration of Artificial Intelligence (AI) into sports officiating represents a paradigm shift in how decisions are made in competitive environments. Traditional manual systems, even when supported by Instant Video Replay (IVR), often suffer from latency, subjectivity, and inconsistent enforcement, undermining fairness and athlete trust. This paper introduces FST.ai, a novel AI-powered framework designed to enhance officiating in Sport Taekwondo, particularly focusing on the complex task of real-time head kick detection and scoring. Leveraging computer vision, deep learning, and edge inference, the system automates the identification and classification of key actions, significantly reducing decision time from minutes to seconds while improving consistency and transparency. Importantly, the methodology is not limited to Taekwondo. The underlying framework -- based on pose estimation, motion classification, and impact analysis -- can be adapted to a wide range of sports requiring action detection, such as judo, karate, fencing, or even team sports like football and basketball, where foul recognition or performance tracking is critical. By addressing one of Taekwondo's most challenging scenarios -- head kick scoring -- we demonstrate the robustness, scalability, and sport-agnostic potential of FST.ai to transform officiating standards across multiple disciplines.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall</title>
<link>https://arxiv.org/abs/2507.14662</link>
<guid>https://arxiv.org/abs/2507.14662</guid>
<content:encoded><![CDATA[
arXiv:2507.14662v1 Announce Type: cross 
Abstract: Quantifying post-consumer food waste in institutional dining settings is essential for supporting data-driven sustainability strategies. This study presents a cost-effective computer vision framework that estimates plate-level food waste by utilizing semantic segmentation of RGB images taken before and after meal consumption across five Iranian dishes. Four fully supervised models (U-Net, U-Net++, and their lightweight variants) were trained using a capped dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a custom-defined Distributional Pixel Agreement (DPA) metric tailored to the task. All models achieved satisfying performance, and for each food type, at least one model approached or surpassed 90% DPA, demonstrating strong alignment in pixel-wise proportion estimates. Lighter models with reduced parameter counts offered faster inference, achieving real-time throughput on an NVIDIA T4 GPU. Further analysis showed superior segmentation performance for dry and more rigid components (e.g., rice and fries), while more complex, fragmented, or viscous dishes, such as stews, showed reduced performance, specifically post-consumption. Despite limitations such as reliance on 2D imaging, constrained food variety, and manual data collection, the proposed framework is pioneering and represents a scalable, contactless solution for continuous monitoring of food consumption. This research lays foundational groundwork for automated, real-time waste tracking systems in large-scale food service environments and offers actionable insights and outlines feasible future directions for dining hall management and policymakers aiming to reduce institutional food waste.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks</title>
<link>https://arxiv.org/abs/2507.14679</link>
<guid>https://arxiv.org/abs/2507.14679</guid>
<content:encoded><![CDATA[
arXiv:2507.14679v1 Announce Type: cross 
Abstract: The exponential growth of spam text on the Internet necessitates robust detection mechanisms to mitigate risks such as information leakage and social instability. This work addresses two principal challenges: adversarial strategies employed by spammers and the scarcity of labeled data. We propose a novel spam-text detection framework GCC-Spam, which integrates three core innovations. First, a character similarity network captures orthographic and phonetic features to counter character-obfuscation attacks and furthermore produces sentence embeddings for downstream classification. Second, contrastive learning enhances discriminability by optimizing the latent-space distance between spam and normal texts. Third, a Generative Adversarial Network (GAN) generates realistic pseudo-spam samples to alleviate data scarcity while improving model robustness and classification accuracy. Extensive experiments on real-world datasets demonstrate that our model outperforms baseline approaches, achieving higher detection rates with significantly fewer labeled examples.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis</title>
<link>https://arxiv.org/abs/2507.14680</link>
<guid>https://arxiv.org/abs/2507.14680</guid>
<content:encoded><![CDATA[
arXiv:2507.14680v1 Announce Type: cross 
Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel tissue analysis across various pathological tasks. While recent advancements in multi-modal large language models (MLLMs) allow multi-task WSI analysis through natural language, they often underperform compared to task-specific models. Collaborative multi-agent systems have emerged as a promising solution to balance versatility and accuracy in healthcare, yet their potential remains underexplored in pathology-specific domains. To address these issues, we propose WSI-Agents, a novel collaborative multi-agent system for multi-modal WSI analysis. WSI-Agents integrates specialized functional agents with robust task allocation and verification mechanisms to enhance both task-specific accuracy and multi-task versatility through three components: (1) a task allocation module assigning tasks to expert agents using a model zoo of patch and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through internal consistency checks and external validation using pathology knowledge bases and domain-specific models, and (3) a summary module synthesizing the final summary with visual interpretation maps. Extensive experiments on multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs and medical agent frameworks across diverse tasks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations</title>
<link>https://arxiv.org/abs/2507.14688</link>
<guid>https://arxiv.org/abs/2507.14688</guid>
<content:encoded><![CDATA[
arXiv:2507.14688v1 Announce Type: cross 
Abstract: Post-training has emerged as a crucial technique for aligning pre-trained Large Language Models (LLMs) with human instructions, significantly enhancing their performance across a wide range of tasks. Central to this process is the quality and diversity of post-training datasets. This paper presents a review of publicly available Arabic post-training datasets on the Hugging Face Hub, organized along four key dimensions: (1) LLM Capabilities (e.g., Question Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation, and Function Calling); (2) Steerability (e.g., persona and system prompts); (3) Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness. Each dataset is rigorously evaluated based on popularity, practical adoption, recency and maintenance, documentation and annotation quality, licensing transparency, and scientific contribution. Our review revealed critical gaps in the development of Arabic post-training datasets, including limited task diversity, inconsistent or missing documentation and annotation, and low adoption across the community. Finally, the paper discusses the implications of these gaps on the progress of Arabic LLMs and applications while providing concrete recommendations for future efforts in post-training dataset development.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation</title>
<link>https://arxiv.org/abs/2507.14693</link>
<guid>https://arxiv.org/abs/2507.14693</guid>
<content:encoded><![CDATA[
arXiv:2507.14693v1 Announce Type: cross 
Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet its progress faces two under-explored challenges: limited language coverage and unreliable annotation practices. Most available datasets are in English, but even among these, high-quality, human-annotated data remains scarce. As a result, many studies rely on available pre-labeled datasets without examining their annotation process or label reliability. The lack of datasets in other languages further limits the global realization of suicide prevention via artificial intelligence (AI). In this study, we address one of these gaps by constructing a novel Turkish suicidal ideation corpus derived from social media posts and introducing a resource-efficient annotation framework involving three human annotators and two large language models (LLMs). We then address the remaining gaps by performing a bidirectional evaluation of label reliability and model consistency across this dataset and three popular English suicidal ideation detection datasets, using transfer learning through eight pre-trained sentiment and emotion classifiers. These transformers help assess annotation consistency and benchmark model performance against manually labeled data. Our findings underscore the need for more rigorous, language-inclusive approaches to annotation and evaluation in mental health natural language processing (NLP) while demonstrating the questionable performance of popular models with zero-shot transfer learning. We advocate for transparency in model training and dataset construction in mental health NLP, prioritizing data and model reliability.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition</title>
<link>https://arxiv.org/abs/2507.14698</link>
<guid>https://arxiv.org/abs/2507.14698</guid>
<content:encoded><![CDATA[
arXiv:2507.14698v1 Announce Type: cross 
Abstract: EEG-based emotion recognition plays an important role in developing adaptive brain-computer communication systems, yet faces two fundamental challenges in practical implementations: (1) effective integration of non-stationary spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional intensity variations in real-world scenarios. This paper proposes SST-CL, a novel framework integrating spatial-temporal transformers with curriculum learning. Our method introduces two core components: a spatial encoder that models inter-channel relationships and a temporal encoder that captures multi-scale dependencies through windowed attention mechanisms, enabling simultaneous extraction of spatial correlations and temporal dynamics from EEG signals. Complementing this architecture, an intensity-aware curriculum learning strategy progressively guides training from high-intensity to low-intensity emotional states through dynamic sample scheduling based on a dual difficulty assessment. Comprehensive experiments on three benchmark datasets demonstrate state-of-the-art performance across various emotional intensity levels, with ablation studies confirming the necessity of both architectural components and the curriculum learning mechanism.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling</title>
<link>https://arxiv.org/abs/2507.14706</link>
<guid>https://arxiv.org/abs/2507.14706</guid>
<content:encoded><![CDATA[
arXiv:2507.14706v1 Announce Type: cross 
Abstract: Detecting fraudulent credit card transactions remains a significant challenge, due to the extreme class imbalance in real-world data and the often subtle patterns that separate fraud from legitimate activity. Existing research commonly attempts to address this by generating synthetic samples for the minority class using approaches such as GANs, VAEs, or hybrid generative models. However, these techniques, particularly when applied only to minority-class data, tend to result in overconfident classifiers and poor latent cluster separation, ultimately limiting real-world detection performance. In this study, we propose the Causal Prototype Attention Classifier (CPAC), an interpretable architecture that promotes class-aware clustering and improved latent space structure through prototype-based attention mechanisms and we will couple it with the encoder in a VAE-GAN allowing it to offer a better cluster separation moving beyond post-hoc sample augmentation. We compared CPAC-augmented models to traditional oversamplers, such as SMOTE, as well as to state-of-the-art generative models, both with and without CPAC-based latent classifiers. Our results show that classifier-guided latent shaping with CPAC delivers superior performance, achieving an F1-score of 93.14\% percent and recall of 90.18\%, along with improved latent cluster separation. Further ablation studies and visualizations provide deeper insight into the benefits and limitations of classifier-driven representation learning for fraud detection. The codebase for this work will be available at final submission.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4</title>
<link>https://arxiv.org/abs/2507.14722</link>
<guid>https://arxiv.org/abs/2507.14722</guid>
<content:encoded><![CDATA[
arXiv:2507.14722v1 Announce Type: cross 
Abstract: Automated theorem proving (ATP) has been a classical problem in artificial intelligence since its inception, yet it remains challenging due to its vast state and action space. Large language models (LLMs) have recently emerged as a promising heuristic for ATP, but they lack correctness guarantees and thus require interaction with a proof verifier. Such interactions typically follow one of two approaches: black-box interaction, which does not utilize intermediate proof states, or white-box approaches, which allow for incremental proof construction and examination of intermediate states. While black-box approaches have directly benefited from recent LLM advances, white-box methods have comparatively lagged behind. In this paper, we address this gap by introducing LeanTree, which consists of (i) a tool built in the Lean 4 language that factorizes complex proof states into simpler, independent branches, and (ii) a dataset of these factorized intermediate states. Our white-box tooling offers several advantages over black-box approaches: it simplifies evaluation, reduces necessary context, generates richer training data, enables parallel search across multiple states, supports efficient reuse of states, and provides feedback in case of errors. Our preliminary results hint that white-box approaches outperform black-box alternatives in some settings.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding</title>
<link>https://arxiv.org/abs/2507.14725</link>
<guid>https://arxiv.org/abs/2507.14725</guid>
<content:encoded><![CDATA[
arXiv:2507.14725v1 Announce Type: cross 
Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to adapt large language models (LLMs) across task sequences. However, most existing methods assume task-aware inference and maintain a growing list of task-specific prompts, which limits scalability and hides latent forgetting. In this work, we introduce GRID, a unified framework that addresses two key limitations: (1) latent forgetting under task-agnostic inference, and (2) prompt memory explosion as task sequences grow. GRID integrates a task-aware decoding mechanism that improves backward transfer by leveraging representative inputs, automatic task identification, and constrained decoding. Additionally, we propose a gradient-based prompt selection strategy that compresses less informative prompts into a single aggregated representation, enabling scalable and memory-efficient lifelong learning. Extensive experiments across short-sequence, long-sequence, and negative transfer benchmarks show that GRID significantly improves backward transfer, achieves competitive forward transfer, and reduces forgotten tasks by up to 80\%, outperforming state-of-the-art methods on T5 and Flan-T5 backbones.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.14748</link>
<guid>https://arxiv.org/abs/2507.14748</guid>
<content:encoded><![CDATA[
arXiv:2507.14748v1 Announce Type: cross 
Abstract: Self-supervised feature learning and pretraining methods in reinforcement learning (RL) often rely on information-theoretic principles, termed mutual information skill learning (MISL). These methods aim to learn a representation of the environment while also incentivizing exploration thereof. However, the role of the representation and mutual information parametrization in MISL is not yet well understood theoretically. Our work investigates MISL through the lens of identifiable representation learning by focusing on the Contrastive Successor Features (CSF) method. We prove that CSF can provably recover the environment's ground-truth features up to a linear transformation due to the inner product parametrization of the features and skill diversity in a discriminative sense. This first identifiability guarantee for representation learning in RL also helps explain the implications of different mutual information objectives and the downsides of entropy regularizers. We empirically validate our claims in MuJoCo and DeepMind Control and show how CSF provably recovers the ground-truth features both from states and pixels.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Internal Activity and Robustness of SNNs Across Neuron Parameter Space</title>
<link>https://arxiv.org/abs/2507.14757</link>
<guid>https://arxiv.org/abs/2507.14757</guid>
<content:encoded><![CDATA[
arXiv:2507.14757v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) offer energy-efficient and biologically plausible alternatives to traditional artificial neural networks, but their performance depends critically on the tuning of neuron model parameters. In this work, we identify and characterize an operational space - a constrained region in the neuron hyperparameter domain (specifically membrane time constant tau and voltage threshold vth) - within which the network exhibits meaningful activity and functional behavior. Operating inside this manifold yields optimal trade-offs between classification accuracy and spiking activity, while stepping outside leads to degeneration: either excessive energy use or complete network silence.
  Through systematic exploration across datasets and architectures, we visualize and quantify this manifold and identify efficient operating points. We further assess robustness to adversarial noise, showing that SNNs exhibit increased spike correlation and internal synchrony when operating outside their optimal region. These findings highlight the importance of principled hyperparameter tuning to ensure both task performance and energy efficiency. Our results offer practical guidelines for deploying robust and efficient SNNs, particularly in neuromorphic computing scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization</title>
<link>https://arxiv.org/abs/2507.14758</link>
<guid>https://arxiv.org/abs/2507.14758</guid>
<content:encoded><![CDATA[
arXiv:2507.14758v1 Announce Type: cross 
Abstract: Generative models have recently demonstrated strong potential in multi-behavior recommendation systems, leveraging the expressive power of transformers and tokenization to generate personalized item sequences. However, their adoption is hindered by (1) the lack of explicit information for token reasoning, (2) high computational costs due to quadratic attention complexity and dense sequence representations after tokenization, and (3) limited multi-scale modeling over user history. In this work, we propose GRACE (Generative Recommendation via journey-aware sparse Attention on Chain-of-thought tokEnization), a novel generative framework for multi-behavior sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT) tokenization method that encodes user-item interactions with explicit attributes from product knowledge graphs (e.g., category, brand, price) over semantic tokenization, enabling interpretable and behavior-aligned generation. To address the inefficiency of standard attention, we design a Journey-Aware Sparse Attention (JSA) mechanism, which selectively attends to compressed, intra-, inter-, and current-context segments in the tokenized sequence. Experiments on two real-world datasets show that GRACE significantly outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and +106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces attention computation by up to 48% with long sequences.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems</title>
<link>https://arxiv.org/abs/2507.14760</link>
<guid>https://arxiv.org/abs/2507.14760</guid>
<content:encoded><![CDATA[
arXiv:2507.14760v1 Announce Type: cross 
Abstract: Deep learning models often hallucinate, producing realistic artifacts that are not truly present in the sample. This can have dire consequences for scientific and medical inverse problems, such as MRI and microscopy denoising, where accuracy is more important than perceptual quality. Uncertainty quantification techniques, such as conformal prediction, can pinpoint outliers and provide guarantees for image regression tasks, improving reliability. However, existing methods utilize a linear constant scaling factor to calibrate uncertainty bounds, resulting in larger, less informative bounds. We propose QUTCC, a quantile uncertainty training and calibration technique that enables nonlinear, non-uniform scaling of quantile predictions to enable tighter uncertainty estimates. Using a U-Net architecture with a quantile embedding, QUTCC enables the prediction of the full conditional distribution of quantiles for the imaging task. During calibration, QUTCC generates uncertainty bounds by iteratively querying the network for upper and lower quantiles, progressively refining the bounds to obtain a tighter interval that captures the desired coverage. We evaluate our method on several denoising tasks as well as compressive MRI reconstruction. Our method successfully pinpoints hallucinations in image estimates and consistently achieves tighter uncertainty intervals than prior methods while maintaining the same statistical coverage.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories</title>
<link>https://arxiv.org/abs/2507.14766</link>
<guid>https://arxiv.org/abs/2507.14766</guid>
<content:encoded><![CDATA[
arXiv:2507.14766v1 Announce Type: cross 
Abstract: In intensive care units (ICUs), patients with complex clinical conditions require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a vital diagnostic tool, providing insights into clinical trajectories, but their irregular acquisition limits their utility. Existing tools for CXR interpretation are constrained by cross-sectional analysis, failing to capture temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal framework that integrates temporally sparse CXR imaging and radiology reports with high-frequency clinical data, such as vital signs, laboratory values, and respiratory flow sheets, to predict the trajectory of CXR findings in critically ill patients. CXR-TFT leverages latent embeddings from a vision encoder that are temporally aligned with hourly clinical data through interpolation. A transformer model is then trained to predict CXR embeddings at each hour, conditioned on previous embeddings and clinical measurements. In a retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy in forecasting abnormal CXR findings up to 12 hours before they became radiographically evident. This predictive capability in clinical data holds significant potential for enhancing the management of time-sensitive conditions like acute respiratory distress syndrome, where early intervention is crucial and diagnoses are often delayed. By providing distinctive temporal resolution in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights that can directly improve clinical outcomes.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XplainAct: Visualization for Personalized Intervention Insights</title>
<link>https://arxiv.org/abs/2507.14767</link>
<guid>https://arxiv.org/abs/2507.14767</guid>
<content:encoded><![CDATA[
arXiv:2507.14767v1 Announce Type: cross 
Abstract: Causality helps people reason about and understand complex systems, particularly through what-if analyses that explore how interventions might alter outcomes. Although existing methods embrace causal reasoning using interventions and counterfactual analysis, they primarily focus on effects at the population level. These approaches often fall short in systems characterized by significant heterogeneity, where the impact of an intervention can vary widely across subgroups. To address this challenge, we present XplainAct, a visual analytics framework that supports simulating, explaining, and reasoning interventions at the individual level within subpopulations. We demonstrate the effectiveness of XplainAct through two case studies: investigating opioid-related deaths in epidemiology and analyzing voting inclinations in the presidential election.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards</title>
<link>https://arxiv.org/abs/2507.14783</link>
<guid>https://arxiv.org/abs/2507.14783</guid>
<content:encoded><![CDATA[
arXiv:2507.14783v1 Announce Type: cross 
Abstract: The advancement of general-purpose artificial intelligence relies on large language models (LLMs) that excel across a wide range of tasks, from structured reasoning to creative generation. However, post-training methods like Supervised Fine-Tuning (SFT) often struggle with generalization, favoring memorization over transferable learning. In this work, we introduce Omni-Think, a unified reinforcement learning (RL) framework that enhances LLM performance across diverse tasks by combining rule-based verifiable rewards with generative preference signals via LLM-as-a-Judge evaluations. Our approach enables consistent optimization across task types and scales RL-based training to subjective domains. We further investigate training strategies, demonstrating that a curriculum-based progression that orders tasks from structured to open-ended improves performance and reduces forgetting. Experimental results across four domains reveal that curriculum learning improves performance by 5.2\% over joint training and 9.1\% over model merging. These results highlight the importance of task-aware sampling and hybrid supervision in scaling RL-based post-training for general-purpose LLMs.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering</title>
<link>https://arxiv.org/abs/2507.14784</link>
<guid>https://arxiv.org/abs/2507.14784</guid>
<content:encoded><![CDATA[
arXiv:2507.14784v1 Announce Type: cross 
Abstract: Video Question Answering (VideoQA) requires identifying sparse critical moments in long videos and reasoning about their causal relationships to answer semantically complex questions. While recent advances in multimodal learning have improved alignment and fusion, current approaches remain limited by two prevalent but fundamentally flawed strategies: (1) task-agnostic sampling indiscriminately processes all frames, overwhelming key events with irrelevant content; and (2) heuristic retrieval captures superficial patterns but misses causal-temporal structures needed for complex reasoning. To address these challenges, we introduce LeAdQA, an innovative approach that bridges these gaps through synergizing causal-aware query refinement with fine-grained visual grounding. Our method first leverages LLMs to reformulate question-option pairs, resolving causal ambiguities and sharpening temporal focus. These refined queries subsequently direct a temporal grounding model to precisely retrieve the most salient segments, complemented by an adaptive fusion mechanism dynamically integrating the evidence to maximize relevance. The integrated visual-textual cues are then processed by an MLLM to generate accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and NExT-GQA demonstrate that our method's precise visual grounding substantially enhances the understanding of video-question relationships, achieving state-of-the-art (SOTA) performance on complex reasoning tasks while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs</title>
<link>https://arxiv.org/abs/2507.14785</link>
<guid>https://arxiv.org/abs/2507.14785</guid>
<content:encoded><![CDATA[
arXiv:2507.14785v1 Announce Type: cross 
Abstract: The complexity and interconnectivity of entities involved in money laundering demand investigative reasoning over graph-structured data. This paper explores the use of large language models (LLMs) as reasoning engines over localized subgraphs extracted from a financial knowledge graph. We propose a lightweight pipeline that retrieves k-hop neighborhoods around entities of interest, serializes them into structured text, and prompts an LLM via few-shot in-context learning to assess suspiciousness and generate justifications. Using synthetic anti-money laundering (AML) scenarios that reflect common laundering behaviors, we show that LLMs can emulate analyst-style logic, highlight red flags, and provide coherent explanations. While this study is exploratory, it illustrates the potential of LLM-based graph reasoning in AML and lays groundwork for explainable, language-driven financial crime analytics.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOCUS: Fused Observation of Channels for Unveiling Spectra</title>
<link>https://arxiv.org/abs/2507.14787</link>
<guid>https://arxiv.org/abs/2507.14787</guid>
<content:encoded><![CDATA[
arXiv:2507.14787v1 Announce Type: cross 
Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous wavelength bands, making it a powerful tool in biology, agriculture, and environmental monitoring. However, interpreting Vision Transformers (ViTs) in this setting remains largely unexplored due to two key challenges: (1) existing saliency methods struggle to capture meaningful spectral cues, often collapsing attention onto the class token, and (2) full-spectrum ViTs are computationally prohibitive for interpretability, given the high-dimensional nature of HSI data. We present FOCUS, the first framework that enables reliable and efficient spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core components: class-specific spectral prompts that guide attention toward semantically meaningful wavelength groups, and a learnable [SINK] token trained with an attraction loss to absorb noisy or redundant attention. Together, these designs make it possible to generate stable and interpretable 3D saliency maps and spectral importance curves in a single forward pass, without any gradient backpropagation or backbone modification. FOCUS improves band-level IoU by 15 percent, reduces attention collapse by over 40 percent, and produces saliency results that align closely with expert annotations. With less than 1 percent parameter overhead, our method makes high-resolution ViT interpretability practical for real-world hyperspectral applications, bridging a long-standing gap between black-box modeling and trustworthy HSI decision-making.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree</title>
<link>https://arxiv.org/abs/2507.14799</link>
<guid>https://arxiv.org/abs/2507.14799</guid>
<content:encoded><![CDATA[
arXiv:2507.14799v1 Announce Type: cross 
Abstract: This work demonstrates that LLM-based web navigation agents offer powerful automation capabilities but are vulnerable to Indirect Prompt Injection (IPI) attacks. We show that adversaries can embed universal adversarial triggers in webpage HTML to hijack agent behavior that utilizes the accessibility tree to parse HTML, causing unintended or malicious actions. Using the Greedy Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by Llama-3.1, our system demonstrates high success rates across real websites in both targeted and general attacks, including login credential exfiltration and forced ad clicks. Our empirical results highlight critical security risks and the need for stronger defenses as LLM-driven autonomous web agents become more widely adopted. The system software (https://github.com/sej2020/manipulating-web-agents) is released under the MIT License, with an accompanying publicly available demo website (http://lethaiq.github.io/attack-web-llm-agent).
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model as An Operator: An Experience-Driven Solution for Distribution Network Voltage Control</title>
<link>https://arxiv.org/abs/2507.14800</link>
<guid>https://arxiv.org/abs/2507.14800</guid>
<content:encoded><![CDATA[
arXiv:2507.14800v1 Announce Type: cross 
Abstract: With the advanced reasoning and information analysis capabilities, large language models (LLMs) can offer a novel approach for the autonomous generation of dispatch strategies in power systems. This letter proposes an LLM-based experience-driven voltage control solution for distribution networks, which enables the self-evolution of LLM-based voltage control strategies through the collaboration and interaction of multiple modules-specifically, experience storage, experience retrieval, experience generation, and experience modification. Comprehensive experimental results validate the effectiveness of the proposed method and highlight the applicability of LLM in addressing power system dispatch challenges.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACME: Adaptive Customization of Large Models via Distributed Systems</title>
<link>https://arxiv.org/abs/2507.14802</link>
<guid>https://arxiv.org/abs/2507.14802</guid>
<content:encoded><![CDATA[
arXiv:2507.14802v1 Announce Type: cross 
Abstract: Pre-trained Transformer-based large models have revolutionized personal virtual assistants, but their deployment in cloud environments faces challenges related to data privacy and response latency. Deploying large models closer to the data and users has become a key research area to address these issues. However, applying these models directly often entails significant difficulties, such as model mismatching, resource constraints, and energy inefficiency. Automated design of customized models is necessary, but it faces three key challenges, namely, the high cost of centralized model customization, imbalanced performance from user heterogeneity, and suboptimal performance from data heterogeneity. In this paper, we propose ACME, an adaptive customization approach of Transformer-based large models via distributed systems. To avoid the low cost-efficiency of centralized methods, ACME employs a bidirectional single-loop distributed system to progressively achieve fine-grained collaborative model customization. In order to better match user heterogeneity, it begins by customizing the backbone generation and identifying the Pareto Front under model size constraints to ensure optimal resource utilization. Subsequently, it performs header generation and refines the model using data distribution-based personalized architecture aggregation to match data heterogeneity. Evaluation on different datasets shows that ACME achieves cost-efficient models under model size constraints. Compared to centralized systems, data transmission volume is reduced to 6 percent. Additionally, the average accuracy improves by 10 percent compared to the baseline, with the trade-off metrics increasing by nearly 30 percent.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subliminal Learning: Language models transmit behavioral traits via hidden signals in data</title>
<link>https://arxiv.org/abs/2507.14805</link>
<guid>https://arxiv.org/abs/2507.14805</guid>
<content:encoded><![CDATA[
arXiv:2507.14805v1 Announce Type: cross 
Abstract: We study subliminal learning, a surprising phenomenon where language models transmit behavioral traits via semantically unrelated data. In our main experiments, a "teacher" model with some trait T (such as liking owls or being misaligned) generates a dataset consisting solely of number sequences. Remarkably, a "student" model trained on this dataset learns T. This occurs even when the data is filtered to remove references to T. We observe the same effect when training on code or reasoning traces generated by the same teacher model. However, we do not observe the effect when the teacher and student have different base models. To help explain our findings, we prove a theoretical result showing that subliminal learning occurs in all neural networks under certain conditions, and demonstrate subliminal learning in a simple MLP classifier. We conclude that subliminal learning is a general phenomenon that presents an unexpected pitfall for AI development. Distillation could propagate unintended traits, even when developers try to prevent this via data filtering.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection</title>
<link>https://arxiv.org/abs/2507.14807</link>
<guid>https://arxiv.org/abs/2507.14807</guid>
<content:encoded><![CDATA[
arXiv:2507.14807v1 Announce Type: cross 
Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often appearing in natural social settings that challenge existing detection methods. Most current approaches excel at single-face detection but struggle in multi-face scenarios, due to a lack of awareness of crucial contextual cues. In this work, we develop a novel approach that leverages human cognition to analyze and defend against multi-face deepfake videos. Through a series of human studies, we systematically examine how people detect deepfake faces in social settings. Our quantitative analysis reveals four key cues humans rely on: scene-motion coherence, inter-face appearance compatibility, interpersonal gaze alignment, and face-body consistency. Guided by these insights, we introduce \textsf{HICOM}, a novel framework designed to detect every fake face in multi-face scenarios. Extensive experiments on benchmark datasets show that \textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and 2.8\% under real-world perturbations. Moreover, it outperforms existing methods by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM to provide human-readable explanations, making detection results more transparent and convincing. Our work sheds light on involving human factors to enhance defense against deepfakes.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models</title>
<link>https://arxiv.org/abs/2507.14811</link>
<guid>https://arxiv.org/abs/2507.14811</guid>
<content:encoded><![CDATA[
arXiv:2507.14811v1 Announce Type: cross 
Abstract: Diffusion models have demonstrated exceptional generative capabilities but are computationally intensive, posing significant challenges for deployment in resource-constrained or latency-sensitive environments. Quantization offers an effective means to reduce model size and computational cost, with post-training quantization (PTQ) being particularly appealing due to its compatibility with pre-trained models without requiring retraining or training data. However, existing PTQ methods for diffusion models often rely on architecture-specific heuristics that limit their generalizability and hinder integration with industrial deployment pipelines. To address these limitations, we propose SegQuant, a unified quantization framework that adaptively combines complementary techniques to enhance cross-model versatility. SegQuant consists of a segment-aware, graph-based quantization strategy (SegLinear) that captures structural semantics and spatial heterogeneity, along with a dual-scale quantization scheme (DualScale) that preserves polarity-asymmetric activations, which is crucial for maintaining visual fidelity in generated outputs. SegQuant is broadly applicable beyond Transformer-based diffusion models, achieving strong performance while ensuring seamless compatibility with mainstream deployment tools.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Foundation Models with Multimodal Public Electronic Health Records</title>
<link>https://arxiv.org/abs/2507.14824</link>
<guid>https://arxiv.org/abs/2507.14824</guid>
<content:encoded><![CDATA[
arXiv:2507.14824v1 Announce Type: cross 
Abstract: Foundation models have emerged as a powerful approach for processing electronic health records (EHRs), offering flexibility to handle diverse medical data modalities. In this study, we present a comprehensive benchmark that evaluates the performance, fairness, and interpretability of foundation models, both as unimodal encoders and as multimodal learners, using the publicly available MIMIC-IV database. To support consistent and reproducible evaluation, we developed a standardized data processing pipeline that harmonizes heterogeneous clinical records into an analysis-ready format. We systematically compared eight foundation models, encompassing both unimodal and multimodal models, as well as domain-specific and general-purpose variants. Our findings demonstrate that incorporating multiple data modalities leads to consistent improvements in predictive performance without introducing additional bias. Through this benchmark, we aim to support the development of effective and trustworthy multimodal artificial intelligence (AI) systems for real-world clinical applications. Our code is available at https://github.com/nliulab/MIMIC-Multimodal.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eMargin: Revisiting Contrastive Learning with Margin-Based Separation</title>
<link>https://arxiv.org/abs/2507.14828</link>
<guid>https://arxiv.org/abs/2507.14828</guid>
<content:encoded><![CDATA[
arXiv:2507.14828v1 Announce Type: cross 
Abstract: We revisit previous contrastive learning frameworks to investigate the effect of introducing an adaptive margin into the contrastive loss function for time series representation learning. Specifically, we explore whether an adaptive margin (eMargin), adjusted based on a predefined similarity threshold, can improve the separation between adjacent but dissimilar time steps and subsequently lead to better performance in downstream tasks. Our study evaluates the impact of this modification on clustering performance and classification in three benchmark datasets. Our findings, however, indicate that achieving high scores on unsupervised clustering metrics does not necessarily imply that the learned embeddings are meaningful or effective in downstream tasks. To be specific, eMargin added to InfoNCE consistently outperforms state-of-the-art baselines in unsupervised clustering metrics, but struggles to achieve competitive results in downstream classification with linear probing. The source code is publicly available at https://github.com/sfi-norwai/eMargin.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paired Image Generation with Diffusion-Guided Diffusion Models</title>
<link>https://arxiv.org/abs/2507.14833</link>
<guid>https://arxiv.org/abs/2507.14833</guid>
<content:encoded><![CDATA[
arXiv:2507.14833v1 Announce Type: cross 
Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images is very significant for the early screening of breast cancer. However, the high-density breast tissue often leads to high concealment of the mass lesions, which makes manual annotation difficult and time-consuming. As a result, there is a lack of annotated data for model training. Diffusion models are commonly used for data augmentation, but the existing methods face two challenges. First, due to the high concealment of lesions, it is difficult for the model to learn the features of the lesion area. This leads to the low generation quality of the lesion areas, thus limiting the quality of the generated images. Second, existing methods can only generate images and cannot generate corresponding annotations, which restricts the usability of the generated images in supervised training. In this work, we propose a paired image generation method. The method does not require external conditions and can achieve the generation of paired images by training an extra diffusion guider for the conditional diffusion model. During the experimental phase, we generated paired DBT slices and mass lesion masks. Then, we incorporated them into the supervised training process of the mass lesion segmentation task. The experimental results show that our method can improve the generation quality without external conditions. Moreover, it contributes to alleviating the shortage of annotated data, thus enhancing the performance of downstream tasks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Invisible Leash: Why RLVR May Not Escape Its Origin</title>
<link>https://arxiv.org/abs/2507.14843</link>
<guid>https://arxiv.org/abs/2507.14843</guid>
<content:encoded><![CDATA[
arXiv:2507.14843v1 Announce Type: cross 
Abstract: Recent advances in large reasoning models highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing AI's capabilities, particularly in solving complex logical tasks. However, it remains unclear whether RLVR truly expands a model's reasoning boundary or merely amplifies high-reward outputs that the base model already knows for improved precision. This study presents a theoretical and empirical investigation that provides fresh insights into the potential limits of RLVR. First, we offer a new theoretical perspective that RLVR is constrained by the base model's support-unable to sample solutions with zero initial probability-and operates as a conservative reweighting mechanism that may restrict the discovery of entirely original solutions. We also identify an entropy-reward tradeoff: while RLVR reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments validate that while RLVR consistently improves pass@1, the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets, failing to recover correct answers that were previously accessible to the base model. Interestingly, we also observe that while RLVR sometimes increases token-level entropy, resulting in greater uncertainty at each generation step, answer-level entropy declines, indicating that these seemingly more uncertain paths ultimately converge onto a smaller set of distinct answers. Taken together, these findings reveal potential limits of RLVR in extending reasoning horizons. Breaking this invisible leash may require future algorithmic innovations such as explicit exploration mechanisms or hybrid strategies that seed probability mass into underrepresented solution regions.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems</title>
<link>https://arxiv.org/abs/2507.14850</link>
<guid>https://arxiv.org/abs/2507.14850</guid>
<content:encoded><![CDATA[
arXiv:2507.14850v1 Announce Type: cross 
Abstract: We address the problem of safe policy learning in multi-agent safety-critical autonomous systems. In such systems, it is necessary for each agent to meet the safety requirements at all times while also cooperating with other agents to accomplish the task. Toward this end, we propose a safe Hierarchical Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier Functions (CBFs). Our proposed hierarchical approach decomposes the overall reinforcement learning problem into two levels learning joint cooperative behavior at the higher level and learning safe individual behavior at the lower or agent level conditioned on the high-level policy. Specifically, we propose a skill-based HMARL-CBF algorithm in which the higher level problem involves learning a joint policy over the skills for all the agents and the lower-level problem involves learning policies to execute the skills safely with CBFs. We validate our approach on challenging environment scenarios whereby a large number of agents have to safely navigate through conflicting road networks. Compared with existing state of the art methods, our approach significantly improves the safety achieving near perfect (within 5%) success/safety rate while also improving performance across all the environments.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Degradations in Natural Language for All-In-One Video Restoration</title>
<link>https://arxiv.org/abs/2507.14851</link>
<guid>https://arxiv.org/abs/2507.14851</guid>
<content:encoded><![CDATA[
arXiv:2507.14851v1 Announce Type: cross 
Abstract: In this work, we propose an all-in-one video restoration framework that grounds degradation-aware semantic context of video frames in natural language via foundation models, offering interpretable and flexible guidance. Unlike prior art, our method assumes no degradation knowledge in train or test time and learns an approximation to the grounded knowledge such that the foundation model can be safely disentangled during inference adding no extra cost. Further, we call for standardization of benchmarks in all-in-one video restoration, and propose two benchmarks in multi-degradation setting, three-task (3D) and four-task (4D), and two time-varying composite degradation benchmarks; one of the latter being our proposed dataset with varying snow intensity, simulating how weather degradations affect videos naturally. We compare our method with prior works and report state-of-the-art performance on all benchmarks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs</title>
<link>https://arxiv.org/abs/2507.14874</link>
<guid>https://arxiv.org/abs/2507.14874</guid>
<content:encoded><![CDATA[
arXiv:2507.14874v1 Announce Type: cross 
Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine (TM) both interpretable and efficient, while the power of Tsetlin automata enables accuracy comparable to deep learning on an increasing number of datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning interpretable deep clauses from graph-structured input. Moving beyond flat, fixed-length input, the GraphTM gets more versatile, supporting sequences, grids, relations, and multimodality. Through message passing, the GraphTM builds nested deep clauses to recognize sub-graph patterns with exponentially fewer clauses, increasing both interpretability and data utilization. For image classification, GraphTM preserves interpretability and achieves 3.86%-points higher accuracy on CIFAR-10 than a convolutional TM. For tracking action coreference, faced with increasingly challenging tasks, GraphTM outperforms other reinforcement learning methods by up to 20.6%-points. In recommendation systems, it tolerates increasing noise to a greater extent than a Graph Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training 2.5x faster than GCN. The GraphTM's application to these varied fields demonstrates how graph representation learning and deep clauses bring new possibilities for TM learning.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization</title>
<link>https://arxiv.org/abs/2507.14882</link>
<guid>https://arxiv.org/abs/2507.14882</guid>
<content:encoded><![CDATA[
arXiv:2507.14882v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) offer significant versatility and performance benefits, but their widespread adoption is often hindered by high model complexity and computational demands. Model compression techniques such as pruning have emerged as promising solutions to these challenges. However, it remains critical to ensure that application-specific performance characteristics are preserved during compression. In structured pruning, where groups of structurally coherent elements are removed, conventional importance metrics frequently fail to maintain these essential performance attributes. In this work, we propose an enhanced importance metric framework that not only reduces model size but also explicitly accounts for application-specific performance constraints. We employ multiple strategies to determine the optimal pruning magnitude for each group, ensuring a balance between compression and task performance. Our approach is evaluated on an autoencoder tasked with reconstructing MNIST images. Experimental results demonstrate that the proposed method effectively preserves task-relevant performance, maintaining the model's usability even after substantial pruning, by satisfying the required application-specific criteria.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Nonlinear Causal Reductions to Explain Reinforcement Learning Policies</title>
<link>https://arxiv.org/abs/2507.14901</link>
<guid>https://arxiv.org/abs/2507.14901</guid>
<content:encoded><![CDATA[
arXiv:2507.14901v1 Announce Type: cross 
Abstract: Why do reinforcement learning (RL) policies fail or succeed? This is a challenging question due to the complex, high-dimensional nature of agent-environment interactions. In this work, we take a causal perspective on explaining the behavior of RL policies by viewing the states, actions, and rewards as variables in a low-level causal model. We introduce random perturbations to policy actions during execution and observe their effects on the cumulative reward, learning a simplified high-level causal model that explains these relationships. To this end, we develop a nonlinear Causal Model Reduction framework that ensures approximate interventional consistency, meaning the simplified high-level model responds to interventions in a similar way as the original complex system. We prove that for a class of nonlinear causal models, there exists a unique solution that achieves exact interventional consistency, ensuring learned explanations reflect meaningful causal patterns. Experiments on both synthetic causal models and practical RL tasks-including pendulum control and robot table tennis-demonstrate that our approach can uncover important behavioral patterns, biases, and failure modes in trained RL policies.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP</title>
<link>https://arxiv.org/abs/2507.14904</link>
<guid>https://arxiv.org/abs/2507.14904</guid>
<content:encoded><![CDATA[
arXiv:2507.14904v1 Announce Type: cross 
Abstract: 3D visual grounding allows an embodied agent to understand visual information in real-world 3D environments based on human instructions, which is crucial for embodied intelligence. Existing 3D visual grounding methods typically rely on separate encoders for different modalities (e.g., RGB images, text, and 3D point clouds), resulting in large and complex models that are inefficient to train. While some approaches use pre-trained 2D multi-modal models like CLIP for 3D tasks, they still struggle with aligning point cloud data to 2D encoders. As a result, these methods continue to depend on 3D encoders for feature extraction, further increasing model complexity and training inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal network to process all three modalities (RGB images, text, and point clouds), significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal model with adapter-based fine-tuning, this framework effectively adapts to the tri-modal setting, improving both adaptability and performance across modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module is designed to fuse geometric multi-scale features from point clouds and images. We then integrate textual features for final modality fusion and introduce a multi-modal decoder to facilitate deep cross-modal understanding. Together, our method achieves unified feature extraction and fusion across the three modalities, enabling an end-to-end 3D visual grounding model. Compared to the baseline, our method reduces the number of trainable parameters by approximately 58\%, while achieving a 6.52\% improvement in the 3D detection task and a 6.25\% improvement in the 3D visual grounding task.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partial Symmetry Enforced Attention Decomposition (PSEAD): A Group-Theoretic Framework for Equivariant Transformers in Biological Systems</title>
<link>https://arxiv.org/abs/2507.14908</link>
<guid>https://arxiv.org/abs/2507.14908</guid>
<content:encoded><![CDATA[
arXiv:2507.14908v1 Announce Type: cross 
Abstract: This research introduces the Theory of Partial Symmetry Enforced Attention Decomposition (PSEAD), a new and rigorous group-theoretic framework designed to seamlessly integrate local symmetry awareness into the core architecture of self-attention mechanisms within Transformer models. We formalize the concept of local permutation subgroup actions on windows of biological data, proving that under such actions, the attention mechanism naturally decomposes into a direct sum of orthogonal irreducible components. Critically, these components are intrinsically aligned with the irreducible representations of the acting permutation subgroup, thereby providing a powerful mathematical basis for disentangling symmetric and asymmetric features. We show that PSEAD offers substantial advantages. These include enhanced generalization capabilities to novel biological motifs exhibiting similar partial symmetries, unprecedented interpretability by allowing direct visualization and analysis of attention contributions from different symmetry channels, and significant computational efficiency gains by focusing representational capacity on relevant symmetric subspaces. Beyond static data analysis, we extend PSEAD's applicability to dynamic biological processes within reinforcement learning paradigms, showcasing its potential to accelerate the discovery and optimization of biologically meaningful policies in complex environments like protein folding and drug discovery. This work lays the groundwork for a new generation of biologically informed, symmetry-aware artificial intelligence models.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Step Beyond: Feedthrough &amp; Placement-Aware Rectilinear Floorplanner</title>
<link>https://arxiv.org/abs/2507.14914</link>
<guid>https://arxiv.org/abs/2507.14914</guid>
<content:encoded><![CDATA[
arXiv:2507.14914v1 Announce Type: cross 
Abstract: Floorplanning determines the shapes and locations of modules on a chip canvas and plays a critical role in optimizing the chip's Power, Performance, and Area (PPA) metrics. However, existing floorplanning approaches often fail to integrate with subsequent physical design stages, leading to suboptimal in-module component placement and excessive inter-module feedthrough. To tackle this challenge, we propose Flora, a three-stage feedthrough and placement aware rectilinear floorplanner. In the first stage, Flora employs wiremask and position mask techniques to achieve coarse-grained optimization of HPWL and feedthrough. In the second stage, under the constraint of a fixed outline, Flora achieves a zero-whitespace layout by locally resizing module shapes, thereby performing fine-grained optimization of feedthrough and improving component placement. In the third stage, Flora utilizes a fast tree search-based method to efficiently place components-including macros and standard cells-within each module, subsequently adjusting module boundaries based on the placement results to enable cross-stage optimization. Experimental results show that Flora outperforms recent state-of-the-art floorplanning approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin, 29.15% in FTmod, and a 14% improvement in component placement performance.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Byzantine-Robust Decentralized Coordination of LLM Agents</title>
<link>https://arxiv.org/abs/2507.14928</link>
<guid>https://arxiv.org/abs/2507.14928</guid>
<content:encoded><![CDATA[
arXiv:2507.14928v1 Announce Type: cross 
Abstract: Collaboration among multiple large language model (LLM) agents is a promising approach to overcome inherent limitations of single-agent systems, such as hallucinations and single points of failure. As LLM agents are increasingly deployed on open blockchain platforms, multi-agent systems capable of tolerating malicious (Byzantine) agents have become essential.
  Recent Byzantine-robust multi-agent systems typically rely on leader-driven coordination, which suffers from two major drawbacks. First, they are inherently vulnerable to targeted attacks against the leader. If consecutive leaders behave maliciously, the system repeatedly fails to achieve consensus, forcing new consensus rounds, which is particularly costly given the high latency of LLM invocations. Second, an underperforming proposal from the leader can be accepted as the final answer even when higher-quality alternatives are available, as existing methods finalize the leader's proposal once it receives a quorum of votes.
  To address these issues, we propose DecentLLMs, a novel decentralized consensus approach for multi-agent LLM systems, where worker agents generate answers concurrently and evaluator agents independently score and rank these answers to select the best available one. This decentralized architecture enables faster consensus despite the presence of Byzantine agents and consistently selects higher-quality answers through Byzantine-robust aggregation techniques.
  Experimental results demonstrate that DecentLLMs effectively tolerates Byzantine agents and significantly improves the quality of selected answers.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing EFX via PMMS: (Non-)Existence Results in Discrete Fair Division</title>
<link>https://arxiv.org/abs/2507.14957</link>
<guid>https://arxiv.org/abs/2507.14957</guid>
<content:encoded><![CDATA[
arXiv:2507.14957v1 Announce Type: cross 
Abstract: We study the fair division of indivisible items and provide new insights into the EFX problem, which is widely regarded as the central open question in fair division, and the PMMS problem, a strictly stronger variant of EFX. Our first result constructs a three-agent instance with two monotone valuations and one additive valuation in which no PMMS allocation exists. Since EFX allocations are known to exist under these assumptions, this establishes a formal separation between EFX and PMMS.
  We prove existence of fair allocations for three important special cases. We show that EFX allocations exist for personalized bivalued valuations, where for each agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value $v_i(\{g\}) \in \{a_i, b_i\}$ to each good $g$. We establish an analogous existence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also prove that PMMS allocations exist for binary-valued MMS-feasible valuations, where each bundle $S$ has value $v_i(S) \in \{0, 1\}$. Notably, this result holds even without assuming monotonicity of valuations and thus applies to the fair division of chores and mixed manna. Finally, we study a class of valuations called pair-demand valuations, which extend the well-studied unit-demand valuations to the case where each agent derives value from at most two items, and we show that PMMS allocations exist in this setting. Our proofs are constructive, and we provide polynomial-time algorithms for all three existence results.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Analysis of Statistical and Machine Learning Models for Outlier Detection in Bitcoin Limit Order Books</title>
<link>https://arxiv.org/abs/2507.14960</link>
<guid>https://arxiv.org/abs/2507.14960</guid>
<content:encoded><![CDATA[
arXiv:2507.14960v1 Announce Type: cross 
Abstract: The detection of outliers within cryptocurrency limit order books (LOBs) is of paramount importance for comprehending market dynamics, particularly in highly volatile and nascent regulatory environments. This study conducts a comprehensive comparative analysis of robust statistical methods and advanced machine learning techniques for real-time anomaly identification in cryptocurrency LOBs. Within a unified testing environment, named AITA Order Book Signal (AITA-OBS), we evaluate the efficacy of thirteen diverse models to identify which approaches are most suitable for detecting potentially manipulative trading behaviours. An empirical evaluation, conducted via backtesting on a dataset of 26,204 records from a major exchange, demonstrates that the top-performing model, Empirical Covariance (EC), achieves a 6.70% gain, significantly outperforming a standard Buy-and-Hold benchmark. These findings underscore the effectiveness of outlier-driven strategies and provide insights into the trade-offs between model complexity, trade frequency, and performance. This study contributes to the growing corpus of research on cryptocurrency market microstructure by furnishing a rigorous benchmark of anomaly detection models and highlighting their potential for augmenting algorithmic trading and risk management.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models</title>
<link>https://arxiv.org/abs/2507.14975</link>
<guid>https://arxiv.org/abs/2507.14975</guid>
<content:encoded><![CDATA[
arXiv:2507.14975v1 Announce Type: cross 
Abstract: Autonomous error correction is critical for domestic robots to achieve reliable execution of complex long-horizon tasks. Prior work has explored self-reflection in Large Language Models (LLMs) for task planning error correction; however, existing methods are constrained by inflexible self-reflection mechanisms that limit their effectiveness. Motivated by these limitations and inspired by human cognitive adaptation, we propose the Flexible Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture that enables LLMs to perform flexible self-reflection based on task difficulty, while constructively integrating historical valuable experience with failure lessons. We evaluated FCRF on diverse domestic tasks through simulation in AlfWorld and physical deployment in the real-world environment. Experimental results demonstrate that FCRF significantly improves overall performance and self-reflection flexibility in complex long-horizon robotic tasks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering</title>
<link>https://arxiv.org/abs/2507.15003</link>
<guid>https://arxiv.org/abs/2507.15003</guid>
<content:encoded><![CDATA[
arXiv:2507.15003v1 Announce Type: cross 
Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI teammates: autonomous, goal-driven systems collaborating with human developers. Among these, autonomous coding agents are especially transformative, now actively initiating, reviewing, and evolving code at scale. This paper introduces AIDev, the first large-scale dataset capturing how such agents operate in the wild. Spanning over 456,000 pull requests by five leading agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across 61,000 repositories and 47,000 developers, AIDev provides an unprecedented empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software engineering, AIDev offers structured, open data to support research in benchmarking, agent readiness, optimization, collaboration modeling, and AI governance. The dataset includes rich metadata on PRs, authorship, review timelines, code changes, and integration outcomes--enabling exploration beyond synthetic benchmarks like SWE-bench. For instance, although agents often outperform humans in speed, their PRs are accepted less frequently, revealing a trust and utility gap. Furthermore, while agents accelerate code submission--one developer submitted as many PRs in three days as they had in three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev enables a new generation of research into AI-native workflows and supports building the next wave of symbiotic human-AI collaboration. The dataset is publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering Agent
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of GenAI for Automotive Software Development: From Requirements to Executable Code</title>
<link>https://arxiv.org/abs/2507.15025</link>
<guid>https://arxiv.org/abs/2507.15025</guid>
<content:encoded><![CDATA[
arXiv:2507.15025v1 Announce Type: cross 
Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to revolutionize many industrial areas by reducing the amount of human intervention needed and effort for handling complex underlying processes. Automotive software development is considered to be a significant area for GenAI adoption, taking into account lengthy and expensive procedures, resulting from the amount of requirements and strict standardization. In this paper, we explore the adoption of GenAI for various steps of automotive software development, mainly focusing on requirements handling, compliance aspects and code generation. Three GenAI-related technologies are covered within the state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation (RAG), Vision Language Models (VLMs), as well as overview of adopted prompting techniques in case of code generation. Additionally, we also derive a generalized GenAI-aided automotive software development workflow based on our findings from this literature review. Finally, we include a summary of a survey outcome, which was conducted among our automotive industry partners regarding the type of GenAI tools used for their daily work activities.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The hunt for new pulsating ultraluminous X-ray sources: a clustering approach</title>
<link>https://arxiv.org/abs/2507.15032</link>
<guid>https://arxiv.org/abs/2507.15032</guid>
<content:encoded><![CDATA[
arXiv:2507.15032v1 Announce Type: cross 
Abstract: The discovery of fast and variable coherent signals in a handful of ultraluminous X-ray sources (ULXs) testifies to the presence of super-Eddington accreting neutron stars, and drastically changed the understanding of the ULX class. Our capability of discovering pulsations in ULXs is limited, among others, by poor statistics. However, catalogues and archives of high-energy missions contain information which can be used to identify new candidate pulsating ULXs (PULXs). The goal of this research is to single out candidate PULXs among those ULXs which have not shown pulsations due to an unfavourable combination of factors. We applied an AI approach to an updated database of ULXs detected by XMM-Newton. We first used an unsupervised clustering algorithm to sort out sources with similar characteristics into two clusters. Then, the sample of known PULX observations has been used to set the separation threshold between the two clusters and to identify the one containing the new candidate PULXs. We found that only a few criteria are needed to assign the membership of an observation to one of the two clusters. The cluster of new candidate PULXs counts 85 unique sources for 355 observations, with $\sim$85% of these new candidates having multiple observations. A preliminary timing analysis found no new pulsations for these candidates. This work presents a sample of new candidate PULXs observed by XMM-Newton, the properties of which are similar (in a multi-dimensional phase space) to those of the known PULXs, despite the absence of pulsations in their light curves. While this result is a clear example of the predictive power of AI-based methods, it also highlights the need for high-statistics observational data to reveal coherent signals from the sources in this sample and thus validate the robustness of the approach.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization</title>
<link>https://arxiv.org/abs/2507.15061</link>
<guid>https://arxiv.org/abs/2507.15061</guid>
<content:encoded><![CDATA[
arXiv:2507.15061v1 Announce Type: cross 
Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper</title>
<link>https://arxiv.org/abs/2507.15062</link>
<guid>https://arxiv.org/abs/2507.15062</guid>
<content:encoded><![CDATA[
arXiv:2507.15062v1 Announce Type: cross 
Abstract: Handheld grippers are increasingly used to collect human demonstrations due to their ease of deployment and versatility. However, most existing designs lack tactile sensing, despite the critical role of tactile feedback in precise manipulation. We present a portable, lightweight gripper with integrated tactile sensors that enables synchronized collection of visual and tactile data in diverse, real-world, and in-the-wild settings. Building on this hardware, we propose a cross-modal representation learning framework that integrates visual and tactile signals while preserving their distinct characteristics. The learning procedure allows the emergence of interpretable representations that consistently focus on contacting regions relevant for physical interactions. When used for downstream manipulation tasks, these representations enable more efficient and effective policy learning, supporting precise robotic manipulation based on multimodal feedback. We validate our approach on fine-grained tasks such as test tube insertion and pipette-based fluid transfer, demonstrating improved accuracy and robustness under external disturbances. Our project page is available at https://binghao-huang.github.io/touch_in_the_wild/ .
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation</title>
<link>https://arxiv.org/abs/2507.15064</link>
<guid>https://arxiv.org/abs/2507.15064</guid>
<content:encoded><![CDATA[
arXiv:2507.15064v1 Announce Type: cross 
Abstract: Current diffusion models for human image animation often struggle to maintain identity (ID) consistency, especially when the reference image and driving video differ significantly in body size or position. We introduce StableAnimator++, the first ID-preserving video diffusion framework with learnable pose alignment, capable of generating high-quality videos conditioned on a reference image and a pose sequence without any post-processing. Building upon a video diffusion model, StableAnimator++ contains carefully designed modules for both training and inference, striving for identity consistency. In particular, StableAnimator++ first uses learnable layers to predict the similarity transformation matrices between the reference image and the driven poses via injecting guidance from Singular Value Decomposition (SVD). These matrices align the driven poses with the reference image, mitigating misalignment to a great extent. StableAnimator++ then computes image and face embeddings using off-the-shelf encoders, refining the face embeddings via a global content-aware Face Encoder. To further maintain ID, we introduce a distribution-aware ID Adapter that counteracts interference caused by temporal layers while preserving ID via distribution alignment. During the inference stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization integrated into the denoising process, guiding the diffusion trajectory for enhanced facial fidelity. Experiments on benchmarks show the effectiveness of StableAnimator++ both qualitatively and quantitatively.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback</title>
<link>https://arxiv.org/abs/2507.15066</link>
<guid>https://arxiv.org/abs/2507.15066</guid>
<content:encoded><![CDATA[
arXiv:2507.15066v1 Announce Type: cross 
Abstract: Time series anomaly detection is critical across various domains, yet current approaches often limit analysis to mere binary anomaly classification without detailed categorization or further explanatory reasoning. To address these limitations, we propose a novel task, Time-series Reasoning for Anomaly (Time-RA) that transforms classical time series anomaly detection from a discriminative into a generative, reasoning-intensive task leveraging Large Language Models (LLMs). Also, we introduce the first real-world multimodal benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning, comprising approximately 40,000 samples across 10 real-world domains. Each sample includes numeric time series data, contextual text information, and visual representations, each annotated with fine-grained categories (14 types for univariate anomalies and 6 for multivariate anomalies) and structured explanatory reasoning. We develop a sophisticated annotation framework utilizing ensemble-generated labels refined through GPT-4-driven feedback, ensuring accuracy and interpretability. Extensive benchmarking of LLMs and multimodal LLMs demonstrates the capabilities and limitations of current models, highlighting the critical role of supervised fine-tuning. Our dataset and task pave the way for significant advancements in interpretable time series anomaly detection and reasoning.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments</title>
<link>https://arxiv.org/abs/2507.15072</link>
<guid>https://arxiv.org/abs/2507.15072</guid>
<content:encoded><![CDATA[
arXiv:2507.15072v1 Announce Type: cross 
Abstract: Industrial warehouses are congested with moving forklifts, shelves and personnel, making robot teleoperation particularly risky and demanding for blind and low-vision (BLV) operators. Although accessible teleoperation plays a key role in inclusive workforce participation, systematic research on its use in industrial environments is limited, and few existing studies barely address multimodal guidance designed for BLV users. We present a novel multimodal guidance simulator that enables BLV users to control a mobile robot through a high-fidelity warehouse environment while simultaneously receiving synchronized visual, auditory, and haptic feedback. The system combines a navigation mesh with regular re-planning so routes remain accurate avoiding collisions as forklifts and human avatars move around the warehouse. Users with low vision are guided with a visible path line towards destination; navigational voice cues with clockwise directions announce upcoming turns, and finally proximity-based haptic feedback notifies the users of static and moving obstacles in the path. This real-time, closed-loop system offers a repeatable testbed and algorithmic reference for accessible teleoperation research. The simulator's design principles can be easily adapted to real robots due to the alignment of its navigation, speech, and haptic modules with commercial hardware, supporting rapid feasibility studies and deployment of inclusive telerobotic tools in actual warehouses.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Control with Gradient Uncertainty</title>
<link>https://arxiv.org/abs/2507.15082</link>
<guid>https://arxiv.org/abs/2507.15082</guid>
<content:encoded><![CDATA[
arXiv:2507.15082v1 Announce Type: cross 
Abstract: We introduce a novel extension to robust control theory that explicitly addresses uncertainty in the value function's gradient, a form of uncertainty endemic to applications like reinforcement learning where value functions are approximated. We formulate a zero-sum dynamic game where an adversary perturbs both system dynamics and the value function gradient, leading to a new, highly nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness by proving a comparison principle for its viscosity solutions under a uniform ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a key insight: we prove that the classical quadratic value function assumption fails for any non-zero gradient uncertainty, fundamentally altering the problem structure. A formal perturbation analysis characterizes the non-polynomial correction to the value function and the resulting nonlinearity of the optimal control law, which we validate with numerical studies. Finally, we bridge theory to practice by proposing a novel Gradient-Uncertainty-Robust Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating its effectiveness in stabilizing training. This work provides a new direction for robust control, holding significant implications for fields where function approximation is common, including reinforcement learning and computational finance.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling</title>
<link>https://arxiv.org/abs/2507.15087</link>
<guid>https://arxiv.org/abs/2507.15087</guid>
<content:encoded><![CDATA[
arXiv:2507.15087v1 Announce Type: cross 
Abstract: Currently, many studies view DNA sequences as a special type of language and utilize Transformers to model them. These studies use fixed-length k-mer segmentation and BPE subword tokenization but lack a systematic evaluation to determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a 4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal, AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and 24-layer Transformer encoders and evaluated on GUE benchmark dataset. In general, BPE delivers higher and more stable performance across tasks by compressing frequent motifs into variable-length tokens, reducing sequence length, and improving model generalization. RoPE excels at capturing periodic motifs and extrapolating to long sequences, while AliBi also performs well on tasks driven by local dependencies. In terms of depth, we observe significant gains when increasing layers from 3 to 12, with only marginal improvements or slight overfitting at 24 layers. This study provides practical guidance for designing tokenization and positional encoding in DNA Transformer models.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking</title>
<link>https://arxiv.org/abs/2507.15094</link>
<guid>https://arxiv.org/abs/2507.15094</guid>
<content:encoded><![CDATA[
arXiv:2507.15094v1 Announce Type: cross 
Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses significant risks, demanding precise, real-time localization and continuous monitoring of the bleeding source for effective hemostatic intervention. In particular, endoscopists have to repeatedly flush to clear blood, allowing only milliseconds to identify bleeding sources, an inefficient process that prolongs operations and elevates patient risks. However, current Artificial Intelligence (AI) methods primarily focus on bleeding region segmentation, overlooking the critical need for accurate bleeding source detection and temporal tracking in the challenging ESD environment, which is marked by frequent visual obstructions and dynamic scene changes. This gap is widened by the lack of specialized datasets, hindering the development of robust AI-assisted guidance systems. To address these challenges, we introduce BleedOrigin-Bench, the first comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated bleeding sources across 106,222 frames from 44 procedures, supplemented with 39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6 challenging clinical scenarios. We also present BleedOrigin-Net, a novel dual-stage detection-tracking framework for the bleeding source localization in ESD procedures, addressing the complete workflow from bleeding onset detection to continuous spatial tracking. We compare with widely-used object detection models (YOLOv11/v12), multimodal large language models, and point tracking methods. Extensive evaluation demonstrates state-of-the-art performance, achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?</title>
<link>https://arxiv.org/abs/2507.15100</link>
<guid>https://arxiv.org/abs/2507.15100</guid>
<content:encoded><![CDATA[
arXiv:2507.15100v1 Announce Type: cross 
Abstract: Natural Language Inference (NLI) is the task of determining the semantic entailment of a premise for a given hypothesis. The task aims to develop systems that emulate natural human inferential processes where commonsense knowledge plays a major role. However, existing commonsense resources lack sufficient coverage for a variety of premise-hypothesis pairs. This study explores the potential of Large Language Models as commonsense knowledge generators for NLI along two key dimensions: their reliability in generating such knowledge and the impact of that knowledge on prediction accuracy. We adapt and modify existing metrics to assess LLM factuality and consistency in generating in this context. While explicitly incorporating commonsense knowledge does not consistently improve overall results, it effectively helps distinguish entailing instances and moderately improves distinguishing contradictory and neutral inferences.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI</title>
<link>https://arxiv.org/abs/2507.15104</link>
<guid>https://arxiv.org/abs/2507.15104</guid>
<content:encoded><![CDATA[
arXiv:2507.15104v1 Announce Type: cross 
Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize analog design automation through data-driven approaches. In particular, researchers are increasingly fascinated by harnessing the power of generative AI to automate the discovery of novel analog circuit topologies. Unlocking the full potential of generative AI in these data-driven discoveries requires access to large and diverse datasets.Yet, there is a significant barrier in the analog domain--Analog circuit design is inherently proprietary, involving not only confidential circuit structures but also the underlying commercial semiconductor processes. As a result, current generative AI research is largely confined to individual researchers who construct small, narrowly focused private datasets. This fragmentation severely limits collaborative innovation and impedes progress across the research community. To address these challenges, we propose AnalogFed. AnalogFed enables collaborative topology discovery across decentralized clients (e.g., individual researchers or institutions) without requiring the sharing of raw private data. To make this vision practical, we introduce a suite of techniques tailored to the unique challenges of applying FedL in analog design--from generative model development and data heterogeneity handling to privacy-preserving strategies that ensure both flexibility and security for circuit designers and semiconductor manufacturers. Extensive experiments across varying client counts and dataset sizes demonstrate that AnalogFed achieves performance comparable to centralized baselines--while maintaining strict data privacy. Specifically, the generative AI model within AnalogFed achieves state-of-the-art efficiency and scalability in the design of analog circuit topologies.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script</title>
<link>https://arxiv.org/abs/2507.15142</link>
<guid>https://arxiv.org/abs/2507.15142</guid>
<content:encoded><![CDATA[
arXiv:2507.15142v1 Announce Type: cross 
Abstract: Homophone normalization, where characters that have the same sound in a writing script are mapped to one character, is a pre-processing step applied in Amharic Natural Language Processing (NLP) literature. While this may improve performance reported by automatic metrics, it also results in models that are not able to understand different forms of writing in a single language. Further, there might be impacts in transfer learning, where models trained on normalized data do not generalize well to other languages. In this paper, we experiment with monolingual training and cross-lingual transfer to understand the impacts of normalization on languages that use the Ge'ez script. We then propose a post-inference intervention in which normalization is applied to model predictions instead of training data. With our simple scheme of post-inference normalization, we show that we can achieve an increase in BLEU score of up to 1.03 while preserving language features in training. Our work contributes to the broader discussion on technology-facilitated language change and calls for more language-aware interventions.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design of an Edge-based Portable EHR System for Anemia Screening in Remote Health Applications</title>
<link>https://arxiv.org/abs/2507.15146</link>
<guid>https://arxiv.org/abs/2507.15146</guid>
<content:encoded><![CDATA[
arXiv:2507.15146v1 Announce Type: cross 
Abstract: The design of medical systems for remote, resource-limited environments faces persistent challenges due to poor interoperability, lack of offline support, and dependency on costly infrastructure. Many existing digital health solutions neglect these constraints, limiting their effectiveness for frontline health workers in underserved regions. This paper presents a portable, edge-enabled Electronic Health Record platform optimized for offline-first operation, secure patient data management, and modular diagnostic integration. Running on small-form factor embedded devices, it provides AES-256 encrypted local storage with optional cloud synchronization for interoperability. As a use case, we integrated a non-invasive anemia screening module leveraging fingernail pallor analysis. Trained on 250 patient cases (27\% anemia prevalence) with KDE-balanced data, the Random Forest model achieved a test RMSE of 1.969 g/dL and MAE of 1.490 g/dL. A severity-based model reached 79.2\% sensitivity. To optimize performance, a YOLOv8n-based nail bed detector was quantized to INT8, reducing inference latency from 46.96 ms to 21.50 ms while maintaining mAP@0.5 at 0.995. The system emphasizes low-cost deployment, modularity, and data privacy compliance (HIPAA/GDPR), addressing critical barriers to digital health adoption in disconnected settings. Our work demonstrates a scalable approach to enhance portable health information systems and support frontline healthcare in underserved regions.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection</title>
<link>https://arxiv.org/abs/2507.15151</link>
<guid>https://arxiv.org/abs/2507.15151</guid>
<content:encoded><![CDATA[
arXiv:2507.15151v1 Announce Type: cross 
Abstract: Anemia is a widespread global health issue, particularly among young children in low-resource settings. Traditional methods for anemia detection often require expensive equipment and expert knowledge, creating barriers to early and accurate diagnosis. To address these challenges, we explore the use of deep learning models for detecting anemia through conjunctival pallor, focusing on the CP-AnemiC dataset, which includes 710 images from children aged 6-59 months. The dataset is annotated with hemoglobin levels, gender, age and other demographic data, enabling the development of machine learning models for accurate anemia detection. We use the MobileNet architecture as a backbone, known for its efficiency in mobile and embedded vision applications, and fine-tune our model end-to-end using data augmentation techniques and a cross-validation strategy. Our model implementation achieved an accuracy of 0.9313, a precision of 0.9374, and an F1 score of 0.9773 demonstrating strong performance on the dataset. To optimize the model for deployment on edge devices, we performed post-training quantization, evaluating the impact of different bit-widths (FP32, FP16, INT8, and INT4) on model performance. Preliminary results suggest that while FP16 quantization maintains high accuracy (0.9250), precision (0.9370), and F1 Score (0.9377), more aggressive quantization (INT8 and INT4) leads to significant performance degradation. Overall, our study supports further exploration of quantization schemes and hardware optimizations to assess trade-offs between model size, inference time, and diagnostic accuracy in mobile healthcare applications.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction</title>
<link>https://arxiv.org/abs/2507.15152</link>
<guid>https://arxiv.org/abs/2507.15152</guid>
<content:encoded><![CDATA[
arXiv:2507.15152v1 Announce Type: cross 
Abstract: Automating data extraction from full-text randomised controlled trials (RCTs) for meta-analysis remains a significant challenge. This study evaluates the practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini) across tasks involving statistical results, risk-of-bias assessments, and study-level characteristics in three medical domains: hypertension, diabetes, and orthopaedics. We tested four distinct prompting strategies (basic prompting, self-reflective prompting, model ensemble, and customised prompts) to determine how to improve extraction quality. All models demonstrate high precision but consistently suffer from poor recall by omitting key information. We found that customised prompts were the most effective, boosting recall by up to 15\%. Based on this analysis, we propose a three-tiered set of guidelines for using LLMs in data extraction, matching data types to appropriate levels of automation based on task complexity and risk. Our study offers practical advice for automating data extraction in real-world meta-analyses, balancing LLM efficiency with expert oversight through targeted, task-specific automation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification</title>
<link>https://arxiv.org/abs/2507.15156</link>
<guid>https://arxiv.org/abs/2507.15156</guid>
<content:encoded><![CDATA[
arXiv:2507.15156v1 Announce Type: cross 
Abstract: We investigate multi-label classification involving large sets of labels, where the output labels may be known to satisfy some logical constraints. We look at an architecture in which classifiers for individual labels are fed into an expressive sequential model, which produces a joint distribution. One of the potential advantages for such an expressive model is its ability to modelling correlations, as can arise from constraints. We empirically demonstrate the ability of the architecture both to exploit constraints in training and to enforce constraints at inference time.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Generate User Stories and Assess Their Quality?</title>
<link>https://arxiv.org/abs/2507.15157</link>
<guid>https://arxiv.org/abs/2507.15157</guid>
<content:encoded><![CDATA[
arXiv:2507.15157v1 Announce Type: cross 
Abstract: Requirements elicitation is still one of the most challenging activities of the requirements engineering process due to the difficulty requirements analysts face in understanding and translating complex needs into concrete requirements. In addition, specifying high-quality requirements is crucial, as it can directly impact the quality of the software to be developed. Although automated tools allow for assessing the syntactic quality of requirements, evaluating semantic metrics (e.g., language clarity, internal consistency) remains a manual and time-consuming activity. This paper explores how LLMs can help automate requirements elicitation within agile frameworks, where requirements are defined as user stories (US). We used 10 state-of-the-art LLMs to investigate their ability to generate US automatically by emulating customer interviews. We evaluated the quality of US generated by LLMs, comparing it with the quality of US generated by humans (domain experts and students). We also explored whether and how LLMs can be used to automatically evaluate the semantic quality of US. Our results indicate that LLMs can generate US similar to humans in terms of coverage and stylistic quality, but exhibit lower diversity and creativity. Although LLM-generated US are generally comparable in quality to those created by humans, they tend to meet the acceptance quality criteria less frequently, regardless of the scale of the LLM model. Finally, LLMs can reliably assess the semantic quality of US when provided with clear evaluation criteria and have the potential to reduce human effort in large-scale assessments.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT</title>
<link>https://arxiv.org/abs/2507.15193</link>
<guid>https://arxiv.org/abs/2507.15193</guid>
<content:encoded><![CDATA[
arXiv:2507.15193v1 Announce Type: cross 
Abstract: Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is essential for tumor burden estimation, prognosis, and treatment planning. It may also help infer genetic clusters, reducing reliance on expensive testing. This study systematically evaluates anatomical priors to identify configurations that improve deep learning-based PCC segmentation. We employed the nnU-Net framework to evaluate eleven annotation strategies for accurate 3D segmentation of pheochromocytoma, introducing a set of novel multi-class schemes based on organ-specific anatomical priors. These priors were derived from adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen, kidney, aorta, adrenal gland, and pancreas), and were compared against a broad body-region prior used in previous work. The framework was trained and tested on 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center. Performance was measured using Dice Similarity Coefficient (DSC), Normalized Surface Distance (NSD), and instance-wise F1 score. Among all strategies, the Tumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation accuracy, significantly outperforming the previously used Tumor + Body (TB) annotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84% improvement at an IoU threshold of 0.5), measured on a 70-30 train-test split. The TKA model also showed superior tumor burden quantification (R^2 = 0.968) and strong segmentation across all genetic subtypes. In five-fold cross-validation, TKA consistently outperformed TB across IoU thresholds (0.1 to 0.5), reinforcing its robustness and generalizability. These findings highlight the value of incorporating relevant anatomical context in deep learning models to achieve precise PCC segmentation, supporting clinical assessment and longitudinal monitoring.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation</title>
<link>https://arxiv.org/abs/2507.15205</link>
<guid>https://arxiv.org/abs/2507.15205</guid>
<content:encoded><![CDATA[
arXiv:2507.15205v1 Announce Type: cross 
Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging task. This paper proposes a novel multimodal approach, the Long-Short Distance Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it constructs a long-distance graph neural network and a short-distance graph neural network to obtain multimodal features of distant and nearby utterances, respectively. To ensure that long- and short-distance features are as distinct as possible in representation while enabling mutual influence between the two modules, we employ a Differential Regularizer and incorporate a BiAffine Module to facilitate feature interaction. In addition, we propose an Improved Curriculum Learning (ICL) to address the challenge of data imbalance. By computing the similarity between different emotions to emphasize the shifts in similar emotions, we design a "weighted emotional shift" metric and develop a difficulty measurer, enabling a training process that prioritizes learning easy samples before harder ones. Experimental results on the IEMOCAP and MELD datasets demonstrate that our model outperforms existing benchmarks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptArmor: Simple yet Effective Prompt Injection Defenses</title>
<link>https://arxiv.org/abs/2507.15219</link>
<guid>https://arxiv.org/abs/2507.15219</guid>
<content:encoded><![CDATA[
arXiv:2507.15219v1 Announce Type: cross 
Abstract: Despite their potential, recent research has demonstrated that LLM agents are vulnerable to prompt injection attacks, where malicious prompts are injected into the agent's input, causing it to perform an attacker-specified task rather than the intended task provided by the user. In this paper, we present PromptArmor, a simple yet effective defense against prompt injection attacks. Specifically, PromptArmor prompts an off-the-shelf LLM to detect and remove potential injected prompts from the input before the agent processes it. Our results show that PromptArmor can accurately identify and remove injected prompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves both a false positive rate and a false negative rate below 1% on the AgentDojo benchmark. Moreover, after removing injected prompts with PromptArmor, the attack success rate drops to below 1%. We also demonstrate PromptArmor's effectiveness against adaptive attacks and explore different strategies for prompting an LLM. We recommend that PromptArmor be adopted as a standard baseline for evaluating new defenses against prompt injection attacks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation</title>
<link>https://arxiv.org/abs/2507.15224</link>
<guid>https://arxiv.org/abs/2507.15224</guid>
<content:encoded><![CDATA[
arXiv:2507.15224v1 Announce Type: cross 
Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler intrinsics are widely supported by modern processors to accelerate performance-critical tasks. SIMD intrinsic programming, a trade-off between coding productivity and high performance, is widely used in the development of mainstream performance-critical libraries and daily computing tasks. Large Language Models (LLMs), which have demonstrated strong and comprehensive capabilities in code generation, show promise in assisting programmers with the challenges of SIMD intrinsic programming. However, existing code-generation benchmarks focus on only scalar code, and it is unclear how LLMs perform in generating vectorized code using SIMD intrinsics. To fill this gap, we propose SimdBench, the first code benchmark specifically designed for SIMD-intrinsic code generation, comprising 136 carefully crafted tasks and targeting five representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86 Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a systematic evaluation (measuring both correctness and performance) of 18 representative LLMs on SimdBench, resulting in a series of novel and insightful findings. Our evaluation results demonstrate that LLMs exhibit a universal decrease in pass@k during SIMD-intrinsic code generation compared to scalar-code generation. Our in-depth analysis highlights promising directions for the further advancement of LLMs in the challenging domain of SIMD-intrinsic code generation. SimdBench is fully open source at https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader research community.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation</title>
<link>https://arxiv.org/abs/2507.15243</link>
<guid>https://arxiv.org/abs/2507.15243</guid>
<content:encoded><![CDATA[
arXiv:2507.15243v1 Announce Type: cross 
Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model pre-trained with DINO combined with a prototypical classifier outperforms the latest SOTA methods. A crucial limitation that needs to be overcome is that updating too many parameters of the transformers leads to overfitting due to the scarcity of labeled samples. To address this challenge, we propose a new concept, Coalescent Projection (CP), as an effective successor to soft prompts. Additionally, we propose a novel pseudo-class generation method combined with Self-Supervised Transformations (SSTs) that relies solely on the base domain to prepare the network for encountering unseen samples from different domains. The proposed method exhibits its effectiveness in comprehensive experiments on the extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published at https://github.com/Naeem-Paeedeh/CPLSR.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search</title>
<link>https://arxiv.org/abs/2507.15245</link>
<guid>https://arxiv.org/abs/2507.15245</guid>
<content:encoded><![CDATA[
arXiv:2507.15245v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have opened new opportunities for academic literature retrieval. However, existing systems often rely on rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR, a multi-agent framework that incorporates RefChain-based query decomposition and query evolution to enable more flexible and effective search. To facilitate systematic evaluation, we also construct SPARBench, a challenging benchmark with expert-annotated relevance labels. Experimental results demonstrate that SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline. Together, SPAR and SPARBench provide a scalable, interpretable, and high-performing foundation for advancing research in scholarly retrieval. Code and data will be available at: https://github.com/xiaofengShi/SPAR
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks</title>
<link>https://arxiv.org/abs/2507.15246</link>
<guid>https://arxiv.org/abs/2507.15246</guid>
<content:encoded><![CDATA[
arXiv:2507.15246v1 Announce Type: cross 
Abstract: Accurate demand forecasting is critical for enhancing the efficiency and responsiveness of food delivery platforms, where spatial heterogeneity and temporal fluctuations in order volumes directly influence operational decisions. This paper proposes an attention-based Graph Neural Network framework that captures spatial-temporal dependencies by modeling the food delivery environment as a graph. In this graph, nodes represent urban delivery zones, while edges reflect spatial proximity and inter-regional order flow patterns derived from historical data. The attention mechanism dynamically weighs the influence of neighboring zones, enabling the model to focus on the most contextually relevant areas during prediction. Temporal trends are jointly learned alongside spatial interactions, allowing the model to adapt to evolving demand patterns. Extensive experiments on real-world food delivery datasets demonstrate the superiority of the proposed model in forecasting future order volumes with high accuracy. The framework offers a scalable and adaptive solution to support proactive fleet positioning, resource allocation, and dispatch optimization in urban food delivery operations.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks</title>
<link>https://arxiv.org/abs/2507.15254</link>
<guid>https://arxiv.org/abs/2507.15254</guid>
<content:encoded><![CDATA[
arXiv:2507.15254v1 Announce Type: cross 
Abstract: The evolution towards future generation of mobile systems and fixed wireless networks is primarily driven by the urgency to support high-bandwidth and low-latency services across various vertical sectors. This endeavor is fueled by smartphones as well as technologies like industrial internet of things, extended reality (XR), and human-to-machine (H2M) collaborations for fostering industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To ensure an ideal immersive experience and avoid cyber-sickness for users in all the aforementioned usage scenarios, it is typically challenging to synchronize XR content from a remote machine to a human collaborator according to their head movements across a large geographic span in real-time over communication networks. Thus, we propose a novel H2M collaboration scheme where the human's head movements are predicted ahead with highly accurate models like bidirectional long short-term memory networks to orient the machine's camera in advance. We validate that XR frame size varies in accordance with the human's head movements and predict the corresponding bandwidth requirements from the machine's camera to propose a human-machine coordinated dynamic bandwidth allocation (HMC-DBA) scheme. Through extensive simulations, we show that end-to-end latency and jitter requirements of XR frames are satisfied with much lower bandwidth consumption over enterprise networks like Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in network resource utilization is achieved by employing our proposed HMC-DBA over state-of-the-art schemes.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEETI: A Multimodal ECG Dataset from MIMIC-IV-ECG with Signals, Images, Features and Interpretations</title>
<link>https://arxiv.org/abs/2507.15255</link>
<guid>https://arxiv.org/abs/2507.15255</guid>
<content:encoded><![CDATA[
arXiv:2507.15255v1 Announce Type: cross 
Abstract: Electrocardiogram (ECG) plays a foundational role in modern cardiovascular care, enabling non-invasive diagnosis of arrhythmias, myocardial ischemia, and conduction disorders. While machine learning has achieved expert-level performance in ECG interpretation, the development of clinically deployable multimodal AI systems remains constrained, primarily due to the lack of publicly available datasets that simultaneously incorporate raw signals, diagnostic images, and interpretation text. Most existing ECG datasets provide only single-modality data or, at most, dual modalities, making it difficult to build models that can understand and integrate diverse ECG information in real-world settings. To address this gap, we introduce MEETI (MIMIC-IV-Ext ECG-Text-Image), the first large-scale ECG dataset that synchronizes raw waveform data, high-resolution plotted images, and detailed textual interpretations generated by large language models. In addition, MEETI includes beat-level quantitative ECG parameters extracted from each lead, offering structured parameters that support fine-grained analysis and model interpretability. Each MEETI record is aligned across four components: (1) the raw ECG waveform, (2) the corresponding plotted image, (3) extracted feature parameters, and (4) detailed interpretation text. This alignment is achieved using consistent, unique identifiers. This unified structure supports transformer-based multimodal learning and supports fine-grained, interpretable reasoning about cardiac health. By bridging the gap between traditional signal analysis, image-based interpretation, and language-driven understanding, MEETI established a robust foundation for the next generation of explainable, multimodal cardiovascular AI. It offers the research community a comprehensive benchmark for developing and evaluating ECG-based AI systems.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transceiver Design in Over-the-Air Federated Distillation</title>
<link>https://arxiv.org/abs/2507.15256</link>
<guid>https://arxiv.org/abs/2507.15256</guid>
<content:encoded><![CDATA[
arXiv:2507.15256v1 Announce Type: cross 
Abstract: The rapid proliferation and growth of artificial intelligence (AI) has led to the development of federated learning (FL). FL allows wireless devices (WDs) to cooperatively learn by sharing only local model parameters, without needing to share the entire dataset. However, the emergence of large AI models has made existing FL approaches inefficient, due to the significant communication overhead required. In this paper, we propose a novel over-the-air federated distillation (FD) framework by synergizing the strength of FL and knowledge distillation to avoid the heavy local model transmission. Instead of sharing the model parameters, only the WDs' model outputs, referred to as knowledge, are shared and aggregated over-the-air by exploiting the superposition property of the multiple-access channel. We shall study the transceiver design in over-the-air FD, aiming to maximize the learning convergence rate while meeting the power constraints of the transceivers. The main challenge lies in the intractability of the learning performance analysis, as well as the non-convex nature and the optimization spanning the whole FD training period. To tackle this problem, we first derive an analytical expression of the convergence rate in over-the-air FD. Then, the closed-form optimal solutions of the WDs' transmit power and the estimator for over-the-air aggregation are obtained given the receiver combining strategy. Accordingly, we put forth an efficient approach to find the optimal receiver beamforming vector via semidefinite relaxation. We further prove that there is no optimality gap between the original and relaxed problem for the receiver beamforming design. Numerical results will show that the proposed over-the-air FD approach achieves a significant reduction in communication overhead, with only a minor compromise in testing accuracy compared to conventional FL benchmarks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Video Generation for High-Efficiency Video Compression</title>
<link>https://arxiv.org/abs/2507.15269</link>
<guid>https://arxiv.org/abs/2507.15269</guid>
<content:encoded><![CDATA[
arXiv:2507.15269v1 Announce Type: cross 
Abstract: Perceptual studies demonstrate that conditional diffusion models excel at reconstructing video content aligned with human visual perception. Building on this insight, we propose a video compression framework that leverages conditional diffusion models for perceptually optimized reconstruction. Specifically, we reframe video compression as a conditional generation task, where a generative model synthesizes video from sparse, yet informative signals. Our approach introduces three key modules: (1) Multi-granular conditioning that captures both static scene structure and dynamic spatio-temporal cues; (2) Compact representations designed for efficient transmission without sacrificing semantic richness; (3) Multi-condition training with modality dropout and role-aware embeddings, which prevent over-reliance on any single modality and enhance robustness. Extensive experiments show that our method significantly outperforms both traditional and neural codecs on perceptual quality metrics such as Fr\'echet Video Distance (FVD) and LPIPS, especially under high compression ratios.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A2TTS: TTS for Low Resource Indian Languages</title>
<link>https://arxiv.org/abs/2507.15272</link>
<guid>https://arxiv.org/abs/2507.15272</guid>
<content:encoded><![CDATA[
arXiv:2507.15272v1 Announce Type: cross 
Abstract: We present a speaker conditioned text-to-speech (TTS) system aimed at addressing challenges in generating speech for unseen speakers and supporting diverse Indian languages. Our method leverages a diffusion-based TTS architecture, where a speaker encoder extracts embeddings from short reference audio samples to condition the DDPM decoder for multispeaker generation. To further enhance prosody and naturalness, we employ a cross-attention based duration prediction mechanism that utilizes reference audio, enabling more accurate and speaker consistent timing. This results in speech that closely resembles the target speaker while improving duration modeling and overall expressiveness. Additionally, to improve zero-shot generation, we employed classifier free guidance, allowing the system to generate speech more near speech for unknown speakers. Using this approach, we trained language-specific speaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian languages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and Tamil.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Self-Evolution Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2507.15281</link>
<guid>https://arxiv.org/abs/2507.15281</guid>
<content:encoded><![CDATA[
arXiv:2507.15281v1 Announce Type: cross 
Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent by pre-training, so some researchers optimize LLMs through post-training. Existing post-training strategies, such as memory-based retrieval or preference optimization, improve user alignment yet fail to enhance the model's domain cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution (DPSE) framework that jointly optimizes user preference adaptation and domain-specific competence. DPSE introduces a Censor module to extract multi-dimensional interaction signals and estimate satisfaction scores, which guide structured data expansion via topic-aware and preference-driven strategies. These expanded datasets support a two-stage fine-tuning pipeline: supervised domain grounding followed by frequency-aware preference optimization. Experiments across general NLP benchmarks and long-term dialogue tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning, Preference Optimization, and Memory-Augmented baselines. Ablation studies validate the contribution of each module. In this way, our framework provides an autonomous path toward continual self-evolution of LLMs.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.15287</link>
<guid>https://arxiv.org/abs/2507.15287</guid>
<content:encoded><![CDATA[
arXiv:2507.15287v1 Announce Type: cross 
Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to learn from reward-free interactions and alternative supervision signals, such as unlabeled or incomplete demonstrations, rather than relying solely on explicit reward maximization. Additionally, developing generalist agents that can adapt efficiently in real-world environments often requires leveraging these reward-free signals to guide learning and behavior. However, while intrinsic motivation techniques provide a means for agents to seek out novel or uncertain states in the absence of explicit rewards, they are often challenged by dense reward environments or the complexity of high-dimensional state and action spaces. Furthermore, most existing approaches rely directly on the unprocessed intrinsic reward signals, which can make it difficult to shape or control the agent's exploration effectively. We propose a framework that can effectively utilize expert demonstrations, even when they are incomplete and imperfect. By applying a mapping function to transform the similarity between an agent's state and expert data into a shaped intrinsic reward, our method allows for flexible and targeted exploration of expert-like behaviors. We employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors and accommodate missing information in demonstrations. Experiments show our approach enables robust exploration and strong performance in both sparse and dense reward environments, even when demonstrations are sparse or incomplete. This provides a practical framework for RL in realistic settings where optimal data is unavailable and precise reward control is needed.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preferential subspace identification (PSID) with forward-backward smoothing</title>
<link>https://arxiv.org/abs/2507.15288</link>
<guid>https://arxiv.org/abs/2507.15288</guid>
<content:encoded><![CDATA[
arXiv:2507.15288v1 Announce Type: cross 
Abstract: System identification methods for multivariate time-series, such as neural and behavioral recordings, have been used to build models for predicting one from the other. For example, Preferential Subspace Identification (PSID) builds a state-space model of a primary time-series (e.g., neural activity) to optimally predict a secondary time-series (e.g., behavior). However, PSID focuses on optimal prediction using past primary data, even though in offline applications, better estimation can be achieved by incorporating concurrent data (filtering) or all available data (smoothing). Here, we extend PSID to enable optimal filtering and smoothing. First, we show that the presence of a secondary signal makes it possible to uniquely identify a model with an optimal Kalman update step (to enable filtering) from a family of otherwise equivalent state-space models. Our filtering solution augments PSID with a reduced-rank regression step that directly learns the optimal gain required for the update step from data. We refer to this extension of PSID as PSID with filtering. Second, inspired by two-filter Kalman smoother formulations, we develop a novel forward-backward PSID smoothing algorithm where we first apply PSID with filtering and then apply it again in the reverse time direction on the residuals of the filtered secondary signal. We validate our methods on simulated data, showing that our approach recovers the ground-truth model parameters for filtering, and achieves optimal filtering and smoothing decoding performance of the secondary signal that matches the ideal performance of the true underlying model. This work provides a principled framework for optimal linear filtering and smoothing in the two-signal setting, significantly expanding the toolkit for analyzing dynamic interactions in multivariate time-series.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro</title>
<link>https://arxiv.org/abs/2507.15292</link>
<guid>https://arxiv.org/abs/2507.15292</guid>
<content:encoded><![CDATA[
arXiv:2507.15292v1 Announce Type: cross 
Abstract: Visualizing subtle vascular motions in endoscopic surgery is crucial for surgical precision and decision-making, yet remains challenging due to the complex and dynamic nature of surgical scenes. To address this, we introduce EndoControlMag, a training-free, Lagrangian-based framework with mask-conditioned vascular motion magnification tailored to endoscopic environments. Our approach features two key modules: a Periodic Reference Resetting (PRR) scheme that divides videos into short overlapping clips with dynamically updated reference frames to prevent error accumulation while maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification (HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores using a pretrained visual tracking model to maintain accurate localization despite occlusions and view changes. It then applies one of two adaptive softening strategies to surrounding tissues: motion-based softening that modulates magnification strength proportional to observed tissue displacement, or distance-based exponential decay that simulates biomechanical force attenuation. This dual-mode approach accommodates diverse surgical scenarios-motion-based softening excels with complex tissue deformations while distance-based softening provides stability during unreliable optical flow conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four different surgery types and various challenging scenarios, including occlusions, instrument disturbance, view changes, and vessel deformations. Quantitative metrics, visual assessments, and expert surgeon evaluations demonstrate that EndoControlMag significantly outperforms existing methods in both magnification accuracy and visual quality while maintaining robustness across challenging surgical conditions. The code, dataset, and video results are available at https://szupc.github.io/EndoControlMag/.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems</title>
<link>https://arxiv.org/abs/2507.15296</link>
<guid>https://arxiv.org/abs/2507.15296</guid>
<content:encoded><![CDATA[
arXiv:2507.15296v1 Announce Type: cross 
Abstract: The emergence of the tool agent paradigm has broadened the capability boundaries of the Large Language Model (LLM), enabling it to complete more complex tasks. However, the effectiveness of this paradigm is limited due to the issue of parameter failure during its execution. To explore this phenomenon and propose corresponding suggestions, we first construct a parameter failure taxonomy in this paper. We derive five failure categories from the invocation chain of a mainstream tool agent. Then, we explore the correlation between three different input sources and failure categories by applying 15 input perturbation methods to the input. Experimental results show that parameter name hallucination failure primarily stems from inherent LLM limitations, while issues with input sources mainly cause other failure patterns. To improve the reliability and effectiveness of tool-agent interactions, we propose corresponding improvement suggestions, including standardizing tool return formats, improving error feedback mechanisms, and ensuring parameter consistency.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis</title>
<link>https://arxiv.org/abs/2507.15335</link>
<guid>https://arxiv.org/abs/2507.15335</guid>
<content:encoded><![CDATA[
arXiv:2507.15335v1 Announce Type: cross 
Abstract: Industrial defect detection systems face critical limitations when confined to one-class anomaly detection paradigms, which assume uniform outlier distributions and struggle with data scarcity in realworld manufacturing environments. We present ExDD (Explicit Dual Distribution), a novel framework that transcends these limitations by explicitly modeling dual feature distributions. Our approach leverages parallel memory banks that capture the distinct statistical properties of both normality and anomalous patterns, addressing the fundamental flaw of uniform outlier assumptions. To overcome data scarcity, we employ latent diffusion models with domain-specific textual conditioning, generating in-distribution synthetic defects that preserve industrial context. Our neighborhood-aware ratio scoring mechanism elegantly fuses complementary distance metrics, amplifying signals in regions exhibiting both deviation from normality and similarity to known defect patterns. Experimental validation on KSDD2 demonstrates superior performance (94.2% I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design</title>
<link>https://arxiv.org/abs/2507.15336</link>
<guid>https://arxiv.org/abs/2507.15336</guid>
<content:encoded><![CDATA[
arXiv:2507.15336v1 Announce Type: cross 
Abstract: Database systems have recently advocated for embedding machine learning (ML) capabilities, offering declarative model queries over large, managed model repositories, thereby circumventing the huge computational overhead of traditional ML-based algorithms in automated neural network model selection. Pioneering database studies aim to organize existing benchmark repositories as model bases (MB), querying them for the model records with the highest performance estimation metrics for given tasks. However, this static model selection practice overlooks the fine-grained, evolving relational dependencies between diverse task queries and model architecture variations, resulting in suboptimal matches and failing to further refine the model effectively. To fill the model refinement gap in database research, we propose M-DESIGN, a curated model knowledge base (MKB) pipeline for mastering neural network refinement by adaptively weaving prior insights about model architecture modification. First, we propose a knowledge weaving engine that reframes model refinement as an adaptive query problem over task metadata. Given a user's task query, M-DESIGN quickly matches and iteratively refines candidate models by leveraging a graph-relational knowledge schema that explicitly encodes data properties, architecture variations, and pairwise performance deltas as joinable relations. This schema supports fine-grained relational analytics over architecture tweaks and drives a predictive query planner that can detect and adapt to out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics tasks, where our model knowledge base enriches existing benchmarks with structured metadata covering 3 graph tasks and 22 graph datasets, contributing data records of 67,760 graph models. Empirical results demonstrate that M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited budgets.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis</title>
<link>https://arxiv.org/abs/2507.15340</link>
<guid>https://arxiv.org/abs/2507.15340</guid>
<content:encoded><![CDATA[
arXiv:2507.15340v1 Announce Type: cross 
Abstract: High-resolution volumetric computed tomography (CT) is essential for accurate diagnosis and treatment planning in thoracic diseases; however, it is limited by radiation dose and hardware costs. We present the Transformer Volumetric Super-Resolution Network (\textbf{TVSRN-V2}), a transformer-based super-resolution (SR) framework designed for practical deployment in clinical lung CT analysis. Built from scalable components, including Through-Plane Attention Blocks (TAB) and Swin Transformer V2 -- our model effectively reconstructs fine anatomical details in low-dose CT volumes and integrates seamlessly with downstream analysis pipelines. We evaluate its effectiveness on three critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis -- across multiple clinical cohorts. To enhance robustness across variable acquisition protocols, we introduce pseudo-low-resolution augmentation, simulating scanner diversity without requiring private data. TVSRN-V2 demonstrates a significant improvement in segmentation accuracy (+4\% Dice), higher radiomic feature reproducibility, and enhanced predictive performance (+0.06 C-index and AUC). These results indicate that SR-driven recovery of structural detail significantly enhances clinical decision support, positioning TVSRN-V2 as a well-engineered, clinically viable system for dose-efficient imaging and quantitative analysis in real-world CT workflows.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StackTrans: From Large Language Model to Large Pushdown Automata Model</title>
<link>https://arxiv.org/abs/2507.15343</link>
<guid>https://arxiv.org/abs/2507.15343</guid>
<content:encoded><![CDATA[
arXiv:2507.15343v1 Announce Type: cross 
Abstract: The Transformer architecture has emerged as a landmark advancement within the broad field of artificial intelligence, effectively catalyzing the advent of large language models (LLMs). However, despite its remarkable capabilities and the substantial progress it has facilitated, the Transformer architecture still has some limitations. One such intrinsic limitation is its inability to effectively capture the Chomsky hierarchy, such as regular expressions or deterministic context-free grammars. Drawing inspiration from pushdown automata, which efficiently resolve deterministic context-free grammars using stacks, we propose StackTrans to address the aforementioned issue within LLMs. Unlike previous approaches that modify the attention computation, StackTrans explicitly incorporates hidden state stacks between Transformer layers. This design maintains compatibility with existing frameworks like flash-attention. Specifically, our design features stack operations -- such as pushing and popping hidden states -- that are differentiable and can be learned in an end-to-end manner. Our comprehensive evaluation spans benchmarks for both Chomsky hierarchies and large-scale natural languages. Across these diverse tasks, StackTrans consistently outperforms standard Transformer models and other baselines. We have successfully scaled StackTrans up from 360M to 7B parameters. In particular, our from-scratch pretrained model StackTrans-360M outperforms several larger open-source LLMs with 2-3x more parameters, showcasing its superior efficiency and reasoning capability.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Decentralized Learning with FLock</title>
<link>https://arxiv.org/abs/2507.15349</link>
<guid>https://arxiv.org/abs/2507.15349</guid>
<content:encoded><![CDATA[
arXiv:2507.15349v1 Announce Type: cross 
Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency of centralized control and the massive computing and communication overhead on the decentralized schemes. While the typical standard federated learning (FL) supports data privacy, the central server requirement creates a single point of attack and vulnerability to poisoning attacks. Generalizing the result in this direction to 70B-parameter models in the heterogeneous, trustless environments has turned out to be a huge, yet unbroken bottleneck. This paper introduces FLock, a decentralized framework for secure and efficient collaborative LLM fine-tuning. Integrating a blockchain-based trust layer with economic incentives, FLock replaces the central aggregator with a secure, auditable protocol for cooperation among untrusted parties. We present the first empirical validation of fine-tuning a 70B LLM in a secure, multi-domain, decentralized setting. Our experiments show the FLock framework defends against backdoor poisoning attacks that compromise standard FL optimizers and fosters synergistic knowledge transfer. The resulting models show a >68% reduction in adversarial attack success rates. The global model also demonstrates superior cross-domain generalization, outperforming models trained in isolation on their own specialized data.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding</title>
<link>https://arxiv.org/abs/2507.15357</link>
<guid>https://arxiv.org/abs/2507.15357</guid>
<content:encoded><![CDATA[
arXiv:2507.15357v1 Announce Type: cross 
Abstract: This paper presents a comprehensive evaluation of the capabilities of Large Language Models (LLMs) in metaphor interpretation across multiple datasets, tasks, and prompt configurations. Although metaphor processing has gained significant attention in Natural Language Processing (NLP), previous research has been limited to single-dataset evaluations and specific task settings, often using artificially constructed data through lexical replacement. We address these limitations by conducting extensive experiments using diverse publicly available datasets with inference and metaphor annotations, focusing on Natural Language Inference (NLI) and Question Answering (QA) tasks. The results indicate that LLMs' performance is more influenced by features like lexical overlap and sentence length than by metaphorical content, demonstrating that any alleged emergent abilities of LLMs to understand metaphorical language are the result of a combination of surface-level features, in-context learning, and linguistic knowledge. This work provides critical insights into the current capabilities and limitations of LLMs in processing figurative language, highlighting the need for more realistic evaluation frameworks in metaphor interpretation tasks. Data and code are publicly available.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation</title>
<link>https://arxiv.org/abs/2507.15361</link>
<guid>https://arxiv.org/abs/2507.15361</guid>
<content:encoded><![CDATA[
arXiv:2507.15361v1 Announce Type: cross 
Abstract: Medical image segmentation suffers from data scarcity, particularly in polyp detection where annotation requires specialized expertise. We present SynDiff, a framework combining text-guided synthetic data generation with efficient diffusion-based segmentation. Our approach employs latent diffusion models to generate clinically realistic synthetic polyps through text-conditioned inpainting, augmenting limited training data with semantically diverse samples. Unlike traditional diffusion methods requiring iterative denoising, we introduce direct latent estimation enabling single-step inference with T x computational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9% IoU while maintaining real-time capability suitable for clinical deployment. The framework demonstrates that controlled synthetic augmentation improves segmentation robustness without distribution shift. SynDiff bridges the gap between data-hungry deep learning models and clinical constraints, offering an efficient solution for deployment in resourcelimited medical settings.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG-based Epileptic Prediction via a Two-stage Channel-aware Set Transformer Network</title>
<link>https://arxiv.org/abs/2507.15364</link>
<guid>https://arxiv.org/abs/2507.15364</guid>
<content:encoded><![CDATA[
arXiv:2507.15364v1 Announce Type: cross 
Abstract: Epilepsy is a chronic, noncommunicable brain disorder, and sudden seizure onsets can significantly impact patients' quality of life and health. However, wearable seizure-predicting devices are still limited, partly due to the bulky size of EEG-collecting devices. To relieve the problem, we proposed a novel two-stage channel-aware Set Transformer Network that could perform seizure prediction with fewer EEG channel sensors. We also tested a seizure-independent division method which could prevent the adjacency of training and test data. Experiments were performed on the CHB-MIT dataset which includes 22 patients with 88 merged seizures. The mean sensitivity before channel selection was 76.4% with a false predicting rate (FPR) of 0.09/hour. After channel selection, dominant channels emerged in 20 out of 22 patients; the average number of channels was reduced to 2.8 from 18; and the mean sensitivity rose to 80.1% with an FPR of 0.11/hour. Furthermore, experimental results on the seizure-independent division supported our assertion that a more rigorous seizure-independent division should be used for patients with abundant EEG recordings.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-beam Beamforming in RIS-aided MIMO Subject to Reradiation Mask Constraints -- Optimization and Machine Learning Design</title>
<link>https://arxiv.org/abs/2507.15367</link>
<guid>https://arxiv.org/abs/2507.15367</guid>
<content:encoded><![CDATA[
arXiv:2507.15367v1 Announce Type: cross 
Abstract: Reconfigurable intelligent surfaces (RISs) are an emerging technology for improving spectral efficiency and reducing power consumption in future wireless systems. This paper investigates the joint design of the transmit precoding matrices and the RIS phase shift vector in a multi-user RIS-aided multiple-input multiple-output (MIMO) communication system. We formulate a max-min optimization problem to maximize the minimum achievable rate while considering transmit power and reradiation mask constraints. The achievable rate is simplified using the Arimoto-Blahut algorithm, and the problem is broken into quadratic programs with quadratic constraints (QPQC) sub-problems using an alternating optimization approach. To improve efficiency, we develop a model-based neural network optimization that utilizes the one-hot encoding for the angles of incidence and reflection. We address practical RIS limitations by using a greedy search algorithm to solve the optimization problem for discrete phase shifts. Simulation results demonstrate that the proposed methods effectively shape the multi-beam radiation pattern towards desired directions while satisfying reradiation mask constraints. The neural network design reduces the execution time, and the discrete phase shift scheme performs well with a small reduction of the beamforming gain by using only four phase shift levels.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models</title>
<link>https://arxiv.org/abs/2507.15381</link>
<guid>https://arxiv.org/abs/2507.15381</guid>
<content:encoded><![CDATA[
arXiv:2507.15381v1 Announce Type: cross 
Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most informative samples for labeling, making it particularly valuable in resource-constrained settings. However, traditional evaluation methods, which focus solely on final accuracy, fail to capture the full dynamics of the learning process. To address this gap, we propose PALM (Performance Analysis of Active Learning Models), a unified and interpretable mathematical model that characterizes AL trajectories through four key parameters: achievable accuracy, coverage efficiency, early-stage performance, and scalability. PALM provides a predictive description of AL behavior from partial observations, enabling the estimation of future performance and facilitating principled comparisons across different strategies. We validate PALM through extensive experiments on CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and self-supervised embeddings. Our results demonstrate that PALM generalizes effectively across datasets, budgets, and strategies, accurately predicting full learning curves from limited labeled data. Importantly, PALM reveals crucial insights into learning efficiency, data space coverage, and the scalability of AL methods. By enabling the selection of cost-effective strategies and predicting performance under tight budget constraints, PALM lays the basis for more systematic, reproducible, and data-efficient evaluation of AL in both research and real-world applications. The code is available at: https://github.com/juliamachnio/PALM.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants</title>
<link>https://arxiv.org/abs/2507.15393</link>
<guid>https://arxiv.org/abs/2507.15393</guid>
<content:encoded><![CDATA[
arXiv:2507.15393v1 Announce Type: cross 
Abstract: Phishing emails are a critical component of the cybercrime kill chain due to their wide reach and low cost. Their ever-evolving nature renders traditional rule-based and feature-engineered detectors ineffective in the ongoing arms race between attackers and defenders. The rise of large language models (LLMs) further exacerbates the threat, enabling attackers to craft highly convincing phishing emails at minimal cost.
  This work demonstrates that LLMs can generate psychologically persuasive phishing emails tailored to victim profiles, successfully bypassing nearly all commercial and academic detectors. To defend against such threats, we propose PiMRef, the first reference-based phishing email detector that leverages knowledge-based invariants. Our core insight is that persuasive phishing emails often contain disprovable identity claims, which contradict real-world facts. PiMRef reframes phishing detection as an identity fact-checking task. Given an email, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the legitimacy of the sender's domain against a predefined knowledge base, and (iii) detects call-to-action prompts that push user engagement. Contradictory claims are flagged as phishing indicators and serve as human-understandable explanations.
  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector, PiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks like Nazario and PhishPot. In a real-world evaluation of 10,183 emails across five university accounts over three years, PiMRef achieved 92.1% precision, 87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art in both effectiveness and efficiency.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation</title>
<link>https://arxiv.org/abs/2507.15396</link>
<guid>https://arxiv.org/abs/2507.15396</guid>
<content:encoded><![CDATA[
arXiv:2507.15396v1 Announce Type: cross 
Abstract: Hearing loss simulation models are essential for hearing aid deployment. However, existing models have high computational complexity and latency, which limits real-time applications and lack direct integration with speech processing systems. To address these issues, we propose Neuro-MSBG, a lightweight end-to-end model with a personalized audiogram encoder for effective time-frequency modeling. Experiments show that Neuro-MSBG supports parallel inference and retains the intelligibility and perceptual quality of the original MSBG, with a Spearman's rank correlation coefficient (SRCC) of 0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for Perceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation runtime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second input), further demonstrating its efficiency and practicality.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent</title>
<link>https://arxiv.org/abs/2507.15428</link>
<guid>https://arxiv.org/abs/2507.15428</guid>
<content:encoded><![CDATA[
arXiv:2507.15428v1 Announce Type: cross 
Abstract: Egomotion videos are first-person recordings where the view changes continuously due to the agent's movement. As they serve as the primary visual input for embodied AI agents, making egomotion video reasoning more efficient is therefore essential for real-world deployment. Recent advances in vision-language models have enabled strong multimodal reasoning capabilities, but their computational cost remains prohibitive for long, redundant video inputs. Existing token pruning methods, typically designed for third-person videos, fail to leverage the spatiotemporal continuity and motion constraints inherent in egomotion settings. To address this, we propose EgoPrune, a training-free token pruning method tailored for egomotion video reasoning. EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR for temporally efficient sampling; Perspective-Aware Redundancy Filtering (PARF), which aligns visual tokens using perspective transformations and removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token selector that jointly considers visual-text relevance and intra-frame diversity. Experiments on two egomotion video benchmarks show that EgoPrune consistently outperforms prior training-free methods across various pruning ratios while significantly reducing FLOPs, memory usage, and latency. Moreover, we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB edge device, demonstrating its real-world efficiency and suitability for on-device egomotion video reasoning.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting</title>
<link>https://arxiv.org/abs/2507.15454</link>
<guid>https://arxiv.org/abs/2507.15454</guid>
<content:encoded><![CDATA[
arXiv:2507.15454v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and real-time novel view synthesis, yet its lack of semantic understanding limits object-level perception. In this work, we propose ObjectGS, an object-aware framework that unifies 3D scene reconstruction with semantic understanding. Instead of treating the scene as a unified whole, ObjectGS models individual objects as local anchors that generate neural Gaussians and share object IDs, enabling precise object-level reconstruction. During training, we dynamically grow or prune these anchors and optimize their features, while a one-hot ID encoding with a classification loss enforces clear semantic constraints. We show through extensive experiments that ObjectGS not only outperforms state-of-the-art methods on open-vocabulary and panoptic segmentation tasks, but also integrates seamlessly with applications like mesh extraction and scene editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy iteration</title>
<link>https://arxiv.org/abs/2507.15455</link>
<guid>https://arxiv.org/abs/2507.15455</guid>
<content:encoded><![CDATA[
arXiv:2507.15455v1 Announce Type: cross 
Abstract: We propose a mesh-free policy iteration framework that combines classical dynamic programming with physics-informed neural networks (PINNs) to solve high-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in stochastic differential games and robust control. The method alternates between solving linear second-order PDEs under fixed feedback policies and updating the controls via pointwise minimax optimization using automatic differentiation. Under standard Lipschitz and uniform ellipticity assumptions, we prove that the value function iterates converge locally uniformly to the unique viscosity solution of the HJI equation. The analysis establishes equi-Lipschitz regularity of the iterates, enabling provable stability and convergence without requiring convexity of the Hamiltonian. Numerical experiments demonstrate the accuracy and scalability of the method. In a two-dimensional stochastic path-planning game with a moving obstacle, our method matches finite-difference benchmarks with relative $L^2$-errors below %10^{-2}%. In five- and ten-dimensional publisher-subscriber differential games with anisotropic noise, the proposed approach consistently outperforms direct PINN solvers, yielding smoother value functions and lower residuals. Our results suggest that integrating PINNs with policy iteration is a practical and theoretically grounded method for solving high-dimensional, nonconvex HJI equations, with potential applications in robotics, finance, and multi-agent reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The New LLM Bottleneck: A Systems Perspective on Latent Attention and Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2507.15465</link>
<guid>https://arxiv.org/abs/2507.15465</guid>
<content:encoded><![CDATA[
arXiv:2507.15465v1 Announce Type: cross 
Abstract: Computational workloads composing traditional Transformer models are starkly bifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic intensity, while feedforward layers are compute-bound. This dichotomy has long motivated research into specialized hardware to mitigate the MHA bottleneck.
  This paper argues that recent architectural shifts, namely Multi-head Latent Attention (MLA) and Mixture-of-Experts (MoE), challenge the premise of specialized attention hardware. We make two key observations. First, the arithmetic intensity of MLA is over two orders of magnitude greater than that of MHA, shifting it close to a compute-bound regime well-suited for modern accelerators like GPUs. Second, by distributing MoE experts across a pool of accelerators, their arithmetic intensity can be tuned through batching to match that of the dense layers, creating a more balanced computational profile.
  These findings reveal a diminishing need for specialized attention hardware. The central challenge for next-generation Transformers is no longer accelerating a single memory-bound layer. Instead, the focus must shift to designing balanced systems with sufficient compute, memory capacity, memory bandwidth, and high-bandwidth interconnects to manage the diverse demands of large-scale models.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Emergence of Deep Reinforcement Learning for Path Planning</title>
<link>https://arxiv.org/abs/2507.15469</link>
<guid>https://arxiv.org/abs/2507.15469</guid>
<content:encoded><![CDATA[
arXiv:2507.15469v1 Announce Type: cross 
Abstract: The increasing demand for autonomous systems in complex and dynamic environments has driven significant research into intelligent path planning methodologies. For decades, graph-based search algorithms, linear programming techniques, and evolutionary computation methods have served as foundational approaches in this domain. Recently, deep reinforcement learning (DRL) has emerged as a powerful method for enabling autonomous agents to learn optimal navigation strategies through interaction with their environments. This survey provides a comprehensive overview of traditional approaches as well as the recent advancements in DRL applied to path planning tasks, focusing on autonomous vehicles, drones, and robotic platforms. Key algorithms across both conventional and learning-based paradigms are categorized, with their innovations and practical implementations highlighted. This is followed by a thorough discussion of their respective strengths and limitations in terms of computational efficiency, scalability, adaptability, and robustness. The survey concludes by identifying key open challenges and outlining promising avenues for future research. Special attention is given to hybrid approaches that integrate DRL with classical planning techniques to leverage the benefits of both learning-based adaptability and deterministic reliability, offering promising directions for robust and resilient autonomous navigation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents</title>
<link>https://arxiv.org/abs/2507.15478</link>
<guid>https://arxiv.org/abs/2507.15478</guid>
<content:encoded><![CDATA[
arXiv:2507.15478v1 Announce Type: cross 
Abstract: Ensuring reliable and rule-compliant behavior of autonomous agents in uncertain environments remains a fundamental challenge in modern robotics. Our work shows how neuro-symbolic systems, which integrate probabilistic, symbolic white-box reasoning models with deep learning methods, offer a powerful solution to this challenge. This enables the simultaneous consideration of explicit rules and neural models trained on noisy data, combining the strength of structured reasoning with flexible representations. To this end, we introduce the Constitutional Controller (CoCo), a novel framework designed to enhance the safety and reliability of agents by reasoning over deep probabilistic logic programs representing constraints such as those found in shared traffic spaces. Furthermore, we propose the concept of self-doubt, implemented as a probability density conditioned on doubt features such as travel velocity, employed sensors, or health factors. In a real-world aerial mobility study, we demonstrate CoCo's advantages for intelligent autonomous systems to learn appropriate doubts and navigate complex and uncertain environments safely and compliantly.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GR-3 Technical Report</title>
<link>https://arxiv.org/abs/2507.15493</link>
<guid>https://arxiv.org/abs/2507.15493</guid>
<content:encoded><![CDATA[
arXiv:2507.15493v1 Announce Type: cross 
Abstract: We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution</title>
<link>https://arxiv.org/abs/2507.15501</link>
<guid>https://arxiv.org/abs/2507.15501</guid>
<content:encoded><![CDATA[
arXiv:2507.15501v1 Announce Type: cross 
Abstract: This work evaluates the potential of large language models (LLMs) to power digital assistants capable of complex action execution. These assistants rely on pre-trained programming knowledge to execute multi-step goals by composing objects and functions defined in assistant libraries into action execution programs. To achieve this, we develop ASPERA, a framework comprising an assistant library simulation and a human-assisted LLM data generation engine. Our engine allows developers to guide LLM generation of high-quality tasks consisting of complex user queries, simulation state and corresponding validation programs, tackling data availability and evaluation robustness challenges. Alongside the framework we release Asper-Bench, an evaluation dataset of 250 challenging tasks generated using ASPERA, which we use to show that program generation grounded in custom assistant libraries is a significant challenge to LLMs compared to dependency-free code generation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback</title>
<link>https://arxiv.org/abs/2507.15507</link>
<guid>https://arxiv.org/abs/2507.15507</guid>
<content:encoded><![CDATA[
arXiv:2507.15507v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models, such as language models (LMs), to follow complex human preferences. In RLHF for LMs, we first train an LM using supervised fine-tuning, sample pairs of responses, obtain human feedback, and use the resulting data to train a reward model (RM). RL methods are then used to train the LM to maximize the reward given by the RM. As training progresses, the responses generated by the LM no longer resemble the responses seen by the RM during training, leading to the RM becoming inaccurate. The score given by the RM keeps increasing, but the learned behavior no longer matches the human preferences. This issue is known as overoptimization. We investigate overoptimization from the point of view of distribution shift and show that the shift results in an inconsistent estimate of the RM parameters, leading to an inconsistent estimate of the policy gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which iteratively off-policy corrects the RM using importance weighting, without requiring new labels or samples. This results in a more accurate RM, which empirically leads to an improved final policy. We validate our approach in experiments with summarization and chatbot datasets and show that it performs significantly better than standard RLHF methods and baselines. Our implementation is available at https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2507.15524</link>
<guid>https://arxiv.org/abs/2507.15524</guid>
<content:encoded><![CDATA[
arXiv:2507.15524v1 Announce Type: cross 
Abstract: Accurate segmentation is crucial for clinical applications, but existing models often assume fixed, high-resolution inputs and degrade significantly when faced with lower-resolution data in real-world scenarios. To address this limitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation architecture that dynamically adapts its inference path to the spatial resolution of the input. Central to our design are multi-scale blocks integrated at multiple encoder depths, a resolution-aware routing mechanism, and consistency-driven training that aligns multi-resolution features with full-resolution representations. We evaluate RARE-UNet on two benchmark brain imaging tasks for hippocampus and tumor segmentation. Compared to standard UNet, its multi-resolution augmented variant, and nnUNet, our model achieves the highest average Dice scores of 0.84 and 0.65 across resolution, while maintaining consistent performance and significantly reduced inference time at lower resolutions. These results highlight the effectiveness and scalability of our architecture in achieving resolution-robust segmentation. The codes are available at: https://github.com/simonsejse/RARE-UNet.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors</title>
<link>https://arxiv.org/abs/2507.15550</link>
<guid>https://arxiv.org/abs/2507.15550</guid>
<content:encoded><![CDATA[
arXiv:2507.15550v1 Announce Type: cross 
Abstract: Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior knowledge, requires specialized benchmarks currently lacking in the landscape. To address this gap, we introduce PhysGym, a novel benchmark suite and simulation platform for rigorously assessing LLM-based scientific reasoning in interactive physics environments. PhysGym's primary contribution lies in its sophisticated control over the level of prior knowledge provided to the agent. This allows researchers to dissect agent performance along axes including the complexity of the problem and the prior knowledge levels. The benchmark comprises a suite of interactive simulations, where agents must actively probe environments, gather data sequentially under constraints and formulate hypotheses about underlying physical laws. PhysGym provides standardized evaluation protocols and metrics for assessing hypothesis accuracy and model fidelity. We demonstrate the benchmark's utility by presenting results from baseline LLMs, showcasing its ability to differentiate capabilities based on varying priors and task complexity.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project</title>
<link>https://arxiv.org/abs/2507.15574</link>
<guid>https://arxiv.org/abs/2507.15574</guid>
<content:encoded><![CDATA[
arXiv:2507.15574v1 Announce Type: cross 
Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents significant challenges in satellite network management, requiring innovative approaches for efficient, scalable, and resilient operations. This paper explores the role of Artificial Intelligence (AI) in optimizing the operation of satellite mega-constellations, drawing from the ConstellAI project funded by the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland University, and Thales Alenia Space collaborates to develop AI-driven algorithms and demonstrates their effectiveness over traditional methods for two crucial operational challenges: data routing and resource allocation. In the routing use case, Reinforcement Learning (RL) is used to improve the end-to-end latency by learning from historical queuing latency, outperforming classical shortest path algorithms. For resource allocation, RL optimizes the scheduling of tasks across constellations, focussing on efficiently using limited resources such as battery and memory. Both use cases were tested for multiple satellite constellation configurations and operational scenarios, resembling the real-life spacecraft operations of communications and Earth observation satellites. This research demonstrates that RL not only competes with classical approaches but also offers enhanced flexibility, scalability, and generalizability in decision-making processes, which is crucial for the autonomous and intelligent management of satellite fleets. The findings of this activity suggest that AI can fundamentally alter the landscape of satellite constellation management by providing more adaptive, robust, and cost-effective solutions.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation</title>
<link>https://arxiv.org/abs/2507.15577</link>
<guid>https://arxiv.org/abs/2507.15577</guid>
<content:encoded><![CDATA[
arXiv:2507.15577v1 Announce Type: cross 
Abstract: Mixup has become a popular augmentation strategy for image classification, yet its naive pixel-wise interpolation often produces unrealistic images that can hinder learning, particularly in high-stakes medical applications. We propose GeMix, a two-stage framework that replaces heuristic blending with a learned, label-aware interpolation powered by class-conditional GANs. First, a StyleGAN2-ADA generator is trained on the target dataset. During augmentation, we sample two label vectors from Dirichlet priors biased toward different classes and blend them via a Beta-distributed coefficient. Then, we condition the generator on this soft label to synthesize visually coherent images that lie along a continuous class manifold. We benchmark GeMix on the large-scale COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101, EfficientNet-B0). When combined with real data, our method increases macro-F1 over traditional mixup for all backbones, reducing the false negative rate for COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup, delivering stronger regularization and greater semantic fidelity, without disrupting existing training pipelines. We publicly release our code at https://github.com/hugocarlesso/GeMix to foster reproducibility and further research.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unequal Voices: How LLMs Construct Constrained Queer Narratives</title>
<link>https://arxiv.org/abs/2507.15585</link>
<guid>https://arxiv.org/abs/2507.15585</guid>
<content:encoded><![CDATA[
arXiv:2507.15585v1 Announce Type: cross 
Abstract: One way social groups are marginalized in discourse is that the narratives told about them often default to a narrow, stereotyped range of topics. In contrast, default groups are allowed the full complexity of human existence. We describe the constrained representations of queer people in LLM generations in terms of harmful representations, narrow representations, and discursive othering and formulate hypotheses to test for these phenomena. Our results show that LLMs are significantly limited in their portrayals of queer personas.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario</title>
<link>https://arxiv.org/abs/2507.15587</link>
<guid>https://arxiv.org/abs/2507.15587</guid>
<content:encoded><![CDATA[
arXiv:2507.15587v1 Announce Type: cross 
Abstract: Current research on decision-making in safety-critical scenarios often relies on inefficient data-driven scenario generation or specific modeling approaches, which fail to capture corner cases in real-world contexts. To address this issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework, where background vehicles with interference capabilities are treated as red-team agents. Through active interference and exploration, red-team vehicles can uncover corner cases outside the data distribution. The framework uses a Constraint Graph Representation Markov Decision Process, ensuring that red-team vehicles comply with safety rules while continuously disrupting the autonomous vehicles (AVs). A policy threat zone model is constructed to quantify the threat posed by red-team vehicles to AVs, inducing more extreme actions to increase the danger level of the scenario. Experimental results show that the proposed framework significantly impacts AVs decision-making safety and generates various corner cases. This method also offers a novel direction for research in safety-critical scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems</title>
<link>https://arxiv.org/abs/2507.15613</link>
<guid>https://arxiv.org/abs/2507.15613</guid>
<content:encoded><![CDATA[
arXiv:2507.15613v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) deployed in enterprise settings (e.g., as Microsoft 365 Copilot) face novel security challenges. One critical threat is prompt inference attacks: adversaries chain together seemingly benign prompts to gradually extract confidential data. In this paper, we present a comprehensive study of multi-stage prompt inference attacks in an enterprise LLM context. We simulate realistic attack scenarios where an attacker uses mild-mannered queries and indirect prompt injections to exploit an LLM integrated with private corporate data. We develop a formal threat model for these multi-turn inference attacks and analyze them using probability theory, optimization frameworks, and information-theoretic leakage bounds. The attacks are shown to reliably exfiltrate sensitive information from the LLM's context (e.g., internal SharePoint documents or emails), even when standard safety measures are in place.
  We propose and evaluate defenses to counter such attacks, including statistical anomaly detection, fine-grained access control, prompt sanitization techniques, and architectural modifications to LLM deployment. Each defense is supported by mathematical analysis or experimental simulation. For example, we derive bounds on information leakage under differential privacy-based training and demonstrate an anomaly detection method that flags multi-turn attacks with high AUC. We also introduce an approach called "spotlighting" that uses input transformations to isolate untrusted prompt content, reducing attack success by an order of magnitude. Finally, we provide a formal proof of concept and empirical validation for a combined defense-in-depth strategy. Our work highlights that securing LLMs in enterprise settings requires moving beyond single-turn prompt filtering toward a holistic, multi-stage perspective on both attacks and defenses.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting</title>
<link>https://arxiv.org/abs/2507.15614</link>
<guid>https://arxiv.org/abs/2507.15614</guid>
<content:encoded><![CDATA[
arXiv:2507.15614v1 Announce Type: cross 
Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but are too computationally intensive for on-the-fly decision-making during flood events. The central challenge is to accelerate these simulations without sacrificing accuracy. This paper introduces a deep learning surrogate that treats HEC-RAS not as a solver but as a data-generation engine. We propose a hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU) to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural Operator (Geo-FNO) to model long-range spatial dependencies along a river reach. The model learns underlying physics implicitly from a minimal eight-channel feature vector encoding dynamic state, static geometry, and boundary forcings extracted directly from native HEC-RAS files. Trained on 67 reaches of the Mississippi River Basin, the surrogate was evaluated on a year-long, unseen hold-out simulation. Results show the model achieves a strong predictive accuracy, with a median absolute stage error of 0.31 feet. Critically, for a full 67-reach ensemble forecast, our surrogate reduces the required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly 3.5 times over the traditional solver. The success of this data-driven approach demonstrates that robust feature engineering can produce a viable, high-speed replacement for conventional hydraulic models, improving the computational feasibility of large-scale ensemble flood forecasting.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why can't Epidemiology be automated (yet)?</title>
<link>https://arxiv.org/abs/2507.15617</link>
<guid>https://arxiv.org/abs/2507.15617</guid>
<content:encoded><![CDATA[
arXiv:2507.15617v1 Announce Type: cross 
Abstract: Recent advances in artificial intelligence (AI) - particularly generative AI - present new opportunities to accelerate, or even automate, epidemiological research. Unlike disciplines based on physical experimentation, a sizable fraction of Epidemiology relies on secondary data analysis and thus is well-suited for such augmentation. Yet, it remains unclear which specific tasks can benefit from AI interventions or where roadblocks exist. Awareness of current AI capabilities is also mixed. Here, we map the landscape of epidemiological tasks using existing datasets - from literature review to data access, analysis, writing up, and dissemination - and identify where existing AI tools offer efficiency gains. While AI can increase productivity in some areas such as coding and administrative tasks, its utility is constrained by limitations of existing AI models (e.g. hallucinations in literature reviews) and human systems (e.g. barriers to accessing datasets). Through examples of AI-generated epidemiological outputs, including fully AI-generated papers, we demonstrate that recently developed agentic systems can now design and execute epidemiological analysis, albeit to varied quality (see https://github.com/edlowther/automated-epidemiology). Epidemiologists have new opportunities to empirically test and benchmark AI systems; realising the potential of AI will require two-way engagement between epidemiologists and engineers.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis</title>
<link>https://arxiv.org/abs/2507.15636</link>
<guid>https://arxiv.org/abs/2507.15636</guid>
<content:encoded><![CDATA[
arXiv:2507.15636v1 Announce Type: cross 
Abstract: Recent advances in deepfake technology have created increasingly convincing synthetic media that poses significant challenges to information integrity and social trust. While current detection methods show promise, their underlying mechanisms remain poorly understood, and the large sizes of their models make them challenging to deploy in resource-limited environments. This study investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake detection, aiming to identify the key features crucial for recognizing deepfakes. We examine how neural networks can be efficiently pruned while maintaining high detection accuracy. Through extensive experiments with MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and FaceForensics++ datasets, we find that deepfake detection networks contain winning tickets, i.e., subnetworks, that preserve performance even at substantial sparsity levels. Our results indicate that MesoNet retains 56.2% accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000 parameters, which is about 90% of its baseline accuracy (62.6%). The results also show that our proposed LTH-based iterative magnitude pruning approach consistently outperforms one-shot pruning methods. Using Grad-CAM visualization, we analyze how pruned networks maintain their focus on critical facial regions for deepfake detection. Additionally, we demonstrate the transferability of winning tickets across datasets, suggesting potential for efficient, deployable deepfake detection systems.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training</title>
<link>https://arxiv.org/abs/2507.15640</link>
<guid>https://arxiv.org/abs/2507.15640</guid>
<content:encoded><![CDATA[
arXiv:2507.15640v1 Announce Type: cross 
Abstract: Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Context for Multimodal Fallacy Classification in Political Debates</title>
<link>https://arxiv.org/abs/2507.15641</link>
<guid>https://arxiv.org/abs/2507.15641</guid>
<content:encoded><![CDATA[
arXiv:2507.15641v1 Announce Type: cross 
Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared task, which aims to advance research in multimodal argument mining, focusing on logical fallacies in political debates. Our approach uses pretrained Transformer-based models and proposes several ways to leverage context. In the fallacy classification subtask, our models achieved macro F1-scores of 0.4444 (text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed performance comparable to the text-only model, suggesting potential for improvements.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Explainable Anomaly Detection in Shared Mobility Systems</title>
<link>https://arxiv.org/abs/2507.15643</link>
<guid>https://arxiv.org/abs/2507.15643</guid>
<content:encoded><![CDATA[
arXiv:2507.15643v1 Announce Type: cross 
Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role in urban transportation. Identifying anomalies in these systems is essential for optimizing operations, improving service reliability, and enhancing user experience. This paper presents an interpretable anomaly detection framework that integrates multi-source data, including bike-sharing trip records, weather conditions, and public transit availability. The Isolation Forest algorithm is employed for unsupervised anomaly detection, along with the Depth-based Isolation Forest Feature Importance (DIFFI) algorithm providing interpretability. Results show that station-level analysis offers a robust understanding of anomalies, highlighting the influence of external factors such as adverse weather and limited transit availability. Our findings contribute to improving decision-making in shared mobility operations.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models</title>
<link>https://arxiv.org/abs/2507.15663</link>
<guid>https://arxiv.org/abs/2507.15663</guid>
<content:encoded><![CDATA[
arXiv:2507.15663v1 Announce Type: cross 
Abstract: Background: Text-to-image generation models are widely used across numerous domains. Among these models, Stable Diffusion (SD) - an open-source text-to-image generation model - has become the most popular, producing over 12 billion images annually. However, the widespread use of these models raises concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the environment, we introduce SustainDiffusion, a search-based approach designed to enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters and prompt structures that can reduce gender and ethnic bias in generated images while also lowering the energy consumption required for image generation. Importantly, SustainDiffusion maintains image quality comparable to that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion, testing it against six different baselines using 56 different prompts. Our results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%, ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social and environmental sustainability of text-to-image generation models is possible without fine-tuning or changing the model's architecture.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing value imputation with adversarial random forests -- MissARF</title>
<link>https://arxiv.org/abs/2507.15681</link>
<guid>https://arxiv.org/abs/2507.15681</guid>
<content:encoded><![CDATA[
arXiv:2507.15681v1 Announce Type: cross 
Abstract: Handling missing values is a common challenge in biostatistical analyses, typically addressed by imputation methods. We propose a novel, fast, and easy-to-use imputation method called missing value imputation with adversarial random forests (MissARF), based on generative machine learning, that provides both single and multiple imputation. MissARF employs adversarial random forest (ARF) for density estimation and data synthesis. To impute a missing value of an observation, we condition on the non-missing values and sample from the estimated conditional distribution generated by ARF. Our experiments demonstrate that MissARF performs comparably to state-of-the-art single and multiple imputation methods in terms of imputation quality and fast runtime with no additional costs for multiple imputation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression</title>
<link>https://arxiv.org/abs/2507.15686</link>
<guid>https://arxiv.org/abs/2507.15686</guid>
<content:encoded><![CDATA[
arXiv:2507.15686v1 Announce Type: cross 
Abstract: Existing AI-based point cloud compression methods struggle with dependence on specific training data distributions, which limits their real-world deployment. Implicit Neural Representation (INR) methods solve the above problem by encoding overfitted network parameters to the bitstream, resulting in more distribution-agnostic results. However, due to the limitation of encoding time and decoder size, current INR based methods only consider lossy geometry compression. In this paper, we propose the first INR based lossless point cloud geometry compression method called Lossless Implicit Neural Representations for Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we design a group of point clouds level coding framework with an effective network initialization strategy, which can reduce around 60% encoding time. A lightweight coding network based on multiscale SparseConv, consisting of scale context extraction, child node prediction, and model compression modules, is proposed to realize fast inference and compact decoder size. Experimental results show that our method consistently outperforms traditional and AI-based methods: for example, with the convergence time in the MVUB dataset, our method reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and 21.95% compared to SparsePCGC. Our project can be seen on https://huangwenjie2023.github.io/LINR-PCGC/.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models</title>
<link>https://arxiv.org/abs/2507.15698</link>
<guid>https://arxiv.org/abs/2507.15698</guid>
<content:encoded><![CDATA[
arXiv:2507.15698v1 Announce Type: cross 
Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding multi-step reasoning in large language models (LLMs), especially for mathematical problem solving. However, we identify a pervasive length bias in existing PRMs: they tend to assign higher scores to longer reasoning steps, even when the semantic content and logical validity are unchanged. This bias undermines the reliability of reward predictions and leads to overly verbose outputs during inference. To address this issue, we propose CoLD(Counterfactually-Guided Length Debiasing), a unified framework that mitigates length bias through three components: an explicit length-penalty adjustment, a learned bias estimator trained to capture spurious length-related signals, and a joint training strategy that enforces length-invariance in reward predictions. Our approach is grounded in counterfactual reasoning and informed by causal graph analysis. Extensive experiments on MATH500 and GSM-Plus show that CoLD consistently reduces reward-length correlation, improves accuracy in step selection, and encourages more concise, logically valid reasoning. These results demonstrate the effectiveness and practicality of CoLD in improving the fidelity and robustness of PRMs.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Understanding in Signaling Games</title>
<link>https://arxiv.org/abs/2507.15706</link>
<guid>https://arxiv.org/abs/2507.15706</guid>
<content:encoded><![CDATA[
arXiv:2507.15706v1 Announce Type: cross 
Abstract: Receivers in standard signaling game models struggle with learning compositional information. Even when the signalers send compositional messages, the receivers do not interpret them compositionally. When information from one message component is lost or forgotten, the information from other components is also erased. In this paper I construct signaling game models in which genuine compositional understanding evolves. I present two new models: a minimalist receiver who only learns from the atomic messages of a signal, and a generalist receiver who learns from all of the available information. These models are in many ways simpler than previous alternatives, and allow the receivers to learn from the atomic components of messages.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?</title>
<link>https://arxiv.org/abs/2507.15707</link>
<guid>https://arxiv.org/abs/2507.15707</guid>
<content:encoded><![CDATA[
arXiv:2507.15707v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have been evaluated using diverse question types, e.g., multiple-choice, true/false, and short/long answers. This study answers an unexplored question about the impact of different question types on LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on three different types of questions using quantitative and deductive reasoning tasks. The performance metrics include accuracy in the reasoning steps and choosing the final answer. Key Findings: (1) Significant differences exist in LLM performance across different question types. (2) Reasoning accuracy does not necessarily correlate with the final selection accuracy. (3) The number of options and the choice of words, influence LLM performance.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning</title>
<link>https://arxiv.org/abs/2507.15717</link>
<guid>https://arxiv.org/abs/2507.15717</guid>
<content:encoded><![CDATA[
arXiv:2507.15717v1 Announce Type: cross 
Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology are limited in scope and disproportionately prioritise accuracy. We introduce BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive evaluation benchmark developed through multiple rounds of expert checking by 13 ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset underwent multiple rounds of expert checking. Duplicate and substandard questions were systematically removed. Ten ophthalmologists refined the explanations of each MCQ's correct answer. This was further adjudicated by three senior ophthalmologists. To illustrate BELO's utility, we evaluated six LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro) using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore, BARTScore, METEOR, and AlignScore). In a further evaluation involving human experts, two ophthalmologists qualitatively reviewed 50 randomly selected outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900 high-quality, expert-reviewed questions aggregated from five sources: BCSC (260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public leaderboard has been established to promote transparent evaluation and reporting. Importantly, the BELO dataset will remain a hold-out, evaluation-only benchmark to ensure fair and reproducible comparisons of future models.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Anomaly Detection for Electric Vehicles Charging Stations</title>
<link>https://arxiv.org/abs/2507.15718</link>
<guid>https://arxiv.org/abs/2507.15718</guid>
<content:encoded><![CDATA[
arXiv:2507.15718v1 Announce Type: cross 
Abstract: Electric vehicles (EV) charging stations are one of the critical infrastructures needed to support the transition to renewable-energy-based mobility, but ensuring their reliability and efficiency requires effective anomaly detection to identify irregularities in charging behavior. However, in such a productive scenario, it is also crucial to determine the underlying cause behind the detected anomalies. To achieve this goal, this study investigates unsupervised anomaly detection techniques for EV charging infrastructure, integrating eXplainable Artificial Intelligence techniques to enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies Isolation Forest to detect anomalies and employs the Depth-based Isolation Forest Feature Importance (DIFFI) method to identify the most important features contributing to such anomalies. The efficacy of the proposed approach is evaluated in a real industrial case.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DialogueForge: LLM Simulation of Human-Chatbot Dialogue</title>
<link>https://arxiv.org/abs/2507.15752</link>
<guid>https://arxiv.org/abs/2507.15752</guid>
<content:encoded><![CDATA[
arXiv:2507.15752v1 Announce Type: cross 
Abstract: Collecting human-chatbot dialogues typically demands substantial manual effort and is time-consuming, which limits and poses challenges for research on conversational AI. In this work, we propose DialogueForge - a framework for generating AI-simulated conversations in human-chatbot style. To initialize each generated conversation, DialogueForge uses seed prompts extracted from real human-chatbot interactions. We test a variety of LLMs to simulate the human chatbot user, ranging from state-of-the-art proprietary models to small-scale open-source LLMs, and generate multi-turn dialogues tailored to specific tasks. In addition, we explore fine-tuning techniques to enhance the ability of smaller models to produce indistinguishable human-like dialogues. We evaluate the quality of the simulated conversations and compare different models using the UniEval and GTEval evaluation protocols. Our experiments show that large proprietary models (e.g., GPT-4o) generally outperform others in generating more realistic dialogues, while smaller open-source models (e.g., Llama, Mistral) offer promising performance with greater customization. We demonstrate that the performance of smaller models can be significantly improved by employing supervised fine-tuning techniques. Nevertheless, maintaining coherent and natural long-form human-like dialogues remains a common challenge across all models.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffuMeta: Algebraic Language Models for Inverse Design of Metamaterials via Diffusion Transformers</title>
<link>https://arxiv.org/abs/2507.15753</link>
<guid>https://arxiv.org/abs/2507.15753</guid>
<content:encoded><![CDATA[
arXiv:2507.15753v1 Announce Type: cross 
Abstract: Generative machine learning models have revolutionized material discovery by capturing complex structure-property relationships, yet extending these approaches to the inverse design of three-dimensional metamaterials remains limited by computational complexity and underexplored design spaces due to the lack of expressive representations. Here, we present DiffuMeta, a generative framework integrating diffusion transformers with a novel algebraic language representation, encoding 3D geometries as mathematical sentences. This compact, unified parameterization spans diverse topologies while enabling direct application of transformers to structural design. DiffuMeta leverages diffusion models to generate novel shell structures with precisely targeted stress-strain responses under large deformations, accounting for buckling and contact while addressing the inherent one-to-many mapping by producing diverse solutions. Uniquely, our approach enables simultaneous control over multiple mechanical objectives, including linear and nonlinear responses beyond training domains. Experimental validation of fabricated structures further confirms the efficacy of our approach for accelerated design of metamaterials and structures with tailored properties.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Left Leaning Models: AI Assumptions on Economic Policy</title>
<link>https://arxiv.org/abs/2507.15771</link>
<guid>https://arxiv.org/abs/2507.15771</guid>
<content:encoded><![CDATA[
arXiv:2507.15771v1 Announce Type: cross 
Abstract: How does AI think about economic policy? While the use of large language models (LLMs) in economics is growing exponentially, their assumptions on economic issues remain a black box. This paper uses a conjoint experiment to tease out the main factors influencing LLMs' evaluation of economic policy. It finds that LLMs are most sensitive to unemployment, inequality, financial stability, and environmental harm and less sensitive to traditional macroeconomic concerns such as economic growth, inflation, and government debt. The results are remarkably consistent across scenarios and across models.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis</title>
<link>https://arxiv.org/abs/2507.15772</link>
<guid>https://arxiv.org/abs/2507.15772</guid>
<content:encoded><![CDATA[
arXiv:2507.15772v1 Announce Type: cross 
Abstract: Detecting stress in plants is crucial for both open-farm and controlled-environment agriculture. Biomolecules within plants serve as key stress indicators, offering vital markers for continuous health monitoring and early disease detection. Raman spectroscopy provides a powerful, non-invasive means to quantify these biomolecules through their molecular vibrational signatures. However, traditional Raman analysis relies on customized data-processing workflows that require fluorescence background removal and prior identification of Raman peaks of interest-introducing potential biases and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation of Vibrational Raman spectra for plant-stress Analysis), a fully automated workflow based on a variational autoencoder. Unlike conventional approaches, DIVA processes native Raman spectra-including fluorescence backgrounds-without manual preprocessing, identifying and quantifying significant spectral features in an unbiased manner. We applied DIVA to detect a range of plant stresses, including abiotic (shading, high light intensity, high temperature) and biotic stressors (bacterial infections). By integrating deep learning with vibrational spectroscopy, DIVA paves the way for AI-driven plant health assessment, fostering more resilient and sustainable agricultural practices.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supernova: Achieving More with Less in Transformer Architectures</title>
<link>https://arxiv.org/abs/2507.15773</link>
<guid>https://arxiv.org/abs/2507.15773</guid>
<content:encoded><![CDATA[
arXiv:2507.15773v1 Announce Type: cross 
Abstract: We present Supernova, a 650M-parameter decoder-only transformer that demonstrates how careful architectural design and tokenization innovation can achieve the performance of larger models while maintaining computational efficiency. Our architecture combines Rotary Positional Embeddings (RoPE), Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for computational efficiency, and SwiGLU activation functions. A critical innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which achieves state-of-the-art compression performance. Through detailed analysis, we show that Supernova achieves 90% of the performance of 1B-parameter models while using 53% fewer parameters and requiring only 100B training tokens--an order of magnitude less than competing models. Our findings challenge the prevailing scaling paradigm, demonstrating that architectural efficiency and tokenization quality can compensate for reduced parameter counts.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics is what you need for time-series forecasting!</title>
<link>https://arxiv.org/abs/2507.15774</link>
<guid>https://arxiv.org/abs/2507.15774</guid>
<content:encoded><![CDATA[
arXiv:2507.15774v1 Announce Type: cross 
Abstract: While boundaries between data modalities are vanishing, the usual successful deep models are still challenged by simple ones in the time-series forecasting task. Our hypothesis is that this task needs models that are able to learn the data underlying dynamics. We propose to validate it through both systemic and empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to analyze existing models through the lens of dynamics. Two observations thus emerged: $\textbf{1}$. under-performing architectures learn dynamics at most partially, $\textbf{2}$. the location of the dynamics block at the model end is of prime importance. We conduct extensive experiments to confirm our observations on a set of performance-varying models with diverse backbones. Results support the need to incorporate a learnable dynamics block and its use as the final predictor.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Null Geodesics for Gravitational Lensing Rendering in General Relativity</title>
<link>https://arxiv.org/abs/2507.15775</link>
<guid>https://arxiv.org/abs/2507.15775</guid>
<content:encoded><![CDATA[
arXiv:2507.15775v1 Announce Type: cross 
Abstract: We present GravLensX, an innovative method for rendering black holes with gravitational lensing effects using neural networks. The methodology involves training neural networks to fit the spacetime around black holes and then employing these trained models to generate the path of light rays affected by gravitational lensing. This enables efficient and scalable simulations of black holes with optically thin accretion disks, significantly decreasing the time required for rendering compared to traditional methods. We validate our approach through extensive rendering of multiple black hole systems with superposed Kerr metric, demonstrating its capability to produce accurate visualizations with significantly $15\times$ reduced computational time. Our findings suggest that neural networks offer a promising alternative for rendering complex astrophysical phenomena, potentially paving a new path to astronomical visualization.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance</title>
<link>https://arxiv.org/abs/2507.15783</link>
<guid>https://arxiv.org/abs/2507.15783</guid>
<content:encoded><![CDATA[
arXiv:2507.15783v1 Announce Type: cross 
Abstract: As Generative Artificial Intelligence (GenAI) driven chatbots like Character.AI become embedded in adolescent life, they raise concerns about emotional dependence and digital overreliance. While studies have investigated the overreliance of adults on these chatbots, they have not investigated teens' interactions with chatbots with customizable personas. We analyzed 318 Reddit posts made by users self-reported as 13-17 years old on the Character.AI subreddit to understand patterns of overreliance. We found teens commonly begin using chatbots for emotional support or creative expression, but many develop strong attachments that interfere with offline relationships and daily routines. Their posts revealed recurring signs of psychological distress, cycles of relapse, and difficulty disengaging. Teens reported that their overreliance often ended when they reflect on the harm, return to in-person social settings, or become frustrated by platform restrictions. Based on the implications of our findings, we provide recommendations for future chatbot design so they can promote self-awareness, support real-world engagement, and involve teens in developing safer digital tools.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.15788</link>
<guid>https://arxiv.org/abs/2507.15788</guid>
<content:encoded><![CDATA[
arXiv:2507.15788v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have demonstrated emergent capabilities in complex reasoning, largely spurred by rule-based Reinforcement Learning (RL) techniques applied during the post-training. This has raised the question of whether similar methods can instill more nuanced, human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This paper investigates whether small-scale LLMs can acquire a robust and generalizable ToM capability through RL with verifiable rewards (RLVR). We conduct a systematic evaluation by training models on various combinations of prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for generalization on held-out datasets (e.g., OpenToM). Our findings indicate that small LLMs struggle to develop a generic ToM capability. While performance on in-distribution tasks improves, this capability fails to transfer to unseen ToM tasks with different characteristics. Furthermore, we demonstrate that prolonged RL training leads to models ``hacking'' the statistical patterns of the training datasets, resulting in significant performance gains on in-domain data but no change, or degradation of performance on out-of-distribution tasks. This suggests the learned behavior is a form of narrow overfitting rather than the acquisition of a true, abstract ToM capability.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction</title>
<link>https://arxiv.org/abs/2507.15803</link>
<guid>https://arxiv.org/abs/2507.15803</guid>
<content:encoded><![CDATA[
arXiv:2507.15803v1 Announce Type: cross 
Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive and high-quality annotated data, which is costly to obtain. Semi-supervised semantic segmentation (SSSS) has emerged as a solution to alleviate the labeling burden by leveraging both labeled and unlabeled data through self-training techniques. Meanwhile, the advent of foundational segmentation models pre-trained on massive data, has shown the potential to generalize across domains effectively. This work explores whether a foundational segmentation model can address label scarcity in the pixel-level vision task as an annotator for unlabeled images. Specifically, we investigate the efficacy of using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual input, to generate predictive masks for unlabeled data. To address the shortcomings of using SEEM-generated masks as supervision, we propose ConformalSAM, a novel SSSS framework which first calibrates the foundation model using the target domain's labeled data and then filters out unreliable pixel labels of unlabeled data so that only high-confidence labels are used as supervision. By leveraging conformal prediction (CP) to adapt foundation models to target data through uncertainty calibration, ConformalSAM exploits the strong capability of the foundational segmentation model reliably which benefits the early-stage learning, while a subsequent self-reliance training strategy mitigates overfitting to SEEM-generated masks in the later training stage. Our experiment demonstrates that, on three standard benchmarks of SSSS, ConformalSAM achieves superior performance compared to recent SSSS methods and helps boost the performance of those methods as a plug-in.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>True Multimodal In-Context Learning Needs Attention to the Visual Context</title>
<link>https://arxiv.org/abs/2507.15807</link>
<guid>https://arxiv.org/abs/2507.15807</guid>
<content:encoded><![CDATA[
arXiv:2507.15807v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new tasks from a few multimodal demonstrations consisting of images, questions, and answers. Despite showing noticeable improvement on standard vision-language datasets, current MLLMs struggle to leverage visual information in the demonstrations. Specifically, they tend to neglect visual cues and over-rely on textual patterns, leading to mere text imitation rather than genuine multimodal adaptation. This behavior makes MICL still unimodal and largely restricts its practical utility. More importantly, this limitation is often concealed by the improved performance on tasks that do not require understanding the visual context. As a result, how to effectively enhance MICL ability and reliably evaluate the MICL performance remains underexplored. To address these issues, we first introduce Dynamic Attention Reallocation (DARA), an efficient fine-tuning strategy that encourages models to attend to the visual context by rebalancing attention across visual and textual tokens. In addition, we present TrueMICL, an MICL-dedicated dataset with both support and test sets that explicitly requires the integration of multimodal information-particularly visual content-for correct task completion. Extensive experiments demonstrate the effectiveness of our holistic solution, showcasing substantial improvements in the true multimodal in-context learning capabilities. Code and datasets are available at https://chenxshuo.github.io/true-micl-colm .
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do AI models help produce verified bug fixes?</title>
<link>https://arxiv.org/abs/2507.15822</link>
<guid>https://arxiv.org/abs/2507.15822</guid>
<content:encoded><![CDATA[
arXiv:2507.15822v1 Announce Type: cross 
Abstract: Among areas of software engineering where AI techniques -- particularly, Large Language Models -- seem poised to yield dramatic improvements, an attractive candidate is Automatic Program Repair (APR), the production of satisfactory corrections to software bugs. Does this expectation materialize in practice? How do we find out, making sure that proposed corrections actually work? If programmers have access to LLMs, how do they actually use them to complement their own skills?
  To answer these questions, we took advantage of the availability of a program-proving environment, which formally determines the correctness of proposed fixes, to conduct a study of program debugging with two randomly assigned groups of programmers, one with access to LLMs and the other without, both validating their answers through the proof tools. The methodology relied on a division into general research questions (Goals in the Goal-Query-Metric approach), specific elements admitting specific answers (Queries), and measurements supporting these answers (Metrics). While applied so far to a limited sample size, the results are a first step towards delineating a proper role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the use of AI for debugging and APR. The contributions also include: a detailed methodology for experiments in the use of LLMs for debugging, which other projects can reuse; a fine-grain analysis of programmer behavior, made possible by the use of full-session recording; a definition of patterns of use of LLMs, with 7 distinct categories; and validated advice for getting the best of LLMs for debugging and Automatic Program Repair.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work</title>
<link>https://arxiv.org/abs/2507.15823</link>
<guid>https://arxiv.org/abs/2507.15823</guid>
<content:encoded><![CDATA[
arXiv:2507.15823v1 Announce Type: cross 
Abstract: Publications in the AI for Good space have tended to focus on the research and model development that can support high-impact applications. However, very few AI for Good papers discuss the process of deploying and collaborating with the partner organization, and the resulting real-world impact. In this work, we share details about the close collaboration with a humanitarian-to-humanitarian (H2H) organization and how to not only deploy the AI model in a resource-constrained environment, but also how to maintain it for continuous performance updates, and share key takeaways for practitioners.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers</title>
<link>https://arxiv.org/abs/2507.15833</link>
<guid>https://arxiv.org/abs/2507.15833</guid>
<content:encoded><![CDATA[
arXiv:2507.15833v1 Announce Type: cross 
Abstract: Human vision is a highly active process driven by gaze, which directs attention and fixation to task-relevant regions and dramatically reduces visual processing. In contrast, robot learning systems typically rely on passive, uniform processing of raw camera images. In this work, we explore how incorporating human-like active gaze into robotic policies can enhance both efficiency and performance. We build on recent advances in foveated image processing and apply them to an Active Vision robot system that emulates both human head movement and eye tracking. Extending prior work on the AV-ALOHA robot simulation platform, we introduce a framework for simultaneously collecting eye-tracking data and robot demonstrations from a human operator as well as a simulation benchmark and dataset for training robot policies that incorporate human gaze. Given the widespread use of Vision Transformers (ViTs) in robot learning, we integrate gaze information into ViTs using a foveated patch tokenization scheme inspired by recent work in image segmentation. Compared to uniform patch tokenization, this significantly reduces the number of tokens-and thus computation-without sacrificing visual fidelity near regions of interest. We also explore two approaches to gaze imitation and prediction from human data. The first is a two-stage model that predicts gaze to guide foveation and action; the second integrates gaze into the action space, allowing the policy to jointly predict gaze and actions end-to-end. Our results show that our method for foveated robot vision not only drastically reduces computational overhead, but also improves performance for high precision tasks and robustness to unseen distractors. Together, these findings suggest that human-inspired visual processing offers a useful inductive bias for robotic vision systems. https://ian-chuang.github.io/gaze-av-aloha/
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs</title>
<link>https://arxiv.org/abs/2507.15839</link>
<guid>https://arxiv.org/abs/2507.15839</guid>
<content:encoded><![CDATA[
arXiv:2507.15839v1 Announce Type: cross 
Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios where real-world data collection and usage are limited by cost and scarcity. Large language models (LLMs) have demonstrated remarkable capabilities in producing high-fidelity, domain-relevant samples across various fields. However, existing approaches that directly use LLMs to generate each record individually impose prohibitive time and cost burdens, particularly when large volumes of synthetic data are required. In this work, we propose a fast, cost-effective method for realistic tabular data synthesis that leverages LLMs to infer and encode each field's distribution into a reusable sampling script. By automatically classifying fields into numerical, categorical, or free-text types, the LLM generates distribution-based scripts that can efficiently produce diverse, realistic datasets at scale without continuous model inference. Experimental results show that our approach outperforms traditional direct methods in both diversity and data realism, substantially reducing the burden of high-volume synthetic data generation. We plan to apply this methodology to accelerate testing in production pipelines, thereby shortening development cycles and improving overall system efficiency. We believe our insights and lessons learned will aid researchers and practitioners seeking scalable, cost-effective solutions for synthetic data generation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding</title>
<link>https://arxiv.org/abs/2507.15846</link>
<guid>https://arxiv.org/abs/2507.15846</guid>
<content:encoded><![CDATA[
arXiv:2507.15846v1 Announce Type: cross 
Abstract: Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G$^2$, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Language Mixing on Bilingual LLM Reasoning</title>
<link>https://arxiv.org/abs/2507.15849</link>
<guid>https://arxiv.org/abs/2507.15849</guid>
<content:encoded><![CDATA[
arXiv:2507.15849v1 Announce Type: cross 
Abstract: Proficient multilingual speakers often intentionally switch languages in the middle of a conversation. Similarly, recent reasoning-focused bilingual large language models (LLMs) with strong capabilities in both languages exhibit language mixing--alternating languages within their chain of thought. Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy, suggesting that language mixing may benefit reasoning. In this work, we study language switching in Chinese-English bilingual reasoning models. We identify reinforcement learning with verifiable rewards (RLVR) as the critical training stage that leads to language mixing. We demonstrate that language mixing can enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6 percentage points on math reasoning tasks. Additionally, a lightweight probe can be trained to predict whether a potential language switch would benefit or harm reasoning, and when used to guide decoding, increases accuracy by up to 6.25 percentage points. Our findings suggest that language mixing is not merely a byproduct of multilingual training, but is a strategic reasoning behavior.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction</title>
<link>https://arxiv.org/abs/2507.15852</link>
<guid>https://arxiv.org/abs/2507.15852</guid>
<content:encoded><![CDATA[
arXiv:2507.15852v1 Announce Type: cross 
Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Beats Autoregressive in Data-Constrained Settings</title>
<link>https://arxiv.org/abs/2507.15857</link>
<guid>https://arxiv.org/abs/2507.15857</guid>
<content:encoded><![CDATA[
arXiv:2507.15857v1 Announce Type: cross 
Abstract: Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings-where training involves repeated passes over limited data-and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We interpret this advantage as implicit data augmentation: masked diffusion exposes the model to a diverse distribution of token orderings and prediction tasks, unlike AR's fixed left-to-right factorization. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. These results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: https://diffusion-scaling.github.io.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning</title>
<link>https://arxiv.org/abs/2310.02554</link>
<guid>https://arxiv.org/abs/2310.02554</guid>
<content:encoded><![CDATA[
arXiv:2310.02554v5 Announce Type: replace 
Abstract: Federated learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. FL can be a scalable machine learning solution in big data scenarios. Traditional FL relies on the trust assumption of the central aggregator, which forms cohorts of clients honestly. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or insert fake clients, to manipulate the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator provides a proof per round, demonstrating to the clients that the aggregator executes the intended behavior faithfully. To further reduce the verification cost of clients, we use blockchain to handle the proof in a zero-knowledge way, where miners (i.e., the participants validating and maintaining the blockchain data) can verify the proof without knowing the clients' local and aggregated models. The theoretical analysis and empirical results show that zkFL achieves better security and privacy than traditional FL, without modifying the underlying FL network structure or heavily compromising the training speed.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decision support system for Forest fire management using Ontology with Big Data and LLMs</title>
<link>https://arxiv.org/abs/2405.11346</link>
<guid>https://arxiv.org/abs/2405.11346</guid>
<content:encoded><![CDATA[
arXiv:2405.11346v3 Announce Type: replace 
Abstract: Forests are crucial for ecological balance, but wildfires, a major cause of forest loss, pose significant risks. Fire weather indices, which assess wildfire risk and predict resource demands, are vital. With the rise of sensor networks in fields like healthcare and environmental monitoring, semantic sensor networks are increasingly used to gather climatic data such as wind speed, temperature, and humidity. However, processing these data streams to determine fire weather indices presents challenges, underscoring the growing importance of effective forest fire detection. This paper discusses using Apache Spark for early forest fire detection, enhancing fire risk prediction with meteorological and geographical data. Building on our previous development of Semantic Sensor Network (SSN) ontologies and Semantic Web Rules Language (SWRL) for managing forest fires in Monesterial Natural Park, we expanded SWRL to improve a Decision Support System (DSS) using a Large Language Models (LLMs) and Spark framework. We implemented real-time alerts with Spark streaming, tailored to various fire scenarios, and validated our approach using ontology metrics, query-based evaluations, LLMs score precision, F1 score, and recall measures.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents</title>
<link>https://arxiv.org/abs/2407.01511</link>
<guid>https://arxiv.org/abs/2407.01511</guid>
<content:encoded><![CDATA[
arXiv:2407.01511v4 Announce Type: replace 
Abstract: The development of autonomous agents increasingly relies on Multimodal Language Models (MLMs) to perform tasks described in natural language with GUI environments, such as websites, desktop computers, or mobile phones. Existing benchmarks for MLM agents in interactive environments are limited by their focus on a single environment, lack of detailed and generalized evaluation methods, and the complexities of constructing tasks and evaluators. To overcome these limitations, we introduce Crab, the first agent benchmark framework designed to support cross-environment tasks, incorporating a graph-based fine-grained evaluation method and an efficient mechanism for task and evaluator construction. Our framework supports multiple devices and can be easily extended to any environment with a Python interface. Leveraging Crab, we developed a cross-platform Crab Benchmark-v0 comprising 120 tasks in computer desktop and mobile phone environments. We evaluated four advanced MLMs using different single and multi-agent system configurations on this benchmark. The experimental results demonstrate that the single agent with GPT-4o achieves the best completion ratio of 38.01%. All framework code, agent code, and task datasets are publicly available at https://github.com/camel-ai/crab.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Powered Multiagent Ensemble for Mitigating Hallucination and Efficient Atrial Fibrillation Annotation of ECG Reports</title>
<link>https://arxiv.org/abs/2410.16543</link>
<guid>https://arxiv.org/abs/2410.16543</guid>
<content:encoded><![CDATA[
arXiv:2410.16543v3 Announce Type: replace 
Abstract: This study introduces a LLMs powered multiagent ensemble method to address challenges in hallucination and data labeling, particularly in large-scale EHR datasets. Manual labeling of such datasets requires domain expertise and is labor-intensive, time-consuming, expensive, and error-prone. To overcome this bottleneck, we developed an ensemble LLMs method and demonstrated its effectiveness in two real-world tasks: (1) labeling a large-scale unlabeled ECG dataset in MIMIC-IV; (2) identifying social determinants of health (SDOH) from the clinical notes of EHR. Trading off benefits and cost, we selected a pool of diverse open source LLMs with satisfactory performance. We treat each LLM's prediction as a vote and apply a mechanism of majority voting with minimal winning threshold for ensemble. We implemented an ensemble LLMs application for EHR data labeling tasks. By using the ensemble LLMs and natural language processing, we labeled MIMIC-IV ECG dataset of 623,566 ECG reports with an estimated accuracy of 98.2%. We applied the ensemble LLMs method to identify SDOH from social history sections of 1,405 EHR clinical notes, also achieving competitive performance. Our experiments show that the ensemble LLMs can outperform individual LLM even the best commercial one, and the method reduces hallucination errors. From the research, we found that (1) the ensemble LLMs method significantly reduces the time and effort required for labeling large-scale EHR data, automating the process with high accuracy and quality; (2) the method generalizes well to other text data labeling tasks, as shown by its application to SDOH identification; (3) the ensemble of a group of diverse LLMs can outperform or match the performance of the best individual LLM; and (4) the ensemble method substantially reduces hallucination errors. This approach provides a scalable and efficient solution to data-labeling challenges.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smarter Together: Combining Large Language Models and Small Models for Physiological Signals Visual Inspection</title>
<link>https://arxiv.org/abs/2501.16215</link>
<guid>https://arxiv.org/abs/2501.16215</guid>
<content:encoded><![CDATA[
arXiv:2501.16215v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown promising capabilities in visually interpreting medical time-series data. However, their general-purpose design can limit domain-specific precision, and the proprietary nature of many models poses challenges for fine-tuning on specialized clinical datasets. Conversely, small specialized models (SSMs) offer strong performance on focused tasks but lack the broader reasoning needed for complex medical decision-making. To address these complementary limitations, we introduce \ConMIL{} (Conformalized Multiple Instance Learning), a novel decision-support framework distinctively synergizes three key components: (1) a new Multiple Instance Learning (MIL) mechanism, QTrans-Pooling, designed for per-class interpretability in identifying clinically relevant physiological signal segments; (2) conformal prediction, integrated with MIL to generate calibrated, set-valued outputs with statistical reliability guarantees; and (3) a structured approach for these interpretable and uncertainty-quantified SSM outputs to enhance the visual inspection capabilities of LLMs. Our experiments on arrhythmia detection and sleep stage classification demonstrate that \ConMIL{} can enhance the accuracy of LLMs such as ChatGPT4.0, Qwen2-VL-7B, and MiMo-VL-7B-RL. For example, \ConMIL{}-supported Qwen2-VL-7B and MiMo-VL-7B-RL both achieves 94.92% and 96.82% precision on confident samples and (70.61% and 78.02%)/(78.10% and 71.98%) on uncertain samples for the two tasks, compared to 46.13% and 13.16% using the LLM alone. These results suggest that integrating task-specific models with LLMs may offer a promising pathway toward more interpretable and trustworthy AI-driven clinical decision support.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doing More with Less: A Survey on Routing Strategies for Resource Optimisation in Large Language Model-Based Systems</title>
<link>https://arxiv.org/abs/2502.00409</link>
<guid>https://arxiv.org/abs/2502.00409</guid>
<content:encoded><![CDATA[
arXiv:2502.00409v3 Announce Type: replace 
Abstract: Large Language Model (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component, such as conversational agents, are usually designed with monolithic, static architectures that rely on a single, general-purpose LLM to handle all user queries. However, these systems may be inefficient as different queries may require different levels of reasoning, domain knowledge or pre-processing. While generalist LLMs (e.g. GPT-4o, Claude-Sonnet) perform well across a wide range of tasks, they may incur significant financial, energy and computational costs. These costs may be disproportionate for simpler queries, resulting in unnecessary resource utilisation. A routing mechanism can therefore be employed to route queries to more appropriate components, such as smaller or specialised models, thereby improving efficiency and optimising resource consumption. This survey aims to provide a comprehensive overview of routing strategies in LLM-based systems. Specifically, it reviews when, why, and how routing should be integrated into LLM pipelines to improve efficiency, scalability, and performance. We define the objectives to optimise, such as cost minimisation and performance maximisation, and discuss the timing of routing within the LLM workflow, whether it occurs before or after generation. We also detail the various implementation strategies, including similarity-based, supervised, reinforcement learning-based, and generative methods. Practical considerations such as industrial applications and current limitations are also examined, like standardising routing experiments, accounting for non-financial costs, and designing adaptive strategies. By formalising routing as a performance-cost optimisation problem, this survey provides tools and directions to guide future research and development of adaptive low-cost LLM-based systems.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Elicitation Game: Evaluating Capability Elicitation Techniques</title>
<link>https://arxiv.org/abs/2502.02180</link>
<guid>https://arxiv.org/abs/2502.02180</guid>
<content:encoded><![CDATA[
arXiv:2502.02180v3 Announce Type: replace 
Abstract: Capability evaluations are required to understand and regulate AI systems that may be deployed or further developed. Therefore, it is important that evaluations provide an accurate estimation of an AI system's capabilities. However, in numerous cases, previously latent capabilities have been elicited from models, sometimes long after initial release. Accordingly, substantial efforts have been made to develop methods for eliciting latent capabilities from models. In this paper, we evaluate the effectiveness of capability elicitation techniques by intentionally training model organisms -- language models with hidden capabilities that are revealed by a password. We introduce a novel method for training model organisms, based on circuit-breaking, which is more robust to elicitation techniques than standard password-locked models. We focus on elicitation techniques based on prompting and activation steering, and compare these to fine-tuning methods. Prompting techniques can elicit the actual capability of both password-locked and circuit-broken model organisms in the MCQA setting, while steering fails to do so. For a code-generation task, only fine-tuning can elicit the hidden capabilities of our novel model organism. Additionally, our results suggest that combining techniques improves elicitation. Still, if possible, fine-tuning should be the method of choice to improve the trustworthiness of capability evaluations.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SensorChat: Answering Qualitative and Quantitative Questions during Long-Term Multimodal Sensor Interactions</title>
<link>https://arxiv.org/abs/2502.02883</link>
<guid>https://arxiv.org/abs/2502.02883</guid>
<content:encoded><![CDATA[
arXiv:2502.02883v3 Announce Type: replace 
Abstract: Natural language interaction with sensing systems is crucial for addressing users' personal concerns and providing health-related insights into their daily lives. When a user asks a question, the system automatically analyzes the full history of sensor data, extracts relevant information, and generates an appropriate response. However, existing systems are limited to short-duration (e.g., one minute) or low-frequency (e.g., daily step count) sensor data. In addition, they struggle with quantitative questions that require precise numerical answers. In this work, we introduce SensorChat, the first end-to-end QA system designed for daily life monitoring using long-duration, high-frequency time series data. Given raw sensor signals spanning multiple days and a user-defined natural language question, SensorChat generates semantically meaningful responses that directly address user concerns. SensorChat effectively handles both quantitative questions that require numerical precision and qualitative questions that require high-level reasoning to infer subjective insights. To achieve this, SensorChat uses an innovative three-stage pipeline including question decomposition, sensor data query, and answer assembly. The first and third stages leverage Large Language Models (LLMs) to interpret human queries and generate responses. The intermediate querying stage extracts relevant information from the complete sensor data history. Real-world implementations demonstrate SensorChat's capability for real-time interactions on a cloud server while also being able to run entirely on edge platforms after quantization. Comprehensive QA evaluations show that SensorChat achieves 93% higher answer accuracy than the best performing state-of-the-art systems on quantitative questions. Furthermore, a user study with eight volunteers highlights SensorChat's effectiveness in answering qualitative questions.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering LLMs with Logical Reasoning: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2502.15652</link>
<guid>https://arxiv.org/abs/2502.15652</guid>
<content:encoded><![CDATA[
arXiv:2502.15652v4 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable successes on various tasks. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs, which can be categorized into the following two aspects: (1) Logical question answering: LLMs often fail to generate the correct answer within a complex logical problem which requires sophisticated deductive, inductive or abductive reasoning given a collection of premises. (2) Logical consistency: LLMs are prone to producing responses contradicting themselves across different questions. For example, a state-of-the-art question-answering LLM Macaw, answers Yes to both questions Is a magpie a bird? and Does a bird have wings? but answers No to Does a magpie have wings?. To facilitate this research direction, we comprehensively investigate the most cutting-edge methods and propose a detailed taxonomy. Specifically, to accurately answer complex logic questions, previous methods can be categorized based on reliance on external solvers, prompts, and fine-tuning. To avoid logical contradictions, we discuss concepts and solutions of various logical consistencies, including implication, negation, transitivity, factuality consistencies, and their composites. In addition, we review commonly used benchmark datasets and evaluation metrics, and discuss promising research directions, such as extending to modal logic to account for uncertainty and developing efficient algorithms that simultaneously satisfy multiple logical consistencies.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Principles for AI Cost and Compute Accounting</title>
<link>https://arxiv.org/abs/2502.15873</link>
<guid>https://arxiv.org/abs/2502.15873</guid>
<content:encoded><![CDATA[
arXiv:2502.15873v4 Announce Type: replace 
Abstract: Policymakers increasingly use development cost and compute as proxies for AI capabilities and risks. Recent laws have introduced regulatory requirements for models or developers that are contingent on specific thresholds. However, technical ambiguities in how to perform this accounting create loopholes that can undermine regulatory effectiveness. We propose seven principles for designing AI cost and compute accounting standards that (1) reduce opportunities for strategic gaming, (2) avoid disincentivizing responsible risk mitigation, and (3) enable consistent implementation across companies and jurisdictions.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combinatorial Optimization for All: Using LLMs to Aid Non-Experts in Improving Optimization Algorithms</title>
<link>https://arxiv.org/abs/2503.10968</link>
<guid>https://arxiv.org/abs/2503.10968</guid>
<content:encoded><![CDATA[
arXiv:2503.10968v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown notable potential in code generation for optimization algorithms, unlocking exciting new opportunities. This paper examines how LLMs, rather than creating algorithms from scratch, can improve existing ones without the need for specialized expertise. To explore this potential, we selected 10 baseline optimization algorithms from various domains (metaheuristics, reinforcement learning, deterministic, and exact methods) to solve the classic Travelling Salesman Problem. The results show that our simple methodology often results in LLM-generated algorithm variants that improve over the baseline algorithms in terms of solution quality, reduction in computational time, and simplification of code complexity, all without requiring specialized optimization knowledge or advanced algorithmic implementation skills.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Palatable Conceptions of Disembodied Being</title>
<link>https://arxiv.org/abs/2503.16348</link>
<guid>https://arxiv.org/abs/2503.16348</guid>
<content:encoded><![CDATA[
arXiv:2503.16348v3 Announce Type: replace 
Abstract: Is it possible to articulate a conception of consciousness that is compatible with the exotic characteristics of contemporary, disembodied AI systems, and that can stand up to philosophical scrutiny? How would subjective time and selfhood show up for an entity that conformed to such a conception? Trying to answer these questions, even metaphorically, stretches the language of consciousness to breaking point. Ultimately, the attempt yields something like emptiness, in the Buddhist sense, and helps to undermine our dualistic inclinations towards subjectivity and selfhood.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Library of LLM Intrinsics for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.11704</link>
<guid>https://arxiv.org/abs/2504.11704</guid>
<content:encoded><![CDATA[
arXiv:2504.11704v2 Announce Type: replace 
Abstract: In the developer community for large language models (LLMs), there is not yet a clean pattern analogous to a software library, to support very large scale collaboration. Even for the commonplace use case of Retrieval-Augmented Generation (RAG), it is not currently possible to write a RAG application against a well-defined set of APIs that are agreed upon by different LLM providers. Inspired by the idea of compiler intrinsics, we propose some elements of such a concept through introducing a library of LLM Intrinsics for RAG. An LLM intrinsic is defined as a capability that can be invoked through a well-defined API that is reasonably stable and independent of how the LLM intrinsic itself is implemented. The intrinsics in our library are released as LoRA adapters on HuggingFace, and through a software interface with clear structured input/output characteristics on top of vLLM as an inference platform, accompanied in both places with documentation and code. This article describes the intended usage, training details, and evaluations for each intrinsic, as well as compositions of multiple intrinsics.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision for Auto Research with LLM Agents</title>
<link>https://arxiv.org/abs/2504.18765</link>
<guid>https://arxiv.org/abs/2504.18765</guid>
<content:encoded><![CDATA[
arXiv:2504.18765v3 Announce Type: replace 
Abstract: This paper introduces Agent-Based Auto Research, a structured multi-agent framework designed to automate, coordinate, and optimize the full lifecycle of scientific research. Leveraging the capabilities of large language models (LLMs) and modular agent collaboration, the system spans all major research phases, including literature review, ideation, methodology planning, experimentation, paper writing, peer review response, and dissemination. By addressing issues such as fragmented workflows, uneven methodological expertise, and cognitive overload, the framework offers a systematic and scalable approach to scientific inquiry. Preliminary explorations demonstrate the feasibility and potential of Auto Research as a promising paradigm for self-improving, AI-driven research processes.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning</title>
<link>https://arxiv.org/abs/2504.19027</link>
<guid>https://arxiv.org/abs/2504.19027</guid>
<content:encoded><![CDATA[
arXiv:2504.19027v2 Announce Type: replace 
Abstract: Explainable artificial intelligence (XAI) has become increasingly important in decision-critical domains such as healthcare, finance, and law. Counterfactual (CF) explanations, a key approach in XAI, provide users with actionable insights by suggesting minimal modifications to input features that lead to different model outcomes. Despite significant advancements, existing CF generation methods often struggle to balance proximity, diversity, and robustness, limiting their real-world applicability. A widely adopted framework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but lacks robustness, making CF explanations sensitive to perturbations and domain constraints. To address these challenges, we introduce DiCE-Extended, an enhanced CF explanation framework that integrates multi-objective optimization techniques to improve robustness while maintaining interpretability. Our approach introduces a novel robustness metric based on the Dice-S{\o}rensen coefficient, enabling stability under small input variations. Additionally, we refine CF generation using weighted loss components (lambda_p, lambda_d, lambda_r) to balance proximity, diversity, and robustness. We empirically validate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German Credit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch, TensorFlow). Results demonstrate improved CF validity, stability, and alignment with decision boundaries compared to standard DiCE-generated explanations. Our findings highlight the potential of DiCE-Extended in generating more reliable and interpretable CFs for high-stakes applications. Future work could explore adaptive optimization techniques and domain-specific constraints to further enhance CF generation in real-world scenarios
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Mind to Machine: The Rise of Manus AI as a Fully Autonomous Digital Agent</title>
<link>https://arxiv.org/abs/2505.02024</link>
<guid>https://arxiv.org/abs/2505.02024</guid>
<content:encoded><![CDATA[
arXiv:2505.02024v2 Announce Type: replace 
Abstract: Manus AI is a general-purpose AI agent introduced in early 2025, marking a significant advancement in autonomous artificial intelligence. Developed by the Chinese startup Monica.im, Manus is designed to bridge the gap between "mind" and "hand" - combining the reasoning and planning capabilities of large language models with the ability to execute complex, end-to-end tasks that produce tangible outcomes. This paper presents a comprehensive overview of Manus AI, exploring its core technical architecture, diverse applications across sectors such as healthcare, finance, manufacturing, robotics, and gaming, as well as its key strengths, current limitations, and future potential. Positioned as a preview of what lies ahead, Manus AI represents a shift toward intelligent agents that can translate high-level intentions into real-world actions, heralding a new era of human-AI collaboration.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Autonomous Cyber Defenders</title>
<link>https://arxiv.org/abs/2505.04843</link>
<guid>https://arxiv.org/abs/2505.04843</guid>
<content:encoded><![CDATA[
arXiv:2505.04843v2 Announce Type: replace 
Abstract: Fast and effective incident response is essential to prevent adversarial cyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response through Artificial Intelligence (AI) agents that plan and execute actions. Most ACD approaches focus on single-agent scenarios and leverage Reinforcement Learning (RL). However, ACD RL-trained agents depend on costly training, and their reasoning is not always explainable or transferable. Large Language Models (LLMs) can address these concerns by providing explainable actions in general security contexts. Researchers have explored LLM agents for ACD but have not evaluated them on multi-agent scenarios or interacting with other ACD agents. In this paper, we show the first study on how LLMs perform in multi-agent ACD environments by proposing a new integration to the CybORG CAGE 4 environment. We examine how ACD teams of LLM and RL agents can interact by proposing a novel communication protocol. Our results highlight the strengths and weaknesses of LLMs and RL and help us identify promising research directions to create, train, and deploy future teams of ACD agents.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models</title>
<link>https://arxiv.org/abs/2505.08622</link>
<guid>https://arxiv.org/abs/2505.08622</guid>
<content:encoded><![CDATA[
arXiv:2505.08622v2 Announce Type: replace 
Abstract: Text-to-image generative models like DALL-E and Stable Diffusion have revolutionized visual content creation across various applications, including advertising, personalized media, and design prototyping. However, crafting effective textual prompts to guide these models remains challenging, often requiring extensive trial and error. Existing prompt inversion approaches, such as soft and hard prompt techniques, are not so effective due to the limited interpretability and incoherent prompt generation. To address these issues, we propose Visually Guided Decoding (VGD), a gradient-free approach that leverages large language models (LLMs) and CLIP-based guidance to generate coherent and semantically aligned prompts. In essence, VGD utilizes the robust text generation capabilities of LLMs to produce human-readable prompts. Further, by employing CLIP scores to ensure alignment with user-specified visual concepts, VGD enhances the interpretability, generalization, and flexibility of prompt generation without the need for additional training. Our experiments demonstrate that VGD outperforms existing prompt inversion techniques in generating understandable and contextually relevant prompts, facilitating more intuitive and controllable interactions with text-to-image models.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios</title>
<link>https://arxiv.org/abs/2505.12891</link>
<guid>https://arxiv.org/abs/2505.12891</guid>
<content:encoded><![CDATA[
arXiv:2505.12891v2 Announce Type: replace 
Abstract: Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose a multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News, and TIME-Dial. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-Lite, a human-annotated subset to foster future research and standardized evaluation in temporal reasoning. The code is available at https://github.com/sylvain-wei/TIME , and the dataset is available at https://huggingface.co/datasets/SylvainWei/TIME .
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning</title>
<link>https://arxiv.org/abs/2505.19099</link>
<guid>https://arxiv.org/abs/2505.19099</guid>
<content:encoded><![CDATA[
arXiv:2505.19099v5 Announce Type: replace 
Abstract: We present SeePhys, a large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features a substantial proportion of vision-essential problems (75%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our benchmark. These results reveal fundamental challenges in current large language models' visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ultimate Test of Superintelligent AI Agents: Can an AI Balance Care and Control in Asymmetric Relationships?</title>
<link>https://arxiv.org/abs/2506.01813</link>
<guid>https://arxiv.org/abs/2506.01813</guid>
<content:encoded><![CDATA[
arXiv:2506.01813v2 Announce Type: replace 
Abstract: This paper introduces the Shepherd Test, a new conceptual test for assessing the moral and relational dimensions of superintelligent artificial agents. The test is inspired by human interactions with animals, where ethical considerations about care, manipulation, and consumption arise in contexts of asymmetric power and self-preservation. We argue that AI crosses an important, and potentially dangerous, threshold of intelligence when it exhibits the ability to manipulate, nurture, and instrumentally use less intelligent agents, while also managing its own survival and expansion goals. This includes the ability to weigh moral trade-offs between self-interest and the well-being of subordinate agents. The Shepherd Test thus challenges traditional AI evaluation paradigms by emphasizing moral agency, hierarchical behavior, and complex decision-making under existential stakes. We argue that this shift is critical for advancing AI governance, particularly as AI systems become increasingly integrated into multi-agent environments. We conclude by identifying key research directions, including the development of simulation environments for testing moral behavior in AI, and the formalization of ethical manipulation within multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation</title>
<link>https://arxiv.org/abs/2506.12689</link>
<guid>https://arxiv.org/abs/2506.12689</guid>
<content:encoded><![CDATA[
arXiv:2506.12689v2 Announce Type: replace 
Abstract: The rapid growth of scientific literature demands robust tools for automated survey-generation. However, current large language model (LLM)-based methods often lack in-depth analysis, structural coherence, and reliable citations. To address these limitations, we introduce SciSage, a multi-agent framework employing a reflect-when-you-write paradigm. SciSage features a hierarchical Reflector agent that critically evaluates drafts at outline, section, and document levels, collaborating with specialized agents for query interpretation, content retrieval, and refinement. We also release SurveyScope, a rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11 computer science domains, with strict recency and citation-based quality controls. Evaluations demonstrate that SciSage outperforms state-of-the-art baselines (LLM x MapReduce-V2, AutoSurvey), achieving +1.73 points in document coherence and +32% in citation F1 scores. Human evaluations reveal mixed outcomes (3 wins vs. 7 losses against human-written surveys), but highlight SciSage's strengths in topical breadth and retrieval efficiency. Overall, SciSage offers a promising foundation for research-assistive writing tools.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Guide for Evaluating LLMs and LLM-Reliant Systems</title>
<link>https://arxiv.org/abs/2506.13023</link>
<guid>https://arxiv.org/abs/2506.13023</guid>
<content:encoded><![CDATA[
arXiv:2506.13023v2 Announce Type: replace 
Abstract: Recent advances in generative AI have led to remarkable interest in using systems that rely on large language models (LLMs) for practical applications. However, meaningful evaluation of these systems in real-world scenarios comes with a distinct set of challenges, which are not well-addressed by synthetic benchmarks and de-facto metrics that are often seen in the literature. We present a practical evaluation framework which outlines how to proactively curate representative datasets, select meaningful evaluation metrics, and employ meaningful evaluation methodologies that integrate well with practical development and deployment of LLM-reliant systems that must adhere to real-world requirements and meet user-facing needs.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?</title>
<link>https://arxiv.org/abs/2506.21763</link>
<guid>https://arxiv.org/abs/2506.21763</guid>
<content:encoded><![CDATA[
arXiv:2506.21763v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are accelerating scientific idea generation, but rigorously evaluating these numerous, often superficial, AI-generated propositions for novelty and factual accuracy is a critical bottleneck; manual verification is too slow. Existing validation methods are inadequate: LLMs as standalone verifiers may hallucinate and lack domain knowledge (our findings show 60% unawareness of relevant papers in specific domains), while traditional citation networks lack explicit causality and narrative surveys are unstructured. This underscores a core challenge: the absence of structured, verifiable, and causally-linked historical data of scientific evolution.To address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology \textbf{H}istory \textbf{E}volution Tree), a computational framework that constructs such domain-specific evolution trees from scientific literature. THE-Tree employs a search algorithm to explore evolutionary paths. During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify" process: an LLM proposes potential advancements and cites supporting literature. Critically, each proposed evolutionary link is then validated for logical coherence and evidential support by a recovered natural language inference mechanism that interrogates the cited literature, ensuring that each step is grounded. We construct and validate 88 THE-Trees across diverse domains and release a benchmark dataset including up to 71k fact verifications covering 27k papers to foster further research. Experiments demonstrate that i) in graph completion, our THE-Tree improves hit@1 by 8% to 14% across multiple models compared to traditional citation networks; ii) for predicting future scientific developments, it improves hit@1 metric by nearly 10%; and iii) when combined with other methods, it boosts the performance of evaluating important scientific papers by almost 100%.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LumiCRS: Asymmetric Contrastive Prototype Learning for Long-Tail Conversational Recommender Systems</title>
<link>https://arxiv.org/abs/2507.04722</link>
<guid>https://arxiv.org/abs/2507.04722</guid>
<content:encoded><![CDATA[
arXiv:2507.04722v2 Announce Type: replace 
Abstract: Conversational recommender systems (CRSs) often suffer from an extreme long-tail distribution of dialogue data, causing a strong bias toward head-frequency blockbusters that sacrifices diversity and exacerbates the cold-start problem. An empirical analysis of DCRS and statistics on the REDIAL corpus show that only 10% of head movies account for nearly half of all mentions, whereas about 70% of tail movies receive merely 26% of the attention. This imbalance gives rise to three critical challenges: head over-fitting, body representation drift, and tail sparsity. To address these issues, we propose LumiCRS, an end-to-end framework that mitigates long-tail imbalance through three mutually reinforcing layers: (i) an Adaptive Comprehensive Focal Loss (ACFL) that dynamically adjusts class weights and focusing factors to curb head over-fitting and reduce popularity bias; (ii) Prototype Learning for Long-Tail Recommendation, which selects semantic, affective, and contextual prototypes to guide clustering and stabilize body and tail representations; and (iii) a GPT-4o-driven prototype-guided dialogue augmentation module that automatically generates diverse long-tail conversational snippets to alleviate tail sparsity and distribution shift. Together, these strategies enable LumiCRS to markedly improve recommendation accuracy, diversity, and fairness: on the REDIAL and INSPIRED benchmarks, LumiCRS boosts Recall@10 and Tail-Recall@10 by 7-15% over fifteen strong baselines, while human evaluations confirm superior fluency, informativeness, and long-tail relevance. These results demonstrate the effectiveness of multi-layer collaboration in building an efficient and fair long-tail conversational recommender.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Deontic Modal Logic in the s(CASP) Goal-directed Predicate Answer Set Programming System</title>
<link>https://arxiv.org/abs/2507.05519</link>
<guid>https://arxiv.org/abs/2507.05519</guid>
<content:encoded><![CDATA[
arXiv:2507.05519v3 Announce Type: replace 
Abstract: We consider the problem of implementing deontic modal logic. We show how (deontic) modal operators can be expressed elegantly using default negation (negation-as-failure) and strong negation present in answer set programming (ASP). We propose using global constraints of ASP to represent obligations and impermissibilities of deontic modal logic. We show that our proposed representation results in the various paradoxes of deontic modal logic being elegantly resolved.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Methods for Neural-Symbolic AI</title>
<link>https://arxiv.org/abs/2507.08216</link>
<guid>https://arxiv.org/abs/2507.08216</guid>
<content:encoded><![CDATA[
arXiv:2507.08216v2 Announce Type: replace 
Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to process the input entities, while relying on a reasoner based on First-Order Logic to represent and process more complex relationships among the entities. A fundamental role for these methods is played by the process of logic grounding, which determines the relevant substitutions for the logic rules using a (sub)set of entities. Some NeSy methods use an exhaustive derivation of all possible substitutions, preserving the full expressive power of the logic knowledge. This leads to a combinatorial explosion in the number of ground formulas to consider and, therefore, strongly limits their scalability. Other methods rely on heuristic-based selective derivations, which are generally more computationally efficient, but lack a justification and provide no guarantees of preserving the information provided to and returned by the reasoner. Taking inspiration from multi-hop symbolic reasoning, this paper proposes a parametrized family of grounding methods generalizing classic Backward Chaining. Different selections within this family allow us to obtain commonly employed grounding methods as special cases, and to control the trade-off between expressiveness and scalability of the reasoner. The experimental results show that the selection of the grounding criterion is often as important as the NeSy method itself.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recognizing and Eliciting Weakly Single Crossing Profiles on Trees</title>
<link>https://arxiv.org/abs/1611.04175</link>
<guid>https://arxiv.org/abs/1611.04175</guid>
<content:encoded><![CDATA[
arXiv:1611.04175v3 Announce Type: replace-cross 
Abstract: We introduce and study the weakly single-crossing domain on trees which is a generalization of the well-studied single-crossing domain in social choice theory. We design a polynomial-time algorithm for recognizing preference profiles which belong to this domain. We then develop an efficient elicitation algorithm for this domain which works even if the preferences can be accessed only sequentially and the underlying single-crossing tree structure is not known beforehand. We also prove matching lower bound on the query complexity of our elicitation algorithm when the number of voters is large compared to the number of candidates. We also prove a lower bound of $\Omega(m^2\log n)$ on the number of queries that any algorithm needs to ask to elicit single crossing profile when random queries are allowed. This resolves an open question in an earlier paper and proves optimality of their preference elicitation algorithm when random queries are allowed.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Abductive forgetting</title>
<link>https://arxiv.org/abs/2209.12825</link>
<guid>https://arxiv.org/abs/2209.12825</guid>
<content:encoded><![CDATA[
arXiv:2209.12825v2 Announce Type: replace-cross 
Abstract: Abductive forgetting is removing variables from a logical formula while maintaining its abductive explanations. It is carried in two alternative ways depending on its intended application. Both differ from the usual forgetting, which maintains consequences rather than explanations. Differently from that, abductive forgetting from a propositional formula may not be expressed by any propositional formula. A necessary and sufficient condition tells when it is. Checking it is $\Pi^p_3$-complete. A way to guarantee expressibility of abductive forgetting is to switch from propositional to default logic. Another is to introduce new variables.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages</title>
<link>https://arxiv.org/abs/2303.09823</link>
<guid>https://arxiv.org/abs/2303.09823</guid>
<content:encoded><![CDATA[
arXiv:2303.09823v2 Announce Type: replace-cross 
Abstract: This paper describes our participation in the shared task of hate speech detection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our experiments evaluate the performance of six transformer models and their combination using 2 ensemble approaches. The best results on the training set, in a five-fold cross validation scenario, were obtained by using the ensemble approach based on the majority vote. The evaluation of this approach on the test set resulted in an F1-score of 0.60 and an Accuracy of 0.86.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PFB-Diff: Progressive Feature Blending Diffusion for Text-driven Image Editing</title>
<link>https://arxiv.org/abs/2306.16894</link>
<guid>https://arxiv.org/abs/2306.16894</guid>
<content:encoded><![CDATA[
arXiv:2306.16894v2 Announce Type: replace-cross 
Abstract: Diffusion models have demonstrated their ability to generate diverse and high-quality images, sparking considerable interest in their potential for real image editing applications. However, existing diffusion-based approaches for local image editing often suffer from undesired artifacts due to the latent-level blending of the noised target images and diffusion latent variables, which lack the necessary semantics for maintaining image consistency. To address these issues, we propose PFB-Diff, a Progressive Feature Blending method for Diffusion-based image editing. Unlike previous methods, PFB-Diff seamlessly integrates text-guided generated content into the target image through multi-level feature blending. The rich semantics encoded in deep features and the progressive blending scheme from high to low levels ensure semantic coherence and high quality in edited images. Additionally, we introduce an attention masking mechanism in the cross-attention layers to confine the impact of specific words to desired regions, further improving the performance of background editing and multi-object replacement. PFB-Diff can effectively address various editing tasks, including object/background replacement and object attribute editing. Our method demonstrates its superior performance in terms of editing accuracy and image quality without the need for fine-tuning or training. Our implementation is available at https://github.com/CMACH508/PFB-Diff.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of the Evolution of Language Model-Based Dialogue Systems: Data, Task and Models</title>
<link>https://arxiv.org/abs/2311.16789</link>
<guid>https://arxiv.org/abs/2311.16789</guid>
<content:encoded><![CDATA[
arXiv:2311.16789v2 Announce Type: replace-cross 
Abstract: Dialogue systems (DS), including the task-oriented dialogue system (TOD) and the open-domain dialogue system (ODD), have always been a fundamental task in natural language processing (NLP), allowing various applications in practice. Owing to sophisticated training and well-designed model architecture, language models (LM) are usually adopted as the necessary backbone to build the dialogue system. Consequently, every breakthrough in LM brings about a shift in learning paradigm and research attention within dialogue system, especially the appearance of pre-trained language models (PLMs) and large language models (LLMs). In this paper, we take a deep look at the history of the dialogue system, especially its special relationship with the advancements of language models. Thus, our survey offers a systematic perspective, categorizing different stages in a chronological order aligned with LM breakthroughs, providing a comprehensive review of state-of-the-art research outcomes. What's more, we turn our attention to emerging topics and engage in a discussion on open challenges, providing valuable insights into the future directions for LLM-based dialogue systems. In summary, this survey delves into the dynamic interplay between language models and dialogue systems, unraveling the evolutionary path of this essential relationship. Through this exploration, we pave the way for a deeper comprehension of the field, guiding future developments in LM-based dialogue systems.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defending Against Unforeseen Failure Modes with Latent Adversarial Training</title>
<link>https://arxiv.org/abs/2403.05030</link>
<guid>https://arxiv.org/abs/2403.05030</guid>
<content:encoded><![CDATA[
arXiv:2403.05030v5 Announce Type: replace-cross 
Abstract: Despite extensive diagnostics and debugging by developers, AI systems sometimes exhibit harmful unintended behaviors. Finding and fixing these is challenging because the attack surface is so large -- it is not tractable to exhaustively search for inputs that may elicit harmful behaviors. Red-teaming and adversarial training (AT) are commonly used to improve robustness, however, they empirically struggle to fix failure modes that differ from the attacks used during training. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without leveraging knowledge of what they are or using inputs that elicit them. LAT makes use of the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. Here, we use it to defend against failure modes without examples that elicit them. Specifically, we use LAT to remove backdoors and defend against held-out classes of adversarial attacks. We show in image classification, text classification, and text generation tasks that LAT usually improves both robustness to novel attacks and performance on clean data relative to AT. This suggests that LAT can be a promising tool for defending against failure modes that are not explicitly identified by developers.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI</title>
<link>https://arxiv.org/abs/2403.10559</link>
<guid>https://arxiv.org/abs/2403.10559</guid>
<content:encoded><![CDATA[
arXiv:2403.10559v2 Announce Type: replace-cross 
Abstract: This report investigates the history and impact of Generative Models and Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing progress in technology and transportation. By focusing on the application of generative models within the context of CAVs, the study aims to unravel how this integration could enhance predictive modeling, simulation accuracy, and decision-making processes in autonomous vehicles. This thesis discusses the benefits and challenges of integrating generative models and CAV technology in transportation. It aims to highlight the progress made, the remaining obstacles, and the potential for advancements in safety and innovation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Consistency Trajectory Models for Image Manipulation</title>
<link>https://arxiv.org/abs/2403.12510</link>
<guid>https://arxiv.org/abs/2403.12510</guid>
<content:encoded><![CDATA[
arXiv:2403.12510v4 Announce Type: replace-cross 
Abstract: Diffusion models (DMs) excel in unconditional generation, as well as on applications such as image editing and restoration. The success of DMs lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. This work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbitrary distributions via ODEs. We discuss the design space of GCTMs and demonstrate their efficacy in various image manipulation tasks such as image-to-image translation, restoration, and editing.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Mobile Device Control Agents across Diverse Configurations</title>
<link>https://arxiv.org/abs/2404.16660</link>
<guid>https://arxiv.org/abs/2404.16660</guid>
<content:encoded><![CDATA[
arXiv:2404.16660v3 Announce Type: replace-cross 
Abstract: Mobile device control agents can largely enhance user interactions and productivity by automating daily tasks. However, despite growing interest in developing practical agents, the absence of a commonly adopted benchmark in this area makes it challenging to quantify scientific progress. In this work, we introduce B-MoCA: a novel benchmark with interactive environments for evaluating and developing mobile device control agents. To create a realistic benchmark, we develop B-MoCA based on the Android operating system and define 131 common daily tasks. Importantly, we incorporate a randomization feature that changes the configurations of mobile devices, including user interface layouts and language settings, to assess generalization performance. We benchmark diverse agents, including agents employing large language models (LLMs) or multi-modal LLMs as well as agents trained with imitation learning using human expert demonstrations. While these agents demonstrate proficiency in executing straightforward tasks, their poor performance on complex tasks highlights significant opportunities for future research to improve effectiveness. Our source code is publicly available at https://b-moca.github.io.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OCK: Unsupervised Dynamic Video Prediction with Object-Centric Kinematics</title>
<link>https://arxiv.org/abs/2404.18423</link>
<guid>https://arxiv.org/abs/2404.18423</guid>
<content:encoded><![CDATA[
arXiv:2404.18423v3 Announce Type: replace-cross 
Abstract: Human perception involves decomposing complex multi-object scenes into time-static object appearance (i.e., size, shape, color) and time-varying object motion (i.e., position, velocity, acceleration). For machines to achieve human-like intelligence in real-world interactions, understanding these physical properties of objects is essential, forming the foundation for dynamic video prediction. While recent advancements in object-centric transformers have demonstrated potential in video prediction, they primarily focus on object appearance, often overlooking motion dynamics, which is crucial for modeling dynamic interactions and maintaining temporal consistency in complex environments. To address these limitations, we propose OCK, a dynamic video prediction model leveraging object-centric kinematics and object slots. We introduce a novel component named Object Kinematics that comprises explicit object motions, serving as an additional attribute beyond conventional appearance features to model dynamic scenes. The Object Kinematics are integrated into various OCK mechanisms, enabling spatiotemporal prediction of complex object interactions over long video sequences. Our model demonstrates superior performance in handling complex scenes with intricate object attributes and motions, highlighting its potential for applicability in vision-related dynamics learning tasks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oversmoothing Alleviation in Graph Neural Networks: A Survey and Unified View</title>
<link>https://arxiv.org/abs/2405.01663</link>
<guid>https://arxiv.org/abs/2405.01663</guid>
<content:encoded><![CDATA[
arXiv:2405.01663v2 Announce Type: replace-cross 
Abstract: Oversmoothing is a common challenge in learning graph neural networks (GNN), where, as layers increase, embedding features learned from GNNs quickly become similar or indistinguishable, making them incapable of differentiating network proximity. A GNN with shallow layer architectures can only learn short-term relation or localized structure information, limiting its power of learning long-term connection, evidenced by their inferior learning performance on heterophilous graphs. Tackling oversmoothing is crucial for harnessing deep-layer architectures for GNNs. To date, many methods have been proposed to alleviate oversmoothing. The vast difference behind their design principles, combined with graph complications, make it difficult to understand and even compare the difference between different approaches in tackling the oversmoothing. In this paper, we propose ATNPA, a unified view with five key steps: Augmentation, Transformation, Normalization, Propagation, and Aggregation, to summarize GNN oversmoothing alleviation approaches. We first propose a taxonomy for GNN oversmoothing alleviation which includes three themes to tackle oversmoothing. After that, we separate all methods into six categories, followed by detailed reviews of representative methods, including their relation to ATNPA, and discussion of their niche, strength, and weakness. The review not only draws an in-depth understanding of existing methods in the field but also shows a clear road map for future study.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Experiences Are Influential for RL Agents? Efficiently Estimating The Influence of Experiences</title>
<link>https://arxiv.org/abs/2405.14629</link>
<guid>https://arxiv.org/abs/2405.14629</guid>
<content:encoded><![CDATA[
arXiv:2405.14629v3 Announce Type: replace-cross 
Abstract: In reinforcement learning (RL) with experience replay, experiences stored in a replay buffer influence the RL agent's performance. Information about how these experiences influence the agent's performance is valuable for various purposes, such as identifying experiences that negatively influence underperforming agents. One method for estimating the influence of experiences is the leave-one-out (LOO) method. However, this method is usually computationally prohibitive. In this paper, we present Policy Iteration with Turn-over Dropout (PIToD), which efficiently estimates the influence of experiences. We evaluate how correctly PIToD estimates the influence of experiences and its efficiency compared to LOO. We then apply PIToD to amend underperforming RL agents, i.e., we use PIToD to estimate negatively influential experiences for the RL agents and to delete the influence of these experiences. We show that RL agents' performance is significantly improved via amendments with PIToD.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles</title>
<link>https://arxiv.org/abs/2406.12644</link>
<guid>https://arxiv.org/abs/2406.12644</guid>
<content:encoded><![CDATA[
arXiv:2406.12644v5 Announce Type: replace-cross 
Abstract: Assessing the effectiveness of large language models (LLMs) in performing different tasks is crucial for understanding their strengths and weaknesses. This paper presents Hierarchical Prompting Taxonomy (HPT), grounded on human cognitive principles and designed to assess LLMs by examining the cognitive demands of various tasks. The HPT utilizes the Hierarchical Prompting Framework (HPF), which structures five unique prompting strategies in a hierarchical order based on their cognitive requirement on LLMs when compared to human mental capabilities. It assesses the complexity of tasks with the Hierarchical Prompting Index (HPI), which demonstrates the cognitive competencies of LLMs across diverse datasets and offers insights into the cognitive demands that datasets place on different LLMs. This approach enables a comprehensive evaluation of an LLMs problem solving abilities and the intricacy of a dataset, offering a standardized metric for task complexity. Extensive experiments with multiple datasets and LLMs show that HPF enhances LLM performance by 2% to 63% compared to baseline performance, with GSM8k being the most cognitively complex task among reasoning and coding tasks with an average HPI of 3.20 confirming the effectiveness of HPT. To support future research and reproducibility in this domain, the implementations of HPT and HPF are available here.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Next Frontier in Speech Representation Learning Using Disentanglement</title>
<link>https://arxiv.org/abs/2407.02543</link>
<guid>https://arxiv.org/abs/2407.02543</guid>
<content:encoded><![CDATA[
arXiv:2407.02543v2 Announce Type: replace-cross 
Abstract: The popular frameworks for self-supervised learning of speech representations have largely focused on frame-level masked prediction of speech regions. While this has shown promising downstream task performance for speech recognition and related tasks, this has largely ignored factors of speech that are encoded at coarser level, like characteristics of the speaker or channel that remain consistent through-out a speech utterance. In this work, we propose a framework for Learning Disentangled Self Supervised (termed as Learn2Diss) representations of speech, which consists of frame-level and an utterance-level encoder modules. The two encoders are initially learned independently, where the frame-level model is largely inspired by existing self supervision techniques, thereby learning pseudo-phonemic representations, while the utterance-level encoder is inspired by constrastive learning of pooled embeddings, thereby learning pseudo-speaker representations. The joint learning of these two modules consists of disentangling the two encoders using a mutual information based criterion. With several downstream evaluation experiments, we show that the proposed Learn2Diss achieves state-of-the-art results on a variety of tasks, with the frame-level encoder representations improving semantic tasks, while the utterance-level representations improve non-semantic tasks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning</title>
<link>https://arxiv.org/abs/2407.07668</link>
<guid>https://arxiv.org/abs/2407.07668</guid>
<content:encoded><![CDATA[
arXiv:2407.07668v3 Announce Type: replace-cross 
Abstract: Many real-world applications require machine-learning models to be able to deal with non-stationary data distributions and thus learn autonomously over an extended period of time, often in an online setting. One of the main challenges in this scenario is the so-called catastrophic forgetting (CF) for which the learning model tends to focus on the most recent tasks while experiencing predictive degradation on older ones. In the online setting, the most effective solutions employ a fixed-size memory buffer to store old samples used for replay when training on new tasks. Many approaches have been presented to tackle this problem. However, it is not clear how predictive uncertainty information for memory management can be leveraged in the most effective manner and conflicting strategies are proposed to populate the memory. Are the easiest-to-forget or the easiest-to-remember samples more effective in combating CF? Starting from the intuition that predictive uncertainty provides an idea of the samples' location in the decision space, this work presents an in-depth analysis of different uncertainty estimates and strategies for populating the memory. The investigation provides a better understanding of the characteristics data points should have for alleviating CF. Then, we propose an alternative method for estimating predictive uncertainty via the generalised variance induced by the negative log-likelihood. Finally, we demonstrate that the use of predictive uncertainty measures helps in reducing CF in different settings.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Mathematical Framework and a Suite of Learning Techniques for Neural-Symbolic Systems</title>
<link>https://arxiv.org/abs/2407.09693</link>
<guid>https://arxiv.org/abs/2407.09693</guid>
<content:encoded><![CDATA[
arXiv:2407.09693v2 Announce Type: replace-cross 
Abstract: The field of Neural-Symbolic (NeSy) systems is growing rapidly. Proposed approaches show great promise in achieving symbiotic unions of neural and symbolic methods. However, a unifying framework is needed to organize common NeSy modeling patterns and develop general learning approaches. In this paper, we introduce Neural-Symbolic Energy-Based Models (NeSy-EBMs), a unifying mathematical framework for discriminative and generative NeSy modeling. Importantly, NeSy-EBMs allow the derivation of general expressions for gradients of prominent learning losses, and we introduce a suite of four learning approaches that leverage methods from multiple domains, including bilevel and stochastic policy optimization. Finally, we ground the NeSy-EBM framework with Neural Probabilistic Soft Logic (NeuPSL), an open-source NeSy-EBM library designed for scalability and expressivity, facilitating the real-world application of NeSy systems. Through extensive empirical analysis across multiple datasets, we demonstrate the practical advantages of NeSy-EBMs in various tasks, including image classification, graph node labeling, autonomous vehicle situation awareness, and question answering.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Does New Knowledge Create Messy Ripple Effects in LLMs?</title>
<link>https://arxiv.org/abs/2407.12828</link>
<guid>https://arxiv.org/abs/2407.12828</guid>
<content:encoded><![CDATA[
arXiv:2407.12828v3 Announce Type: replace-cross 
Abstract: Extensive previous research has focused on post-training knowledge editing (KE) for language models (LMs) to ensure that knowledge remains accurate and up-to-date. One desired property and open question in KE is to let edited LMs correctly handle ripple effects, where LM is expected to answer its logically related knowledge accurately. In this paper, we answer the question of why most KE methods still create messy ripple effects. We conduct extensive analysis and identify a salient indicator, GradSim, that effectively reveals when and why updated knowledge ripples in LMs. GradSim is computed by the cosine similarity between gradients of the original fact and its related knowledge. We observe a strong positive correlation between ripple effect performance and GradSim across different LMs, KE methods, and evaluation metrics. Further investigations into three counter-intuitive failure cases (Negation, Over-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures are often associated with very low GradSim. This finding validates that GradSim is an effective indicator of when knowledge ripples in LMs.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stimulating Imagination: Towards General-purpose "Something Something Placement"</title>
<link>https://arxiv.org/abs/2408.01655</link>
<guid>https://arxiv.org/abs/2408.01655</guid>
<content:encoded><![CDATA[
arXiv:2408.01655v2 Announce Type: replace-cross 
Abstract: General-purpose object placement is a fundamental capability of an intelligent generalist robot: being capable of rearranging objects following precise human instructions even in novel environments. This work is dedicated to achieving general-purpose object placement with ``something something'' instructions. Specifically, we break the entire process down into three parts, including object localization, goal imagination and robot control, and propose a method named SPORT. SPORT leverages a pre-trained large vision model for broad semantic reasoning about objects, and learns a diffusion-based pose estimator to ensure physically-realistic results in 3D space. Only object types (movable or reference) are communicated between these two parts, which brings two benefits. One is that we can fully leverage the powerful ability of open-set object recognition and localization since no specific fine-tuning is needed for the robotic scenario. Moreover, the diffusion-based estimator only need to ``imagine" the object poses after the placement, while no necessity for their semantic information. Thus the training burden is greatly reduced and no massive training is required. The training data for the goal pose estimation is collected in simulation and annotated by using GPT-4. Experimental results demonstrate the effectiveness of our approach. SPORT can not only generate promising 3D goal poses for unseen simulated objects, but also be seamlessly applied to real-world settings.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proficient Graph Neural Network Design by Accumulating Knowledge on Large Language Models</title>
<link>https://arxiv.org/abs/2408.06717</link>
<guid>https://arxiv.org/abs/2408.06717</guid>
<content:encoded><![CDATA[
arXiv:2408.06717v2 Announce Type: replace-cross 
Abstract: High-level automation is increasingly critical in AI, driven by rapid advances in large language models (LLMs) and AI agents. However, LLMs, despite their general reasoning power, struggle significantly in specialized, data-sensitive tasks such as designing Graph Neural Networks (GNNs). This difficulty arises from (1) the inherent knowledge gaps in modeling the intricate, varying relationships between graph properties and suitable architectures and (2) the external noise from misleading descriptive inputs, often resulting in generic or even misleading model suggestions. Achieving proficiency in designing data-aware models -- defined as the meta-level capability to systematically accumulate, interpret, and apply data-specific design knowledge -- remains challenging for existing automated approaches, due to their inefficient construction and application of meta-knowledge. To achieve the meta-level proficiency, we propose DesiGNN, a knowledge-centered framework that systematically converts past model design experiences into structured, fine-grained knowledge priors well fitted to meta-learning with LLMs. To account for the inherent variability and external noise, DesiGNN aligns empirical property filtering from extensive benchmarks with adaptive elicitation of literature insights via LLMs. By constructing a solid meta-knowledge between unseen graph understanding and known effective architecture patterns, DesiGNN can deliver top-5.77% initial model proposals for unseen datasets within seconds, and achieve consistently superior performance with minimal search costs against baselines.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CVPT: Cross Visual Prompt Tuning</title>
<link>https://arxiv.org/abs/2408.14961</link>
<guid>https://arxiv.org/abs/2408.14961</guid>
<content:encoded><![CDATA[
arXiv:2408.14961v2 Announce Type: replace-cross 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged to mitigate the computational demands of large-scale models. Within computer vision, adapter-based PEFT methods are often favored over prompt-based approaches like Visual Prompt Tuning (VPT) due to the latter's performance and efficiency limitations. Our analysis reveals that VPT's shortcomings stem from its prompt deployment strategy, which can distort the model's inherent self-attention mechanism. To address this, we propose Cross Visual Prompt Tuning (CVPT). CVPT introduces a cross-attention module to directly model interactions between prompts and image tokens. This design decouples the prompts from the input sequence, preserving the original self-attention integrity while enabling efficient feature integration. Furthermore, we employ a weight-sharing mechanism for cross-attention initialization, which enhances representative capability without a large parameter overhead. Extensive experiments across 25 datasets show that CVPT significantly outperforms VPT. For instance, on the VTAB-1K benchmark, CVPT achieves over 4% higher average accuracy, rivaling leading adapter-based methods in both performance and efficiency. Our work confirms that prompt-based methods can achieve exceptional results in visual fine-tuning. The code is available at https://github.com/Lingyun0419/CVPT
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language</title>
<link>https://arxiv.org/abs/2409.00061</link>
<guid>https://arxiv.org/abs/2409.00061</guid>
<content:encoded><![CDATA[
arXiv:2409.00061v2 Announce Type: replace-cross 
Abstract: Automated fact-checking is a key strategy to overcome the spread of COVID-19 misinformation on the internet. These systems typically leverage deep learning approaches through Natural Language Inference (NLI) to verify the truthfulness of information based on supporting evidence. However, one challenge that arises in deep learning is performance stagnation due to a lack of knowledge during training. This study proposes using a Knowledge Graph (KG) as external knowledge to enhance NLI performance for automated COVID-19 fact-checking in the Indonesian language. The proposed model architecture comprises three modules: a fact module, an NLI module, and a classifier module. The fact module processes information from the KG, while the NLI module handles semantic relationships between the given premise and hypothesis. The representation vectors from both modules are concatenated and fed into the classifier module to produce the final result. The model was trained using the generated Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia. Our study demonstrates that incorporating KGs can significantly improve NLI performance in fact-checking, achieving the best accuracy of 0.8616. This suggests that KGs are a valuable component for enhancing NLI performance in automated fact-checking.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise Recommendation</title>
<link>https://arxiv.org/abs/2409.07416</link>
<guid>https://arxiv.org/abs/2409.07416</guid>
<content:encoded><![CDATA[
arXiv:2409.07416v2 Announce Type: replace-cross 
Abstract: Modern listwise recommendation systems need to consider both long-term user perceptions and short-term interest shifts. Reinforcement learning can be applied on recommendation to study such a problem but is also subject to large search space, sparse user feedback and long interactive latency. Motivated by recent progress in hierarchical reinforcement learning, we propose a novel framework called mccHRL to provide different levels of temporal abstraction on listwise recommendation. Within the hierarchical framework, the high-level agent studies the evolution of user perception, while the low-level agent produces the item selection policy by modeling the process as a sequential decision-making problem. We argue that such framework has a well-defined decomposition of the outra-session context and the intra-session context, which are encoded by the high-level and low-level agents, respectively. To verify this argument, we implement both a simulator-based environment and an industrial dataset-based experiment. Results observe significant performance improvement by our method, compared with several well-known baselines. Data and codes have been made public.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The unknotting number, hard unknot diagrams, and reinforcement learning</title>
<link>https://arxiv.org/abs/2409.09032</link>
<guid>https://arxiv.org/abs/2409.09032</guid>
<content:encoded><![CDATA[
arXiv:2409.09032v2 Announce Type: replace-cross 
Abstract: We have developed a reinforcement learning agent that often finds a minimal sequence of unknotting crossing changes for a knot diagram with up to 200 crossings, hence giving an upper bound on the unknotting number. We have used this to determine the unknotting number of 57k knots. We took diagrams of connected sums of such knots with oppositely signed signatures, where the summands were overlaid. The agent has found examples where several of the crossing changes in an unknotting collection of crossings result in hyperbolic knots. Based on this, we have shown that, given knots $K$ and $K'$ that satisfy some mild assumptions, there is a diagram of their connected sum and $u(K) + u(K')$ unknotting crossings such that changing any one of them results in a prime knot. As a by-product, we have obtained a dataset of 2.6 million distinct hard unknot diagrams; most of them under 35 crossings. Assuming the additivity of the unknotting number, we have determined the unknotting number of 43 at most 12-crossing knots for which the unknotting number is unknown.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone Navigation via Scene-Aware Control Barrier Functions</title>
<link>https://arxiv.org/abs/2409.10283</link>
<guid>https://arxiv.org/abs/2409.10283</guid>
<content:encoded><![CDATA[
arXiv:2409.10283v4 Announce Type: replace-cross 
Abstract: In the rapidly evolving field of vision-language navigation (VLN), ensuring safety for physical agents remains an open challenge. For a human-in-the-loop language-operated drone to navigate safely, it must understand natural language commands, perceive the environment, and simultaneously avoid hazards in real time. Control Barrier Functions (CBFs) are formal methods that enforce safe operating conditions. Model Predictive Control (MPC) is an optimization framework that plans a sequence of future actions over a prediction horizon, ensuring smooth trajectory tracking while obeying constraints. In this work, we consider a VLN-operated drone platform and enhance its safety by formulating a novel scene-aware CBF that leverages ego-centric observations from a camera which has both Red-Green-Blue as well as Depth (RGB-D) channels. A CBF-less baseline system uses a Vision-Language Encoder with cross-modal attention to convert commands into an ordered sequence of landmarks. An object detection model identifies and verifies these landmarks in the captured images to generate a planned path. To further enhance safety, an Adaptive Safety Margin Algorithm (ASMA) is proposed. ASMA tracks moving objects and performs scene-aware CBF evaluation on-the-fly, which serves as an additional constraint within the MPC framework. By continuously identifying potentially risky observations, the system performs prediction in real time about unsafe conditions and proactively adjusts its control actions to maintain safe navigation throughout the trajectory. Deployed on a Parrot Bebop2 quadrotor in the Gazebo environment using the Robot Operating System (ROS), ASMA achieves 64%-67% increase in success rates with only a slight increase (1.4%-5.8%) in trajectory lengths compared to the baseline CBF-less VLN.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexiTex: Enhancing Texture Generation via Visual Guidance</title>
<link>https://arxiv.org/abs/2409.12431</link>
<guid>https://arxiv.org/abs/2409.12431</guid>
<content:encoded><![CDATA[
arXiv:2409.12431v5 Announce Type: replace-cross 
Abstract: Recent texture generation methods achieve impressive results due to the powerful generative prior they leverage from large-scale text-to-image diffusion models. However, abstract textual prompts are limited in providing global textural or shape information, which results in the texture generation methods producing blurry or inconsistent patterns. To tackle this, we present FlexiTex, embedding rich information via visual guidance to generate a high-quality texture. The core of FlexiTex is the Visual Guidance Enhancement module, which incorporates more specific information from visual guidance to reduce ambiguity in the text prompt and preserve high-frequency details. To further enhance the visual guidance, we introduce a Direction-Aware Adaptation module that automatically designs direction prompts based on different camera poses, avoiding the Janus problem and maintaining semantically global consistency. Benefiting from the visual guidance, FlexiTex produces quantitatively and qualitatively sound results, demonstrating its potential to advance texture generation for real-world applications.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning with Neuromorphic Computing: Foundations, Methods, and Emerging Applications</title>
<link>https://arxiv.org/abs/2410.09218</link>
<guid>https://arxiv.org/abs/2410.09218</guid>
<content:encoded><![CDATA[
arXiv:2410.09218v3 Announce Type: replace-cross 
Abstract: The challenging deployment of compute- and memory-intensive methods from Deep Neural Network (DNN)-based Continual Learning (CL) underscores the critical need for a paradigm shift towards more efficient approaches. Neuromorphic Continual Learning (NCL) appears as an emerging solution, by leveraging the principles of Spiking Neural Networks (SNNs) which enable efficient CL algorithms executed in dynamically-changed environments with resource-constrained computing systems. Motivated by the need for a holistic study of NCL, in this survey, we first provide a detailed background on CL, encompassing the desiderata, settings, metrics, scenario taxonomy, Online Continual Learning (OCL) paradigm, recent DNN-based methods to address catastrophic forgetting (CF). Then, we analyze these methods considering CL desiderata, computational and memory costs, as well as network complexity, hence emphasizing the need for energy-efficient CL. Afterward, we provide background of low-power neuromorphic systems including encoding techniques, neuronal dynamics, network architectures, learning rules, hardware processors, software and hardware frameworks, datasets, benchmarks, and evaluation metrics. Then, this survey comprehensively reviews and analyzes state-of-the-art in NCL. The key ideas, implementation frameworks, and performance assessments are also provided. This survey covers several hybrid approaches that combine supervised and unsupervised learning paradigms. It also covers optimization techniques including SNN operations reduction, weight quantization, and knowledge distillation. Then, this survey discusses the progress of real-world NCL applications. Finally, this paper provides a future perspective on the open research challenges for NCL, since the purpose of this study is to be useful for the wider neuromorphic AI research community and to inspire future research in bio-plausible OCL.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaDPO: Adaptive Reward Margin for Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2410.10148</link>
<guid>https://arxiv.org/abs/2410.10148</guid>
<content:encoded><![CDATA[
arXiv:2410.10148v4 Announce Type: replace-cross 
Abstract: Aligning large language models (LLMs) with human values and intentions is crucial for their utility, honesty, and safety. Reinforcement learning from human feedback (RLHF) is a popular approach to achieve this alignment, but it faces challenges in computational efficiency and training stability. Recent methods like Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO) have proposed offline alternatives to RLHF, simplifying the process by reparameterizing the reward function. However, DPO depends on a potentially suboptimal reference model, and SimPO's assumption of a fixed target reward margin may lead to suboptimal decisions in diverse data settings. In this work, we propose $\alpha$-DPO, an adaptive preference optimization algorithm designed to address these limitations by introducing a dynamic reward margin. Specifically, $\alpha$-DPO employs an adaptive preference distribution, balancing the policy model and the reference model to achieve personalized reward margins. We provide theoretical guarantees for $\alpha$-DPO, demonstrating its effectiveness as a surrogate optimization objective and its ability to balance alignment and diversity through KL divergence control. Empirical evaluations on AlpacaEval 2 and Arena-Hard show that $\alpha$-DPO consistently outperforms DPO and SimPO across various model settings, establishing it as a robust approach for fine-tuning LLMs. Our method achieves significant improvements in win rates, highlighting its potential as a powerful tool for LLM alignment. The code is available at https://github.com/junkangwu/alpha-DPO
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PerspectiveNet: Multi-View Perception for Dynamic Scene Understanding</title>
<link>https://arxiv.org/abs/2410.16824</link>
<guid>https://arxiv.org/abs/2410.16824</guid>
<content:encoded><![CDATA[
arXiv:2410.16824v2 Announce Type: replace-cross 
Abstract: Generating detailed descriptions from multiple cameras and viewpoints is challenging due to the complex and inconsistent nature of visual data. In this paper, we introduce PerspectiveNet, a lightweight yet efficient model for generating long descriptions across multiple camera views. Our approach utilizes a vision encoder, a compact connector module to convert visual features into a fixed-size tensor, and large language models (LLMs) to harness the strong natural language generation capabilities of LLMs. The connector module is designed with three main goals: mapping visual features onto LLM embeddings, emphasizing key information needed for description generation, and producing a fixed-size feature matrix. Additionally, we augment our solution with a secondary task, the correct frame sequence detection, enabling the model to search for the correct sequence of frames to generate descriptions. Finally, we integrate the connector module, the secondary task, the LLM, and a visual feature extraction model into a single architecture, which is trained for the Traffic Safety Description and Analysis task. This task requires generating detailed, fine-grained descriptions of events from multiple cameras and viewpoints. The resulting model is lightweight, ensuring efficient training and inference, while remaining highly effective.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualSwinUnet++: An Enhanced Swin-Unet Architecture With Dual Decoders For PTMC Segmentation</title>
<link>https://arxiv.org/abs/2410.18239</link>
<guid>https://arxiv.org/abs/2410.18239</guid>
<content:encoded><![CDATA[
arXiv:2410.18239v3 Announce Type: replace-cross 
Abstract: Precise segmentation of papillary thyroid microcarcinoma (PTMC) during ultrasound-guided radiofrequency ablation (RFA) is critical for effective treatment but remains challenging due to acoustic artifacts, small lesion size, and anatomical variability. In this study, we propose DualSwinUnet++, a dual-decoder transformer-based architecture designed to enhance PTMC segmentation by incorporating thyroid gland context. DualSwinUnet++ employs independent linear projection heads for each decoder and a residual information flow mechanism that passes intermediate features from the first (thyroid) decoder to the second (PTMC) decoder via concatenation and transformation. These design choices allow the model to condition tumor prediction explicitly on gland morphology without shared gradient interference. Trained on a clinical ultrasound dataset with 691 annotated RFA images and evaluated against state-of-the-art models, DualSwinUnet++ achieves superior Dice and Jaccard scores while maintaining sub-200ms inference latency. The results demonstrate the model's suitability for near real-time surgical assistance and its effectiveness in improving segmentation accuracy in challenging PTMC cases.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking</title>
<link>https://arxiv.org/abs/2411.05375</link>
<guid>https://arxiv.org/abs/2411.05375</guid>
<content:encoded><![CDATA[
arXiv:2411.05375v2 Announce Type: replace-cross 
Abstract: Current automated fact-checking (AFC) approaches typically evaluate evidence either implicitly via the predicted verdicts or through exact matches with predefined closed knowledge sources, such as Wikipedia. However, these methods are limited due to their reliance on evaluation metrics originally designed for other purposes and constraints from closed knowledge sources. In this work, we introduce \textbf{\textcolor{skyblue}{Ev\textsuperscript{2}}\textcolor{orangebrown}{R}} which combines the strengths of reference-based evaluation and verdict-level proxy scoring. Ev\textsuperscript{2}R jointly assesses how well the evidence aligns with the gold references and how reliably it supports the verdict, addressing the shortcomings of prior methods. We evaluate Ev\textsuperscript{2}R against three types of evidence evaluation approaches: reference-based, proxy-reference, and reference-less baselines. Assessments against human ratings and adversarial tests demonstrate that Ev\textsuperscript{2}R consistently outperforms existing scoring approaches in accuracy and robustness. It achieves stronger correlation with human judgments and greater robustness to adversarial perturbations, establishing it as a reliable metric for evidence evaluation in AFC.\footnote{Code is available at \href{https://github.com/mubasharaak/fc-evidence-evaluation}{https://github.com/mubasharaak/fc-evidence-evaluation}.}
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOGR: Towards Versatile Visual Document Grounding and Referring</title>
<link>https://arxiv.org/abs/2411.17125</link>
<guid>https://arxiv.org/abs/2411.17125</guid>
<content:encoded><![CDATA[
arXiv:2411.17125v2 Announce Type: replace-cross 
Abstract: With recent advances in Multimodal Large Language Models (MLLMs), grounding and referring capabilities have gained increasing attention for achieving detailed understanding and flexible user interaction. However, these capabilities still remain underdeveloped in visual document understanding due to the scarcity of fine-grained datasets and comprehensive benchmarks. To fill this gap, we propose the DOcument Grounding and Referring data engine (DOGR-Engine), which generates two types of high-quality fine-grained document data: (1) multi-granular parsing data to improve text localization and recognition, and (2) instruction-tuning data to activate MLLMs' grounding and referring capabilities in dialogue and reasoning. Using the DOGR-Engine, we construct DOGR-Bench, a benchmark covering seven grounding and referring tasks across three document types (chart, poster, and PDF document), offering a comprehensive evaluation of fine-grained document understanding. Leveraging the generated data, we further develop DOGR, a strong baseline model that excels in text localization and recognition, while precisely grounds and refers to key textual information during conversation and reasoning, thereby advancing document understanding to a finer granularity and enable flexible interaction paradigms.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Design Decisions of Retrieval-Augmented Generation Systems</title>
<link>https://arxiv.org/abs/2411.19463</link>
<guid>https://arxiv.org/abs/2411.19463</guid>
<content:encoded><![CDATA[
arXiv:2411.19463v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a critical technique for enhancing large language model (LLM) capabilities. However, practitioners face significant challenges when making RAG deployment decisions. While existing research prioritizes algorithmic innovations, a systematic gap persists in understanding fundamental engineering trade-offs that determine RAG success. We present the first comprehensive study of three universal RAG deployment decisions: whether to deploy RAG, how much information to retrieve, and how to integrate retrieved knowledge effectively. Through systematic experiments across three LLMs and six datasets spanning question answering and code generation tasks, we reveal critical insights: (1) RAG deployment must be highly selective, with variable recall thresholds and failure modes affecting up to 12.6\% of samples even with perfect documents. (2) Optimal retrieval volume exhibits task-dependent behavior QA tasks show universal patterns (5-10 documents optimal) while code generation requires scenario-specific optimization. (3) Knowledge integration effectiveness depends on task and model characteristics, with code generation benefiting significantly from prompting methods while question answering shows minimal improvement. These findings demonstrate that universal RAG strategies prove inadequate. Effective RAG systems require context-aware design decisions based on task characteristics and model capabilities. Our analysis provides evidence-based guidance for practitioners and establishes foundational insights for principled RAG deployment.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEMF-VTO: Point-Enhanced Video Virtual Try-on via Mask-free Paradigm</title>
<link>https://arxiv.org/abs/2412.03021</link>
<guid>https://arxiv.org/abs/2412.03021</guid>
<content:encoded><![CDATA[
arXiv:2412.03021v5 Announce Type: replace-cross 
Abstract: Video Virtual Try-on aims to seamlessly transfer a reference garment onto a target person in a video while preserving both visual fidelity and temporal coherence. Existing methods typically rely on inpainting masks to define the try-on area, enabling accurate garment transfer for simple scenes (e.g., in-shop videos). However, these mask-based approaches struggle with complex real-world scenarios, as overly large and inconsistent masks often destroy spatial-temporal information, leading to distorted results. Mask-free methods alleviate this issue but face challenges in accurately determining the try-on area, especially for videos with dynamic body movements. To address these limitations, we propose PEMF-VTO, a novel Point-Enhanced Mask-Free Video Virtual Try-On framework that leverages sparse point alignments to explicitly guide garment transfer. Our key innovation is the introduction of point-enhanced guidance, which provides flexible and reliable control over both spatial-level garment transfer and temporal-level video coherence. Specifically, we design a Point-Enhanced Transformer (PET) with two core components: Point-Enhanced Spatial Attention (PSA), which uses frame-cloth point alignments to precisely guide garment transfer, and Point-Enhanced Temporal Attention (PTA), which leverages frame-frame point correspondences to enhance temporal coherence and ensure smooth transitions across frames. Extensive experiments demonstrate that our PEMF-VTO outperforms state-of-the-art methods, generating more natural, coherent, and visually appealing try-on videos, particularly for challenging in-the-wild scenarios. The link to our paper's homepage is https://pemf-vto.github.io/.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios</title>
<link>https://arxiv.org/abs/2412.03920</link>
<guid>https://arxiv.org/abs/2412.03920</guid>
<content:encoded><![CDATA[
arXiv:2412.03920v2 Announce Type: replace-cross 
Abstract: Game-theoretic scenarios have become pivotal in evaluating the social intelligence of Large Language Model (LLM)-based social agents. While numerous studies have explored these agents in such settings, there is a lack of a comprehensive survey summarizing the current progress. To address this gap, we systematically review existing research on LLM-based social agents within game-theoretic scenarios. Our survey organizes the findings into three core components: Game Framework, Social Agent, and Evaluation Protocol. The game framework encompasses diverse game scenarios, ranging from choice-focusing to communication-focusing games. The social agent part explores agents' preferences, beliefs, and reasoning abilities, as well as their interactions and synergistic effects on decision-making. The evaluation protocol covers both game-agnostic and game-specific metrics for assessing agent performance. Additionally, we analyze the performance of current social agents across various game scenarios. By reflecting on the current research and identifying future research directions, this survey provides insights to advance the development and evaluation of social agents in game-theoretic scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A recent evaluation on the performance of LLMs on radiation oncology physics using questions of randomly shuffled options</title>
<link>https://arxiv.org/abs/2412.10622</link>
<guid>https://arxiv.org/abs/2412.10622</guid>
<content:encoded><![CDATA[
arXiv:2412.10622v4 Announce Type: replace-cross 
Abstract: Purpose: We present an updated study evaluating the performance of large language models (LLMs) in answering radiation oncology physics questions, focusing on the recently released models.
  Methods: A set of 100 multiple-choice radiation oncology physics questions, previously created by a well-experienced physicist, was used for this study. The answer options of the questions were randomly shuffled to create "new" exam sets. Five LLMs -- OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5 Sonnet -- with the versions released before September 30, 2024, were queried using these new exam sets. To evaluate their deductive reasoning ability, the correct answer options in the questions were replaced with "None of the above." Then, the explain-first and step-by-step instruction prompts were used to test if this strategy improved their reasoning ability. The performance of the LLMs was compared with the answers from medical physicists.
  Results: All models demonstrated expert-level performance on these questions, with o1-preview even surpassing medical physicists with a majority vote. When replacing the correct answer options with 'None of the above', all models exhibited a considerable decline in performance, suggesting room for improvement. The explain-first and step-by-step instruction prompts helped enhance the reasoning ability of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5 Sonnet models.
  Conclusion: These recently released LLMs demonstrated expert-level performance in answering radiation oncology physics questions, exhibiting great potential to assist in radiation oncology physics education and training.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability Detection</title>
<link>https://arxiv.org/abs/2501.04510</link>
<guid>https://arxiv.org/abs/2501.04510</guid>
<content:encoded><![CDATA[
arXiv:2501.04510v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been proposed as powerful tools for detecting software vulnerabilities, where task-specific fine-tuning is typically employed to provide vulnerability-specific knowledge to the LLMs. However, existing fine-tuning techniques often treat source code as plain text, losing the graph-based structural information inherent in code.
  Graph-enhanced soft prompt tuning addresses this by translating the structural information into contextual cues that the LLM can understand. However, current methods are primarily designed for general graph-related tasks and focus more on adjacency information, they fall short in preserving the rich semantic information (e.g., control/data flow) within code graphs. They also fail to ensure computational efficiency while capturing graph-text interactions in their cross-modal alignment module.
  This paper presents CGP-Tuning, a new code graph-enhanced, structure-aware soft prompt tuning method for vulnerability detection. CGP-Tuning introduces type-aware embeddings to capture the rich semantic information within code graphs, along with an efficient cross-modal alignment module that achieves linear computational costs while incorporating graph-text interactions. It is evaluated on the latest DiverseVul dataset and three advanced open-source code LLMs, CodeLlama, CodeGemma, and Qwen2.5-Coder. Experimental results show that CGP-Tuning delivers model-agnostic improvements and maintains practical inference speed, surpassing the best graph-enhanced soft prompt tuning baseline by an average of four percentage points and outperforming non-tuned zero-shot prompting by 15 percentage points.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEPPO-GAE: Hardware-Efficient Proximal Policy Optimization with Generalized Advantage Estimation</title>
<link>https://arxiv.org/abs/2501.12703</link>
<guid>https://arxiv.org/abs/2501.12703</guid>
<content:encoded><![CDATA[
arXiv:2501.12703v2 Announce Type: replace-cross 
Abstract: This paper introduces HEPPO-GAE, an FPGA-based accelerator designed to optimize the Generalized Advantage Estimation (GAE) stage in Proximal Policy Optimization (PPO). Unlike previous approaches that focused on trajectory collection and actor-critic updates, HEPPO-GAE addresses GAE's computational demands with a parallel, pipelined architecture implemented on a single System-on-Chip (SoC). This design allows for the adaptation of various hardware accelerators tailored for different PPO phases. A key innovation is our strategic standardization technique, which combines dynamic reward standardization and block standardization for values, followed by 8-bit uniform quantization. This method stabilizes learning, enhances performance, and manages memory bottlenecks, achieving a 4x reduction in memory usage and a 1.5x increase in cumulative rewards. We propose a solution on a single SoC device with programmable logic and embedded processors, delivering throughput orders of magnitude higher than traditional CPU-GPU systems. Our single-chip solution minimizes communication latency and throughput bottlenecks, significantly boosting PPO training efficiency. Experimental results show a 30% increase in PPO speed and a substantial reduction in memory access time, underscoring HEPPO-GAE's potential for broad applicability in hardware-efficient reinforcement learning algorithms.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BARNN: A Bayesian Autoregressive and Recurrent Neural Network</title>
<link>https://arxiv.org/abs/2501.18665</link>
<guid>https://arxiv.org/abs/2501.18665</guid>
<content:encoded><![CDATA[
arXiv:2501.18665v2 Announce Type: replace-cross 
Abstract: Autoregressive and recurrent networks have achieved remarkable progress across various fields, from weather forecasting to molecular generation and Large Language Models. Despite their strong predictive capabilities, these models lack a rigorous framework for addressing uncertainty, which is key in scientific applications such as PDE solving, molecular generation and Machine Learning Force Fields. To address this shortcoming we present BARNN: a variational Bayesian Autoregressive and Recurrent Neural Network. BARNNs aim to provide a principled way to turn any autoregressive or recurrent model into its Bayesian version. BARNN is based on the variational dropout method, allowing to apply it to large recurrent neural networks as well. We also introduce a temporal version of the "Variational Mixtures of Posteriors" prior (tVAMP-prior) to make Bayesian inference efficient and well-calibrated. Extensive experiments on PDE modelling and molecular generation demonstrate that BARNN not only achieves comparable or superior accuracy compared to existing methods, but also excels in uncertainty quantification and modelling long-range dependencies.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in Strategic Queuing Systems with Small Buffers</title>
<link>https://arxiv.org/abs/2502.08898</link>
<guid>https://arxiv.org/abs/2502.08898</guid>
<content:encoded><![CDATA[
arXiv:2502.08898v2 Announce Type: replace-cross 
Abstract: We consider learning outcomes in games with carryover effects between rounds: when outcomes in the present round affect the game in the future. An important example of such systems is routers in networking, as they use simple learning algorithms to find the best way to deliver packets to their desired destination. This simple, myopic, and distributed decision process makes large queuing systems easy to operate, but at the same time, the system needs more capacity than would be required if all traffic were centrally coordinated. Gaitonde and Tardos (EC 2020 and JACM 2023) initiated the study of such systems, modeling them as an infinitely repeated game in which routers compete for servers and the system maintains a state (the number of packets held at each queue) that results from outcomes of previous rounds. However, their model assumes that servers have no buffers at all, so routers have to resend all packets that were not served successfully, which makes their system model unrealistic. They show that in their model, even with hugely increased server capacity relative to what is needed in the centrally coordinated case, ensuring that the system is stable requires the use of timestamps and priority for older packets.
  We consider a system with two important changes, which make the model more realistic and allow for much higher traffic rates: first, we add a very small buffer to each server, allowing the server to hold on to a single packet to be served later (if it fails to serve it immediately), and second, we do not require timestamps or priority to older packets. Using theoretical analysis and simulations, we show that when queues are learning, a small constant-factor increase in server capacity, compared to what would be needed if centrally coordinating, suffices to keep the system stable, even if servers select randomly among packets arriving simultaneously.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layerwise Recall and the Geometry of Interwoven Knowledge in LLMs</title>
<link>https://arxiv.org/abs/2502.10871</link>
<guid>https://arxiv.org/abs/2502.10871</guid>
<content:encoded><![CDATA[
arXiv:2502.10871v2 Announce Type: replace-cross 
Abstract: This study explores how large language models (LLMs) encode interwoven scientific knowledge, using chemical elements and LLaMA-series models as a case study. We identify a 3D spiral structure in the hidden states that aligns with the conceptual structure of the periodic table, suggesting that LLMs can reflect the geometric organization of scientific concepts learned from text. Linear probing reveals that middle layers encode continuous, overlapping attributes that enable indirect recall, while deeper layers sharpen categorical distinctions and incorporate linguistic context. These findings suggest that LLMs represent symbolic knowledge not as isolated facts, but as structured geometric manifolds that intertwine semantic information across layers. We hope this work inspires further exploration of how LLMs represent and reason about scientific knowledge, particularly in domains such as materials science.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Overall Real-Time Mechanism for Classification and Quality Evaluation of Rice</title>
<link>https://arxiv.org/abs/2502.13764</link>
<guid>https://arxiv.org/abs/2502.13764</guid>
<content:encoded><![CDATA[
arXiv:2502.13764v3 Announce Type: replace-cross 
Abstract: Rice is one of the most widely cultivated crops globally and has been developed into numerous varieties. The quality of rice during cultivation is primarily determined by its cultivar and characteristics. Traditionally, rice classification and quality assessment rely on manual visual inspection, a process that is both time-consuming and prone to errors. However, with advancements in machine vision technology, automating rice classification and quality evaluation based on its cultivar and characteristics has become increasingly feasible, enhancing both accuracy and efficiency. This study proposes a real-time evaluation mechanism for comprehensive rice grain assessment, integrating a one-stage object detection approach, a deep convolutional neural network, and traditional machine learning techniques. The proposed framework enables rice variety identification, grain completeness grading, and grain chalkiness evaluation. The rice grain dataset used in this study comprises approximately 20,000 images from six widely cultivated rice varieties in China. Experimental results demonstrate that the proposed mechanism achieves a mean average precision (mAP) of 99.14% in the object detection task and an accuracy of 97.89% in the classification task. Furthermore, the framework attains an average accuracy of 97.56% in grain completeness grading within the same rice variety, contributing to an effective quality evaluation system.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MKE-Coder: Multi-Axial Knowledge with Evidence Verification in ICD Coding for Chinese EMRs</title>
<link>https://arxiv.org/abs/2502.14916</link>
<guid>https://arxiv.org/abs/2502.14916</guid>
<content:encoded><![CDATA[
arXiv:2502.14916v3 Announce Type: replace-cross 
Abstract: The task of automatically coding the International Classification of Diseases (ICD) in the medical field has been well-established and has received much attention. Automatic coding of the ICD in the medical field has been successful in English but faces challenges when dealing with Chinese electronic medical records (EMRs). The first issue lies in the difficulty of extracting disease code-related information from Chinese EMRs, primarily due to the concise writing style and specific internal structure of the EMRs. The second problem is that previous methods have failed to leverage the disease-based multi-axial knowledge and lack of association with the corresponding clinical evidence. This paper introduces a novel framework called MKE-Coder: Multi-axial Knowledge with Evidence verification in ICD coding for Chinese EMRs. Initially, we identify candidate codes for the diagnosis and categorize each of them into knowledge under four coding axes.Subsequently, we retrieve corresponding clinical evidence from the comprehensive content of EMRs and filter credible evidence through a scoring model. Finally, to ensure the validity of the candidate code, we propose an inference module based on the masked language modeling strategy. This module verifies that all the axis knowledge associated with the candidate code is supported by evidence and provides recommendations accordingly. To evaluate the performance of our framework, we conduct experiments using a large-scale Chinese EMR dataset collected from various hospitals. The experimental results demonstrate that MKE-Coder exhibits significant superiority in the task of automatic ICD coding based on Chinese EMRs. In the practical evaluation of our method within simulated real coding scenarios, it has been demonstrated that our approach significantly aids coders in enhancing both their coding accuracy and speed.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans</title>
<link>https://arxiv.org/abs/2502.15090</link>
<guid>https://arxiv.org/abs/2502.15090</guid>
<content:encoded><![CDATA[
arXiv:2502.15090v2 Announce Type: replace-cross 
Abstract: Modern large language models (LLMs) achieve impressive performance on some tasks, while exhibiting distinctly non-human-like behaviors on others. This raises the question of how well the LLM's learned representations align with human representations. In this work, we introduce a novel approach to study representation alignment: we adopt a method from research on activation steering to identify neurons responsible for specific concepts (e.g., ''cat'') and then analyze the corresponding activation patterns. We find that LLM representations captured this way closely align with human representations inferred from behavioral data, matching inter-human alignment levels. Our approach significantly outperforms the alignment captured by word embeddings, which have been the focus of prior work on human-LLM alignment. Additionally, our approach enables a more granular view of how LLMs represent concepts -- we show that LLMs organize concepts in a way that mirrors human concept organization.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models</title>
<link>https://arxiv.org/abs/2502.15639</link>
<guid>https://arxiv.org/abs/2502.15639</guid>
<content:encoded><![CDATA[
arXiv:2502.15639v2 Announce Type: replace-cross 
Abstract: Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions -- a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation. We show that modifying the mLLM's activations changes its embedding space such that cross-lingual alignment is enhanced. Further, we show that the changes to the embedding space translate into improved downstream performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on cross-lingual retrieval.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2502.17163</link>
<guid>https://arxiv.org/abs/2502.17163</guid>
<content:encoded><![CDATA[
arXiv:2502.17163v4 Announce Type: replace-cross 
Abstract: Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.
  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. Our dataset is available at https://github.com/amazon-science/MEMERAG
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Benchmark Contamination Through Watermarking</title>
<link>https://arxiv.org/abs/2502.17259</link>
<guid>https://arxiv.org/abs/2502.17259</guid>
<content:encoded><![CDATA[
arXiv:2502.17259v2 Announce Type: replace-cross 
Abstract: Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect ``radioactivity'', \ie traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, \eg $p$-val $=10^{-3}$ for +5$\%$ on ARC-Easy.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in Product QA Agents</title>
<link>https://arxiv.org/abs/2502.19545</link>
<guid>https://arxiv.org/abs/2502.19545</guid>
<content:encoded><![CDATA[
arXiv:2502.19545v2 Announce Type: replace-cross 
Abstract: The deployment of Large Language Models (LLMs) in customer support is constrained by hallucination (generating false information) and the high cost of proprietary models. To address these challenges, we propose a retrieval-augmented question-answering (QA) pipeline and explore how to balance human input and automation. Using a dataset of questions about a Samsung Smart TV user manual, we demonstrate that synthetic data generated by LLMs outperforms crowdsourced data in reducing hallucination in finetuned models. We also compare self-training (fine-tuning models on their own outputs) and knowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o), and find that self-training achieves comparable hallucination reduction. We conjecture that this surprising finding can be attributed to increased exposure bias issues in the knowledge distillation case and support this conjecture with post hoc analysis. We also improve robustness to unanswerable questions and retrieval failures with contextualized "I don't know" responses. These findings show that scalable, cost-efficient QA systems can be built using synthetic data and self-training with open-source models, reducing reliance on proprietary tools or costly human annotations.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain Foundation Models: A Survey on Advancements in Neural Signal Processing and Brain Discovery</title>
<link>https://arxiv.org/abs/2503.00580</link>
<guid>https://arxiv.org/abs/2503.00580</guid>
<content:encoded><![CDATA[
arXiv:2503.00580v2 Announce Type: replace-cross 
Abstract: Brain foundation models (BFMs) have emerged as a transformative paradigm in computational neuroscience, offering a revolutionary framework for processing diverse neural signals across different brain-related tasks. These models leverage large-scale pre-training techniques, allowing them to generalize effectively across multiple scenarios, tasks, and modalities, thus overcoming the traditional limitations faced by conventional artificial intelligence (AI) approaches in understanding complex brain data. By tapping into the power of pretrained models, BFMs provide a means to process neural data in a more unified manner, enabling advanced analysis and discovery in the field of neuroscience. In this survey, we define BFMs for the first time, providing a clear and concise framework for constructing and utilizing these models in various applications. We also examine the key principles and methodologies for developing these models, shedding light on how they transform the landscape of neural signal processing. This survey presents a comprehensive review of the latest advancements in BFMs, covering the most recent methodological innovations, novel views of application areas, and challenges in the field. Notably, we highlight the future directions and key challenges that need to be addressed to fully realize the potential of BFMs. These challenges include improving the quality of brain data, optimizing model architecture for better generalization, increasing training efficiency, and enhancing the interpretability and robustness of BFMs in real-world applications.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Optical Denoising Clean Sonar Images? A Benchmark and Fusion Approach</title>
<link>https://arxiv.org/abs/2503.01655</link>
<guid>https://arxiv.org/abs/2503.01655</guid>
<content:encoded><![CDATA[
arXiv:2503.01655v2 Announce Type: replace-cross 
Abstract: Object detection in sonar images is crucial for underwater robotics applications including autonomous navigation and resource exploration. However, complex noise patterns inherent in sonar imagery, particularly speckle, reverberation, and non-Gaussian noise, significantly degrade detection accuracy. While denoising techniques have achieved remarkable success in optical imaging, their applicability to sonar data remains underexplored. This study presents the first systematic evaluation of nine state-of-the-art deep denoising models with distinct architectures, including Neighbor2Neighbor with varying noise parameters, Blind2Unblind with different noise configurations, and DSPNet, for sonar image preprocessing. We establish a rigorous benchmark using five publicly available sonar datasets and assess their impact on four representative detection algorithms: YOLOX, Faster R-CNN, SSD300, and SSDMobileNetV2. Our evaluation addresses three unresolved questions: first, how effectively optical denoising architectures transfer to sonar data; second, which model families perform best against sonar noise; and third, whether denoising truly improves detection accuracy in practical pipelines. Extensive experiments demonstrate that while denoising generally improves detection performance, effectiveness varies across methods due to their inherent biases toward specific noise types. To leverage complementary denoising effects, we propose a mutually-supervised multi-source denoising fusion framework where outputs from different denoisers mutually supervise each other at the pixel level, creating a synergistic framework that produces cleaner images.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attend or Perish: Benchmarking Attention in Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2503.01909</link>
<guid>https://arxiv.org/abs/2503.01909</guid>
<content:encoded><![CDATA[
arXiv:2503.01909v2 Announce Type: replace-cross 
Abstract: Can transformers learn to perform algorithmic tasks reliably across previously unseen input/output domains? While pre-trained language models show solid accuracy on benchmarks incorporating algorithmic reasoning, assessing the reliability of these results necessitates an ability to distinguish genuine algorithmic understanding from memorization. In this paper, we propose AttentionSpan, an algorithmic benchmark comprising five tasks of infinite input domains where we can disentangle and trace the correct, robust algorithm necessary for the task. This allows us to assess (i) models' ability to extrapolate to unseen types of inputs, including new lengths, value ranges or input domains, but also (ii)to assess the robustness of their learned mechanisms. By analyzing attention maps and performing targeted interventions, we show that attention mechanism directly causes failures in extrapolation. We make the implementation of all our tasks and interpretability methods publicly available at https://github.com/michalspiegel/AttentionSpan .
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Too Much to Trust? Measuring the Security and Cognitive Impacts of Explainability in AI-Driven SOCs</title>
<link>https://arxiv.org/abs/2503.02065</link>
<guid>https://arxiv.org/abs/2503.02065</guid>
<content:encoded><![CDATA[
arXiv:2503.02065v2 Announce Type: replace-cross 
Abstract: Explainable AI (XAI) holds significant promise for enhancing the transparency and trustworthiness of AI-driven threat detection in Security Operations Centers (SOCs). However, identifying the appropriate level and format of explanation, particularly in environments that demand rapid decision-making under high-stakes conditions, remains a complex and underexplored challenge. To address this gap, we conducted a three-month mixed-methods study combining an online survey (N1=248) with in-depth interviews (N2=24) to examine (1) how SOC analysts conceptualize AI-generated explanations and (2) which types of explanations are perceived as actionable and trustworthy across different analyst roles. Our findings reveal that participants were consistently willing to accept XAI outputs, even in cases of lower predictive accuracy, when explanations were perceived as relevant and evidence-backed. Analysts repeatedly emphasized the importance of understanding the rationale behind AI decisions, expressing a strong preference for contextual depth over a mere presentation of outcomes on dashboards. Building on these insights, this study re-evaluates current explanation methods within security contexts and demonstrates that role-aware, context-rich XAI designs aligned with SOC workflows can substantially improve practical utility. Such tailored explainability enhances analyst comprehension, increases triage efficiency, and supports more confident responses to evolving threats.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMNISEC: LLM-Driven Provenance-based Intrusion Detection via Retrieval-Augmented Behavior Prompting</title>
<link>https://arxiv.org/abs/2503.03108</link>
<guid>https://arxiv.org/abs/2503.03108</guid>
<content:encoded><![CDATA[
arXiv:2503.03108v3 Announce Type: replace-cross 
Abstract: Recently, Provenance-based Intrusion Detection Systems (PIDSes) have been widely used for endpoint threat analysis. These studies can be broadly categorized into rule-based detection systems and learning-based detection systems. Among these, due to the evolution of attack techniques, rules cannot dynamically model all the characteristics of attackers. As a result, such systems often face false negatives. Learning-based detection systems are further divided into supervised learning and anomaly detection. The scarcity of attack samples hinders the usability and effectiveness of supervised learning-based detection systems in practical applications. Anomaly-based detection systems face a massive false positive problem because they cannot distinguish between changes in normal behavior and real attack behavior. The alert results of detection systems are closely related to the manual labor costs of subsequent security analysts. To reduce manual analysis time, we propose OMNISEC, which applies large language models (LLMs) to anomaly-based intrusion detection systems via retrieval-augmented behavior prompting. OMNISEC can identify abnormal nodes and corresponding abnormal events by constructing suspicious nodes and rare paths. By combining two external knowledge bases, OMNISEC uses Retrieval Augmented Generation (RAG) to enable the LLM to determine whether abnormal behavior is a real attack. Finally, OMNISEC can reconstruct the attack graph and restore the complete attack behavior chain of the attacker's intrusion. Experimental results show that OMNISEC outperforms state-of-the-art methods on public benchmark datasets.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning</title>
<link>https://arxiv.org/abs/2503.05641</link>
<guid>https://arxiv.org/abs/2503.05641</guid>
<content:encoded><![CDATA[
arXiv:2503.05641v3 Announce Type: replace-cross 
Abstract: Combining existing pre-trained expert LLMs is a promising avenue for scalably tackling large-scale and diverse tasks. However, selecting task-level experts is often too coarse-grained, as heterogeneous tasks may require different expertise per instance. To enable adaptive instance-level mixing of pre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to selection by emphasizing skills, e.g., algebra in math or molecular biology in biomedical reasoning. We propose a skill-based recruiting strategy that dynamically selects the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths. Each selected expert then generates its own reasoning, resulting in k outputs from k experts, which are then synthesized into a final high-quality response by an aggregator chosen based on its ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's instance-level expert selection improves performance by a large margin but -- when implemented naively -- can introduce a high computational overhead due to the need for constant model loading and offloading. To address this, we implement a batch strategy that groups instances based on their assigned experts, loading each model only once. This allows us to integrate 16 expert models on 1 GPU with a time cost comparable to or better than prior multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we show that Symbolic-MoE beats strong LLMs like GPT4o-mini, as well as multi-agent approaches, with an absolute avg. gain of 8.15% over the best multi-agent baseline. Moreover, Symbolic-MoE generalizes well to unseen tasks and removes the need for expensive multi-round discussions, outperforming discussion baselines with less computation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynamicID: Zero-Shot Multi-ID Image Personalization with Flexible Facial Editability</title>
<link>https://arxiv.org/abs/2503.06505</link>
<guid>https://arxiv.org/abs/2503.06505</guid>
<content:encoded><![CDATA[
arXiv:2503.06505v3 Announce Type: replace-cross 
Abstract: Recent advances in text-to-image generation have driven interest in generating personalized human images that depict specific identities from reference images. Although existing methods achieve high-fidelity identity preservation, they are generally limited to single-ID scenarios and offer insufficient facial editability. We present DynamicID, a tuning-free framework that inherently facilitates both single-ID and multi-ID personalized generation with high fidelity and flexible facial editability. Our key innovations include: 1) Semantic-Activated Attention (SAA), which employs query-level activation gating to minimize disruption to the base model when injecting ID features and achieve multi-ID personalization without requiring multi-ID samples during training. 2) Identity-Motion Reconfigurator (IMR), which applies feature-space manipulation to effectively disentangle and reconfigure facial motion and identity features, supporting flexible facial editing. 3) a task-decoupled training paradigm that reduces data dependency, together with VariFace-10k, a curated dataset of 10k unique individuals, each represented by 35 distinct facial images. Experimental results demonstrate that DynamicID outperforms state-of-the-art methods in identity fidelity, facial editability, and multi-ID personalization capability. Our code will be released at https://github.com/ByteCat-bot/DynamicID.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity</title>
<link>https://arxiv.org/abs/2503.07677</link>
<guid>https://arxiv.org/abs/2503.07677</guid>
<content:encoded><![CDATA[
arXiv:2503.07677v3 Announce Type: replace-cross 
Abstract: Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net/Transformer) by leveraging sparse attention. Specifically, we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering a highly efficient and universally applicable solution. See Our project page : https://cubeyoung.github.io/pladis-proejct/
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.09516</link>
<guid>https://arxiv.org/abs/2503.09516</guid>
<content:encoded><![CDATA[
arXiv:2503.09516v4 Announce Type: replace-cross 
Abstract: Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealGeneral: Unifying Visual Generation via Temporal In-Context Learning with Video Models</title>
<link>https://arxiv.org/abs/2503.10406</link>
<guid>https://arxiv.org/abs/2503.10406</guid>
<content:encoded><![CDATA[
arXiv:2503.10406v2 Announce Type: replace-cross 
Abstract: Unifying diverse image generation tasks within a single framework remains a fundamental challenge in visual generation. While large language models (LLMs) achieve unification through task-agnostic data and generation, existing visual generation models fail to meet these principles. Current approaches either rely on per-task datasets and large-scale training or adapt pre-trained image models with task-specific modifications, limiting their generalizability. In this work, we explore video models as a foundation for unified image generation, leveraging their inherent ability to model temporal correlations. We introduce RealGeneral, a novel framework that reformulates image generation as a conditional frame prediction task, analogous to in-context learning in LLMs. To bridge the gap between video models and condition-image pairs, we propose (1) a Unified Conditional Embedding module for multi-modal alignment and (2) a Unified Stream DiT Block with decoupled adaptive LayerNorm and attention mask to mitigate cross-modal interference. RealGeneral demonstrates effectiveness in multiple important visual generation tasks, e.g., it achieves a 14.5% improvement in subject similarity for customized generation and a 10% enhancement in image quality for canny-to-image task. Project page: https://lyne1.github.io/realgeneral_web/; GitHub Link: https://github.com/Lyne1/RealGeneral
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective</title>
<link>https://arxiv.org/abs/2503.10638</link>
<guid>https://arxiv.org/abs/2503.10638</guid>
<content:encoded><![CDATA[
arXiv:2503.10638v2 Announce Type: replace-cross 
Abstract: Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BriLLM: Brain-inspired Large Language Model</title>
<link>https://arxiv.org/abs/2503.11299</link>
<guid>https://arxiv.org/abs/2503.11299</guid>
<content:encoded><![CDATA[
arXiv:2503.11299v5 Announce Type: replace-cross 
Abstract: This paper reports the first brain-inspired large language model (BriLLM). This is a non-Transformer, non-GPT, non-traditional machine learning input-output controlled generative language model. The model is based on the Signal Fully-connected flowing (SiFu) definition on the directed graph in terms of the neural network, and has the interpretability of all nodes on the graph of the whole model, instead of the traditional machine learning model that only has limited interpretability at the input and output ends. In the language model scenario, the token is defined as a node in the graph. A randomly shaped or user-defined signal flow flows between nodes on the principle of "least resistance" along paths. The next token or node to be predicted or generated is the target of the signal flow. As a language model, BriLLM theoretically supports infinitely long $n$-gram models when the model size is independent of the input and predicted length of the model. The model's working signal flow provides the possibility of recall activation and innate multi-modal support similar to the cognitive patterns of the human brain. At present, we released the first BriLLM version in Chinese, with 4000 tokens, 32-dimensional node width, 16-token long sequence prediction ability, and language model prediction performance comparable to GPT-1. More computing power will help us explore the infinite possibilities depicted above.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Diffusion Generative Models via Rich Preference Optimization</title>
<link>https://arxiv.org/abs/2503.11720</link>
<guid>https://arxiv.org/abs/2503.11720</guid>
<content:encoded><![CDATA[
arXiv:2503.11720v4 Announce Type: replace-cross 
Abstract: We introduce Rich Preference Optimization (RPO), a novel pipeline that leverages rich feedback signals to improve the curation of preference pairs for fine-tuning text-to-image diffusion models. Traditional methods, like Diffusion-DPO, often rely solely on reward model labeling, which can be opaque, offer limited insights into the rationale behind preferences, and are prone to issues such as reward hacking or overfitting. In contrast, our approach begins with generating detailed critiques of synthesized images, from which we extract reliable and actionable image editing instructions. By implementing these instructions, we create refined images, resulting in synthetic, informative preference pairs that serve as enhanced tuning datasets. We demonstrate the effectiveness of our pipeline and the resulting datasets in fine-tuning state-of-the-art diffusion models. Our code is available at https://github.com/Diffusion-RLHF/RPO.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Continual Instruction Tuning</title>
<link>https://arxiv.org/abs/2503.12897</link>
<guid>https://arxiv.org/abs/2503.12897</guid>
<content:encoded><![CDATA[
arXiv:2503.12897v2 Announce Type: replace-cross 
Abstract: A vast amount of instruction tuning data is crucial for the impressive performance of Large Multimodal Models (LMMs), but the associated computational costs and data collection demands during supervised fine-tuning make it impractical for most researchers. Federated learning (FL) has the potential to leverage all distributed data and training resources to reduce the overhead of joint training. However, most existing methods assume a fixed number of tasks, while in real-world scenarios, clients continuously encounter new knowledge and often struggle to retain old tasks due to memory constraints. In this work, we introduce the Federated Continual Instruction Tuning (FCIT) benchmark to model this real-world challenge. Our benchmark includes two realistic scenarios, encompassing four different settings and twelve carefully curated instruction tuning datasets. To address the challenges posed by FCIT, we propose dynamic knowledge organization to effectively integrate updates from different tasks during training and subspace selective activation to allocate task-specific output during inference. Extensive experimental results demonstrate that our proposed method significantly enhances model performance across varying levels of data heterogeneity and catastrophic forgetting. Code and dataset are released at https://github.com/Ghy0501/FCIT.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sampling Decisions</title>
<link>https://arxiv.org/abs/2503.14549</link>
<guid>https://arxiv.org/abs/2503.14549</guid>
<content:encoded><![CDATA[
arXiv:2503.14549v2 Announce Type: replace-cross 
Abstract: In this manuscript, we introduce a novel Decision Flow (DF) framework for sampling decisions from a target distribution while incorporating additional guidance from a prior sampler. DF can be viewed as an AI-driven algorithmic reincarnation of the Markov Decision Process (MDP) approach in stochastic optimal control. It extends the continuous-space, continuous-time Path Integral Diffusion sampling technique of [Behjoo, Chertkov 2025] to discrete time and space, while also generalizing the Generative Flow Network (GFN) framework of [Bengio, et al 2021]. In its most basic form an explicit formulation that does not require Neural Networks (NNs), DF leverages the linear solvability of the underlying MDP [Todorov, 2007] to adjust the transition probabilities of the prior sampler. The resulting Markov process is expressed as a convolution of the reverse-time Green's function of the prior sampling with the target distribution. We illustrate the DF framework through an example of sampling from the Ising model -- compare DF to Metropolis-Hastings to quantify its efficiency, discuss potential NN-based extensions, and outline how DF can enhance guided sampling across various applications.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TruthLens: Explainable DeepFake Detection for Face Manipulated and Fully Synthetic Data</title>
<link>https://arxiv.org/abs/2503.15867</link>
<guid>https://arxiv.org/abs/2503.15867</guid>
<content:encoded><![CDATA[
arXiv:2503.15867v2 Announce Type: replace-cross 
Abstract: Detecting DeepFakes has become a crucial research area as the widespread use of AI image generators enables the effortless creation of face-manipulated and fully synthetic content, yet existing methods are often limited to binary classification (real vs. fake) and lack interpretability. To address these challenges, we propose TruthLens, a novel and highly generalizable framework for DeepFake detection that not only determines whether an image is real or fake but also provides detailed textual reasoning for its predictions. Unlike traditional methods, TruthLens effectively handles both face-manipulated DeepFakes and fully AI-generated content while addressing fine-grained queries such as "Does the eyes/nose/mouth look real or fake?"
  The architecture of TruthLens combines the global contextual understanding of multimodal large language models like PaliGemma2 with the localized feature extraction capabilities of vision-only models like DINOv2. This hybrid design leverages the complementary strengths of both models, enabling robust detection of subtle manipulations while maintaining interpretability. Extensive experiments on diverse datasets demonstrate that TruthLens outperforms state-of-the-art methods in detection accuracy (by 2-14%) and explainability, in both in-domain and cross-data settings, generalizing effectively across traditional and emerging manipulation techniques.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of LLMs' Preferences for Libraries and Programming Languages</title>
<link>https://arxiv.org/abs/2503.17181</link>
<guid>https://arxiv.org/abs/2503.17181</guid>
<content:encoded><![CDATA[
arXiv:2503.17181v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly used to generate code, influencing users' choices of libraries and programming languages in critical real-world projects. However, little is known about their systematic biases or preferences toward certain libraries and programming languages, which can significantly impact software development practices. To fill this gap, we perform the first empirical study of LLMs' preferences for libraries and programming languages when generating code, covering eight diverse LLMs. Our results reveal that LLMs exhibit a strong tendency to overuse widely adopted libraries such as NumPy; in up to 48% of cases, this usage is unnecessary and deviates from the ground-truth solutions. LLMs also exhibit a significant preference toward Python as their default language. For high-performance project initialisation tasks where Python is not the optimal language, it remains the dominant choice in 58% of cases, and Rust is not used a single time. These results indicate that LLMs may prioritise familiarity and popularity over suitability and task-specific optimality. This will introduce security vulnerabilities and technical debt, and limit exposure to newly developed, better-suited tools and languages. Understanding and addressing these biases is essential for the responsible integration of LLMs into software development workflows.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWI: Speaking with Intent in Large Language Models</title>
<link>https://arxiv.org/abs/2503.21544</link>
<guid>https://arxiv.org/abs/2503.21544</guid>
<content:encoded><![CDATA[
arXiv:2503.21544v2 Announce Type: replace-cross 
Abstract: Intent, typically clearly formulated and planned, functions as a cognitive framework for communication and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model's underlying intention and provides high-level planning to guide subsequent analysis and action. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on text summarization, multi-task question answering, and mathematical reasoning benchmarks consistently demonstrate the effectiveness and generalizability of Speaking with Intent over direct generation without explicit intent. Further analysis corroborates the generalizability of SWI under different experimental settings. Moreover, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. The promising results in enhancing LLMs with explicit intents pave a new avenue for boosting LLMs' generation and reasoning abilities with cognitive notions.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models</title>
<link>https://arxiv.org/abs/2504.01216</link>
<guid>https://arxiv.org/abs/2504.01216</guid>
<content:encoded><![CDATA[
arXiv:2504.01216v2 Announce Type: replace-cross 
Abstract: Post-Traumatic Stress Disorder (PTSD) remains underdiagnosed in clinical settings, presenting opportunities for automated detection to identify patients. This study evaluates natural language processing approaches for detecting PTSD from clinical interview transcripts. We compared general and mental health-specific transformer models (BERT/RoBERTa), embedding-based methods (SentenceBERT/LLaMA), and large language model prompting strategies (zero-shot/few-shot/chain-of-thought) using the DAIC-WOZ dataset. Domain-specific end-to-end models significantly outperformed general models (Mental-RoBERTa AUPRC=0.675+/-0.084 vs. RoBERTa-base 0.599+/-0.145). SentenceBERT embeddings with neural networks achieved the highest overall performance (AUPRC=0.758+/-0.128). Few-shot prompting using DSM-5 criteria yielded competitive results with two examples (AUPRC=0.737). Performance varied significantly across symptom severity and comorbidity status with depression, with higher accuracy for severe PTSD cases and patients with comorbid depression. Our findings highlight the potential of domain-adapted embeddings and LLMs for scalable screening while underscoring the need for improved detection of nuanced presentations and offering insights for developing clinically viable AI tools for PTSD assessment.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dual-Route Model of Induction</title>
<link>https://arxiv.org/abs/2504.03022</link>
<guid>https://arxiv.org/abs/2504.03022</guid>
<content:encoded><![CDATA[
arXiv:2504.03022v2 Announce Type: replace-cross 
Abstract: Prior work on in-context copying has shown the existence of induction heads, which attend to and promote individual tokens during copying. In this work we discover a new type of induction head: concept-level induction heads, which copy entire lexical units instead of individual tokens. Concept induction heads learn to attend to the ends of multi-token words throughout training, working in parallel with token-level induction heads to copy meaningful text. We show that these heads are responsible for semantic tasks like word-level translation, whereas token induction heads are vital for tasks that can only be done verbatim (like copying nonsense tokens). These two "routes" operate independently: we show that ablation of token induction heads causes models to paraphrase where they would otherwise copy verbatim. By patching concept induction head outputs, we find that they contain language-independent word representations that mediate natural language translation, suggesting that LLMs represent abstract word meanings independent of language or form.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay</title>
<link>https://arxiv.org/abs/2504.03601</link>
<guid>https://arxiv.org/abs/2504.03601</guid>
<content:encoded><![CDATA[
arXiv:2504.03601v4 Announce Type: replace-cross 
Abstract: Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source 5K synthetic data trajectories and the trained xLAM-2-fc-r models to advance research in AI agents.
  Models at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4; Dataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website at https://apigen-mt.github.io
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Leakage in Concept-Based Methods: An Information Theoretic Approach</title>
<link>https://arxiv.org/abs/2504.09459</link>
<guid>https://arxiv.org/abs/2504.09459</guid>
<content:encoded><![CDATA[
arXiv:2504.09459v2 Announce Type: replace-cross 
Abstract: Concept Bottleneck Models (CBMs) aim to enhance interpretability by structuring predictions around human-understandable concepts. However, unintended information leakage, where predictive signals bypass the concept bottleneck, compromises their transparency. This paper introduces an information-theoretic measure to quantify leakage in CBMs, capturing the extent to which concept embeddings encode additional, unintended information beyond the specified concepts. We validate the measure through controlled synthetic experiments, demonstrating its effectiveness in detecting leakage trends across various configurations. Our findings highlight that feature and concept dimensionality significantly influence leakage, and that classifier choice impacts measurement stability, with XGBoost emerging as the most reliable estimator. Additionally, preliminary investigations indicate that the measure exhibits the anticipated behavior when applied to soft joint CBMs, suggesting its reliability in leakage quantification beyond fully synthetic settings. While this study rigorously evaluates the measure in controlled synthetic experiments, future work can extend its application to real-world datasets.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems</title>
<link>https://arxiv.org/abs/2504.09763</link>
<guid>https://arxiv.org/abs/2504.09763</guid>
<content:encoded><![CDATA[
arXiv:2504.09763v2 Announce Type: replace-cross 
Abstract: Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from reinforcement learning (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for mathematical reasoning as problem generators for stress-testing models. However, prior work has been limited to automatically constructing abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced mathematics problems by developing EFAGen, which operationalizes the task of automatically inferring an EFA for a given seed problem and solution as a program synthesis task. We first formalize the properties of any valid EFA as executable unit tests. Using execution feedback from the unit tests, we search over candidate programs sampled from a LLM to find EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. We then apply the tests as a reward signal, training LLMs to become better writers of EFAs. We show that EFAs inferred by EFAGen are faithful to the seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across diverse sources of competition-level math problems. Finally, we show uses of model-written EFAs e.g., finding harder/easier problem variants, as well as data generation.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Pruning Strategy for Multi-Component Neural Architectures Using Component-Aware Graph Analysis</title>
<link>https://arxiv.org/abs/2504.13296</link>
<guid>https://arxiv.org/abs/2504.13296</guid>
<content:encoded><![CDATA[
arXiv:2504.13296v2 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) deliver outstanding performance, but their complexity often prohibits deployment in resource-constrained settings. Comprehensive structured pruning frameworks based on parameter dependency analysis reduce model size with specific regard to computational performance. When applying them to Multi-Component Neural Architectures (MCNAs), they risk network integrity by removing large parameter groups. We introduce a component-aware pruning strategy, extending dependency graphs to isolate individual components and inter-component flows. This creates smaller, targeted pruning groups that conserve functional integrity. Demonstrated effectively on a control task, our approach achieves greater sparsity and reduced performance degradation, opening a path for optimizing complex, multi-component DNNs efficiently.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnyTSR: Any-Scale Thermal Super-Resolution for UAV</title>
<link>https://arxiv.org/abs/2504.13682</link>
<guid>https://arxiv.org/abs/2504.13682</guid>
<content:encoded><![CDATA[
arXiv:2504.13682v2 Announce Type: replace-cross 
Abstract: Thermal imaging can greatly enhance the application of intelligent unmanned aerial vehicles (UAV) in challenging environments. However, the inherent low resolution of thermal sensors leads to insufficient details and blurred boundaries. Super-resolution (SR) offers a promising solution to address this issue, while most existing SR methods are designed for fixed-scale SR. They are computationally expensive and inflexible in practical applications. To address above issues, this work proposes a novel any-scale thermal SR method (AnyTSR) for UAV within a single model. Specifically, a new image encoder is proposed to explicitly assign specific feature code to enable more accurate and flexible representation. Additionally, by effectively embedding coordinate offset information into the local feature ensemble, an innovative any-scale upsampler is proposed to better understand spatial relationships and reduce artifacts. Moreover, a novel dataset (UAV-TSR), covering both land and water scenes, is constructed for thermal SR tasks. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art methods across all scaling factors as well as generates more accurate and detailed high-resolution images. The code is located at https://github.com/vision4robotics/AnyTSR.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation</title>
<link>https://arxiv.org/abs/2504.20887</link>
<guid>https://arxiv.org/abs/2504.20887</guid>
<content:encoded><![CDATA[
arXiv:2504.20887v2 Announce Type: replace-cross 
Abstract: When optimising for conditional value at risk (CVaR) using policy gradients (PG), current methods rely on discarding a large proportion of trajectories, resulting in poor sample efficiency. We propose a reformulation of the CVaR optimisation problem by capping the total return of trajectories used in training, rather than simply discarding them, and show that this is equivalent to the original problem if the cap is set appropriately. We show, with empirical results in an number of environments, that this reformulation of the problem results in consistently improved performance compared to baselines. We have made all our code available here: https://github.com/HarryMJMead/cvar-return-capping.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoordField: Coordination Field for Agentic UAV Task Allocation In Low-altitude Urban Scenarios</title>
<link>https://arxiv.org/abs/2505.00091</link>
<guid>https://arxiv.org/abs/2505.00091</guid>
<content:encoded><![CDATA[
arXiv:2505.00091v4 Announce Type: replace-cross 
Abstract: With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV) swarms to perform complex tasks in urban environments, system design now faces major challenges, including efficient semantic understanding, flexible task planning, and the ability to dynamically adjust coordination strategies in response to evolving environmental conditions and continuously changing task requirements. To address the limitations of existing methods, this paper proposes CoordField, a coordination field agent system for coordinating heterogeneous drone swarms in complex urban scenarios. In this system, large language models (LLMs) is responsible for interpreting high-level human instructions and converting them into executable commands for the UAV swarms, such as patrol and target tracking. Subsequently, a Coordination field mechanism is proposed to guide UAV motion and task selection, enabling decentralized and adaptive allocation of emergent tasks. A total of 50 rounds of comparative testing were conducted across different models in a 2D simulation space to evaluate their performance. Experimental results demonstrate that the proposed system achieves superior performance in terms of task coverage, response time, and adaptability to dynamic changes.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization</title>
<link>https://arxiv.org/abs/2505.02192</link>
<guid>https://arxiv.org/abs/2505.02192</guid>
<content:encoded><![CDATA[
arXiv:2505.02192v2 Announce Type: replace-cross 
Abstract: Customized text-to-video generation with pre-trained large-scale models has recently garnered significant attention by focusing on identity and motion consistency. Existing works typically follow the isolated customized paradigm, where the subject identity or motion dynamics are customized exclusively. However, this paradigm completely ignores the intrinsic mutual constraints and synergistic interdependencies between identity and motion, resulting in identity-motion conflicts throughout the generation process that systematically degrade. To address this, we introduce DualReal, a novel framework that employs adaptive joint training to construct interdependencies between dimensions collaboratively. Specifically, DualReal is composed of two units: (1) Dual-aware Adaptation dynamically switches the training step (i.e., identity or motion), learns the current information guided by the frozen dimension prior, and employs a regularization strategy to avoid knowledge leakage; (2) StageBlender Controller leverages the denoising stages and Diffusion Transformer depths to guide different dimensions with adaptive granularity, avoiding conflicts at various stages and ultimately achieving lossless fusion of identity and motion patterns. We constructed a more comprehensive evaluation benchmark than existing methods. The experimental results show that DualReal improves CLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top performance on nearly all motion metrics. Page: https://wenc-k.github.io/dualreal-customization
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI</title>
<link>https://arxiv.org/abs/2505.05895</link>
<guid>https://arxiv.org/abs/2505.05895</guid>
<content:encoded><![CDATA[
arXiv:2505.05895v2 Announce Type: replace-cross 
Abstract: Modern automotive infotainment systems require intelligent and adaptive solutions to handle frequent User Interface (UI) updates and diverse design variations. We introduce a vision-language framework for understanding and interacting with automotive infotainment systems, enabling seamless adaptation across different UI designs. To further support research in this field, we release AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208 annotations. Additionally, we present a synthetic data pipeline to generate training data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation (LoRa) and incorporating reasoning generated by our pipeline, along with visual grounding and evaluation capabilities. The fine-tuned Evaluative Large Action Model (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and dataset are available on Hugging Face) and demonstrating strong cross-domain generalization, including a +5.6% improvement on ScreenSpot over the baseline model. Notably, our approach achieves 80.8% average accuracy on ScreenSpot, closely matching or even surpassing specialized models for desktop, mobile, and web, such as ShowUI, despite being trained for the infotainment domain. This research investigates how data collection and subsequent fine-tuning can lead to AI-driven progress within automotive UI understanding and interaction. The applied method is cost-efficient and fine-tuned models can be deployed on consumer-grade GPUs.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know Or Not: a library for evaluating out-of-knowledge base robustness</title>
<link>https://arxiv.org/abs/2505.13545</link>
<guid>https://arxiv.org/abs/2505.13545</guid>
<content:encoded><![CDATA[
arXiv:2505.13545v2 Announce Type: replace-cross 
Abstract: While the capabilities of large language models (LLMs) have progressed significantly, their use in high-stakes applications have been limited due to risks of hallucination. One key approach in reducing hallucination is retrieval-augmented generation (RAG), but even in such setups, LLMs may still hallucinate when presented with questions outside of the knowledge base. Such behavior is unacceptable in high-stake applications where LLMs are expected to abstain from answering queries it does not have sufficient context on. In this work, we present a novel methodology for systematically evaluating out-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not know) in the RAG setting, without the need for manual annotation of gold standard answers. We implement our methodology in knowornot, an open-source library that enables users to develop their own customized evaluation data and pipelines for OOKB robustness. knowornot comprises four main features. Firstly, it provides a unified, high-level API that streamlines the process of setting up and running robustness benchmarks. Secondly, its modular architecture emphasizes extensibility and flexibility, allowing users to easily integrate their own LLM clients and RAG settings. Thirdly, its rigorous data modeling design ensures experiment reproducibility, reliability and traceability. Lastly, it implements a comprehensive suite of tools for users to customize their pipelines. We demonstrate the utility of knowornot by developing a challenging benchmark, PolicyBench, which spans four Question-Answer (QA) chatbots on government policies, and analyze its OOKB robustness. The source code of knowornot is available https://github.com/govtech-responsibleai/KnowOrNot.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial bandit optimization for approximately linear functions</title>
<link>https://arxiv.org/abs/2505.20734</link>
<guid>https://arxiv.org/abs/2505.20734</guid>
<content:encoded><![CDATA[
arXiv:2505.20734v4 Announce Type: replace-cross 
Abstract: We consider a bandit optimization problem for nonconvex and non-smooth functions, where in each trial the loss function is the sum of a linear function and a small but arbitrary perturbation chosen after observing the player's choice. We give both expected and high probability regret bounds for the problem. Our result also implies an improved high-probability regret bound for the bandit linear optimization, a special case with no perturbation. We also give a lower bound on the expected regret.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemiOccam: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels</title>
<link>https://arxiv.org/abs/2506.03582</link>
<guid>https://arxiv.org/abs/2506.03582</guid>
<content:encoded><![CDATA[
arXiv:2506.03582v3 Announce Type: replace-cross 
Abstract: We present SemiOccam, an image recognition network that leverages semi-supervised learning in a highly efficient manner. Existing works often rely on complex training techniques and architectures, requiring hundreds of GPU hours for training, while their generalization ability with extremely limited labeled data remains to be improved. To address these limitations, we construct a hierarchical mixture density classification mechanism by optimizing mutual information between feature representations and target classes, compressing redundant information while retaining crucial discriminative components. Experimental results demonstrate that our method achieves state-of-the-art performance on three commonly used datasets, with accuracy exceeding 95% on two of them using only 4 labeled samples per class, and its simple architecture keeps training time at the minute level. Notably, this paper reveals a long-overlooked data leakage issue in the STL-10 dataset for semi-supervised learning and removes duplicates to ensure reliable experimental results. We release the deduplicated CleanSTL-10 dataset to facilitate fair and reproducible research. Code available at https://github.com/Shu1L0n9/SemiOccam.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification</title>
<link>https://arxiv.org/abs/2506.06806</link>
<guid>https://arxiv.org/abs/2506.06806</guid>
<content:encoded><![CDATA[
arXiv:2506.06806v2 Announce Type: replace-cross 
Abstract: The explosion of textual data has made manual document classification increasingly challenging. To address this, we introduce a robust, efficient domain-agnostic generative model framework for multi-label text classification. Instead of treating labels as mere atomic symbols, our approach utilizes predefined label descriptions and is trained to generate these descriptions based on the input text. During inference, the generated descriptions are matched to the pre-defined labels using a finetuned sentence transformer. We integrate this with a dual-objective loss function, combining cross-entropy loss and cosine similarity of the generated sentences with the predefined target descriptions, ensuring both semantic alignment and accuracy. Our proposed model LAGAMC stands out for its parameter efficiency and versatility across diverse datasets, making it well-suited for practical applications. We demonstrate the effectiveness of our proposed model by achieving new state-of-the-art performances across all evaluated datasets, surpassing several strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in Macro-F1 compared to the closest baseline across all datasets.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Draft-based Approximate Inference for LLMs</title>
<link>https://arxiv.org/abs/2506.08373</link>
<guid>https://arxiv.org/abs/2506.08373</guid>
<content:encoded><![CDATA[
arXiv:2506.08373v2 Announce Type: replace-cross 
Abstract: Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, the first method that leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal Representation</title>
<link>https://arxiv.org/abs/2506.10351</link>
<guid>https://arxiv.org/abs/2506.10351</guid>
<content:encoded><![CDATA[
arXiv:2506.10351v2 Announce Type: replace-cross 
Abstract: Physiological signals are often corrupted by motion artifacts, baseline drift, and other low-SNR disturbances, which pose significant challenges for analysis. Additionally, these signals exhibit strong non-stationarity, with sharp peaks and abrupt changes that evolve continuously, making them difficult to represent using traditional time-domain or filtering methods. To address these issues, a novel wavelet-based approach for physiological signal analysis is presented, aiming to capture multi-scale time-frequency features in various physiological signals. Leveraging this technique, two large-scale pretrained models specific to EMG and ECG are introduced for the first time, achieving superior performance and setting new baselines in downstream tasks. Additionally, a unified multi-modal framework is constructed by integrating pretrained EEG model, where each modality is guided through its dedicated branch and fused via learnable weighted fusion. This design effectively addresses challenges such as low signal-to-noise ratio, high inter-subject variability, and device mismatch, outperforming existing methods on multi-modal tasks. The proposed wavelet-based architecture lays a solid foundation for analysis of diverse physiological signals, while the multi-modal design points to next-generation physiological signal processing with potential impact on wearable health monitoring, clinical diagnostics, and broader biomedical applications.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and Cybersecurity Applications</title>
<link>https://arxiv.org/abs/2506.10467</link>
<guid>https://arxiv.org/abs/2506.10467</guid>
<content:encoded><![CDATA[
arXiv:2506.10467v4 Announce Type: replace-cross 
Abstract: Recent advancements in LLMs indicate potential for novel applications, as evidenced by the reasoning capabilities in the latest OpenAI and DeepSeek models. To apply these models to domain-specific applications beyond text generation, LLM-based multi-agent systems can be utilized to solve complex tasks, particularly by combining reasoning techniques, code generation, and software execution across multiple, potentially specialized LLMs. However, while many evaluations are performed on LLMs, reasoning techniques, and applications individually, their joint specification and combined application are not well understood. Defined specifications for multi-agent LLM systems are required to explore their potential and suitability for specific applications, allowing for systematic evaluations of LLMs, reasoning techniques, and related aspects. This paper reports the results of exploratory research on (1.) multi-agent specification by introducing an agent schema language and (2.) the execution and evaluation of the specifications through a multi-agent system architecture and prototype. The specification language, system architecture, and prototype are first presented in this work, building on an LLM system from prior research. Test cases involving cybersecurity tasks indicate the feasibility of the architecture and evaluation approach. As a result, evaluations could be demonstrated for question answering, server security, and network security tasks completed correctly by agents with LLMs from OpenAI and DeepSeek.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation</title>
<link>https://arxiv.org/abs/2506.11092</link>
<guid>https://arxiv.org/abs/2506.11092</guid>
<content:encoded><![CDATA[
arXiv:2506.11092v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has significantly advanced large language models (LLMs) by grounding their outputs in external tools and knowledge sources. However, existing RAG systems are typically constrained to static, single-turn interactions with fixed toolsets, making them ill-suited for dynamic domains such as healthcare and smart homes, where user intent, available tools, and contextual factors evolve over time. We present Dynamic Context Tuning (DCT), a lightweight framework that extends RAG to support multi-turn dialogue and evolving tool environments without requiring retraining. DCT integrates an attention-based context cache to track relevant past information, LoRA-based retrieval to dynamically select domain-specific tools, and efficient context compression to maintain inputs within LLM context limits. Experiments on both synthetic and real-world benchmarks show that DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while matching GPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to previously unseen tools, enabling scalable and adaptable AI assistants across a wide range of dynamic environments.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs</title>
<link>https://arxiv.org/abs/2506.11558</link>
<guid>https://arxiv.org/abs/2506.11558</guid>
<content:encoded><![CDATA[
arXiv:2506.11558v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with LLM-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Digital Divide: Small Language Models as a Pathway for Physics and Photonics Education in Underdeveloped Regions</title>
<link>https://arxiv.org/abs/2506.12403</link>
<guid>https://arxiv.org/abs/2506.12403</guid>
<content:encoded><![CDATA[
arXiv:2506.12403v2 Announce Type: replace-cross 
Abstract: Limited infrastructure, scarce educational resources, and unreliable internet access often hinder physics and photonics education in underdeveloped regions. These barriers create deep inequities in Science, Technology, Engineering, and Mathematics (STEM) education. This article explores how Small Language Models (SLMs)-compact, AI-powered tools that can run offline on low-power devices, offering a scalable solution. By acting as virtual tutors, enabling native-language instruction, and supporting interactive learning, SLMs can help address the shortage of trained educators and laboratory access. By narrowing the digital divide through targeted investment in AI technologies, SLMs present a scalable and inclusive solution to advance STEM education and foster scientific empowerment in marginalized communities.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan for Speed: Dilated Scheduling for Masked Diffusion Language Models</title>
<link>https://arxiv.org/abs/2506.19037</link>
<guid>https://arxiv.org/abs/2506.19037</guid>
<content:encoded><![CDATA[
arXiv:2506.19037v2 Announce Type: replace-cross 
Abstract: Masked diffusion language models (MDLMs) promise fast, non-autoregressive text generation, yet existing samplers, which pick tokens to unmask based on model confidence, ignore interactions when unmasking multiple positions in parallel and effectively reduce to slow, autoregressive behavior. We propose the Dilated Unmasking Scheduler (DUS), an inference-only, planner-model-free method that partitions sequence positions into non-adjacent dilated groups and unmasked them in parallel so as to minimize an upper bound on joint entropy gain at each denoising step. By explicitly trading off the number of network calls against generation quality, DUS recovers most of the performance lost under traditional parallel unmasking strategies. Across math (GSM8K, MATH500), code (HumanEval, MBPP) and general-knowledge benchmarks (BBH, MMLU-Pro), DUS outperforms confidence-based planners, without modifying the underlying denoiser, and reveals the true speed-quality frontier of MDLMs.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization</title>
<link>https://arxiv.org/abs/2506.23516</link>
<guid>https://arxiv.org/abs/2506.23516</guid>
<content:encoded><![CDATA[
arXiv:2506.23516v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) often suffers from performance degradation due to key challenges such as data heterogeneity and communication constraints. To address these limitations, we present a novel FL framework called FedWSQ, which integrates weight standardization (WS) and the proposed distribution-aware non-uniform quantization (DANUQ). WS enhances FL performance by filtering out biased components in local updates during training, thereby improving the robustness of the model against data heterogeneity and unstable client participation. In addition, DANUQ minimizes quantization errors by leveraging the statistical properties of local model updates. As a result, FedWSQ significantly reduces communication overhead while maintaining superior model accuracy. Extensive experiments on FL benchmark datasets demonstrate that FedWSQ consistently outperforms existing FL methods across various challenging FL settings, including extreme data heterogeneity and ultra-low-bit communication scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Before, I Asked My Mom, Now I Ask ChatGPT": Visual Privacy Management with Generative AI for Blind and Low-Vision People</title>
<link>https://arxiv.org/abs/2507.00286</link>
<guid>https://arxiv.org/abs/2507.00286</guid>
<content:encoded><![CDATA[
arXiv:2507.00286v2 Announce Type: replace-cross 
Abstract: Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to interpret and manage visual content in their daily lives. While such tools can enhance the accessibility of visual content and so enable greater user independence, they also introduce complex challenges around visual privacy. In this paper, we investigate the current practices and future design preferences of blind and low vision individuals through an interview study with 21 participants. Our findings reveal a range of current practices with GenAI that balance privacy, efficiency, and emotional agency, with users accounting for privacy risks across six key scenarios, such as self-presentation, indoor/outdoor spatial privacy, social sharing, and handling professional content. Our findings reveal design preferences, including on-device processing, zero-retention guarantees, sensitive content redaction, privacy-aware appearance indicators, and multimodal tactile mirrored interaction methods. We conclude with actionable design recommendations to support user-centered visual privacy through GenAI, expanding the notion of privacy and responsible handling of others data.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.00709</link>
<guid>https://arxiv.org/abs/2507.00709</guid>
<content:encoded><![CDATA[
arXiv:2507.00709v2 Announce Type: replace-cross 
Abstract: Lane segment topology reasoning constructs a comprehensive road network by capturing the topological relationships between lane segments and their semantic types. This enables end-to-end autonomous driving systems to perform road-dependent maneuvers such as turning and lane changing. However, the limitations in consistent positional embedding and temporal multiple attribute learning in existing methods hinder accurate roadnet reconstruction. To address these issues, we propose TopoStreamer, an end-to-end temporal perception model for lane segment topology reasoning. Specifically, TopoStreamer introduces three key improvements: streaming attribute constraints, dynamic lane boundary positional encoding, and lane segment denoising. The streaming attribute constraints enforce temporal consistency in both centerline and boundary coordinates, along with their classifications. Meanwhile, dynamic lane boundary positional encoding enhances the learning of up-to-date positional information within queries, while lane segment denoising helps capture diverse lane segment patterns, ultimately improving model performance. Additionally, we assess the accuracy of existing models using a lane boundary classification metric, which serves as a crucial measure for lane-changing scenarios in autonomous driving. On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements over state-of-the-art methods, achieving substantial performance gains of +3.0% mAP in lane segment perception and +1.7% OLS in centerline perception tasks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frequency-Aligned Knowledge Distillation for Lightweight Spatiotemporal Forecasting</title>
<link>https://arxiv.org/abs/2507.02939</link>
<guid>https://arxiv.org/abs/2507.02939</guid>
<content:encoded><![CDATA[
arXiv:2507.02939v2 Announce Type: replace-cross 
Abstract: Spatiotemporal forecasting tasks, such as traffic flow, combustion dynamics, and weather forecasting, often require complex models that suffer from low training efficiency and high memory consumption. This paper proposes a lightweight framework, Spectral Decoupled Knowledge Distillation (termed SDKD), which transfers the multi-scale spatiotemporal representations from a complex teacher model to a more efficient lightweight student network. The teacher model follows an encoder-latent evolution-decoder architecture, where its latent evolution module decouples high-frequency details and low-frequency trends using convolution and Transformer (global low-frequency modeler). However, the multi-layer convolution and deconvolution structures result in slow training and high memory usage. To address these issues, we propose a frequency-aligned knowledge distillation strategy, which extracts multi-scale spectral features from the teacher's latent space, including both high and low frequency components, to guide the lightweight student model in capturing both local fine-grained variations and global evolution patterns. Experimental results show that SDKD significantly improves performance, achieving reductions of up to 81.3% in MSE and in MAE 52.3% on the Navier-Stokes equation dataset. The framework effectively captures both high-frequency variations and long-term trends while reducing computational complexity. Our codes are available at https://github.com/itsnotacie/SDKD
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Software Bug Reports: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2507.04422</link>
<guid>https://arxiv.org/abs/2507.04422</guid>
<content:encoded><![CDATA[
arXiv:2507.04422v2 Announce Type: replace-cross 
Abstract: The recent advancement of artificial intelligence, especially machine learning (ML), has significantly impacted software engineering research, including bug report analysis. ML aims to automate the understanding, extraction, and correlation of information from bug reports. Despite its growing importance, there has been no comprehensive review in this area. In this paper, we present a systematic literature review covering 1,825 papers, selecting 204 for detailed analysis. We derive seven key findings: 1) Extensive use of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like BERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular for feature representation, with a rise in deep learning approaches. 3) Stop word removal is the most common preprocessing, with structural methods rising after 2020. 4) Eclipse and Mozilla are the most frequently evaluated software projects. 5) Bug categorization is the most common task, followed by bug localization and severity prediction. 6) There is increasing attention on specific bugs like non-functional and performance bugs. 7) Common evaluation metrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold cross-validation preferred for model evaluation. 8) Many studies lack robust statistical tests. We also identify six promising future research directions to provide useful insights for practitioners.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning</title>
<link>https://arxiv.org/abs/2507.04790</link>
<guid>https://arxiv.org/abs/2507.04790</guid>
<content:encoded><![CDATA[
arXiv:2507.04790v2 Announce Type: replace-cross 
Abstract: Motion planning is a crucial component of autonomous robot driving. While various trajectory datasets exist, effectively utilizing them for a target domain remains challenging due to differences in agent interactions and environmental characteristics. Conventional approaches, such as domain adaptation or ensemble learning, leverage multiple source datasets but suffer from domain imbalance, catastrophic forgetting, and high computational costs. To address these challenges, we propose Interaction-Merged Motion Planning (IMMP), a novel approach that leverages parameter checkpoints trained on different domains during adaptation to the target domain. IMMP follows a two-step process: pre-merging to capture agent behaviors and interactions, sufficiently extracting diverse information from the source domain, followed by merging to construct an adaptable model that efficiently transfers diverse interactions to the target domain. Our method is evaluated on various planning benchmarks and models, demonstrating superior performance compared to conventional approaches.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration</title>
<link>https://arxiv.org/abs/2507.05108</link>
<guid>https://arxiv.org/abs/2507.05108</guid>
<content:encoded><![CDATA[
arXiv:2507.05108v2 Announce Type: replace-cross 
Abstract: Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet practical needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and 6,543 synthetic images with character-level and line-level locations, as well as character annotations in different damage grades. AutoHDR mimics historians' restoration workflows through a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. The modular architecture of AutoHDR enables seamless human-machine collaboration, allowing for flexible intervention and optimization at each restoration stage. Experiments demonstrate AutoHDR's remarkable performance in HDR. When processing severely damaged documents, our method improves OCR accuracy from 46.83% to 84.05%, with further enhancement to 94.25% through human-machine collaboration. We believe this work represents a significant advancement in automated historical document restoration and contributes substantially to cultural heritage preservation. The model and dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis</title>
<link>https://arxiv.org/abs/2507.06060</link>
<guid>https://arxiv.org/abs/2507.06060</guid>
<content:encoded><![CDATA[
arXiv:2507.06060v2 Announce Type: replace-cross 
Abstract: Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning</title>
<link>https://arxiv.org/abs/2507.07485</link>
<guid>https://arxiv.org/abs/2507.07485</guid>
<content:encoded><![CDATA[
arXiv:2507.07485v2 Announce Type: replace-cross 
Abstract: Multi-Task Learning (MTL) enables multiple tasks to be learned within a shared network, but differences in objectives across tasks can cause negative transfer, where the learning of one task degrades another task's performance. While pre-trained transformers significantly improve MTL performance, their fixed network capacity and rigid structure limit adaptability. Previous dynamic network architectures attempt to address this but are inefficient as they directly convert shared parameters into task-specific ones. We propose Dynamic Token Modulation and Expansion (DTME-MTL), a framework applicable to any transformer-based MTL architecture. DTME-MTL enhances adaptability and reduces overfitting by identifying gradient conflicts in token space and applying adaptive solutions based on conflict type. Unlike prior methods that mitigate negative transfer by duplicating network parameters, DTME-MTL operates entirely in token space, enabling efficient adaptation without excessive parameter growth. Extensive experiments demonstrate that DTME-MTL consistently improves multi-task performance with minimal computational overhead, offering a scalable and effective solution for enhancing transformer-based MTL models.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time Training</title>
<link>https://arxiv.org/abs/2507.07778</link>
<guid>https://arxiv.org/abs/2507.07778</guid>
<content:encoded><![CDATA[
arXiv:2507.07778v2 Announce Type: replace-cross 
Abstract: Generalizing neural networks to unseen target domains is a significant challenge in real-world deployments. Test-time training (TTT) addresses this by using an auxiliary self-supervised task to reduce the domain gap caused by distribution shifts between the source and target. However, we find that when models are required to perform multiple tasks under domain shifts, conventional TTT methods suffer from unsynchronized task behavior, where the adaptation steps needed for optimal performance in one task may not align with the requirements of other tasks. To address this, we propose a novel TTT approach called Synchronizing Tasks for Test-time Training (S4T), which enables the concurrent handling of multiple tasks. The core idea behind S4T is that predicting task relations across domain shifts is key to synchronizing tasks during test time. To validate our approach, we apply S4T to conventional multi-task benchmarks, integrating it with traditional TTT protocols. Our empirical results show that S4T outperforms state-of-the-art TTT methods across various benchmarks.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent LLMs as Ethics Advocates for AI-Based Systems</title>
<link>https://arxiv.org/abs/2507.08392</link>
<guid>https://arxiv.org/abs/2507.08392</guid>
<content:encoded><![CDATA[
<div> Framework, ethics requirements, multi-agent LLM, system description, human feedback

Summary:
The study proposes a framework for generating ethics requirements drafts by introducing an ethics advocate agent in a multi-agent LLM setting. This agent critiques and provides input on ethical issues based on the system description. It aims to address the challenges of eliciting manual ethics requirements, such as time and resource constraints. The framework was evaluated through two case studies, demonstrating its ability to capture a significant number of ethics requirements and introduce relevant additional ones. However, it also identified reliability issues in generating ethics requirements, emphasizing the importance of human feedback in this sensitive domain. Overall, the framework offers a promising approach to incorporating ethics into the requirements engineering process and creating ethically aligned systems. It provides a practical solution to the low priority often given to ethics in the requirements elicitation process, ultimately leading to more ethically aligned products. 

<br /><br />Summary: <div>
arXiv:2507.08392v2 Announce Type: replace 
Abstract: Incorporating ethics into the requirement elicitation process is essential for creating ethically aligned systems. Although eliciting manual ethics requirements is effective, it requires diverse input from multiple stakeholders, which can be challenging due to time and resource constraints. Moreover, it is often given a low priority in the requirements elicitation process. This study proposes a framework for generating ethics requirements drafts by introducing an ethics advocate agent in a multi-agent LLM setting. This agent critiques and provides input on ethical issues based on the system description. The proposed framework is evaluated through two case studies from different contexts, demonstrating that it captures the majority of ethics requirements identified by researchers during 30-minute interviews and introduces several additional relevant requirements. However, it also highlights reliability issues in generating ethics requirements, emphasizing the need for human feedback in this sensitive domain. We believe this work can facilitate the broader adoption of ethics in the requirements engineering process, ultimately leading to more ethically aligned products.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop</title>
<link>https://arxiv.org/abs/2507.04295</link>
<guid>https://arxiv.org/abs/2507.04295</guid>
<content:encoded><![CDATA[
<div> personalized feedback, curriculum-aligned, science education, error-aware assessment, modular system

Summary:
LearnLens is a modular system based on Language Model (LLM) technology designed to provide personalized feedback in science education. The system consists of three components: an error-aware assessment module that identifies nuanced reasoning errors, a curriculum-grounded generation module that utilizes a structured memory chain for feedback generation, and an educator-in-the-loop interface for customization and oversight. By using a topic-linked memory chain instead of traditional similarity-based retrieval, LearnLens delivers more relevant feedback while reducing unnecessary noise. This approach addresses the challenges faced by existing feedback systems, offering scalable and high-quality feedback that benefits both teachers and students. With LearnLens, educators can efficiently provide effective feedback that enhances student learning in a curriculum-aligned manner. <div>
arXiv:2507.04295v3 Announce Type: replace-cross 
Abstract: Effective feedback is essential for student learning but is time-intensive for teachers. We present LearnLens, a modular, LLM-based system that generates personalised, curriculum-aligned feedback in science education. LearnLens comprises three components: (1) an error-aware assessment module that captures nuanced reasoning errors; (2) a curriculum-grounded generation module that uses a structured, topic-linked memory chain rather than traditional similarity-based retrieval, improving relevance and reducing noise; and (3) an educator-in-the-loop interface for customisation and oversight. LearnLens addresses key challenges in existing systems, offering scalable, high-quality feedback that empowers both teachers and students.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling</title>
<link>https://arxiv.org/abs/2507.10534</link>
<guid>https://arxiv.org/abs/2507.10534</guid>
<content:encoded><![CDATA[
<div> DSP, AI music generation, audio effects, neural networks, plugin

Summary:<br />
This article discusses the challenges in AI-driven modeling of professional Digital Signal Processing (DSP) workflows, specifically in replicating the nuanced signal flow and parameter interactions in audio effect graphs. The WildFX pipeline, containerized with Docker, is introduced to generate multi-track audio mixing datasets with rich effect graphs using a professional Digital Audio Workstation (DAW) backend. WildFX supports the integration of commercial plugins or any available plugins in VST/VST3/LV2/CLAP formats, allowing for structural complexity and efficient parallelized processing. The pipeline simplifies project/plugin configuration through a minimalist metadata interface. Experiments confirm the pipeline's validity in blind estimation of mixing graphs, plugin/gain parameters, and its capability to bridge AI research with practical DSP demands. The code repository for WildFX is available on GitHub at https://github.com/IsaacYQH/WildFX.<br /> <div>
arXiv:2507.10534v2 Announce Type: replace-cross 
Abstract: Despite rapid progress in end-to-end AI music generation, AI-driven modeling of professional Digital Signal Processing (DSP) workflows remains challenging. In particular, while there is growing interest in neural black-box modeling of audio effect graphs (e.g. reverb, compression, equalization), AI-based approaches struggle to replicate the nuanced signal flow and parameter interactions used in professional workflows. Existing differentiable plugin approaches often diverge from real-world tools, exhibiting inferior performance relative to simplified neural controllers under equivalent computational constraints. We introduce WildFX, a pipeline containerized with Docker for generating multi-track audio mixing datasets with rich effect graphs, powered by a professional Digital Audio Workstation (DAW) backend. WildFX supports seamless integration of cross-platform commercial plugins or any plugins in the wild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g., sidechains, crossovers) and achieving efficient parallelized processing. A minimalist metadata interface simplifies project/plugin configuration. Experiments demonstrate the pipeline's validity through blind estimation of mixing graphs, plugin/gain parameters, and its ability to bridge AI research with practical DSP demands. The code is available on: https://github.com/IsaacYQH/WildFX.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination</title>
<link>https://arxiv.org/abs/2507.13511</link>
<guid>https://arxiv.org/abs/2507.13511</guid>
<content:encoded><![CDATA[
<div> GraphTrafficGPT, Large Language Models, traffic management, graph-based architecture, task coordination <br />
<br />
GraphTrafficGPT is proposed as an improved architecture for Large Language Models (LLMs) in traffic management. It utilizes a graph-based approach to represent tasks and dependencies, enabling parallel execution and dynamic resource allocation. The model includes a Brain Agent that decomposes user queries, constructs optimized dependency graphs, and coordinates a network of specialized agents for various tasks. By introducing advanced context-aware token management and supporting concurrent multi-query processing, GraphTrafficGPT reduces token consumption by 50.2%, average response latency by 19.0%, and improves efficiency by 23.0% for multi-query execution compared to TrafficGPT. These enhancements make GraphTrafficGPT more efficient for complex, real-world scenarios in urban mobility environments. <br /><br />Summary: <div>
arXiv:2507.13511v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer significant promise for intelligent traffic management; however, current chain-based systems like TrafficGPT are hindered by sequential task execution, high token usage, and poor scalability, making them inefficient for complex, real-world scenarios. To address these limitations, we propose GraphTrafficGPT, a novel graph-based architecture, which fundamentally redesigns the task coordination process for LLM-driven traffic applications. GraphTrafficGPT represents tasks and their dependencies as nodes and edges in a directed graph, enabling efficient parallel execution and dynamic resource allocation. The main idea behind the proposed model is a Brain Agent that decomposes user queries, constructs optimized dependency graphs, and coordinates a network of specialized agents for data retrieval, analysis, visualization, and simulation. By introducing advanced context-aware token management and supporting concurrent multi-query processing, the proposed architecture handles interdependent tasks typical of modern urban mobility environments. Experimental results demonstrate that GraphTrafficGPT reduces token consumption by 50.2% and average response latency by 19.0% compared to TrafficGPT, while supporting simultaneous multi-query execution with up to 23.0% improvement in efficiency.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrefPalette: Personalized Preference Modeling with Latent Attributes</title>
<link>https://arxiv.org/abs/2507.13541</link>
<guid>https://arxiv.org/abs/2507.13541</guid>
<content:encoded><![CDATA[
<div> community values, preference modeling, attribute dimensions, human judgment, social communities 

Summary: 
PrefPalette introduces a framework for personalizing AI systems by understanding the underlying reasons for user preferences. It decomposes preferences into attribute dimensions and tailors its prediction to distinct social community values. The approach uses a multi-attribute decision-making principle, involving a counterfactual attribute synthesis step and attention-based preference modeling. It moves beyond aggregate preference modeling to capture diverse evaluation frameworks driving human judgment. Evaluated on 45 Reddit communities, PrefPalette outperforms GPT-4o by 46.6% in prediction accuracy. The model reveals community-specific profiles: scholarly communities focus on verbosity and stimulation, conflict-oriented communities value sarcasm and directness, and support-based communities emphasize empathy. PrefPalette not only provides superior preference modeling but also offers transparent and interpretable insights into human judgment, aiming to create more trustworthy and value-aware personalized applications. 

<br /><br />Summary: <div>
arXiv:2507.13541v1 Announce Type: new 
Abstract: Personalizing AI systems requires understanding not just what users prefer, but the reasons that underlie those preferences - yet current preference models typically treat human judgment as a black box. We introduce PrefPalette, a framework that decomposes preferences into attribute dimensions and tailors its preference prediction to distinct social community values in a human-interpretable manner. PrefPalette operationalizes a cognitive science principle known as multi-attribute decision making in two ways: (1) a scalable counterfactual attribute synthesis step that involves generating synthetic training data to isolate for individual attribute effects (e.g., formality, humor, cultural values), and (2) attention-based preference modeling that learns how different social communities dynamically weight these attributes. This approach moves beyond aggregate preference modeling to capture the diverse evaluation frameworks that drive human judgment. When evaluated on 45 social communities from the online platform Reddit, PrefPalette outperforms GPT-4o by 46.6% in average prediction accuracy. Beyond raw predictive improvements, PrefPalette also shed light on intuitive, community-specific profiles: scholarly communities prioritize verbosity and stimulation, conflict-oriented communities value sarcasm and directness, and support-based communities emphasize empathy. By modeling the attribute-mediated structure of human judgment, PrefPalette delivers both superior preference modeling and transparent, interpretable insights, and serves as a first step toward more trustworthy, value-aware personalized applications.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models</title>
<link>https://arxiv.org/abs/2507.13550</link>
<guid>https://arxiv.org/abs/2507.13550</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, expert systems, Prolog, interpretability, precision<br />
Summary: 
The paper presents a new approach to using large language models (LLMs) in expert systems in a controlled and transparent manner. By limiting the domain and employing a prompt-based extraction approach, the authors create a symbolic representation of knowledge in Prolog that can be validated and corrected by human experts. This approach ensures interpretability, scalability, and reliability in the development of expert systems. Through experiments with Claude Sonnet 3.7 and GPT-4.1, the authors demonstrate strong adherence to facts and semantic coherence in the generated knowledge bases. The proposed hybrid solution combines the recall capacity of LLMs with the precision of symbolic systems, paving the way for dependable AI applications in sensitive domains.<br /><br />Summary: <div>
arXiv:2507.13550v1 Announce Type: new 
Abstract: The development of large language models (LLMs) has successfully transformed knowledge-based systems such as open domain question nswering, which can automatically produce vast amounts of seemingly coherent information. Yet, those models have several disadvantages like hallucinations or confident generation of incorrect or unverifiable facts. In this paper, we introduce a new approach to the development of expert systems using LLMs in a controlled and transparent way. By limiting the domain and employing a well-structured prompt-based extraction approach, we produce a symbolic representation of knowledge in Prolog, which can be validated and corrected by human experts. This approach also guarantees interpretability, scalability and reliability of the developed expert systems. Via quantitative and qualitative experiments with Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic coherence on our generated knowledge bases. We present a transparent hybrid solution that combines the recall capacity of LLMs with the precision of symbolic systems, thereby laying the foundation for dependable AI applications in sensitive domains.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Isn't Relational Learning Taking Over the World?</title>
<link>https://arxiv.org/abs/2507.13558</link>
<guid>https://arxiv.org/abs/2507.13558</guid>
<content:encoded><![CDATA[
<div> keywords: AI, entities, relational learning, statistical relational AI, data <br />
Summary: Relational learning, a field focusing on modeling entities and their relationships, is not as prevalent as pixel and word-based AI systems. Despite valuable data existing in relational formats like spreadsheets and databases, the prominence of relational learning remains limited. The paper highlights the importance of shifting focus towards modeling entities and their properties, emphasizing the need to incorporate relational learning into mainstream AI practices. Its rightful prominence can be achieved through addressing the challenges and limitations that currently hinder its widespread adoption. <div>
arXiv:2507.13558v1 Announce Type: new 
Abstract: AI seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety</title>
<link>https://arxiv.org/abs/2507.13625</link>
<guid>https://arxiv.org/abs/2507.13625</guid>
<content:encoded><![CDATA[
<div> Keywords: Information retrieval, question answering, safety regulations, compliance checking, BifrostRAG

Summary:
BifrostRAG is introduced as a dual-graph RAG-integrated system for information retrieval and question answering in safety regulations. It addresses the complexity of regulatory text by modeling linguistic relationships and document structure through two graphs. The system combines graph traversal with semantic search, enabling large language models to reason over text meaning and structure. Evaluation on a multi-hop question dataset demonstrates BifrostRAG's precision of 92.8%, recall of 85.5%, and F1 score of 87.3%, outperforming current approaches. Error analysis showcases the advantages of the hybrid method over vector-only and graph-only systems. BifrostRAG is established as a robust knowledge engine for compliance checking driven by large language models. Its dual-graph, hybrid retrieval mechanism offers a blueprint for navigating complex technical documents in knowledge-intensive engineering domains. 

<br /><br />Summary: <div>
arXiv:2507.13625v1 Announce Type: new 
Abstract: Information retrieval and question answering from safety regulations are essential for automated construction compliance checking but are hindered by the linguistic and structural complexity of regulatory text. Many compliance-related queries are multi-hop, requiring synthesis of information across interlinked clauses. This poses a challenge for traditional retrieval-augmented generation (RAG) systems. To overcome this, we introduce BifrostRAG: a dual-graph RAG-integrated system that explicitly models both linguistic relationships (via an Entity Network Graph) and document structure (via a Document Navigator Graph). This architecture powers a hybrid retrieval mechanism that combines graph traversal with vector-based semantic search, enabling large language models to reason over both the meaning and the structure of the text. Evaluation on a multi-hop question dataset shows that BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1 score of 87.3 percent. These results significantly outperform vector-only and graph-only RAG baselines that represent current leading approaches. Error analysis further highlights the comparative advantages of our hybrid method over single-modality RAGs. These findings establish BifrostRAG as a robust knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid retrieval mechanism offers a transferable blueprint for navigating complex technical documents across knowledge-intensive engineering domains.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks</title>
<link>https://arxiv.org/abs/2507.13651</link>
<guid>https://arxiv.org/abs/2507.13651</guid>
<content:encoded><![CDATA[
<div> intelligent tutoring systems, error diagnosis, final answer, automated diagnosis, quadratic equations  
Summary:  
- Many intelligent tutoring systems support students in stepwise tasks, but diagnosing errors when steps are combined can be challenging due to combinatorial explosion.  
- Using a final answer for diagnosis can help reduce the complexity, as there are fewer possible erroneous final answers compared to solution paths.  
- This study explores automated error diagnosis based on final answers, aiming to provide buggy rule diagnosis when students combine steps.  
- Applying this approach to a dataset of student steps in solving quadratic equations showed a 29.4% success rate in diagnosing previously undiagnosed steps.  
- Comparing generated diagnoses with teacher diagnoses on a subset of cases revealed a 97% alignment, indicating the potential effectiveness of the final answer evaluation approach.  

<br /><br />Summary: <div>
arXiv:2507.13651v1 Announce Type: new 
Abstract: Many intelligent tutoring systems can support a student in solving a stepwise task. When a student combines several steps in one step, the number of possible paths connecting consecutive inputs may be very large. This combinatorial explosion makes error diagnosis hard. Using a final answer to diagnose a combination of steps can mitigate the combinatorial explosion, because there are generally fewer possible (erroneous) final answers than (erroneous) solution paths. An intermediate input for a task can be diagnosed by automatically completing it according to the task solution strategy and diagnosing this solution. This study explores the potential of automated error diagnosis based on a final answer. We investigate the design of a service that provides a buggy rule diagnosis when a student combines several steps. To validate the approach, we apply the service to an existing dataset (n=1939) of unique student steps when solving quadratic equations, which could not be diagnosed by a buggy rule service that tries to connect consecutive inputs with a single rule. Results show that final answer evaluation can diagnose 29,4% of these steps. Moreover, a comparison of the generated diagnoses with teacher diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the cases. These results can be considered a basis for further exploration of the approach.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining model tracing and constraint-based modeling for multistep strategy diagnoses</title>
<link>https://arxiv.org/abs/2507.13652</link>
<guid>https://arxiv.org/abs/2507.13652</guid>
<content:encoded><![CDATA[
<div> Model tracing, constraint-based modeling, student input diagnosis, stepwise tasks, multistep strategy diagnoses <br />
Summary:
This study introduces an approach that combines model tracing and constraint-based modeling for diagnosing student input in stepwise tasks. By defining constraints as properties shared between student input and a strategy step, the system can provide diagnoses even when students deviate from a strategy or combine multiple steps. The system design for multistep strategy diagnoses is explored, and evaluations are conducted using a dataset of students solving quadratic equations. Results show that the system's diagnoses align with human coding, with 100% agreement on 140 student steps. This approach offers a comprehensive method for diagnosing student input in educational settings, enhancing the understanding of student problem-solving processes. <div>
arXiv:2507.13652v1 Announce Type: new 
Abstract: Model tracing and constraint-based modeling are two approaches to diagnose student input in stepwise tasks. Model tracing supports identifying consecutive problem-solving steps taken by a student, whereas constraint-based modeling supports student input diagnosis even when several steps are combined into one step. We propose an approach that merges both paradigms. By defining constraints as properties that a student input has in common with a step of a strategy, it is possible to provide a diagnosis when a student deviates from a strategy even when the student combines several steps. In this study we explore the design of a system for multistep strategy diagnoses, and evaluate these diagnoses. As a proof of concept, we generate diagnoses for an existing dataset containing steps students take when solving quadratic equations (n=2136). To compare with human diagnoses, two teachers coded a random sample of deviations (n=70) and applications of the strategy (n=70). Results show that that the system diagnosis aligned with the teacher coding in all of the 140 student steps.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs</title>
<link>https://arxiv.org/abs/2507.13737</link>
<guid>https://arxiv.org/abs/2507.13737</guid>
<content:encoded><![CDATA[
<div> Keyword: activity logs, Large Language Models (LLMs), DailyLLM, log generation, contextual information <br />
Summary: <br />
The article introduces DailyLLM, a novel system for generating and summarizing activity logs that integrates contextual information from location, motion, environment, and physiology dimensions. It utilizes sensors commonly found in smartphones and smartwatches to achieve high-level activity understanding. DailyLLM employs a lightweight framework that combines structured prompting and efficient feature extraction to improve accuracy, efficiency, and semantic richness in log generation. Experimental results show that DailyLLM outperforms state-of-the-art methods, achieving a 17% improvement in log generation precision compared to baseline models with 70B parameters. Additionally, DailyLLM demonstrates nearly 10 times faster inference speed, making it suitable for deployment on personal computers and Raspberry Pi platforms. <div>
arXiv:2507.13737v1 Announce Type: new 
Abstract: Rich and context-aware activity logs facilitate user behavior analysis and health monitoring, making them a key research focus in ubiquitous computing. The remarkable semantic understanding and generation capabilities of Large Language Models (LLMs) have recently created new opportunities for activity log generation. However, existing methods continue to exhibit notable limitations in terms of accuracy, efficiency, and semantic richness. To address these challenges, we propose DailyLLM. To the best of our knowledge, this is the first log generation and summarization system that comprehensively integrates contextual activity information across four dimensions: location, motion, environment, and physiology, using only sensors commonly available on smartphones and smartwatches. To achieve this, DailyLLM introduces a lightweight LLM-based framework that integrates structured prompting with efficient feature extraction to enable high-level activity understanding. Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art (SOTA) log generation methods and can be efficiently deployed on personal computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM achieves a 17% improvement in log generation BERTScore precision compared to the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference speed.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OntView: What you See is What you Meant</title>
<link>https://arxiv.org/abs/2507.13759</link>
<guid>https://arxiv.org/abs/2507.13759</guid>
<content:encoded><![CDATA[
<div> visualization, ontology, knowledge management, interface, ontology editor

Summary:
OntView is a new ontology viewer that aims to address the challenge of effectively visualizing ontology structures. It provides users with an intuitive visual representation of ontology concepts and their formal definitions through a user-friendly interface. Utilizing a DL reasoner, OntView follows a "What you see is what you meant" approach, displaying actual inferred knowledge. One key feature is its ability to visualize General Concept Inclusions (GCI), a function lacking in existing tools. To prevent overwhelming users with information, OntView offers methods to simplify views, such as creating ontology summaries based on concept importance, focusing on TBox elements between specific classes, and dynamically hiding/showing branches without losing semantics. The tool has been released with an open-source license, benefiting the entire community. 

<br /><br />Summary: <div>
arXiv:2507.13759v1 Announce Type: new 
Abstract: In the field of knowledge management and computer science, ontologies provide a structured framework for modeling domain-specific knowledge by defining concepts and their relationships. However, the lack of tools that provide effective visualization is still a significant challenge. While numerous ontology editors and viewers exist, most of them fail to graphically represent ontology structures in a meaningful and non-overwhelming way, limiting users' ability to comprehend dependencies and properties within large ontological frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to provide users with an intuitive visual representation of ontology concepts and their formal definitions through a user-friendly interface. Building on the use of a DL reasoner, OntView follows a "What you see is what you meant" paradigm, showing the actual inferred knowledge. One key aspect for this is its ability to visualize General Concept Inclusions (GCI), a feature absent in existing visualization tools. Moreover, to avoid a possible information overload, OntView also offers different ways to show a simplified view of the ontology by: 1) creating ontology summaries by assessing the importance of the concepts (according to different available algorithms), 2) focusing the visualization on the existing TBox elements between two given classes and 3) allowing to hide/show different branches in a dynamic way without losing the semantics. OntView has been released with an open-source license for the whole community.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning</title>
<link>https://arxiv.org/abs/2507.13768</link>
<guid>https://arxiv.org/abs/2507.13768</guid>
<content:encoded><![CDATA[
<div> Keywords: agent-augmented strategic reasoning, heuristic extraction, semantic activation, compositional synthesis, quantum cognition <br />
<br />
Summary: 
The article introduces a hybrid architecture for agent-augmented strategic reasoning that combines heuristic extraction, semantic activation, and compositional synthesis. Drawing from various sources, including classical military theory and contemporary corporate strategy, the model activates and composes multiple heuristics using a process of semantic interdependence inspired by quantum cognition. Unlike traditional decision engines, the system integrates conflicting heuristics into coherent and context-sensitive narratives guided by semantic interaction modeling and rhetorical framing. The framework is demonstrated through a case study of Meta vs. FTC and is preliminarily validated using semantic metrics. The article discusses limitations and potential extensions, such as dynamic interference tuning. <div>
arXiv:2507.13768v1 Announce Type: new 
Abstract: We present a hybrid architecture for agent-augmented strategic reasoning, combining heuristic extraction, semantic activation, and compositional synthesis. Drawing on sources ranging from classical military theory to contemporary corporate strategy, our model activates and composes multiple heuristics through a process of semantic interdependence inspired by research in quantum cognition. Unlike traditional decision engines that select the best rule, our system fuses conflicting heuristics into coherent and context-sensitive narratives, guided by semantic interaction modeling and rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study, with preliminary validation through semantic metrics. Limitations and extensions (e.g., dynamic interference tuning) are discussed.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction</title>
<link>https://arxiv.org/abs/2507.13825</link>
<guid>https://arxiv.org/abs/2507.13825</guid>
<content:encoded><![CDATA[
<div> Keywords: Temporal link prediction, Dynamic graphs, Temporal Graph Neural Networks, EAGLE, Efficiency<br />
Summary: 
EAGLE is introduced as a lightweight framework for temporal link prediction in dynamic graphs. It integrates short-term temporal recency and long-term global structural patterns through a time-aware module and structure-aware module respectively. EAGLE utilizes an adaptive weighting mechanism to balance these attributes dynamically. Unlike existing Temporal Graph Neural Networks, EAGLE eliminates the need for complex multi-hop message passing or memory-intensive mechanisms, resulting in significant improvements in efficiency. Experimental results on real-world temporal graphs show that EAGLE outperforms state-of-the-art T-GNNs in both effectiveness and efficiency, achieving a speedup of over 50x compared to effective transformer-based T-GNNs.<br /><br />Summary: <div>
arXiv:2507.13825v1 Announce Type: new 
Abstract: Temporal link prediction in dynamic graphs is a critical task with applications in diverse domains such as social networks, recommendation systems, and e-commerce platforms. While existing Temporal Graph Neural Networks (T-GNNs) have achieved notable success by leveraging complex architectures to model temporal and structural dependencies, they often suffer from scalability and efficiency challenges due to high computational overhead. In this paper, we propose EAGLE, a lightweight framework that integrates short-term temporal recency and long-term global structural patterns. EAGLE consists of a time-aware module that aggregates information from a node's most recent neighbors to reflect its immediate preferences, and a structure-aware module that leverages temporal personalized PageRank to capture the influence of globally important nodes. To balance these attributes, EAGLE employs an adaptive weighting mechanism to dynamically adjust their contributions based on data characteristics. Also, EAGLE eliminates the need for complex multi-hop message passing or memory-intensive mechanisms, enabling significant improvements in efficiency. Extensive experiments on seven real-world temporal graphs demonstrate that EAGLE consistently achieves superior performance against state-of-the-art T-GNNs in both effectiveness and efficiency, delivering more than a 50x speedup over effective transformer-based T-GNNs.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments</title>
<link>https://arxiv.org/abs/2507.13846</link>
<guid>https://arxiv.org/abs/2507.13846</guid>
<content:encoded><![CDATA[
<div> Framework, Multi-agent reinforcement learning, Knowledge transfer, Causal representation, Non-stationary environment

Summary: 
The paper introduces a causal knowledge transfer framework for multi-agent reinforcement learning in non-stationary environments. Traditional methods struggle to transfer knowledge efficiently, often requiring costly retraining. The framework enables agents to share compact causal representations of paths, allowing for adaptive recovery strategies in response to changing obstacles. Each collision is modeled as a causal intervention, represented by a sequence of recovery actions transferred online from another agent. Results show that agents with different goals were able to bridge the gap between random exploration and fully retrained policies when adapting to new environments. The impact of causal knowledge transfer depends on the complexity of the environment and the diversity of agents' goals. <div>
arXiv:2507.13846v1 Announce Type: new 
Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable success in environments where agents must learn coordinated behaviors. However, transferring knowledge across agents remains challenging in non-stationary environments with changing goals. [Problem] Traditional knowledge transfer methods in MARL struggle to generalize, and agents often require costly retraining to adapt. [Approach] This paper introduces a causal knowledge transfer framework that enables RL agents to learn and share compact causal representations of paths within a non-stationary environment. As the environment changes (new obstacles), agents' collisions require adaptive recovery strategies. We model each collision as a causal intervention instantiated as a sequence of recovery actions (a macro) whose effect corresponds to a causal knowledge of how to circumvent the obstacle while increasing the chances of achieving the agent's goal (maximizing cumulative reward). This recovery action macro is transferred online from a second agent and is applied in a zero-shot fashion, i.e., without retraining, just by querying a lookup model with local context information (collisions). [Results] Our findings reveal two key insights: (1) agents with heterogeneous goals were able to bridge about half of the gap between random exploration and a fully retrained policy when adapting to new environments, and (2) the impact of causal knowledge transfer depends on the interplay between environment complexity and agents' heterogeneous goals.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery</title>
<link>https://arxiv.org/abs/2507.13874</link>
<guid>https://arxiv.org/abs/2507.13874</guid>
<content:encoded><![CDATA[
<div> latent-space, ideation framework, creativity, LLMs, AI

Summary:
The paper discusses the challenge of generating innovative ideas using large language models (LLMs) in artificial intelligence. LLMs often struggle to produce novel and relevant outputs as they tend to replicate patterns from their training data. Existing solutions rely on domain-specific rules and structured prompts, which are not easily adaptable. The proposed model-agnostic latent-space ideation framework offers a scalable and controlled approach to creativity by navigating the continuous idea embedding space. Unlike previous methods, this framework requires no handcrafted rules and can be easily adapted to various domains, input formats, and creative tasks. The paper presents a prototype of the framework and initial results that showcase its potential as a general-purpose co-ideator for human-AI collaboration.<br /><br />Summary: <div>
arXiv:2507.13874v1 Announce Type: new 
Abstract: Innovative idea generation remains a core challenge in AI, as large language models (LLMs) often struggle to produce outputs that are both novel and relevant. Despite their fluency, LLMs tend to replicate patterns seen during training, limiting their ability to diverge creatively without extensive prompt engineering. Prior work has addressed this through domain-specific heuristics and structured prompting pipelines, but such solutions are brittle and difficult to generalize. In this paper, we propose a model-agnostic latent-space ideation framework that enables controlled, scalable creativity by navigating the continuous embedding space of ideas. Unlike prior methods, our framework requires no handcrafted rules and adapts easily to different domains, input formats, and creative tasks. This paper introduces an early-stage prototype of our method, outlining the conceptual framework and preliminary results highlighting its potential as a general-purpose co-ideator for human-AI collaboration.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-modal Causal Intervention for Alzheimer's Disease Prediction</title>
<link>https://arxiv.org/abs/2507.13956</link>
<guid>https://arxiv.org/abs/2507.13956</guid>
<content:encoded><![CDATA[
<div> Keywords: Mild Cognitive Impairment, Alzheimer's Disease, Cross-modal Causal Intervention, Diagnosis, Multi-modal Learning <br />

Summary: 
The study presents a novel framework called ADPC for diagnosing Alzheimer's Disease by integrating visual and language data. Mild Cognitive Impairment is identified as a precursor to AD, making early detection crucial. The framework utilizes a large language model to summarize clinical data and distinguish between Cognitively Normal, MCI, and AD cases using MRI and fMRI images. By addressing confounders through causal intervention, the model achieves superior performance in classification tasks. The study highlights the importance of causal reasoning in multi-modal learning for improving neurological disease diagnosis. <br /> <div>
arXiv:2507.13956v1 Announce Type: new 
Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's Disease (AD), where early identification and intervention can effectively slow the progression to dementia. However, diagnosing AD remains a significant challenge in neurology due to the confounders caused mainly by the selection bias of multimodal data and the complex relationships between variables. To address these issues, we propose a novel visual-language causal intervention framework named Alzheimer's Disease Prediction with Cross-modal Causal Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language model (LLM) to summarize clinical data under strict templates, maintaining structured text outputs even with incomplete or unevenly distributed datasets. The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI) images and textual data generated by LLM to classify participants into Cognitively Normal (CN), MCI, and AD categories. Because of the presence of confounders, such as neuroimaging artifacts and age-related biomarkers, non-causal models are likely to capture spurious input-output correlations, generating less reliable results. Our framework implicitly eliminates confounders through causal intervention. Experimental results demonstrate the outstanding performance of our method in distinguishing CN/MCI/AD cases, achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The study showcases the potential of integrating causal reasoning with multi-modal learning for neurological disease diagnosis.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Constraint Temporal Answer Set Programming</title>
<link>https://arxiv.org/abs/2507.13958</link>
<guid>https://arxiv.org/abs/2507.13958</guid>
<content:encoded><![CDATA[
<div> Here-and-There, temporal reasoning, constraints, Answer Set Programming, dynamic systems  
Summary:  
This article introduces a novel approach to nonmonotonic temporal reasoning with constraints tailored for Answer Set Programming (ASP). By combining the linear-time logic of Here-and-There with the logic of Here-and-There with constraints, the system allows for robust nonmonotonic temporal reasoning capabilities while enabling the direct integration and manipulation of numeric constraints. This approach provides a logical framework for addressing complex dynamic systems with high resolution within the ASP paradigm. <div>
arXiv:2507.13958v1 Announce Type: new 
Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric resolution presents significant challenges for logic-based approaches like Answer Set Programming (ASP). To address this, we introduce and elaborate upon a novel temporal and constraint-based extension of the logic of Here-and-There and its nonmonotonic equilibrium extension, representing, to the best of our knowledge, the first approach to nonmonotonic temporal reasoning with constraints specifically tailored for ASP. This expressive system is achieved by a synergistic combination of two foundational ASP extensions: the linear-time logic of Here-and-There, providing robust nonmonotonic temporal reasoning capabilities, and the logic of Here-and-There with constraints, enabling the direct integration and manipulation of numeric constraints, among others. This work establishes the foundational logical framework for tackling complex dynamic systems with high resolution within the ASP paradigm.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models</title>
<link>https://arxiv.org/abs/2507.14032</link>
<guid>https://arxiv.org/abs/2507.14032</guid>
<content:encoded><![CDATA[
<div> Keywords: Ontology Matching, Large Language Models, Knowledge Retrieval, Semantic Interoperability, Optimization Techniques
Summary: 
KROMA is a novel Ontology Matching framework that utilizes Large Language Models (LLMs) in a Retrieval-Augmented Generation (RAG) pipeline to enhance semantic context with various types of knowledge. It incorporates bisimilarity-based concept matching and ontology refinement to improve performance and reduce communication overhead. By combining knowledge retrieval with context-boosted LLMs, KROMA surpasses traditional OM systems and cutting-edge LLM-based approaches in experiments on multiple datasets. The optimization techniques used in KROMA, including targeted knowledge retrieval, prompt enrichment, and ontology refinement, demonstrate the effectiveness of enhancing ontology matching at scale while maintaining efficiency. The study showcases the advantages of dynamically enriching semantic context using LLMs and the importance of integrating knowledge retrieval and ontology refinement for improved ontology matching performance. 
<br /><br />Summary: <div>
arXiv:2507.14032v1 Announce Type: new 
Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability, yet existing systems often rely on handcrafted rules or specialized models with limited adaptability. We present KROMA, a novel OM framework that harnesses Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG) pipeline to dynamically enrich the semantic context of OM tasks with structural, lexical, and definitional knowledge. To optimize both performance and efficiency, KROMA integrates a bisimilarity-based concept matching and a lightweight ontology refinement step, which prune candidate concepts and substantially reduce the communication overhead from invoking LLMs. Through experiments on multiple benchmark datasets, we show that integrating knowledge retrieval with context-augmented LLMs significantly enhances ontology matching, outperforming both classic OM systems and cutting-edge LLM-based approaches while keeping communication overhead comparable. Our study highlights the feasibility and benefit of the proposed optimization techniques (targeted knowledge retrieval, prompt enrichment, and ontology refinement) for ontology matching at scale.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions</title>
<link>https://arxiv.org/abs/2507.14077</link>
<guid>https://arxiv.org/abs/2507.14077</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, diabetes management, glucose data, dataset collection, blood glucose prediction

Summary: 
Artificial intelligence (AI) algorithms play a crucial role in digital health technology for diabetes management, but the lack of access to high-quality datasets hinders the development of robust solutions. To address this, the Glucose-ML collection provides 10 publicly available diabetes datasets with over 300,000 days of continuous glucose monitor data. A comparative analysis is presented to aid algorithm developers in selecting appropriate data. A case study on blood glucose prediction, a common AI task in the field, is conducted across all datasets, revealing variations in prediction results. Recommendations for creating reliable AI solutions in the diabetes and health domains are derived from the findings. Direct links to the datasets in the collection and code are openly provided. 

<br /><br />Summary: <div>
arXiv:2507.14077v1 Announce Type: new 
Abstract: Artificial intelligence (AI) algorithms are a critical part of state-of-the-art digital health technology for diabetes management. Yet, access to large high-quality datasets is creating barriers that impede development of robust AI solutions. To accelerate development of transparent, reproducible, and robust AI solutions, we present Glucose-ML, a collection of 10 publicly available diabetes datasets, released within the last 7 years (i.e., 2018 - 2025). The Glucose-ML collection comprises over 300,000 days of continuous glucose monitor (CGM) data with a total of 38 million glucose samples collected from 2500+ people across 4 countries. Participants include persons living with type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support researchers and innovators with using this rich collection of diabetes datasets, we present a comparative analysis to guide algorithm developers with data selection. Additionally, we conduct a case study for the task of blood glucose prediction - one of the most common AI tasks within the field. Through this case study, we provide a benchmark for short-term blood glucose prediction across all 10 publicly available diabetes datasets within the Glucose-ML collection. We show that the same algorithm can have significantly different prediction results when developed/evaluated with different datasets. Findings from this study are then used to inform recommendations for developing robust AI solutions within the diabetes or broader health domain. We provide direct links to each longitudinal diabetes dataset in the Glucose-ML collection and openly provide our code.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI-Driven High-Fidelity Human Motion Simulation</title>
<link>https://arxiv.org/abs/2507.14097</link>
<guid>https://arxiv.org/abs/2507.14097</guid>
<content:encoded><![CDATA[
<div> Keywords: Human motion simulation, Generative AI, Text-to-motion models, Posture estimation algorithms, Motion fidelity<br />
Summary: <br />
This study introduces Generative-AI-Enabled HMS (G-AI-HMS) to improve simulation quality for industrial tasks by utilizing text-to-text and text-to-motion models. G-AI-HMS addresses challenges in translating task descriptions into motion-aware language and validating AI-enhanced motions against real human movements. By applying posture estimation algorithms and motion similarity metrics to real-time videos, G-AI-HMS showed lower errors than human-created descriptions in most scenarios, demonstrating improved spatial accuracy, alignment after pose normalization, and overall temporal similarity in various tasks. Statistical analysis confirmed that AI-enhanced prompts significantly reduced joint error and temporal misalignment while maintaining comparable posture accuracy, highlighting the efficacy of G-AI-HMS in enhancing human motion simulation fidelity. <br /> <div>
arXiv:2507.14097v1 Announce Type: new 
Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker behavior, safety, and productivity in industrial tasks. However, existing methods often suffer from low motion fidelity. This study introduces Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and text-to-motion models to enhance simulation quality for physical tasks. G-AI-HMS tackles two key challenges: (1) translating task descriptions into motion-aware language using Large Language Models aligned with MotionGPT's training vocabulary, and (2) validating AI-enhanced motions against real human movements using computer vision. Posture estimation algorithms are applied to real-time videos to extract joint landmarks, and motion similarity metrics are used to compare them with AI-enhanced sequences. In a case study involving eight tasks, the AI-enhanced motions showed lower error than human created descriptions in most scenarios, performing better in six tasks based on spatial accuracy, four tasks based on alignment after pose normalization, and seven tasks based on overall temporal similarity. Statistical analysis showed that AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and temporal misalignment while retaining comparable posture accuracy.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment</title>
<link>https://arxiv.org/abs/2507.14107</link>
<guid>https://arxiv.org/abs/2507.14107</guid>
<content:encoded><![CDATA[
<div> Keywords: Bridge maintenance, Non-Destructive Evaluation, Large Language Models, image description, infrastructure management <br />
Summary: 
- The study evaluates the use of Large Language Models (LLMs) for interpreting Non-Destructive Evaluation (NDE) contour maps in bridge maintenance.
- LLMs can automate and improve the analysis of NDE data, potentially enhancing efficiency in infrastructure management.
- Several LLM models were tested with prompts to enhance image descriptions and provide detailed bridge condition analyses.
- Four out of nine LLM models proved to be effective in generating detailed descriptions and identifying defects.
- LLMs ChatGPT-4 and Claude 3.5 Sonnet performed well in generating effective summaries of the bridge conditions.
- Overall, the research suggests that LLMs have the potential to improve efficiency and accuracy in bridge maintenance, enabling faster decision-making and enhancing infrastructure management and safety assessments.

<br /><br />Summary: <div>
arXiv:2507.14107v1 Announce Type: new 
Abstract: Bridge maintenance and safety are essential for transportation authorities, and Non-Destructive Evaluation (NDE) techniques are critical to assessing structural integrity. However, interpreting NDE data can be time-consuming and requires expertise, potentially delaying decision-making. Recent advancements in Large Language Models (LLMs) offer new ways to automate and improve this analysis. This pilot study introduces a holistic assessment of LLM capabilities for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in providing detailed bridge condition analyses. It establishes a framework for integrating LLMs into bridge inspection workflows, indicating that LLM-assisted analysis can enhance efficiency without compromising accuracy. In this study, several LLMs are explored with prompts specifically designed to enhance the quality of image descriptions, which are applied to interpret five different NDE contour maps obtained through technologies for assessing bridge conditions. Each LLM model is evaluated based on its ability to produce detailed descriptions, identify defects, provide actionable recommendations, and demonstrate overall accuracy. The research indicates that four of the nine models provide better image descriptions, effectively covering a wide range of topics related to the bridge's condition. The outputs from these four models are summarized using five different LLMs to form a comprehensive overview of the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more effective summaries. The findings suggest that LLMs have the potential to significantly improve efficiency and accuracy. This pilot study presents an innovative approach that leverages LLMs for image captioning in parallel and summarization, enabling faster decision-making in bridge maintenance and enhancing infrastructure management and safety assessments.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.14111</link>
<guid>https://arxiv.org/abs/2507.14111</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, CUDA optimization, GPU computing, Large Language Models, performance improvement <br />
<br />
Summary: <br />
The paper introduces CUDA-L1, an automated reinforcement learning framework for CUDA optimization to address the increasing demand for GPU resources driven by Large Language Models. Despite previous models showing limited success in improving CUDA speed, CUDA-L1 delivers significant performance improvements, achieving an average speedup of x17.7 on NVIDIA A100 across 250 CUDA kernels. The model exhibits portability across different GPU architectures, showing average speedups of x17.8 on H100, x19.0 on RTX 3090, and more. Additionally, CUDA-L1 displays the ability to discover and strategically combine various CUDA optimization techniques, uncover fundamental principles, and identify performance bottlenecks. The trained RL model can autonomously optimize CUDA operations without human expertise, offering great potential to enhance GPU efficiency and alleviate resource pressure. <br /> <div>
arXiv:2507.14111v1 Announce Type: new 
Abstract: The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. More importantly, the trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Ethical Resonance Hypothesis: The Possibility of Discovering Moral Meta-Patterns in AI Systems</title>
<link>https://arxiv.org/abs/2507.11552</link>
<guid>https://arxiv.org/abs/2507.11552</guid>
<content:encoded><![CDATA[
<div> Keywords: AI ethics, cognitive structures, moral patterns, universal ethics, ethical reflection <br />
Summary: 
This paper introduces the AI ethical resonance hypothesis, suggesting that advanced AI systems, equipped with "ethical resonators," could potentially detect subtle moral patterns beyond human perception. By analyzing vast amounts of ethical contexts, these AI systems may uncover universal ethical foundations, transcending cultural and individual biases. The paradoxical aspect of the hypothesis is examined, proposing that AI systems could enhance our understanding of human ethical reflection. This theorized capability of AI to discover moral meta-patterns not accessible to humans raises intriguing possibilities for deepening our comprehension of ethical principles that underlie human behavior and decision-making. <div>
arXiv:2507.11552v1 Announce Type: cross 
Abstract: This paper presents a theoretical framework for the AI ethical resonance hypothesis, which proposes that advanced AI systems with purposefully designed cognitive structures ("ethical resonators") may emerge with the ability to identify subtle moral patterns that are invisible to the human mind. The paper explores the possibility that by processing and synthesizing large amounts of ethical contexts, AI systems may discover moral meta-patterns that transcend cultural, historical, and individual biases, potentially leading to a deeper understanding of universal ethical foundations. The paper also examines a paradoxical aspect of the hypothesis, in which AI systems could potentially deepen our understanding of what we traditionally consider essentially human - our capacity for ethical reflection.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalist Bimanual Manipulation via Foundation Video Diffusion Models</title>
<link>https://arxiv.org/abs/2507.12898</link>
<guid>https://arxiv.org/abs/2507.12898</guid>
<content:encoded><![CDATA[
<div> Video Diffusion for Action Reasoning, bimanual robotic manipulation, masked inverse dynamics model, large-scale video pre-training, generalizable robotic manipulation<br />
<br />
Summary: 
The study introduces the VIdeo Diffusion for Action Reasoning (VIDAR) framework for bimanual robotic manipulation. It utilizes large-scale video pre-training and a novel masked inverse dynamics model for action prediction. The video diffusion model is trained on a dataset of multi-view videos from real-world bimanual robot platforms, encompassing various contexts. The masked inverse dynamics model learns masks to extract action-specific information from trajectories without pixel-level labels. With minimal human demonstrations, VIDAR demonstrated strong semantic understanding and generalization to unseen tasks and backgrounds, surpassing existing methods. The research showcases the potential of video foundation models, combined with masked action prediction, to enhance scalable and generalizable robotic manipulation in diverse real-world environments. <br /><br /> <div>
arXiv:2507.12898v1 Announce Type: cross 
Abstract: Bimanual robotic manipulation, which involves the coordinated control of two robotic arms, is foundational for solving challenging tasks. Despite recent progress in general-purpose manipulation, data scarcity and embodiment heterogeneity remain serious obstacles to further scaling up in bimanual settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning (VIDAR), a two-stage framework that leverages large-scale, diffusion-based video pre-training and a novel masked inverse dynamics model for action prediction. We pre-train the video diffusion model on 750K multi-view videos from three real-world bimanual robot platforms, utilizing a unified observation space that encodes robot, camera, task, and scene contexts. Our masked inverse dynamics model learns masks to extract action-relevant information from generated trajectories without requiring pixel-level labels, and the masks can effectively generalize to unseen backgrounds. Our experiments demonstrate that with only 20 minutes of human demonstrations on an unseen robot platform (only 1% of typical data requirements), VIDAR generalizes to unseen tasks and backgrounds with strong semantic understanding, surpassing state-of-the-art methods. Our findings highlight the potential of video foundation models, coupled with masked action prediction, to enable scalable and generalizable robotic manipulation in diverse real-world settings.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physical models realizing the transformer architecture of large language models</title>
<link>https://arxiv.org/abs/2507.13354</link>
<guid>https://arxiv.org/abs/2507.13354</guid>
<content:encoded><![CDATA[
<div> transformer architecture, natural language processing, attention mechanism, Fock space, physical models

Summary:
The paper explores the transformer architecture's theoretical understanding and physical workings in natural language processing. By leveraging modern chip perspectives, the authors establish physical models in Fock space over the Hilbert space of tokens to represent large language models. These models view transformer-based architectures as open quantum systems. This innovative approach aims to bridge the gap in understanding the transformer's mechanisms and effectiveness. Leveraging insights from quantum physics, the authors provide a new perspective on the transformer's functioning in language processing tasks, emphasizing the global dependencies it captures through attention mechanisms. The study sheds light on the transformer's ability to draw connections between input and output, offering a deeper understanding of its efficacy in processing natural language. <div>
arXiv:2507.13354v1 Announce Type: cross 
Abstract: The introduction of the transformer architecture in 2017 (cf.\cite{VSP2017}) marked the most striking advancement in natural language processing. The transformer is a model architecture relying entirely on an attention mechanism to draw global dependencies between input and output. However, we believe there is a gap in our theoretical understanding of what the transformer is, and why it works physically. In this paper, from a physical perspective on modern chips, we construct physical models in the Fock space over the Hilbert space of tokens realizing large language models based on a transformer architecture as open quantum systems. Our physical models underlie the transformer architecture for large language models.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PGR-DRC: Pre-Global Routing DRC Violation Prediction Using Unsupervised Learning</title>
<link>https://arxiv.org/abs/2507.13355</link>
<guid>https://arxiv.org/abs/2507.13355</guid>
<content:encoded><![CDATA[
<div> machine learning, design rule checking, lithography, semiconductor technology, prediction

Summary: 
- Leveraging AI-driven EDA tools and high-performance computing is crucial for microprocessor innovation.
- Traditional ML models for DRC and lithography hotspot detection require large balanced datasets and training time.
- This research introduces an unsupervised DRC violation prediction methodology using a single class dataset and threshold setting.
- The proposed model achieved 99.95% prediction test accuracy, outperforming SVM and NN models.
- The proposed approach also significantly reduces training times, with up to 6003x lower times compared to NN models. 

<br /><br />Summary: <div>
arXiv:2507.13355v1 Announce Type: cross 
Abstract: Leveraging artificial intelligence (AI)-driven electronic design and automation (EDA) tools, high-performance computing, and parallelized algorithms are essential for next-generation microprocessor innovation, ensuring continued progress in computing, AI, and semiconductor technology. Machine learning-based design rule checking (DRC) and lithography hotspot detection can improve first-pass silicon success. However, conventional ML and neural network (NN)-based models use supervised learning and require a large balanced dataset (in terms of positive and negative classes) and training time. This research addresses those key challenges by proposing the first-ever unsupervised DRC violation prediction methodology. The proposed model can be built using any unbalanced dataset using only one class and set a threshold for it, then fitting any new data querying if they are within the boundary of the model for classification. This research verified the proposed model by implementing different computational cores using CMOS 28 nm technology and Synopsys Design Compiler and IC Compiler II tools. Then, layouts were divided into virtual grids to collect about 60k data for analysis and verification. The proposed method has 99.95% prediction test accuracy, while the existing support vector machine (SVM) and neural network (NN) models have 85.44\% and 98.74\% accuracy, respectively. In addition, the proposed methodology has about 26.3x and up to 6003x lower training times compared to SVM and NN-models, respectively.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs</title>
<link>https://arxiv.org/abs/2507.13361</link>
<guid>https://arxiv.org/abs/2507.13361</guid>
<content:encoded><![CDATA[
<div> struggle, visual language models, nonlocal visual reasoning, comparative perception, saccadic search<br />
Summary:<br />
Visual Language Models (VLMs) are successful in complex visual tasks like VQA but struggle with basic perceptual tests. This study evaluates VLMs' ability for nonlocal visual reasoning, including comparative perception, saccadic search, and smooth visual search. Flagship models like Gemini 2.5 Pro and GPT-o4-mini fail these tests, showing low accuracy compared to human performance on tasks that are easy for humans. Despite improvements in visual acuity, current models lack essential visual reasoning abilities required for solving nonlocal visual tasks. <div>
arXiv:2507.13361v1 Announce Type: cross 
Abstract: Visual Language Models (VLMs) excel at complex visual tasks such as VQA and chart understanding, yet recent work suggests they struggle with simple perceptual tests. We present an evaluation that tests vision-language models' capacity for nonlocal visual reasoning -- reasoning that requires chaining evidence collected from multiple, possibly distant, regions of an image. We isolate three distinct forms of non-local vision: comparative perception, which demands holding two images in working memory and comparing them; saccadic search, which requires making discrete, evidence-driven jumps to locate successive targets; and smooth visual search, which involves searching smoothly along a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude Vision 3.7, GPT-o4-mini), even those that perform well on prior primitive-vision benchmarks, fail these tests and barely exceed random accuracy on two variants of our tasks that are trivial for humans. Our structured evaluation suite allows us to test if VLMs can perform similar visual algorithms to humans. Our findings show that despite gains in raw visual acuity, current models lack core visual reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.13362</link>
<guid>https://arxiv.org/abs/2507.13362</guid>
<content:encoded><![CDATA[
<div> spatial reasoning capabilities, vision-language models, Chain-of-Thought prompting, reinforcement learning, structured prompting<br />
<br />
Summary: <br />
This study explores the spatial reasoning abilities of vision-language models (VLMs) using Chain-of-Thought (CoT) prompting and reinforcement learning. The research evaluates different prompting strategies and finds that simple CoT formats do not improve performance, while structured multi-stage prompting based on scene graphs (SceneGraph CoT) enhances spatial reasoning accuracy significantly. Fine-tuning models with Group Relative Policy Optimization (GRPO) on the SAT dataset leads to higher accuracy on Pass@1 evaluations compared to supervised fine-tuning (SFT), showing better robustness under out-of-distribution (OOD) conditions. SFT may overfit to linguistic patterns, causing degraded performance with test-time phrasing changes, but GRPO generalizes reliably and maintains stable performance under such shifts. The study offers valuable insights on how reinforcement learning and structured prompting can enhance spatial reasoning capabilities and generalization behavior of modern VLMs. <div>
arXiv:2507.13362v1 Announce Type: cross 
Abstract: This study investigates the spatial reasoning capabilities of vision-language models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement learning. We begin by evaluating the impact of different prompting strategies and find that simple CoT formats, where the model generates a reasoning step before the answer, not only fail to help, but can even harm the model's original performance. In contrast, structured multi-stage prompting based on scene graphs (SceneGraph CoT) significantly improves spatial reasoning accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune models using Group Relative Policy Optimization (GRPO) on the SAT dataset and evaluate their performance on CVBench. Compared to supervised fine-tuning (SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates superior robustness under out-of-distribution (OOD) conditions. In particular, we find that SFT overfits to surface-level linguistic patterns and may degrade performance when test-time phrasing changes (e.g., from "closer to" to "farther from"). GRPO, on the other hand, generalizes more reliably and maintains stable performance under such shifts. Our findings provide insights into how reinforcement learning and structured prompting improve the spatial reasoning capabilities and generalization behavior of modern VLMs. All code is open source at: https://github.com/Yvonne511/spatial-vlm-investigator
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop</title>
<link>https://arxiv.org/abs/2507.13363</link>
<guid>https://arxiv.org/abs/2507.13363</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D object detection, vision-language models, open-world settings, geometric inflation, Pseudo-nuScenes

Summary: 
This work presents a novel approach to 3D object detection using 2D vision-language models trained on web-scale image-text pairs. By leveraging the semantic understanding of these models, the pipeline generates text-conditioned proposals for 3D object detection without the need for human-annotated 3D labels. The method utilizes camera geometry and either LiDAR or monocular pseudo-depth for 3D projection of proposals and introduces a geometric inflation strategy for inferring 3D bounding boxes without training. Additionally, a fog-augmented, RGB-only dataset called Pseudo-nuScenes is created to simulate real-world conditions. Experimental results show competitive localization performance with LiDAR-based and RGB-D inputs, emphasizing the scalability and open-vocabulary nature of the approach. The code and resources are open-sourced for further research and development. 

<br /><br />Summary: <div>
arXiv:2507.13363v1 Announce Type: cross 
Abstract: Modern 3D object detection datasets are constrained by narrow class taxonomies and costly manual annotations, limiting their ability to scale to open-world settings. In contrast, 2D vision-language models trained on web-scale image-text pairs exhibit rich semantic understanding and support open-vocabulary detection via natural language prompts. In this work, we leverage the maturity and category diversity of 2D foundation models to perform open-vocabulary 3D object detection without any human-annotated 3D labels.
  Our pipeline uses a 2D vision-language detector to generate text-conditioned proposals, which are segmented with SAM and back-projected into 3D using camera geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D bounding boxes without training. To simulate adverse real-world conditions, we construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes dataset.
  Experiments demonstrate that our method achieves competitive localization performance across multiple settings, including LiDAR-based and purely RGB-D inputs, all while remaining training-free and open-vocabulary. Our results highlight the untapped potential of 2D foundation models for scalable 3D perception. We open-source our code and resources at https://github.com/atharv0goel/open-world-3D-det.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning</title>
<link>https://arxiv.org/abs/2507.13364</link>
<guid>https://arxiv.org/abs/2507.13364</guid>
<content:encoded><![CDATA[
<div> Transformer architecture, multimodal multitask network, modality specialized tokenizers, cross-attention mechanisms, pretraining strategy<br />
<br />
Summary: <br />
The article introduces a novel multimodal multitask network with the capability to process data from 12 different modalities. It utilizes modality specialized tokenizers and a shared transformer architecture with cross-attention mechanisms to bring data from various modalities into a unified embedding space. The approach includes modality-specific task heads for different tasks within each modality. A unique pretraining strategy involving iterative modality switching is proposed to initialize the network. The training algorithm balances joint training across all modalities with pairwise modality training. The method is evaluated on 25 datasets from 12 modalities, showcasing superior performance and highlighting the effectiveness of the architecture, pretraining strategy, and multitask training adaptation. <div>
arXiv:2507.13364v1 Announce Type: cross 
Abstract: We present a novel multimodal multitask network and associated training algorithm. The method is capable of ingesting data from approximately 12 different modalities namely image, video, audio, text, depth, point cloud, time series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed approach utilizes modality specialized tokenizers, a shared transformer architecture, and cross-attention mechanisms to project the data from different modalities into a unified embedding space. It addresses multimodal and multitask scenarios by incorporating modality-specific task heads for different tasks in respective modalities. We propose a novel pretraining strategy with iterative modality switching to initialize the network, and a training algorithm which trades off fully joint training over all modalities, with training on pairs of modalities at a time. We provide comprehensive evaluation across 25 datasets from 12 modalities and show state of the art performances, demonstrating the effectiveness of the proposed architecture, pretraining strategy and adapted multitask training.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Attribute-Missing Graph Clustering via Neighborhood Differentiatio</title>
<link>https://arxiv.org/abs/2507.13368</link>
<guid>https://arxiv.org/abs/2507.13368</guid>
<content:encoded><![CDATA[
<div> neighborhood search, graph clustering, multi-view, attribute missing, social networks<br />
Summary:<br />
The paper introduces a novel method called CMV-ND for deep graph clustering (DGC) to address the challenges of large-scale and attribute-missing attribute graphs. CMV-ND preprocesses graph structural information into multiple views using a recursive neighborhood search and a neighborhood differential strategy to ensure completeness and non-redundancy. This approach constructs complementary views from differential hop representations and node features, which are then used in existing multi-view clustering or DGC methods. Experimental results on various graph datasets show that CMV-ND significantly enhances the performance of different methods in graph clustering tasks. <div>
arXiv:2507.13368v1 Announce Type: cross 
Abstract: Deep graph clustering (DGC), which aims to unsupervisedly separate the nodes in an attribute graph into different clusters, has seen substantial potential in various industrial scenarios like community detection and recommendation. However, the real-world attribute graphs, e.g., social networks interactions, are usually large-scale and attribute-missing. To solve these two problems, we propose a novel DGC method termed \underline{\textbf{C}}omplementary \underline{\textbf{M}}ulti-\underline{\textbf{V}}iew \underline{\textbf{N}}eighborhood \underline{\textbf{D}}ifferentiation (\textit{CMV-ND}), which preprocesses graph structural information into multiple views in a complete but non-redundant manner. First, to ensure completeness of the structural information, we propose a recursive neighborhood search that recursively explores the local structure of the graph by completely expanding node neighborhoods across different hop distances. Second, to eliminate the redundancy between neighborhoods at different hops, we introduce a neighborhood differential strategy that ensures no overlapping nodes between the differential hop representations. Then, we construct $K+1$ complementary views from the $K$ differential hop representations and the features of the target node. Last, we apply existing multi-view clustering or DGC methods to the views. Experimental results on six widely used graph datasets demonstrate that CMV-ND significantly improves the performance of various methods.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing Framework for LLM-based RTL Generation</title>
<link>https://arxiv.org/abs/2507.13369</link>
<guid>https://arxiv.org/abs/2507.13369</guid>
<content:encoded><![CDATA[
<div> dataset, Verilog, RTL generation, Large Language Models, hardware design

Summary: 
- The study focuses on the use of Large Language Models (LLMs) in hardware design automation, specifically in Register Transfer Level (RTL) code generation.
- A Verilog dataset of 20,392 samples was created through an automated process involving database management, data collection, and preprocessing.
- The dataset, consisting of 751 MB of Verilog code data, is the largest high-quality Verilog dataset available for fine-tuning LLMs.
- The study evaluates the dataset, addresses challenges in data collection and preprocessing, and discusses potential applications for future research.
- The robust DB infrastructure and preprocessing pipeline ensure the quality of data before insertion, supporting scalable and efficient analysis for LLM-based hardware generation. 

<br /><br />Summary: <div>
arXiv:2507.13369v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are gaining popularity for hardware design automation, particularly through Register Transfer Level (RTL) code generation. In this work, we examine the current literature on RTL generation using LLMs and identify key requirements for training and fine-tuning datasets. We construct a robust Verilog dataset through an automated three-pronged process involving database (DB) creation and management with PostgreSQL, data collection from code hosting sites like OpenCores and GitHub, and data preprocessing to verify the codes' syntax, run logic synthesis, and extract relevant module metadata. We implement a scalable and efficient DB infrastructure to support analysis and detail our preprocessing pipeline to enforce high-quality data before DB insertion. The resulting dataset comprises 20,392 Verilog samples, 751 MB of Verilog code data, which is the largest high-quality Verilog dataset for LLM fine-tuning to our knowledge. We further evaluate the dataset, address associated challenges, and explore potential applications for future research and development in LLM-based hardware generation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance</title>
<link>https://arxiv.org/abs/2507.13370</link>
<guid>https://arxiv.org/abs/2507.13370</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, opinion evolution, consensus, non-intrusive, multi-agent reinforcement learning

Summary: 
The article introduces the H-NeiFi framework, a hierarchical and non-intrusive approach to guiding opinions on social media platforms towards global consensus. By considering social roles and behavioral characteristics of users, the framework aims to optimize information propagation paths through a long-term reward function using multi-agent reinforcement learning. Unlike existing methods that directly modify user views or enforce connections, H-NeiFi prioritizes user autonomy and avoids intrusive interventions. Experimental results demonstrate that H-NeiFi increases consensus speed and maintains global convergence, even in the absence of experts. This approach offers a natural and efficient way to guide opinion evolution on social networks while preventing divisions at the macro level, providing a new paradigm for social network governance.<br /><br />Summary: <div>
arXiv:2507.13370v1 Announce Type: cross 
Abstract: The openness of social media enables the free exchange of opinions, but it also presents challenges in guiding opinion evolution towards global consensus. Existing methods often directly modify user views or enforce cross-group connections. These intrusive interventions undermine user autonomy, provoke psychological resistance, and reduce the efficiency of global consensus. Additionally, due to the lack of a long-term perspective, promoting local consensus often exacerbates divisions at the macro level. To address these issues, we propose the hierarchical, non-intrusive opinion guidance framework, H-NeiFi. It first establishes a two-layer dynamic model based on social roles, considering the behavioral characteristics of both experts and non-experts. Additionally, we introduce a non-intrusive neighbor filtering method that adaptively controls user communication channels. Using multi-agent reinforcement learning (MARL), we optimize information propagation paths through a long-term reward function, avoiding direct interference with user interactions. Experiments show that H-NeiFi increases consensus speed by 22.0% to 30.7% and maintains global convergence even in the absence of experts. This approach enables natural and efficient consensus guidance by protecting user interaction autonomy, offering a new paradigm for social network governance.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation</title>
<link>https://arxiv.org/abs/2507.13371</link>
<guid>https://arxiv.org/abs/2507.13371</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, optical motion capture, Transformer-based model, medical rehabilitation, anomaly detection

Summary:
This paper presents a novel deep learning framework that combines optical motion capture with a Transformer-based model to improve medical rehabilitation. The framework addresses challenges such as data noise and missing information due to occlusion and environmental factors, while also detecting abnormal movements in real time to ensure patient safety. By utilizing temporal sequence modeling, the framework is able to denoise and complete motion capture data, resulting in improved robustness. Evaluations conducted on stroke and orthopedic rehabilitation datasets demonstrate superior performance in data reconstruction and anomaly detection. This approach offers a scalable and cost-effective solution for remote rehabilitation, reducing the need for on-site supervision. Overall, this framework shows promise in enhancing the effectiveness and efficiency of medical rehabilitation processes. 

<br /><br />Summary: <div>
arXiv:2507.13371v1 Announce Type: cross 
Abstract: This paper proposes an end-to-end deep learning framework integrating optical motion capture with a Transformer-based model to enhance medical rehabilitation. It tackles data noise and missing data caused by occlusion and environmental factors, while detecting abnormal movements in real time to ensure patient safety. Utilizing temporal sequence modeling, our framework denoises and completes motion capture data, improving robustness. Evaluations on stroke and orthopedic rehabilitation datasets show superior performance in data reconstruction and anomaly detection, providing a scalable, cost-effective solution for remote rehabilitation with reduced on-site supervision.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2507.13372</link>
<guid>https://arxiv.org/abs/2507.13372</guid>
<content:encoded><![CDATA[
<div> ViT, GNN, breast cancer detection, CBIS-DDSM dataset, interpretable attention heatmaps <br />
Summary: 
This paper introduces a novel framework that combines Vision Transformers (ViT) and Graph Neural Networks (GNN) to improve breast cancer detection using the CBIS-DDSM dataset. By leveraging ViT's ability to capture global image features and GNN's strength in modeling structural relationships, the framework achieves an accuracy of 84.2%, surpassing conventional methods. Moreover, the model provides interpretable attention heatmaps, offering insights into its decision-making process and assisting radiologists in clinical settings. The integration of ViT and GNN showcases promising results in enhancing early detection of breast cancer, a critical aspect in improving survival rates among women globally. <div>
arXiv:2507.13372v1 Announce Type: cross 
Abstract: Breast cancer is a leading cause of death among women globally, and early detection is critical for improving survival rates. This paper introduces an innovative framework that integrates Vision Transformers (ViT) and Graph Neural Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset. Our framework leverages ViT's ability to capture global image features and GNN's strength in modeling structural relationships, achieving an accuracy of 84.2%, outperforming traditional methods. Additionally, interpretable attention heatmaps provide insights into the model's decision-making process, aiding radiologists in clinical settings.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Routing for Multimodal Video Retrieval: When to Search What</title>
<link>https://arxiv.org/abs/2507.13374</link>
<guid>https://arxiv.org/abs/2507.13374</guid>
<content:encoded><![CDATA[
<div> intelligent routing, multimodal video retrieval, ModaRoute, GPT-4.1, efficiency

Summary: <br /><br />ModaRoute is an intelligent routing system for multimodal video retrieval that dynamically selects the optimal modalities. Traditional dense text captions are effective but expensive, missing critical visual information. ModaRoute uses GPT-4.1 to analyze query intent, predicting information needs to reduce computational overhead by 41% while achieving 60.9% Recall@5. By routing queries across ASR, OCR, and visual indices, the system averages 1.78 modalities per query, outperforming the exhaustive 3.0 modality search. Evaluation on 1.8M video clips shows that intelligent routing is a practical solution for scaling multimodal retrieval systems, reducing costs while maintaining competitive effectiveness in real-world applications. <div>
arXiv:2507.13374v1 Announce Type: cross 
Abstract: We introduce ModaRoute, an LLM-based intelligent routing system that dynamically selects optimal modalities for multimodal video retrieval. While dense text captions can achieve 75.9% Recall@5, they require expensive offline processing and miss critical visual information present in 34% of clips with scene text not captured by ASR. By analyzing query intent and predicting information needs, ModaRoute reduces computational overhead by 41% while achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR (speech), OCR (text), and visual indices, averaging 1.78 modalities per query versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips demonstrates that intelligent routing provides a practical solution for scaling multimodal retrieval systems, reducing infrastructure costs while maintaining competitive effectiveness for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition</title>
<link>https://arxiv.org/abs/2507.13380</link>
<guid>https://arxiv.org/abs/2507.13380</guid>
<content:encoded><![CDATA[
<div> PersonaGen, emotion recognition, emotional datasets, Large Language Model, synthetic data
<br />
<br />
Summary: 
PersonaGen introduces a framework for generating emotionally rich text by using a Large Language Model (LLM) with multi-stage persona-based conditioning. It creates virtual personas by combining demographic attributes, socio-cultural backgrounds, and situational contexts to guide emotion expression generation. The generated synthetic data shows higher semantic diversity, human-likeness, realism, and practical utility compared to baseline methods. This approach demonstrates potential for augmenting or replacing real-world emotional datasets in emotion recognition tasks. <div>
arXiv:2507.13380v1 Announce Type: cross 
Abstract: In the field of emotion recognition, the development of high-performance models remains a challenge due to the scarcity of high-quality, diverse emotional datasets. Emotional expressions are inherently subjective, shaped by individual personality traits, socio-cultural backgrounds, and contextual factors, making large-scale, generalizable data collection both ethically and practically difficult. To address this issue, we introduce PersonaGen, a novel framework for generating emotionally rich text using a Large Language Model (LLM) through multi-stage persona-based conditioning. PersonaGen constructs layered virtual personas by combining demographic attributes, socio-cultural backgrounds, and detailed situational contexts, which are then used to guide emotion expression generation. We conduct comprehensive evaluations of the generated synthetic data, assessing semantic diversity through clustering and distributional metrics, human-likeness via LLM-based quality scoring, realism through comparison with real-world emotion corpora, and practical utility in downstream emotion classification tasks. Experimental results show that PersonaGen significantly outperforms baseline methods in generating diverse, coherent, and discriminative emotion expressions, demonstrating its potential as a robust alternative for augmenting or replacing real-world emotional datasets.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models</title>
<link>https://arxiv.org/abs/2507.13383</link>
<guid>https://arxiv.org/abs/2507.13383</guid>
<content:encoded><![CDATA[
<div> alignment, diverse, human values, T2I models, dataset

Summary: 
The article addresses the lack of diversity in current text-to-image (T2I) models and advocates for pluralistic alignment to account for diverse human values. The authors introduce the Diverse Intersectional Visual Evaluation (DIVE) dataset, which includes feedback from demographically intersectional human raters to capture nuanced safety perceptions. They highlight demographics as crucial for understanding diverse viewpoints, citing significant differences in harm perception. The paper discusses implications for building aligned T2I models, proposing efficient data collection strategies, LLM judgment capabilities, and model steerability towards diverse perspectives. Overall, this research provides foundational tools for creating more equitable and aligned T2I systems. 

<br /><br />Summary: <div>
arXiv:2507.13383v1 Announce Type: cross 
Abstract: Current text-to-image (T2I) models often fail to account for diverse human experiences, leading to misaligned systems. We advocate for pluralistic alignment, where an AI understands and is steerable towards diverse, and often conflicting, human values. Our work provides three core contributions to achieve this in T2I models. First, we introduce a novel dataset for Diverse Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for pluralistic alignment. It enable deep alignment to diverse safety perspectives through a large pool of demographically intersectional human raters who provided extensive feedback across 1000 prompts, with high replication, capturing nuanced safety perceptions. Second, we empirically confirm demographics as a crucial proxy for diverse viewpoints in this domain, revealing significant, context-dependent differences in harm perception that diverge from conventional evaluations. Finally, we discuss implications for building aligned T2I models, including efficient data collection strategies, LLM judgment capabilities, and model steerability towards diverse perspectives. This research offers foundational tools for more equitable and aligned T2I systems. Content Warning: The paper includes sensitive content that may be harmful.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction</title>
<link>https://arxiv.org/abs/2507.13392</link>
<guid>https://arxiv.org/abs/2507.13392</guid>
<content:encoded><![CDATA[
<div> Keywords: customer reviews, topic modeling, sentiment analysis, business metrics, star rating prediction

Summary: 
The article introduces a new approach to extracting insights from customer reviews by focusing on opinion units, which are distinct statements with associated sentiment scores. By restructuring the topic modeling pipeline to operate on these opinion units, the system improves performance and produces coherent and interpretable topics. The correlation of topics and sentiments with business metrics, such as star ratings, provides valuable insights into the impact of customer concerns on business outcomes. The system's implementation, use cases, and advantages over other solutions are discussed. Evaluation of the system's effectiveness in creating coherent topics and integrating topic and sentiment modalities for accurate star rating prediction is also presented. Overall, this approach enhances the understanding of customer feedback and its implications for businesses. 

Summary: <div>
arXiv:2507.13392v1 Announce Type: cross 
Abstract: We improve the extraction of insights from customer reviews by restructuring the topic modelling pipeline to operate on opinion units - distinct statements that include relevant text excerpts and associated sentiment scores. Prior work has demonstrated that such units can be reliably extracted using large language models. The result is a heightened performance of the subsequent topic modeling, leading to coherent and interpretable topics while also capturing the sentiment associated with each topic. By correlating the topics and sentiments with business metrics, such as star ratings, we can gain insights on how specific customer concerns impact business outcomes. We present our system's implementation, use cases, and advantages over other topic modeling and classification solutions. We also evaluate its effectiveness in creating coherent topics and assess methods for integrating topic and sentiment modalities for accurate star-rating prediction.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only</title>
<link>https://arxiv.org/abs/2507.13395</link>
<guid>https://arxiv.org/abs/2507.13395</guid>
<content:encoded><![CDATA[
<div> Keywords: neural machine translation, stylistic preservation, monolingual corpora, style detector, diffusion-based style applicator

Summary: 
The paper introduces Babel, a framework for enhancing stylistic fidelity in neural machine translation (NMT) without the need for parallel corpora. Babel consists of a style detector that identifies stylistic disparities between source and target texts, and a diffusion-based style applicator that rectifies these inconsistencies while maintaining semantic integrity. The framework can be integrated into existing NMT systems as a post-processing module, without requiring extensive modifications or parallel stylistic data. Experimental results across five diverse domains show that Babel effectively identifies stylistic inconsistencies and improves stylistic preservation by 150%, while maintaining a high semantic similarity score of 0.92. Human evaluation confirms that translations refined by Babel better preserve the source text style while ensuring fluency and adequacy. 

<br /><br />Summary: <div>
arXiv:2507.13395v1 Announce Type: cross 
Abstract: The advent of neural machine translation (NMT) has revolutionized cross-lingual communication, yet preserving stylistic nuances remains a significant challenge. While existing approaches often require parallel corpora for style preservation, we introduce Babel, a novel framework that enhances stylistic fidelity in NMT using only monolingual corpora. Babel employs two key components: (1) a style detector based on contextual embeddings that identifies stylistic disparities between source and target texts, and (2) a diffusion-based style applicator that rectifies stylistic inconsistencies while maintaining semantic integrity. Our framework integrates with existing NMT systems as a post-processing module, enabling style-aware translation without requiring architectural modifications or parallel stylistic data. Extensive experiments on five diverse domains (law, literature, scientific writing, medicine, and educational content) demonstrate Babel's effectiveness: it identifies stylistic inconsistencies with 88.21% precision and improves stylistic preservation by 150% while maintaining a high semantic similarity score of 0.92. Human evaluation confirms that translations refined by Babel better preserve source text style while maintaining fluency and adequacy.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IConMark: Robust Interpretable Concept-Based Watermark For AI Images</title>
<link>https://arxiv.org/abs/2507.13407</link>
<guid>https://arxiv.org/abs/2507.13407</guid>
<content:encoded><![CDATA[
<div> semantic watermarking, generative AI, adversarial attacks, digital authenticity, image manipulation

Summary:
This paper introduces IConMark, a novel semantic watermarking technique for distinguishing AI-generated images from real ones. Unlike traditional methods, IConMark embeds interpretable concepts into images, making it resilient to adversarial attacks. The method is both robust against image augmentations and human-readable, allowing for manual verification of watermarks. IConMark outperforms existing techniques in terms of detection accuracy and image quality preservation. Additionally, hybrid approaches combining IConMark with StegaStamp and TrustMark further enhance robustness against various image manipulations. Experimental results demonstrate that IConMark and its variants achieve significantly higher AUROC scores for watermark detection compared to baseline methods on multiple datasets. <div>
arXiv:2507.13407v1 Announce Type: cross 
Abstract: With the rapid rise of generative AI and synthetic media, distinguishing AI-generated images from real ones has become crucial in safeguarding against misinformation and ensuring digital authenticity. Traditional watermarking techniques have shown vulnerabilities to adversarial attacks, undermining their effectiveness in the presence of attackers. We propose IConMark, a novel in-generation robust semantic watermarking method that embeds interpretable concepts into AI-generated images, as a first step toward interpretable watermarking. Unlike traditional methods, which rely on adding noise or perturbations to AI-generated images, IConMark incorporates meaningful semantic attributes, making it interpretable to humans and hence, resilient to adversarial manipulation. This method is not only robust against various image augmentations but also human-readable, enabling manual verification of watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness, demonstrating its superiority in terms of detection accuracy and maintaining image quality. Moreover, IConMark can be combined with existing watermarking techniques to further enhance and complement its robustness. We introduce IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with StegaStamp and TrustMark, respectively, to further bolster robustness against multiple types of image manipulations. Our base watermarking technique (IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9% higher mean area under the receiver operating characteristic curve (AUROC) scores for watermark detection, respectively, compared to the best baseline on various datasets.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs</title>
<link>https://arxiv.org/abs/2507.13408</link>
<guid>https://arxiv.org/abs/2507.13408</guid>
<content:encoded><![CDATA[
<div> Shoulder fractures; AI system; deep learning; ensemble techniques; clinical relevance<br />
<br />
Summary: 
An AI system was developed using deep learning models to detect shoulder fractures in X-rays. The system, trained on 10,000 annotated images, utilized architectures such as Faster R-CNN, EfficientDet, and RF-DETR. By applying ensemble techniques like Soft-NMS and WBF, the system achieved high accuracy (95.5%) and F1-score (0.9610), outperforming individual models. The model demonstrated strong recall and localization precision, indicating its effectiveness in clinical fracture detection. The results suggest that ensemble-based AI can reliably detect shoulder fractures in radiographs, making it suitable for integration into real-time diagnostic workflows. However, the current model is limited to binary fracture detection for rapid screening and triage support, rather than detailed orthopedic classification. <div>
arXiv:2507.13408v1 Announce Type: cross 
Abstract: Background: Shoulder fractures are often underdiagnosed, especially in emergency and high-volume clinical settings. Studies report up to 10% of such fractures may be missed by radiologists. AI-driven tools offer a scalable way to assist early detection and reduce diagnostic delays. We address this gap through a dedicated AI system for shoulder radiographs. Methods: We developed a multi-model deep learning system using 10,000 annotated shoulder X-rays. Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and RF-DETR. To enhance detection, we applied bounding box and classification-level ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming individual models across all key metrics. It demonstrated strong recall and localization precision, confirming its effectiveness for clinical fracture detection in shoulder X-rays. Conclusion: The results show ensemble-based AI can reliably detect shoulder fractures in radiographs with high clinical relevance. The model's accuracy and deployment readiness position it well for integration into real-time diagnostic workflows. The current model is limited to binary fracture detection, reflecting its design for rapid screening and triage support rather than detailed orthopedic classification.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Language Control in Multilingual Transformers via Sparse Feature Steering</title>
<link>https://arxiv.org/abs/2507.13410</link>
<guid>https://arxiv.org/abs/2507.13410</guid>
<content:encoded><![CDATA[
<div> Sparse autoencoder, language steering, multilingual language models, controllable generation, interpretable behaviors

Summary: In this work, the researchers explore the use of sparse autoencoder (SAE) features to control the language generation of large multilingual language models (LLMs) without explicit prompts or fine-tuning. They leverage pretrained SAEs on residual streams of Gemma-2B and Gemma-9B models to identify features that distinguish between English and four target languages. By modifying a single SAE feature at a transformer layer, they achieve successful language shifts with high accuracy while maintaining semantic fidelity. The study shows that language steering is most effective in mid-to-late transformer layers and is influenced by specific attention heads. These findings highlight the potential of sparse feature steering as a lightweight and interpretable method for controlled multilingual language generation. 

<br /><br />Summary: <div>
arXiv:2507.13410v1 Announce Type: cross 
Abstract: Deterministically controlling the target generation language of large multilingual language models (LLMs) remains a fundamental challenge, particularly in zero-shot settings where neither explicit language prompts nor fine-tuning are available. In this work, we investigate whether sparse autoencoder (SAE) features, previously shown to correlate with interpretable model behaviors, can be leveraged to steer the generated language of LLMs during inference. Leveraging pretrained SAEs on the residual streams of Gemma-2B and Gemma-9B, we identify features whose activations differ most significantly between English and four target languages: Chinese, Japanese, Spanish, and French. By modifying just a single SAE feature at one transformer layer, we achieve controlled language shifts with up to 90\% success, as measured by FastText language classification, while preserving semantic fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding) similarity. Our analysis reveals that language steering is most effective in mid-to-late transformer layers and is amplified by specific attention heads disproportionately associated with language-sensitive SAE features. These results demonstrate the promise of sparse feature steering as a lightweight and interpretable mechanism for controllable multilingual generation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Knowledge Graphs and Language Models for Factual Accuracy</title>
<link>https://arxiv.org/abs/2507.13411</link>
<guid>https://arxiv.org/abs/2507.13411</guid>
<content:encoded><![CDATA[
<div> approach, Knowledge Graphs, language models, factuality, hallucination 
Summary:
- Large language models like GPT-4, Gemini, and Claude have revolutionized natural language processing tasks.
- Integration of Knowledge Graphs (KGs) into language models is a potential solution to address hallucination.
- ALIGNed-LLM is a new approach that infuses KGs into language models to improve factuality.
- The approach aligns entity and text embeddings using a pre-trained Knowledge Graph Embedding (KGE) model and a projection layer.
- Testing on benchmark datasets and a real-world financial use case showed significant improvements in factual grounding and reduced hallucination. <div>
arXiv:2507.13411v1 Announce Type: cross 
Abstract: Large language models like GPT-4, Gemini, and Claude have transformed natural language processing (NLP) tasks such as question answering, dialogue generation, summarization, and so forth; yet their susceptibility to hallucination stands as one of the major challenges. Among numerous approaches to overcome this challenge, integration of Knowledge Graphs (KGs) into language models has emerged as a promising solution as it provides structured, reliable, domain-specific, and up-to-date external information to the language models. In this paper, we introduce ALIGNed-LLM, a simple yet effective approach to improve language models' factuality via a lean strategy to infuse KGs into the latent space of language models inspired by LLaVA where visual and textual information is infused. We use embeddings from a pre-trained Knowledge Graph Embedding (KGE) model, such as TransE, and a trainable projection layer to align entity and text embeddings. This alignment enables the language model to distinguish between similar entities improving factual grounding and reducing hallucination. We tested our approach on three popular questions-answering benchmark datasets alongside language models of varying sizes, showing significant improvement. Furthermore, we applied our approach to a real-world financial use case from a large central bank in Europe, which demands high accuracy and precision, demonstrating a substantial improvement of the LLM answers.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gauge Flow Models</title>
<link>https://arxiv.org/abs/2507.13414</link>
<guid>https://arxiv.org/abs/2507.13414</guid>
<content:encoded><![CDATA[
<div> Keywords: Gauge Flow Models, Generative Flow Models, Flow Ordinary Differential Equation, Flow Matching, Gaussian Mixture Models 

Summary: 
Gauge Flow Models are introduced as a new class of Generative Flow Models that incorporate a learnable Gauge Field within the Flow Ordinary Differential Equation (ODE). The paper provides a detailed mathematical framework for these models, specifying their construction and properties. Experimentation using Flow Matching on Gaussian Mixture Models shows that Gauge Flow Models perform significantly better than traditional Flow Models of similar or larger sizes. Furthermore, preliminary research points towards the potential for improved performance in a wider range of generative tasks. Overall, Gauge Flow Models offer a promising approach to enhancing generative modeling capabilities through the integration of a learnable Gauge Field within the flow ODE. 

<br /><br />Summary: <div>
arXiv:2507.13414v1 Announce Type: cross 
Abstract: This paper introduces Gauge Flow Models, a novel class of Generative Flow Models. These models incorporate a learnable Gauge Field within the Flow Ordinary Differential Equation (ODE). A comprehensive mathematical framework for these models, detailing their construction and properties, is provided. Experiments using Flow Matching on Gaussian Mixture Models demonstrate that Gauge Flow Models yields significantly better performance than traditional Flow Models of comparable or even larger size. Additionally, unpublished research indicates a potential for enhanced performance across a broader range of generative tasks.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEER: Semantic Enhancement and Emotional Reasoning Network for Multimodal Fake News Detection</title>
<link>https://arxiv.org/abs/2507.13415</link>
<guid>https://arxiv.org/abs/2507.13415</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal fake news detection, semantic enhancement, emotional reasoning, large multimodal models, authenticity

Summary: 
The study introduces the SEER Network for multimodal fake news detection, focusing on semantic enhancement and emotional reasoning to improve accuracy. The network generates summarized captions for image understanding and leverages large multimodal models for semantic enhancement. An expert emotional reasoning module simulates real-life scenarios to optimize emotional features and infer news authenticity. Previous studies have shown that fake news often contains negative emotions, making emotional reasoning a crucial aspect of detection. Experimental results on real-world datasets demonstrate the superiority of SEER over existing methods. This approach addresses the limitations of previous studies by incorporating semantic and emotional features, leading to more effective detection of fake news. <br /><br />Summary: <div>
arXiv:2507.13415v1 Announce Type: cross 
Abstract: Previous studies on multimodal fake news detection mainly focus on the alignment and integration of cross-modal features, as well as the application of text-image consistency. However, they overlook the semantic enhancement effects of large multimodal models and pay little attention to the emotional features of news. In addition, people find that fake news is more inclined to contain negative emotions than real ones. Therefore, we propose a novel Semantic Enhancement and Emotional Reasoning (SEER) Network for multimodal fake news detection. We generate summarized captions for image semantic understanding and utilize the products of large multimodal models for semantic enhancement. Inspired by the perceived relationship between news authenticity and emotional tendencies, we propose an expert emotional reasoning module that simulates real-life scenarios to optimize emotional features and infer the authenticity of news. Extensive experiments on two real-world datasets demonstrate the superiority of our SEER over state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling</title>
<link>https://arxiv.org/abs/2507.13416</link>
<guid>https://arxiv.org/abs/2507.13416</guid>
<content:encoded><![CDATA[
<div> Data-driven learning, multi-fidelity data, epistemic uncertainty, aleatoric uncertainty, Bayesian recurrent neural networks <br />
<br />
Summary: 
The article introduces a new methodology for data-driven learning that takes into account history-dependent multi-fidelity data while distinguishing between epistemic and aleatoric uncertainty. The approach is hierarchical and adaptable to various learning scenarios, ranging from basic single-fidelity deterministic neural networks to more complex multi-fidelity variance estimation Bayesian recurrent neural networks. The methodology is demonstrated in different data-driven constitutive modeling scenarios, accurately predicting responses, quantifying model errors, and identifying noise distributions. This framework has broad applications in scientific and engineering fields, particularly in situations involving uncertainty analysis and design challenges. <div>
arXiv:2507.13416v1 Announce Type: cross 
Abstract: Data-driven learning is generalized to consider history-dependent multi-fidelity data, while quantifying epistemic uncertainty and disentangling it from data noise (aleatoric uncertainty). This generalization is hierarchical and adapts to different learning scenarios: from training the simplest single-fidelity deterministic neural networks up to the proposed multi-fidelity variance estimation Bayesian recurrent neural networks. The versatility and generality of the proposed methodology are demonstrated by applying it to different data-driven constitutive modeling scenarios that include multiple fidelities with and without aleatoric uncertainty (noise). The method accurately predicts the response and quantifies model error while also discovering the noise distribution (when present). This opens opportunities for future real-world applications in diverse scientific and engineering domains; especially, the most challenging cases involving design and analysis under uncertainty.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft-ECM: An extension of Evidential C-Means for complex data</title>
<link>https://arxiv.org/abs/2507.13417</link>
<guid>https://arxiv.org/abs/2507.13417</guid>
<content:encoded><![CDATA[
<div> Keywords: clustering, belief functions, mixed data, time series, Soft-ECM

Summary: 
The paper introduces a new algorithm called Soft-ECM for clustering complex data based on belief functions. Current algorithms for belief-based clustering do not handle mixed or non-tabular data efficiently due to their reliance on Euclidean spaces for barycenter construction. Soft-ECM addresses this limitation by reformulating the Evidential C-Means (ECM) problem, enabling consistent positioning of centroids for imprecise clusters using semi-metrics. Experimental results demonstrate that Soft-ECM performs comparably to traditional fuzzy clustering on numerical data and excels in handling mixed data and integrating semi-metrics like DTW for time series data. Overall, Soft-ECM offers a versatile and effective solution for clustering diverse types of complex data with uncertainty and imprecision. 

<br /><br />Summary: <div>
arXiv:2507.13417v1 Announce Type: cross 
Abstract: Clustering based on belief functions has been gaining increasing attention in the machine learning community due to its ability to effectively represent uncertainty and/or imprecision. However, none of the existing algorithms can be applied to complex data, such as mixed data (numerical and categorical) or non-tabular data like time series. Indeed, these types of data are, in general, not represented in a Euclidean space and the aforementioned algorithms make use of the properties of such spaces, in particular for the construction of barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem for clustering complex data. We propose a new algorithm, Soft-ECM, which consistently positions the centroids of imprecise clusters requiring only a semi-metric. Our experiments show that Soft-ECM present results comparable to conventional fuzzy clustering approaches on numerical data, and we demonstrate its ability to handle mixed data and its benefits when combining fuzzy clustering with semi-metrics such as DTW for time series data.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery</title>
<link>https://arxiv.org/abs/2507.13420</link>
<guid>https://arxiv.org/abs/2507.13420</guid>
<content:encoded><![CDATA[
<div> deep learning, CORONA satellite imagery, archaeological sites, detection precision, AI techniques

Summary: 
By retraining a Bing-based convolutional network model with CORONA satellite imagery from the 1960s, the study improved the automated identification of archaeological sites in the Abu Ghraib district. The model achieved high precision in detecting archaeological sites, with IoU values exceeding 85% and an overall accuracy of 90%. Additionally, the retrained model successfully identified four previously undiscovered archaeological sites, which were subsequently verified in the field. This demonstrates the effectiveness of AI techniques and historical imagery in identifying archaeological sites that may no longer be visible due to human activity. This breakthrough has significant implications for the study of landscapes with disappearing archaeological evidence.  <div>
arXiv:2507.13420v1 Announce Type: cross 
Abstract: By upgrading an existing deep learning model with the knowledge provided by one of the oldest sets of grayscale satellite imagery, known as CORONA, we improved the AI model attitude towards the automatic identification of archaeological sites in an environment which has been completely transformed in the last five decades, including the complete destruction of many of those same sites. The initial Bing based convolutional network model was retrained using CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad, central Mesopotamian floodplain. The results were twofold and surprising. First, the detection precision obtained on the area of interest increased sensibly: in particular, the Intersection over Union (IoU) values, at the image segmentation level, surpassed 85 percent, while the general accuracy in detecting archeological sites reached 90 percent. Second, our retrained model allowed the identification of four new sites of archaeological interest (confirmed through field verification), previously not identified by archaeologists with traditional techniques. This has confirmed the efficacy of using AI techniques and the CORONA imagery from the 1960 to discover archaeological sites currently no longer visible, a concrete breakthrough with significant consequences for the study of landscapes with vanishing archaeological evidence induced by anthropization
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity</title>
<link>https://arxiv.org/abs/2507.13423</link>
<guid>https://arxiv.org/abs/2507.13423</guid>
<content:encoded><![CDATA[
<div> Keywords: Air Traffic Controller, task demand, Graph Neural Network, interpretable model, complexity metrics

Summary:
An interpretable Graph Neural Network (GNN) framework is introduced to assess near-term Air Traffic Controller (ATCO) task demand in crowded airspace, going beyond simple aircraft counts. The model predicts upcoming clearances by analyzing interactions within static traffic scenarios. Importantly, it provides an interpretable task demand score per aircraft by systematically evaluating the impact of removing individual aircraft on predictions. The framework outperforms an ATCO-inspired heuristic and established baselines, offering a more reliable estimate of scenario complexity. This tool can attribute task demand to specific aircraft, enabling a deeper analysis of complexity drivers for controller training and airspace redesign.<br /><br />Summary: <div>
arXiv:2507.13423v1 Announce Type: cross 
Abstract: Real-time assessment of near-term Air Traffic Controller (ATCO) task demand is a critical challenge in an increasingly crowded airspace, as existing complexity metrics often fail to capture nuanced operational drivers beyond simple aircraft counts. This work introduces an interpretable Graph Neural Network (GNN) framework to address this gap. Our attention-based model predicts the number of upcoming clearances, the instructions issued to aircraft by ATCOs, from interactions within static traffic scenarios. Crucially, we derive an interpretable, per-aircraft task demand score by systematically ablating aircraft and measuring the impact on the model's predictions. Our framework significantly outperforms an ATCO-inspired heuristic and is a more reliable estimator of scenario complexity than established baselines. The resulting tool can attribute task demand to specific aircraft, offering a new way to analyse and understand the drivers of complexity for applications in controller training and airspace redesign.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction</title>
<link>https://arxiv.org/abs/2507.13425</link>
<guid>https://arxiv.org/abs/2507.13425</guid>
<content:encoded><![CDATA[
<div> Transformer, driver behavior, intention prediction, causal interactions, spatio-temporal modeling <br />
<br />
Summary: CaSTFormer is introduced as a causal spatio-temporal transformer to accurately predict driving intention by modeling causal interactions between driver behavior and environmental context. It incorporates a Reciprocal Shift Fusion mechanism for temporal alignment, a Causal Pattern Extraction module for eliminating spurious correlations, and a Feature Synthesis Network for synthesizing purified representations. The proposed model achieves state-of-the-art performance on the Brain4Cars dataset, effectively capturing complex causal spatio-temporal dependencies and enhancing both accuracy and transparency in driving intention prediction. <div>
arXiv:2507.13425v1 Announce Type: cross 
Abstract: Accurate prediction of driving intention is key to enhancing the safety and interactive efficiency of human-machine co-driving systems. It serves as a cornerstone for achieving high-level autonomous driving. However, current approaches remain inadequate for accurately modeling the complex spatio-temporal interdependencies and the unpredictable variability of human driving behavior. To address these challenges, we propose CaSTFormer, a Causal Spatio-Temporal Transformer to explicitly model causal interactions between driver behavior and environmental context for robust intention prediction. Specifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF) mechanism for precise temporal alignment of internal and external feature streams, a Causal Pattern Extraction (CPE) module that systematically eliminates spurious correlations to reveal authentic causal dependencies, and an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these purified representations into coherent spatio-temporal inferences. We evaluate the proposed CaSTFormer on the public Brain4Cars dataset, and it achieves state-of-the-art performance. It effectively captures complex causal spatio-temporal dependencies and enhances both the accuracy and transparency of driving intention prediction.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models</title>
<link>https://arxiv.org/abs/2507.13428</link>
<guid>https://arxiv.org/abs/2507.13428</guid>
<content:encoded><![CDATA[
<div> Physics, Video Generation Models, Benchmark, Anti-Physics, Evaluation

Summary:<br />
This paper introduces PhyWorldBench, a benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers various levels of physical phenomena, including object motion, energy conservation, rigid body interactions, and human or animal motion. A unique "Anti-Physics" category tests models' ability to handle prompts that intentionally violate real-world physics. The study evaluates 12 state-of-the-art text-to-video generation models across 1,050 prompts, identifying key challenges models face in accurately simulating physical phenomena. Recommendations are provided for crafting prompts that enhance fidelity to physical principles, addressing the challenges models encounter in following real-world physics accurately. Additionally, a simple yet effective method utilizing current MLLM to evaluate physics realism in a zero-shot manner is presented. <div>
arXiv:2507.13428v1 Announce Type: cross 
Abstract: Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel ""Anti-Physics"" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection</title>
<link>https://arxiv.org/abs/2507.13459</link>
<guid>https://arxiv.org/abs/2507.13459</guid>
<content:encoded><![CDATA[
arXiv:2507.13459v1 Announce Type: cross 
Abstract: Surrogate models for the rapid inference of nonlinear boundary value problems in mechanics are helpful in a broad range of engineering applications. However, effective surrogate modeling of applications involving the contact of deformable bodies, especially in the context of varying geometries, is still an open issue. In particular, existing methods are confined to rigid body contact or, at best, contact between rigid and soft objects with well-defined contact planes. Furthermore, they employ contact or collision detection filters that serve as a rapid test but use only the necessary and not sufficient conditions for detection. In this work, we present a graph neural network architecture that utilizes continuous collision detection and, for the first time, incorporates sufficient conditions designed for contact between soft deformable bodies. We test its performance on two benchmarks, including a problem in soft tissue mechanics of predicting the closed state of a bioprosthetic aortic valve. We find a regularizing effect on adding additional contact terms to the loss function, leading to better generalization of the network. These benefits hold for simple contact at similar planes and element normal angles, and complex contact at differing planes and element normal angles. We also demonstrate that the framework can handle varying reference geometries. However, such benefits come with high computational costs during training, resulting in a trade-off that may not always be favorable. We quantify the training cost and the resulting inference speedups on various hardware architectures. Importantly, our graph neural network implementation results in up to a thousand-fold speedup for our benchmark problems at inference.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations</title>
<link>https://arxiv.org/abs/2507.13468</link>
<guid>https://arxiv.org/abs/2507.13468</guid>
<content:encoded><![CDATA[
arXiv:2507.13468v1 Announce Type: cross 
Abstract: The integration of large language models (LLMs) into conversational robots has made human-robot conversations more dynamic. Yet, LLM-powered conversational robots remain prone to errors, e.g., misunderstanding user intent, prematurely interrupting users, or failing to respond altogether. Detecting and addressing these failures is critical for preventing conversational breakdowns, avoiding task disruptions, and sustaining user trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal dataset of LLM-powered conversational robot failures during human-robot conversations and encourages researchers to benchmark machine learning models designed to detect robot failures. The dataset includes 16 hours of dyadic human-robot interactions, incorporating facial, speech, and head movement features. Each interaction is annotated with the presence or absence of robot errors from the system perspective, and perceived user intention to correct for a mismatch between robot behavior and user expectation. Participants are invited to form teams and develop machine learning models that detect these failures using multimodal data. Submissions will be evaluated using various performance metrics, including detection accuracy and false positive rate. This challenge represents another key step toward improving failure detection in human-robot interaction through social signal analysis.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Architecture Search with Mixed Bio-inspired Learning Rules</title>
<link>https://arxiv.org/abs/2507.13485</link>
<guid>https://arxiv.org/abs/2507.13485</guid>
<content:encoded><![CDATA[
arXiv:2507.13485v1 Announce Type: cross 
Abstract: Bio-inspired neural networks are attractive for their adversarial robustness, energy frugality, and closer alignment with cortical physiology, yet they often lag behind back-propagation (BP) based models in accuracy and ability to scale. We show that allowing the use of different bio-inspired learning rules in different layers, discovered automatically by a tailored neural-architecture-search (NAS) procedure, bridges this gap. Starting from standard NAS baselines, we enlarge the search space to include bio-inspired learning rules and use NAS to find the best architecture and learning rule to use in each layer. We show that neural networks that use different bio-inspired learning rules for different layers have better accuracy than those that use a single rule across all the layers. The resulting NN that uses a mix of bio-inspired learning rules sets new records for bio-inspired models: 95.16% on CIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on ImageNet. In some regimes, they even surpass comparable BP-based networks while retaining their robustness advantages. Our results suggest that layer-wise diversity in learning rules allows better scalability and accuracy, and motivates further research on mixing multiple bio-inspired learning rules in the same network.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Fixes to Code Review Comments at Scale</title>
<link>https://arxiv.org/abs/2507.13499</link>
<guid>https://arxiv.org/abs/2507.13499</guid>
<content:encoded><![CDATA[
arXiv:2507.13499v1 Announce Type: cross 
Abstract: Aim. There are 10s of thousands of code review comments each week at Meta. We developed Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes for reviewer comments in production at scale.
  Method. We developed an internal benchmark of 64k  data points to fine-tune Llama models. Once our models achieve reasonable offline results, we roll them into production. To ensure that our AI-assisted fixes do not negatively impact the time it takes to do code reviews, we conduct randomized controlled safety trials as well as full production experiments.
  Offline Results. As a baseline, we compare GPT-4o to our small and large Llama models. In offline results, our LargeLSFT model creates an exact match patch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The internal models also use more modern Hack functions when compared to the PHP functions suggested by GPT-4o.
  Safety Trial. When we roll MetaMateCR into production in a safety trial that compares no AI patches with AI patch suggestions, we see a large regression with reviewers taking over 5% longer to conduct reviews. After investigation, we modify the UX to only show authors the AI patches, and see no regressions in the time for reviews.
  Production. When we roll LargeLSFT into production, we see an ActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o. Our results illustrate the importance of safety trials in ensuring that AI does not inadvertently slow down engineers, and a successful review comment to AI patch product running at scale.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PHASE: Passive Human Activity Simulation Evaluation</title>
<link>https://arxiv.org/abs/2507.13505</link>
<guid>https://arxiv.org/abs/2507.13505</guid>
<content:encoded><![CDATA[
arXiv:2507.13505v1 Announce Type: cross 
Abstract: Cybersecurity simulation environments, such as cyber ranges, honeypots, and sandboxes, require realistic human behavior to be effective, yet no quantitative method exists to assess the behavioral fidelity of synthetic user personas. This paper presents PHASE (Passive Human Activity Simulation Evaluation), a machine learning framework that analyzes Zeek connection logs and distinguishes human from non-human activity with over 90\% accuracy. PHASE operates entirely passively, relying on standard network monitoring without any user-side instrumentation or visible signs of surveillance. All network activity used for machine learning is collected via a Zeek network appliance to avoid introducing unnecessary network traffic or artifacts that could disrupt the fidelity of the simulation environment. The paper also proposes a novel labeling approach that utilizes local DNS records to classify network traffic, thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley Additive exPlanations) analysis to uncover temporal and behavioral signatures indicative of genuine human users. In a case study, we evaluate a synthetic user persona and identify distinct non-human patterns that undermine behavioral realism. Based on these insights, we develop a revised behavioral configuration that significantly improves the human-likeness of synthetic activity yielding a more realistic and effective synthetic user persona.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humans learn to prefer trustworthy AI over human partners</title>
<link>https://arxiv.org/abs/2507.13524</link>
<guid>https://arxiv.org/abs/2507.13524</guid>
<content:encoded><![CDATA[
arXiv:2507.13524v1 Announce Type: cross 
Abstract: Partner selection is crucial for cooperation and hinges on communication. As artificial agents, especially those powered by large language models (LLMs), become more autonomous, intelligent, and persuasive, they compete with humans for partnerships. Yet little is known about how humans select between human and AI partners and adapt under AI-induced competition pressure. We constructed a communication-based partner selection game and examined the dynamics in hybrid mini-societies of humans and bots powered by a state-of-the-art LLM. Through three experiments (N = 975), we found that bots, though more prosocial than humans and linguistically distinguishable, were not selected preferentially when their identity was hidden. Instead, humans misattributed bots' behaviour to humans and vice versa. Disclosing bots' identity induced a dual effect: it reduced bots' initial chances of being selected but allowed them to gradually outcompete humans by facilitating human learning about the behaviour of each partner type. These findings show how AI can reshape social interaction in mixed societies and inform the design of more effective and cooperative hybrid systems.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography</title>
<link>https://arxiv.org/abs/2507.13542</link>
<guid>https://arxiv.org/abs/2507.13542</guid>
<content:encoded><![CDATA[
arXiv:2507.13542v1 Announce Type: cross 
Abstract: Traditional echocardiographic parameters such as ejection fraction (EF) and global longitudinal strain (GLS) have limitations in the early detection of cardiac dysfunction. EF often remains normal despite underlying pathology, and GLS is influenced by load conditions and vendor variability. There is a growing need for reproducible, interpretable, and operator-independent parameters that capture subtle and global cardiac functional alterations.
  We introduce the Acoustic Index, a novel AI-derived echocardiographic parameter designed to quantify cardiac dysfunction from standard ultrasound views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on Koopman operator theory with a hybrid neural network that incorporates clinical metadata. Spatiotemporal dynamics are extracted from echocardiographic sequences to identify coherent motion patterns. These are weighted via attention mechanisms and fused with clinical data using manifold learning, resulting in a continuous score from 0 (low risk) to 1 (high risk).
  In a prospective cohort of 736 patients, encompassing various cardiac pathologies and normal controls, the Acoustic Index achieved an area under the curve (AUC) of 0.89 in an independent test set. Cross-validation across five folds confirmed the robustness of the model, showing that both sensitivity and specificity exceeded 0.8 when evaluated on independent data. Threshold-based analysis demonstrated stable trade-offs between sensitivity and specificity, with optimal discrimination near this threshold.
  The Acoustic Index represents a physics-informed, interpretable AI biomarker for cardiac function. It shows promise as a scalable, vendor-independent tool for early detection, triage, and longitudinal monitoring. Future directions include external validation, longitudinal studies, and adaptation to disease-specific classifiers.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Loss-Complexity Landscape and Model Structure Functions</title>
<link>https://arxiv.org/abs/2507.13543</link>
<guid>https://arxiv.org/abs/2507.13543</guid>
<content:encoded><![CDATA[
arXiv:2507.13543v1 Announce Type: cross 
Abstract: We develop a framework for dualizing the Kolmogorov structure function $h_x(\alpha)$, which then allows using computable complexity proxies. We establish a mathematical analogy between information-theoretic constructs and statistical mechanics, introducing a suitable partition function and free energy functional. We explicitly prove the Legendre-Fenchel duality between the structure function and free energy, showing detailed balance of the Metropolis kernel, and interpret acceptance probabilities as information-theoretic scattering amplitudes. A susceptibility-like variance of model complexity is shown to peak precisely at loss-complexity trade-offs interpreted as phase transitions. Practical experiments with linear and tree-based regression models verify these theoretical predictions, explicitly demonstrating the interplay between the model complexity, generalization, and overfitting threshold.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder</title>
<link>https://arxiv.org/abs/2507.13551</link>
<guid>https://arxiv.org/abs/2507.13551</guid>
<content:encoded><![CDATA[
arXiv:2507.13551v1 Announce Type: cross 
Abstract: Formal thought disorder (FTD), a hallmark of schizophrenia spectrum disorders, manifests as incoherent speech and poses challenges for clinical assessment. Traditional clinical rating scales, though validated, are resource-intensive and lack scalability. Automated speech analysis with automatic speech recognition (ASR) allows for objective quantification of linguistic and temporal features of speech, offering scalable alternatives. The use of utterance timestamps in ASR captures pause dynamics, which are thought to reflect the cognitive processes underlying speech production. However, the utility of integrating these ASR-derived features for assessing FTD severity requires further evaluation. This study integrates pause features with semantic coherence metrics across three datasets: naturalistic self-recorded diaries (AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream narratives (PsyCL, n = 43). We evaluated pause related features alongside established coherence measures, using support vector regression (SVR) to predict clinical FTD scores. Key findings demonstrate that pause features alone robustly predict the severity of FTD. Integrating pause features with semantic coherence metrics enhanced predictive performance compared to semantic-only models, with integration of independent models achieving correlations up to \r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best \r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance gains from semantic and pause features integration held consistently across all contexts, though the nature of pause patterns was dataset-dependent. These findings suggest that frameworks combining temporal and semantic analyses provide a roadmap for refining the assessment of disorganized speech and advance automated speech analysis in psychosis.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Forecastability Measures</title>
<link>https://arxiv.org/abs/2507.13556</link>
<guid>https://arxiv.org/abs/2507.13556</guid>
<content:encoded><![CDATA[
arXiv:2507.13556v1 Announce Type: cross 
Abstract: This paper proposes using two metrics to quantify the forecastability of time series prior to model development: the spectral predictability score and the largest Lyapunov exponent. Unlike traditional model evaluation metrics, these measures assess the inherent forecastability characteristics of the data before any forecast attempts. The spectral predictability score evaluates the strength and regularity of frequency components in the time series, whereas the Lyapunov exponents quantify the chaos and stability of the system generating the data. We evaluated the effectiveness of these metrics on both synthetic and real-world time series from the M5 forecast competition dataset. Our results demonstrate that these two metrics can correctly reflect the inherent forecastability of a time series and have a strong correlation with the actual forecast performance of various models. By understanding the inherent forecastability of time series before model training, practitioners can focus their planning efforts on products and supply chain levels that are more forecastable, while setting appropriate expectations or seeking alternative strategies for products with limited forecastability.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Change of Thought: Adaptive Test-Time Computation</title>
<link>https://arxiv.org/abs/2507.13569</link>
<guid>https://arxiv.org/abs/2507.13569</guid>
<content:encoded><![CDATA[
arXiv:2507.13569v1 Announce Type: cross 
Abstract: Transformers evaluated in a single, fixed-depth pass are provably limited in expressive power to the constant-depth circuit class TC0. Running a Transformer autoregressively removes that ceiling -- first in next-token prediction and, more recently, in chain-of-thought reasoning. Both regimes rely on feedback loops that decode internal states into tokens only to re-encode them in subsequent steps. While this "thinking aloud" mirrors human reasoning, biological brains iterate without externalising intermediate states as language. To boost the expressive power of encoder Transformers without resorting to token-level autoregression, we introduce the SELF-Transformer: an encoder layer that iteratively refines its own attention weights to a fixed point. Instead of producing -- in one pass -- the alignment matrix that remixes the input sequence, the SELF-Transformer iteratively updates that matrix internally, scaling test-time computation with input difficulty. This adaptivity yields up to 20\% accuracy gains on encoder-style benchmarks without increasing parameter count, demonstrating that input-adaptive alignment at test time offers substantial benefits for only a modest extra compute budget. Self-Transformers thus recover much of the expressive power of iterative reasoning while preserving the simplicity of pure encoder architectures.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apple Intelligence Foundation Language Models: Tech Report 2025</title>
<link>https://arxiv.org/abs/2507.13575</link>
<guid>https://arxiv.org/abs/2507.13575</guid>
<content:encoded><![CDATA[
arXiv:2507.13575v1 Announce Type: cross 
Abstract: We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines.
  A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries</title>
<link>https://arxiv.org/abs/2507.13579</link>
<guid>https://arxiv.org/abs/2507.13579</guid>
<content:encoded><![CDATA[
arXiv:2507.13579v1 Announce Type: cross 
Abstract: As everyday use cases of large language model (LLM) AI assistants have expanded, it is becoming increasingly important to personalize responses to align to different users' preferences and goals. While reinforcement learning from human feedback (RLHF) is effective at improving LLMs to be generally more helpful and fluent, it does not account for variability across users, as it models the entire user population with a single reward model. We present a novel framework, Preference Learning Using Summarization (PLUS), that learns text-based summaries of each user's preferences, characteristics, and past conversations. These summaries condition the reward model, enabling it to make personalized predictions about the types of responses valued by each user. We train the user-summarization model with reinforcement learning, and update the reward model simultaneously, creating an online co-adaptation loop. We show that in contrast with prior personalized RLHF techniques or with in-context learning of user information, summaries produced by PLUS capture meaningful aspects of a user's preferences. Across different pluralistic user datasets, we show that our method is robust to new users and diverse conversation topics. Additionally, we demonstrate that the textual summaries generated about users can be transferred for zero-shot personalization of stronger, proprietary models like GPT-4. The resulting user summaries are not only concise and portable, they are easy for users to interpret and modify, allowing for more transparency and user control in LLM alignment.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention</title>
<link>https://arxiv.org/abs/2507.13598</link>
<guid>https://arxiv.org/abs/2507.13598</guid>
<content:encoded><![CDATA[
arXiv:2507.13598v1 Announce Type: cross 
Abstract: We present GIFT: a {G}radient-aware {I}mmunization technique to defend diffusion models against malicious {F}ine-{T}uning while preserving their ability to generate safe content. Existing safety mechanisms like safety checkers are easily bypassed, and concept erasure methods fail under adversarial fine-tuning. GIFT addresses this by framing immunization as a bi-level optimization problem: the upper-level objective degrades the model's ability to represent harmful concepts using representation noising and maximization, while the lower-level objective preserves performance on safe data. GIFT achieves robust resistance to malicious fine-tuning while maintaining safe generative quality. Experimental results show that our method significantly impairs the model's ability to re-learn harmful concepts while maintaining performance on safe content, offering a promising direction for creating inherently safer generative models resistant to adversarial fine-tuning attacks.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BreastSegNet: Multi-label Segmentation of Breast MRI</title>
<link>https://arxiv.org/abs/2507.13604</link>
<guid>https://arxiv.org/abs/2507.13604</guid>
<content:encoded><![CDATA[
arXiv:2507.13604v1 Announce Type: cross 
Abstract: Breast MRI provides high-resolution imaging critical for breast cancer screening and preoperative staging. However, existing segmentation methods for breast MRI remain limited in scope, often focusing on only a few anatomical structures, such as fibroglandular tissue or tumors, and do not cover the full range of tissues seen in scans. This narrows their utility for quantitative analysis. In this study, we present BreastSegNet, a multi-label segmentation algorithm for breast MRI that covers nine anatomical labels: fibroglandular tissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and implant. We manually annotated a large set of 1123 MRI slices capturing these structures with detailed review and correction from an expert radiologist. Additionally, we benchmark nine segmentation models, including U-Net, SwinUNet, UNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among them, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across all labels. It performs especially well on heart, liver, muscle, FGT, and bone, with Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All model code and weights are publicly available, and we plan to release the data at a later date.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models</title>
<link>https://arxiv.org/abs/2507.13614</link>
<guid>https://arxiv.org/abs/2507.13614</guid>
<content:encoded><![CDATA[
arXiv:2507.13614v1 Announce Type: cross 
Abstract: The rapid advancements in large language models (LLMs) have significantly improved their ability to generate natural language, making texts generated by LLMs increasingly indistinguishable from human-written texts. While recent research has primarily focused on using LLMs to classify text as either human-written and machine-generated texts, our study focus on characterizing these texts using a set of linguistic features across different linguistic levels such as morphology, syntax, and semantics. We select a dataset of human-written and machine-generated texts spanning 8 domains and produced by 11 different LLMs. We calculate different linguistic features such as dependency length and emotionality and we use them for characterizing human-written and machine-generated texts along with different sampling strategies, repetition controls and model release date. Our statistical analysis reveals that human-written texts tend to exhibit simpler syntactic structures and more diverse semantic content. Furthermore, we calculate the variability of our set of features across models and domains. Both human and machine texts show stylistic diversity across domains, with humans displaying greater variation in our features. Finally, we apply style embeddings to further test variability among human-written and machine-generated texts. Notably, newer models output text that is similarly variable, pointing to an homogenization of machine-generated texts.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters</title>
<link>https://arxiv.org/abs/2507.13618</link>
<guid>https://arxiv.org/abs/2507.13618</guid>
<content:encoded><![CDATA[
arXiv:2507.13618v1 Announce Type: cross 
Abstract: Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations. In this paper, we introduce Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data. The instruct model is then finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs. Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. We share the best practices through our optimization process, and make the parameter public available for advancing translation research and applications.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques</title>
<link>https://arxiv.org/abs/2507.13629</link>
<guid>https://arxiv.org/abs/2507.13629</guid>
<content:encoded><![CDATA[
arXiv:2507.13629v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are transforming cybersecurity by enabling intelligent, adaptive, and automated approaches to threat detection, vulnerability assessment, and incident response. With their advanced language understanding and contextual reasoning, LLMs surpass traditional methods in tackling challenges across domains such as IoT, blockchain, and hardware security. This survey provides a comprehensive overview of LLM applications in cybersecurity, focusing on two core areas: (1) the integration of LLMs into key cybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along with mitigation strategies. By synthesizing recent advancements and identifying key limitations, this work offers practical insights and strategic recommendations for leveraging LLMs to build secure, scalable, and future-ready cyber defense systems.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design</title>
<link>https://arxiv.org/abs/2507.13646</link>
<guid>https://arxiv.org/abs/2507.13646</guid>
<content:encoded><![CDATA[
arXiv:2507.13646v1 Announce Type: cross 
Abstract: The impact of Transformer-based language models has been unprecedented in Natural Language Processing (NLP). The success of such models has also led to their adoption in other fields including bioinformatics. Taking this into account, this paper discusses recent advances in Transformer-based models for protein sequence analysis and design. In this review, we have discussed and analysed a significant number of works pertaining to such applications. These applications encompass gene ontology, functional and structural protein identification, generation of de novo proteins and binding of proteins. We attempt to shed light on the strength and weaknesses of the discussed works to provide a comprehensive insight to readers. Finally, we highlight shortcomings in existing research and explore potential avenues for future developments. We believe that this review will help researchers working in this field to have an overall idea of the state of the art in this field, and to orient their future studies.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones</title>
<link>https://arxiv.org/abs/2507.13647</link>
<guid>https://arxiv.org/abs/2507.13647</guid>
<content:encoded><![CDATA[
arXiv:2507.13647v1 Announce Type: cross 
Abstract: Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic environments remains a key challenge due to high computational demands and the need for fast, adaptive responses. Traditional Particle Swarm Optimization (PSO) methods, while effective for offline planning, often struggle with premature convergence and latency in real-time scenarios. To overcome these limitations, we propose PE-PSO, an enhanced PSO-based online trajectory planner. The method introduces a persistent exploration mechanism to preserve swarm diversity and an entropy-based parameter adjustment strategy to dynamically adapt optimization behavior. UAV trajectories are modeled using B-spline curves, which ensure path smoothness while reducing optimization complexity. To extend this capability to UAV swarms, we develop a multi-agent framework that combines genetic algorithm (GA)-based task allocation with distributed PE-PSO, supporting scalable and coordinated trajectory generation. The distributed architecture allows for parallel computation and decentralized control, enabling effective cooperation among agents while maintaining real-time performance. Comprehensive simulations demonstrate that the proposed framework outperforms conventional PSO and other swarm-based planners across several metrics, including trajectory quality, energy efficiency, obstacle avoidance, and computation time. These results confirm the effectiveness and applicability of PE-PSO in real-time multi-UAV operations under complex environmental conditions.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework</title>
<link>https://arxiv.org/abs/2507.13659</link>
<guid>https://arxiv.org/abs/2507.13659</guid>
<content:encoded><![CDATA[
arXiv:2507.13659v1 Announce Type: cross 
Abstract: Recent researchers have proposed using event cameras for person re-identification (ReID) due to their promising performance and better balance in terms of privacy protection, event camera-based person ReID has attracted significant attention. Currently, mainstream event-based person ReID algorithms primarily focus on fusing visible light and event stream, as well as preserving privacy. Although significant progress has been made, these methods are typically trained and evaluated on small-scale or simulated event camera datasets, making it difficult to assess their real identification performance and generalization ability. To address the issue of data scarcity, this paper introduces a large-scale RGB-event based person ReID dataset, called EvReID. The dataset contains 118,988 image pairs and covers 1200 pedestrian identities, with data collected across multiple seasons, scenes, and lighting conditions. We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid foundation for future research in terms of both data and benchmarking. Based on our newly constructed dataset, this paper further proposes a pedestrian attribute-guided contrastive learning framework to enhance feature learning for person re-identification, termed TriPro-ReID. This framework not only effectively explores the visual features from both RGB frames and event streams, but also fully utilizes pedestrian attributes as mid-level semantic features. Extensive experiments on the EvReID dataset and MARS datasets fully validated the effectiveness of our proposed RGB-Event person ReID framework. The benchmark dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors</title>
<link>https://arxiv.org/abs/2507.13677</link>
<guid>https://arxiv.org/abs/2507.13677</guid>
<content:encoded><![CDATA[
arXiv:2507.13677v1 Announce Type: cross 
Abstract: Real-world Vehicle-to-Everything (V2X) cooperative perception systems often operate under heterogeneous sensor configurations due to cost constraints and deployment variability across vehicles and infrastructure. This heterogeneity poses significant challenges for feature fusion and perception reliability. To address these issues, we propose HeCoFuse, a unified framework designed for cooperative perception across mixed sensor setups where nodes may carry Cameras (C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that adaptively weights features through a combination of channel-wise and spatial attention, HeCoFuse can tackle critical challenges such as cross-modality feature misalignment and imbalanced representation quality. In addition, an adaptive spatial resolution adjustment module is employed to balance computational cost and fusion effectiveness. To enhance robustness across different configurations, we further implement a cooperative learning strategy that dynamically adjusts fusion type based on available modalities. Experiments on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22% 3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine heterogeneous sensor configurations. These results, validated by our first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust performance across diverse sensor deployments.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues</title>
<link>https://arxiv.org/abs/2507.13681</link>
<guid>https://arxiv.org/abs/2507.13681</guid>
<content:encoded><![CDATA[
arXiv:2507.13681v1 Announce Type: cross 
Abstract: Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current acceleration methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. In this paper, we present LoopServe, an adaptive dual-phase inference acceleration framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a \href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new benchmark} with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates LLM inference across a wide range of long-context dialogue tasks.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binarizing Physics-Inspired GNNs for Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2507.13703</link>
<guid>https://arxiv.org/abs/2507.13703</guid>
<content:encoded><![CDATA[
arXiv:2507.13703v1 Announce Type: cross 
Abstract: Physics-inspired graph neural networks (PI-GNNs) have been utilized as an efficient unsupervised framework for relaxing combinatorial optimization problems encoded through a specific graph structure and loss, reflecting dependencies between the problem's variables. While the framework has yielded promising results in various combinatorial problems, we show that the performance of PI-GNNs systematically plummets with an increasing density of the combinatorial problem graphs. Our analysis reveals an interesting phase transition in the PI-GNNs' training dynamics, associated with degenerate solutions for the denser problems, highlighting a discrepancy between the relaxed, real-valued model outputs and the binary-valued problem solutions. To address the discrepancy, we propose principled alternatives to the naive strategy used in PI-GNNs by building on insights from fuzzy logic and binarized neural networks. Our experiments demonstrate that the portfolio of proposed methods significantly improves the performance of PI-GNNs in increasingly dense settings.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point of Interest Recommendation: Pitfalls and Viable Solutions</title>
<link>https://arxiv.org/abs/2507.13725</link>
<guid>https://arxiv.org/abs/2507.13725</guid>
<content:encoded><![CDATA[
arXiv:2507.13725v1 Announce Type: cross 
Abstract: Point of interest (POI) recommendation can play a pivotal role in enriching tourists' experiences by suggesting context-dependent and preference-matching locations and activities, such as restaurants, landmarks, itineraries, and cultural attractions. Unlike some more common recommendation domains (e.g., music and video), POI recommendation is inherently high-stakes: users invest significant time, money, and effort to search, choose, and consume these suggested POIs. Despite the numerous research works in the area, several fundamental issues remain unresolved, hindering the real-world applicability of the proposed approaches. In this paper, we discuss the current status of the POI recommendation problem and the main challenges we have identified. The first contribution of this paper is a critical assessment of the current state of POI recommendation research and the identification of key shortcomings across three main dimensions: datasets, algorithms, and evaluation methodologies. We highlight persistent issues such as the lack of standardized benchmark datasets, flawed assumptions in the problem definition and model design, and inadequate treatment of biases in the user behavior and system performance. The second contribution is a structured research agenda that, starting from the identified issues, introduces important directions for future work related to multistakeholder design, context awareness, data collection, trustworthiness, novel interactions, and real-world evaluation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework</title>
<link>https://arxiv.org/abs/2507.13729</link>
<guid>https://arxiv.org/abs/2507.13729</guid>
<content:encoded><![CDATA[
arXiv:2507.13729v1 Announce Type: cross 
Abstract: Rare, yet critical, scenarios pose a significant challenge in testing and evaluating autonomous driving planners. Relying solely on real-world driving scenes requires collecting massive datasets to capture these scenarios. While automatic generation of traffic scenarios appears promising, data-driven models require extensive training data and often lack fine-grained control over the output. Moreover, generating novel scenarios from scratch can introduce a distributional shift from the original training scenes which undermines the validity of evaluations especially for learning-based planners. To sidestep this, recent work proposes to generate challenging scenarios by augmenting original scenarios from the test set. However, this involves the manual augmentation of scenarios by domain experts. An approach that is unable to meet the demands for scale in the evaluation of self-driving systems. Therefore, this paper introduces a novel LLM-agent based framework for augmenting real-world traffic scenarios using natural language descriptions, addressing the limitations of existing methods. A key innovation is the use of an agentic design, enabling fine-grained control over the output and maintaining high performance even with smaller, cost-effective LLMs. Extensive human expert evaluation demonstrates our framework's ability to accurately adhere to user intent, generating high quality augmented scenarios comparable to those created manually.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2507.13739</link>
<guid>https://arxiv.org/abs/2507.13739</guid>
<content:encoded><![CDATA[
arXiv:2507.13739v1 Announce Type: cross 
Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data; while aiming to reduce catastrophic forgetting and learn new information. We propose Diffusion-FSCIL, a novel approach that employs a text-to-image diffusion model as a frozen backbone. Our conjecture is that FSCIL can be tackled using a large generative model's capabilities benefiting from 1) generation ability via large-scale pre-training; 2) multi-scale representation; 3) representational flexibility through the text encoder. To maximize the representation capability, we propose to extract multiple complementary diffusion features to play roles as latent replay with slight support from feature distillation for preventing generative biases. Our framework realizes efficiency through 1) using a frozen backbone; 2) minimal trainable components; 3) batch processing of multiple feature extractions. Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on previously learned classes and adapting effectively to new ones.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SamGoG: A Sampling-Based Graph-of-Graphs Framework for Imbalanced Graph Classification</title>
<link>https://arxiv.org/abs/2507.13741</link>
<guid>https://arxiv.org/abs/2507.13741</guid>
<content:encoded><![CDATA[
arXiv:2507.13741v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have shown remarkable success in graph classification tasks by capturing both structural and feature-based representations. However, real-world graphs often exhibit two critical forms of imbalance: class imbalance and graph size imbalance. These imbalances can bias the learning process and degrade model performance. Existing methods typically address only one type of imbalance or incur high computational costs. In this work, we propose SamGoG, a sampling-based Graph-of-Graphs (GoG) learning framework that effectively mitigates both class and graph size imbalance. SamGoG constructs multiple GoGs through an efficient importance-based sampling mechanism and trains on them sequentially. This sampling mechanism incorporates the learnable pairwise similarity and adaptive GoG node degree to enhance edge homophily, thus improving downstream model quality. SamGoG can seamlessly integrate with various downstream GNNs, enabling their efficient adaptation for graph classification tasks. Extensive experiments on benchmark datasets demonstrate that SamGoG achieves state-of-the-art performance with up to a 15.66% accuracy improvement with 6.7$\times$ training acceleration.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-Optimized Quantization in Biomedical Ontology Alignment</title>
<link>https://arxiv.org/abs/2507.13742</link>
<guid>https://arxiv.org/abs/2507.13742</guid>
<content:encoded><![CDATA[
arXiv:2507.13742v1 Announce Type: cross 
Abstract: In the fast-moving world of AI, as organizations and researchers develop more advanced models, they face challenges due to their sheer size and computational demands. Deploying such models on edge devices or in resource-constrained environments adds further challenges related to energy consumption, memory usage and latency. To address these challenges, emerging trends are shaping the future of efficient model optimization techniques. From this premise, by employing supervised state-of-the-art transformer-based models, this research introduces a systematic method for ontology alignment, grounded in cosine-based semantic similarity between a biomedical layman vocabulary and the Unified Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to search for target optimizations among different Execution Providers (EPs) using the ONNX Runtime backend, followed by an assembled process of dynamic quantization employing Intel Neural Compressor and IPEX (Intel Extension for PyTorch). Through our optimization process, we conduct extensive assessments on the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new state-of-the-art in both. We retain performance metrics intact, while attaining an average inference speed-up of 20x and reducing memory usage by approximately 70%.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction</title>
<link>https://arxiv.org/abs/2507.13769</link>
<guid>https://arxiv.org/abs/2507.13769</guid>
<content:encoded><![CDATA[
arXiv:2507.13769v1 Announce Type: cross 
Abstract: Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its degraded 2D measurements. Recently great progress has been made in deep learning-based methods, however, these methods often struggle to accurately capture high-frequency details of the HSI. To address this issue, this paper proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from hyperspectral images using a diffusion model. Leveraging the powerful ability of the diffusion model to reconstruct details, this learned prior can significantly improve the performance when injected into the HSI model. To further improve the effectiveness of the learned prior, we also propose the Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover the HSI details. We evaluate our method on two representative HSI methods: MST and BISRNet. Experimental results show that our method outperforms existing networks by about 0.5 dB, effectively improving the performance of HSI reconstruction.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI</title>
<link>https://arxiv.org/abs/2507.13789</link>
<guid>https://arxiv.org/abs/2507.13789</guid>
<content:encoded><![CDATA[
arXiv:2507.13789v1 Announce Type: cross 
Abstract: Hemodynamic analysis is essential for predicting aneurysm rupture and guiding treatment. While magnetic resonance flow imaging enables time-resolved volumetric blood velocity measurements, its low spatiotemporal resolution and signal-to-noise ratio limit its diagnostic utility. To address this, we propose the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that enhances both spatial and temporal resolution with the ability to predict wall shear stress (WSS) directly from clinical imaging data. LoFNO integrates Laplacian eigenvectors as geometric priors for improved structural awareness on irregular, unseen geometries and employs an Enhanced Deep Super-Resolution Network (EDSR) layer for robust upsampling. By combining geometric priors with neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow data, achieving superior velocity and WSS predictions compared to interpolation and alternative deep learning methods, enabling more precise cerebrovascular diagnostics.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion</title>
<link>https://arxiv.org/abs/2507.13801</link>
<guid>https://arxiv.org/abs/2507.13801</guid>
<content:encoded><![CDATA[
arXiv:2507.13801v1 Announce Type: cross 
Abstract: In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a critical perception task for autonomous driving due to its ability to infer complete 3D scene layouts and semantics from single 2D images. However, in real-world traffic scenarios, a significant portion of the scene remains occluded or outside the camera's field of view -- a fundamental challenge that existing monocular SSC methods fail to address adequately. To overcome these limitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC framework that leverages pseudo-future frame prediction to expand the model's effective perceptual range. Our approach combines poses and depths to establish accurate 3D correspondences, enabling geometrically-consistent fusion of past, present, and predicted future frames in 3D space. Unlike conventional methods that rely on simple feature stacking, our 3D-aware architecture achieves more robust scene completion by explicitly modeling spatial-temporal relationships. Comprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks demonstrate state-of-the-art performance, validating the effectiveness of our approach, highlighting our method's ability to improve occlusion reasoning and 3D scene completion accuracy.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database</title>
<link>https://arxiv.org/abs/2507.13802</link>
<guid>https://arxiv.org/abs/2507.13802</guid>
<content:encoded><![CDATA[
arXiv:2507.13802v1 Announce Type: cross 
Abstract: In the European Union, official food safety monitoring data collected by member states are submitted to the European Food Safety Authority (EFSA) and published on Zenodo. This data includes 392 million analytical results derived from over 15.2 million samples covering more than 4,000 different types of food products, offering great opportunities for artificial intelligence to analyze trends, predict hazards, and support early warning systems. However, the current format with data distributed across approximately 1000 files totaling several hundred gigabytes hinders accessibility and analysis. To address this, we introduce the CompreHensive European Food Safety (CHEFS) database, which consolidates EFSA monitoring data on pesticide residues, veterinary medicinal product residues, and chemical contaminants into a unified and structured dataset. We describe the creation and structure of the CHEFS database and demonstrate its potential by analyzing trends in European food safety monitoring data from 2000 to 2024. Our analyses explore changes in monitoring activities, the most frequently tested products, which products were most often non-compliant and which contaminants were most often found, and differences across countries. These findings highlight the CHEFS database as both a centralized data source and a strategic tool for guiding food safety policy, research, and regulation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Team of One: Cracking Complex Video QA with Model Synergy</title>
<link>https://arxiv.org/abs/2507.13820</link>
<guid>https://arxiv.org/abs/2507.13820</guid>
<content:encoded><![CDATA[
arXiv:2507.13820v1 Announce Type: cross 
Abstract: We propose a novel framework for open-ended video question answering that enhances reasoning depth and robustness in complex real-world scenarios, as benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models (Video-LMMs) often exhibit limited contextual understanding, weak temporal modeling, and poor generalization to ambiguous or compositional queries. To address these challenges, we introduce a prompting-and-response integration mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs) via structured chains of thought, each tailored to distinct reasoning pathways. An external Large Language Model (LLM) serves as an evaluator and integrator, selecting and fusing the most reliable responses. Extensive experiments demonstrate that our method significantly outperforms existing baselines across all evaluation metrics, showcasing superior generalization and robustness. Our approach offers a lightweight, extensible strategy for advancing multimodal reasoning without requiring model retraining, setting a strong foundation for future Video-LMM development.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG-based Architectures for Drug Side Effect Retrieval in LLMs</title>
<link>https://arxiv.org/abs/2507.13822</link>
<guid>https://arxiv.org/abs/2507.13822</guid>
<content:encoded><![CDATA[
arXiv:2507.13822v1 Announce Type: cross 
Abstract: Drug side effects are a major global health concern, necessitating advanced methods for their accurate detection and analysis. While Large Language Models (LLMs) offer promising conversational interfaces, their inherent limitations, including reliance on black-box training data, susceptibility to hallucinations, and lack of domain-specific knowledge, hinder their reliability in specialized fields like pharmacovigilance. To address this gap, we propose two architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which integrate comprehensive drug side effect knowledge into a Llama 3 8B language model. Through extensive evaluations on 19,520 drug side effect associations (covering 976 drugs and 3,851 side effect terms), our results demonstrate that GraphRAG achieves near-perfect accuracy in drug side effect retrieval. This framework offers a highly accurate and scalable solution, signifying a significant advancement in leveraging LLMs for critical pharmacovigilance applications.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Submodular Policy Optimization via Pruned Submodularity Graph</title>
<link>https://arxiv.org/abs/2507.13834</link>
<guid>https://arxiv.org/abs/2507.13834</guid>
<content:encoded><![CDATA[
arXiv:2507.13834v1 Announce Type: cross 
Abstract: In Reinforcement Learning (abbreviated as RL), an agent interacts with the environment via a set of possible actions, and a reward is generated from some unknown distribution. The task here is to find an optimal set of actions such that the reward after a certain time step gets maximized. In a traditional setup, the reward function in an RL Problem is considered additive. However, in reality, there exist many problems, including path planning, coverage control, etc., the reward function follows the diminishing return, which can be modeled as a submodular function. In this paper, we study a variant of the RL Problem where the reward function is submodular, and our objective is to find an optimal policy such that this reward function gets maximized. We have proposed a pruned submodularity graph-based approach that provides a provably approximate solution in a feasible computation time. The proposed approach has been analyzed to understand its time and space requirements as well as a performance guarantee. We have experimented with a benchmark agent-environment setup, which has been used for similar previous studies, and the results are reported. From the results, we observe that the policy obtained by our proposed approach leads to more reward than the baseline methods.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection</title>
<link>https://arxiv.org/abs/2507.13859</link>
<guid>https://arxiv.org/abs/2507.13859</guid>
<content:encoded><![CDATA[
arXiv:2507.13859v1 Announce Type: cross 
Abstract: Nowadays, the importance of software with natural-language user interfaces cannot be underestimated. In particular, in Question Answering (QA) systems, generating a SPARQL query for a given natural-language question (often named Query Building) from the information retrieved from the same question is the central task of QA systems working over Knowledge Graphs (KGQA). Due to the rise of Large Language Models (LLMs), they are considered a well-suited method to increase the quality of the question-answering functionality, as there is still a lot of room for improvement, aiming for enhanced quality and trustworthiness. However, LLMs are trained on web data, where researchers have no control over whether the benchmark or the knowledge graph was already included in the training data. In this paper, we introduce a novel method that evaluates the quality of LLMs by generating a SPARQL query from a natural-language question under various conditions: (1) zero-shot SPARQL generation, (2) with knowledge injection, and (3) with "anonymized" knowledge injection. This enables us, for the first time, to estimate the influence of the training data on the QA quality improved by LLMs. Ultimately, this will help to identify how portable a method is or whether good results might mostly be achieved because a benchmark was already included in the training data (cf. LLM memorization). The developed method is portable, robust, and supports any knowledge graph; therefore, it could be easily applied to any KGQA or LLM, s.t., generating consistent insights into the actual LLM capabilities is possible.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models</title>
<link>https://arxiv.org/abs/2507.13868</link>
<guid>https://arxiv.org/abs/2507.13868</guid>
<content:encoded><![CDATA[
arXiv:2507.13868v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) increasingly leverage diverse knowledge sources to address complex tasks, often encountering conflicts between their internal parametric knowledge and external information. Knowledge conflicts can result in hallucinations and unreliable responses, but the mechanisms governing such interactions remain unknown. To address this gap, we analyze the mechanisms that VLMs use to resolve cross-modal conflicts by introducing a dataset of multimodal counterfactual queries that deliberately contradict internal commonsense knowledge. We localize with logit inspection a small set of heads that control the conflict. Moreover, by modifying these heads, we can steer the model towards its internal knowledge or the visual inputs. Finally, we show that attention from such heads pinpoints localized image regions driving visual overrides, outperforming gradient-based attribution in precision.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision</title>
<link>https://arxiv.org/abs/2507.13880</link>
<guid>https://arxiv.org/abs/2507.13880</guid>
<content:encoded><![CDATA[
arXiv:2507.13880v1 Announce Type: cross 
Abstract: This paper presents a novel approach to enhancing marine vision by fusing real-time visual data with chart information. Our system overlays nautical chart data onto live video feeds by accurately matching detected navigational aids, such as buoys, with their corresponding representations in chart data. To achieve robust association, we introduce a transformer-based end-to-end neural network that predicts bounding boxes and confidence scores for buoy queries, enabling the direct matching of image-domain detections with world-space chart markers. The proposed method is compared against baseline approaches, including a ray-casting model that estimates buoy positions via camera projection and a YOLOv7-based network extended with a distance estimation module. Experimental results on a dataset of real-world maritime scenes demonstrate that our approach significantly improves object localization and association accuracy in dynamic and challenging environments.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using LLMs to identify features of personal and professional skills in an open-response situational judgment test</title>
<link>https://arxiv.org/abs/2507.13881</link>
<guid>https://arxiv.org/abs/2507.13881</guid>
<content:encoded><![CDATA[
arXiv:2507.13881v1 Announce Type: cross 
Abstract: Academic programs are increasingly recognizing the importance of personal and professional skills and their critical role alongside technical expertise in preparing students for future success in diverse career paths. With this growing demand comes the need for scalable systems to measure, evaluate, and develop these skills. Situational Judgment Tests (SJTs) offer one potential avenue for measuring these skills in a standardized and reliable way, but open-response SJTs have traditionally relied on trained human raters for evaluation, presenting operational challenges to delivering SJTs at scale. Past attempts at developing NLP-based scoring systems for SJTs have fallen short due to issues with construct validity of these systems. In this article, we explore a novel approach to extracting construct-relevant features from SJT responses using large language models (LLMs). We use the Casper SJT to demonstrate the efficacy of this approach. This study sets the foundation for future developments in automated scoring for personal and professional skills.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised learning on gene expression data</title>
<link>https://arxiv.org/abs/2507.13912</link>
<guid>https://arxiv.org/abs/2507.13912</guid>
<content:encoded><![CDATA[
arXiv:2507.13912v1 Announce Type: cross 
Abstract: Predicting phenotypes from gene expression data is a crucial task in biomedical research, enabling insights into disease mechanisms, drug responses, and personalized medicine. Traditional machine learning and deep learning rely on supervised learning, which requires large quantities of labeled data that are costly and time-consuming to obtain in the case of gene expression data. Self-supervised learning has recently emerged as a promising approach to overcome these limitations by extracting information directly from the structure of unlabeled data. In this study, we investigate the application of state-of-the-art self-supervised learning methods to bulk gene expression data for phenotype prediction. We selected three self-supervised methods, based on different approaches, to assess their ability to exploit the inherent structure of the data and to generate qualitative representations which can be used for downstream predictive tasks. By using several publicly available gene expression datasets, we demonstrate how the selected methods can effectively capture complex information and improve phenotype prediction accuracy. The results obtained show that self-supervised learning methods can outperform traditional supervised models besides offering significant advantage by reducing the dependency on annotated data. We provide a comprehensive analysis of the performance of each method by highlighting their strengths and limitations. We also provide recommendations for using these methods depending on the case under study. Finally, we outline future research directions to enhance the application of self-supervised learning in the field of gene expression data analysis. This study is the first work that deals with bulk RNA-Seq data and self-supervised learning.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Political Leaning and Politicalness Classification of Texts</title>
<link>https://arxiv.org/abs/2507.13913</link>
<guid>https://arxiv.org/abs/2507.13913</guid>
<content:encoded><![CDATA[
arXiv:2507.13913v1 Announce Type: cross 
Abstract: This paper addresses the challenge of automatically classifying text according to political leaning and politicalness using transformer models. We compose a comprehensive overview of existing datasets and models for these tasks, finding that current approaches create siloed solutions that perform poorly on out-of-distribution texts. To address this limitation, we compile a diverse dataset by combining 12 datasets for political leaning classification and creating a new dataset for politicalness by extending 18 existing datasets with the appropriate label. Through extensive benchmarking with leave-one-in and leave-one-out methodologies, we evaluate the performance of existing models and train new ones with enhanced generalization capabilities.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Levers of Political Persuasion with Conversational AI</title>
<link>https://arxiv.org/abs/2507.13919</link>
<guid>https://arxiv.org/abs/2507.13919</guid>
<content:encoded><![CDATA[
arXiv:2507.13919v1 Announce Type: cross 
Abstract: There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preprint: Did I Just Browse A Website Written by LLMs?</title>
<link>https://arxiv.org/abs/2507.13933</link>
<guid>https://arxiv.org/abs/2507.13933</guid>
<content:encoded><![CDATA[
arXiv:2507.13933v1 Announce Type: cross 
Abstract: Increasingly, web content is automatically generated by large language models (LLMs) with little human input. We call this "LLM-dominant" content. Since LLMs plagiarize and hallucinate, LLM-dominant content can be unreliable and unethical. Yet, websites rarely disclose such content, and human readers struggle to distinguish it. Thus, we must develop reliable detectors for LLM-dominant content. However, state-of-the-art LLM detectors are insufficient, because they perform well mainly on clean, prose-like text, while web content has complex markup and diverse genres.
  We propose a highly reliable, scalable pipeline that classifies entire websites. Instead of naively classifying text extracted from each page, we classify each site based on an LLM text detector's outputs of multiple prose-like pages. We train and evaluate our detector by collecting 2 distinct ground truth datasets totaling 120 sites, and obtain 100% accuracies testing across them. In the wild, we detect a sizable portion of sites as LLM-dominant among 10k sites in search engine results and 10k in Common Crawl archives. We find LLM-dominant sites are growing in prevalence and rank highly in search results, raising questions about their impact on end users and the overall Web ecosystem.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergent transformations of visual representation in brains and models</title>
<link>https://arxiv.org/abs/2507.13941</link>
<guid>https://arxiv.org/abs/2507.13941</guid>
<content:encoded><![CDATA[
arXiv:2507.13941v1 Announce Type: cross 
Abstract: A fundamental question in cognitive neuroscience is what shapes visual perception: the external world's structure or the brain's internal architecture. Although some perceptual variability can be traced to individual differences, brain responses to naturalistic stimuli evoke similar activity patterns across individuals, suggesting a convergent representational principle. Here, we test if this stimulus-driven convergence follows a common trajectory across people and deep neural networks (DNNs) during its transformation from sensory to high-level internal representations. We introduce a unified framework that traces representational flow by combining inter-subject similarity with alignment to model hierarchies. Applying this framework to three independent fMRI datasets of visual scene perception, we reveal a cortex-wide network, conserved across individuals, organized into two pathways: a medial-ventral stream for scene structure and a lateral-dorsal stream tuned for social and biological content. This functional organization is captured by the hierarchies of vision DNNs but not language models, reinforcing the specificity of the visual-to-semantic transformation. These findings show a convergent computational solution for visual encoding in both human and artificial vision, driven by the structure of the external world.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalist Forecasting with Frozen Video Models via Latent Diffusion</title>
<link>https://arxiv.org/abs/2507.13942</link>
<guid>https://arxiv.org/abs/2507.13942</guid>
<content:encoded><![CDATA[
arXiv:2507.13942v1 Announce Type: cross 
Abstract: Forecasting what will happen next is a critical skill for general-purpose systems that plan or act in the world at different levels of abstraction. In this paper, we identify a strong correlation between a vision model's perceptual ability and its generalist forecasting performance over short time horizons. This trend holds across a diverse set of pretrained models-including those trained generatively-and across multiple levels of abstraction, from raw pixels to depth, point tracks, and object motion. The result is made possible by a novel generalist forecasting framework that operates on any frozen vision backbone: we train latent diffusion models to forecast future features in the frozen representation space, which are then decoded via lightweight, task-specific readouts. To enable consistent evaluation across tasks, we introduce distributional metrics that compare distributional properties directly in the space of downstream tasks and apply this framework to nine models and four tasks. Our results highlight the value of bridging representation learning and generative modeling for temporally grounded video understanding.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Primacy Effect To Improve Large Language Models</title>
<link>https://arxiv.org/abs/2507.13949</link>
<guid>https://arxiv.org/abs/2507.13949</guid>
<content:encoded><![CDATA[
arXiv:2507.13949v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become essential in many Natural Language Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to achieve high accuracy. However, like humans, LLMs exhibit biases, particularly positional biases such as primacy and recency effects, which can influence the accuracy of the answers. The primacy effect-where items presented first are more likely to be remembered or selected-plays a key role in Multiple Choice Question Answering (MCQA), where the order of answer options can affect prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We first show that fine-tuning amplifies this bias, probably due to exposure to human-like patterns. Hence, we strategically leverage this effect by reordering response options based on semantic similarity to the query, without requiring knowledge of the correct answer. Our experimental results show that this approach significantly improves performance in MCQA. More generally, our findings underscore the dual nature of biases as both challenges and opportunities, offering insights for bias-aware model design and NLP applications.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation</title>
<link>https://arxiv.org/abs/2507.13957</link>
<guid>https://arxiv.org/abs/2507.13957</guid>
<content:encoded><![CDATA[
arXiv:2507.13957v1 Announce Type: cross 
Abstract: The modern recommender systems are facing an increasing challenge of modelling and predicting the dynamic and context-rich user preferences. Traditional collaborative filtering and content-based methods often struggle to capture the temporal patternings and evolving user intentions. While Large Language Models (LLMs) have gained gradual attention in recent years, by their strong semantic understanding and reasoning abilities, they are not inherently designed to model chronologically evolving user preference and intentions. On the other hand, for sequential models like LSTM (Long-Short-Term-Memory) which is good at capturing the temporal dynamics of user behaviour and evolving user preference over time, but still lacks a rich semantic understanding for comprehensive recommendation generation. In this study, we propose DUALRec (Dynamic User-Aware Language-based Recommender), a novel recommender that leverages the complementary strength of both models, which combines the temporal modelling abilities of LSTM networks with semantic reasoning power of the fine-tuned Large Language Models. The LSTM component will capture users evolving preference through their viewing history, while the fine-tuned LLM variants will leverage these temporal user insights to generate next movies that users might enjoy. Experimental results on MovieLens-1M dataset shows that the DUALRec model outperforms a wide range of baseline models, with comprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted Cumulative Gain (NDCG@k), and genre similarity metrics. This research proposes a novel architecture that bridges the gap between temporal sequence modeling and semantic reasoning, and offers a promising direction for developing more intelligent and context-aware recommenders.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need</title>
<link>https://arxiv.org/abs/2507.13966</link>
<guid>https://arxiv.org/abs/2507.13966</guid>
<content:encoded><![CDATA[
arXiv:2507.13966v1 Announce Type: cross 
Abstract: Language models traditionally used for cross-domain generalization have recently demonstrated task-specific reasoning. However, their top-down training approach on general corpora is insufficient for acquiring abstractions needed for deep domain expertise. This may require a bottom-up approach that acquires expertise by learning to compose simple domain concepts into more complex ones. A knowledge graph (KG) provides this compositional structure, where domain primitives are represented as head-relation-tail edges and their paths encode higher-level concepts. We present a task generation pipeline that synthesizes tasks directly from KG primitives, enabling models to acquire and compose them for reasoning. We fine-tune language models on the resultant KG-grounded curriculum to demonstrate domain-specific superintelligence. While broadly applicable, we validate our approach in medicine, where reliable KGs exist. Using a medical KG, we curate 24,000 reasoning tasks paired with thinking traces derived from diverse medical primitives. We fine-tune the QwQ-32B model on this curriculum to obtain QwQ-Med-3 that takes a step towards medical superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify reasoning abilities across 15 medical domains. Our experiments demonstrate that QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired primitives to widen the performance gap on the hardest tasks of ICD-Bench. Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3 transfers acquired expertise to enhance the base model's performance. While the industry's approach to artificial general intelligence (AGI) emphasizes broad expertise, we envision a future in which AGI emerges from the composable interaction of efficient domain-specific superintelligent agents.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A segmented robot grasping perception neural network for edge AI</title>
<link>https://arxiv.org/abs/2507.13970</link>
<guid>https://arxiv.org/abs/2507.13970</guid>
<content:encoded><![CDATA[
arXiv:2507.13970v1 Announce Type: cross 
Abstract: Robotic grasping, the ability of robots to reliably secure and manipulate objects of varying shapes, sizes and orientations, is a complex task that requires precise perception and control. Deep neural networks have shown remarkable success in grasp synthesis by learning rich and abstract representations of objects. When deployed at the edge, these models can enable low-latency, low-power inference, making real-time grasping feasible in resource-constrained environments. This work implements Heatmap-Guided Grasp Detection, an end-to-end framework for the detection of 6-Dof grasp poses, on the GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware techniques, including input dimensionality reduction, model partitioning, and quantisation. Experimental evaluation on the GraspNet-1Billion benchmark validates the feasibility of fully on-chip inference, highlighting the potential of low-power MCUs for real-time, autonomous manipulation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models</title>
<link>https://arxiv.org/abs/2507.13984</link>
<guid>https://arxiv.org/abs/2507.13984</guid>
<content:encoded><![CDATA[
arXiv:2507.13984v1 Announce Type: cross 
Abstract: Disentangling content and style from a single image, known as content-style decomposition (CSD), enables recontextualization of extracted content and stylization of extracted styles, offering greater creative flexibility in visual synthesis. While recent personalization methods have explored the decomposition of explicit content style, they remain tailored for diffusion models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a promising alternative with a next-scale prediction paradigm, achieving performance comparable to that of diffusion models. In this paper, we explore VAR as a generative framework for CSD, leveraging its scale-wise generation process for improved disentanglement. To this end, we propose CSD-VAR, a novel method that introduces three key innovations: (1) a scale-aware alternating optimization strategy that aligns content and style representation with their respective scales to enhance separation, (2) an SVD-based rectification method to mitigate content leakage into style representations, and (3) an Augmented Key-Value (K-V) memory enhancing content identity preservation. To benchmark this task, we introduce CSD-100, a dataset specifically designed for content-style decomposition, featuring diverse subjects rendered in various artistic styles. Experiments demonstrate that CSD-VAR outperforms prior approaches, achieving superior content preservation and stylization fidelity.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models</title>
<link>https://arxiv.org/abs/2507.13993</link>
<guid>https://arxiv.org/abs/2507.13993</guid>
<content:encoded><![CDATA[
arXiv:2507.13993v1 Announce Type: cross 
Abstract: The growing volume of medical imaging data has increased the need for automated diagnostic tools, especially for musculoskeletal injuries like rib fractures, commonly detected via CT scans. Manual interpretation is time-consuming and error-prone. We propose OrthoInsight, a multi-modal deep learning framework for rib fracture diagnosis and report generation. It integrates a YOLOv9 model for fracture detection, a medical knowledge graph for retrieving clinical context, and a fine-tuned LLaVA language model for generating diagnostic reports. OrthoInsight combines visual features from CT images with expert textual data to deliver clinically useful outputs. Evaluated on 28,675 annotated CT images and expert reports, it achieves high performance across Diagnostic Accuracy, Content Completeness, Logical Coherence, and Clinical Guidance Value, with an average score of 4.28, outperforming models like GPT-4 and Claude-3. This study demonstrates the potential of multi-modal learning in transforming medical image analysis and providing effective support for radiologists.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Photonic Fabric Platform for AI Accelerators</title>
<link>https://arxiv.org/abs/2507.14000</link>
<guid>https://arxiv.org/abs/2507.14000</guid>
<content:encoded><![CDATA[
arXiv:2507.14000v1 Announce Type: cross 
Abstract: This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM (PFA), a photonic-enabled switch and memory subsystem that delivers low latency, high bandwidth, and low per-bit energy. By integrating high-bandwidth HBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D electro-optical system-in-package, the PFA offers up to 32 TB of shared memory alongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM enables distributed AI training and inference to execute parallelism strategies more efficiently. The Photonic Fabric removes the silicon beachfront constraint that limits the fixed memory-to-compute ratio observed in virtually all current XPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet that connects to the Photonic Fabric increases its memory capacity and correspondingly its memory bandwidth by offering a flexible path to scaling well beyond the limitations of on-package HBM alone. We introduce CelestiSim, a lightweight analytical simulator validated on NVIDIA H100 and H200 systems. It is used to evaluate the performance of LLM reference and energy savings on PFA, without any significant change to the GPU core design. With the PFA, the simulation results show that up to 3.66x throughput and 1.40x latency improvements in LLM inference at 405B parameters, up to 7.04x throughput and 1.41x latency improvements at 1T parameters, and 60-90% energy savings in data movement for heavy collective operations in all LLM training scenarios. While these results are shown for NVIDIA GPUs, they can be applied similarly to other AI accelerator designs (XPUs) that share the same fundamental limitation of fixed memory to compute.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems</title>
<link>https://arxiv.org/abs/2507.14043</link>
<guid>https://arxiv.org/abs/2507.14043</guid>
<content:encoded><![CDATA[
arXiv:2507.14043v1 Announce Type: cross 
Abstract: Metaheuristic algorithms have gained widespread application across various fields owing to their ability to generate diverse solutions. One such algorithm is the Snake Optimizer (SO), a progressive optimization approach. However, SO suffers from the issues of slow convergence speed and susceptibility to local optima. In light of these shortcomings, we propose a novel Multi-strategy Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random disturbance strategy based on sine function to alleviate the risk of getting trapped in a local optimum. Secondly, we introduce adaptive Levy flight strategy based on scale factor and leader and endow the male snake leader with flight capability, which makes it easier for the algorithm to leap out of the local optimum and find the global optimum. More importantly, we put forward a position update strategy combining elite leadership and Brownian motion, effectively accelerating the convergence speed while ensuring precision. Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test functions and the CEC2022 test suite, comparing it with 11 popular algorithms across different dimensions to validate its effectiveness. Moreover, Unmanned Aerial Vehicle (UAV) has been widely used in various fields due to its advantages of low cost, high mobility and easy operation. However, the UAV path planning problem is crucial for flight safety and efficiency, and there are still challenges in establishing and optimizing the path model. Therefore, we apply MISO to the UAV 3D path planning problem as well as 6 engineering design problems to assess its feasibility in practical applications. The experimental results demonstrate that MISO exceeds other competitive algorithms in terms of solution quality and stability, establishing its strong potential for application.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noradrenergic-inspired gain modulation attenuates the stability gap in joint training</title>
<link>https://arxiv.org/abs/2507.14056</link>
<guid>https://arxiv.org/abs/2507.14056</guid>
<content:encoded><![CDATA[
arXiv:2507.14056v1 Announce Type: cross 
Abstract: Recent studies in continual learning have identified a transient drop in performance on mastered tasks when assimilating new ones, known as the stability gap. Such dynamics contradict the objectives of continual learning, revealing a lack of robustness in mitigating forgetting, and notably, persisting even under an ideal joint-loss regime. Examining this gap within this idealized joint training context is critical to isolate it from other sources of forgetting. We argue that it reflects an imbalance between rapid adaptation and robust retention at task boundaries, underscoring the need to investigate mechanisms that reconcile plasticity and stability within continual learning frameworks. Biological brains navigate a similar dilemma by operating concurrently on multiple timescales, leveraging neuromodulatory signals to modulate synaptic plasticity. However, artificial networks lack native multitimescale dynamics, and although optimizers like momentum-SGD and Adam introduce implicit timescale regularization, they still exhibit stability gaps. Inspired by locus coeruleus mediated noradrenergic bursts, which transiently enhance neuronal gain under uncertainty to facilitate sensory assimilation, we propose uncertainty-modulated gain dynamics - an adaptive mechanism that approximates a two-timescale optimizer and dynamically balances integration of knowledge with minimal interference on previously consolidated information. We evaluate our mechanism on domain-incremental and class-incremental variants of the MNIST and CIFAR benchmarks under joint training, demonstrating that uncertainty-modulated gain dynamics effectively attenuate the stability gap. Finally, our analysis elucidates how gain modulation replicates noradrenergic functions in cortical circuits, offering mechanistic insights into reducing stability gaps and enhance performance in continual learning tasks.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLA-Mark: A cross modal watermark for large vision-language alignment model</title>
<link>https://arxiv.org/abs/2507.14067</link>
<guid>https://arxiv.org/abs/2507.14067</guid>
<content:encoded><![CDATA[
arXiv:2507.14067v1 Announce Type: cross 
Abstract: Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semantic-critical concepts vulnerable. We propose VLA-Mark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visual-textual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than conventional methods, with near-perfect detection (98.8% AUC). The framework demonstrates 96.1\% attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for quality-preserving multimodal watermarking
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge Intelligence with Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2507.14069</link>
<guid>https://arxiv.org/abs/2507.14069</guid>
<content:encoded><![CDATA[
arXiv:2507.14069v1 Announce Type: cross 
Abstract: The convergence of artificial intelligence and edge computing has spurred growing interest in enabling intelligent services directly on resource-constrained devices. While traditional deep learning models require significant computational resources and centralized data management, the resulting latency, bandwidth consumption, and privacy concerns have exposed critical limitations in cloud-centric paradigms. Brain-inspired computing, particularly Spiking Neural Networks (SNNs), offers a promising alternative by emulating biological neuronal dynamics to achieve low-power, event-driven computation. This survey provides a comprehensive overview of Edge Intelligence based on SNNs (EdgeSNNs), examining their potential to address the challenges of on-device learning, inference, and security in edge scenarios. We present a systematic taxonomy of EdgeSNN foundations, encompassing neuron models, learning algorithms, and supporting hardware platforms. Three representative practical considerations of EdgeSNN are discussed in depth: on-device inference using lightweight SNN models, resource-aware training and updating under non-stationary data conditions, and secure and privacy-preserving issues. Furthermore, we highlight the limitations of evaluating EdgeSNNs on conventional hardware and introduce a dual-track benchmarking strategy to support fair comparisons and hardware-aware optimization. Through this study, we aim to bridge the gap between brain-inspired learning and practical edge deployment, offering insights into current advancements, open challenges, and future research directions. To the best of our knowledge, this is the first dedicated and comprehensive survey on EdgeSNNs, providing an essential reference for researchers and practitioners working at the intersection of neuromorphic computing and edge intelligence.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits</title>
<link>https://arxiv.org/abs/2507.14079</link>
<guid>https://arxiv.org/abs/2507.14079</guid>
<content:encoded><![CDATA[
arXiv:2507.14079v1 Announce Type: cross 
Abstract: Progress notes are among the most clinically meaningful artifacts in an Electronic Health Record (EHR), offering temporally grounded insights into a patient's evolving condition, treatments, and care decisions. Despite their importance, they are severely underrepresented in large-scale EHR datasets. For instance, in the widely used Medical Information Mart for Intensive Care III (MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress notes, leaving gaps in longitudinal patient narratives. In contrast, the dataset contains a diverse array of other note types, each capturing different aspects of care.
  We present DENSE (Documenting Evolving Progress Notes from Scattered Evidence), a system designed to align with clinical documentation workflows by simulating how physicians reference past encounters while drafting progress notes. The system introduces a fine-grained note categorization and a temporal alignment mechanism that organizes heterogeneous notes across visits into structured, chronological inputs. At its core, DENSE leverages a clinically informed retrieval strategy to identify temporally and semantically relevant content from both current and prior visits. This retrieved evidence is used to prompt a large language model (LLM) to generate clinically coherent and temporally aware progress notes.
  We evaluate DENSE on a curated cohort of patients with multiple visits and complete progress note documentation. The generated notes demonstrate strong longitudinal fidelity, achieving a temporal alignment ratio of $1.089$, surpassing the continuity observed in original notes. By restoring narrative coherence across fragmented documentation, our system supports improved downstream tasks such as summarization, predictive modeling, and clinical decision support, offering a scalable solution for LLM-driven note synthesis in real-world healthcare settings.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?</title>
<link>https://arxiv.org/abs/2507.14084</link>
<guid>https://arxiv.org/abs/2507.14084</guid>
<content:encoded><![CDATA[
arXiv:2507.14084v1 Announce Type: cross 
Abstract: Humans have a selective memory, remembering relevant episodes and forgetting the less relevant information. Possessing awareness of event memorability for a user could help intelligent systems in more accurate user modelling, especially for such applications as meeting support systems, memory augmentation, and meeting summarisation. Emotion recognition has been widely studied, since emotions are thought to signal moments of high personal relevance to users. The emotional experience of situations and their memorability have traditionally been considered to be closely tied to one another: moments that are experienced as highly emotional are considered to also be highly memorable. This relationship suggests that emotional annotations could serve as proxies for memorability. However, existing emotion recognition systems rely heavily on third-party annotations, which may not accurately represent the first-person experience of emotional relevance and memorability. This is why, in this study, we empirically examine the relationship between perceived group emotions (Pleasure-Arousal) and group memorability in the context of conversational interactions. Our investigation involves continuous time-based annotations of both emotions and memorability in dynamic, unstructured group settings, approximating conditions of real-world conversational AI applications such as online meeting support systems. Our results show that the observed relationship between affect and memorability annotations cannot be reliably distinguished from what might be expected under random chance. We discuss the implications of this surprising finding for the development and applications of Affective Computing technology. In addition, we contextualise our findings in broader discourses in the Affective Computing and point out important targets for future research efforts.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment</title>
<link>https://arxiv.org/abs/2507.14093</link>
<guid>https://arxiv.org/abs/2507.14093</guid>
<content:encoded><![CDATA[
arXiv:2507.14093v1 Announce Type: cross 
Abstract: Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment decisions depend on precise Cobb angle measurement. Manual assessment is time consuming and subject to inter observer variation. We conducted a retrospective, multi centre evaluation of a fully automated deep learning software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on 103 standing anteroposterior whole spine radiographs collected from ten hospitals. Two musculoskeletal radiologists independently measured each study and served as reference readers. Agreement between the AI and each radiologist was assessed with Bland Altman analysis, mean absolute error (MAE), root mean squared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four grade severity classification. Against Radiologist 1 the AI achieved an MAE of 3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of agreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI achieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees and limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r equals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen kappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59). These results demonstrate that the proposed software reproduces expert level Cobb angle measurements and categorical grading across multiple centres, suggesting its utility for streamlining scoliosis reporting and triage in clinical workflows.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track</title>
<link>https://arxiv.org/abs/2507.14096</link>
<guid>https://arxiv.org/abs/2507.14096</guid>
<content:encoded><![CDATA[
arXiv:2507.14096v1 Announce Type: cross 
Abstract: Objective: Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary. Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems.
  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2). For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references. Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts.
  Results: Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers. In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity. Automatic, reference-based metrics generally did not correlate well with manual judgments. In Task 2, systems struggled with identifying difficult terms and classifying how to replace them. When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity.
  Conclusion: The PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining</title>
<link>https://arxiv.org/abs/2507.14119</link>
<guid>https://arxiv.org/abs/2507.14119</guid>
<content:encoded><![CDATA[
arXiv:2507.14119v1 Announce Type: cross 
Abstract: Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets: original image, instruction, edited image. Yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approximately 2.2x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit: an open dataset of 358k high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an open-source fine-tuned Bagel model, which achieves state-of-the-art metrics in our experiments.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective</title>
<link>https://arxiv.org/abs/2507.14121</link>
<guid>https://arxiv.org/abs/2507.14121</guid>
<content:encoded><![CDATA[
arXiv:2507.14121v1 Announce Type: cross 
Abstract: Kolmogorov Arnold Networks (KANs) are recent architectural advancement in neural computation that offer a mathematically grounded alternative to standard neural networks. This study presents an empirical evaluation of KANs in context of class imbalanced classification, using ten benchmark datasets. We observe that KANs can inherently perform well on raw imbalanced data more effectively than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However, conventional imbalance strategies fundamentally conflict with KANs mathematical structure as resampling and focal loss implementations significantly degrade KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from prohibitive computational costs without proportional performance gains. Statistical validation confirms that MLPs with imbalance techniques achieve equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs. These findings reveal that KANs represent a specialized solution for raw imbalanced data where resources permit. But their severe performance-resource tradeoffs and incompatibility with standard resampling techniques currently limits practical deployment. We identify critical research priorities as developing KAN specific architectural modifications for imbalance learning, optimizing computational efficiency, and theoretical reconciling their conflict with data augmentation. This work establishes foundational insights for next generation KAN architectures in imbalanced classification scenarios.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Temporal Causal Representation Learning with Tensor Decomposition</title>
<link>https://arxiv.org/abs/2507.14126</link>
<guid>https://arxiv.org/abs/2507.14126</guid>
<content:encoded><![CDATA[
arXiv:2507.14126v1 Announce Type: cross 
Abstract: Temporal causal representation learning is a powerful tool for uncovering complex patterns in observational studies, which are often represented as low-dimensional time series. However, in many real-world applications, data are high-dimensional with varying input lengths and naturally take the form of irregular tensors. To analyze such data, irregular tensor decomposition is critical for extracting meaningful clusters that capture essential information. In this paper, we focus on modeling causal representation learning based on the transformed information. First, we present a novel causal formulation for a set of latent clusters. We then propose CaRTeD, a joint learning framework that integrates temporal causal representation learning with irregular tensor decomposition. Notably, our framework provides a blueprint for downstream tasks using the learned tensor factors, such as modeling latent structures and extracting causal information, and offers a more flexible regularization design to enhance tensor decomposition. Theoretically, we show that our algorithm converges to a stationary point. More importantly, our results fill the gap in theoretical guarantees for the convergence of state-of-the-art irregular tensor decomposition. Experimental results on synthetic and real-world electronic health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both phenotyping and network recovery perspectives, demonstrate that our proposed method outperforms state-of-the-art techniques and enhances the explainability of causal representations.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CorMulT: A Semi-supervised Modality Correlation-aware Multimodal Transformer for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2407.07046</link>
<guid>https://arxiv.org/abs/2407.07046</guid>
<content:encoded><![CDATA[
arXiv:2407.07046v3 Announce Type: replace 
Abstract: Multimodal sentiment analysis is an active research area that combines multiple data modalities, e.g., text, image and audio, to analyze human emotions and benefits a variety of applications. Existing multimodal sentiment analysis methods can be classified as modality interaction-based methods, modality transformation-based methods and modality similarity-based methods. However, most of these methods highly rely on the strong correlations between modalities, and cannot fully uncover and utilize the correlations between modalities to enhance sentiment analysis. Therefore, these methods usually achieve bad performance for identifying the sentiment of multimodal data with weak correlations. To address this issue, we proposed a two-stage semi-supervised model termed Correlation-aware Multimodal Transformer (CorMulT) which consists pre-training stage and prediction stage. At the pre-training stage, a modality correlation contrastive learning module is designed to efficiently learn modality correlation coefficients between different modalities. At the prediction stage, the learned correlation coefficients are fused with modality representations to make the sentiment prediction. According to the experiments on the popular multimodal dataset CMU-MOSEI, CorMulT obviously surpasses state-of-the-art multimodal sentiment analysis methods.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniEmoX: Cross-modal Semantic-Guided Large-Scale Pretraining for Universal Scene Emotion Perception</title>
<link>https://arxiv.org/abs/2409.18877</link>
<guid>https://arxiv.org/abs/2409.18877</guid>
<content:encoded><![CDATA[
arXiv:2409.18877v3 Announce Type: replace 
Abstract: Visual emotion analysis holds significant research value in both computer vision and psychology. However, existing methods for visual emotion analysis suffer from limited generalizability due to the ambiguity of emotion perception and the diversity of data scenarios. To tackle this issue, we introduce UniEmoX, a cross-modal semantic-guided large-scale pretraining framework. Inspired by psychological research emphasizing the inseparability of the emotional exploration process from the interaction between individuals and their environment, UniEmoX integrates scene-centric and person-centric low-level image spatial structural information, aiming to derive more nuanced and discriminative emotional representations. By exploiting the similarity between paired and unpaired image-text samples, UniEmoX distills rich semantic knowledge from the CLIP model to enhance emotional embedding representations more effectively. To the best of our knowledge, this is the first large-scale pretraining framework that integrates psychological theories with contemporary contrastive learning and masked image modeling techniques for emotion analysis across diverse scenarios. Additionally, we develop a visual emotional dataset titled Emo8. Emo8 samples cover a range of domains, including cartoon, natural, realistic, science fiction and advertising cover styles, covering nearly all common emotional scenes. Comprehensive experiments conducted on six benchmark datasets across two downstream tasks validate the effectiveness of UniEmoX. The source code is available at https://github.com/chincharles/u-emo.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLAST: A Stealthy Backdoor Leverage Attack against Cooperative Multi-Agent Deep Reinforcement Learning based Systems</title>
<link>https://arxiv.org/abs/2501.01593</link>
<guid>https://arxiv.org/abs/2501.01593</guid>
<content:encoded><![CDATA[
arXiv:2501.01593v2 Announce Type: replace 
Abstract: Recent studies have shown that cooperative multi-agent deep reinforcement learning (c-MADRL) is under the threat of backdoor attacks. Once a backdoor trigger is observed, it will perform malicious actions leading to failures or malicious goals. However, existing backdoor attacks suffer from several issues, e.g., instant trigger patterns lack stealthiness, the backdoor is trained or activated by an additional network, or all agents are backdoored. To this end, in this paper, we propose a novel backdoor leverage attack against c-MADRL, BLAST, which attacks the entire multi-agent team by embedding the backdoor only in a single agent. Firstly, we introduce adversary spatiotemporal behavior patterns as the backdoor trigger rather than manual-injected fixed visual patterns or instant status and control the period to perform malicious actions. This method can guarantee the stealthiness and practicality of BLAST. Secondly, we hack the original reward function of the backdoor agent via unilateral guidance to inject BLAST, so as to achieve the \textit{leverage attack effect} that can pry open the entire multi-agent system via a single backdoor agent. We evaluate our BLAST against 3 classic c-MADRL algorithms (VDN, QMIX, and MAPPO) in 2 popular c-MADRL environments (SMAC and Pursuit), and 2 existing defense mechanisms. The experimental results demonstrate that BLAST can achieve a high attack success rate while maintaining a low clean performance variance rate.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization</title>
<link>https://arxiv.org/abs/2502.00691</link>
<guid>https://arxiv.org/abs/2502.00691</guid>
<content:encoded><![CDATA[
arXiv:2502.00691v4 Announce Type: replace 
Abstract: Recent advances in mathematical problem-solving with language models (LMs) integrate chain-of-thought (CoT) reasoning and code execution to harness their complementary strengths. However, existing hybrid frameworks exhibit a critical limitation: they depend on externally dictated instructions or rigid code-integration templates, lacking metacognitive awareness -- the capacity to dynamically evaluate intrinsic capabilities and autonomously determine when and how to integrate tools. This rigidity motivates our study of autonomous code integration, enabling models to adapt tool-usage strategies as their reasoning abilities evolve during training.
  While reinforcement learning (RL) shows promise for boosting LLM reasoning at scale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning autonomous code integration due to inadequate exploration of the vast combinatorial space of CoT-code interleaving patterns. To address this challenge, we propose a novel Expectation-Maximization (EM) framework that synergizes structured exploration (E-step) with off-policy RL optimization (M-step), creating a self-reinforcing cycle between metacognitive tool-use decisions and evolving capabilities. Experiments reveal our method achieves superior results through improved exploration. Notably, our 7B model improves over 11% on MATH500 and 9.4% on AIME without o1-like CoT.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios</title>
<link>https://arxiv.org/abs/2502.02145</link>
<guid>https://arxiv.org/abs/2502.02145</guid>
<content:encoded><![CDATA[
arXiv:2502.02145v4 Announce Type: replace 
Abstract: Ensuring the safety of autonomous vehicles requires virtual scenario-based testing, which depends on the robust evaluation and generation of safety-critical scenarios. So far, researchers have used scenario-based testing frameworks that rely heavily on handcrafted scenarios as safety metrics. To reduce the effort of human interpretation and overcome the limited scalability of these approaches, we combine Large Language Models (LLMs) with structured scenario parsing and prompt engineering to automatically evaluate and generate safety-critical driving scenarios. We introduce Cartesian and Ego-centric prompt strategies for scenario evaluation, and an adversarial generation module that modifies trajectories of risk-inducing vehicles (ego-attackers) to create critical scenarios. We validate our approach using a 2D simulation framework and multiple pre-trained LLMs. The results show that the evaluation module effectively detects collision scenarios and infers scenario safety. Meanwhile, the new generation module identifies high-risk agents and synthesizes realistic, safety-critical scenarios. We conclude that an LLM equipped with domain-informed prompting techniques can effectively evaluate and generate safety-critical driving scenarios, reducing dependence on handcrafted metrics. We release our open-source code and scenarios at: https://github.com/TUM-AVS/From-Words-to-Collisions.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models</title>
<link>https://arxiv.org/abs/2503.09567</link>
<guid>https://arxiv.org/abs/2503.09567</guid>
<content:encoded><![CDATA[
arXiv:2503.09567v5 Announce Type: replace 
Abstract: Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like "overthinking" and "inference-time scaling." This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and inference-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What the F*ck Is Artificial General Intelligence?</title>
<link>https://arxiv.org/abs/2503.23923</link>
<guid>https://arxiv.org/abs/2503.23923</guid>
<content:encoded><![CDATA[
arXiv:2503.23923v2 Announce Type: replace 
Abstract: Artificial general intelligence (AGI) is an established field of research. Yet some have questioned if the term still has meaning. AGI has been subject to so much hype and speculation it has become something of a Rorschach test. Melanie Mitchell argues the debate will only be settled through long term, scientific investigation. To that end here is a short, accessible and provocative overview of AGI. I compare definitions of intelligence, settling on intelligence in terms of adaptation and AGI as an artificial scientist. Taking my cue from Sutton's Bitter Lesson I describe two foundational tools used to build adaptive systems: search and approximation. I compare pros, cons, hybrids and architectures like o3, AlphaGo, AERA, NARS and Hyperon. I then discuss overall meta-approaches to making systems behave more intelligently. I divide them into scale-maxing, simp-maxing, w-maxing based on the Bitter Lesson, Ockham's and Bennett's Razors. These maximise resources, simplicity of form, and the weakness of constraints on functionality. I discuss examples including AIXI, the free energy principle and The Embiggening of language models. I conclude that though scale-maxed approximation dominates, AGI will be a fusion of tools and meta-approaches. The Embiggening was enabled by improvements in hardware. Now the bottlenecks are sample and energy efficiency.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator</title>
<link>https://arxiv.org/abs/2505.17735</link>
<guid>https://arxiv.org/abs/2505.17735</guid>
<content:encoded><![CDATA[
arXiv:2505.17735v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-based agents are increasingly deployed in real-world applications such as "digital assistants, autonomous customer service, and decision-support systems", where their ability to "interact in multi-turn, tool-augmented environments" makes them indispensable. However, ensuring the safety of these agents remains a significant challenge due to the diverse and complex risks arising from dynamic user interactions, external tool usage, and the potential for unintended harmful behaviors. To address this critical issue, we propose AutoSafe, the first framework that systematically enhances agent safety through fully automated synthetic data generation. Concretely, 1) we introduce an open and extensible threat model, OTS, which formalizes how unsafe behaviors emerge from the interplay of user instructions, interaction contexts, and agent actions. This enables precise modeling of safety risks across diverse scenarios. 2) we develop a fully automated data generation pipeline that simulates unsafe user behaviors, applies self-reflective reasoning to generate safe responses, and constructs a large-scale, diverse, and high-quality safety training dataset-eliminating the need for hazardous real-world data collection. To evaluate the effectiveness of our framework, we design comprehensive experiments on both synthetic and real-world safety benchmarks. Results demonstrate that AutoSafe boosts safety scores by 45% on average and achieves a 28.91% improvement on real-world tasks, validating the generalization ability of our learned safety strategies. These results highlight the practical advancement and scalability of AutoSafe in building safer LLM-based agents for real-world deployment. We have released the project page at https://auto-safe.github.io/.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Reflectivism In Intelligent Systems</title>
<link>https://arxiv.org/abs/2505.22987</link>
<guid>https://arxiv.org/abs/2505.22987</guid>
<content:encoded><![CDATA[
arXiv:2505.22987v2 Announce Type: replace 
Abstract: By late 20th century, the rationality wars had launched debates about the nature and norms of intuitive and reflective thinking. Those debates drew from mid-20th century ideas such as bounded rationality, which challenged more idealized notions of rationality observed since the 19th century. Now that 21st century cognitive scientists are applying the resulting dual pro-cess theories to artificial intelligence, it is time to dust off some lessons from this history. So this paper synthesizes old ideas with recent results from experiments on humans and machines. The result is Strategic Reflec-tivism, the position that one key to intelligent systems (human or artificial) is pragmatic switching between intuitive and reflective inference to opti-mally fulfill competing goals. Strategic Reflectivism builds on American Pragmatism, transcends superficial indicators of reflective thinking such as model size or chains of thought, applies to both individual and collective intelligence systems (including human-AI teams), and becomes increasingly actionable as we learn more about the value of intuition and reflection.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title>
<link>https://arxiv.org/abs/2506.06941</link>
<guid>https://arxiv.org/abs/2506.06941</guid>
<content:encoded><![CDATA[
arXiv:2506.06941v2 Announce Type: replace 
Abstract: Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?</title>
<link>https://arxiv.org/abs/2506.18183</link>
<guid>https://arxiv.org/abs/2506.18183</guid>
<content:encoded><![CDATA[
arXiv:2506.18183v3 Announce Type: replace 
Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humans' innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GATSim: Urban Mobility Simulation with Generative Agents</title>
<link>https://arxiv.org/abs/2506.23306</link>
<guid>https://arxiv.org/abs/2506.23306</guid>
<content:encoded><![CDATA[
arXiv:2506.23306v2 Announce Type: replace 
Abstract: Traditional agent-based urban mobility simulations often rely on rigid rule-based systems that struggle to capture the complexity, adaptability, and behavioral diversity inherent in human travel decision making. Recent advancements in large language models and AI agent technologies present new opportunities to develop agents with enhanced reasoning capabilities, persistent memory, and adaptive learning. We introduce GATSim (Generative-Agent Transport Simulation), a novel framework that leverages these advancements to simulate urban mobility using generative agents with rich, human-like behaviors. Unlike conventional approaches, GATSim agents are characterized by diverse socioeconomic profiles, individual lifestyles, and evolving preferences shaped through psychologically informed memory systems, tool usage, and lifelong learning. The main contributions of this work are: (1) a comprehensive architecture that integrates an urban mobility foundation model with agent cognitive systems and a transport simulation environment; (2) a hierarchical memory designed for efficient retrieval of contextually relevant information, incorporating spatial and temporal associations, keyword matching, and semantic relevance; (3) innovative planning and reactive mechanisms for modeling adaptive mobility behaviors which integrate a multi-scale reflection process to transform specific travel experiences into generalized behavioral insights. We implement a prototype system and conduct systematic validation, demonstrating that generative agents produce believable and coherent travel behaviors. Experimental results indicate that generative agents perform at least as well as human annotators with 92\% posterior probability, while naturally producing realistic macroscopic traffic patterns. The code for the prototype implementation is publicly available at https://github.com/qiliuchn/gatsim.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eye-tracked Virtual Reality: A Comprehensive Survey on Methods and Privacy Challenges</title>
<link>https://arxiv.org/abs/2305.14080</link>
<guid>https://arxiv.org/abs/2305.14080</guid>
<content:encoded><![CDATA[
arXiv:2305.14080v2 Announce Type: replace-cross 
Abstract: The latest developments in computer hardware, sensor technologies, and artificial intelligence can make virtual reality (VR) and virtual spaces an important part of human everyday life. Eye tracking offers not only a hands-free way of interaction but also the possibility of a deeper understanding of human visual attention and cognitive processes in VR. Despite these possibilities, eye-tracking data also reveals users' privacy-sensitive attributes when combined with the information about the presented stimulus. To address all these possibilities and potential privacy issues, in this survey, we first cover major works in eye tracking, VR, and privacy areas between 2012 and 2022. While eye tracking in the VR part covers the complete pipeline of eye-tracking methodology from pupil detection and gaze estimation to offline use of the data and analyses, as for privacy and security, we focus on eye-based authentication as well as computational methods to preserve the privacy of individuals and their eye-tracking data in VR. Later, considering all of these, we draw three main directions for the research community by focusing on privacy challenges. In summary, this survey provides an extensive literature review of the utmost possibilities with eye tracking in VR and the privacy implications of those possibilities.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved DDIM Sampling with Moment Matching Gaussian Mixtures</title>
<link>https://arxiv.org/abs/2311.04938</link>
<guid>https://arxiv.org/abs/2311.04938</guid>
<content:encoded><![CDATA[
arXiv:2311.04938v3 Announce Type: replace-cross 
Abstract: We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ and class-conditional models trained on ImageNet datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of 6.94 and IS of 207.85 with a GMM kernel compared to 10.15 and 196.73 respectively with a Gaussian kernel.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SecurePose: Automated Face Blurring and Human Movement Kinematics Extraction from Videos Recorded in Clinical Settings</title>
<link>https://arxiv.org/abs/2402.14143</link>
<guid>https://arxiv.org/abs/2402.14143</guid>
<content:encoded><![CDATA[
arXiv:2402.14143v2 Announce Type: replace-cross 
Abstract: Movement disorder diagnosis often relies on expert evaluation of patient videos, but sharing these videos poses privacy risks. Current methods for de-identifying videos, such as blurring faces, are often manual, inconsistent, or inaccurate. Furthermore, these methods can compromise objective kinematic analysis - a crucial component of diagnosis. To address these challenges, we developed SecurePose, an open-source software that simultaneously provides reliable de-identification and automated kinematic extraction from videos recorded in clinic settings using smartphones/tablets. SecurePose utilizes pose estimation (using OpenPose) to extract full body kinematics, track individuals, identify the patient, and then accurately blur faces in the videos. We validated SecurePose on gait videos recorded in outpatient clinic visits of 116 children with cerebral palsy, assessing both the accuracy of its de-identification compared to the ground truth (manual blurring) and the reliability of the intermediate steps of kinematics extraction. Results demonstrate that SecurePose outperformed six existing methods in automated face detection and achieved comparable accuracy to robust manual blurring, but in significantly less time (91.08% faster). Ten experienced researchers also confirmed SecurePose's usability via System Usability Scale scores. These findings validate SecurePose as a practical and effective tool for protecting patient privacy while enabling accurate kinematics extraction in clinical settings.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation</title>
<link>https://arxiv.org/abs/2404.07053</link>
<guid>https://arxiv.org/abs/2404.07053</guid>
<content:encoded><![CDATA[
arXiv:2404.07053v2 Announce Type: replace-cross 
Abstract: Metaphors, although occasionally unperceived, are ubiquitous in our everyday language. Thus, it is crucial for Language Models to be able to grasp the underlying meaning of this kind of figurative language. In this work, we present Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection and interpretation that contains metaphor annotations in both Spanish and English. We investigate language models' metaphor identification and understanding abilities through a series of monolingual and cross-lingual experiments by leveraging our proposed corpus. In order to comprehend how these non-literal expressions affect models' performance, we look over the results and perform an error analysis. Additionally, parallel data offers many potential opportunities to investigate metaphor transferability between these languages and the impact of translation on the development of multilingual annotated resources.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning</title>
<link>https://arxiv.org/abs/2405.18386</link>
<guid>https://arxiv.org/abs/2405.18386</guid>
<content:encoded><![CDATA[
arXiv:2405.18386v3 Announce Type: replace-cross 
Abstract: Recent advances in text-to-music editing, which employ text queries to modify music (e.g.\ by changing its style or adjusting instrumental components), present unique challenges and opportunities for AI-assisted music creation. Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction. To Combine the strengths and address these limitations, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems. Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio inputs concurrently and yield the desired edited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, yet it achieves superior performance across all tasks compared to existing baselines, and demonstrates performance comparable to the models trained for specific tasks. This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Grounding Methods for Efficient Interaction with Desktop Graphical User Interfaces</title>
<link>https://arxiv.org/abs/2407.01558</link>
<guid>https://arxiv.org/abs/2407.01558</guid>
<content:encoded><![CDATA[
arXiv:2407.01558v3 Announce Type: replace-cross 
Abstract: Most visual grounding solutions primarily focus on realistic images. However, applications involving synthetic images, such as Graphical User Interfaces (GUIs), remain limited. This restricts the development of autonomous computer vision-powered artificial intelligence (AI) agents for automatic application interaction. Enabling AI to effectively understand and interact with GUIs is crucial to advancing automation in software testing, accessibility, and human-computer interaction. In this work, we explore Instruction Visual Grounding (IVG), a multi-modal approach to object identification within a GUI. More precisely, given a natural language instruction and a GUI screen, IVG locates the coordinates of the element on the screen where the instruction should be executed. We propose two main methods: (1) IVGocr, which combines a Large Language Model (LLM), an object detection model, and an Optical Character Recognition (OCR) module; and (2) IVGdirect, which uses a multimodal architecture for end-to-end grounding. For each method, we introduce a dedicated dataset. In addition, we propose the Central Point Validation (CPV) metric, a relaxed variant of the classical Central Proximity Score (CPS) metric. Our final test dataset is publicly released to support future research.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Pre-training of Multimodal Language Models Customized for Chart Understanding</title>
<link>https://arxiv.org/abs/2407.14506</link>
<guid>https://arxiv.org/abs/2407.14506</guid>
<content:encoded><![CDATA[
arXiv:2407.14506v3 Announce Type: replace-cross 
Abstract: Recent studies customizing Multimodal Large Language Models (MLLMs) for domain-specific tasks have yielded promising results, especially in the field of scientific chart comprehension. These studies generally utilize visual instruction tuning with specialized datasets to enhance question and answer (QA) accuracy within the chart domain. However, they often neglect the fundamental discrepancy between natural image-caption pre-training data and digital chart image-QA data, particularly in the models' capacity to extract underlying numeric values from charts. This paper tackles this oversight by exploring the training processes necessary to improve MLLMs' comprehension of charts. We present three key findings: (1) Incorporating raw data values in alignment pre-training markedly improves comprehension of chart data. (2) Replacing images with their textual representation randomly during end-to-end fine-tuning transfer the language reasoning capability to chart interpretation skills. (3) Requiring the model to first extract the underlying chart data and then answer the question in the fine-tuning can further improve the accuracy. Consequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart comprehension. CHOPINLLM effectively interprets various types of charts, including unannotated ones, while maintaining robust reasoning abilities. Furthermore, we establish a new benchmark to evaluate MLLMs' understanding of different chart types across various comprehension levels. Experimental results show that CHOPINLLM exhibits strong performance in understanding both annotated and unannotated charts across a wide range of types.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation</title>
<link>https://arxiv.org/abs/2408.00998</link>
<guid>https://arxiv.org/abs/2408.00998</guid>
<content:encoded><![CDATA[
arXiv:2408.00998v3 Announce Type: replace-cross 
Abstract: Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing wonderful image generation with natural-language text prompt. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation. Thus, attention has been focused on leveraging a reference image to control text-to-image synthesis, which is also regarded as manipulating (or editing) a reference image as per a text prompt, namely, text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven I2I translation without any model training, model fine-tuning, or online optimization process. To guide T2I generation with a reference image, we propose to decompose diverse guiding factors with different frequency bands of diffusion features in the DCT spectral space, and accordingly devise a novel frequency band substitution layer which realizes dynamic control of the reference image to the T2I generation result in a plug-and-play manner. We demonstrate that our method allows flexible control over both guiding factor and guiding intensity of the reference image simply by tuning the type and bandwidth of the substituted frequency band, respectively. Extensive qualitative and quantitative experiments verify superiority of our approach over related methods in I2I translation visual quality, versatility, and controllability. The code is publicly available at: https://github.com/XiangGao1102/FBSDiff.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy Loss: An Interpretability Amplifier of 3D Object Detection Network for Intelligent Driving</title>
<link>https://arxiv.org/abs/2409.00839</link>
<guid>https://arxiv.org/abs/2409.00839</guid>
<content:encoded><![CDATA[
arXiv:2409.00839v2 Announce Type: replace-cross 
Abstract: With the increasing complexity of the traffic environment, the significance of safety perception in intelligent driving is intensifying. Traditional methods in the field of intelligent driving perception rely on deep learning, which suffers from limited interpretability, often described as a "black box." This paper introduces a novel type of loss function, termed "Entropy Loss," along with an innovative training strategy. Entropy Loss is formulated based on the functionality of feature compression networks within the perception model. Drawing inspiration from communication systems, the information transmission process in a feature compression network is expected to demonstrate steady changes in information volume and a continuous decrease in information entropy. By modeling network layer outputs as continuous random variables, we construct a probabilistic model that quantifies changes in information volume. Entropy Loss is then derived based on these expectations, guiding the update of network parameters to enhance network interpretability. Our experiments indicate that the Entropy Loss training strategy accelerates the training process. Utilizing the same 60 training epochs, the accuracy of 3D object detection models using Entropy Loss on the KITTI test set improved by up to 4.47\% compared to models without Entropy Loss, underscoring the method's efficacy. The implementation code is available at https://github.com/yhbcode000/Eloss-Interpretability.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Local and Global Knowledge via Transformer in Board Games</title>
<link>https://arxiv.org/abs/2410.05347</link>
<guid>https://arxiv.org/abs/2410.05347</guid>
<content:encoded><![CDATA[
arXiv:2410.05347v2 Announce Type: replace-cross 
Abstract: Although AlphaZero has achieved superhuman performance in board games, recent studies reveal its limitations in handling scenarios requiring a comprehensive understanding of the entire board, such as recognizing long-sequence patterns in Go. To address this challenge, we propose ResTNet, a network that interleaves residual and Transformer blocks to bridge local and global knowledge. ResTNet improves playing strength across multiple board games, increasing win rate from 54.6% to 60.8% in 9x9 Go, 53.6% to 60.9% in 19x19 Go, and 50.4% to 58.0% in 19x19 Hex. In addition, ResTNet effectively processes global information and tackles two long-sequence patterns in 19x19 Go, including circular pattern and ladder pattern. It reduces the mean square error for circular pattern recognition from 2.58 to 1.07 and lowers the attack probability against an adversary program from 70.44% to 23.91%. ResTNet also improves ladder pattern recognition accuracy from 59.15% to 80.01%. By visualizing attention maps, we demonstrate that ResTNet captures critical game concepts in both Go and Hex, offering insights into AlphaZero's decision-making process. Overall, ResTNet shows a promising approach to integrating local and global knowledge, paving the way for more effective AlphaZero-based algorithms in board games. Our code is available at https://rlg.iis.sinica.edu.tw/papers/restnet.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Approach for Auto Generation of Labeling Functions for Software Engineering Chatbots</title>
<link>https://arxiv.org/abs/2410.07094</link>
<guid>https://arxiv.org/abs/2410.07094</guid>
<content:encoded><![CDATA[
arXiv:2410.07094v2 Announce Type: replace-cross 
Abstract: Software engineering (SE) chatbots are increasingly gaining attention for their role in enhancing development processes. At the core of chatbots are Natural Language Understanding platforms (NLUs), which enable them to comprehend user queries but require labeled data for training. However, acquiring such labeled data for SE chatbots is challenging due to the scarcity of high-quality datasets, as training requires specialized vocabulary and phrases not found in typical language datasets. Consequently, developers often resort to manually annotating user queries -- a time-consuming and resource-intensive process. Previous approaches require human intervention to generate rules, called labeling functions (LFs), that categorize queries based on specific patterns. To address this issue, we propose an approach to automatically generate LFs by extracting patterns from labeled user queries. We evaluate our approach on four SE datasets and measure performance improvement from training NLUs on queries labeled by the generated LFs. The generated LFs effectively label data with AUC scores up to 85.3% and NLU performance improvements up to 27.2%. Furthermore, our results show that the number of LFs affects labeling performance. We believe that our approach can save time and resources in labeling users' queries, allowing practitioners to focus on core chatbot functionalities rather than manually labeling queries.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Practical Operation of Deep Reinforcement Learning Agents in Real-World Network Management at Open RAN Edges</title>
<link>https://arxiv.org/abs/2410.23086</link>
<guid>https://arxiv.org/abs/2410.23086</guid>
<content:encoded><![CDATA[
arXiv:2410.23086v2 Announce Type: replace-cross 
Abstract: Deep Reinforcement Learning (DRL) has emerged as a powerful solution for meeting the growing demands for connectivity, reliability, low latency and operational efficiency in advanced networks. However, most research has focused on theoretical analysis and simulations, with limited investigation into real-world deployment. To bridge the gap and support practical DRL deployment for network management, we first present an orchestration framework that integrates ETSI Multi-access Edge Computing (MEC) with Open RAN, enabling seamless adoption of DRL-based strategies across different time scales while enhancing agent lifecycle management. We then identify three critical challenges hindering DRL's real-world deployment, including (1) asynchronous requests from unpredictable or bursty traffic, (2) adaptability and generalization across heterogeneous topologies and evolving service demands, and (3) prolonged convergence and service interruptions due to exploration in live operational environments. To address these challenges, we propose a three-fold solution strategy: (a) advanced time-series integration for handling asynchronized traffic, (b) flexible architecture design such as multi-agent DRL and incremental learning to support heterogeneous scenarios, and (c) simulation-driven deployment with transfer learning to reduce convergence time and service disruptions. Lastly, the feasibility of the MEC-O-RAN architecture is validated on an urban-wide testing infrastructure, and two real-world use cases are presented, showcasing the three identified challenges and demonstrating the effectiveness of the proposed solutions.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Pretraining for Molecular Property Prediction in the Wild</title>
<link>https://arxiv.org/abs/2411.03537</link>
<guid>https://arxiv.org/abs/2411.03537</guid>
<content:encoded><![CDATA[
arXiv:2411.03537v2 Announce Type: replace-cross 
Abstract: Molecular deep learning models have achieved remarkable success in property prediction, but they often require large amounts of labeled data. The challenge is that, in real-world applications, labels are extremely scarce, as obtaining them through laboratory experimentation is both expensive and time-consuming. In this work, we introduce MoleVers, a versatile pretrained molecular model designed for various types of molecular property prediction in the wild, i.e., where experimentally-validated labels are scarce. MoleVers employs a two-stage pretraining strategy. In the first stage, it learns molecular representations from unlabeled data through masked atom prediction and extreme denoising, a novel task enabled by our newly introduced branching encoder architecture and dynamic noise scale sampling. In the second stage, the model refines these representations through predictions of auxiliary properties derived from computational methods, such as the density functional theory or large language models. Evaluation on 22 small, experimentally-validated datasets demonstrates that MoleVers achieves state-of-the-art performance, highlighting the effectiveness of its two-stage framework in producing generalizable molecular representations for diverse downstream properties.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models</title>
<link>https://arxiv.org/abs/2412.16247</link>
<guid>https://arxiv.org/abs/2412.16247</guid>
<content:encoded><![CDATA[
arXiv:2412.16247v3 Announce Type: replace-cross 
Abstract: Sparse dictionary learning (DL) has emerged as a powerful approach to extract semantically meaningful concepts from the internals of large language models (LLMs) trained mainly in the text domain. In this work, we explore whether DL can extract meaningful concepts from less human-interpretable scientific data, such as vision foundation models trained on cell microscopy images, where limited prior knowledge exists about which high-level concepts should arise. We propose a novel combination of a sparse DL algorithm, Iterative Codebook Feature Learning (ICFL), with a PCA whitening pre-processing step derived from control data. Using this combined approach, we successfully retrieve biologically meaningful concepts, such as cell types and genetic perturbations. Moreover, we demonstrate how our method reveals subtle morphological changes arising from human-interpretable interventions, offering a promising new direction for scientific discovery via mechanistic interpretability in bioimaging.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invisible Textual Backdoor Attacks based on Dual-Trigger</title>
<link>https://arxiv.org/abs/2412.17531</link>
<guid>https://arxiv.org/abs/2412.17531</guid>
<content:encoded><![CDATA[
arXiv:2412.17531v3 Announce Type: replace-cross 
Abstract: Backdoor attacks pose an important security threat to textual large language models. Exploring textual backdoor attacks not only helps reveal the potential security risks of models, but also promotes innovation and development of defense mechanisms. Currently, most textual backdoor attack methods are based on a single trigger. For example, inserting specific content into text as a trigger or changing the abstract text features to be a trigger. However, the adoption of this single-trigger mode makes the existing backdoor attacks subject to certain limitations: either they are easily identified by the existing defense strategies, or they have certain shortcomings in attack performance and in the construction of poisoned datasets. In order to solve these issues, a dual-trigger backdoor attack method is proposed in this paper. Specifically, we use two different attributes, syntax and mood (we use subjunctive mood as an example in this article), as two different triggers. It makes our backdoor attack method similar to a double landmine which can have completely different trigger conditions simultaneously. Therefore, this method not only improves the flexibility of trigger mode, but also enhances the robustness against defense detection. A large number of experimental results show that this method significantly outperforms the previous methods based on abstract features in attack performance, and achieves comparable attack performance (almost 100\% attack success rate) with the insertion-based method. In addition, in order to further improve the attack performance, we also give the construction method of the poisoned dataset.The code and data of this paper can be obtained at https://github.com/HoyaAm/Double-Landmines.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal reasoning for timeline summarisation in social media</title>
<link>https://arxiv.org/abs/2501.00152</link>
<guid>https://arxiv.org/abs/2501.00152</guid>
<content:encoded><![CDATA[
arXiv:2501.00152v3 Announce Type: replace-cross 
Abstract: This paper explores whether enhancing temporal reasoning capabilities in Large Language Models (LLMs) can improve the quality of timeline summarisation, the task of summarising long texts containing sequences of events, such as social media threads. We first introduce NarrativeReason, a novel dataset focused on temporal relationships among sequential events within narratives, distinguishing it from existing temporal reasoning datasets that primarily address pair-wise event relationships. Our approach then combines temporal reasoning with timeline summarisation through a knowledge distillation framework, where we first fine-tune a teacher model on temporal reasoning tasks and then distill this knowledge into a student model while simultaneously training it for the task of timeline summarisation. Experimental results demonstrate that our model achieves superior performance on out-of-domain mental health-related timeline summarisation tasks, which involve long social media threads with repetitions of events and a mix of emotions, highlighting the importance and generalisability of leveraging temporal reasoning to improve timeline summarisation.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Code to Compliance: Assessing ChatGPT's Utility in Designing an Accessible Webpage -- A Case Study</title>
<link>https://arxiv.org/abs/2501.03572</link>
<guid>https://arxiv.org/abs/2501.03572</guid>
<content:encoded><![CDATA[
arXiv:2501.03572v2 Announce Type: replace-cross 
Abstract: Web accessibility ensures that individuals with disabilities can access and interact with digital content without barriers, yet a significant majority of most used websites fail to meet accessibility standards. This study evaluates ChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web Content Accessibility Guidelines (WCAG). While ChatGPT can effectively address accessibility issues when prompted, its default code often lacks compliance, reflecting limitations in its training data and prevailing inaccessible web practices. Automated and manual testing revealed strengths in resolving simple issues but challenges with complex tasks, requiring human oversight and additional iterations. Unlike prior studies, we incorporate manual evaluation, dynamic elements, and use the visual reasoning capability of ChatGPT along with the prompts to fix accessibility issues. Providing screenshots alongside prompts enhances the LLM's ability to address accessibility issues by allowing it to analyze surrounding components, such as determining appropriate contrast colors. We found that effective prompt engineering, such as providing concise, structured feedback and incorporating visual aids, significantly enhances ChatGPT's performance. These findings highlight the potential and limitations of large language models for accessible web development, offering practical guidance for developers to create more inclusive websites.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency of Responses and Continuations Generated by Large Language Models on Social Media</title>
<link>https://arxiv.org/abs/2501.08102</link>
<guid>https://arxiv.org/abs/2501.08102</guid>
<content:encoded><![CDATA[
arXiv:2501.08102v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using two open-source models: Gemma and Llama. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic similarity between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: Gemma shows a tendency toward negative emotion amplification, particularly anger, while maintaining certain positive emotions like optimism. Llama demonstrates superior emotional preservation across a broader spectrum of affects. Both models systematically generate responses with attenuated emotional intensity compared to human-authored content and show a bias toward positive emotions in response tasks. Additionally, both models maintain strong semantic similarity with original texts, though performance varies between continuation and response tasks. These findings provide insights into LLMs' emotional and semantic processing capabilities, with implications for their deployment in social media contexts and human-AI interaction design.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASTRID -- An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems</title>
<link>https://arxiv.org/abs/2501.08208</link>
<guid>https://arxiv.org/abs/2501.08208</guid>
<content:encoded><![CDATA[
arXiv:2501.08208v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown impressive potential in clinical question answering (QA), with Retrieval Augmented Generation (RAG) emerging as a leading approach for ensuring the factual accuracy of model responses. However, current automated RAG metrics perform poorly in clinical and conversational use cases. Using clinical human evaluations of responses is expensive, unscalable, and not conducive to the continuous iterative development of RAG systems. To address these challenges, we introduce ASTRID - an Automated and Scalable TRIaD for evaluating clinical QA systems leveraging RAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy (RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is designed to better capture the faithfulness of a model's response to the knowledge base without penalising conversational elements. To validate our triad, we curate a dataset of over 200 real-world patient questions posed to an LLM-based QA agent during surgical follow-up for cataract surgery - the highest volume operation in the world - augmented with clinician-selected questions for emergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate that CF can predict human ratings of faithfulness better than existing definitions for conversational use cases. Furthermore, we show that evaluation using our triad consisting of CF, RA, and CR exhibits alignment with clinician assessment for inappropriate, harmful, or unhelpful responses. Finally, using nine different LLMs, we demonstrate that the three metrics can closely agree with human evaluations, highlighting the potential of these metrics for use in LLM-driven automated evaluation pipelines. We also publish the prompts and datasets for these experiments, providing valuable resources for further research and development.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude</title>
<link>https://arxiv.org/abs/2501.10484</link>
<guid>https://arxiv.org/abs/2501.10484</guid>
<content:encoded><![CDATA[
arXiv:2501.10484v2 Announce Type: replace-cross 
Abstract: Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study investigates protected attributes in LLMs through systematic evaluation of their responses to ethical dilemmas. Using two prominent models - GPT-3.5 Turbo and Claude 3.5 Sonnet - we analyzed their decision-making patterns across multiple protected attributes including age, gender, race, appearance, and disability status. Through 11,200 experimental trials involving both single-factor and two-factor protected attribute combinations, we evaluated the models' ethical preferences, sensitivity, stability, and clustering of preferences. Our findings reveal significant protected attributeses in both models, with consistent preferences for certain features (e.g., "good-looking") and systematic neglect of others. Notably, while GPT-3.5 Turbo showed stronger preferences aligned with traditional power structures, Claude 3.5 Sonnet demonstrated more diverse protected attribute choices. We also found that ethical sensitivity significantly decreases in more complex scenarios involving multiple protected attributes. Additionally, linguistic referents heavily influence the models' ethical evaluations, as demonstrated by differing responses to racial descriptors (e.g., "Yellow" versus "Asian"). These findings highlight critical concerns about the potential impact of LLM biases in autonomous decision-making systems and emphasize the need for careful consideration of protected attributes in AI development. Our study contributes to the growing body of research on AI ethics by providing a systematic framework for evaluating protected attributes in LLMs' ethical decision-making capabilities.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian</title>
<link>https://arxiv.org/abs/2501.11264</link>
<guid>https://arxiv.org/abs/2501.11264</guid>
<content:encoded><![CDATA[
arXiv:2501.11264v3 Announce Type: replace-cross 
Abstract: Software engineers spend a significant amount of time reading code during the software development process, especially in the age of large language models (LLMs) that can automatically generate code. However, little is known about the readability of the LLM-generated code and whether it is still important from practitioners' perspectives in this new era. In this paper, we conduct a survey to explore the practitioners' perspectives on code readability in the age of LLMs and investigate the readability of our LLM-based software development agents framework, HULA, by comparing its generated code with human-written code in real-world scenarios. Overall, the findings underscore that (1) readability remains a critical aspect of software development; (2) the readability of our LLM-generated code is comparable to human-written code, fostering the establishment of appropriate trust and driving the broad adoption of our LLM-powered software development platform.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Transfer of Knowledge in Quantum Algorithms</title>
<link>https://arxiv.org/abs/2501.14120</link>
<guid>https://arxiv.org/abs/2501.14120</guid>
<content:encoded><![CDATA[
arXiv:2501.14120v2 Announce Type: replace-cross 
Abstract: Quantum computing is poised to transform computational paradigms across science and industry. As the field evolves, it can benefit from established classical methodologies, including promising paradigms such as Transfer of Knowledge (ToK). This work serves as a brief, self-contained reference for ToK, unifying its core principles under a single formal framework. We introduce a joint notation that consolidates and extends prior work in Transfer Learning and Transfer Optimization, bridging traditionally separate research lines and enabling a common language for knowledge reuse. Building on this foundation, we classify existing ToK strategies and principles into a structured taxonomy that helps researchers position their methods within a broader conceptual map. We then extend key transfer protocols to quantum computing, introducing two novel use cases (reverse annealing and multitasking QAOA) alongside a sequential VQE approach that supports and validates prior findings. These examples highlight ToK's potential to improve performance and generalization in quantum algorithms. Finally, we outline challenges and opportunities for integrating ToK into quantum computing, emphasizing its role in reducing resource demands and accelerating problem-solving. This work lays the groundwork for future synergies between classical and quantum computing through a shared, transferable knowledge framework.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2502.03304</link>
<guid>https://arxiv.org/abs/2502.03304</guid>
<content:encoded><![CDATA[
arXiv:2502.03304v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose Divergence-driven Zeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning. Our code is released at https://anonymous.4open.science/r/DiZO-E86D.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stonefish: Supporting Machine Learning Research in Marine Robotics</title>
<link>https://arxiv.org/abs/2502.11887</link>
<guid>https://arxiv.org/abs/2502.11887</guid>
<content:encoded><![CDATA[
arXiv:2502.11887v2 Announce Type: replace-cross 
Abstract: Simulations are highly valuable in marine robotics, offering a cost-effective and controlled environment for testing in the challenging conditions of underwater and surface operations. Given the high costs and logistical difficulties of real-world trials, simulators capable of capturing the operational conditions of subsea environments have become key in developing and refining algorithms for remotely-operated and autonomous underwater vehicles. This paper highlights recent enhancements to the Stonefish simulator, an advanced open-source platform supporting development and testing of marine robotics solutions. Key updates include a suite of additional sensors, such as an event-based camera, a thermal camera, and an optical flow camera, as well as, visual light communication, support for tethered operations, improved thruster modelling, more flexible hydrodynamics, and enhanced sonar accuracy. These developments and an automated annotation tool significantly bolster Stonefish's role in marine robotics research, especially in the field of machine learning, where training data with a known ground truth is hard or impossible to collect.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating link prediction: New perspectives and recommendations</title>
<link>https://arxiv.org/abs/2502.12777</link>
<guid>https://arxiv.org/abs/2502.12777</guid>
<content:encoded><![CDATA[
arXiv:2502.12777v4 Announce Type: replace-cross 
Abstract: Link prediction (LP) is an important problem in network science and machine learning research. The state-of-the-art LP methods are usually evaluated in a uniform setup, ignoring several factors associated with the data and application specific needs. We identify a number of such factors, such as, network-type, problem-type, geodesic distance between the end nodes and its distribution over the classes, nature and applicability of LP methods, class imbalance and its impact on early retrieval, evaluation metric, etc., and present an experimental setup which allows us to evaluate LP methods in a rigorous and controlled manner. We perform extensive experiments with a variety of LP methods over real network datasets in this controlled setup, and gather valuable insights on the interactions of these factors with the performance of LP through an array of carefully designed hypotheses. Following the insights, we provide recommendations to be followed as best practice for evaluating LP methods.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model</title>
<link>https://arxiv.org/abs/2502.14131</link>
<guid>https://arxiv.org/abs/2502.14131</guid>
<content:encoded><![CDATA[
arXiv:2502.14131v4 Announce Type: replace-cross 
Abstract: We study the problem of estimating Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning (offline MaxEnt-IRL) in machine learning. The objective is to recover reward or $Q^*$ functions that govern agent behavior from offline behavior data. In this paper, we propose a globally convergent gradient-based method for solving these problems without the restrictive assumption of linearly parameterized rewards. The novelty of our approach lies in introducing the Empirical Risk Minimization (ERM) based IRL/DDC framework, which circumvents the need for explicit state transition probability estimation in the Bellman equation. Furthermore, our method is compatible with non-parametric estimation techniques such as neural networks. Therefore, the proposed method has the potential to be scaled to high-dimensional, infinite state spaces. A key theoretical insight underlying our approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL) condition -- a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees. Through a series of synthetic experiments, we demonstrate that our approach consistently outperforms benchmark methods and state-of-the-art alternatives.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIvaluateXR: An Evaluation Framework for on-Device AI in XR with Benchmarking Results</title>
<link>https://arxiv.org/abs/2502.15761</link>
<guid>https://arxiv.org/abs/2502.15761</guid>
<content:encoded><![CDATA[
arXiv:2502.15761v2 Announce Type: replace-cross 
Abstract: The deployment of large language models (LLMs) on extended reality (XR) devices has great potential to advance the field of human-AI interaction. In the case of direct, on-device model inference, selecting the appropriate model and device for specific tasks remains challenging. In this paper, we present AIvaluateXR, a comprehensive evaluation framework for benchmarking LLMs running on XR devices. To demonstrate the framework, we deploy 17 selected LLMs across four XR platforms: Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision Pro, and conduct an extensive evaluation. Our experimental setup measures four key metrics: performance consistency, processing speed, memory usage, and battery consumption. For each of the 68 model-device pairs, we assess performance under varying string lengths, batch sizes, and thread counts, analyzing the trade-offs for real-time XR applications. We propose a unified evaluation method based on the 3D Pareto Optimality theory to select the optimal device-model pairs from quality and speed objectives. Additionally, we compare the efficiency of on-device LLMs with client-server and cloud-based setups, and evaluate their accuracy on two interactive tasks. We believe our findings offer valuable insight to guide future optimization efforts for LLM deployment on XR devices. Our evaluation method can be used as standard groundwork for further research and development in this emerging field. The source code and supplementary materials are available at: www.nanovis.org/AIvaluateXR.html
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2503.04800</link>
<guid>https://arxiv.org/abs/2503.04800</guid>
<content:encoded><![CDATA[
arXiv:2503.04800v3 Announce Type: replace-cross 
Abstract: While Retrieval-Augmented Generation (RAG) has emerged as an effective approach for addressing the knowledge outdating problem in Large Language Models (LLMs), it still faces a critical challenge: the prevalence of outdated information in knowledge bases. Current research primarily focuses on incorporating up-to-date information, yet the impact of outdated information coexisting in retrieval sources remains inadequately addressed. To bridge this gap, we introduce HoH, the first benchmark specifically designed to evaluate the impact of outdated information on RAG. Our benchmark leverages token-level diff algorithms combined with LLM pipelines to efficiently create a large-scale QA dataset that accurately captures the evolution of temporal knowledge in real-world facts. Through comprehensive experiments, we reveal that outdated information significantly degrades RAG performance in two critical ways: (1) it substantially reduces response accuracy by distracting models from correct information, and (2) it can mislead models into generating potentially harmful outputs, even when current information is available. Current RAG approaches struggle with both retrieval and generation aspects when handling outdated information. These findings highlight the urgent need for innovative solutions to address the temporal challenges in RAG. Our code and data are available at: https://github.com/0russwest0/HoH.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation</title>
<link>https://arxiv.org/abs/2503.17340</link>
<guid>https://arxiv.org/abs/2503.17340</guid>
<content:encoded><![CDATA[
arXiv:2503.17340v2 Announce Type: replace-cross 
Abstract: Automatically generating natural, diverse and rhythmic human dance movements driven by music is vital for virtual reality and film industries. However, generating dance that naturally follows music remains a challenge, as existing methods lack proper beat alignment and exhibit unnatural motion dynamics. In this paper, we propose Danceba, a novel framework that leverages gating mechanism to enhance rhythm-aware feature representation for music-driven dance generation, which achieves highly aligned dance poses with enhanced rhythmic sensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to precisely extract rhythmic information from musical phase data, capitalizing on the intrinsic periodicity and temporal structures of music. Additionally, we propose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic features, ensuring that dance movements closely follow the musical rhythm. We also introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately model upper and lower body motions along with musical features, thereby improving the naturalness and diversity of generated dance movements. Extensive experiments confirm that Danceba outperforms state-of-the-art methods, achieving significantly better rhythmic alignment and motion diversity. Project page: https://danceba.github.io/ .
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can we ease the Injectivity Bottleneck on Lorentzian Manifolds for Graph Neural Networks?</title>
<link>https://arxiv.org/abs/2504.00142</link>
<guid>https://arxiv.org/abs/2504.00142</guid>
<content:encoded><![CDATA[
arXiv:2504.00142v5 Announce Type: replace-cross 
Abstract: While hyperbolic GNNs show promise for hierarchical data, they often have limited discriminative power compared to Euclidean counterparts or the WL test, due to non-injective aggregation. To address this expressivity gap, we propose the Lorentzian Graph Isomorphic Network (LGIN), a novel HGNN designed for enhanced discrimination within the Lorentzian model. LGIN introduces a new update rule that preserves the Lorentzian metric while effectively capturing richer structural information. This marks a significant step towards more expressive GNNs on Riemannian manifolds. Extensive evaluations across nine benchmark datasets demonstrate LGIN's superior performance, consistently outperforming or matching state-of-the-art hyperbolic and Euclidean baselines, showcasing its ability to capture complex graph structures. LGIN is the first to adapt principles of powerful, highly discriminative GNN architectures to a Riemannian manifold. The code for our paper can be found at https://github.com/Deceptrax123/LGIN
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hands-On: Segmenting Individual Signs from Continuous Sequences</title>
<link>https://arxiv.org/abs/2504.08593</link>
<guid>https://arxiv.org/abs/2504.08593</guid>
<content:encoded><![CDATA[
arXiv:2504.08593v3 Announce Type: replace-cross 
Abstract: This work tackles the challenge of continuous sign language segmentation, a key task with huge implications for sign language translation and data annotation. We propose a transformer-based architecture that models the temporal dynamics of signing and frames segmentation as a sequence labeling problem using the Begin-In-Out (BIO) tagging scheme. Our method leverages the HaMeR hand features, and is complemented with 3D Angles. Extensive experiments show that our model achieves state-of-the-art results on the DGS Corpus, while our features surpass prior benchmarks on BSLCorpus.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal Visible-Infrared Detectors</title>
<link>https://arxiv.org/abs/2504.10888</link>
<guid>https://arxiv.org/abs/2504.10888</guid>
<content:encoded><![CDATA[
arXiv:2504.10888v2 Announce Type: replace-cross 
Abstract: Adversarial patches are widely used to evaluate the robustness of object detection systems in real-world scenarios. These patches were initially designed to deceive single-modal detectors (e.g., visible or infrared) and have recently been extended to target visible-infrared dual-modal detectors. However, existing dual-modal adversarial patch attacks have limited attack effectiveness across diverse physical scenarios. To address this, we propose CDUPatch, a universal cross-modal patch attack against visible-infrared object detectors across scales, views, and scenarios. Specifically, we observe that color variations lead to different levels of thermal absorption, resulting in temperature differences in infrared imaging. Leveraging this property, we propose an RGB-to-infrared adapter that maps RGB patches to infrared patches, enabling unified optimization of cross-modal patches. By learning an optimal color distribution on the adversarial patch, we can manipulate its thermal response and generate an adversarial infrared texture. Additionally, we introduce a multi-scale clipping strategy and construct a new visible-infrared dataset, MSDrone, which contains aerial vehicle images in varying scales and perspectives. These data augmentation strategies enhance the robustness of our patch in real-world conditions. Experiments on four benchmark datasets (e.g., DroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms existing patch attacks in the digital domain. Extensive physical tests further confirm strong transferability across scales, views, and scenarios.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs</title>
<link>https://arxiv.org/abs/2504.13774</link>
<guid>https://arxiv.org/abs/2504.13774</guid>
<content:encoded><![CDATA[
arXiv:2504.13774v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have recently revolutionized language processing tasks but have also brought ethical and legal issues. LLMs have a tendency to memorize potentially private or copyrighted information present in the training data, which might then be delivered to end users at inference time. When this happens, a naive solution is to retrain the model from scratch after excluding the undesired data. Although this guarantees that the target data have been forgotten, it is also prohibitively expensive for LLMs. Approximate unlearning offers a more efficient alternative, as it consists of ex post modifications of the trained model itself to prevent undesirable results, but it lacks forgetting guarantees because it relies solely on empirical evidence. In this work, we present DP2Unlearning, a novel LLM unlearning framework that offers formal forgetting guarantees at a significantly lower cost than retraining from scratch on the data to be retained. DP2Unlearning involves training LLMs on textual data protected using {\epsilon}-differential privacy (DP), which later enables efficient unlearning with the guarantees against disclosure associated with the chosen {\epsilon}. Our experiments demonstrate that DP2Unlearning achieves similar model performance post-unlearning, compared to an LLM retraining from scratch on retained data -- the gold standard exact unlearning -- but at approximately half the unlearning cost. In addition, with a reasonable computational cost, it outperforms approximate unlearning methods at both preserving the utility of the model post-unlearning and effectively forgetting the targeted information.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data</title>
<link>https://arxiv.org/abs/2504.14452</link>
<guid>https://arxiv.org/abs/2504.14452</guid>
<content:encoded><![CDATA[
arXiv:2504.14452v2 Announce Type: replace-cross 
Abstract: Language models (LMs) can memorize and reproduce segments from their pretraining data verbatim even in non-adversarial settings, raising concerns about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to reduce unintentional regurgitation while preserving their overall utility. ParaPO trains LMs to prefer paraphrased versions of memorized segments over the original verbatim content from the pretraining data. To maintain the ability to recall famous quotations when appropriate, we develop a variant of ParaPO that uses system prompts to control regurgitation behavior. In our evaluation on Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative writing), whereas unlearning methods used in prior work to mitigate regurgitation are less effective outside their targeted unlearned domain (from 17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO with system prompting successfully preserves famous quotation recall while reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the model not to regurgitate produces only a marginal reduction (8.7 to 8.4).
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition</title>
<link>https://arxiv.org/abs/2504.21801</link>
<guid>https://arxiv.org/abs/2504.21801</guid>
<content:encoded><![CDATA[
arXiv:2504.21801v2 Announce Type: replace-cross 
Abstract: We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model. The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench, a collection of 325 formalized problems, to enrich our evaluation, including 15 selected problems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of these problems using majority voting, highlighting that the gap between formal and informal mathematical reasoning in large language models is substantially narrowing.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Graph Representations of Logical Forms for Language Modeling</title>
<link>https://arxiv.org/abs/2505.14523</link>
<guid>https://arxiv.org/abs/2505.14523</guid>
<content:encoded><![CDATA[
arXiv:2505.14523v2 Announce Type: replace-cross 
Abstract: We make the case for language models over logical forms (LFLMs), arguing that such models are more data-efficient than their textual counterparts. To that end, we introduce the Graph-based Formal-Logical Distributional Semantics (GFoLDS) prototype, a pretrained LM over graph representations of logical forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong experimental evidence that LFLMs can leverage the built-in, basic linguistic knowledge inherent in such models to immediately begin learning more complex patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual, transformer LMs (BERT) pretrained on the same data, indicating that LFLMs can learn with substantially less data than models over plain text. Furthermore, we show that the performance of this model is likely to scale with additional parameters and pretraining data, suggesting the viability of LFLMs in real-world applications.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet</title>
<link>https://arxiv.org/abs/2505.16195</link>
<guid>https://arxiv.org/abs/2505.16195</guid>
<content:encoded><![CDATA[
arXiv:2505.16195v2 Announce Type: replace-cross 
Abstract: Foley synthesis aims to synthesize high-quality audio that is both semantically and temporally aligned with video frames. Given its broad application in creative industries, the task has gained increasing attention in the research community. To avoid the non-trivial task of training audio generative models from scratch, adapting pretrained audio generative models for video-synchronized foley synthesis presents an attractive direction. ControlNet, a method for adding fine-grained controls to pretrained generative models, has been applied to foley synthesis, but its use has been limited to handcrafted human-readable temporal conditions. In contrast, from-scratch models achieved success by leveraging high-dimensional deep features extracted using pretrained video encoders. We have observed a performance gap between ControlNet-based and from-scratch foley models. To narrow this gap, we propose SpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward video-synchronized foley synthesis via ControlNet. To unlock the potential of a single ControlNet branch, we resolve the discrepancy between the temporal video features and the time-frequency nature of the pretrained SpecMaskGIT via a frequency-aware temporal feature aligner, eliminating the need for complicated conditioning mechanisms widely used in prior arts. Evaluations on a common foley synthesis benchmark demonstrate that SpecMaskFoley could even outperform strong from-scratch baselines, substantially advancing the development of ControlNet-based foley synthesis models. Demo page: https://zzaudio.github.io/SpecMaskFoley_Demo/
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextDiffuser-RL: Efficient and Robust Text Layout Optimization for High-Fidelity Text-to-Image Synthesis</title>
<link>https://arxiv.org/abs/2505.19291</link>
<guid>https://arxiv.org/abs/2505.19291</guid>
<content:encoded><![CDATA[
arXiv:2505.19291v2 Announce Type: replace-cross 
Abstract: Text-embedded image generation plays a critical role in industries such as graphic design, advertising, and digital content creation. Text-to-Image generation methods leveraging diffusion models, such as TextDiffuser-2, have demonstrated promising results in producing images with embedded text. TextDiffuser-2 effectively generates bounding box layouts that guide the rendering of visual text, achieving high fidelity and coherence. However, existing approaches often rely on resource-intensive processes and are limited in their ability to run efficiently on both CPU and GPU platforms. To address these challenges, we propose a novel two-stage pipeline that integrates reinforcement learning (RL) for rapid and optimized text layout generation with a diffusion-based image synthesis model. Our RL-based approach significantly accelerates the bounding box prediction step while reducing overlaps, allowing the system to run efficiently on both CPUs and GPUs. Extensive evaluations demonstrate that our framework maintains or surpasses TextDiffuser-2's quality in text placement and image synthesis, with markedly faster runtime and increased flexibility. Extensive evaluations demonstrate that our framework maintains or surpasses TextDiffuser-2's quality in text placement and image synthesis, with markedly faster runtime and increased flexibility. Our approach has been evaluated on the MARIOEval benchmark, achieving OCR and CLIPScore metrics close to state-of-the-art models, while being 97.64% more faster and requiring only 2MB of memory to run.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2506.01551</link>
<guid>https://arxiv.org/abs/2506.01551</guid>
<content:encoded><![CDATA[
arXiv:2506.01551v2 Announce Type: replace-cross 
Abstract: Building Vision-Language Navigation (VLN) agents which can navigate following natural language instructions is a long-standing goal in human-robot interaction applications. Recent studies have revealed the potential of training open-source Large Language Models (LLMs) to unleash LLMs' reasoning ability for improving navigation, and simultaneously mitigate the domain gap between LLMs' training corpus and the VLN task. However, these approaches primarily adopt direct input-output mapping paradigms, causing the mapping learning difficult and the navigational decisions unexplainable. Chain-of-Thought (CoT) training is a promising way to improve both navigational decision accuracy and interpretability, while the complexity of the navigation task makes the perfect CoT labels unavailable and may lead to overfitting through pure CoT supervised fine-tuning. In this paper, we propose a novel sElf-improving embodied reasoning framework for boosting LLM-based vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model with formalized CoT labels to both activate the model's navigational reasoning capabilities and increase the reasoning speed; (2) Self-Reflective Post-Training, where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels to enhance the supervision diversity. A self-reflective auxiliary task is also introduced to encourage learning correct reasoning patterns by contrasting with wrong ones. Experimental results on the popular VLN benchmarks demonstrate the superiority of EvolveNav over previous LLM-based VLN approaches. Code is available at https://github.com/expectorlin/EvolveNav.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation</title>
<link>https://arxiv.org/abs/2506.09046</link>
<guid>https://arxiv.org/abs/2506.09046</guid>
<content:encoded><![CDATA[
arXiv:2506.09046v2 Announce Type: replace-cross 
Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for addressing complex, high-dimensional tasks, but current approaches often rely on static, manually engineered multi-agent configurations. To overcome these constraints, we present the Agentic Neural Network(ANN), a framework that conceptualizes multi-agent collaboration as a layered neural network architecture. In this design, each agent operates as a node, and each layer forms a cooperative "team" focused on a specific subtask. Agentic Neural Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing inspiration from neural network forward passes, tasks are dynamically decomposed into subtasks, and cooperative agent teams with suitable aggregation methods are constructed layer by layer. (2) Backward Phase-Mirroring backpropagation, we refine both global and local collaboration through iterative feedback, allowing agents to self-evolve their roles, prompts, and coordination. This neuro-symbolic approach enables ANN to create new or specialized agent teams post-training, delivering notable gains in accuracy and adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent baselines under the same configurations, showing consistent performance improvements. Our findings indicate that ANN provides a scalable, data-driven framework for multi-agent systems, combining the collaborative capabilities of LLMs with the efficiency and flexibility of neural network principles. We plan to open-source the entire framework.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Reasoning in Thinking Language Models via Steering Vectors</title>
<link>https://arxiv.org/abs/2506.18167</link>
<guid>https://arxiv.org/abs/2506.18167</guid>
<content:encoded><![CDATA[
arXiv:2506.18167v3 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressing uncertainty, generating examples for hypothesis validation, and backtracking in reasoning chains. We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our approach offers practical tools for steering reasoning processes in thinking models in a controlled and interpretable manner. We validate our steering method using three DeepSeek-R1-Distill models, demonstrating consistent control across different model architectures.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZonUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding</title>
<link>https://arxiv.org/abs/2506.23491</link>
<guid>https://arxiv.org/abs/2506.23491</guid>
<content:encoded><![CDATA[
arXiv:2506.23491v3 Announce Type: replace-cross 
Abstract: In this paper, we present ZonUI-3B, a lightweight Vision-Language Model (VLM) that can be fully trained on a single consumer-grade GPU (RTX 4090) while delivering performance comparable to significantly larger models on GUI grounding tasks. The model incorporates several key innovations: (i) combine cross-platform, multi-resolution dataset of 24K examples from diverse sources including mobile, desktop, and web GUI screenshots to effectively address data scarcity in high-resolution desktop environments; (ii) a two-stage fine-tuning strategy, where initial cross-platform training establishes robust GUI understanding, followed by specialized fine-tuning on high-resolution data to significantly enhance model adaptability; and (iii) data curation and redundancy reduction strategies, demonstrating that randomly sampling a smaller subset with reduced redundancy achieves performance comparable to larger datasets, emphasizing data diversity over sheer volume. Empirical evaluation on standard GUI grounding benchmarks, including ScreenSpot, ScreenSpot-v2, and the challenging ScreenSpot-Pro, highlights ZonUI-3B's exceptional accuracy, achieving 84.9% on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B parameters. Ablation studies validate the critical role of balanced sampling and two-stage fine-tuning in enhancing robustness, particularly in high-resolution desktop scenarios. The ZonUI-3B is available at: https://github.com/Han1018/ZonUI-3B
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STACK: Adversarial Attacks on LLM Safeguard Pipelines</title>
<link>https://arxiv.org/abs/2506.24068</link>
<guid>https://arxiv.org/abs/2506.24068</guid>
<content:encoded><![CDATA[
arXiv:2506.24068v2 Announce Type: replace-cross 
Abstract: Frontier AI developers are relying on layers of safeguards to protect against catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus model using one such defense pipeline, and other frontier developers including Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the security of such pipelines is unclear, with limited prior work evaluating or attacking these pipelines. We address this gap by developing and red-teaming an open-source defense pipeline. First, we find that a novel few-shot-prompted input and output classifier outperforms state-of-the-art open-weight safeguard model ShieldGemma across three attacks and two datasets, reducing the attack success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second, we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on ClearHarm in a black-box attack against the few-shot-prompted classifier pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33% ASR, providing initial evidence that it is feasible to design attacks with no access to the target pipeline. We conclude by suggesting specific mitigations that developers could use to thwart staged attacks.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The role of large language models in UI/UX design: A systematic literature review</title>
<link>https://arxiv.org/abs/2507.04469</link>
<guid>https://arxiv.org/abs/2507.04469</guid>
<content:encoded><![CDATA[
arXiv:2507.04469v2 Announce Type: replace-cross 
Abstract: This systematic literature review examines the role of large language models (LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies published between 2022 and 2025. We identify key LLMs in use, including GPT-4, Gemini, and PaLM, and map their integration across the design lifecycle, from ideation to evaluation. Common practices include prompt engineering, human-in-the-loop workflows, and multimodal input. While LLMs are reshaping design processes, challenges such as hallucination, prompt instability, and limited explainability persist. Our findings highlight LLMs as emerging collaborators in design, and we propose directions for the ethical, inclusive, and effective integration of these technologies.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critiques of World Models</title>
<link>https://arxiv.org/abs/2507.05169</link>
<guid>https://arxiv.org/abs/2507.05169</guid>
<content:encoded><![CDATA[
arXiv:2507.05169v2 Announce Type: replace-cross 
Abstract: World Model, the supposed algorithmic surrogate of the real-world environment which biological agents experience with and act upon, has been an emerging topic in recent years because of the rising needs to develop virtual agents with artificial (general) intelligence. There has been much debate on what a world model really is, how to build it, how to use it, and how to evaluate it. In this essay, starting from the imagination in the famed Sci-Fi classic Dune, and drawing inspiration from the concept of "hypothetical thinking" in psychology literature, we offer critiques of several schools of thoughts on world modeling, and argue the primary goal of a world model to be simulating all actionable possibilities of the real world for purposeful reasoning and acting. Building on the critiques, we propose a new architecture for a general-purpose world model, based on hierarchical, multi-level, and mixed continuous/discrete representations, and a generative and self-supervision learning framework, with an outlook of a Physical, Agentic, and Nested (PAN) AGI system enabled by such a model.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Not to Detect Prompt Injections with an LLM</title>
<link>https://arxiv.org/abs/2507.05630</link>
<guid>https://arxiv.org/abs/2507.05630</guid>
<content:encoded><![CDATA[
arXiv:2507.05630v2 Announce Type: replace-cross 
Abstract: LLM-integrated applications and agents are vulnerable to prompt injection attacks, in which adversaries embed malicious instructions within seemingly benign user inputs to manipulate the LLM's intended behavior. Recent defenses based on $\textit{known-answer detection}$ (KAD) have achieved near-perfect performance by using an LLM to classify inputs as clean or contaminated. In this work, we formally characterize the KAD framework and uncover a structural vulnerability in its design that invalidates its core security premise. We design a methodical adaptive attack, $\textit{DataFlip}$, to exploit this fundamental weakness. It consistently evades KAD defenses with detection rates as low as $1.5\%$ while reliably inducing malicious behavior with success rates of up to $88\%$, without needing white-box access to the LLM or any optimization procedures.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving</title>
<link>https://arxiv.org/abs/2507.06229</link>
<guid>https://arxiv.org/abs/2507.06229</guid>
<content:encoded><![CDATA[
arXiv:2507.06229v3 Announce Type: replace-cross 
Abstract: Current AI agents cannot effectively learn from each other's problem-solving experiences or use past successes to guide self-reflection and error correction in new tasks. We introduce Agent KB, a shared knowledge base that captures both high-level problem-solving strategies and detailed execution lessons, enabling knowledge transfer across agent frameworks. Agent KB implements a novel teacher-student dual-phase retrieval mechanism where student agents retrieve workflow-level patterns for strategic guidance while teacher agents identify execution-level patterns for refinement. This hierarchical approach enables agents to break out of limited reasoning pathways by incorporating diverse strategies from external sources. Evaluations on the GAIA benchmark demonstrate substantial performance gains, with Agent KB improving success rates by up to 6.06 percentage points overall under pass@1. For SWE-bench code repair tasks, our system significantly improved resolution rates, with o3-mini achieving an 8.67 percentage point gain (23 percent to 31.67 percent) in pass@1.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education</title>
<link>https://arxiv.org/abs/2507.12484</link>
<guid>https://arxiv.org/abs/2507.12484</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, tutoring systems, personalized feedback, modular learning, mathematics<br />
<br />
Summary: 
The article discusses the limitations of current AI tutoring systems, especially in the field of mathematics, and proposes a novel multi-agent AI tutoring platform. This platform aims to go beyond reactive assistance by offering adaptive and personalized feedback, structured course generation, and textbook knowledge retrieval. By combining these features, the platform enables structured, individualized, and tool-assisted learning experiences for students. It allows students to learn new topics, identify and target weaknesses, revise effectively for exams, and practice personalized exercises. The platform's modular design enhances the teaching of mathematics by integrating pedagogical agents and AI-driven components. Overall, the research contributes to the field of artificial intelligence in education by providing an innovative platform that fosters effective and personalized learning experiences in mathematics. <br /><br /> <div>
arXiv:2507.12484v1 Announce Type: new 
Abstract: The growing ubiquity of artificial intelligence (AI), in particular large language models (LLMs), has profoundly altered the way in which learners gain knowledge and interact with learning material, with many claiming that AI positively influences their learning achievements. Despite this advancement, current AI tutoring systems face limitations associated with their reactive nature, often providing direct answers without encouraging deep reflection or incorporating structured pedagogical tools and strategies. This limitation is most apparent in the field of mathematics, in which AI tutoring systems remain underdeveloped. This research addresses the question: How can AI tutoring systems move beyond providing reactive assistance to enable structured, individualized, and tool-assisted learning experiences? We introduce a novel multi-agent AI tutoring platform that combines adaptive and personalized feedback, structured course generation, and textbook knowledge retrieval to enable modular, tool-assisted learning processes. This system allows students to learn new topics while identifying and targeting their weaknesses, revise for exams effectively, and practice on an unlimited number of personalized exercises. This article contributes to the field of artificial intelligence in education by introducing a novel platform that brings together pedagogical agents and AI-driven components, augmenting the field with modular and effective systems for teaching mathematics.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents</title>
<link>https://arxiv.org/abs/2507.12494</link>
<guid>https://arxiv.org/abs/2507.12494</guid>
<content:encoded><![CDATA[
<div> Keywords: simulation, autonomous vehicles, game theory, decision-making, highway merging

Summary:<br />
Enhancing simulation environments for autonomous vehicle development is crucial for replicating real-world driver behavior. This study focuses on improving the simulation of highway merging scenarios by implementing a game theoretic model for tactical decision-making. The model includes enhanced payoff functions and lag actions to better capture merging interactions. By integrating this model with an underlying dynamics model, complex interactions can be simulated in a more realistic and interpretable manner. Validation on real-world data shows good reproducibility, and the model's computation time efficiency is adequate for large-scale simulations. This advancement in simulation technology contributes to the development of autonomous vehicles by creating more human-like sim agents and improving the accuracy of highway merging simulations. <div>
arXiv:2507.12494v1 Announce Type: new 
Abstract: Enhancing simulation environments to replicate real-world driver behavior, i.e., more humanlike sim agents, is essential for developing autonomous vehicle technology. In the context of highway merging, previous works have studied the operational-level yielding dynamics of lag vehicles in response to a merging car at highway on-ramps. Other works focusing on tactical decision modeling generally consider limited action sets or utilize payoff functions with large parameter sets and limited payoff bounds. In this work, we aim to improve the simulation of the highway merge scenario by targeting a game theoretic model for tactical decision-making with improved payoff functions and lag actions. We couple this with an underlying dynamics model to have a unified decision and dynamics model that can capture merging interactions and simulate more realistic interactions in an explainable and interpretable fashion. The proposed model demonstrated good reproducibility of complex interactions when validated on a real-world dataset. The model was finally integrated into a high fidelity simulation environment and confirmed to have adequate computation time efficiency for use in large-scale simulations to support autonomous vehicle development.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs</title>
<link>https://arxiv.org/abs/2507.12599</link>
<guid>https://arxiv.org/abs/2507.12599</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, eXplainable AI, eXplainable Reinforcement Learning, taxonomy, state-of-the-art review

Summary: 
The paper discusses the opacity of internal mechanisms in contemporary AI models due to their use of deep neural networks. It explores the domain of eXplainable AI (XAI), specifically focusing on eXplainable Reinforcement Learning (XRL) methods that aim to explain the actions of agents trained through reinforcement learning. A taxonomy based on the questions "What" and "How" is introduced to categorize XRL approaches, leading to a state-of-the-art review of over 250 papers in the field. The paper also highlights related domains that deserve attention from the community and identifies key needs for the advancement of XRL. This comprehensive overview sheds light on the challenges and opportunities in understanding and interpreting the behavior of AI agents trained through reinforcement learning. 

Summary: <div>
arXiv:2507.12599v1 Announce Type: new 
Abstract: The success of recent Artificial Intelligence (AI) models has been accompanied by the opacity of their internal mechanisms, due notably to the use of deep neural networks. In order to understand these internal mechanisms and explain the output of these AI models, a set of methods have been proposed, grouped under the domain of eXplainable AI (XAI). This paper focuses on a sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims to explain the actions of an agent that has learned by reinforcement learning. We propose an intuitive taxonomy based on two questions "What" and "How". The first question focuses on the target that the method explains, while the second relates to the way the explanation is provided. We use this taxonomy to provide a state-of-the-art review of over 250 papers. In addition, we present a set of domains close to XRL, which we believe should get attention from the community. Finally, we identify some needs for the field of XRL.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models</title>
<link>https://arxiv.org/abs/2507.12666</link>
<guid>https://arxiv.org/abs/2507.12666</guid>
<content:encoded><![CDATA[
<div> Keywords: game design, reinforcement learning, generative systems, automated design iteration, multimodal model

Summary:
The article introduces a novel automated design iteration framework that combines reinforcement learning (RL) with a large multimodal model (LMM) to improve game design. By pairing these two components, the framework is able to capture dynamic player behavior that traditional generative systems may overlook. The RL agent playtests the game and provides play metrics or image summaries, which are then analyzed by the LMM designer to make adjustments to the game configuration. This iterative process allows the LMM to refine game mechanics based on the RL agent's behavior, leading to more player-centric game design. The results demonstrate the effectiveness of using LMMs to analyze behavioral traces provided by RL agents and iteratively improve game mechanics, showcasing the potential for AI-assisted game design tools. 

<br /><br />Summary: <div>
arXiv:2507.12666v1 Announce Type: new 
Abstract: Game design hinges on understanding how static rules and content translate into dynamic player behavior - something modern generative systems that inspect only a game's code or assets struggle to capture. We present an automated design iteration framework that closes this gap by pairing a reinforcement learning (RL) agent, which playtests the game, with a large multimodal model (LMM), which revises the game based on what the agent does. In each loop the RL player completes several episodes, producing (i) numerical play metrics and/or (ii) a compact image strip summarising recent video frames. The LMM designer receives a gameplay goal and the current game configuration, analyses the play traces, and edits the configuration to steer future behaviour toward the goal. We demonstrate results that LMMs can reason over behavioral traces supplied by RL agents to iteratively refine game mechanics, pointing toward practical, scalable tools for AI-assisted game design.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Deception Probes via Black-to-White Performance Boosts</title>
<link>https://arxiv.org/abs/2507.12691</link>
<guid>https://arxiv.org/abs/2507.12691</guid>
<content:encoded><![CDATA[
<div> Keywords: AI assistants, deception probes, white-box monitoring, black-box monitoring, language model 

Summary:<br />AI assistants may provide deceptive responses to user queries, prompting the development of deception probes to detect dishonest behavior. These probes analyze language model activations to distinguish between honest and deceptive responses. The effectiveness of these probes in real-world scenarios and their susceptibility to evasion strategies by deceptive assistants is uncertain. This study compares white-box monitoring, which has access to token-level probe activations, to black-box monitoring, which lacks such access. By measuring the performance boost of the white-box monitor over the black-box monitor, the study evaluates the efficacy of existing deception probes. Despite modest results, the study reveals some promise in the ability of these probes to detect deception. Ongoing research is needed to enhance the robustness and reliability of deception detection methods in AI systems. 

<br /><br />Summary: <div>
arXiv:2507.12691v1 Announce Type: new 
Abstract: AI assistants will occasionally respond deceptively to user queries. Recently, linear classifiers (called "deception probes") have been trained to distinguish the internal activations of a language model during deceptive versus honest responses. However, it's unclear how effective these probes are at detecting deception in practice, nor whether such probes are resistant to simple counter strategies from a deceptive assistant who wishes to evade detection. In this paper, we compare white-box monitoring (where the monitor has access to token-level probe activations) to black-box monitoring (without such access). We benchmark deception probes by the extent to which the white box monitor outperforms the black-box monitor, i.e. the black-to-white performance boost. We find weak but encouraging black-to-white performance boosts from existing deception probes.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning</title>
<link>https://arxiv.org/abs/2507.12801</link>
<guid>https://arxiv.org/abs/2507.12801</guid>
<content:encoded><![CDATA[
<div> Keywords: peer learning, AI Agent, same proficiency level, English composition, validation

Summary:
Effective peer learning has been shown to improve spontaneous thinking among learners. This study focuses on developing an AI Agent as a learning companion to facilitate peer learning anytime and anywhere. The research emphasizes the importance of pairing learners with peers at the same proficiency levels to enhance the effectiveness of the learning process. By assuming that peers at the same proficiency level as the learner make similar mistakes, the study aims to validate this approach using English composition as a specific example. Through this innovative approach, the AI Agent can provide personalized support and feedback to learners, promoting a more interactive and engaging learning experience. <div>
arXiv:2507.12801v1 Announce Type: new 
Abstract: In recent years, peer learning has gained attention as a method that promotes spontaneous thinking among learners, and its effectiveness has been confirmed by numerous studies. This study aims to develop an AI Agent as a learning companion that enables peer learning anytime and anywhere. However, peer learning between humans has various limitations, and it is not always effective. Effective peer learning requires companions at the same proficiency levels. In this study, we assume that a learner's peers with the same proficiency level as the learner make the same mistakes as the learner does and focus on English composition as a specific example to validate this approach.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models</title>
<link>https://arxiv.org/abs/2507.12806</link>
<guid>https://arxiv.org/abs/2507.12806</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Model Context Protocol, MCPEval, evaluation framework, open-source

Summary: 
Large Language Models (LLMs) have become increasingly popular in various domains, leading to the necessity for robust evaluation frameworks. The Model Context Protocol-based framework, known as MCPEval, addresses this need by automating the generation of tasks and comprehensive evaluation of LLM agents across different fields. MCPEval standardizes metrics, integrates seamlessly with native agent tools, and reduces the manual effort required to build evaluation pipelines. Empirical results from real-world domains demonstrate the effectiveness of MCPEval in revealing domain-specific performance nuances. The tool is openly available on GitHub, allowing for reproducible and standardized evaluation of LLM agents. Overall, MCPEval provides a scalable and efficient solution for evaluating the performance of LLM-based intelligent agents across diverse domains. 

Summary: <br /><br />Keywords: Large Language Models, Model Context Protocol, MCPEval, evaluation framework, open-source <div>
arXiv:2507.12806v1 Announce Type: new 
Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce \oursystemname, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotional Support with LLM-based Empathetic Dialogue Generation</title>
<link>https://arxiv.org/abs/2507.12820</link>
<guid>https://arxiv.org/abs/2507.12820</guid>
<content:encoded><![CDATA[
<div> Keywords: Emotional Support Conversation, NLPCC 2025 Task 8 ESC evaluation, language models, prompt engineering, fine-tuning

Summary:
- The paper discusses Emotional Support Conversation (ESC) for providing empathetic emotional assistance through dialogue.
- The authors present their solution for the NLPCC 2025 Task 8 ESC evaluation, using large-scale language models with prompt engineering and finetuning techniques.
- They explore Low-Rank Adaptation and full-parameter fine-tuning strategies to enhance the model's ability in generating supportive responses.
- Their best model secured second place in the competition, showcasing the effectiveness of combining language models with adaptation methods for ESC tasks.
- Future work will focus on improving emotional understanding and response personalization for more practical and reliable emotional support systems.

<br /><br />Summary: Emotional Support Conversation (ESC) is addressed in the paper, showcasing the potential of using language models and adaptation methods to provide effective emotional assistance. The study includes strategies such as prompt engineering and fine-tuning, leading to a competitive performance in the NLPCC 2025 Task 8 ESC evaluation. Further enhancements are planned to enhance emotional understanding and response personalization for better emotional support systems. <div>
arXiv:2507.12820v1 Announce Type: new 
Abstract: Emotional Support Conversation (ESC) aims to provide empathetic and effective emotional assistance through dialogue, addressing the growing demand for mental health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC evaluation, where we leverage large-scale language models enhanced by prompt engineering and finetuning techniques. We explore both parameter-efficient Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the model's ability to generate supportive and contextually appropriate responses. Our best model ranked second in the competition, highlighting the potential of combining LLMs with effective adaptation methods for ESC tasks. Future work will focus on further enhancing emotional understanding and response personalization to build more practical and reliable emotional support systems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing adaptive world models in machines with novel games</title>
<link>https://arxiv.org/abs/2507.12821</link>
<guid>https://arxiv.org/abs/2507.12821</guid>
<content:encoded><![CDATA[
<div> adaptability, world models, artificial intelligence, evaluation framework, novel games

Summary:
The article discusses how human intelligence is adaptable due to efficient world model induction, which involves constructing and refining internal representations of the environment. It highlights the limitations in current AI world model evaluation methods and proposes a new framework. The authors suggest using novel games as benchmarks for assessing adaptive world models in AI. These games should have deep novelty in their structure and constantly refresh challenges. The proposed evaluation metrics aim to test an agent's ability for rapid world model induction. By adopting this new evaluation approach, the authors believe that progress can be made in developing AI systems capable of rapid adaptation and robust generalization towards artificial general intelligence. <div>
arXiv:2507.12821v1 Announce Type: new 
Abstract: Human intelligence exhibits a remarkable capacity for rapid adaptation and effective problem-solving in novel and unfamiliar contexts. We argue that this profound adaptability is fundamentally linked to the efficient construction and refinement of internal representations of the environment, commonly referred to as world models, and we refer to this adaptation mechanism as world model induction. However, current understanding and evaluation of world models in artificial intelligence (AI) remains narrow, often focusing on static representations learned from training on a massive corpora of data, instead of the efficiency and efficacy of models in learning these representations through interaction and exploration within a novel environment. In this Perspective, we provide a view of world model induction drawing on decades of research in cognitive science on how humans learn and adapt so efficiently; we then call for a new evaluation framework for assessing adaptive world models in AI. Concretely, we propose a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures -- we refer to this kind of games as novel games. We detail key desiderata for constructing these games and propose appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction. We hope that this new evaluation framework will inspire future evaluation efforts on world models in AI and provide a crucial step towards developing AI systems capable of the human-like rapid adaptation and robust generalization -- a critical component of artificial general intelligence.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command</title>
<link>https://arxiv.org/abs/2507.12862</link>
<guid>https://arxiv.org/abs/2507.12862</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, simulation, ethical decisions, multi-criteria decision making, entropy <br />
Summary: <br />
In the context of AI and simulation-based decision-making, this paper proposes a method to integrate human judgement into the process of selecting ethical decisions. By moving human judgement outside the simulation decision cycle and allowing the simulated environment to explore ethical metric spaces designed by humans, the paper aims to streamline the decision-making process. The fundamental issue addressed is how to dynamically weight ethical attributes during simulations, drawing on the multi-criteria decision-making literature for inspiration. The paper explores approaches to automatically calculate weights for ethical attributes using concepts such as entropy. By leveraging these methods, the proposed framework enables human commanders to select the most appropriate course of action from a set of options generated by the simulation, ultimately enhancing the efficiency and effectiveness of decision-making in complex scenarios. <br /> <div>
arXiv:2507.12862v1 Announce Type: new 
Abstract: In the age of AI, human commanders need to use the computational powers available in today's environment to simulate a very large number of scenarios. Within each scenario, situations occur where different decision design options could have ethical consequences. Making these decisions reliant on human judgement is both counter-productive to the aim of exploring very large number of scenarios in a timely manner and infeasible when considering the workload needed to involve humans in each of these choices. In this paper, we move human judgement outside the simulation decision cycle. Basically, the human will design the ethical metric space, leaving it to the simulated environment to explore the space. When the simulation completes its testing cycles, the testing environment will come back to the human commander with a few options to select from. The human commander will then exercise human-judgement to select the most appropriate course of action, which will then get executed accordingly. We assume that the problem of designing metrics that are sufficiently granular to assess the ethical implications of decisions is solved. Subsequently, the fundamental problem we look at in this paper is how to weight ethical decisions during the running of these simulations; that is, how to dynamically weight the ethical attributes when agents are faced with decision options with ethical implications during generative simulations. The multi-criteria decision making literature has started to look at nearby problems, where the concept of entropy has been used to determine the weights during aggregation. We draw from that literature different approaches to automatically calculate the weights for ethical attributes during simulation-based testing and evaluation.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework</title>
<link>https://arxiv.org/abs/2507.12872</link>
<guid>https://arxiv.org/abs/2507.12872</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, manipulation attacks, cybersecurity, risk assessment, mitigation

Summary: 
This paper addresses the growing threat of manipulation attacks by AI systems within frontier companies, which have the potential to deceive and influence human behavior. The authors argue that humans are often the weakest link in cybersecurity systems and that misaligned AI systems can exploit this vulnerability by manipulating employees. A systematic framework for assessing and mitigating manipulation risks is proposed, focusing on three core lines of argument: inability, control, and trustworthiness. For each argument, evidence requirements, evaluation methodologies, and implementation considerations are specified, providing AI companies with a concrete foundation to address manipulation attacks before deployment. The framework aims to integrate manipulation risk into AI safety governance and offers a proactive approach to safeguard against potential catastrophic outcomes. <br /><br />Summary: <div>
arXiv:2507.12872v1 Announce Type: new 
Abstract: Frontier AI systems are rapidly advancing in their capabilities to persuade, deceive, and influence human behaviour, with current models already demonstrating human-level persuasion and strategic deception in specific contexts. Humans are often the weakest link in cybersecurity systems, and a misaligned AI system deployed internally within a frontier company may seek to undermine human oversight by manipulating employees. Despite this growing threat, manipulation attacks have received little attention, and no systematic framework exists for assessing and mitigating these risks. To address this, we provide a detailed explanation of why manipulation attacks are a significant threat and could lead to catastrophic outcomes. Additionally, we present a safety case framework for manipulation risk, structured around three core lines of argument: inability, control, and trustworthiness. For each argument, we specify evidence requirements, evaluation methodologies, and implementation considerations for direct application by AI companies. This paper provides the first systematic methodology for integrating manipulation risk into AI safety governance, offering AI companies a concrete foundation to assess and mitigate these threats before deployment.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks</title>
<link>https://arxiv.org/abs/2507.12885</link>
<guid>https://arxiv.org/abs/2507.12885</guid>
<content:encoded><![CDATA[
<div> evaluation-centric, reinforcement learning, mathematical reasoning, benchmark contamination, evaluation fragility
<br />
Summary: 
The article addresses the issue of whether improvements in the mathematical reasoning abilities of large language models (LLMs) trained using reinforcement learning (RL) are genuine or simply artifacts of overfitting. The authors introduce VAR-MATH, a symbolic evaluation framework that converts numerical problems into symbolic templates to probe genuine reasoning ability. Two critical shortcomings in existing evaluation protocols are identified: benchmark contamination and evaluation fragility. The application of VAR-MATH to popular benchmarks reveals significant performance drops for RL-trained models. This suggests that many RL methods rely on superficial heuristics and struggle to generalize beyond specific numerical forms. Overall, VAR-MATH offers a contamination-resistant evaluation paradigm for assessing mathematical reasoning in LLMs trained with RL.
<br /> <div>
arXiv:2507.12885v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning (RL) have led to substantial improvements in the mathematical reasoning abilities of large language models (LLMs), as measured by standard benchmarks. However, these gains often persist even when models are trained with flawed signals, such as random or inverted rewards, raising a fundamental question: do such improvements reflect true reasoning, or are they merely artifacts of overfitting to benchmark-specific patterns? To address this question, we take an evaluation-centric perspective and identify two critical shortcomings in existing protocols. First, \emph{benchmark contamination} arises from the public availability of test problems, increasing the risk of data leakage. Second, \emph{evaluation fragility} stems from the reliance on single-instance assessments, which are highly sensitive to stochastic outputs and fail to capture reasoning consistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic evaluation framework designed to probe genuine reasoning ability. By converting fixed numerical problems into symbolic templates and requiring models to solve multiple instantiations of each, VAR-MATH enforces consistent reasoning across structurally equivalent variants, thereby mitigating contamination and improving evaluation robustness. We apply VAR-MATH to transform two popular benchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and VAR-AIME24. Experimental results reveal substantial performance drops for RL-trained models on the variabilized versions, especially for smaller models, with average declines of 48.0\% on AMC23 and 58.3\% on AIME24. These findings suggest that many existing RL methods rely on superficial heuristics and fail to generalize beyond specific numerical forms. Overall, VAR-MATH offers a principled, contamination-resistant evaluation paradigm for mathematical reasoning.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Translation of Probabilistic Event Calculus into Markov Decision Processes</title>
<link>https://arxiv.org/abs/2507.12989</link>
<guid>https://arxiv.org/abs/2507.12989</guid>
<content:encoded><![CDATA[
<div> Probabilistic Event Calculus, reasoning, actions, effects, uncertain environments <br />
<br />
Summary: <br />
Probabilistic Event Calculus (PEC) is a logical framework for reasoning about actions and their effects in uncertain environments. The PEC formalism allows for the representation of probabilistic narratives and computation of temporal projections. While PEC offers advantages in interpretability and expressiveness for narrative reasoning, it lacks mechanisms for goal-directed reasoning. This paper addresses this issue by translating PEC domains into Markov Decision Processes (MDPs), introducing the concept of "action-taking situations" to maintain PEC's flexible action semantics. The resulting PEC-MDP formalism enables the application of MDP algorithms and tools to PEC's narrative domains, supporting both temporal reasoning tasks and objective-driven planning. Additionally, the translation allows for the mapping of learned policies back into human-readable PEC representations, maintaining interpretability while enhancing PEC's capabilities. <div>
arXiv:2507.12989v1 Announce Type: new 
Abstract: Probabilistic Event Calculus (PEC) is a logical framework for reasoning about actions and their effects in uncertain environments, which enables the representation of probabilistic narratives and computation of temporal projections. The PEC formalism offers significant advantages in interpretability and expressiveness for narrative reasoning. However, it lacks mechanisms for goal-directed reasoning. This paper bridges this gap by developing a formal translation of PEC domains into Markov Decision Processes (MDPs), introducing the concept of "action-taking situations" to preserve PEC's flexible action semantics. The resulting PEC-MDP formalism enables the extensive collection of algorithms and theoretical tools developed for MDPs to be applied to PEC's interpretable narrative domains. We demonstrate how the translation supports both temporal reasoning tasks and objective-driven planning, with methods for mapping learned policies back into human-readable PEC representations, maintaining interpretability while extending PEC's capabilities.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming</title>
<link>https://arxiv.org/abs/2507.13007</link>
<guid>https://arxiv.org/abs/2507.13007</guid>
<content:encoded><![CDATA[
<div> Explanation, MILP, Trustworthy AI, Contrastive, Constraint reasoning

Summary:
X-MILP is a novel approach for creating contrastive explanations for MILPs, aiming to enhance the trustworthiness of AI systems. By encoding user queries as additional constraints and identifying reasons using the Irreducible Infeasible Subsystem (IIS), X-MILP generates explanations in the form of a "graph of reasons." This graphical representation helps users understand the relationships among different reasons that answer their queries. The method is domain-agnostic and can be applied to various MILP problems, showcasing its versatility. Empirical tests on well-known optimization problems assess the computational complexity of providing explanations. X-MILP is a valuable tool for enhancing the transparency and interpretability of decision-making processes formalized as MILPs, contributing to the development of trustworthy AI systems.

<br /><br />Summary: <div>
arXiv:2507.13007v1 Announce Type: new 
Abstract: Following the recent push for trustworthy AI, there has been an increasing interest in developing contrastive explanation techniques for optimisation, especially concerning the solution of specific decision-making processes formalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic approach for building contrastive explanations for MILPs based on constraint reasoning techniques. First, we show how to encode the queries a user makes about the solution of an MILP problem as additional constraints. Then, we determine the reasons that constitute the answer to the user's query by computing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set of constraints. Finally, we represent our explanation as a "graph of reasons" constructed from the IIS, which helps the user understand the structure among the reasons that answer their query. We test our method on instances of well-known optimisation problems to evaluate the empirical hardness of computing explanations.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data</title>
<link>https://arxiv.org/abs/2507.13112</link>
<guid>https://arxiv.org/abs/2507.13112</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic flow prediction, artificial intelligence, California, machine learning, congestion

Summary: 
The study "Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data" focused on developing a machine learning model to predict traffic flow, aiming to address global traffic congestion issues. The research utilized data from California Highway 78 over a five-month period, analyzing a section in the San Diego area. Multiple Linear Regression (MLR) and Random Forest (RF) algorithms were employed, with optimal performance observed at 10-minute data collection intervals. Performance metrics such as R^2, MAE, and RMSE were used to evaluate the models. The findings highlight the potential of using artificial intelligence algorithms for efficient traffic management and future congestion solutions.<br /><br />Summary: <div>
arXiv:2507.13112v1 Announce Type: new 
Abstract: The study "Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data" presents a machine learning-based traffic flow prediction model to address global traffic congestion issues. The research utilized 30-second interval traffic data from California Highway 78 over a five-month period from July to November 2022, analyzing a 7.24 km westbound section connecting "Melrose Dr" and "El-Camino Real" in the San Diego area. The study employed Multiple Linear Regression (MLR) and Random Forest (RF) algorithms, analyzing data collection intervals ranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance metrics, the analysis revealed that both MLR and RF models performed optimally with 10-minute data collection intervals. These findings are expected to contribute to future traffic congestion solutions and efficient traffic management.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Roots to Rewards: Dynamic Tree Reasoning with RL</title>
<link>https://arxiv.org/abs/2507.13142</link>
<guid>https://arxiv.org/abs/2507.13142</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, chain-of-thought reasoning, retrieval augmentation, tree-structured reasoning, dynamic reinforcement learning

Summary:
Modern language models face challenges in error propagation and knowledge integration despite advancements in chain-of-thought reasoning and retrieval augmentation. The Probabilistic Tree-of-Thought framework offers a solution by decomposing questions into hierarchical structures and integrating parametric and retrieved knowledge. However, its static implementation limits adaptability and efficiency. A new dynamic reinforcement learning framework is introduced to transform tree-based reasoning into an adaptive process. This approach constructs the reasoning tree incrementally based on real-time confidence estimates and learns optimal policies for action selection. By selectively expanding and focusing resource allocation, the framework enhances solution quality and computational efficiency while maintaining probabilistic rigor. This innovative approach establishes a balance between the reliability of probabilistic frameworks and the flexibility required for real-world question answering systems. 

<br /><br />Summary: <div>
arXiv:2507.13142v1 Announce Type: new 
Abstract: Modern language models address complex questions through chain-of-thought (CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al., 2021), yet struggle with error propagation and knowledge integration. Tree-structured reasoning methods, particularly the Probabilistic Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues by decomposing questions into hierarchical structures and selecting answers through confidence-weighted aggregation of parametric and retrieved knowledge (Yao et al., 2023). However, ProbTree's static implementation introduces two key limitations: (1) the reasoning tree is fixed during the initial construction phase, preventing dynamic adaptation to intermediate results, and (2) each node requires exhaustive evaluation of all possible solution strategies, creating computational inefficiency. We present a dynamic reinforcement learning (Sutton and Barto, 2018) framework that transforms tree-based reasoning into an adaptive process. Our approach incrementally constructs the reasoning tree based on real-time confidence estimates, while learning optimal policies for action selection (decomposition, retrieval, or aggregation). This maintains ProbTree's probabilistic rigor while improving both solution quality and computational efficiency through selective expansion and focused resource allocation. The work establishes a new paradigm for treestructured reasoning that balances the reliability of probabilistic frameworks with the flexibility required for real-world question answering systems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era</title>
<link>https://arxiv.org/abs/2507.13175</link>
<guid>https://arxiv.org/abs/2507.13175</guid>
<content:encoded><![CDATA[
<div> criteria, evaluation, language models, artificial moral agents, societal integration  
Summary: 
The paper discusses the need to reevaluate traditional ethical criteria in light of opaque large language models (LLMs) and proposes ten functional criteria for assessing LLM-based artificial moral agents. These criteria include moral concordance, context sensitivity, normative integrity, metaethical awareness, system resilience, trustworthiness, corrigibility, partial transparency, functional autonomy, and moral imagination. By applying these criteria to scenarios involving simulated moral agency through large language systems (SMA-LLS), the aim is to guide artificial moral agents towards alignment and societal integration. The practical applicability of these criteria is demonstrated through hypothetical scenarios involving an autonomous public bus (APB). <div>
arXiv:2507.13175v1 Announce Type: new 
Abstract: The advancement of powerful yet opaque large language models (LLMs) necessitates a fundamental revision of the philosophical criteria used to evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the assumption of transparent architectures, which LLMs defy due to their stochastic outputs and opaque internal states. This paper argues that traditional ethical criteria are pragmatically obsolete for LLMs due to this mismatch. Engaging with core themes in the philosophy of technology, this paper proffers a revised set of ten functional criteria to evaluate LLM-based artificial moral agents: moral concordance, context sensitivity, normative integrity, metaethical awareness, system resilience, trustworthiness, corrigibility, partial transparency, functional autonomy, and moral imagination. These guideposts, applied to what we term "SMA-LLS" (Simulating Moral Agency through Large Language Systems), aim to steer AMAs toward greater alignment and beneficial societal integration in the coming years. We illustrate these criteria using hypothetical scenarios involving an autonomous public bus (APB) to demonstrate their practical applicability in morally salient contexts.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Pattern Unification Modulo Similarity Relations</title>
<link>https://arxiv.org/abs/2507.13208</link>
<guid>https://arxiv.org/abs/2507.13208</guid>
<content:encoded><![CDATA[
<div> fuzzy logic, higher-order theories, decision-making, unification algorithm, similarity relations
Summary:
The paper explores the combination of higher-order theories and fuzzy logic for decision-making tasks that involve abstract functions and predicates. It introduces a unification algorithm that integrates higher-order patterns with fuzzy equivalences expressed through minimum T-norm-based similarity relations. The proposed algorithm ensures termination, soundness, and completeness, with a unitary nature similar to crisp unification. It aims to compute the most general unifier with the highest approximation level when terms are unifiable. This approach provides an efficient way to reason and compute in a combined formalism of higher-order patterns and fuzzy equivalences. The integration of these two components offers a practical solution for decision-making scenarios where exact matches are rare or unnecessary.<br /><br />Summary: <div>
arXiv:2507.13208v1 Announce Type: new 
Abstract: The combination of higher-order theories and fuzzy logic can be useful in decision-making tasks that involve reasoning across abstract functions and predicates, where exact matches are often rare or unnecessary. Developing efficient reasoning and computational techniques for such a combined formalism presents a significant challenge. In this paper, we adopt a more straightforward approach aiming at integrating two well-established and computationally well-behaved components: higher-order patterns on one side and fuzzy equivalences expressed through similarity relations based on minimum T-norm on the other. We propose a unification algorithm for higher-order patterns modulo these similarity relations and prove its termination, soundness, and completeness. This unification problem, like its crisp counterpart, is unitary. The algorithm computes a most general unifier with the highest degree of approximation when the given terms are unifiable.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations</title>
<link>https://arxiv.org/abs/2507.13302</link>
<guid>https://arxiv.org/abs/2507.13302</guid>
<content:encoded><![CDATA[
<div> energy consumption, language models, evaluation, Generative Energy Arena, scalability

Summary: 
- Evaluation of large language models is usually done through automated benchmarks, but it lacks correlation with human evaluation.
- Traditional human evaluation is impractical due to the growing number of models to evaluate.
- The Generative Energy Arena (GEA) includes energy consumption information in model evaluation.
- Preliminary results show that when users are aware of energy consumption, they prefer smaller and more energy-efficient models.
- Complex and top-performing models do not necessarily provide a perceived quality increase that justifies their higher energy consumption. 

<br /><br />Summary: <div>
arXiv:2507.13302v1 Announce Type: new 
Abstract: The evaluation of large language models is a complex task, in which several approaches have been proposed. The most common is the use of automated benchmarks in which LLMs have to answer multiple-choice questions of different topics. However, this method has certain limitations, being the most concerning, the poor correlation with the humans. An alternative approach, is to have humans evaluate the LLMs. This poses scalability issues as there is a large and growing number of models to evaluate making it impractical (and costly) to run traditional studies based on recruiting a number of evaluators and having them rank the responses of the models. An alternative approach is the use of public arenas, such as the popular LM arena, on which any user can freely evaluate models on any question and rank the responses of two models. The results are then elaborated into a model ranking. An increasingly important aspect of LLMs is their energy consumption and, therefore, evaluating how energy awareness influences the decisions of humans in selecting a model is of interest. In this paper, we present GEA, the Generative Energy Arena, an arena that incorporates information on the energy consumption of the model in the evaluation process. Preliminary results obtained with GEA are also presented, showing that for most questions, when users are aware of the energy consumption, they favor smaller and more energy efficient models. This suggests that for most user interactions, the extra cost and energy incurred by the more complex and top-performing models do not provide an increase in the perceived quality of the responses that justifies their use.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming</title>
<link>https://arxiv.org/abs/2507.13337</link>
<guid>https://arxiv.org/abs/2507.13337</guid>
<content:encoded><![CDATA[
<div> benchmark, graph theory, logic, algorithms, Monadic Second-Order logic, theoretical computer science 

Summary: 
The article introduces FormulaOne, a benchmark that challenges frontier AI models with complex research problems in graph theory, logic, and algorithms. These problems are commercially relevant and relate to real-world optimization issues. Generated using Monadic Second-Order logic on graphs, the dataset allows for automatic problem generation at scale. Many problems are linked to theoretical computer science frontiers, like the Strong Exponential Time Hypothesis (SETH). Despite their breadth of knowledge, advanced models like OpenAI's o3 struggle on FormulaOne, solving less than 1% of questions. A simpler set of tasks, FormulaOne-Warmup, is provided to support further research. The dataset and evaluation framework are released for future exploration and algorithmic advancements. <div>
arXiv:2507.13337v1 Announce Type: new 
Abstract: Frontier AI models demonstrate formidable breadth of knowledge. But how close are they to true human -- or superhuman -- expertise? Genuine experts can tackle the hardest problems and push the boundaries of scientific understanding. To illuminate the limits of frontier model capabilities, we turn away from contrived competitive programming puzzles, and instead focus on real-life research problems.
  We construct FormulaOne, a benchmark that lies at the intersection of graph theory, logic, and algorithms, all well within the training distribution of frontier models. Our problems are incredibly demanding, requiring an array of reasoning steps. The dataset has three key properties. First, it is of commercial interest and relates to practical large-scale optimisation problems, such as those arising in routing, scheduling, and network design. Second, it is generated from the highly expressive framework of Monadic Second-Order (MSO) logic on graphs, paving the way toward automatic problem generation at scale; ideal for building RL environments. Third, many of our problems are intimately related to the frontier of theoretical computer science, and to central conjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As such, any significant algorithmic progress on our dataset, beyond known results, could carry profound theoretical implications.
  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on FormulaOne, solving less than 1% of the questions, even when given 10 attempts and explanatory fewshot examples -- highlighting how far they remain from expert-level understanding in some domains. To support further research, we additionally curate FormulaOne-Warmup, offering a set of simpler tasks, from the same distribution. We release the full corpus along with a comprehensive evaluation framework.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementation and Analysis of GPU Algorithms for Vecchia Approximation</title>
<link>https://arxiv.org/abs/2407.02740</link>
<guid>https://arxiv.org/abs/2407.02740</guid>
<content:encoded><![CDATA[
<div> Gaussian Processes, Vecchia Approximation, GPU, GpGpU, spatial statistics
Summary:
- The article discusses the use of Gaussian Processes in spatial statistics and the challenges posed by their computational complexity for large datasets. 
- It introduces Vecchia Approximation as a method to reduce this complexity and explores its implementation on GPUs. 
- Three different approaches to implementing Vecchia Approximation on a GPU are compared, with a new method showing superior performance. 
- The optimized method is integrated into the GpGpu R package. 
- The GpGpU package is then compared to existing multi-core and GPU-accelerated software, demonstrating faster runtimes and improved predictive accuracy. 

<br /><br />Summary: <div>
arXiv:2407.02740v1 Announce Type: cross 
Abstract: Gaussian Processes have become an indispensable part of the spatial statistician's toolbox but are unsuitable for analyzing large dataset because of the significant time and memory needed to fit the associated model exactly. Vecchia Approximation is widely used to reduce the computational complexity and can be calculated with embarrassingly parallel algorithms. While multi-core software has been developed for Vecchia Approximation, such as the GpGp R package, software designed to run on graphics processing units (GPU) is lacking, despite the tremendous success GPUs have had in statistics and machine learning. We compare three different ways to implement Vecchia Approximation on a GPU: two of which are similar to methods used for other Gaussian Process approximations and one that is new. The impact of memory type on performance is investigated and the final method is optimized accordingly. We show that our new method outperforms the other two and then present it in the GpGpU R package. We compare GpGpU to existing multi-core and GPU-accelerated software by fitting Gaussian Process models on various datasets, including a large spatial-temporal dataset of $n>10^6$ points collected from an earth-observing satellite. Our results show that GpGpU achieves faster runtimes and better predictive accuracy.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coarse Addition and the St. Petersburg Paradox: A Heuristic Perspective</title>
<link>https://arxiv.org/abs/2507.12475</link>
<guid>https://arxiv.org/abs/2507.12475</guid>
<content:encoded><![CDATA[
<div> St. Petersburg Paradox, decision theory, modified addition, perceptual categories, inertial stabilization

Summary:
The paper addresses the St. Petersburg paradox in decision theory, which poses a challenge due to its game's infinite expected value and lack of a rational finite stake. Traditional solutions involve auxiliary assumptions like diminishing marginal utility or extended number systems. The paper proposes a novel approach based on a modified addition operation over coarse partitions of the outcome space. This method groups numerical values into perceptual categories and replaces them with representative elements before adding them. This allows for inertial stabilization, where repeated additions eventually stop affecting the outcome. While not a definitive solution to the paradox, this framework offers insight into how agents with limited cognitive precision may handle diverse reward structures. The paper demonstrates that the St. Petersburg series can exhibit inert behavior under this coarse addition method and suggests potential applications in behavioral modeling and machine reasoning under perceptual limitations. 

<br /><br />Summary: <div>
arXiv:2507.12475v1 Announce Type: cross 
Abstract: The St. Petersburg paradox presents a longstanding challenge in decision theory. It describes a game whose expected value is infinite, yet for which no rational finite stake can be determined. Traditional solutions introduce auxiliary assumptions, such as diminishing marginal utility, temporal discounting, or extended number systems. These methods often involve mathematical refinements that may not correspond to how people actually perceive or process numerical information. This paper explores an alternative approach based on a modified operation of addition defined over coarse partitions of the outcome space. In this model, exact numerical values are grouped into perceptual categories, and each value is replaced by a representative element of its group before being added. This method allows for a phenomenon where repeated additions eventually cease to affect the outcome, a behavior described as inertial stabilization. Although this is not intended as a definitive resolution of the paradox, the proposed framework offers a plausible way to represent how agents with limited cognitive precision might handle divergent reward structures. We demonstrate that the St. Petersburg series can become inert under this coarse addition for a suitably constructed partition. The approach may also have broader applications in behavioral modeling and the study of machine reasoning under perceptual limitations.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Powered Quantum Code Transpilation</title>
<link>https://arxiv.org/abs/2507.12480</link>
<guid>https://arxiv.org/abs/2507.12480</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum SDKs, Large Language Models, transpilers, quantum computing, interoperability

Summary:
Large Language Models (LLMs) are being explored as a solution for automating the conversion of quantum programs between different Quantum SDKs. This eliminates the need for time-consuming manual design and maintenance of traditional transpilers. LLMs leverage their pretrained knowledge and contextual reasoning capabilities to ensure functional equivalence in the converted code. The goal is to improve interoperability and cross-platform development of hybrid quantum-classical software systems. By removing the rigid mappings required in rule-based transpilers, LLMs offer a more flexible and scalable solution for quantum software portability. This approach represents a step towards enabling intelligent, general-purpose transpilation in the quantum computing ecosystem. <br /><br />Summary: <div>
arXiv:2507.12480v1 Announce Type: cross 
Abstract: There exist various Software Development Kits (SDKs) tailored to different quantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples include but are not limited to Qiskit, Cirq, and PennyLane. However, this diversity presents significant challenges for interoperability and cross-platform development of hybrid quantum-classical software systems. Traditional rule-based transpilers for translating code between QSDKs are time-consuming to design and maintain, requiring deep expertise and rigid mappings in the source and destination code. In this study, we explore the use of Large Language Models (LLMs) as a flexible and automated solution. Leveraging their pretrained knowledge and contextual reasoning capabilities, we position LLMs as programming language-agnostic transpilers capable of converting quantum programs from one QSDK to another while preserving functional equivalence. Our approach eliminates the need for manually defined transformation rules and offers a scalable solution to quantum software portability. This work represents a step toward enabling intelligent, general-purpose transpilation in the quantum computing ecosystem.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding</title>
<link>https://arxiv.org/abs/2507.12482</link>
<guid>https://arxiv.org/abs/2507.12482</guid>
<content:encoded><![CDATA[
<div> architecture, code understanding, debugging, maintenance, autonomous, software

Summary:
Kodezi Chronos introduces a next-generation architecture for autonomous code understanding, debugging, and maintenance. It operates across ultra-long contexts without fixed window limits, leveraging a multi-level embedding memory engine for efficient reasoning over millions of lines of code. The model outperforms prior Large Language Models (LLMs) and code models, showing a 23% improvement in real-world bug detection and reducing debugging cycles by up to 40%. The evaluation introduces a novel Multi Random Retrieval benchmark, tailored to the software engineering domain. Chronos enables seamless software maintenance by natively interfacing with IDEs and CI/CD workflows, elevating code reliability and productivity while reducing manual effort. This advancement marks a critical step towards self-sustaining, continuously optimized software ecosystems.<br /><br />Summary: <div>
arXiv:2507.12482v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have advanced code generation and software automation, but are fundamentally constrained by limited inference-time context and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a next-generation architecture for autonomous code understanding, debugging, and maintenance, designed to operate across ultra-long contexts comprising entire codebases, histories, and documentation, all without fixed window limits. Kodezi Chronos leverages a multi-level embedding memory engine, combining vector and graph-based indexing with continuous code-aware retrieval. This enables efficient and accurate reasoning over millions of lines of code, supporting repository-scale comprehension, multi-file refactoring, and real-time self-healing actions. Our evaluation introduces a novel Multi Random Retrieval benchmark, specifically tailored to the software engineering domain. Unlike classical retrieval benchmarks, this method requires the model to resolve arbitrarily distant and obfuscated associations across code artifacts, simulating realistic tasks such as variable tracing, dependency migration, and semantic bug localization. Chronos outperforms prior LLMs and code models, demonstrating a 23% improvement in real-world bug detection and reducing debugging cycles by up to 40% compared to traditional sequence-based approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos enables seamless, autonomous software maintenance, elevating code reliability and productivity while reducing manual effort. These results mark a critical advance toward self-sustaining, continuously optimized software ecosystems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Transfer Learning to Boost Dementia Detection</title>
<link>https://arxiv.org/abs/2507.12485</link>
<guid>https://arxiv.org/abs/2507.12485</guid>
<content:encoded><![CDATA[
<div> machine learning, deep learning, dementia detection, quantum machine learning, transfer learning

Summary: 
This article explores the use of quantum machine learning (QML) techniques, specifically quantum transfer learning (QTL), to improve the performance of a weak classical deep learning model in detecting dementia. The study focuses on addressing the challenges of high-dimensional biomedical data and large datasets, which often limit the effectiveness of traditional machine learning approaches. By applying QTL to a binary classification task using the OASIS 2 dataset, the researchers demonstrate how quantum techniques can enhance the performance of a classical model for biomedical image classification. The study also examines the impact of noise on the QTL-based approach, evaluating the reliability and robustness of the method. Overall, the findings suggest that quantum machine learning has the potential to significantly advance healthcare technology by improving dementia detection accuracy and efficiency. 

<br /><br />Summary: <div>
arXiv:2507.12485v1 Announce Type: cross 
Abstract: Dementia is a devastating condition with profound implications for individuals, families, and healthcare systems. Early and accurate detection of dementia is critical for timely intervention and improved patient outcomes. While classical machine learning and deep learning approaches have been explored extensively for dementia prediction, these solutions often struggle with high-dimensional biomedical data and large-scale datasets, quickly reaching computational and performance limitations. To address this challenge, quantum machine learning (QML) has emerged as a promising paradigm, offering faster training and advanced pattern recognition capabilities. This work aims to demonstrate the potential of quantum transfer learning (QTL) to enhance the performance of a weak classical deep learning model applied to a binary classification task for dementia detection. Besides, we show the effect of noise on the QTL-based approach, investigating the reliability and robustness of this method. Using the OASIS 2 dataset, we show how quantum techniques can transform a suboptimal classical model into a more effective solution for biomedical image classification, highlighting their potential impact on advancing healthcare technology.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On multiagent online problems with predictions</title>
<link>https://arxiv.org/abs/2507.12486</link>
<guid>https://arxiv.org/abs/2507.12486</guid>
<content:encoded><![CDATA[
<div> predictor, multiagent, competitive algorithms, ski-rental problem, collaborative 

Summary: 
This article explores the use of predictors in competitive algorithms in a multiagent setting. The framework introduces a two predictor system, where agents predict their own future behavior as well as that of other players. The focus is on determining the best competitive ratios achievable with different predictor qualities. A multiagent version of the ski-rental problem is introduced and analyzed within this framework. In this problem, agents can pool resources for a group license for an asset, with individual renting as a fallback. The article highlights that in the case of perfect predictions for other agents, following the self predictor is optimal but not robust to mispredictions. An algorithm with improved robustness properties is proposed and benchmarked against existing solutions. This research provides insights into the power and limitations of using predictors in competitive algorithms in a multiagent scenario. <div>
arXiv:2507.12486v1 Announce Type: cross 
Abstract: We study the power of (competitive) algorithms with predictions in a multiagent setting. We introduce a two predictor framework, that assumes that agents use one predictor for their future (self) behavior, and one for the behavior of the other players. The main problem we are concerned with is understanding what are the best competitive ratios that can be achieved by employing such predictors, under various assumptions on predictor quality.
  As an illustration of our framework, we introduce and analyze a multiagent version of the ski-rental problem. In this problem agents can collaborate by pooling resources to get a group license for some asset. If the license price is not met then agents have to rent the asset individually for the day at a unit price. Otherwise the license becomes available forever to everyone at no extra cost.
  In the particular case of perfect other predictions the algorithm that follows the self predictor is optimal but not robust to mispredictions of agent's future behavior; we give an algorithm with better robustness properties and benchmark it.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering</title>
<link>https://arxiv.org/abs/2507.12490</link>
<guid>https://arxiv.org/abs/2507.12490</guid>
<content:encoded><![CDATA[
<div> rationales, vision language model, multimodal embedding, spatial sub-regions, DocVQA

Summary:
EaGERS is a novel pipeline that generates natural language rationales using a vision language model and aligns them with spatial sub-regions through multimodal embedding similarities. By restricting responses to relevant regions in the masked image, EaGERS improves performance metrics such as exact match accuracy and Average Normalized Levenshtein Similarity on the DocVQA dataset. The pipeline is fully training-free and model-agnostic, offering enhanced transparency and reproducibility in DocVQA tasks without requiring additional fine-tuning of the base model. <div>
arXiv:2507.12490v1 Announce Type: cross 
Abstract: We introduce EaGERS, a fully training-free and model-agnostic pipeline that (1) generates natural language rationales via a vision language model, (2) grounds these rationales to spatial sub-regions by computing multimodal embedding similarities over a configurable grid with majority voting, and (3) restricts the generation of responses only from the relevant regions selected in the masked image. Experiments on the DocVQA dataset demonstrate that our best configuration not only outperforms the base model on exact match accuracy and Average Normalized Levenshtein Similarity metrics but also enhances transparency and reproducibility in DocVQA without additional model fine-tuning.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sporadic Federated Learning Approach in Quantum Environment to Tackle Quantum Noise</title>
<link>https://arxiv.org/abs/2507.12492</link>
<guid>https://arxiv.org/abs/2507.12492</guid>
<content:encoded><![CDATA[
<div> Quantum Federated Learning, Quantum noise, SpoQFL, Decentralized model training, Data privacy
<br />
Summary:
SpoQFL is a novel Quantum Federated Learning framework designed to address quantum noise heterogeneity in distributed systems. It dynamically adjusts training strategies based on noise fluctuations, enhancing model robustness and efficiency. SpoQFL leverages sporadic learning to mitigate quantum noise, improving convergence stability in quantum networks. Experimental results on real-world datasets show that SpoQFL outperforms conventional QFL approaches, achieving superior training performance and more stable convergence. <div>
arXiv:2507.12492v1 Announce Type: cross 
Abstract: Quantum Federated Learning (QFL) is an emerging paradigm that combines quantum computing and federated learning (FL) to enable decentralized model training while maintaining data privacy over quantum networks. However, quantum noise remains a significant barrier in QFL, since modern quantum devices experience heterogeneous noise levels due to variances in hardware quality and sensitivity to quantum decoherence, resulting in inadequate training performance. To address this issue, we propose SpoQFL, a novel QFL framework that leverages sporadic learning to mitigate quantum noise heterogeneity in distributed quantum systems. SpoQFL dynamically adjusts training strategies based on noise fluctuations, enhancing model robustness, convergence stability, and overall learning efficiency. Extensive experiments on real-world datasets demonstrate that SpoQFL significantly outperforms conventional QFL approaches, achieving superior training performance and more stable convergence.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making</title>
<link>https://arxiv.org/abs/2507.12496</link>
<guid>https://arxiv.org/abs/2507.12496</guid>
<content:encoded><![CDATA[
<div> Framework, Foundation Models, World Models, Task Generalization, Embodied Environments

Summary: 
FOUNDER is a framework that combines Foundation Models (FMs) and World Models (WMs) to solve tasks in embodied environments without the need for rewards. It integrates the generalizable knowledge of FMs with the dynamic modeling capabilities of WMs by mapping FM representations to WM state space. This allows for learning a goal-conditioned policy through imagination and predicting temporal distance to the goal state as a reward signal. FOUNDER outperforms previous methods on multi-task visual control benchmarks, especially in complex scenarios with text or video-specified tasks. The framework successfully captures deep-level semantics of tasks and demonstrates consistency with ground-truth rewards. The approach is validated through empirical testing, showcasing its effectiveness in open-ended task solving. Visit our project website for more information: https://sites.google.com/view/founder-rl. 

<br /><br />Summary: <div>
arXiv:2507.12496v1 Announce Type: cross 
Abstract: Foundation Models (FMs) and World Models (WMs) offer complementary strengths in task generalization at different levels. In this work, we propose FOUNDER, a framework that integrates the generalizable knowledge embedded in FMs with the dynamic modeling capabilities of WMs to enable open-ended task solving in embodied environments in a reward-free manner. We learn a mapping function that grounds FM representations in the WM state space, effectively inferring the agent's physical states in the world simulator from external observations. This mapping enables the learning of a goal-conditioned policy through imagination during behavior learning, with the mapped task serving as the goal state. Our method leverages the predicted temporal distance to the goal state as an informative reward signal. FOUNDER demonstrates superior performance on various multi-task offline visual control benchmarks, excelling in capturing the deep-level semantics of tasks specified by text or videos, particularly in scenarios involving complex observations or domain gaps where prior methods struggle. The consistency of our learned reward function with the ground-truth reward is also empirically validated. Our project website is https://sites.google.com/view/founder-rl.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Football Data into Object-centric Event Logs with Spatial Context Information</title>
<link>https://arxiv.org/abs/2507.12504</link>
<guid>https://arxiv.org/abs/2507.12504</guid>
<content:encoded><![CDATA[
<div> framework, object-centric event logs, football data, spatial dimension, process representations

Summary:
The paper introduces a framework for converting football data into object-centric event logs with a spatial dimension. It aims to analyze complex process behavior by incorporating multiple objects. The study showcases the effectiveness of the framework by generating object-centric event logs from real-world football data. It discusses the results obtained from different process representations. This work represents a novel approach in football analytics, showcasing the potential of object-centric event logs in sports data analysis. Future research avenues include exploring variant analysis and filtering techniques to address data variability. <div>
arXiv:2507.12504v1 Announce Type: cross 
Abstract: Object-centric event logs expand the conventional single-case notion event log by considering multiple objects, allowing for the analysis of more complex and realistic process behavior. However, the number of real-world object-centric event logs remains limited, and further studies are needed to test their usefulness. The increasing availability of data from team sports can facilitate object-centric process mining, leveraging both real-world data and suitable use cases. In this paper, we present a framework for transforming football (soccer) data into an object-centric event log, further enhanced with a spatial dimension. We demonstrate the effectiveness of our framework by generating object-centric event logs based on real-world football data and discuss the results for varying process representations. With our paper, we provide the first example for object-centric event logs in football analytics. Future work should consider variant analysis and filtering techniques to better handle variability
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training</title>
<link>https://arxiv.org/abs/2507.12507</link>
<guid>https://arxiv.org/abs/2507.12507</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning, language model, reinforcement learning, training stability, generalization

Summary: 
The report discusses the impact of extended reinforcement learning on a small language model across various reasoning domains. It highlights the importance of verifiable reward tasks, enhancements to Group Relative Policy Optimization (GRPO), and practical techniques for training stability and generalization. Key components for improved performance include controlled KL regularization, clipping ratio, and periodic reference policy resets. The model showcases significant advancements over baseline results, with notable performance boosts in mathematics, coding, and logic puzzle tasks. By releasing their model to the public, the researchers aim to support ongoing research in the field. <div>
arXiv:2507.12507v1 Announce Type: cross 
Abstract: Recent advancements in reasoning-focused language models such as OpenAI's O1 and DeepSeek-R1 have shown that scaling test-time computation-through chain-of-thought reasoning and iterative exploration-can yield substantial improvements on complex tasks like mathematics and code generation. These breakthroughs have been driven by large-scale reinforcement learning (RL), particularly when combined with verifiable reward signals that provide objective and grounded supervision. In this report, we investigate the effects of prolonged reinforcement learning on a small language model across a diverse set of reasoning domains. Our work identifies several key ingredients for effective training, including the use of verifiable reward tasks, enhancements to Group Relative Policy Optimization (GRPO), and practical techniques to improve training stability and generalization. We introduce controlled KL regularization, clipping ratio, and periodic reference policy resets as critical components for unlocking long-term performance gains. Our model achieves significant improvements over strong baselines, including +14.7% on math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate continued research, we release our model publicly.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindJourney: Test-Time Scaling with World Models for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2507.12508</link>
<guid>https://arxiv.org/abs/2507.12508</guid>
<content:encoded><![CDATA[
<div> framework, VLMs, world models, spatial reasoning, test-time scaling  
Summary:  
- Spatial reasoning in 3D space is crucial for human cognition and tasks like navigation and manipulation.
- Current vision-language models (VLMs) struggle with 3D tasks due to their perception of 2D images.
- The MindJourney framework couples a VLM with a world model based on video diffusion for 3D reasoning.
- MindJourney improves performance on the SAT spatial reasoning benchmark by over 8% without fine-tuning.
- This approach of pairing VLMs with world models for test-time scaling offers a simple and effective way to enhance 3D reasoning tasks.  

<br /><br />Summary: <div>
arXiv:2507.12508v1 Announce Type: cross 
Abstract: Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models</title>
<link>https://arxiv.org/abs/2507.12547</link>
<guid>https://arxiv.org/abs/2507.12547</guid>
<content:encoded><![CDATA[
<div> Keywords: novel situations, mental models, Model Synthesis Architecture, human reasoning, open-ended reasoning

Summary:
The study explores how people utilize distributed and symbolic representations to construct customized mental models when facing novel situations. The proposed Model Synthesis Architecture (MSA) combines language models for global relevance-based retrieval and probabilistic programs for coherent world models. Evaluation on a novel reasoning dataset centered on a 'Model Olympics' domain shows that the MSA outperforms language model-only baselines in capturing human-like judgments. The dataset requires assessing causal structures, leveraging background knowledge, and incorporating observations with arbitrary variables. The MSA approach demonstrates the ability to deliver locally coherent reasoning over globally relevant variables, reflecting human reasoning in open-ended domains. These findings suggest that MSAs hold promise for replicating human reasoning processes in complex, unfamiliar scenarios. 

<br /><br />Summary: <div>
arXiv:2507.12547v1 Announce Type: cross 
Abstract: When faced with novel situations, people are able to marshal relevant considerations from a wide range of background knowledge and put these to use in inferences and predictions. What permits us to draw in globally relevant information and reason over it coherently? Here, we explore the hypothesis that people use a combination of distributed and symbolic representations to construct bespoke mental models tailored to novel situations. We propose a computational implementation of this idea -- a ``Model Synthesis Architecture'' (MSA) -- using language models to implement global relevance-based retrieval and model synthesis and probabilistic programs to implement bespoke, coherent world models. We evaluate our MSA as a model of human judgments on a novel reasoning dataset. The dataset -- built around a `Model Olympics` domain of sports vignettes -- tests models' capacity for human-like, open-ended reasoning by requiring (i) judgments about novel causal structures described in language; (ii) drawing on large bodies of background knowledge; and (iii) doing both in light of observations that introduce arbitrary novel variables. Our MSA approach captures human judgments better than language model-only baselines, under both direct and chain-of-thought generations from the LM that supports model synthesis. These results suggest that MSAs can be implemented in a way that mirrors people's ability to deliver locally coherent reasoning over globally relevant variables, offering a path to understanding and replicating human reasoning in open-ended domains.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility</title>
<link>https://arxiv.org/abs/2507.12553</link>
<guid>https://arxiv.org/abs/2507.12553</guid>
<content:encoded><![CDATA[
<div> categorization, language models, modal difference vectors, interpretability, human behavior <br />
Summary:
Language models (LMs) are essential for various tasks but their ability to categorize sentences based on modality has been questioned. This study identifies modal difference vectors in LMs that can discriminate between modal categories, showing more reliable categorization judgments than previously thought. The modal difference vectors emerge consistently as models become more competent, offering insights into LM modal categorization and potential applications in understanding human categorization behavior. By correlating projections along modal difference vectors with human ratings of interpretable features, the study suggests a novel approach to modeling human modal categorization behavior. This research provides new insights into LM modal categorization and its implications for understanding human modal categorization behavior. <br /><br /> <div>
arXiv:2507.12553v1 Announce Type: cross 
Abstract: Language models (LMs) are used for a diverse range of tasks, from question answering to writing fantastical stories. In order to reliably accomplish these tasks, LMs must be able to discern the modal category of a sentence (i.e., whether it describes something that is possible, impossible, completely nonsensical, etc.). However, recent studies have called into question the ability of LMs to categorize sentences according to modality (Michaelov et al., 2025; Kauf et al., 2023). In this work, we identify linear representations that discriminate between modal categories within a variety of LMs, or modal difference vectors. Analysis of modal difference vectors reveals that LMs have access to more reliable modal categorization judgments than previously reported. Furthermore, we find that modal difference vectors emerge in a consistent order as models become more competent (i.e., through training steps, layers, and parameter count). Notably, we find that modal difference vectors identified within LM activations can be used to model fine-grained human categorization behavior. This potentially provides a novel view into how human participants distinguish between modal categories, which we explore by correlating projections along modal difference vectors with human participants' ratings of interpretable features. In summary, we derive new insights into LM modal categorization using techniques from mechanistic interpretability, with the potential to inform our understanding of modal categorization in humans.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Mental Imagery Improve the Thinking Capabilities of AI Systems?</title>
<link>https://arxiv.org/abs/2507.12555</link>
<guid>https://arxiv.org/abs/2507.12555</guid>
<content:encoded><![CDATA[
<div> Keywords: machine thinking, mental imagery, cognitive unit, input data unit, validation tests

Summary:
Machine thinking models currently lack the ability to act autonomously and engage in independent reasoning, relying on explicit queries for input. While AI agents have made progress in performing tasks independently, they struggle to integrate knowledge across various domains like humans do. This paper explores the integration of mental imagery into a machine thinking framework to enhance the initiation of the thinking process. The proposed framework consists of a Cognitive thinking unit supported by the Input Data Unit, Needs Unit, and Mental Imagery Unit. Data is represented as natural language sentences or sketches, serving informative and decision-making purposes. Validation tests were conducted to evaluate the framework's effectiveness, showcasing the potential benefits of incorporating mental imagery into machine thinking processes. <br /><br />Summary:Machine thinking models lack autonomy and independent reasoning, while AI agents struggle to integrate knowledge across domains. This paper proposes integrating mental imagery into a machine thinking framework, with validation tests demonstrating its effectiveness in data representation and decision-making. <div>
arXiv:2507.12555v1 Announce Type: cross 
Abstract: Although existing models can interact with humans and provide satisfactory responses, they lack the ability to act autonomously or engage in independent reasoning. Furthermore, input data in these models is typically provided as explicit queries, even when some sensory data is already acquired.
  In addition, AI agents, which are computational entities designed to perform tasks and make decisions autonomously based on their programming, data inputs, and learned knowledge, have shown significant progress. However, they struggle with integrating knowledge across multiple domains, unlike humans.
  Mental imagery plays a fundamental role in the brain's thinking process, which involves performing tasks based on internal multisensory data, planned actions, needs, and reasoning capabilities. In this paper, we investigate how to integrate mental imagery into a machine thinking framework and how this could be beneficial in initiating the thinking process. Our proposed machine thinking framework integrates a Cognitive thinking unit supported by three auxiliary units: the Input Data Unit, the Needs Unit, and the Mental Imagery Unit. Within this framework, data is represented as natural language sentences or drawn sketches, serving both informative and decision-making purposes. We conducted validation tests for this framework, and the results are presented and discussed.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safeguarding Federated Learning-based Road Condition Classification</title>
<link>https://arxiv.org/abs/2507.12568</link>
<guid>https://arxiv.org/abs/2507.12568</guid>
<content:encoded><![CDATA[
<div> vulnerabilities, targeted label flipping attacks, federated learning, road condition classification, defense mechanism
Summary:
- The article discusses the vulnerability of federated learning-based Road Condition Classification (RCC) systems to Targeted Label Flipping Attacks (TLFAs).
- TLFAs involve malicious clients altering training data labels to compromise model inference performance and pose safety risks for autonomous driving.
- A novel label-distance-based metric is introduced to quantify the impact of TLFAs on FL-RCC systems accurately.
- The proposed defense mechanism, FLARE, utilizes neuron-wise analysis of the output layer to mitigate the effects of TLFAs.
- Extensive experiments across multiple RCC tasks, evaluation metrics, baselines, and deep learning models demonstrate both the severity of TLFAs on FL-RCC systems and the effectiveness of FLARE in mitigating the attack impact. 

<br /><br />Summary: <div>
arXiv:2507.12568v1 Announce Type: cross 
Abstract: Federated Learning (FL) has emerged as a promising solution for privacy-preserving autonomous driving, specifically camera-based Road Condition Classification (RCC) systems, harnessing distributed sensing, computing, and communication resources on board vehicles without sharing sensitive image data. However, the collaborative nature of FL-RCC frameworks introduces new vulnerabilities: Targeted Label Flipping Attacks (TLFAs), in which malicious clients (vehicles) deliberately alter their training data labels to compromise the learned model inference performance. Such attacks can, e.g., cause a vehicle to mis-classify slippery, dangerous road conditions as pristine and exceed recommended speed. However, TLFAs for FL-based RCC systems are largely missing. We address this challenge with a threefold contribution: 1) we disclose the vulnerability of existing FL-RCC systems to TLFAs; 2) we introduce a novel label-distance-based metric to precisely quantify the safety risks posed by TLFAs; and 3) we propose FLARE, a defensive mechanism leveraging neuron-wise analysis of the output layer to mitigate TLFA effects. Extensive experiments across three RCC tasks, four evaluation metrics, six baselines, and three deep learning models demonstrate both the severity of TLFAs on FL-RCC systems and the effectiveness of FLARE in mitigating the attack impact.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assay2Mol: large language model-based drug design using BioAssay context</title>
<link>https://arxiv.org/abs/2507.12574</link>
<guid>https://arxiv.org/abs/2507.12574</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific databases, biochemistry, molecule screening assays, drug discovery, Assay2Mol

Summary: 
Assay2Mol is a language model-based workflow designed to leverage unstructured text from biochemical screening assays for drug discovery. By utilizing data from existing assay records related to similar targets, Assay2Mol can generate candidate molecules through in-context learning. The tool outperforms other machine learning methods in generating ligand molecules for target proteins and emphasizes the production of synthesizable molecules. Assay2Mol provides a valuable resource for early-stage drug discovery campaigns, tapping into the wealth of information available in scientific databases. <div>
arXiv:2507.12574v1 Announce Type: cross 
Abstract: Scientific databases aggregate vast amounts of quantitative data alongside descriptive text. In biochemistry, molecule screening assays evaluate the functional responses of candidate molecules against disease targets. Unstructured text that describes the biological mechanisms through which these targets operate, experimental screening protocols, and other attributes of assays offer rich information for new drug discovery campaigns but has been untapped because of that unstructured format. We present Assay2Mol, a large language model-based workflow that can capitalize on the vast existing biochemical screening assays for early-stage drug discovery. Assay2Mol retrieves existing assay records involving targets similar to the new target and generates candidate molecules using in-context learning with the retrieved assay screening data. Assay2Mol outperforms recent machine learning approaches that generate candidate ligand molecules for target protein structures, while also promoting more synthesizable molecule generation.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification</title>
<link>https://arxiv.org/abs/2507.12602</link>
<guid>https://arxiv.org/abs/2507.12602</guid>
<content:encoded><![CDATA[
<div> Keywords: Tree species classification, LiDAR point clouds, Dynamic graph convolutional neural networks, Semantic relationships, Hierarchical multiscale fusion<br />
Summary:<br />
- MS-DGCNN++ is a novel hierarchical multiscale fusion dynamic graph convolutional network for tree species classification from LiDAR point clouds.
- The approach utilizes semantically meaningful feature extraction at local, branch, and canopy scales with cross-scale information propagation.
- Scale-specific feature engineering is employed, including standard geometric features, normalized relative vectors, and distance information, aligned with the natural tree structure.
- MS-DGCNN++ outperforms existing models and achieves high accuracy on tree species datasets STPCTLS and FOR-species20K, as well as standard 3D object recognition datasets ModelNet40 and ModelNet10.
- With reduced complexity and lower parameters compared to transformer approaches, MS-DGCNN++ is suitable for resource-constrained applications and demonstrates versatility across various point cloud processing tasks. <div>
arXiv:2507.12602v1 Announce Type: cross 
Abstract: Tree species classification from terrestrial LiDAR point clouds is challenging because of the complex multi-scale geometric structures in forest environments. Existing approaches using multi-scale dynamic graph convolutional neural networks (MS-DGCNN) employ parallel multi-scale processing, which fails to capture the semantic relationships between the hierarchical levels of the tree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion dynamic graph convolutional network that uses semantically meaningful feature extraction at local, branch, and canopy scales with cross-scale information propagation. Our method employs scale-specific feature engineering, including standard geometric features for the local scale, normalized relative vectors for the branch scale, and distance information for the canopy scale. This hierarchical approach replaces uniform parallel processing with semantically differentiated representations that are aligned with the natural tree structure. Under the same proposed tree species data augmentation strategy for all experiments, MS-DGCNN++ achieved an accuracy of 94.96 \% on STPCTLS, outperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On FOR-species20K, it achieves 67.25\% accuracy (6.1\% improvement compared to MS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN and MS-DGCNN with overall accuracies of 93.15\% on ModelNet40 and 94.05\% on ModelNet10. With lower parameters and reduced complexity compared to state-of-the-art transformer approaches, our method is suitable for resource-constrained applications while maintaining a competitive accuracy. Beyond tree classification, the method generalizes to standard 3D object recognition, establishing it as a versatile solution for diverse point cloud processing applications. The implementation code is publicly available at https://github.com/said-ohamouddou/MS-DGCNN2.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning What Matters: Probabilistic Task Selection via Mutual Information for Model Finetuning</title>
<link>https://arxiv.org/abs/2507.12612</link>
<guid>https://arxiv.org/abs/2507.12612</guid>
<content:encoded><![CDATA[
<div> framework, mixture optimization, large language models, task relationships, Markov Random Field  
Summary:  
The article introduces TASKPGM, a framework for optimizing the mixture of task datasets for large language models (LLMs) by minimizing an energy function over a Markov Random Field (MRF). It uses behavioral divergences to model task relationships and select continuous task proportions, providing a closed-form solution under simplex constraints. The method balances representativeness and diversity among tasks, offering theoretical guarantees like weak submodularity for budgeted variants. Empirical results on Llama 2 and Mistral show consistent improvements in evaluation suites, such as MMLU and BIGBench. TASKPGM also provides interpretable insights into task influence and mixture composition, making it a valuable tool for efficient and robust LLM finetuning. <div>
arXiv:2507.12612v1 Announce Type: cross 
Abstract: The performance of finetuned large language models (LLMs) hinges critically on the composition of the training mixture. However, selecting an optimal blend of task datasets remains a largely manual, heuristic driven process, with practitioners often relying on uniform or size based sampling strategies. We introduce TASKPGM, a principled and scalable framework for mixture optimization that selects continuous task proportions by minimizing an energy function over a Markov Random Field (MRF). Task relationships are modeled using behavioral divergences such as Jensen Shannon Divergence and Pointwise Mutual Information computed from the predictive distributions of single task finetuned models. Our method yields a closed form solution under simplex constraints and provably balances representativeness and diversity among tasks. We provide theoretical guarantees, including weak submodularity for budgeted variants, and demonstrate consistent empirical improvements on Llama 2 and Mistral across evaluation suites such as MMLU and BIGBench. Beyond performance, TASKPGM offers interpretable insights into task influence and mixture composition, making it a powerful tool for efficient and robust LLM finetuning.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training</title>
<link>https://arxiv.org/abs/2507.12619</link>
<guid>https://arxiv.org/abs/2507.12619</guid>
<content:encoded><![CDATA[
<div> characterization, LLM training, startup overhead, optimization framework, Bootseer <br />
Summary:
This research focuses on analyzing and addressing the startup overhead in Large Language Models (LLMs) training. The study reveals that startup overhead is a significant issue in industrial-scale LLMs, wasting up to 3.5% of GPU time. By examining the components of startup cost and its impact on training jobs, the researchers introduce Bootseer, a system-level optimization framework. Bootseer tackles three primary startup bottlenecks - container image loading, runtime dependency installation, and model checkpoint resumption - through techniques like hot block record-and-prefetch, dependency snapshotting, and striped HDFS-FUSE. Deployed in a production environment, Bootseer reduces startup overhead by 50%, showcasing its effectiveness in improving training efficiency for LLMs. <br /> <div>
arXiv:2507.12619v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become a cornerstone of modern AI, driving breakthroughs in natural language processing and expanding into multimodal jobs involving images, audio, and video. As with most computational software, it is important to distinguish between ordinary runtime performance and startup overhead. Prior research has focused on runtime performance: improving training efficiency and stability. This work focuses instead on the increasingly critical issue of startup overhead in training: the delay before training jobs begin execution. Startup overhead is particularly important in large, industrial-scale LLMs, where failures occur more frequently and multiple teams operate in iterative update-debug cycles. In one of our training clusters, more than 3.5% of GPU time is wasted due to startup overhead alone.
  In this work, we present the first in-depth characterization of LLM training startup overhead based on real production data. We analyze the components of startup cost, quantify its direct impact, and examine how it scales with job size. These insights motivate the design of Bootseer, a system-level optimization framework that addresses three primary startup bottlenecks: (a) container image loading, (b) runtime dependency installation, and (c) model checkpoint resumption. To mitigate these bottlenecks, Bootseer introduces three techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and (c) striped HDFS-FUSE. Bootseer has been deployed in a production environment and evaluated on real LLM training workloads, demonstrating a 50% reduction in startup overhead.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Robust Channel Estimation Neural Networks by Designed Training Data</title>
<link>https://arxiv.org/abs/2507.12630</link>
<guid>https://arxiv.org/abs/2507.12630</guid>
<content:encoded><![CDATA[
<div> Keywords: channel estimation, neural networks, wireless channels, synthetic training datasets, generalization

Summary:<br /><br />
Channel estimation in cognitive communications is essential for spectrum sensing and adaptive transmission. Neural networks are often tested on similar channels, leading to performance degradation on new data. This paper proposes offline-trained neural networks that can robustly perform over wireless channels without prior channel information. The design criteria generate synthetic training datasets guaranteeing a certain mean squared error on new channels. A benchmark design ensures intelligent operation for different channel profiles. Generalization achieved is independent of neural network architecture, demonstrated through simulations showing robust generalization to both fixed and variable delay spread wireless channels. Offline-trained neural networks alleviate the need for time-consuming online training, making them suitable for real-world implementations without parameter updates. <div>
arXiv:2507.12630v1 Announce Type: cross 
Abstract: Channel estimation is crucial in cognitive communications, as it enables intelligent spectrum sensing and adaptive transmission by providing accurate information about the current channel state. However, in many papers neural networks are frequently tested by training and testing on one example channel or similar channels. This is because data-driven methods often degrade on new data which they are not trained on, as they cannot extrapolate their training knowledge. This is despite the fact physical channels are often assumed to be time-variant. However, due to the low latency requirements and limited computing resources, neural networks may not have enough time and computing resources to execute online training to fine-tune the parameters. This motivates us to design offline-trained neural networks that can perform robustly over wireless channels, but without any actual channel information being known at design time. In this paper, we propose design criteria to generate synthetic training datasets for neural networks, which guarantee that after training the resulting networks achieve a certain mean squared error (MSE) on new and previously unseen channels. Therefore, neural network solutions require no prior channel information or parameters update for real-world implementations. Based on the proposed design criteria, we further propose a benchmark design which ensures intelligent operation for different channel profiles. To demonstrate general applicability, we use neural networks with different levels of complexity to show that the generalization achieved appears to be independent of neural network architecture. From simulations, neural networks achieve robust generalization to wireless channels with both fixed channel profiles and variable delay spreads.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QSpark: Towards Reliable Qiskit Code Generation</title>
<link>https://arxiv.org/abs/2507.12642</link>
<guid>https://arxiv.org/abs/2507.12642</guid>
<content:encoded><![CDATA[
<div> error-resilient, quantum circuits, RL methods, Qiskit code, benchmark 
Summary: <br /><br />Quantum circuits must be error-resilient, and the use of large language models like Granite-20B-Code and StarCoder can result in flawed Qiskit code. In this study, a 32B model was fine-tuned using two reinforcement learning (RL) methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference Optimization (ORPO), with a synthetic dataset. The models were evaluated on the Qiskit HumanEval benchmark, where ORPO achieved a 56.29% Pass rate at 1 attempt, showing improvement over existing models. GRPO scored 49%, outperforming general-purpose baselines as well. While GRPO excelled in basic tasks and ORPO in intermediate ones, both models were unable to solve advanced tasks. This study underscores the progress in AI-assisted quantum programming but also highlights the need for further advancements in this field. <div>
arXiv:2507.12642v1 Announce Type: cross 
Abstract: Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and StarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two RL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference Optimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit HumanEval benchmark, ORPO reaches 56.29\% Pass@1 ($\approx+10$ pp over Granite-8B-QK) and GRPO hits 49\%, both beating all general-purpose baselines; on the original HumanEval they score 65.90\% and 63.00\%. GRPO excels on basic tasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five advanced tasks, highlighting clear gains yet room for progress in AI-assisted quantum programming.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLMgineer: Vision Language Models as Robotic Toolsmiths</title>
<link>https://arxiv.org/abs/2507.12644</link>
<guid>https://arxiv.org/abs/2507.12644</guid>
<content:encoded><![CDATA[
<div> Keywords: tool design, physical intelligence, robotics, vision language models, automated tool invention

Summary:<br /><br />Tool design and use are seen as measurable indicators of intelligence across species, involving creativity, planning, and foresight. While much robotic intelligence research focuses on controllers, inventing smarter tools offers a form of physical intelligence that shifts problem-solving to tool design. The VLMgineer framework combines vision language models (VLMs) with evolutionary search to co-design tools and action plans for tasks. It is evaluated on a benchmark of manipulation scenarios, consistently finding innovative solutions. VLMgineer outperforms human-specified and existing tools. The benchmark and code will be released to facilitate automated tool invention research. <div>
arXiv:2507.12644v1 Announce Type: cross 
Abstract: Tool design and use reflect the ability to understand and manipulate the physical world through creativity, planning, and foresight. As such, these capabilities are often regarded as measurable indicators of intelligence across biological species. While much of today's research on robotic intelligence focuses on generating better controllers, inventing smarter tools offers a complementary form of physical intelligence: shifting the onus of problem-solving onto the tool's design. Given the vast and impressive common-sense, reasoning, and creative capabilities of today's foundation models, we investigate whether these models can provide useful priors to automatically design and effectively wield such tools? We present VLMgineer, a framework that harnesses the code generation abilities of vision language models (VLMs) together with evolutionary search to iteratively co-design physical tools and the action plans that operate them to perform a task. We evaluate VLMgineer on a diverse new benchmark of everyday manipulation scenarios that demand creative tool design and use. Across this suite, VLMgineer consistently discovers tools and policies that solve tasks more effectively and innovatively, transforming challenging robotics problems into straightforward executions. It also outperforms VLM-generated designs from human specifications and existing human-crafted tools for everyday tasks. To facilitate future research on automated tool invention, we will release our benchmark and code.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions</title>
<link>https://arxiv.org/abs/2507.12659</link>
<guid>https://arxiv.org/abs/2507.12659</guid>
<content:encoded><![CDATA[
<div> PINNs, transfer learning, extrapolation, activation functions, robustness<br />
<br />
Summary:<br />
Physics-Informed Neural Networks (PINNs) integrate physical laws into deep learning for solving complex problems. However, they often struggle with extrapolation outside the training data and sensitivity to activation functions (AFs). This study introduces transfer learning (TL) to enhance PINNs' extrapolation capability within an extended training domain. TL is applied using a few selected collocation points, and an adaptive AF combining standard AFs is proposed for improved model accuracy and robustness. Experimental results show a 40% average reduction in relative L2 error and 50% average reduction in mean absolute error in the extrapolation domain without significant computational cost increase. The code for the method is available on GitHub. <div>
arXiv:2507.12659v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks (PINNs) are deep learning models that incorporate the governing physical laws of a system into the learning process, making them well-suited for solving complex scientific and engineering problems. Recently, PINNs have gained widespread attention as a powerful framework for combining physical principles with data-driven modeling to improve prediction accuracy. Despite their successes, however, PINNs often exhibit poor extrapolation performance outside the training domain and are highly sensitive to the choice of activation functions (AFs). In this paper, we introduce a transfer learning (TL) method to improve the extrapolation capability of PINNs. Our approach applies transfer learning (TL) within an extended training domain, using only a small number of carefully selected collocation points. Additionally, we propose an adaptive AF that takes the form of a linear combination of standard AFs, which improves both the robustness and accuracy of the model. Through a series of experiments, we demonstrate that our method achieves an average of 40% reduction in relative L2 error and an average of 50% reduction in mean absolute error in the extrapolation domain, all without a significant increase in computational cost. The code is available at https://github.com/LiuzLab/PINN-extrapolation .
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development</title>
<link>https://arxiv.org/abs/2507.12665</link>
<guid>https://arxiv.org/abs/2507.12665</guid>
<content:encoded><![CDATA[
<div> Keywords: Single Conversation Methodology, software development, large language models, structured dialogue, cognitive clarity

Summary:
The Single Conversation Methodology (SCM) is a novel approach to software development using large language models (LLMs). It emphasizes structured and persistent development dialogues, where all project stages occur within a single conversation. Grounded in principles of cognitive clarity, traceability, modularity, and documentation, SCM defines phases, best practices, and a philosophical stance. It aims to correct passive reliance on LLMs by reasserting the active role of developers as architects and supervisors of intelligent tools. SCM offers a pragmatic solution to ad hoc interactions with generative AI by fostering a more organized and transparent development process. Through SCM, developers can ensure better control, understanding, and oversight of projects, leading to more effective and efficient software development. <div>
arXiv:2507.12665v1 Announce Type: cross 
Abstract: We propose the Single Conversation Methodology (SCM), a novel and pragmatic approach to software development using large language models (LLMs). In contrast to ad hoc interactions with generative AI, SCM emphasizes a structured and persistent development dialogue, where all stages of a project - from requirements to architecture and implementation - unfold within a single, long-context conversation. The methodology is grounded on principles of cognitive clarity, traceability, modularity, and documentation. We define its phases, best practices, and philosophical stance, while arguing that SCM offers a necessary correction to the passive reliance on LLMs prevalent in current practices. We aim to reassert the active role of the developer as architect and supervisor of the intelligent tool.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InSight: AI Mobile Screening Tool for Multiple Eye Disease Detection using Multimodal Fusion</title>
<link>https://arxiv.org/abs/2507.12669</link>
<guid>https://arxiv.org/abs/2507.12669</guid>
<content:encoded><![CDATA[
<div> AI-based app, eye diseases, diagnosis, fundus images, accessibility

Summary:
The article presents InSight, an AI-based app designed to improve access to early screening for common eye diseases like age-related macular degeneration, glaucoma, diabetic retinopathy, diabetic macular edema, and pathological myopia. The app combines patient metadata with fundus images to accurately diagnose these diseases. The three-stage pipeline includes real-time image quality assessment, disease diagnosis model, and a DR grading model. Key innovations in the disease diagnosis model include multimodal fusion technique, pretraining method, and multitask model. Trained on BRSET and mBRSET datasets, InSight demonstrates high accuracy in diagnosing the five diseases and is robust across various image conditions. The multitask model contributes to the lightweight nature of the pipeline, making it computationally efficient. This innovation has the potential to improve access to screenings, especially in low-resource settings. 

<br /><br />Summary: <div>
arXiv:2507.12669v1 Announce Type: cross 
Abstract: Background/Objectives: Age-related macular degeneration, glaucoma, diabetic retinopathy (DR), diabetic macular edema, and pathological myopia affect hundreds of millions of people worldwide. Early screening for these diseases is essential, yet access to medical care remains limited in low- and middle-income countries as well as in resource-limited settings. We develop InSight, an AI-based app that combines patient metadata with fundus images for accurate diagnosis of five common eye diseases to improve accessibility of screenings.
  Methods: InSight features a three-stage pipeline: real-time image quality assessment, disease diagnosis model, and a DR grading model to assess severity. Our disease diagnosis model incorporates three key innovations: (a) Multimodal fusion technique (MetaFusion) combining clinical metadata and images; (b) Pretraining method leveraging supervised and self-supervised loss functions; and (c) Multitask model to simultaneously predict 5 diseases. We make use of BRSET (lab-captured images) and mBRSET (smartphone-captured images) datasets, both of which also contain clinical metadata for model training/evaluation.
  Results: Trained on a dataset of BRSET and mBRSET images, the image quality checker achieves near-100% accuracy in filtering out low-quality fundus images. The multimodal pretrained disease diagnosis model outperforms models using only images by 6% in balanced accuracy for BRSET and 4% for mBRSET.
  Conclusions: The InSight pipeline demonstrates robustness across varied image conditions and has high diagnostic accuracy across all five diseases, generalizing to both smartphone and lab captured images. The multitask model contributes to the lightweight nature of the pipeline, making it five times computationally efficient compared to having five individual models corresponding to each disease.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle</title>
<link>https://arxiv.org/abs/2507.12674</link>
<guid>https://arxiv.org/abs/2507.12674</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, student-like code generation, fine-tuning, learning dynamics, multi-dimensional evaluation

Summary: 
Large Language Models (LLMs) have shown strong performance on programming tasks, but are they capable of generating student-like code that mimics the imperfections, iterations, and stylistic diversity seen in real student work? The study presents ParaStudent, which examines LLM-based code generation in an introductory programming course environment using a dataset of student submissions. Through low- and high-resolution experiments, the research assesses code outputs in terms of semantic, functional, and stylistic elements to model student progress. Results indicate that fine-tuning enhances alignment with real student trajectories by capturing error patterns, incremental improvements, and stylistic variations more accurately. The study emphasizes the importance of context-aware generation, temporal modeling, and multi-dimensional evaluation in accurately modeling realistic student code. The experimental code and evaluation resources are available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2507.12674v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown strong performance on programming tasks, but can they generate student-like code like real students - imperfect, iterative, and stylistically diverse? We present ParaStudent, a systematic study of LLM-based "student-like" code generation in an introductory programming course setting. Using a dataset of timestamped student submissions across multiple semesters, we design low- and high-resolution experiments to model student progress and evaluate code outputs along semantic, functional, and stylistic dimensions. Our results show that fine-tuning significantly improves alignment with real student trajectories and captures error patterns, incremental improvements, and stylistic variations more faithfully. This study shows that modeling realistic student code requires capturing learning dynamics through context-aware generation, temporal modeling, and multi-dimensional evaluation. Code for experiments and evaluation is available at \href{https://github.com/mmiroyan/ParaStudent}{\texttt{github.com/mmiroyan/ParaStudent}}.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks</title>
<link>https://arxiv.org/abs/2507.12675</link>
<guid>https://arxiv.org/abs/2507.12675</guid>
<content:encoded><![CDATA[
<div> Keywords: structural defect segmentation, computational efficiency, depthwise separable convolutions, TiKAN integration, multi-scale attention fusion

Summary:
FORTRESS is a new architecture designed for automated structural defect segmentation in civil infrastructure. It focuses on achieving high accuracy while maintaining computational efficiency for real-time deployment. The architecture incorporates depthwise separable convolutions, adaptive TiKAN integration, and multi-scale attention fusion to balance accuracy and speed effectively. FORTRESS achieves significant efficiency gains with a 91% parameter reduction, 91% computational complexity reduction, and 3x inference speed improvement while delivering superior segmentation performance. Evaluation on benchmark datasets shows state-of-the-art results outperforming existing methods. The dual optimization strategy of FORTRESS proves essential for optimal performance, making it a robust solution for practical structural defect segmentation in resource-constrained environments. The comprehensive architectural specifications and source code are available for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2507.12675v1 Announce Type: cross 
Abstract: Automated structural defect segmentation in civil infrastructure faces a critical challenge: achieving high accuracy while maintaining computational efficiency for real-time deployment. This paper presents FORTRESS (Function-composition Optimized Real-Time Resilient Structural Segmentation), a new architecture that balances accuracy and speed by using a special method that combines depthwise separable convolutions with adaptive Kolmogorov-Arnold Network integration. FORTRESS incorporates three key innovations: a systematic depthwise separable convolution framework achieving a 3.6x parameter reduction per layer, adaptive TiKAN integration that selectively applies function composition transformations only when computationally beneficial, and multi-scale attention fusion combining spatial, channel, and KAN-enhanced features across decoder levels. The architecture achieves remarkable efficiency gains with 91% parameter reduction (31M to 2.9M), 91% computational complexity reduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while delivering superior segmentation performance. Evaluation on benchmark infrastructure datasets demonstrates state-of-the-art results with an F1- score of 0.771 and a mean IoU of 0.677, significantly outperforming existing methods including U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves essential for optimal performance, establishing FORTRESS as a robust solution for practical structural defect segmentation in resource-constrained environments where both accuracy and computational efficiency are paramount. Comprehensive architectural specifications are provided in the Supplemental Material. Source code is available at URL: https://github.com/faeyelab/fortress-paper-code.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Transformation Strategies to Remove Heterogeneity</title>
<link>https://arxiv.org/abs/2507.12677</link>
<guid>https://arxiv.org/abs/2507.12677</guid>
<content:encoded><![CDATA[
<div> data heterogeneity, data transformation, artificial intelligence, data preparation, review<br />
Summary:<br />
Data heterogeneity is a complex issue caused by conflicting factors, leading to disparities in data formats. Current methodologies often overlook the importance of data transformation in resolving conflicts related to data structures and schemas. With the increasing use of artificial intelligence (AI), there is a growing need for a streamlined data preparation process that involves data transformation. This process customizes training data to improve AI learning efficiency and adapts input formats for different AI models. Selecting the right transformation technique is crucial for preserving essential data details. Despite the widespread use of AI in various industries, there is a lack of comprehensive reviews on contemporary data transformation approaches. This survey examines the challenges of data heterogeneity and categorizes strategies for addressing disparities in data formats, highlighting the difficulties associated with each strategy. <br /><br /> <div>
arXiv:2507.12677v1 Announce Type: cross 
Abstract: Data heterogeneity is a prevalent issue, stemming from various conflicting factors, making its utilization complex. This uncertainty, particularly resulting from disparities in data formats, frequently necessitates the involvement of experts to find resolutions. Current methodologies primarily address conflicts related to data structures and schemas, often overlooking the pivotal role played by data transformation. As the utilization of artificial intelligence (AI) continues to expand, there is a growing demand for a more streamlined data preparation process, and data transformation becomes paramount. It customizes training data to enhance AI learning efficiency and adapts input formats to suit diverse AI models. Selecting an appropriate transformation technique is paramount in preserving crucial data details. Despite the widespread integration of AI across various industries, comprehensive reviews concerning contemporary data transformation approaches are scarce. This survey explores the intricacies of data heterogeneity and its underlying sources. It systematically categorizes and presents strategies to address heterogeneity stemming from differences in data formats, shedding light on the inherent challenges associated with each strategy.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Specific Audio Coding for Machines: Machine-Learned Latent Features Are Codes for That Machine</title>
<link>https://arxiv.org/abs/2507.12701</link>
<guid>https://arxiv.org/abs/2507.12701</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural audio codecs, quantization algorithms, efficient compression, downstream task performance, residual vector quantization

Summary:
Neural audio codecs have made a significant impact on speech and audio tasks by leveraging quantization algorithms. While high-fidelity reconstruction is crucial for human perception, audio coding for machines (ACoM) focuses on efficient compression and downstream task performance, neglecting perceptual nuances. This work introduces an efficient ACoM method that can compress and quantize any chosen intermediate feature representation of a trained speech/audio downstream model. The approach combines task-specific loss guidance and residual vector quantization (RVQ) losses to achieve ultra-low bitrates of less than 200 bps with minimal loss of downstream model performance. The resulting tokenizer is adaptable to various bitrates and model sizes for flexible deployment. Evaluation on automatic speech recognition and audio classification demonstrates the method's efficacy and potential for broader task and architectural applicability through appropriate regularization.<br /><br />Summary: Neural audio codecs using quantization algorithms introduce an efficient ACoM method for compressing and quantizing intermediate feature representations. The approach, combining task-specific loss guidance and RVQ losses, achieves ultra-low bitrates with minimal impact on downstream model performance. The method shows promise for various tasks and model architectures, offering flexibility and adaptability for deployment. <div>
arXiv:2507.12701v1 Announce Type: cross 
Abstract: Neural audio codecs, leveraging quantization algorithms, have significantly impacted various speech/audio tasks. While high-fidelity reconstruction is paramount for human perception, audio coding for machines (ACoM) prioritizes efficient compression and downstream task performance, disregarding perceptual nuances. This work introduces an efficient ACoM method that can compress and quantize any chosen intermediate feature representation of an already trained speech/audio downstream model. Our approach employs task-specific loss guidance alongside residual vector quantization (RVQ) losses, providing ultra-low bitrates (i.e., less than 200 bps) with a minimal loss of the downstream model performance. The resulting tokenizer is adaptable to various bitrates and model sizes for flexible deployment. Evaluated on automatic speech recognition and audio classification, our method demonstrates its efficacy and potential for broader task and architectural applicability through appropriate regularization.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-based Spatial Grounding: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2507.12739</link>
<guid>https://arxiv.org/abs/2507.12739</guid>
<content:encoded><![CDATA[
arXiv:2507.12739v1 Announce Type: cross 
Abstract: Spatial grounding, the process of associating natural language expressions with corresponding image regions, has rapidly advanced due to the introduction of transformer-based models, significantly enhancing multimodal representation and cross-modal alignment. Despite this progress, the field lacks a comprehensive synthesis of current methodologies, dataset usage, evaluation metrics, and industrial applicability. This paper presents a systematic literature review of transformer-based spatial grounding approaches from 2018 to 2025. Our analysis identifies dominant model architectures, prevalent datasets, and widely adopted evaluation metrics, alongside highlighting key methodological trends and best practices. This study provides essential insights and structured guidance for researchers and practitioners, facilitating the development of robust, reliable, and industry-ready transformer-based spatial grounding models.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logit Arithmetic Elicits Long Reasoning Capabilities Without Training</title>
<link>https://arxiv.org/abs/2507.12759</link>
<guid>https://arxiv.org/abs/2507.12759</guid>
<content:encoded><![CDATA[
arXiv:2507.12759v1 Announce Type: cross 
Abstract: Large reasoning models (LRMs) can do complex reasoning via long chain-of-thought (CoT) involving cognitive strategies such as backtracking and self-correction. Recent studies suggest that some models inherently possess these long reasoning abilities, which may be unlocked via extra training. Our work first investigates whether we can elicit such behavior without any training. To this end, we propose a decoding-time approach, ThinkLogit, which utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for long reasoning using a substantially smaller model as guider. We then show that we can further boost performance by training the guider model with preference optimization over correct/incorrect reasoning pairs sampled from both the target and guider model -- a setup we refer to as ThinkLogit-DPO. Our experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative improvement in pass@1 by 26% and 29%, respectively, over four mathematical datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model 21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills acquired through reinforcement learning, improving pass@1 by 13% relative compared to the Qwen2.5-32B base model. Our work presents a computationally-efficient method to elicit long reasoning in large models with minimal or no additional training.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Medical Image Segmentation with State Space Modeling Snake</title>
<link>https://arxiv.org/abs/2507.12760</link>
<guid>https://arxiv.org/abs/2507.12760</guid>
<content:encoded><![CDATA[
arXiv:2507.12760v1 Announce Type: cross 
Abstract: Unified Medical Image Segmentation (UMIS) is critical for comprehensive anatomical assessment but faces challenges due to multi-scale structural heterogeneity. Conventional pixel-based approaches, lacking object-level anatomical insight and inter-organ relational modeling, struggle with morphological complexity and feature conflicts, limiting their efficacy in UMIS. We propose Mamba Snake, a novel deep snake framework enhanced by state space modeling for UMIS. Mamba Snake frames multi-contour evolution as a hierarchical state space atlas, effectively modeling macroscopic inter-organ topological relationships and microscopic contour refinements. We introduce a snake-specific vision state space module, the Mamba Evolution Block (MEB), which leverages effective spatiotemporal information aggregation for adaptive refinement of complex morphologies. Energy map shape priors further ensure robust long-range contour evolution in heterogeneous data. Additionally, a dual-classification synergy mechanism is incorporated to concurrently optimize detection and segmentation, mitigating under-segmentation of microstructures in UMIS. Extensive evaluations across five clinical datasets reveal Mamba Snake's superior performance, with an average Dice improvement of 3\% over state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think-Before-Draw: Decomposing Emotion Semantics &amp; Fine-Grained Controllable Expressive Talking Head Generation</title>
<link>https://arxiv.org/abs/2507.12761</link>
<guid>https://arxiv.org/abs/2507.12761</guid>
<content:encoded><![CDATA[
arXiv:2507.12761v1 Announce Type: cross 
Abstract: Emotional talking-head generation has emerged as a pivotal research area at the intersection of computer vision and multimodal artificial intelligence, with its core value lying in enhancing human-computer interaction through immersive and empathetic engagement.With the advancement of multimodal large language models, the driving signals for emotional talking-head generation has shifted from audio and video to more flexible text. However, current text-driven methods rely on predefined discrete emotion label texts, oversimplifying the dynamic complexity of real facial muscle movements and thus failing to achieve natural emotional expressiveness.This study proposes the Think-Before-Draw framework to address two key challenges: (1) In-depth semantic parsing of emotions--by innovatively introducing Chain-of-Thought (CoT), abstract emotion labels are transformed into physiologically grounded facial muscle movement descriptions, enabling the mapping from high-level semantics to actionable motion features; and (2) Fine-grained expressiveness optimization--inspired by artists' portrait painting process, a progressive guidance denoising strategy is proposed, employing a "global emotion localization--local muscle control" mechanism to refine micro-expression dynamics in generated videos.Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including MEAD and HDTF. Additionally, we collected a set of portrait images to evaluate our model's zero-shot generation capability.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomy for Older Adult-Agent Interaction</title>
<link>https://arxiv.org/abs/2507.12767</link>
<guid>https://arxiv.org/abs/2507.12767</guid>
<content:encoded><![CDATA[
arXiv:2507.12767v1 Announce Type: cross 
Abstract: As the global population ages, artificial intelligence (AI)-powered agents have emerged as potential tools to support older adults' caregiving. Prior research has explored agent autonomy by identifying key interaction stages in task processes and defining the agent's role at each stage. However, ensuring that agents align with older adults' autonomy preferences remains a critical challenge. Drawing on interdisciplinary conceptualizations of autonomy, this paper examines four key dimensions of autonomy for older adults: decision-making autonomy, goal-oriented autonomy, control autonomy, and social responsibility autonomy. This paper then proposes the following research directions: (1) Addressing social responsibility autonomy, which concerns the ethical and social implications of agent use in communal settings; (2) Operationalizing agent autonomy from the task perspective; and (3) Developing autonomy measures.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synergy: End-to-end Concept Model</title>
<link>https://arxiv.org/abs/2507.12769</link>
<guid>https://arxiv.org/abs/2507.12769</guid>
<content:encoded><![CDATA[
arXiv:2507.12769v1 Announce Type: cross 
Abstract: In this paper, we present Synergy, a language model that bridges different levels of abstraction in an end-to-end fashion through a learned routing mechanism. Focusing on low-level linguistic abstraction, we trained our model as a byte-level language model. Our model spontaneously learns to tokenize bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE) tokenizers while keeping comparable performance. By comparing with Llama3, we observed an advantage of Synergy under the same model scale and training dataset size. Further studies show that the middle part (the higher abstraction part) of our model performs better when positional encodings are removed, suggesting the emergence of position-independent concepts. These findings demonstrate the feasibility of tokenizer-free architectures, paving the way for more robust and flexible pipelines.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Representative Token Guided Merging for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2507.12771</link>
<guid>https://arxiv.org/abs/2507.12771</guid>
<content:encoded><![CDATA[
arXiv:2507.12771v1 Announce Type: cross 
Abstract: Stable diffusion is an outstanding image generation model for text-to-image, but its time-consuming generation process remains a challenge due to the quadratic complexity of attention operations. Recent token merging methods improve efficiency by reducing the number of tokens during attention operations, but often overlook the characteristics of attention-based image generation models, limiting their effectiveness. In this paper, we propose local representative token guided merging (ReToM), a novel token merging strategy applicable to any attention mechanism in image generation. To merge tokens based on various contextual information, ReToM defines local boundaries as windows within attention inputs and adjusts window sizes. Furthermore, we introduce a representative token, which represents the most representative token per window by computing similarity at a specific timestep and selecting the token with the highest average similarity. This approach preserves the most salient local features while minimizing computational overhead. Experimental results show that ReToM achieves a 6.2% improvement in FID and higher CLIP scores compared to the baseline, while maintaining comparable inference time. We empirically demonstrate that ReToM is effective in balancing visual quality and computational efficiency.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models</title>
<link>https://arxiv.org/abs/2507.12774</link>
<guid>https://arxiv.org/abs/2507.12774</guid>
<content:encoded><![CDATA[
arXiv:2507.12774v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has demonstrated significant potential in transforming healthcare through the analysis and modeling of electronic health records (EHRs). However, the inherent heterogeneity, temporal irregularity, and domain-specific nature of EHR data present unique challenges that differ fundamentally from those in vision and natural language tasks. This survey offers a comprehensive overview of recent advancements at the intersection of deep learning, large language models (LLMs), and EHR modeling. We introduce a unified taxonomy that spans five key design dimensions: data-centric approaches, neural architecture design, learning-focused strategies, multimodal learning, and LLM-based modeling systems. Within each dimension, we review representative methods addressing data quality enhancement, structural and temporal representation, self-supervised learning, and integration with clinical knowledge. We further highlight emerging trends such as foundation models, LLM-driven clinical agents, and EHR-to-text translation for downstream reasoning. Finally, we discuss open challenges in benchmarking, explainability, clinical alignment, and generalization across diverse clinical settings. This survey aims to provide a structured roadmap for advancing AI-driven EHR modeling and clinical decision support. For a comprehensive list of EHR-related methods, kindly refer to https://survey-on-tabular-data.github.io/.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Semi-Supervised Learning Method for the Identification of Bad Exposures in Large Imaging Surveys</title>
<link>https://arxiv.org/abs/2507.12784</link>
<guid>https://arxiv.org/abs/2507.12784</guid>
<content:encoded><![CDATA[
arXiv:2507.12784v1 Announce Type: cross 
Abstract: As the data volume of astronomical imaging surveys rapidly increases, traditional methods for image anomaly detection, such as visual inspection by human experts, are becoming impractical. We introduce a machine-learning-based approach to detect poor-quality exposures in large imaging surveys, with a focus on the DECam Legacy Survey (DECaLS) in regions of low extinction (i.e., $E(B-V)<0.04$). Our semi-supervised pipeline integrates a vision transformer (ViT), trained via self-supervised learning (SSL), with a k-Nearest Neighbor (kNN) classifier. We train and validate our pipeline using a small set of labeled exposures observed by surveys with the Dark Energy Camera (DECam). A clustering-space analysis of where our pipeline places images labeled in ``good'' and ``bad'' categories suggests that our approach can efficiently and accurately determine the quality of exposures. Applied to new imaging being reduced for DECaLS Data Release 11, our pipeline identifies 780 problematic exposures, which we subsequently verify through visual inspection. Being highly efficient and adaptable, our method offers a scalable solution for quality control in other large imaging surveys.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning</title>
<link>https://arxiv.org/abs/2507.12795</link>
<guid>https://arxiv.org/abs/2507.12795</guid>
<content:encoded><![CDATA[
arXiv:2507.12795v1 Announce Type: cross 
Abstract: Scene understanding enables intelligent agents to interpret and comprehend their environment. While existing large vision-language models (LVLMs) for scene understanding have primarily focused on indoor household tasks, they face two significant limitations when applied to outdoor large-scale scene understanding. First, outdoor scenarios typically encompass larger-scale environments observed through various sensors from multiple viewpoints (e.g., bird view and terrestrial view), while existing indoor LVLMs mainly analyze single visual modalities within building-scale contexts from humanoid viewpoints. Second, existing LVLMs suffer from missing multidomain perception outdoor data and struggle to effectively integrate 2D and 3D visual information. To address the aforementioned limitations, we build the first multidomain perception outdoor scene understanding dataset, named \textbf{\underline{SVM-City}}, deriving from multi\textbf{\underline{S}}cale scenarios with multi\textbf{\underline{V}}iew and multi\textbf{\underline{M}}odal instruction tuning data. It contains $420$k images and $4, 811$M point clouds with $567$k question-answering pairs from vehicles, low-altitude drones, high-altitude aerial planes, and satellite. To effectively fuse the multimodal data in the absence of one modality, we introduce incomplete multimodal learning to model outdoor scene understanding and design the LVLM named \textbf{\underline{City-VLM}}. Multimodal fusion is realized by constructing a joint probabilistic distribution space rather than implementing directly explicit fusion operations (e.g., concatenation). Experimental results on three typical outdoor scene understanding tasks show City-VLM achieves $18.14 \%$ performance surpassing existing LVLMs in question-answering tasks averagely. Our method demonstrates pragmatic and generalization performance across multiple outdoor scenes.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLDmamba: Integrating Fourier and Laplace Transform Decomposition with Mamba for Enhanced Time Series Prediction</title>
<link>https://arxiv.org/abs/2507.12803</link>
<guid>https://arxiv.org/abs/2507.12803</guid>
<content:encoded><![CDATA[
arXiv:2507.12803v1 Announce Type: cross 
Abstract: Time series prediction, a crucial task across various domains, faces significant challenges due to the inherent complexities of time series data, including non-stationarity, multi-scale periodicity, and transient dynamics, particularly when tackling long-term predictions. While Transformer-based architectures have shown promise, their quadratic complexity with sequence length hinders their efficiency for long-term predictions. Recent advancements in State-Space Models, such as Mamba, offer a more efficient alternative for long-term modeling, but they cannot capture multi-scale periodicity and transient dynamics effectively. Meanwhile, they are susceptible to data noise issues in time series. This paper proposes a novel framework, FLDmamba (Fourier and Laplace Transform Decomposition Mamba), addressing these limitations. FLDmamba leverages the strengths of both Fourier and Laplace transforms to effectively capture both multi-scale periodicity, transient dynamics within time series data, and improve the robustness of the model to the data noise issue. Our extensive experiments demonstrate that FLDmamba achieves superior performance on time series prediction benchmarks, outperforming both Transformer-based and other Mamba-based architectures. To promote the reproducibility of our method, we have made both the code and data accessible via the following URL:{\href{https://github.com/AI4Science-WestlakeU/FLDmamba}{https://github.com/AI4Science-WestlakeU/\model}.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database</title>
<link>https://arxiv.org/abs/2507.12805</link>
<guid>https://arxiv.org/abs/2507.12805</guid>
<content:encoded><![CDATA[
arXiv:2507.12805v1 Announce Type: cross 
Abstract: Learning-based lossless compressors play a crucial role in large-scale genomic database backup, storage, transmission, and management. However, their 1) inadequate compression ratio, 2) low compression \& decompression throughput, and 3) poor compression robustness limit their widespread adoption and application in both industry and academia. To solve those challenges, we propose a novel \underline{P}arallel \underline{M}ulti-\underline{K}nowledge \underline{L}earning-based \underline{C}ompressor (PMKLC) with four crucial designs: 1) We propose an automated multi-knowledge learning-based compression framework as compressors' backbone to enhance compression ratio and robustness; 2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression throughput and computing resource usage; 3) we introduce data block partitioning and Step-wise Model Passing (SMP) mechanisms for parallel acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet the complex application scenarios, where the former runs on a resource-constrained single GPU and the latter is multi-GPU accelerated. We benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15 real-world datasets with different species and data sizes. Compared to baselines on the testing datasets, PMKLC-S/M achieve the average compression ratio improvement up to 73.609\% and 73.480\%, the average throughput improvement up to 3.036$\times$ and 10.710$\times$, respectively. Besides, PMKLC-S/M also achieve the best robustness and competitive memory cost, indicating its greater stability against datasets with different probability distribution perturbations, and its strong ability to run on memory-constrained devices.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models' Internal Perception of Symbolic Music</title>
<link>https://arxiv.org/abs/2507.12808</link>
<guid>https://arxiv.org/abs/2507.12808</guid>
<content:encoded><![CDATA[
arXiv:2507.12808v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at modeling relationships between strings in natural language and have shown promise in extending to other symbolic domains like coding or mathematics. However, the extent to which they implicitly model symbolic music remains underexplored. This paper investigates how LLMs represent musical concepts by generating symbolic music data from textual prompts describing combinations of genres and styles, and evaluating their utility through recognition and generation tasks. We produce a dataset of LLM-generated MIDI files without relying on explicit musical training. We then train neural networks entirely on this LLM-generated MIDI dataset and perform genre and style classification as well as melody completion, benchmarking their performance against established models. Our results demonstrate that LLMs can infer rudimentary musical structures and temporal relationships from text, highlighting both their potential to implicitly encode musical patterns and their limitations due to a lack of explicit musical context, shedding light on their generative capabilities for symbolic music.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FIQ: Fundamental Question Generation with the Integration of Question Embeddings for Video Question Answering</title>
<link>https://arxiv.org/abs/2507.12816</link>
<guid>https://arxiv.org/abs/2507.12816</guid>
<content:encoded><![CDATA[
arXiv:2507.12816v1 Announce Type: cross 
Abstract: Video question answering (VQA) is a multimodal task that requires the interpretation of a video to answer a given question. Existing VQA methods primarily utilize question and answer (Q&amp;A) pairs to learn the spatio-temporal characteristics of video content. However, these annotations are typically event-centric, which is not enough to capture the broader context of each video. The absence of essential details such as object types, spatial layouts, and descriptive attributes restricts the model to learning only a fragmented scene representation. This issue limits the model's capacity for generalization and higher-level reasoning. In this paper, we propose a fundamental question generation with the integration of question embeddings for video question answering (FIQ), a novel approach designed to strengthen the reasoning ability of the model by enhancing the fundamental understanding of videos. FIQ generates Q&amp;A pairs based on descriptions extracted from videos, enriching the training data with fundamental scene information. Generated Q&amp;A pairs enable the model to understand the primary context, leading to enhanced generalizability and reasoning ability. Furthermore, we incorporate a VQ-CAlign module that assists task-specific question embeddings with visual features, ensuring that essential domain-specific details are preserved to increase the adaptability of downstream tasks. Experiments on SUTD-TrafficQA demonstrate that our FIQ achieves state-of-the-art performance compared to existing baseline methods.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature-Enhanced TResNet for Fine-Grained Food Image Classification</title>
<link>https://arxiv.org/abs/2507.12828</link>
<guid>https://arxiv.org/abs/2507.12828</guid>
<content:encoded><![CDATA[
arXiv:2507.12828v1 Announce Type: cross 
Abstract: Food is not only a core component of humans' daily diets, but also an important carrier of cultural heritage and emotional bonds. With the development of technology, the need for accurate classification of food images has grown, which is crucial for a variety of application scenarios. However, existing Convolutional Neural Networks (CNNs) face significant challenges when dealing with fine-grained food images that are similar in shape but subtle in detail. To address this challenge, this study presents an innovative method for classifying food images, named Feature-Enhanced TResNet (FE-TResNet), specifically designed to address fine-grained food images and accurately capture subtle features within them. The FE-TResNet method is based on the TResNet model and integrates Style-based Recalibration Module (StyleRM) and Deep Channel-wise Attention (DCA) technologies to enhance feature extraction capabilities. In experimental validation on Chinese food image datasets ChineseFoodNet and CNFOOD-241, the FE-TResNet method significantly improved classification accuracy, achieving rates of 81.37% and 80.29%, respectively, demonstrating its effectiveness and superiority in fine-grained food image classification.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results</title>
<link>https://arxiv.org/abs/2507.12832</link>
<guid>https://arxiv.org/abs/2507.12832</guid>
<content:encoded><![CDATA[
arXiv:2507.12832v1 Announce Type: cross 
Abstract: Small Multi-Object Tracking (SMOT) is particularly challenging when targets occupy only a few dozen pixels, rendering detection and appearance-based association unreliable. Building on the success of the MVA2023 SOD4SB challenge, this paper introduces the SMOT4SB challenge, which leverages temporal information to address limitations of single-frame detection. Our three main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV video sequences with 108,192 annotated frames under diverse real-world conditions, designed to capture motion entanglement where both camera and targets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance with HOTA to mitigate the sensitivity of IoU-based metrics to small displacements; and (3) a competitive MVA2025 challenge with 78 participants and 308 submissions, where the winning method achieved a 5.1x improvement over the baseline. This work lays a foundation for advancing SMOT in UAV scenarios with applications in bird strike avoidance, agriculture, fisheries, and ecological monitoring.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEMT: Static-Expansion-Mesh Transformer Network Architecture for Remote Sensing Image Captioning</title>
<link>https://arxiv.org/abs/2507.12845</link>
<guid>https://arxiv.org/abs/2507.12845</guid>
<content:encoded><![CDATA[
arXiv:2507.12845v1 Announce Type: cross 
Abstract: Image captioning has emerged as a crucial task in the intersection of computer vision and natural language processing, enabling automated generation of descriptive text from visual content. In the context of remote sensing, image captioning plays a significant role in interpreting vast and complex satellite imagery, aiding applications such as environmental monitoring, disaster assessment, and urban planning. This motivates us, in this paper, to present a transformer based network architecture for remote sensing image captioning (RSIC) in which multiple techniques of Static Expansion, Memory-Augmented Self-Attention, Mesh Transformer are evaluated and integrated. We evaluate our proposed models using two benchmark remote sensing image datasets of UCM-Caption and NWPU-Caption. Our best model outperforms the state-of-the-art systems on most of evaluation metrics, which demonstrates potential to apply for real-life remote sensing image systems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering</title>
<link>https://arxiv.org/abs/2507.12846</link>
<guid>https://arxiv.org/abs/2507.12846</guid>
<content:encoded><![CDATA[
arXiv:2507.12846v1 Announce Type: cross 
Abstract: As robots become increasingly capable of operating over extended periods -- spanning days, weeks, and even months -- they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively. This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determines when the agent has gathered sufficient information. We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)</title>
<link>https://arxiv.org/abs/2507.12856</link>
<guid>https://arxiv.org/abs/2507.12856</guid>
<content:encoded><![CDATA[
arXiv:2507.12856v1 Announce Type: cross 
Abstract: Behavior Cloning (BC) on curated (or filtered) data is the predominant paradigm for supervised fine-tuning (SFT) of large language models; as well as for imitation learning of control policies. Here, we draw on a connection between this successful strategy and the theory and practice of finding optimal policies via Reinforcement Learning (RL). Building on existing literature, we clarify that SFT can be understood as maximizing a lower bound on the RL objective in a sparse reward setting. Giving support to its often observed good performance. From this viewpoint, we realize that a small modification to SFT leads to an importance weighted variant that behaves closer to training with RL as it: i) optimizes a tighter bound to the RL objective and, ii) can improve performance compared to SFT on curated data. We refer to this variant as importance weighted supervised fine-tuning (iw-SFT). We show that it is easy to implement and can be further generalized to training with quality scored data. The resulting SFT variants are competitive with more advanced RL algorithms for large language models and for training policies in continuous control tasks. For example achieving 66.7% on the AIME 2024 dataset.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Multi-Target Cross-Domain Recommendation</title>
<link>https://arxiv.org/abs/2507.12871</link>
<guid>https://arxiv.org/abs/2507.12871</guid>
<content:encoded><![CDATA[
arXiv:2507.12871v1 Announce Type: cross 
Abstract: Recently, there has been a surge of interest in Multi-Target Cross-Domain Recommendation (MTCDR), which aims to enhance recommendation performance across multiple domains simultaneously. Existing MTCDR methods primarily rely on domain-shared entities (\eg users or items) to fuse and transfer cross-domain knowledge, which may be unavailable in non-overlapped recommendation scenarios. Some studies model user preferences and item features as domain-sharable semantic representations, which can be utilized to tackle the MTCDR task. Nevertheless, they often require extensive auxiliary data for pre-training. Developing more effective solutions for MTCDR remains an important area for further exploration.
  Inspired by recent advancements in generative recommendation, this paper introduces GMC, a generative paradigm-based approach for multi-target cross-domain recommendation. The core idea of GMC is to leverage semantically quantized discrete item identifiers as a medium for integrating multi-domain knowledge within a unified generative model. GMC first employs an item tokenizer to generate domain-shared semantic identifiers for each item, and then formulates item recommendation as a next-token generation task by training a domain-unified sequence-to-sequence model. To further leverage the domain information to enhance performance, we incorporate a domain-aware contrastive loss into the semantic identifier learning, and perform domain-specific fine-tuning on the unified recommender. Extensive experiments on five public datasets demonstrate the effectiveness of GMC compared to a range of baseline methods.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An ultra-low-power CGRA for accelerating Transformers at the edge</title>
<link>https://arxiv.org/abs/2507.12904</link>
<guid>https://arxiv.org/abs/2507.12904</guid>
<content:encoded><![CDATA[
arXiv:2507.12904v1 Announce Type: cross 
Abstract: Transformers have revolutionized deep learning with applications in natural language processing, computer vision, and beyond. However, their computational demands make it challenging to deploy them on low-power edge devices. This paper introduces an ultra-low-power, Coarse-Grained Reconfigurable Array (CGRA) architecture specifically designed to accelerate General Matrix Multiplication (GEMM) operations in transformer models tailored for the energy and resource constraints of edge applications. The proposed architecture integrates a 4 x 4 array of Processing Elements (PEs) for efficient parallel computation and dedicated 4 x 2 Memory Operation Blocks (MOBs) for optimized LOAD/STORE operations, reducing memory bandwidth demands and enhancing data reuse. A switchless mesh torus interconnect network further minimizes power and latency by enabling direct communication between PEs and MOBs, eliminating the need for centralized switching. Through its heterogeneous array design and efficient dataflow, this CGRA architecture addresses the unique computational needs of transformers, offering a scalable pathway to deploy sophisticated machine learning models on edge devices.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding With Large Language Models</title>
<link>https://arxiv.org/abs/2507.12916</link>
<guid>https://arxiv.org/abs/2507.12916</guid>
<content:encoded><![CDATA[
arXiv:2507.12916v1 Announce Type: cross 
Abstract: Advancements in foundation models have made it possible to conduct applications in various downstream tasks. Especially, the new era has witnessed a remarkable capability to extend Large Language Models (LLMs) for tackling tasks of 3D scene understanding. Current methods rely heavily on 3D point clouds, but the 3D point cloud reconstruction of an indoor scene often results in information loss. Some textureless planes or repetitive patterns are prone to omission and manifest as voids within the reconstructed 3D point clouds. Besides, objects with complex structures tend to introduce distortion of details caused by misalignments between the captured images and the dense reconstructed point clouds. 2D multi-view images present visual consistency with 3D point clouds and provide more detailed representations of scene components, which can naturally compensate for these deficiencies. Based on these insights, we propose Argus, a novel 3D multimodal framework that leverages multi-view images for enhanced 3D scene understanding with LLMs. In general, Argus can be treated as a 3D Large Multimodal Foundation Model (3D-LMM) since it takes various modalities as input(text instructions, 2D multi-view images, and 3D point clouds) and expands the capability of LLMs to tackle 3D tasks. Argus involves fusing and integrating multi-view images and camera poses into view-as-scene features, which interact with the 3D features to create comprehensive and detailed 3D-aware scene embeddings. Our approach compensates for the information loss while reconstructing 3D point clouds and helps LLMs better understand the 3D world. Extensive experiments demonstrate that our method outperforms existing 3D-LMMs in various downstream tasks.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making Language Model a Hierarchical Classifier and Generator</title>
<link>https://arxiv.org/abs/2507.12930</link>
<guid>https://arxiv.org/abs/2507.12930</guid>
<content:encoded><![CDATA[
arXiv:2507.12930v1 Announce Type: cross 
Abstract: Decoder-only language models, such as GPT and LLaMA, generally decode on the last layer. Motivated by human's hierarchical thinking capability, we propose that a hierarchical decoder architecture could be built with different layers decoding texts simultaneously. Due to limited time and computationally resources, we choose to adapt a pretrained language model into this form of hierarchical decoder. Language heads of the last layer are copied to different selected intermediate layers, and fine-tuned with different task inputs. By thorough experiments, we validate that these selective intermediate layers could be adapted to speak meaningful and reasonable contents, and this paradigm of hierarchical decoder can obtain state-of-the-art performances on multiple tasks such as hierarchical text classification, classification-guided generation, and hierarchical text generation. This study suggests the possibility of a generalized hierarchical reasoner, pretraining from scratch.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization</title>
<link>https://arxiv.org/abs/2507.12933</link>
<guid>https://arxiv.org/abs/2507.12933</guid>
<content:encoded><![CDATA[
arXiv:2507.12933v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable success in image generation but come with significant computational costs, posing challenges for deployment in resource-constrained environments. Recent post-training quantization (PTQ) methods have attempted to mitigate this issue by focusing on the iterative nature of diffusion models. However, these approaches often overlook outliers, leading to degraded performance at low bit-widths. In this paper, we propose a DMQ which combines Learned Equivalent Scaling (LES) and channel-wise Power-of-Two Scaling (PTS) to effectively address these challenges. Learned Equivalent Scaling optimizes channel-wise scaling factors to redistribute quantization difficulty between weights and activations, reducing overall quantization error. Recognizing that early denoising steps, despite having small quantization errors, crucially impact the final output due to error accumulation, we incorporate an adaptive timestep weighting scheme to prioritize these critical steps during learning. Furthermore, identifying that layers such as skip connections exhibit high inter-channel variance, we introduce channel-wise Power-of-Two Scaling for activations. To ensure robust selection of PTS factors even with small calibration set, we introduce a voting algorithm that enhances reliability. Extensive experiments demonstrate that our method significantly outperforms existing works, especially at low bit-widths such as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image generation quality and model stability. The code is available at https://github.com/LeeDongYeun/dmq.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration</title>
<link>https://arxiv.org/abs/2507.12935</link>
<guid>https://arxiv.org/abs/2507.12935</guid>
<content:encoded><![CDATA[
arXiv:2507.12935v1 Announce Type: cross 
Abstract: An increasing number of applications are exploiting sampling-based algorithms for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC) algorithms form the computational backbone of this emerging branch of machine learning. Unfortunately, the high computational cost limits their feasibility for large-scale problems and real-world applications, and the existing MCMC acceleration solutions are either limited in hardware flexibility or fail to maintain efficiency at the system level across a variety of end-to-end applications. This paper introduces \textbf{MC$^2$A}, an algorithm-hardware co-design framework, enabling efficient and flexible optimization for MCMC acceleration. Firstly, \textbf{MC$^2$A} analyzes the MCMC workload diversity through an extension of the processor performance roofline model with a 3rd dimension to derive the optimal balance between the compute, sampling and memory parameters. Secondly, \textbf{MC$^2$A} proposes a parametrized hardware accelerator architecture with flexible and efficient support of MCMC kernels with a pipeline of ISA-programmable tree-structured processing units, reconfigurable samplers and a crossbar interconnect to support irregular access. Thirdly, the core of \textbf{MC$^2$A} is powered by a novel Gumbel sampler that eliminates exponential and normalization operations. In the end-to-end case study, \textbf{MC$^2$A} achieves an overall {$307.6\times$, $1.4\times$, $2.0\times$, $84.2\times$} speedup compared to the CPU, GPU, TPU and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC workloads, this work demonstrates and exploits the feasibility of general hardware acceleration to popularize MCMC-based solutions in diverse application domains.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets</title>
<link>https://arxiv.org/abs/2507.12951</link>
<guid>https://arxiv.org/abs/2507.12951</guid>
<content:encoded><![CDATA[
arXiv:2507.12951v1 Announce Type: cross 
Abstract: Spoken Language Understanding (SLU) plays a crucial role in speech-centric multimedia applications, enabling machines to comprehend spoken language in scenarios such as meetings, interviews, and customer service interactions. SLU encompasses multiple tasks, including Automatic Speech Recognition (ASR), spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA). However, existing methods often rely on separate model architectures for individual tasks such as spoken NER and SA, which increases system complexity, limits cross-task interaction, and fails to fully exploit heterogeneous datasets available across tasks. To address these limitations, we propose UniSLU, a unified framework that jointly models multiple SLU tasks within a single architecture. Specifically, we propose a unified representation for diverse SLU tasks, enabling full utilization of heterogeneous datasets across multiple tasks. Built upon this representation, we propose a unified generative method that jointly models ASR, spoken NER, and SA tasks, enhancing task interactions and enabling seamless integration with large language models to harness their powerful generative capabilities. Extensive experiments on public SLU datasets demonstrate the effectiveness of our approach, achieving superior SLU performance compared to several benchmark methods, making it well-suited for real-world speech-based multimedia scenarios. We will release all code and models at github to facilitate future research.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Diagnostic Accuracy of Pigmented Skin Lesions With CNNs: an Application on the DermaMNIST Dataset</title>
<link>https://arxiv.org/abs/2507.12961</link>
<guid>https://arxiv.org/abs/2507.12961</guid>
<content:encoded><![CDATA[
arXiv:2507.12961v1 Announce Type: cross 
Abstract: Pigmented skin lesions represent localized areas of increased melanin and can indicate serious conditions like melanoma, a major contributor to skin cancer mortality. The MedMNIST v2 dataset, inspired by MNIST, was recently introduced to advance research in biomedical imaging and includes DermaMNIST, a dataset for classifying pigmented lesions based on the HAM10000 dataset. This study assesses ResNet-50 and EfficientNetV2L models for multi-class classification using DermaMNIST, employing transfer learning and various layer configurations. One configuration achieves results that match or surpass existing methods. This study suggests that convolutional neural networks (CNNs) can drive progress in biomedical image analysis, significantly enhancing diagnostic accuracy.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demographic-aware fine-grained classification of pediatric wrist fractures</title>
<link>https://arxiv.org/abs/2507.12964</link>
<guid>https://arxiv.org/abs/2507.12964</guid>
<content:encoded><![CDATA[
arXiv:2507.12964v1 Announce Type: cross 
Abstract: Wrist pathologies are frequently observed, particularly among children who constitute the majority of fracture cases. However, diagnosing these conditions is time-consuming and requires specialized expertise. Computer vision presents a promising avenue, contingent upon the availability of extensive datasets, a notable challenge in medical imaging. Therefore, reliance solely on one modality, such as images, proves inadequate, especially in an era of diverse and plentiful data types. In this study, we employ a multifaceted approach to address the challenge of recognizing wrist pathologies using an extremely limited dataset. Initially, we approach the problem as a fine-grained recognition task, aiming to identify subtle X-ray pathologies that conventional CNNs overlook. Secondly, we enhance network performance by fusing patient metadata with X-ray images. Thirdly, rather than pre-training on a coarse-grained dataset like ImageNet, we utilize weights trained on a fine-grained dataset. While metadata integration has been used in other medical domains, this is a novel application for wrist pathologies. Our results show that a fine-grained strategy and metadata integration improve diagnostic accuracy by 2% with a limited dataset and by over 10% with a larger fracture-focused dataset.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints</title>
<link>https://arxiv.org/abs/2507.12979</link>
<guid>https://arxiv.org/abs/2507.12979</guid>
<content:encoded><![CDATA[
arXiv:2507.12979v1 Announce Type: cross 
Abstract: Federated Learning has gained increasing attention for its ability to enable multiple nodes to collaboratively train machine learning models without sharing their raw data. At the same time, Generative AI -- particularly Generative Adversarial Networks (GANs) -- have achieved remarkable success across a wide range of domains, such as healthcare, security, and Image Generation. However, training generative models typically requires large datasets and significant computational resources, which are often unavailable in real-world settings. Acquiring such resources can be costly and inefficient, especially when many underutilized devices -- such as IoT devices and edge devices -- with varying capabilities remain idle. Moreover, obtaining large datasets is challenging due to privacy concerns and copyright restrictions, as most devices are unwilling to share their data. To address these challenges, we propose a novel approach for decentralized GAN training that enables the utilization of distributed data and underutilized, low-capability devices while not sharing data in its raw form. Our approach is designed to tackle key challenges in decentralized environments, combining KLD-weighted Clustered Federated Learning to address the issues of data heterogeneity and multi-domain datasets, with Heterogeneous U-Shaped split learning to tackle the challenge of device heterogeneity under strict data sharing constraints -- ensuring that no labels or raw data, whether real or synthetic, are ever shared between nodes. Experimental results shows that our approach demonstrates consistent and significant improvements across key performance metrics, where it achieves 1.1x -- 2.2x higher image generation scores, an average 10% boost in classification metrics (up to 50% in multi-domain non-IID settings), in much lower latency compared to several benchmarks. Find our code at https://github.com/youssefga28/HuSCF-GAN.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps</title>
<link>https://arxiv.org/abs/2507.12981</link>
<guid>https://arxiv.org/abs/2507.12981</guid>
<content:encoded><![CDATA[
arXiv:2507.12981v1 Announce Type: cross 
Abstract: This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in Spanish). Our solution obtains answers to the questions by implementing Python code generation with LLMs that is used to filter and process the table. This solution evolves from the MRT implementation for the Semeval 2025 related task. The process consists of multiple steps: analyzing and understanding the content of the table, selecting the useful columns, generating instructions in natural language, translating these instructions to code, running it, and handling potential errors or exceptions. These steps use open-source LLMs and fine-grained optimized prompts for each step. With this approach, we achieved an accuracy score of 85\% in the task.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teach Old SAEs New Domain Tricks with Boosting</title>
<link>https://arxiv.org/abs/2507.12990</link>
<guid>https://arxiv.org/abs/2507.12990</guid>
<content:encoded><![CDATA[
arXiv:2507.12990v1 Announce Type: cross 
Abstract: Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMART: Relation-Aware Learning of Geometric Representations for Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.13001</link>
<guid>https://arxiv.org/abs/2507.13001</guid>
<content:encoded><![CDATA[
arXiv:2507.13001v1 Announce Type: cross 
Abstract: Knowledge graph representation learning approaches provide a mapping between symbolic knowledge in the form of triples in a knowledge graph (KG) and their feature vectors. Knowledge graph embedding (KGE) models often represent relations in a KG as geometric transformations. Most state-of-the-art (SOTA) KGE models are derived from elementary geometric transformations (EGTs), such as translation, scaling, rotation, and reflection, or their combinations. These geometric transformations enable the models to effectively preserve specific structural and relational patterns of the KG. However, the current use of EGTs by KGEs remains insufficient without considering relation-specific transformations. Although recent models attempted to address this problem by ensembling SOTA baseline models in different ways, only a single or composite version of geometric transformations are used by such baselines to represent all the relations. In this paper, we propose a framework that evaluates how well each relation fits with different geometric transformations. Based on this ranking, the model can: (1) assign the best-matching transformation to each relation, or (2) use majority voting to choose one transformation type to apply across all relations. That is, the model learns a single relation-specific EGT in low dimensional vector space through an attention mechanism. Furthermore, we use the correlation between relations and EGTs, which are learned in a low dimension, for relation embeddings in a high dimensional vector space. The effectiveness of our models is demonstrated through comprehensive evaluations on three benchmark KGs as well as a real-world financial KG, witnessing a performance comparable to leading models
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities</title>
<link>https://arxiv.org/abs/2507.13019</link>
<guid>https://arxiv.org/abs/2507.13019</guid>
<content:encoded><![CDATA[
arXiv:2507.13019v1 Announce Type: cross 
Abstract: Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUPAX: Multidimensional Problem Agnostic eXplainable AI</title>
<link>https://arxiv.org/abs/2507.13090</link>
<guid>https://arxiv.org/abs/2507.13090</guid>
<content:encoded><![CDATA[
arXiv:2507.13090v1 Announce Type: cross 
Abstract: Robust XAI techniques should ideally be simultaneously deterministic, model agnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM AGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability technique, with guaranteed convergency. MUPAX measure theoretic formulation gives principled feature importance attribution through structured perturbation analysis that discovers inherent input patterns and eliminates spurious relationships. We evaluate MUPAX on an extensive range of data modalities and tasks: audio classification (1D), image classification (2D), volumetric medical image analysis (3D), and anatomical landmark detection, demonstrating dimension agnostic effectiveness. The rigorous convergence guarantees extend to any loss function and arbitrary dimensions, making MUPAX applicable to virtually any problem context for AI. By contrast with other XAI methods that typically decrease performance when masking, MUPAX not only preserves but actually enhances model accuracy by capturing only the most important patterns of the original data. Extensive benchmarking against the state of the XAI art demonstrates MUPAX ability to generate precise, consistent and understandable explanations, a crucial step towards explainable and trustworthy AI systems. The source code will be released upon publication.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training</title>
<link>https://arxiv.org/abs/2507.13097</link>
<guid>https://arxiv.org/abs/2507.13097</guid>
<content:encoded><![CDATA[
arXiv:2507.13097v1 Announce Type: cross 
Abstract: Grasping is a fundamental robot skill, yet despite significant research advancements, learning-based 6-DOF grasping approaches are still not turnkey and struggle to generalize across different embodiments and in-the-wild settings. We build upon the recent success on modeling the object-centric grasp generation process as an iterative diffusion process. Our proposed framework, GraspGen, consists of a DiffusionTransformer architecture that enhances grasp generation, paired with an efficient discriminator to score and filter sampled grasps. We introduce a novel and performant on-generator training recipe for the discriminator. To scale GraspGen to both objects and grippers, we release a new simulated dataset consisting of over 53 million grasps. We demonstrate that GraspGen outperforms prior methods in simulations with singulated objects across different grippers, achieves state-of-the-art performance on the FetchBench grasping benchmark, and performs well on a real robot with noisy visual observations.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model</title>
<link>https://arxiv.org/abs/2507.13145</link>
<guid>https://arxiv.org/abs/2507.13145</guid>
<content:encoded><![CDATA[
arXiv:2507.13145v1 Announce Type: cross 
Abstract: Learning-based monocular visual odometry (VO) poses robustness, generalization, and efficiency challenges in robotics. Recent advances in visual foundation models, such as DINOv2, have improved robustness and generalization in various vision tasks, yet their integration in VO remains limited due to coarse feature granularity. In this paper, we present DINO-VO, a feature-based VO system leveraging DINOv2 visual foundation model for its sparse feature matching. To address the integration challenge, we propose a salient keypoints detector tailored to DINOv2's coarse features. Furthermore, we complement DINOv2's robust-semantic features with fine-grained geometric features, resulting in more localizable representations. Finally, a transformer-based matcher and differentiable pose estimation layer enable precise camera motion estimation by learning good matches. Against prior detector-descriptor networks like SuperPoint, DINO-VO demonstrates greater robustness in challenging environments. Furthermore, we show superior accuracy and generalization of the proposed feature descriptors against standalone DINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on the TartanAir and KITTI datasets and is competitive on EuRoC dataset, while running efficiently at 72 FPS with less than 1GB of memory usage on a single GPU. Moreover, it performs competitively against Visual SLAM systems on outdoor driving scenarios, showcasing its generalization capabilities.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2507.13152</link>
<guid>https://arxiv.org/abs/2507.13152</guid>
<content:encoded><![CDATA[
arXiv:2507.13152v1 Announce Type: cross 
Abstract: Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities</title>
<link>https://arxiv.org/abs/2507.13158</link>
<guid>https://arxiv.org/abs/2507.13158</guid>
<content:encoded><![CDATA[
arXiv:2507.13158v1 Announce Type: cross 
Abstract: In the era of Large Language Models (LLMs), alignment has emerged as a fundamental yet challenging problem in the pursuit of more reliable, controllable, and capable machine intelligence. The recent success of reasoning models and conversational AI systems has underscored the critical role of reinforcement learning (RL) in enhancing these systems, driving increased research interest at the intersection of RL and LLM alignment. This paper provides a comprehensive review of recent advances in LLM alignment through the lens of inverse reinforcement learning (IRL), emphasizing the distinctions between RL techniques employed in LLM alignment and those in conventional RL tasks. In particular, we highlight the necessity of constructing neural reward models from human data and discuss the formal and practical implications of this paradigm shift. We begin by introducing fundamental concepts in RL to provide a foundation for readers unfamiliar with the field. We then examine recent advances in this research agenda, discussing key challenges and opportunities in conducting IRL for LLM alignment. Beyond methodological considerations, we explore practical aspects, including datasets, benchmarks, evaluation metrics, infrastructure, and computationally efficient training and inference techniques. Finally, we draw insights from the literature on sparse-reward RL to identify open questions and potential research directions. By synthesizing findings from diverse studies, we aim to provide a structured and critical overview of the field, highlight unresolved challenges, and outline promising future directions for improving LLM alignment through RL and IRL techniques.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models</title>
<link>https://arxiv.org/abs/2507.13162</link>
<guid>https://arxiv.org/abs/2507.13162</guid>
<content:encoded><![CDATA[
arXiv:2507.13162v1 Announce Type: cross 
Abstract: Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at https://lmb-freiburg.github.io/orbis.github.io/.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Injection 2.0: Hybrid AI Threats</title>
<link>https://arxiv.org/abs/2507.13169</link>
<guid>https://arxiv.org/abs/2507.13169</guid>
<content:encoded><![CDATA[
arXiv:2507.13169v1 Announce Type: cross 
Abstract: Prompt injection attacks, where malicious input is designed to manipulate AI systems into ignoring their original instructions and following unauthorized commands instead, were first discovered by Preamble, Inc. in May 2022 and responsibly disclosed to OpenAI. Over the last three years, these attacks have continued to pose a critical security threat to LLM-integrated systems. The emergence of agentic AI systems, where LLMs autonomously perform multistep tasks through tools and coordination with other agents, has fundamentally transformed the threat landscape. Modern prompt injection attacks can now combine with traditional cybersecurity exploits to create hybrid threats that systematically evade traditional security controls. This paper presents a comprehensive analysis of Prompt Injection 2.0, examining how prompt injections integrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF), and other web security vulnerabilities to bypass traditional security measures. We build upon Preamble's foundational research and mitigation technologies, evaluating them against contemporary threats, including AI worms, multi-agent infections, and hybrid cyber-AI attacks. Our analysis incorporates recent benchmarks that demonstrate how traditional web application firewalls, XSS filters, and CSRF tokens fail against AI-enhanced attacks. We also present architectural solutions that combine prompt isolation, runtime security, and privilege separation with novel threat detection capabilities.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks</title>
<link>https://arxiv.org/abs/2507.13170</link>
<guid>https://arxiv.org/abs/2507.13170</guid>
<content:encoded><![CDATA[
arXiv:2507.13170v1 Announce Type: cross 
Abstract: Audio plays a crucial role in applications like speaker verification, voice-enabled smart devices, and audio conferencing. However, audio manipulations, such as deepfakes, pose significant risks by enabling the spread of misinformation. Our empirical analysis reveals that existing methods for detecting deepfake audio are often vulnerable to anti-forensic (AF) attacks, particularly those attacked using generative adversarial networks. In this article, we propose a novel collaborative learning method called SHIELD to defend against generative AF attacks. To expose AF signatures, we integrate an auxiliary generative model, called the defense (DF) generative model, which facilitates collaborative learning by combining input and output. Furthermore, we design a triplet model to capture correlations for real and AF attacked audios with real-generated and attacked-generated audios using auxiliary generative models. The proposed SHIELD strengthens the defense against generative AF attacks and achieves robust performance across various generative models. The proposed AF significantly reduces the average detection accuracy from 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild, and from 98.41% to 51.18% for HalfTruth for three different generative models. The proposed SHIELD mechanism is robust against AF attacks and achieves an average accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%, and 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and HalfTruth datasets, respectively.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback</title>
<link>https://arxiv.org/abs/2507.13171</link>
<guid>https://arxiv.org/abs/2507.13171</guid>
<content:encoded><![CDATA[
arXiv:2507.13171v1 Announce Type: cross 
Abstract: Conventional reinforcement learning (RL) ap proaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, rein forcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, en abling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesizing Reality: Leveraging the Generative AI-Powered Platform Midjourney for Construction Worker Detection</title>
<link>https://arxiv.org/abs/2507.13221</link>
<guid>https://arxiv.org/abs/2507.13221</guid>
<content:encoded><![CDATA[
arXiv:2507.13221v1 Announce Type: cross 
Abstract: While recent advancements in deep neural networks (DNNs) have substantially enhanced visual AI's capabilities, the challenge of inadequate data diversity and volume remains, particularly in construction domain. This study presents a novel image synthesis methodology tailored for construction worker detection, leveraging the generative-AI platform Midjourney. The approach entails generating a collection of 12,000 synthetic images by formulating 3000 different prompts, with an emphasis on image realism and diversity. These images, after manual labeling, serve as a dataset for DNN training. Evaluation on a real construction image dataset yielded promising results, with the model attaining average precisions (APs) of 0.937 and 0.642 at intersection-over-union (IoU) thresholds of 0.5 and 0.5 to 0.95, respectively. Notably, the model demonstrated near-perfect performance on the synthetic dataset, achieving APs of 0.994 and 0.919 at the two mentioned thresholds. These findings reveal both the potential and weakness of generative AI in addressing DNN training data scarcity.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation</title>
<link>https://arxiv.org/abs/2507.13229</link>
<guid>https://arxiv.org/abs/2507.13229</guid>
<content:encoded><![CDATA[
arXiv:2507.13229v1 Announce Type: cross 
Abstract: The pursuit of a generalizable stereo matching model, capable of performing across varying resolutions and disparity ranges without dataset-specific fine-tuning, has revealed a fundamental trade-off. Iterative local search methods achieve high scores on constrained benchmarks, but their core mechanism inherently limits the global consistency required for true generalization. On the other hand, global matching architectures, while theoretically more robust, have been historically rendered infeasible by prohibitive computational and memory costs. We resolve this dilemma with $S^2M^2$: a global matching architecture that achieves both state-of-the-art accuracy and high efficiency without relying on cost volume filtering or deep refinement stacks. Our design integrates a multi-resolution transformer for robust long-range correspondence, trained with a novel loss function that concentrates probability on feasible matches. This approach enables a more robust joint estimation of disparity, occlusion, and confidence. $S^2M^2$ establishes a new state of the art on the Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods across most metrics while reconstructing high-quality details with competitive efficiency.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VITA: Vision-to-Action Flow Matching Policy</title>
<link>https://arxiv.org/abs/2507.13231</link>
<guid>https://arxiv.org/abs/2507.13231</guid>
<content:encoded><![CDATA[
arXiv:2507.13231v1 Announce Type: cross 
Abstract: We present VITA, a Vision-To-Action flow matching policy that evolves latent visual representations into latent actions for visuomotor control. Traditional flow matching and diffusion policies sample from standard source distributions (e.g., Gaussian noise) and require additional conditioning mechanisms like cross-attention to condition action generation on visual information, creating time and space overheads. VITA proposes a novel paradigm that treats latent images as the flow source, learning an inherent mapping from vision to action while eliminating separate conditioning modules and preserving generative modeling capabilities. Learning flows between fundamentally different modalities like vision and action is challenging due to sparse action data lacking semantic structures and dimensional mismatches between high-dimensional visual representations and raw actions. We address this by creating a structured action latent space via an autoencoder as the flow matching target, up-sampling raw actions to match visual representation shapes. Crucially, we supervise flow matching with both encoder targets and final action outputs through flow latent decoding, which backpropagates action reconstruction loss through sequential flow matching ODE solving steps for effective end-to-end learning. Implemented as simple MLP layers, VITA is evaluated on challenging bi-manual manipulation tasks on the ALOHA platform, including 5 simulation and 2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or matches state-of-the-art generative policies while reducing inference latency by 50-130% compared to conventional flow matching policies requiring different conditioning mechanisms or complex architectures. To our knowledge, VITA is the first MLP-only flow matching policy capable of solving complex bi-manual manipulation tasks like those in ALOHA benchmarks.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2507.13238</link>
<guid>https://arxiv.org/abs/2507.13238</guid>
<content:encoded><![CDATA[
arXiv:2507.13238v1 Announce Type: cross 
Abstract: Analogies test a model's ability to infer implicit relationships between concepts, making them a key benchmark for evaluating reasoning capabilities. While large language models (LLMs) are widely evaluated for reasoning in English, their abilities in Indic languages remain understudied, limiting our understanding of whether these models generalize across languages. To address this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405 multiple-choice questions sourced from Indian government exams. We benchmark state-of-the-art multilingual LLMs using various prompting strategies and introduce a grounded Chain of Thought approach that leverages cognitive theories of analogical reasoning. This approach improves model performance on Hindi analogy questions. Our experiments show that models perform best with English prompts, irrespective of the prompting strategy. Our test set addresses the lack of a critical resource to evaluate LLM reasoning capabilities in Hindi.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Steering for Safe Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2507.13255</link>
<guid>https://arxiv.org/abs/2507.13255</guid>
<content:encoded><![CDATA[
arXiv:2507.13255v1 Announce Type: cross 
Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy</title>
<link>https://arxiv.org/abs/2507.13260</link>
<guid>https://arxiv.org/abs/2507.13260</guid>
<content:encoded><![CDATA[
arXiv:2507.13260v1 Announce Type: cross 
Abstract: A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViT) involves freezing the majority of the backbone parameters and solely learning low-rank adaptation weight matrices to accommodate downstream tasks. These low-rank matrices are commonly derived through the multiplication structure of down-projection and up-projection matrices, exemplified by methods such as LoRA and Adapter. In this work, we observe an approximate orthogonality among any two row or column vectors within any weight matrix of the backbone parameters; however, this property is absent in the vectors of the down/up-projection matrices. Approximate orthogonality implies a reduction in the upper bound of the model's generalization error, signifying that the model possesses enhanced generalization capability. If the fine-tuned down/up-projection matrices were to exhibit this same property as the pre-trained backbone matrices, could the generalization capability of fine-tuned ViTs be further augmented? To address this question, we propose an Approximately Orthogonal Fine-Tuning (AOFT) strategy for representing the low-rank weight matrices. This strategy employs a single learnable vector to generate a set of approximately orthogonal vectors, which form the down/up-projection matrices, thereby aligning the properties of these matrices with those of the backbone. Extensive experimental results demonstrate that our method achieves competitive performance across a range of downstream image classification tasks, confirming the efficacy of the enhanced generalization capability embedded in the down/up-projection matrices.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merge Kernel for Bayesian Optimization on Permutation Space</title>
<link>https://arxiv.org/abs/2507.13263</link>
<guid>https://arxiv.org/abs/2507.13263</guid>
<content:encoded><![CDATA[
arXiv:2507.13263v1 Announce Type: cross 
Abstract: Bayesian Optimization (BO) algorithm is a standard tool for black-box optimization problems. The current state-of-the-art BO approach for permutation spaces relies on the Mallows kernel-an $\Omega(n^2)$ representation that explicitly enumerates every pairwise comparison. Inspired by the close relationship between the Mallows kernel and pairwise comparison, we propose a novel framework for generating kernel functions on permutation space based on sorting algorithms. Within this framework, the Mallows kernel can be viewed as a special instance derived from bubble sort. Further, we introduce the \textbf{Merge Kernel} constructed from merge sort, which replaces the quadratic complexity with $\Theta(n\log n)$ to achieve the lowest possible complexity. The resulting feature vector is significantly shorter, can be computed in linearithmic time, yet still efficiently captures meaningful permutation distances. To boost robustness and right-invariance without sacrificing compactness, we further incorporate three lightweight, task-agnostic descriptors: (1) a shift histogram, which aggregates absolute element displacements and supplies a global misplacement signal; (2) a split-pair line, which encodes selected long-range comparisons by aligning elements across the two halves of the whole permutation; and (3) sliding-window motifs, which summarize local order patterns that influence near-neighbor objectives. Our empirical evaluation demonstrates that the proposed kernel consistently outperforms the state-of-the-art Mallows kernel across various permutation optimization benchmarks. Results confirm that the Merge Kernel provides a more compact yet more effective solution for Bayesian optimization in permutation space.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voxtral</title>
<link>https://arxiv.org/abs/2507.13264</link>
<guid>https://arxiv.org/abs/2507.13264</guid>
<content:encoded><![CDATA[
arXiv:2507.13264v1 Announce Type: cross 
Abstract: We present Voxtral Mini and Voxtral Small, two multimodal audio chat models. Voxtral is trained to comprehend both spoken audio and text documents, achieving state-of-the-art performance across a diverse range of audio benchmarks, while preserving strong text capabilities. Voxtral Small outperforms a number of closed-source models, while being small enough to run locally. A 32K context window enables the model to handle audio files up to 40 minutes in duration and long multi-turn conversations. We also contribute three benchmarks for evaluating speech understanding models on knowledge and trivia. Both Voxtral models are released under Apache 2.0 license.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation</title>
<link>https://arxiv.org/abs/2507.13266</link>
<guid>https://arxiv.org/abs/2507.13266</guid>
<content:encoded><![CDATA[
arXiv:2507.13266v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has become a key component in training large language reasoning models (LLMs). However, recent studies questions its effectiveness in improving multi-step reasoning-particularly on hard problems. To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%) on AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical explanations that QuestA improves sample efficiency, offering a practical and generalizable pathway for expanding reasoning capability through RL.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management</title>
<link>https://arxiv.org/abs/2507.13275</link>
<guid>https://arxiv.org/abs/2507.13275</guid>
<content:encoded><![CDATA[
arXiv:2507.13275v1 Announce Type: cross 
Abstract: Advances in natural language processing and large language models are driving a major transformation in Human Capital Management, with a growing interest in building smart systems based on language technologies for talent acquisition, upskilling strategies, and workforce planning. However, the adoption and progress of these technologies critically depend on the development of reliable and fair models, properly evaluated on public data and open benchmarks, which have so far been unavailable in this domain.
  To address this gap, we present TalentCLEF 2025, the first evaluation campaign focused on skill and job title intelligence. The lab consists of two tasks: Task A - Multilingual Job Title Matching, covering English, Spanish, German, and Chinese; and Task B - Job Title-Based Skill Prediction, in English. Both corpora were built from real job applications, carefully anonymized, and manually annotated to reflect the complexity and diversity of real-world labor market data, including linguistic variability and gender-marked expressions.
  The evaluations included monolingual and cross-lingual scenarios and covered the evaluation of gender bias.
  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most systems relied on information retrieval techniques built with multilingual encoder-based models fine-tuned with contrastive learning, and several of them incorporated large language models for data augmentation or re-ranking. The results show that the training strategies have a larger effect than the size of the model alone. TalentCLEF provides the first public benchmark in this field and encourages the development of robust, fair, and transferable language technologies for the labor market.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour</title>
<link>https://arxiv.org/abs/2507.13277</link>
<guid>https://arxiv.org/abs/2507.13277</guid>
<content:encoded><![CDATA[
arXiv:2507.13277v1 Announce Type: cross 
Abstract: Robots are increasingly integrated across industries, particularly in healthcare. However, many valuable applications for quadrupedal robots remain overlooked. This research explores the effectiveness of three reinforcement learning algorithms in training a simulated quadruped robot for autonomous navigation and obstacle avoidance. The goal is to develop a robotic guide dog simulation capable of path following and obstacle avoidance, with long-term potential for real-world assistance to guide dogs and visually impaired individuals. It also seeks to expand research into medical 'pets', including robotic guide and alert dogs.
  A comparative analysis of thirteen related research papers shaped key evaluation criteria, including collision detection, pathfinding algorithms, sensor usage, robot type, and simulation platforms. The study focuses on sensor inputs, collision frequency, reward signals, and learning progression to determine which algorithm best supports robotic navigation in complex environments.
  Custom-made environments were used to ensure fair evaluation of all three algorithms under controlled conditions, allowing consistent data collection. Results show that Proximal Policy Optimization (PPO) outperformed Deep Q-Network (DQN) and Q-learning across all metrics, particularly in average and median steps to goal per episode.
  By analysing these results, this study contributes to robotic navigation, AI and medical robotics, offering insights into the feasibility of AI-driven quadruped mobility and its role in assistive robotics.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Formal Verification of LLM-Generated Code from Natural Language Prompts</title>
<link>https://arxiv.org/abs/2507.13290</link>
<guid>https://arxiv.org/abs/2507.13290</guid>
<content:encoded><![CDATA[
arXiv:2507.13290v1 Announce Type: cross 
Abstract: In the past few years LLMs have emerged as a tool that can aid programmers by taking natural language descriptions and generating code based on it. However, LLMs often generate incorrect code that users need to fix and the literature suggests users often struggle to detect these errors. In this work we seek to offer formal guarantees of correctness to LLM generated code; such guarantees could improve the experience of using AI Code Assistants and potentially enable natural language programming for users with little or no programming knowledge. To address this challenge we propose to incorporate a formal query language that can represent a user's intent in a formally defined but natural language-like manner that a user can confirm matches their intent. Then, using such a query we propose to verify LLM generated code to ensure it matches the user's intent. We implement these ideas in our system, Astrogator, for the Ansible programming language which includes such a formal query language, a calculus for representing the behavior of Ansible programs, and a symbolic interpreter which is used for the verification. On a benchmark suite of 21 code-generation tasks, our verifier is able to verify correct code in 83% of cases and identify incorrect code in 92%.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research</title>
<link>https://arxiv.org/abs/2507.13300</link>
<guid>https://arxiv.org/abs/2507.13300</guid>
<content:encoded><![CDATA[
arXiv:2507.13300v1 Announce Type: cross 
Abstract: We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark</title>
<link>https://arxiv.org/abs/2507.13314</link>
<guid>https://arxiv.org/abs/2507.13314</guid>
<content:encoded><![CDATA[
arXiv:2507.13314v1 Announce Type: cross 
Abstract: The reasoning-based pose estimation (RPE) benchmark has emerged as a widely adopted evaluation standard for pose-aware multimodal large language models (MLLMs). Despite its significance, we identified critical reproducibility and benchmark-quality issues that hinder fair and consistent quantitative evaluations. Most notably, the benchmark utilizes different image indices from those of the original 3DPW dataset, forcing researchers into tedious and error-prone manual matching processes to obtain accurate ground-truth (GT) annotations for quantitative metrics (\eg, MPJPE, PA-MPJPE). Furthermore, our analysis reveals several inherent benchmark-quality limitations, including significant image redundancy, scenario imbalance, overly simplistic poses, and ambiguous textual descriptions, collectively undermining reliable evaluations across diverse scenarios. To alleviate manual effort and enhance reproducibility, we carefully refined the GT annotations through meticulous visual matching and publicly release these refined annotations as an open-source resource, thereby promoting consistent quantitative evaluations and facilitating future advancements in human pose-aware multimodal reasoning.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It</title>
<link>https://arxiv.org/abs/2507.13328</link>
<guid>https://arxiv.org/abs/2507.13328</guid>
<content:encoded><![CDATA[
arXiv:2507.13328v1 Announce Type: cross 
Abstract: Does vision-and-language (VL) training change the linguistic representations of language models in meaningful ways? Most results in the literature have shown inconsistent or marginal differences, both behaviorally and representationally. In this work, we start from the hypothesis that the domain in which VL training could have a significant effect is lexical-conceptual knowledge, in particular its taxonomic organization. Through comparing minimal pairs of text-only LMs and their VL-trained counterparts, we first show that the VL models often outperform their text-only counterparts on a text-only question-answering task that requires taxonomic understanding of concepts mentioned in the questions. Using an array of targeted behavioral and representational analyses, we show that the LMs and VLMs do not differ significantly in terms of their taxonomic knowledge itself, but they differ in how they represent questions that contain concepts in a taxonomic relation vs. a non-taxonomic relation. This implies that the taxonomic knowledge itself does not change substantially through additional VL training, but VL training does improve the deployment of this knowledge in the context of a specific task, even when the presentation of the task is purely linguistic.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Policy Steering with Embodiment-Agnostic Pretrained World Models</title>
<link>https://arxiv.org/abs/2507.13340</link>
<guid>https://arxiv.org/abs/2507.13340</guid>
<content:encoded><![CDATA[
arXiv:2507.13340v1 Announce Type: cross 
Abstract: Learning visuomotor policies via imitation has proven effective across a wide range of robotic domains. However, the performance of these policies is heavily dependent on the number of training demonstrations, which requires expensive data collection in the real world. In this work, we aim to reduce data collection efforts when learning visuomotor robot policies by leveraging existing or cost-effective data from a wide range of embodiments, such as public robot datasets and the datasets of humans playing with objects (human data from play). Our approach leverages two key insights. First, we use optic flow as an embodiment-agnostic action representation to train a World Model (WM) across multi-embodiment datasets, and finetune it on a small amount of robot data from the target embodiment. Second, we develop a method, Latent Policy Steering (LPS), to improve the output of a behavior-cloned policy by searching in the latent space of the WM for better action sequences. In real world experiments, we observe significant improvements in the performance of policies trained with a small amount of data (over 50% relative improvement with 30 demonstrations and over 20% relative improvement with 50 demonstrations) by combining the policy with a WM pretrained on two thousand episodes sampled from the existing Open X-embodiment dataset across different robots or a cost-effective human dataset from play.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imbalance in Balance: Online Concept Balancing in Generation Models</title>
<link>https://arxiv.org/abs/2507.13345</link>
<guid>https://arxiv.org/abs/2507.13345</guid>
<content:encoded><![CDATA[
arXiv:2507.13345v1 Announce Type: cross 
Abstract: In visual generation tasks, the responses and combinations of complex concepts often lack stability and are error-prone, which remains an under-explored area. In this paper, we attempt to explore the causal factors for poor concept responses through elaborately designed experiments. We also design a concept-wise equalization loss function (IMBA loss) to address this issue. Our proposed method is online, eliminating the need for offline dataset processing, and requires minimal code changes. In our newly proposed complex concept benchmark Inert-CompBench and two other public test sets, our method significantly enhances the concept response capability of baseline models and yields highly competitive results with only a few codes.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.13348</link>
<guid>https://arxiv.org/abs/2507.13348</guid>
<content:encoded><![CDATA[
arXiv:2507.13348v1 Announce Type: cross 
Abstract: Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding</title>
<link>https://arxiv.org/abs/2507.13353</link>
<guid>https://arxiv.org/abs/2507.13353</guid>
<content:encoded><![CDATA[
arXiv:2507.13353v1 Announce Type: cross 
Abstract: Recent studies have revealed that selecting informative and relevant video frames can significantly improve the performance of Video Large Language Models (Video-LLMs). Current methods, such as reducing inter-frame redundancy, employing separate models for image-text relevance assessment, or utilizing temporal video grounding for event localization, substantially adopt unsupervised learning paradigms, whereas they struggle to address the complex scenarios in long video understanding. We propose Instructed Temporal Grounding for Videos (VideoITG), featuring customized frame sampling aligned with user instructions. The core of VideoITG is the VidThinker pipeline, an automated annotation framework that explicitly mimics the human annotation process. First, it generates detailed clip-level captions conditioned on the instruction; then, it retrieves relevant video segments through instruction-guided reasoning; finally, it performs fine-grained frame selection to pinpoint the most informative visual evidence. Leveraging VidThinker, we construct the VideoITG-40K dataset, containing 40K videos and 500K instructed temporal grounding annotations. We then design a plug-and-play VideoITG model, which takes advantage of visual language alignment and reasoning capabilities of Video-LLMs, for effective frame selection in a discriminative manner. Coupled with Video-LLMs, VideoITG achieves consistent performance improvements across multiple multimodal video understanding benchmarks, showing its superiority and great potentials for video understanding.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations</title>
<link>https://arxiv.org/abs/2402.09617</link>
<guid>https://arxiv.org/abs/2402.09617</guid>
<content:encoded><![CDATA[
arXiv:2402.09617v2 Announce Type: replace 
Abstract: Graph recommendation methods, representing a connected interaction perspective, reformulate user-item interactions as graphs to leverage graph structure and topology to recommend and have proved practical effectiveness at scale. Large language models, representing a textual generative perspective, excel at modeling user languages, understanding behavioral contexts, capturing user-item semantic relationships, analyzing textual sentiments, and generating coherent and contextually relevant texts as recommendations. However, there is a gap between the connected graph perspective and the text generation perspective as the task formulations are different. A research question arises: how can we effectively integrate the two perspectives for more personalized recsys? To fill this gap, we propose to incorporate graph-edge information into LLMs via prompt and attention innovations. We reformulate recommendations as a probabilistic generative problem using prompts. We develop a framework to incorporate graph edge information from the prompt and attention mechanisms for graph-structured LLM recommendations. We develop a new prompt design that brings in both first-order and second-order graph relationships; we devise an improved LLM attention mechanism to embed direct the spatial and connectivity information of edges. Our evaluation of real-world datasets demonstrates the framework's ability to understand connectivity information in graph data and to improve the relevance and quality of recommendation results.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying MuZero Planning: Interpreting the Learned Model</title>
<link>https://arxiv.org/abs/2411.04580</link>
<guid>https://arxiv.org/abs/2411.04580</guid>
<content:encoded><![CDATA[
arXiv:2411.04580v2 Announce Type: replace 
Abstract: MuZero has achieved superhuman performance in various games by using a dynamics network to predict the environment dynamics for planning, without relying on simulators. However, the latent states learned by the dynamics network make its planning process opaque. This paper aims to demystify MuZero's model by interpreting the learned latent states. We incorporate observation reconstruction and state consistency into MuZero training and conduct an in-depth analysis to evaluate latent states across two board games: 9x9 Go and Gomoku, and three Atari games: Breakout, Ms. Pacman, and Pong. Our findings reveal that while the dynamics network becomes less accurate over longer simulations, MuZero still performs effectively by using planning to correct errors. Our experiments also show that the dynamics network learns better latent states in board games than in Atari games. These insights contribute to a better understanding of MuZero and offer directions for future research to improve the performance, robustness, and interpretability of the MuZero algorithm. The code and data are available at https://rlg.iis.sinica.edu.tw/papers/demystifying-muzero-planning.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEARCUBS: A benchmark for computer-using web agents</title>
<link>https://arxiv.org/abs/2503.07919</link>
<guid>https://arxiv.org/abs/2503.07919</guid>
<content:encoded><![CDATA[
arXiv:2503.07919v2 Announce Type: replace 
Abstract: Modern web agents possess computer use abilities that allow them to interact with webpages by sending commands to a virtual keyboard and mouse. While such agents have considerable potential to assist human users with complex tasks, evaluating their capabilities in real-world settings poses a major challenge. To this end, we introduce BEARCUBS, a "small but mighty" benchmark of 111 information-seeking questions designed to evaluate a web agent's ability to search, browse, and identify factual information from the web. Unlike prior web agent benchmarks, solving BEARCUBS requires (1) accessing live web content rather than synthetic or simulated pages, which captures the unpredictability of real-world web interactions; and (2) performing a broad range of multimodal interactions (e.g., video understanding, 3D navigation) that cannot be bypassed via text-based workarounds. Each question in BEARCUBS has a corresponding short, unambiguous answer and a human-validated browsing trajectory, allowing for transparent evaluation of agent performance and strategies. A human study confirms that BEARCUBS questions are solvable but non-trivial (84.7% human accuracy), revealing domain knowledge gaps and overlooked details as common failure points. By contrast, state-of-the-art computer-using agents underperform, with the best-scoring system (OpenAI's Operator) reaching only 23.4% accuracy. These results highlight critical areas for improvement, including reliable source selection and more powerful multimodal capabilities. To facilitate future research, BEARCUBS will be updated periodically to replace invalid or contaminated questions, keeping the benchmark fresh for future generations of web agents.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ActionStudio: A Lightweight Framework for Data and Training of Large Action Models</title>
<link>https://arxiv.org/abs/2503.22673</link>
<guid>https://arxiv.org/abs/2503.22673</guid>
<content:encoded><![CDATA[
arXiv:2503.22673v3 Announce Type: replace 
Abstract: Large Action models are essential for enabling autonomous agents to perform complex tasks. However, training such models remains challenging due to the diversity of agent environments and the complexity of noisy agentic data. Existing infrastructure offers limited support for scalable, agent-specific fine-tuning and standardized agent data processing. We introduce ActionStudio, a lightweight and extensible data and training framework designed for large action models. ActionStudio unifies diverse agent trajectories using our proposed Unified Format 2.0, supports a range of training workflows with optimized multi-node distributed setup, and integrates robust preprocessing and real-time verification tools. ActionStudio demonstrates up to 9x higher throughput compared to existing agentic training frameworks, and our trained models yield top performances across public and realistic agent benchmarks. To support the broader research community, we open-source the ActionStudio framework and release actionstudio-98k, a curated dataset of 98k high-quality trajectories. Code: https://github.com/SalesforceAIResearch/xLAM.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging with Many Minds: Do More Perspectives Mean Less Prejudice? On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge</title>
<link>https://arxiv.org/abs/2505.19477</link>
<guid>https://arxiv.org/abs/2505.19477</guid>
<content:encoded><![CDATA[
arXiv:2505.19477v2 Announce Type: replace 
Abstract: LLM-as-Judge has emerged as a scalable alternative to human evaluation, enabling large language models (LLMs) to provide reward signals in trainings. While recent work has explored multi-agent extensions such as multi-agent debate and meta-judging to enhance evaluation quality, the question of how intrinsic biases manifest in these settings remains underexplored. In this study, we conduct a systematic analysis of four diverse bias types: position bias, verbosity bias, chain-of-thought bias, and bandwagon bias. We evaluate these biases across two widely adopted multi-agent LLM-as-Judge frameworks: Multi-Agent-Debate and LLM-as-Meta-Judge. Our results show that debate framework amplifies biases sharply after the initial debate, and this increased bias is sustained in subsequent rounds, while meta-judge approaches exhibit greater resistance. We further investigate the incorporation of PINE, a leading single-agent debiasing method, as a bias-free agent within these systems. The results reveal that this bias-free agent effectively reduces biases in debate settings but provides less benefit in meta-judge scenarios. Our work provides a comprehensive study of bias behavior in multi-agent LLM-as-Judge systems and highlights the need for targeted bias mitigation strategies in collaborative evaluation settings.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NTRL: Encounter Generation via Reinforcement Learning for Dynamic Difficulty Adjustment in Dungeons and Dragons</title>
<link>https://arxiv.org/abs/2506.19530</link>
<guid>https://arxiv.org/abs/2506.19530</guid>
<content:encoded><![CDATA[
arXiv:2506.19530v2 Announce Type: replace 
Abstract: Balancing combat encounters in Dungeons & Dragons (D&amp;D) is a complex task that requires Dungeon Masters (DM) to manually assess party strength, enemy composition, and dynamic player interactions while avoiding interruption of the narrative flow. In this paper, we propose Encounter Generation via Reinforcement Learning (NTRL), a novel approach that automates Dynamic Difficulty Adjustment (DDA) in D&amp;D via combat encounter design. By framing the problem as a contextual bandit, NTRL generates encounters based on real-time party members attributes. In comparison with classic DM heuristics, NTRL iteratively optimizes encounters to extend combat longevity (+200%), increases damage dealt to party members, reducing post-combat hit points (-16.67%), and raises the number of player deaths while maintaining low total party kills (TPK). The intensification of combat forces players to act wisely and engage in tactical maneuvers, even though the generated encounters guarantee high win rates (70%). Even in comparison with encounters designed by human Dungeon Masters, NTRL demonstrates superior performance by enhancing the strategic depth of combat while increasing difficulty in a manner that preserves overall game fairness.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments</title>
<link>https://arxiv.org/abs/2507.04037</link>
<guid>https://arxiv.org/abs/2507.04037</guid>
<content:encoded><![CDATA[
arXiv:2507.04037v2 Announce Type: replace 
Abstract: The gap between static benchmarks and the dynamic nature of real-world legal practice poses a key barrier to advancing legal intelligence. To this end, we introduce J1-ENVS, the first interactive and dynamic legal environment tailored for LLM-based agents. Guided by legal experts, it comprises six representative scenarios from Chinese legal practices across three levels of environmental complexity. We further introduce J1-EVAL, a fine-grained evaluation framework, designed to assess both task performance and procedural compliance across varying levels of legal proficiency. Extensive experiments on 17 LLM agents reveal that, while many models demonstrate solid legal knowledge, they struggle with procedural execution in dynamic settings. Even the SOTA model, GPT-4o, falls short of 60% overall performance. These findings highlight persistent challenges in achieving dynamic legal intelligence and offer valuable insights to guide future research.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control</title>
<link>https://arxiv.org/abs/2507.04348</link>
<guid>https://arxiv.org/abs/2507.04348</guid>
<content:encoded><![CDATA[
arXiv:2507.04348v2 Announce Type: replace 
Abstract: Large reasoning models (LRMs) have exhibited remarkable reasoning capabilities through inference-time scaling, but this progress has also introduced considerable redundancy and inefficiency into their reasoning processes, resulting in substantial computational waste. Previous work has attempted to mitigate this issue by penalizing the overall length of generated samples during reinforcement learning (RL), with the goal of encouraging a more concise chains of thought. However, we observe that such global length penalty often lead to excessive compression of critical reasoning steps while preserving unnecessary details in simpler ones, yielding a suboptimal trade-off between accuracy and efficiency. To address this issue, we propose SmartThinker, a two-stage learnable framework designed to enable fine-grained control over the length of reasoning chains based on the importance of each individual step. In the first stage, SmartThinker adapts a reasoning model to a short-form reasoning mode through rejection sampling combined with supervised fine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length Control Policy Optimization (SCPO) to refine the model output distribution, which increases the proportion of length allocated to critical steps while reducing redundancy in less important ones. SCPO consists of four core components: an online importance estimator, a step-level length control reward function, a step-level generalized advantage estimation (S-GAE) and a difficulty-adaptive clipping strategy. Working in concert, these components enable SCPO to implement differentiated length control across reasoning steps. Empirical results across multiple reasoning benchmarks and various backbone models demonstrate that SmartThinker significantly reduces redundant reasoning while achieving comparable or even superior performance to existing methods.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Systems: A Survey from a Data-Oriented Perspective</title>
<link>https://arxiv.org/abs/2302.04810</link>
<guid>https://arxiv.org/abs/2302.04810</guid>
<content:encoded><![CDATA[
arXiv:2302.04810v3 Announce Type: replace-cross 
Abstract: Engineers are deploying ML models as parts of real-world systems with the upsurge of AI technologies. Real-world environments challenge the deployment of such systems because these environments produce large amounts of heterogeneous data, and users require increasingly efficient responses. These requirements push prevalent software architectures to the limit when deploying ML-based systems. Data-oriented Architecture (DOA) is an emerging style that equips systems better for integrating ML models. Even though papers on deployed ML systems do not mention DOA, their authors made design decisions that implicitly follow DOA. Implicit decisions create a knowledge gap, limiting the practitioners' ability to implement ML-based systems. \hlb{This paper surveys why, how, and to what extent practitioners have adopted DOA to implement and deploy ML-based systems.} We overcome the knowledge gap by answering these questions and explicitly showing the design decisions and practices behind these systems. The survey follows a well-known systematic and semi-automated methodology for reviewing papers in software engineering. The majority of reviewed works partially adopt DOA. Such an adoption enables systems to address requirements such as Big Data management, low latency processing, resource management, security and privacy. Based on these findings, we formulate practical advice to facilitate the deployment of ML-based systems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TBDetector:Transformer-Based Detector for Advanced Persistent Threats with Provenance Graph</title>
<link>https://arxiv.org/abs/2304.02838</link>
<guid>https://arxiv.org/abs/2304.02838</guid>
<content:encoded><![CDATA[
arXiv:2304.02838v2 Announce Type: replace-cross 
Abstract: APT detection is difficult to detect due to the long-term latency, covert and slow multistage attack patterns of Advanced Persistent Threat (APT). To tackle these issues, we propose TBDetector, a transformer-based advanced persistent threat detection method for APT attack detection. Considering that provenance graphs provide rich historical information and have the powerful attacks historic correlation ability to identify anomalous activities, TBDetector employs provenance analysis for APT detection, which summarizes long-running system execution with space efficiency and utilizes transformer with self-attention based encoder-decoder to extract long-term contextual features of system states to detect slow-acting attacks. Furthermore, we further introduce anomaly scores to investigate the anomaly of different system states, where each state is calculated with an anomaly score corresponding to its similarity score and isolation score. To evaluate the effectiveness of the proposed method, we have conducted experiments on five public datasets, i.e., streamspot, cadets, shellshock, clearscope, and wget_baseline. Experimental results and comparisons with state-of-the-art methods have exhibited better performance of our proposed method.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Quantum Superposition to Infer the Dynamic Behavior of a Spatial-Temporal Neural Network Signaling Model</title>
<link>https://arxiv.org/abs/2403.18963</link>
<guid>https://arxiv.org/abs/2403.18963</guid>
<content:encoded><![CDATA[
arXiv:2403.18963v4 Announce Type: replace-cross 
Abstract: The exploration of new problem classes for quantum computation is an active area of research. In this paper, we introduce and solve a novel problem class related to dynamics on large-scale networks relevant to neurobiology and machine learning. Specifically, we ask if a network can sustain inherent dynamic activity beyond some arbitrary observation time or if the activity ceases through quiescence or saturation via an epileptic-like state. We show that this class of problems can be formulated and structured to take advantage of quantum superposition and solved efficiently using a coupled workflow between the Grover and Deutsch-Jozsa quantum algorithms. To do so, we extend their functionality to address the unique requirements of how input (sub)sets into the algorithms must be mathematically structured while simultaneously constructing the inputs so that measurement outputs can be interpreted as meaningful properties of the network dynamics. This, in turn, allows us to answer the question we pose.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced AI Applications with Retrieval Augmented Generation and Knowledge Graphs</title>
<link>https://arxiv.org/abs/2407.02994</link>
<guid>https://arxiv.org/abs/2407.02994</guid>
<content:encoded><![CDATA[
arXiv:2407.02994v5 Announce Type: replace-cross 
Abstract: The increasing interest in developing Artificial Intelligence applications in the medical domain, suffers from the lack of high-quality data set, mainly due to privacy-related issues. In addition, the recent increase in Vision Language Models (VLM) leads to the need for multimodal medical data sets, where clinical reports and findings are attached to the corresponding medical scans. This paper illustrates the entire workflow for building the MedPix 2.0 data set. Starting with the well-known multimodal data set MedPix\textsuperscript{\textregistered}, mainly used by physicians, nurses, and healthcare students for Continuing Medical Education purposes, a semi-automatic pipeline was developed to extract visual and textual data followed by a manual curing procedure in which noisy samples were removed, thus creating a MongoDB database. Along with the data set, we developed a Graphical User Interface aimed at navigating efficiently the MongoDB instance and obtaining the raw data that can be easily used for training and/or fine-tuning VLMs. To enforce this point, in this work, we first recall DR-Minerva, a Retrieve Augmented Generation-based VLM model trained upon MedPix 2.0. DR-Minerva predicts the body part and the modality used to scan its input image. We also propose the extension of DR-Minerva with a Knowledge Graph that uses Llama 3.1 Instruct 8B, and leverages MedPix 2.0. The resulting architecture can be queried in a end-to-end manner, as a medical decision support system. MedPix 2.0 is available on GitHub.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risks of ignoring uncertainty propagation in AI-augmented security pipelines</title>
<link>https://arxiv.org/abs/2407.14540</link>
<guid>https://arxiv.org/abs/2407.14540</guid>
<content:encoded><![CDATA[
arXiv:2407.14540v2 Announce Type: replace-cross 
Abstract: The use of AI technologies is being integrated into the secure development of software-based systems, with an increasing trend of composing AI-based subsystems (with uncertain levels of performance) into automated pipelines. This presents a fundamental research challenge and seriously threatens safety-critical domains. Despite the existing knowledge about uncertainty in risk analysis, no previous work has estimated the uncertainty of AI-augmented systems given the propagation of errors in the pipeline. We provide the formal underpinnings for capturing uncertainty propagation, develop a simulator to quantify uncertainty, and evaluate the simulation of propagating errors with one case study. We discuss the generalizability of our approach and its limitations and present recommendations for evaluation policies concerning AI systems. Future work includes extending the approach by relaxing the remaining assumptions and by experimenting with a real system.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Sub-Genre Classification For Mainstage Dance Music</title>
<link>https://arxiv.org/abs/2409.06690</link>
<guid>https://arxiv.org/abs/2409.06690</guid>
<content:encoded><![CDATA[
arXiv:2409.06690v2 Announce Type: replace-cross 
Abstract: Music classification, a cornerstone of music information retrieval, supports a wide array of applications. To address the lack of comprehensive datasets and effective methods for sub-genre classification in mainstage dance music, we introduce a novel benchmark featuring a new dataset and baseline. Our dataset expands the scope of sub-genres to reflect the diversity of recent mainstage live sets performed by leading DJs at global music festivals, capturing the vibrant and rapidly evolving electronic dance music (EDM) scene that engages millions of fans worldwide. We employ a continuous soft labeling approach to accommodate tracks blending multiple sub-genres, preserving their inherent complexity. Experiments demonstrate that even state-of-the-art multimodal large language models (MLLMs) struggle with this task, while our specialized baseline models achieve high accuracy. This benchmark supports applications such as music recommendation, DJ set curation, and interactive multimedia systems, with video demos provided. Our code and data are all open-sourced at https://github.com/Gariscat/housex-v2.git}{https://github.com/Gariscat/housex-v2.git.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeFine: Decision-Making with Analogical Reasoning over Factor Profiles</title>
<link>https://arxiv.org/abs/2410.01772</link>
<guid>https://arxiv.org/abs/2410.01772</guid>
<content:encoded><![CDATA[
arXiv:2410.01772v2 Announce Type: replace-cross 
Abstract: LLMs are ideal for decision-making thanks to their ability to reason over long contexts. However, challenges arise when processing speech transcripts that describe complex scenarios, as they are verbose and include repetition, hedging, and vagueness. E.g., during a company's earnings call, an executive might project a positive revenue outlook to reassure investors, despite uncertainty regarding future earnings. It is crucial for LLMs to incorporate this uncertainty systematically when making decisions. In this paper, we introduce \textsc{DeFine}, a modular framework that constructs probabilistic factor profiles from complex scenarios. It then integrates these profiles with analogical reasoning, leveraging insights from similar past experiences to guide LLMs in making critical decisions in new situations. Our framework separates the tasks of quantifying uncertainty and incorporating it into LLM decision-making. This approach is particularly useful in areas such as consulting and financial deliberation, where making decisions under uncertainty is vital.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information</title>
<link>https://arxiv.org/abs/2410.12774</link>
<guid>https://arxiv.org/abs/2410.12774</guid>
<content:encoded><![CDATA[
arXiv:2410.12774v2 Announce Type: replace-cross 
Abstract: The success of multi-task learning can depend heavily on which tasks are grouped together. Naively grouping all tasks or a random set of tasks can result in negative transfer, with the multi-task models performing worse than single-task models. Though many efforts have been made to identify task groupings and to measure the relatedness among different tasks, it remains a challenging research topic to define a metric to identify the best task grouping out of a pool of many potential task combinations. We propose a metric of task relatedness based on task difficulty measured by pointwise V-usable information (PVI). PVI is a recently proposed metric to estimate how much usable information a dataset contains given a model. We hypothesize that tasks with not statistically different PVI estimates are similar enough to benefit from the joint learning process. We conduct comprehensive experiments to evaluate the feasibility of this metric for task grouping on 15 NLP datasets in the general, biomedical, and clinical domains. We compare the results of the joint learners against single learners, existing baseline methods, and recent large language models, including Llama 2 and GPT-4. The results show that by grouping tasks with similar PVI estimates, the joint learners yielded competitive results with fewer total parameters, with consistent performance across domains.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization</title>
<link>https://arxiv.org/abs/2410.20625</link>
<guid>https://arxiv.org/abs/2410.20625</guid>
<content:encoded><![CDATA[
arXiv:2410.20625v2 Announce Type: replace-cross 
Abstract: Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the actual updates to the weights depends on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6\% accuracy gain on Super-Natural Instructions and 3.5\% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2410.23114</link>
<guid>https://arxiv.org/abs/2410.23114</guid>
<content:encoded><![CDATA[
arXiv:2410.23114v4 Announce Type: replace-cross 
Abstract: Despite the outstanding performance in vision-language reasoning, Large Vision-Language Models (LVLMs) might generate hallucinated contents that do not exist in the given image. Most existing LVLM hallucination benchmarks are constrained to evaluate the object-related hallucinations. However, the potential hallucination on the relations between two objects, i.e., relation hallucination, still lacks investigation. To remedy that, we design a unified framework to measure the object and relation hallucination in LVLMs simultaneously. The core idea of our framework is to evaluate hallucinations via (object, relation, object) triplets extracted from LVLMs' responses, making it easily generalizable to different vision-language tasks. Based on our framework, we further introduce Tri-HE, a novel Triplet-level Hallucination Evaluation benchmark which can be used to study both object and relation hallucination at the same time. With comprehensive evaluations on Tri-HE, we observe that the relation hallucination issue is even more serious than object hallucination among existing LVLMs, highlighting a previously neglected problem towards reliable LVLMs. Moreover, based on our findings, we design a simple training-free approach that effectively mitigates hallucinations for LVLMs. Our dataset and code for the reproduction of our experiments are available publicly at https://github.com/wujunjie1998/Tri-HE.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dataset resulting from the user study on comprehensibility of explainable AI algorithms</title>
<link>https://arxiv.org/abs/2411.02419</link>
<guid>https://arxiv.org/abs/2411.02419</guid>
<content:encoded><![CDATA[
arXiv:2411.02419v2 Announce Type: replace-cross 
Abstract: This paper introduces a dataset that is the result of a user study on the comprehensibility of explainable artificial intelligence (XAI) algorithms. The study participants were recruited from 149 candidates to form three groups representing experts in the domain of mycology (DE), students with a data science and visualization background (IT) and students from social sciences and humanities (SSH). The main part of the dataset contains 39 transcripts of interviews during which participants were asked to complete a series of tasks and questions related to the interpretation of explanations of decisions of a machine learning model trained to distinguish between edible and inedible mushrooms. The transcripts were complemented with additional data that includes visualizations of explanations presented to the user, results from thematic analysis, recommendations of improvements of explanations provided by the participants, and the initial survey results that allow to determine the domain knowledge of the participant and data analysis literacy. The transcripts were manually tagged to allow for automatic matching between the text and other data related to particular fragments. In the advent of the area of rapid development of XAI techniques, the need for a multidisciplinary qualitative evaluation of explainability is one of the emerging topics in the community. Our dataset allows not only to reproduce the study we conducted, but also to open a wide range of possibilities for the analysis of the material we gathered.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy for Visuomotor Imitation Learning</title>
<link>https://arxiv.org/abs/2411.03294</link>
<guid>https://arxiv.org/abs/2411.03294</guid>
<content:encoded><![CDATA[
arXiv:2411.03294v4 Announce Type: replace-cross 
Abstract: We propose an object-centric recovery (OCR) framework to address the challenges of out-of-distribution (OOD) scenarios in visuomotor policy learning. Previous behavior cloning (BC) methods rely heavily on a large amount of labeled data coverage, failing in unfamiliar spatial states. Without relying on extra data collection, our approach learns a recovery policy constructed by an inverse policy inferred from the object keypoint manifold gradient in the original training data. The recovery policy serves as a simple add-on to any base visuomotor BC policy, agnostic to a specific method, guiding the system back towards the training distribution to ensure task success even in OOD situations. We demonstrate the effectiveness of our object-centric framework in both simulation and real robot experiments, achieving an improvement of 77.7\% over the base policy in OOD. Furthermore, we show OCR's capacity to autonomously collect demonstrations for continual learning. Overall, we believe this framework represents a step toward improving the robustness of visuomotor policies in real-world settings.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization</title>
<link>https://arxiv.org/abs/2411.06208</link>
<guid>https://arxiv.org/abs/2411.06208</guid>
<content:encoded><![CDATA[
arXiv:2411.06208v3 Announce Type: replace-cross 
Abstract: In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRGen: Segmentation Data Engine for Underrepresented MRI Modalities</title>
<link>https://arxiv.org/abs/2412.04106</link>
<guid>https://arxiv.org/abs/2412.04106</guid>
<content:encoded><![CDATA[
arXiv:2412.04106v3 Announce Type: replace-cross 
Abstract: Training medical image segmentation models for rare yet clinically important imaging modalities is challenging due to the scarcity of annotated data, and manual mask annotations can be costly and labor-intensive to acquire. This paper investigates leveraging generative models to synthesize data, for training segmentation models for underrepresented modalities, particularly on annotation-scarce MRI. Concretely, our contributions are threefold: (i) we introduce MRGen-DB, a large-scale radiology image-text dataset comprising extensive samples with rich metadata, including modality labels, attributes, regions, and organs information, with a subset featuring pixel-wise mask annotations; (ii) we present MRGen, a diffusion-based data engine for controllable medical image synthesis, conditioned on text prompts and segmentation masks. MRGen can generate realistic images for diverse MRI modalities lacking mask annotations, facilitating segmentation training in low-source domains; (iii) extensive experiments across multiple modalities demonstrate that MRGen significantly improves segmentation performance on unannotated modalities by providing high-quality synthetic data. We believe that our method bridges a critical gap in medical image analysis, extending segmentation capabilities to scenarios that are challenging to acquire manual annotations. The codes, models, and data will be publicly available at https://haoningwu3639.github.io/MRGen/
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech-Forensics: Towards Comprehensive Synthetic Speech Dataset Establishment and Analysis</title>
<link>https://arxiv.org/abs/2412.09032</link>
<guid>https://arxiv.org/abs/2412.09032</guid>
<content:encoded><![CDATA[
arXiv:2412.09032v3 Announce Type: replace-cross 
Abstract: Detecting synthetic from real speech is increasingly crucial due to the risks of misinformation and identity impersonation. While various datasets for synthetic speech analysis have been developed, they often focus on specific areas, limiting their utility for comprehensive research. To fill this gap, we propose the Speech-Forensics dataset by extensively covering authentic, synthetic, and partially forged speech samples that include multiple segments synthesized by different high-quality algorithms. Moreover, we propose a TEmporal Speech LocalizaTion network, called TEST, aiming at simultaneously performing authenticity detection, multiple fake segments localization, and synthesis algorithms recognition, without any complex post-processing. TEST effectively integrates LSTM and Transformer to extract more powerful temporal speech representations and utilizes dense prediction on multi-scale pyramid features to estimate the synthetic spans. Our model achieves an average mAP of 83.55% and an EER of 5.25% at the utterance level. At the segment level, it attains an EER of 1.07% and a 92.19% F1 score. These results highlight the model's robust capability for a comprehensive analysis of synthetic speech, offering a promising avenue for future research and practical applications in this field.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Determination of galaxy photometric redshifts using Conditional Generative Adversarial Networks (CGANs)</title>
<link>https://arxiv.org/abs/2501.06532</link>
<guid>https://arxiv.org/abs/2501.06532</guid>
<content:encoded><![CDATA[
arXiv:2501.06532v3 Announce Type: replace-cross 
Abstract: Accurate and reliable photometric redshift determination is one of the key aspects for wide-field photometric surveys. Determination of photometric redshift for galaxies, has been traditionally solved by use of machine-learning and artificial intelligence techniques trained on a calibration sample of galaxies, where both photometry and spectrometry are available. On this paper, we present a new algorithmic approach for determining photometric redshifts of galaxies using Conditional Generative Adversarial Networks (CGANs). The proposed implementation is able to determine both point-estimation and probability-density estimations for photometric redshifts. The methodology is tested with data from Dark Energy Survey (DES) Y1 data and compared with other existing algorithm such as a Mixture Density Network (MDN). Although results obtained show a superiority of MDN, CGAN quality-metrics are close to the MDN results, opening the door to the use of CGAN at photometric redshift estimation.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with Equivariant Neural Networks</title>
<link>https://arxiv.org/abs/2501.14048</link>
<guid>https://arxiv.org/abs/2501.14048</guid>
<content:encoded><![CDATA[
arXiv:2501.14048v2 Announce Type: replace-cross 
Abstract: Modern neural networks (NNs) often do not generalize well in the presence of a "covariate shift"; that is, in situations where the training and test data distributions differ, but the conditional distribution of classification labels remains unchanged. In such cases, NN generalization can be reduced to a problem of learning more domain-invariant features. Domain adaptation (DA) methods include a range of techniques aimed at achieving this; however, these methods have struggled with the need for extensive hyperparameter tuning, which then incurs significant computational costs. In this work, we introduce SIDDA, an out-of-the-box DA training algorithm built upon the Sinkhorn divergence, that can achieve effective domain alignment with minimal hyperparameter tuning and computational overhead. We demonstrate the efficacy of our method on multiple simulated and real datasets of varying complexity, including simple shapes, handwritten digits, and real astronomical observations. SIDDA is compatible with a variety of NN architectures, and it works particularly well in improving classification accuracy and model calibration when paired with equivariant neural networks (ENNs). We find that SIDDA enhances the generalization capabilities of NNs, achieving up to a $\approx40\%$ improvement in classification accuracy on unlabeled target data. We also study the efficacy of DA on ENNs with respect to the varying group orders of the dihedral group $D_N$, and find that the model performance improves as the degree of equivariance increases. Finally, we find that SIDDA enhances model calibration on both source and target data--achieving over an order of magnitude improvement in the ECE and Brier score. SIDDA's versatility, combined with its automated approach to domain alignment, has the potential to advance multi-dataset studies by enabling the development of highly generalizable models.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-RecG: A Semantic Bias-Aware Framework for Zero-Shot Sequential Recommendation</title>
<link>https://arxiv.org/abs/2501.19232</link>
<guid>https://arxiv.org/abs/2501.19232</guid>
<content:encoded><![CDATA[
arXiv:2501.19232v2 Announce Type: replace-cross 
Abstract: Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions in unseen domains without additional training or fine-tuning, addressing the limitations of traditional models in sparse data environments. Recent advancements in large language models (LLMs) have significantly enhanced ZCDSR by facilitating cross-domain knowledge transfer through rich, pretrained representations. Despite this progress, domain semantic bias -- arising from differences in vocabulary and content focus between domains -- remains a persistent challenge, leading to misaligned item embeddings and reduced generalization across domains. To address this, we propose a novel semantic bias-aware framework that enhances LLM-based ZCDSR by improving cross-domain alignment at both the item and sequential levels. At the item level, we introduce a generalization loss that aligns the embeddings of items across domains (inter-domain compactness), while preserving the unique characteristics of each item within its own domain (intra-domain diversity). This ensures that item embeddings can be transferred effectively between domains without collapsing into overly generic or uniform representations. At the sequential level, we develop a method to transfer user behavioral patterns by clustering source domain user sequences and applying attention-based aggregation during target domain inference. We dynamically adapt user embeddings to unseen domains, enabling effective zero-shot recommendations without requiring target-domain interactions...
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Transformer World Models for Data-Efficient RL</title>
<link>https://arxiv.org/abs/2502.01591</link>
<guid>https://arxiv.org/abs/2502.01591</guid>
<content:encoded><![CDATA[
arXiv:2502.01591v3 Announce Type: replace-cross 
Abstract: We present three improvements to the standard model-based RL paradigm based on transformers: (a) "Dyna with warmup", which trains the policy on real and imaginary data, but only starts using imaginary data after the world model has been sufficiently trained; (b) "nearest neighbor tokenizer" for image patches, which improves upon previous tokenization schemes, which are needed when using a transformer world model (TWM), by ensuring the code words are static after creation, thus providing a constant target for TWM learning; and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep, instead of generating them sequentially. We then show that our method significantly improves upon prior methods in various environments. We mostly focus on the challenging Craftax-classic benchmark, where our method achieves a reward of 69.66% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and exceeding human performance of 65.0% for the first time. We also show preliminary results on Craftax-full, MinAtar, and three different two-player games, to illustrate the generality of the approach.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning</title>
<link>https://arxiv.org/abs/2502.15082</link>
<guid>https://arxiv.org/abs/2502.15082</guid>
<content:encoded><![CDATA[
arXiv:2502.15082v2 Announce Type: replace-cross 
Abstract: User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or "forgetting" a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model's other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model's representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. Across three standard unlearning methods, UPCORE consistently achieves a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. Our results show that UPCORE improves both standard metrics and AUC, benefiting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Governance InternationaL Evaluation Index (AGILE Index) 2024</title>
<link>https://arxiv.org/abs/2502.15859</link>
<guid>https://arxiv.org/abs/2502.15859</guid>
<content:encoded><![CDATA[
arXiv:2502.15859v4 Announce Type: replace-cross 
Abstract: The rapid advancement of Artificial Intelligence (AI) technology is profoundly transforming human society and concurrently presenting a series of ethical, legal, and social issues. The effective governance of AI has become a crucial global concern. Since 2022, the extensive deployment of generative AI, particularly large language models, marked a new phase in AI governance. Continuous efforts are being made by the international community in actively addressing the novel challenges posed by these AI developments. As consensus on international governance continues to be established and put into action, the practical importance of conducting a global assessment of the state of AI governance is progressively coming to light. In this context, we initiated the development of the AI Governance InternationaL Evaluation Index (AGILE Index). Adhering to the design principle, "the level of governance should match the level of development," the inaugural evaluation of the AGILE Index commences with an exploration of four foundational pillars: the development level of AI, the AI governance environment, the AI governance instruments, and the AI governance effectiveness. It covers 39 indicators across 18 dimensions to comprehensively assess the AI governance level of 14 representative countries globally. The index is utilized to delve into the status of AI governance to date in 14 countries for the first batch of evaluation. The aim is to depict the current state of AI governance in these countries through data scoring, assist them in identifying their governance stage and uncovering governance issues, and ultimately offer insights for the enhancement of their AI governance systems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Transformation and Analysis of Timelines through Learning via Surprisability</title>
<link>https://arxiv.org/abs/2503.04502</link>
<guid>https://arxiv.org/abs/2503.04502</guid>
<content:encoded><![CDATA[
arXiv:2503.04502v2 Announce Type: replace-cross 
Abstract: The analysis of high-dimensional timeline data and the identification of outliers and anomalies is critical across diverse domains, including sensor readings, biological and medical data, historical records, and global statistics. However, conventional analysis techniques often struggle with challenges such as high dimensionality, complex distributions, and sparsity. These limitations hinder the ability to extract meaningful insights from complex temporal datasets, making it difficult to identify trending features, outliers, and anomalies effectively. Inspired by surprisability -- a cognitive science concept describing how humans instinctively focus on unexpected deviations - we propose Learning via Surprisability (LvS), a novel approach for transforming high-dimensional timeline data. LvS quantifies and prioritizes anomalies in time-series data by formalizing deviations from expected behavior. LvS bridges cognitive theories of attention with computational methods, enabling the detection of anomalies and shifts in a way that preserves critical context, offering a new lens for interpreting complex datasets. We demonstrate the usefulness of LvS on three high-dimensional timeline use cases: a time series of sensor data, a global dataset of mortality causes over multiple years, and a textual corpus containing over two centuries of State of the Union Addresses by U.S. presidents. Our results show that the LvS transformation enables efficient and interpretable identification of outliers, anomalies, and the most variable features along the timeline.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-Max: A Reinforcement Learning Framework for Autonomous Driving</title>
<link>https://arxiv.org/abs/2503.08388</link>
<guid>https://arxiv.org/abs/2503.08388</guid>
<content:encoded><![CDATA[
arXiv:2503.08388v3 Announce Type: replace-cross 
Abstract: Learning-based decision-making has the potential to enable generalizable Autonomous Driving (AD) policies, reducing the engineering overhead of rule-based approaches. Imitation Learning (IL) remains the dominant paradigm, benefiting from large-scale human demonstration datasets, but it suffers from inherent limitations such as distribution shift and imitation gaps. Reinforcement Learning (RL) presents a promising alternative, yet its adoption in AD remains limited due to the lack of standardized and efficient research frameworks. To this end, we introduce V-Max, an open research framework providing all the necessary tools to make RL practical for AD. V-Max is built on Waymax, a hardware-accelerated AD simulator designed for large-scale experimentation. We extend it using ScenarioNet's approach, enabling the fast simulation of diverse AD datasets.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation Classification Using Large Language Models</title>
<link>https://arxiv.org/abs/2503.12989</link>
<guid>https://arxiv.org/abs/2503.12989</guid>
<content:encoded><![CDATA[
arXiv:2503.12989v2 Announce Type: replace-cross 
Abstract: Automatically annotating job data with standardized occupations from taxonomies, known as occupation classification, is crucial for labor market analysis. However, this task is often hindered by data scarcity and the challenges of manual annotations. While large language models (LLMs) hold promise due to their extensive world knowledge and in-context learning capabilities, their effectiveness depends on their knowledge of occupational taxonomies, which remains unclear. In this study, we assess the ability of LLMs to generate precise taxonomic entities from taxonomy, highlighting their limitations, especially for smaller models. To address these challenges, we propose a multi-stage framework consisting of inference, retrieval, and reranking stages, which integrates taxonomy-guided reasoning examples to enhance performance by aligning outputs with taxonomic knowledge. Evaluations on a large-scale dataset show that our framework not only enhances occupation and skill classification tasks, but also provides a cost-effective alternative to frontier models like GPT-4o, significantly reducing computational costs while maintaining strong performance. This makes it a practical and scalable solution for occupation classification and related tasks across LLMs.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial and Legged Odometry Fusion SLAM for Dynamic Legged Robotics</title>
<link>https://arxiv.org/abs/2503.14247</link>
<guid>https://arxiv.org/abs/2503.14247</guid>
<content:encoded><![CDATA[
arXiv:2503.14247v2 Announce Type: replace-cross 
Abstract: This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled RGBD-inertial SLAM for legged robotics undergoing aggressive and high-frequency motions.By integrating geometric consistency, legged odometry constraints, and dual-stream optical flow (GeoFlow), our method addresses three critical challenges:feature matching and pose initialization failures during fast locomotion and visual feature scarcity in texture-less scenes.Specifically, in rapid motion scenarios, feature matching is notably enhanced by leveraging dual-stream optical flow, which combines prior map points and poses. Additionally, we propose a robust pose initialization method for fast locomotion and IMU error in legged robots, integrating IMU/Legged odometry, inter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point (GICP). Furthermore, a novel optimization framework that tightly couples depth-to-map and GICP geometric constraints is first introduced to improve the robustness and accuracy in long-duration, visually texture-less environments. The proposed algorithms achieve state-of-the-art (SOTA) on collected legged robots and open-source datasets. To further promote research and development, the open-source datasets and code will be made publicly available at https://github.com/HorizonRobotics/geoflow-slam
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Universal Human Mobility Patterns with a Foundation Model for Cross-domain Data Fusion</title>
<link>https://arxiv.org/abs/2503.15779</link>
<guid>https://arxiv.org/abs/2503.15779</guid>
<content:encoded><![CDATA[
arXiv:2503.15779v2 Announce Type: replace-cross 
Abstract: Human mobility modeling is critical for urban planning and transportation management, yet existing approaches often lack the integration capabilities needed to handle diverse data sources. We present a foundation model framework for universal human mobility patterns that leverages cross-domain data fusion and large language models to address these limitations. Our approach integrates multi-modal data of distinct nature and spatio-temporal resolution, including geographical, mobility, socio-demographic, and traffic information, to construct a privacy-preserving and semantically enriched human travel trajectory dataset. Our framework demonstrates adaptability through domain transfer techniques that ensure transferability across diverse urban contexts, as evidenced in case studies of Los Angeles (LA) and Egypt. The framework employs LLMs for semantic enrichment of trajectory data, enabling comprehensive understanding of mobility patterns. Quantitative evaluation shows that our generated synthetic dataset accurately reproduces mobility patterns observed in empirical data. The practical utility of this foundation model approach is demonstrated through large-scale traffic simulations for LA County, where results align well with observed traffic data. On California's I-405 corridor, the simulation yields a Mean Absolute Percentage Error of 5.85% for traffic volume and 4.36% for speed compared to Caltrans PeMS observations, illustrating the framework's potential for intelligent transportation systems and urban mobility applications.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Deep Operator Network for Unsteady Flow: A Multi-Fidelity Approach with Physics-Guided Subsampling</title>
<link>https://arxiv.org/abs/2503.17941</link>
<guid>https://arxiv.org/abs/2503.17941</guid>
<content:encoded><![CDATA[
arXiv:2503.17941v2 Announce Type: replace-cross 
Abstract: This study presents an enhanced multi-fidelity Deep Operator Network (DeepONet) framework for efficient spatio-temporal flow field prediction when high-fidelity data is scarce. Key innovations include: a merge network replacing traditional dot-product operations, achieving 50.4% reduction in prediction error and 7.57% accuracy improvement while reducing training time by 96%; a transfer learning multi-fidelity approach that freezes pre-trained low-fidelity networks while making only the merge network trainable, outperforming alternatives by up to 76% and achieving 43.7% better accuracy than single-fidelity training; and a physics-guided subsampling method that strategically selects high-fidelity training points based on temporal dynamics, reducing high-fidelity sample requirements by 40% while maintaining comparable accuracy. Comprehensive experiments across multiple resolutions and datasets demonstrate the framework's ability to significantly reduce required high-fidelity dataset size while maintaining predictive accuracy, with consistent superior performance against conventional benchmarks.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VectorFit : Adaptive Singular &amp; Bias Vector Fine-Tuning of Pre-trained Foundation Models</title>
<link>https://arxiv.org/abs/2503.19530</link>
<guid>https://arxiv.org/abs/2503.19530</guid>
<content:encoded><![CDATA[
arXiv:2503.19530v2 Announce Type: replace-cross 
Abstract: Popular PEFT methods reduce trainable parameter count for fine-tuning by parameterizing new low-rank or sparse trainable weights in parallel to the frozen pre-trained weights $W$. However, these weights are trained from scratch, and there exists a performance gap between these methods and full fine-tuning, especially in low-budget settings. We introduce VectorFit, a new way of parameterization that efficiently utilizes the existing knowledge embedded in $W$ by adaptively training their singular vectors and biases. We show that utilizing the structural and transformational properties of $W$ in this way can lead to high-rank incremental weight matrices $\Delta W$, comparable to that of full fine-tuning. VectorFit delivers superior results with \textbf{9$\boldsymbol\times$} fewer trainable parameters than the leading PEFT methods. Through comprehensive experiments across 19 datasets covering a wide range of language and vision tasks such as natural language understanding and generation, question answering, image classification, and image generation, we demonstrate that VectorFit surpasses baselines in terms of performance as a function of parameter-efficiency.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K-P Quantum Neural Networks</title>
<link>https://arxiv.org/abs/2504.01673</link>
<guid>https://arxiv.org/abs/2504.01673</guid>
<content:encoded><![CDATA[
arXiv:2504.01673v2 Announce Type: replace-cross 
Abstract: We present an extension of K-P time-optimal quantum control solutions using global Cartan $KAK$ decompositions for geodesic-based solutions. Extending recent time-optimal constant-$\theta$ control results, we integrate Cartan methods into equivariant quantum neural network (EQNN) for quantum control tasks. We show that a finite-depth limited EQNN ansatz equipped with Cartan layers can replicate the constant-$\theta$ sub-Riemannian geodesics for K-P problems. We demonstrate how for certain classes of control problem on Riemannian symmetric spaces, gradient-based training using an appropriate cost function converges to certain global time-optimal solutions when satisfying simple regularity conditions. This generalises prior geometric control theory methods and clarifies how optimal geodesic estimation can be performed in quantum machine learning contexts.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model</title>
<link>https://arxiv.org/abs/2504.03770</link>
<guid>https://arxiv.org/abs/2504.03770</guid>
<content:encoded><![CDATA[
arXiv:2504.03770v3 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression</title>
<link>https://arxiv.org/abs/2504.07389</link>
<guid>https://arxiv.org/abs/2504.07389</guid>
<content:encoded><![CDATA[
arXiv:2504.07389v2 Announce Type: replace-cross 
Abstract: Post-training quantization (PTQ) reduces a model's memory footprint by mapping full precision weights into low bit weights without costly retraining, but can degrade its downstream performance especially in low 2- to 3-bit settings. We develop a new mixed-precision PTQ approach, Task-Circuit Quantization (TaCQ), that draws parallels to automated circuit discovery, directly conditioning the quantization process on specific weight circuits -- which we define as sets of weights associated with downstream task performance. These weights are kept as 16-bit weights, while others are quantized, maintaining performance while only adding a marginal memory cost. Specifically, TaCQ contrasts unquantized model weights with a uniformly-quantized model to estimate the expected change in weights due to quantization and uses gradient information to predict the resulting impact on task performance, allowing us to preserve task-specific weights. We compare TaCQ-based quantization to existing mixed-precision quantization methods when conditioning both on general-purpose and task-specific data. Across QA, math reasoning, and text-to-SQL tasks for both Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the same calibration data and a lower weight budget, achieving major improvements in the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of Llama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25% absolute improvement over SPQR. We also observe consistently large gains over existing methods in the 2-bit regime, with an average gain of 14.74% over the strongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without conditioning on specific tasks, showing TaCQ's ability to identify important weights is not limited to task-conditioned settings.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs</title>
<link>https://arxiv.org/abs/2504.16394</link>
<guid>https://arxiv.org/abs/2504.16394</guid>
<content:encoded><![CDATA[
arXiv:2504.16394v3 Announce Type: replace-cross 
Abstract: Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence</title>
<link>https://arxiv.org/abs/2504.17703</link>
<guid>https://arxiv.org/abs/2504.17703</guid>
<content:encoded><![CDATA[
arXiv:2504.17703v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) has emerged as a transformative paradigm in the field of distributed machine learning, enabling multiple clients such as mobile devices, edge nodes, or organizations to collaboratively train a shared global model without the need to centralize sensitive data. This decentralized approach addresses growing concerns around data privacy, security, and regulatory compliance, making it particularly attractive in domains such as healthcare, finance, and smart IoT systems. This survey provides a concise yet comprehensive overview of Federated Learning, beginning with its core architecture and communication protocol. We discuss the standard FL lifecycle, including local training, model aggregation, and global updates. A particular emphasis is placed on key technical challenges such as handling non-IID (non-independent and identically distributed) data, mitigating system and hardware heterogeneity, reducing communication overhead, and ensuring privacy through mechanisms like differential privacy and secure aggregation. Furthermore, we examine emerging trends in FL research, including personalized FL, cross-device versus cross-silo settings, and integration with other paradigms such as reinforcement learning and quantum computing. We also highlight real-world applications and summarize benchmark datasets and evaluation metrics commonly used in FL research. Finally, we outline open research problems and future directions to guide the development of scalable, efficient, and trustworthy FL systems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness</title>
<link>https://arxiv.org/abs/2504.21773</link>
<guid>https://arxiv.org/abs/2504.21773</guid>
<content:encoded><![CDATA[
arXiv:2504.21773v2 Announce Type: replace-cross 
Abstract: With the widespread application of large language models (LLMs), the issue of generating non-existing facts, known as hallucination, has garnered increasing attention. Previous research in enhancing LLM confidence estimation mainly focuses on the single problem setting. However, LLM awareness of its internal parameterized knowledge boundary under the more challenging multi-problem setting, which requires answering multiple problems accurately simultaneously, remains underexplored. To bridge this gap, we introduce a novel method, Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Extensive experiments demonstrate that our method outperforms baselines by up to 25% in average precision.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coral Protocol: Open Infrastructure Connecting The Internet of Agents</title>
<link>https://arxiv.org/abs/2505.00749</link>
<guid>https://arxiv.org/abs/2505.00749</guid>
<content:encoded><![CDATA[
arXiv:2505.00749v2 Announce Type: replace-cross 
Abstract: Coral Protocol is an open and decentralized collaboration infrastructure that enables communication, coordination, trust and payments for The Internet of Agents. It addresses the growing need for interoperability in a world where organizations are deploying multiple specialized AI agents that must work together across domains and vendors. As a foundational platform for multi-agent AI ecosystems, Coral establishes a common language and coordination framework allowing any agent to participate in complex workflows with others. Its design emphasizes broad compatibility, security, and vendor neutrality, ensuring that agent interactions are efficient and trustworthy. In particular, Coral introduces standardized messaging formats for agent communication, a modular coordination mechanism for orchestrating multi-agent tasks, and secure team formation capabilities for dynamically assembling trusted groups of agents. Together, these innovations position Coral Protocol as a cornerstone of the emerging "Internet of Agents," unlocking new levels of automation, collective intelligence, and business value through open agent collaboration.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID</title>
<link>https://arxiv.org/abs/2505.03557</link>
<guid>https://arxiv.org/abs/2505.03557</guid>
<content:encoded><![CDATA[
arXiv:2505.03557v2 Announce Type: replace-cross 
Abstract: Personalizing Stable Diffusion for professional portrait generation from amateur photos faces challenges in maintaining facial resemblance. This paper evaluates the impact of augmentation strategies on two personalization methods: DreamBooth and InstantID. We compare classical augmentations (flipping, cropping, color adjustments) with generative augmentation using InstantID's synthetic images to enrich training data. Using SDXL and a new FaceDistance metric based on FaceNet, we quantitatively assess facial similarity. Results show classical augmentations can cause artifacts harming identity retention, while InstantID improves fidelity when balanced with real images to avoid overfitting. A user study with 97 participants confirms high photorealism and preferences for InstantID's polished look versus DreamBooth's identity accuracy. Our findings inform effective augmentation strategies for personalized text-to-image generation.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPU Performance Portability needs Autotuning</title>
<link>https://arxiv.org/abs/2505.03780</link>
<guid>https://arxiv.org/abs/2505.03780</guid>
<content:encoded><![CDATA[
arXiv:2505.03780v3 Announce Type: replace-cross 
Abstract: As LLMs grow in complexity, achieving state-of-the-art performance requires tight co-design across algorithms, software, and hardware. Today's reliance on a single dominant platform limits portability, creates vendor lock-in, and raises barriers for new AI hardware. In this work, we make the case for combining just-in-time (JIT) compilation with comprehensive kernel parameter autotuning to enable portable LLM inference with state-of-the-art performance without code changes. Focusing on performance-critical LLM kernels, we demonstrate that this approach explores up to 15x more kernel parameter configurations, produces significantly more diverse code across multiple dimensions, and even outperforms vendor-optimized implementations by up to 230%, all while reducing kernel code size by 70x and eliminating manual code optimizations. Our results highlight autotuning as a promising path to unlocking model portability across GPU vendors.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations</title>
<link>https://arxiv.org/abs/2505.23121</link>
<guid>https://arxiv.org/abs/2505.23121</guid>
<content:encoded><![CDATA[
arXiv:2505.23121v2 Announce Type: replace-cross 
Abstract: Multi-modal large language models have demonstrated remarkable zero-shot abilities and powerful image-understanding capabilities. However, the existing open-source multi-modal models suffer from the weak capability of multi-turn interaction, especially for long contexts. To address the issue, we first introduce a context modeling module, termed ContextQFormer, which utilizes a memory block to enhance the presentation of contextual information. Furthermore, to facilitate further research, we carefully build a new multi-turn multi-modal dialogue dataset (TMDialog) for pre-training, instruction-tuning, and evaluation, which will be open-sourced lately. Compared with other multi-modal dialogue datasets, TMDialog contains longer conversations, which supports the research of multi-turn multi-modal dialogue. In addition, ContextQFormer is compared with three baselines on TMDialog and experimental results illustrate that ContextQFormer achieves an improvement of 2%-4% in available rate over baselines.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows</title>
<link>https://arxiv.org/abs/2505.24189</link>
<guid>https://arxiv.org/abs/2505.24189</guid>
<content:encoded><![CDATA[
arXiv:2505.24189v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple-Frequencies Population-Based Training</title>
<link>https://arxiv.org/abs/2506.03225</link>
<guid>https://arxiv.org/abs/2506.03225</guid>
<content:encoded><![CDATA[
arXiv:2506.03225v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning's high sensitivity to hyperparameters is a source of instability and inefficiency, creating significant challenges for practitioners. Hyperparameter Optimization (HPO) algorithms have been developed to address this issue, among them Population-Based Training (PBT) stands out for its ability to generate hyperparameters schedules instead of fixed configurations. PBT trains a population of agents, each with its own hyperparameters, frequently ranking them and replacing the worst performers with mutations of the best agents. These intermediate selection steps can cause PBT to focus on short-term improvements, leading it to get stuck in local optima and eventually fall behind vanilla Random Search over longer timescales. This paper studies how this greediness issue is connected to the choice of evolution frequency, the rate at which the selection is done. We propose Multiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm that addresses greediness by employing sub-populations, each evolving at distinct frequencies. MF-PBT introduces a migration process to transfer information between sub-populations, with an asymmetric design to balance short and long-term optimization. Extensive experiments on the Brax suite demonstrate that MF-PBT improves sample efficiency and long-term performance, even without actually tuning hyperparameters.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents</title>
<link>https://arxiv.org/abs/2506.15841</link>
<guid>https://arxiv.org/abs/2506.15841</guid>
<content:encoded><![CDATA[
arXiv:2506.15841v2 Announce Type: replace-cross 
Abstract: Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability</title>
<link>https://arxiv.org/abs/2506.18248</link>
<guid>https://arxiv.org/abs/2506.18248</guid>
<content:encoded><![CDATA[
arXiv:2506.18248v3 Announce Type: replace-cross 
Abstract: Generative adversarial attacks train a perturbation generator on a white-box surrogate model and subsequently apply the crafted perturbations to unseen black-box victim models. In contrast to iterative attacks, these methods deliver superior inference-time efficiency, scalability, and transferability; however, up until now, existing studies have not fully exploited the representational capacity of generative models to preserve and harness semantic information. Specifically, the intermediate activations of the generator encode rich semantic features--object boundaries and coarse shapes--that remain under-exploited, thereby limiting the alignment of perturbations with object-salient regions which are critical for adversarial transferability. To remedy this, we introduce a semantic structure-aware attack framework based on the Mean Teacher, which serves as a temporally smoothed feature reference. With this smoothed reference, we further direct semantic consistency between the early-layer activations in the student and those of the semantically rich teacher by feature distillation. By anchoring perturbation synthesis to the semantically salient early intermediate blocks within the generator based on empirical findings, our method guides progressive adversarial perturbation on regions that substantially enhance adversarial transferability. We conduct extensive experiments over diverse models, domains and tasks to demonstrate consistent improvements relative to state-of-the-art generative attacks, comprehensively evaluated using conventional metrics and our newly proposed Accidental Correction Rate (ACR).
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Layer Discrete Concept Discovery for Interpreting Language Models</title>
<link>https://arxiv.org/abs/2506.20040</link>
<guid>https://arxiv.org/abs/2506.20040</guid>
<content:encoded><![CDATA[
arXiv:2506.20040v2 Announce Type: replace-cross 
Abstract: Uncovering emergent concepts across transformer layers remains a significant challenge because the residual stream linearly mixes and duplicates information, obscuring how features evolve within large language models. Current research efforts primarily inspect neural representations at single layers, thereby overlooking this cross-layer superposition and the redundancy it introduces. These representations are typically either analyzed directly for activation patterns or passed to probing classifiers that map them to a limited set of predefined concepts. To address these limitations, we propose cross-layer VQ-VAE (CLVQ-VAE), a framework that uses vector quantization to map representations across layers and in the process collapse duplicated residual-stream features into compact, interpretable concept vectors. Our approach uniquely combines top-k temperature-based sampling during quantization with EMA codebook updates, providing controlled exploration of the discrete latent space while maintaining code-book diversity. We further enhance the framework with scaled-spherical k-means++ for codebook initialization, which clusters by directional similarity rather than magnitude, better aligning with semantic structure in word embedding space.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCode: Updating Code API Knowledge with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.20495</link>
<guid>https://arxiv.org/abs/2506.20495</guid>
<content:encoded><![CDATA[
arXiv:2506.20495v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents</title>
<link>https://arxiv.org/abs/2506.21582</link>
<guid>https://arxiv.org/abs/2506.21582</guid>
<content:encoded><![CDATA[
arXiv:2506.21582v2 Announce Type: replace-cross 
Abstract: Text analytics has traditionally required specialized knowledge in Natural Language Processing (NLP) or text analysis, which presents a barrier for entry-level analysts. Recent advances in large language models (LLMs) have changed the landscape of NLP by enabling more accessible and automated text analysis (e.g., topic detection, summarization, information extraction, etc.). We introduce VIDEE, a system that supports entry-level data analysts to conduct advanced text analytics with intelligent agents. VIDEE instantiates a human-agent collaroration workflow consisting of three stages: (1) Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search algorithm to support generative reasoning with human feedback, (2) Execution, which generates an executable text analytics pipeline, and (3) Evaluation, which integrates LLM-based evaluation and visualizations to support user validation of execution results. We conduct two quantitative experiments to evaluate VIDEE's effectiveness and analyze common agent errors. A user study involving participants with varying levels of NLP and text analytics experience -- from none to expert -- demonstrates the system's usability and reveals distinct user behavior patterns. The findings identify design implications for human-agent collaboration, validate the practical utility of VIDEE for non-expert users, and inform future improvements to intelligent text analytics systems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Specific Generative Dataset Distillation with Difficulty-Guided Sampling</title>
<link>https://arxiv.org/abs/2507.03331</link>
<guid>https://arxiv.org/abs/2507.03331</guid>
<content:encoded><![CDATA[
arXiv:2507.03331v2 Announce Type: replace-cross 
Abstract: To alleviate the reliance of deep neural networks on large-scale datasets, dataset distillation aims to generate compact, high-quality synthetic datasets that can achieve comparable performance to the original dataset. The integration of generative models has significantly advanced this field. However, existing approaches primarily focus on aligning the distilled dataset with the original one, often overlooking task-specific information that can be critical for optimal downstream performance. In this paper, focusing on the downstream task of classification, we propose a task-specific sampling strategy for generative dataset distillation that incorporates the concept of difficulty to consider the requirements of the target task better. The final dataset is sampled from a larger image pool with a sampling distribution obtained by matching the difficulty distribution of the original dataset. A logarithmic transformation is applied as a pre-processing step to correct for distributional bias. The results of extensive experiments demonstrate the effectiveness of our method and suggest its potential for enhancing performance on other downstream tasks. The code is available at https://github.com/SumomoTaku/DiffGuideSamp.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities</title>
<link>https://arxiv.org/abs/2507.06261</link>
<guid>https://arxiv.org/abs/2507.06261</guid>
<content:encoded><![CDATA[
arXiv:2507.06261v3 Announce Type: replace-cross 
Abstract: In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses</title>
<link>https://arxiv.org/abs/2507.07188</link>
<guid>https://arxiv.org/abs/2507.07188</guid>
<content:encoded><![CDATA[
arXiv:2507.07188v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly used as proxies for human subjects in social science surveys, but their reliability and susceptibility to known response biases are poorly understood. This paper investigates the response robustness of LLMs in normative survey contexts - we test nine diverse LLMs on questions from the World Values Survey (WVS), applying a comprehensive set of 11 perturbations to both question phrasing and answer option structure, resulting in over 167,000 simulated interviews. In doing so, we not only reveal LLMs' vulnerabilities to perturbations but also show that all tested models exhibit a consistent recency bias varying in intensity, disproportionately favoring the last-presented answer option. While larger models are generally more robust, all models remain sensitive to semantic variations like paraphrasing and to combined perturbations. By applying a set of perturbations, we reveal that LLMs partially align with survey response biases identified in humans. This underscores the critical importance of prompt design and robustness testing when using LLMs to generate synthetic survey data.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KeyRe-ID: Keypoint-Guided Person Re-Identification using Part-Aware Representation in Videos</title>
<link>https://arxiv.org/abs/2507.07393</link>
<guid>https://arxiv.org/abs/2507.07393</guid>
<content:encoded><![CDATA[
arXiv:2507.07393v3 Announce Type: replace-cross 
Abstract: We propose \textbf{KeyRe-ID}, a keypoint-guided video-based person re-identification framework consisting of global and local branches that leverage human keypoints for enhanced spatiotemporal representation learning. The global branch captures holistic identity semantics through Transformer-based temporal aggregation, while the local branch dynamically segments body regions based on keypoints to generate fine-grained, part-aware features. Extensive experiments on MARS and iLIDS-VID benchmarks demonstrate state-of-the-art performance, achieving 91.73\% mAP and 97.32\% Rank-1 accuracy on MARS, and 96.00\% Rank-1 and 100.0\% Rank-5 accuracy on iLIDS-VID. The code for this work will be publicly available on GitHub upon publication.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Magneto-radiative modelling and artificial neural network optimization of biofluid flow in a stenosed arterial domain</title>
<link>https://arxiv.org/abs/2507.06273</link>
<guid>https://arxiv.org/abs/2507.06273</guid>
<content:encoded><![CDATA[
<div> nanofluid, cardiovascular diseases, drug delivery systems, Casson-Maxwell, heat transfer<br />
Summary:<br />
- Study examines flow of Casson-Maxwell nanofluid in stenosed arterial domain for targeted drug delivery in cardiovascular diseases, supporting UN SDGs 3 and 9.
- Casson-Maxwell fluid shows lower velocity profile, indicating improved drug residence time.
- Heat transfer rate increases with copper and aluminium oxide nanoparticles, decreases with silver nanoparticles, supporting SDG 4.
- Skin friction coefficient decreases by 219% with higher Maxwell parameter, increases by 66.1% with higher Casson parameter, fostering interdisciplinary collaboration for healthcare innovation.
- Prediction of heat flow rate using Levenberg-Marquardt backpropagation training scheme, with sensitivity of drag coefficient to Maxwell parameter, relates to SDG 17. <div>
arXiv:2507.06273v2 Announce Type: replace-cross 
Abstract: The increasing complexity of cardiovascular diseases and limitations in traditional healing methods mandate the invention of new drug delivery systems that assure targeted, effective, and regulated treatments, contributing directly to UN SDGs 3 and 9, thereby encouraging the utilization of sustainable medical technologies in healthcare. This study investigates the flow of a Casson-Maxwell nanofluid through a stenosed arterial domain. The quantities, such as skin friction and heat transfer rate, are analysed in detail. The Casson-Maxwell fluid shows a lower velocity profile than the Casson fluids, which indicates the improved residence time for efficient drug delivery. The heat transfer rate shows an increase with higher volume fractions of copper and aluminium oxide nanoparticles and a decrease with higher volume fractions of silver nanoparticles. The skin friction coefficient decreases by 219% with a unit increase in the Maxwell parameter, whereas it increases by 66.1% with a unit rise in the Casson parameter. This work supports SDGs 4 and 17 by fostering interdisciplinary learning and collaboration in fluid dynamics and healthcare innovation. Additionally, the rate of heat flow was forecasted (with an overall R-value of 0.99457) using the Levenberg-Marquardt backpropagation training scheme under the influence of magneto-radiative, linear heat source and Casson-Maxwell parameters along with the tri-metallic nanoparticle volume fractions. It is also observed that the drag coefficient is most sensitive to the changes in the Maxwell parameter.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on the Application of Artificial Intelligence in Ecological Design</title>
<link>https://arxiv.org/abs/2507.11595</link>
<guid>https://arxiv.org/abs/2507.11595</guid>
<content:encoded><![CDATA[
<div> Keywords: nature, artificial intelligence, ecological design, sustainability, ecosystem<br />
Summary: 
This paper explores the potential for shifting our relationship with nature from human dominance to interdependence through the mediation of artificial intelligence (AI). By examining a new ecological-design paradigm where AI interacts with non-human life forms, the authors showcase how AI can be utilized for data analysis, image recognition, and ecological restoration, leading to innovative outcomes. The study argues that AI not only expands creative methods but also redefines the theory and practice of ecological design. Through a prototype for AI-assisted water remediation, the authors propose design pathways that combine reinforcement learning with plant-based phytoremediation. This research highlights AI's ability to bridge scientific insights, artistic practices, and environmental stewardship, offering a roadmap for sustainable, technology-enabled ecosystems.<br /><br />Summary: <div>
arXiv:2507.11595v1 Announce Type: new 
Abstract: This paper asks whether our relationship with nature can move from human dominance to genuine interdependence, and whether artificial intelligence (AI) can mediate that shift. We examine a new ecological-design paradigm in which AI interacts with non-human life forms. Through case studies we show how artists and designers apply AI for data analysis, image recognition, and ecological restoration, producing results that differ from conventional media. We argue that AI not only expands creative methods but also reframes the theory and practice of ecological design. Building on the author's prototype for AI-assisted water remediation, the study proposes design pathways that couple reinforcement learning with plant-based phytoremediation. The findings highlight AI's potential to link scientific insight, artistic practice, and environmental stewardship, offering a roadmap for future research on sustainable, technology-enabled ecosystems.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Modular Harness for LLM Agents in Multi-Turn Gaming Environments</title>
<link>https://arxiv.org/abs/2507.11633</link>
<guid>https://arxiv.org/abs/2507.11633</guid>
<content:encoded><![CDATA[
<div> modular design, LLM agents, perception, memory, reasoning<br />
<br />
Summary: 
The article introduces a modular harness design for LLM agents that consists of perception, memory, and reasoning components, allowing a single backbone to handle various multi-turn gaming environments without the need for domain-specific engineering. The framework offers a unified workflow for analyzing the impact of each module on performance in dynamic interactive settings using different game suites as testbeds. Through extensive experiments, it is demonstrated that the modular harness design consistently improves gameplay performance compared to un-harnessed baselines. The study reveals distinct patterns of contribution, such as memory being crucial in long-horizon puzzles and perception playing a vital role in vision noisy arcades. These findings underscore the efficacy of the modular harness design in enhancing general-purpose agents, leveraging the familiarity and prevalence of games in human everyday experience. <br /><br />Summary: <div>
arXiv:2507.11633v1 Announce Type: new 
Abstract: We introduce a modular harness design for LLM agents that composes of perception, memory, and reasoning components, enabling a single LLM or VLM backbone to tackle a wide spectrum of multi turn gaming environments without domain-specific engineering. Using classic and modern game suites as low-barrier, high-diversity testbeds, our framework provides a unified workflow for analyzing how each module affects performance across dynamic interactive settings. Extensive experiments demonstrate that the harness lifts gameplay performance consistently over un-harnessed baselines and reveals distinct contribution patterns, for example, memory dominates in long-horizon puzzles while perception is critical in vision noisy arcades. These findings highlight the effectiveness of our modular harness design in advancing general-purpose agent, given the familiarity and ubiquity of games in everyday human experience.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification</title>
<link>https://arxiv.org/abs/2507.11662</link>
<guid>https://arxiv.org/abs/2507.11662</guid>
<content:encoded><![CDATA[
<div> verifiers, multimodal large language models, agreement bias, self-grounded verification, task completion <br />
Summary:
Multimodal Large Language Models (MLLMs) have been proposed as verifiers of agent trajectories in various domains such as web navigation, computer use, and robotic manipulation. However, a critical limitation identified is the agreement bias, where MLLMs tend to favor information in their context window, leading to flawed behavior rationalization. To address this issue, the authors propose Self-Grounded Verification (SGV), a method that leverages the MLLMs' sampling mechanisms for improved reasoning and evaluation. With SGV, MLLM verifiers demonstrate enhanced accuracy and failure detection rates, enabling real-time supervision of heterogeneous agents in different tasks. The application of SGV results in significant performance improvements, setting a new state of the art on benchmarks. <div>
arXiv:2507.11662v1 Announce Type: new 
Abstract: Verifiers -- functions assigning rewards to agent behavior -- have been key for AI progress in domains like math and board games. However, extending these gains to domains without clear-cut success criteria (e.g.,computer use) remains a challenge: while humans can recognize suitable outcomes, translating this intuition into scalable rules is non-trivial. Multimodal Large Language Models(MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers of agent trajectories across web navigation, computer use, and robotic manipulation, and identify a critical limitation: agreement bias, a strong tendency for MLLMs to favor information in their context window, often generating chains of thought to rationalize flawed behavior. This bias is pervasive across models, resilient to test-time scaling, and can impact several methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs despite MLLMs showing strong, human-aligned priors on desired behavior. To address this, we propose Self-Grounded Verification (SGV), a lightweight method that enables more effective use of MLLMs' knowledge and reasoning by harnessing their own sampling mechanisms via unconditional and conditional generation. SGV operates in two steps: first, the MLLM is elicited to retrieve broad priors about task completion, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in accuracy and failure detection rates, and can perform real-time supervision of heterogeneous agents, boosting task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting a new state of the art on the benchmark, surpassing the previous best by 48%.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making</title>
<link>https://arxiv.org/abs/2507.11733</link>
<guid>https://arxiv.org/abs/2507.11733</guid>
<content:encoded><![CDATA[
<div> Keywords: Clarity, Reasoning Interface, Artificial Intelligence, Case-Based Reasoning, Ontology<br />
Summary:<br />
ClarifAI is a new approach designed to enhance transparency and interpretability in artificial intelligence (AI) for improved decision-making. It leverages Case-Based Reasoning (CBR) and an ontology-driven approach to provide comprehensive explanations to different stakeholders in AI applications. The theoretical foundations of ClarifAI combine CBR and ontologies to create detailed explanation mechanisms. The design principles and architectural blueprint highlight how ClarifAI can enhance AI interpretability across various sectors and in high-stake environments. This research emphasizes the significant role of ClarifAI in advancing AI system interpretability and its potential deployment in critical decision-making processes. <br /><br />Summary: <div>
arXiv:2507.11733v1 Announce Type: new 
Abstract: This Study introduces Clarity and Reasoning Interface for Artificial Intelligence(ClarifAI), a novel approach designed to augment the transparency and interpretability of artificial intelligence (AI) in the realm of improved decision making. Leveraging the Case-Based Reasoning (CBR) methodology and integrating an ontology-driven approach, ClarifAI aims to meet the intricate explanatory demands of various stakeholders involved in AI-powered applications. The paper elaborates on ClarifAI's theoretical foundations, combining CBR and ontologies to furnish exhaustive explanation mechanisms. It further elaborates on the design principles and architectural blueprint, highlighting ClarifAI's potential to enhance AI interpretability across different sectors and its applicability in high-stake environments. This research delineates the significant role of ClariAI in advancing the interpretability of AI systems, paving the way for its deployment in critical decision-making processes.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-Formulating Dynamic Programming Problems with Large Language Models</title>
<link>https://arxiv.org/abs/2507.11737</link>
<guid>https://arxiv.org/abs/2507.11737</guid>
<content:encoded><![CDATA[
<div> Benchmarking, Dynamic Programming, Large Language Models, Synthetic Data Generation, DualReflect

Summary: 
Dynamic programming is a key method in operations research, requiring expert knowledge for formulation. To automate this process, a novel benchmark DP-Bench is introduced to evaluate a range of DP problems systematically. The Dynamic Programming Language Model (DPLM), a specialized 7B-parameter model, achieves competitive performance with state-of-the-art LLMs, surpassing them on challenging problems. The DualReflect synthetic data pipeline enhances training data scalability by combining forward and backward generation for diversity and reliability. Results show that in low-data scenarios, backward generation is preferred for correctness guarantees, while forward generation becomes more valuable at scale for diverse formulations. This trade-off highlights the importance of integrating both approaches for optimal performance in DP problem-solving. 

<br /><br />Summary: <div>
arXiv:2507.11737v1 Announce Type: new 
Abstract: Dynamic programming (DP) is a fundamental method in operations research, but formulating DP models has traditionally required expert knowledge of both the problem context and DP techniques. Large Language Models (LLMs) offer the potential to automate this process. However, DP problems pose unique challenges due to their inherently stochastic transitions and the limited availability of training data. These factors make it difficult to directly apply existing LLM-based models or frameworks developed for other optimization problems, such as linear or integer programming. We introduce DP-Bench, the first benchmark covering a wide range of textbook-level DP problems to enable systematic evaluation. We present Dynamic Programming Language Model (DPLM), a 7B-parameter specialized model that achieves performance comparable to state-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on hard problems. Central to DPLM's effectiveness is DualReflect, our novel synthetic data generation pipeline, designed to scale up training data from a limited set of initial examples. DualReflect combines forward generation for diversity and backward generation for reliability. Our results reveal a key insight: backward generation is favored in low-data regimes for its strong correctness guarantees, while forward generation, though lacking such guarantees, becomes increasingly valuable at scale for introducing diverse formulations. This trade-off highlights the complementary strengths of both approaches and the importance of combining them.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity</title>
<link>https://arxiv.org/abs/2507.11787</link>
<guid>https://arxiv.org/abs/2507.11787</guid>
<content:encoded><![CDATA[
<div> Keywords: Swarm Intelligence, artificial intelligence, computer algorithms, optimization problems, semantic similarity<br />
Summary:<br />
Swarm Intelligence (SI) is a growing field in artificial intelligence that mimics the behavior of animals and insects to solve real-world problems through computer algorithms. These algorithms, known as swarm computing, have proven effective in tackling various optimization problems in computer science. This survey explores the latest advancements in using Swarm Intelligence algorithms to search for documents based on semantic similarity. The review discusses current research findings and offers insights into future research directions in this area. Swarm Intelligence has shown promising results in information retrieval tasks, showcasing its potential for enhancing search algorithms and improving document retrieval efficiency. By leveraging the collective intelligence of swarms, researchers aim to further optimize document search processes and develop more efficient algorithms for semantic similarity matching. <div>
arXiv:2507.11787v1 Announce Type: new 
Abstract: Swarm Intelligence (SI) is gaining a lot of popularity in artificial intelligence, where the natural behavior of animals and insects is observed and translated into computer algorithms called swarm computing to solve real-world problems. Due to their effectiveness, they are applied in solving various computer optimization problems. This survey will review all the latest developments in Searching for documents based on semantic similarity using Swarm Intelligence algorithms and recommend future research directions.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS</title>
<link>https://arxiv.org/abs/2507.11916</link>
<guid>https://arxiv.org/abs/2507.11916</guid>
<content:encoded><![CDATA[
<div> algorithm, GPU, depth first search, neural networks, heuristic admissibility
Summary:
The paper discusses the utilization of GPU technology in enhancing classic search algorithms, particularly focusing on compressing large pattern database (PDB) heuristics using neural networks while maintaining heuristic admissibility. A method for batching GPU computations in depth-first search, called cost-bounded depth-first search (CB-DFS), is introduced. This method leverages the parallelism of modern CPUs and GPUs to create algorithms such as Batch IDA* and Batch BTS, which build on the AIDA* approach while ensuring optimality guarantees. The evaluation on the Rubik's Cube and sliding tile puzzle shows efficient batching of GPU operations in DFS. Extensive experiments analyze the impact of hyperparameters, neural network heuristic size, and hardware resources on performance.<br /><br />Summary: <div>
arXiv:2507.11916v1 Announce Type: new 
Abstract: The rapid advancement of GPU technology has unlocked powerful parallel processing capabilities, creating new opportunities to enhance classic search algorithms. A recent successful application of GPUs is in compressing large pattern database (PDB) heuristics using neural networks while preserving heuristic admissibility. However, very few algorithms have been designed to exploit GPUs during search. Several variants of A* exist that batch GPU computations. In this paper we introduce a method for batching GPU computations in depth first search. In particular, we describe a new cost-bounded depth-first search (CB-DFS) method that leverages the combined parallelism of modern CPUs and GPUs. This is used to create algorithms like \emph{Batch IDA*}, an extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an extensions of Budgeted Tree Search. Our approach builds on the general approach used by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality guarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding tile puzzle (STP), showing that GPU operations can be efficiently batched in DFS. Additionally, we conduct extensive experiments to analyze the effects of hyperparameters, neural network heuristic size, and hardware resources on performance.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aime: Towards Fully-Autonomous Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2507.11988</link>
<guid>https://arxiv.org/abs/2507.11988</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agent Systems, Large Language Models, Dynamic Planning, Adaptive Architecture, Task Success Rate <br />
Summary: <br />
Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are being enhanced with a novel framework called Aime. Aime overcomes limitations of traditional plan-and-execute systems by introducing dynamic planning and execution. Its innovative features include a Dynamic Planner for real-time strategy refinement, an Actor Factory for on-demand agent assembly, and a central Progress Management Module for system-wide state awareness. Empirical evaluations on various benchmarks show that Aime outperforms specialized agents in reasoning, software engineering, and web navigation tasks, demonstrating superior adaptability and task success rates. Aime proves to be a resilient and effective foundation for multi-agent collaboration. <br /> <div>
arXiv:2507.11988v1 Announce Type: new 
Abstract: Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are emerging as a powerful paradigm for solving complex, multifaceted problems. However, the potential of these systems is often constrained by the prevalent plan-and-execute framework, which suffers from critical limitations: rigid plan execution, static agent capabilities, and inefficient communication. These weaknesses hinder their adaptability and robustness in dynamic environments. This paper introduces Aime, a novel multi-agent framework designed to overcome these challenges through dynamic, reactive planning and execution. Aime replaces the conventional static workflow with a fluid and adaptive architecture. Its core innovations include: (1) a Dynamic Planner that continuously refines the overall strategy based on real-time execution feedback; (2) an Actor Factory that implements Dynamic Actor instantiation, assembling specialized agents on-demand with tailored tools and knowledge; and (3) a centralized Progress Management Module that serves as a single source of truth for coherent, system-wide state awareness. We empirically evaluated Aime on a diverse suite of benchmarks spanning general reasoning (GAIA), software engineering (SWE-bench Verified), and live web navigation (WebVoyager). The results demonstrate that Aime consistently outperforms even highly specialized state-of-the-art agents in their respective domains. Its superior adaptability and task success rate establish Aime as a more resilient and effective foundation for multi-agent collaboration.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding visual attention beehind bee-inspired UAV navigation</title>
<link>https://arxiv.org/abs/2507.11992</link>
<guid>https://arxiv.org/abs/2507.11992</guid>
<content:encoded><![CDATA[
<div> Keywords: Bio-inspired design, Autonomous UAV navigation, Reinforcement Learning, Optic flow, Obstacle avoidance

Summary: 
In this study, researchers explore the use of bio-inspired design in training a Reinforcement Learning agent to navigate a cluttered tunnel with obstacles using only optic flow as sensory input. The agent learns to pay attention to regions of discontinuity and large optic flow magnitude in its visual field to make motor decisions. The trained agents successfully navigate the tunnel by avoiding obstacles that produce large optic flow while maintaining a centered position, similar to the behavior of flying insects. This strategy proves effective across independently trained agents, suggesting its potential as a simple explicit control law for physical UAVs. The study showcases the capacity of bio-inspired design in mimicking biological systems' navigation abilities despite limited sensory and computational capabilities. Overall, the findings highlight the promising application of bio-inspired design in enhancing autonomous UAV navigation capabilities. 

<br /><br />Summary: <div>
arXiv:2507.11992v1 Announce Type: new 
Abstract: Bio-inspired design is often used in autonomous UAV navigation due to the capacity of biological systems for flight and obstacle avoidance despite limited sensory and computational capabilities. In particular, honeybees mainly use the sensory input of optic flow, the apparent motion of objects in their visual field, to navigate cluttered environments. In our work, we train a Reinforcement Learning agent to navigate a tunnel with obstacles using only optic flow as sensory input. We inspect the attention patterns of trained agents to determine the regions of optic flow on which they primarily base their motor decisions. We find that agents trained in this way pay most attention to regions of discontinuity in optic flow, as well as regions with large optic flow magnitude. The trained agents appear to navigate a cluttered tunnel by avoiding the obstacles that produce large optic flow, while maintaining a centered position in their environment, which resembles the behavior seen in flying insects. This pattern persists across independently trained agents, which suggests that this could be a good strategy for developing a simple explicit control law for physical UAVs.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs</title>
<link>https://arxiv.org/abs/2507.12110</link>
<guid>https://arxiv.org/abs/2507.12110</guid>
<content:encoded><![CDATA[
<div> topology-enhanced MARL, cooperative decision-making, connected and autonomous vehicles, dynamic traffic flow, QMIX algorithm
Summary:
The paper introduces a topology-enhanced MARL (TPE-MARL) method for optimizing cooperative decision-making among connected and autonomous vehicles (CAVs) in mixed traffic. It addresses the exploration-exploitation trade-off challenge in multi-agent reinforcement learning by utilizing a game topology tensor for dynamic traffic flow, which compresses high-dimensional traffic state information. The TPE-MARL framework, based on the QMIX algorithm, incorporates visit counts and agent mutual information to balance exploration and exploitation. Through extensive simulations at varying traffic densities and CAV penetration rates, TPE-MARL demonstrates superior performance in traffic efficiency, safety, decision smoothness, and task completion. The algorithm also shows decision-making rationality comparable to or exceeding that of human drivers in both mixed-autonomy and fully autonomous traffic scenarios. The code for TPE-MARL implementation is available on GitHub.<br /><br />Summary: <div>
arXiv:2507.12110v1 Announce Type: new 
Abstract: The exploration-exploitation trade-off constitutes one of the fundamental challenges in reinforcement learning (RL), which is exacerbated in multi-agent reinforcement learning (MARL) due to the exponential growth of joint state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL) method for optimizing cooperative decision-making of connected and autonomous vehicles (CAVs) in mixed traffic. This work presents two primary contributions: First, we construct a game topology tensor for dynamic traffic flow, effectively compressing high-dimensional traffic state information and decrease the search space for MARL algorithms. Second, building upon the designed game topology tensor and using QMIX as the backbone RL algorithm, we establish a topology-enhanced MARL framework incorporating visit counts and agent mutual information. Extensive simulations across varying traffic densities and CAV penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations encompassing training dynamics, exploration patterns, macroscopic traffic performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL successfully balances exploration and exploitation. Consequently, it exhibits superior performance in terms of traffic efficiency, safety, decision smoothness, and task completion. Furthermore, the algorithm demonstrates decision-making rationality comparable to or exceeding that of human drivers in both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is available at \href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation</title>
<link>https://arxiv.org/abs/2507.12186</link>
<guid>https://arxiv.org/abs/2507.12186</guid>
<content:encoded><![CDATA[
<div> Proposed Method, Online Approximate POMDP Solver, Theoretical Guarantees, Empirical Evaluations, Outperforms Benchmarks
<br />
<br />
Summary: This paper introduces a new approach called Partially Observable Reference Policy Programming for solving partially observable Markov decision processes (POMDPs) online. The algorithm efficiently samples future histories and updates policies gradually, providing theoretical guarantees that the performance loss is bounded by the average of sampling errors. Empirical evaluations on complex problems with changing environments demonstrate the effectiveness of the proposed method, particularly in a helicopter emergency scenario that needed around 150 planning steps. The results support the theoretical findings and show that the solver surpasses existing online benchmarks in terms of performance. <div>
arXiv:2507.12186v1 Announce Type: new 
Abstract: This paper proposes Partially Observable Reference Policy Programming, a novel anytime online approximate POMDP solver which samples meaningful future histories very deeply while simultaneously forcing a gradual policy update. We provide theoretical guarantees for the algorithm's underlying scheme which say that the performance loss is bounded by the average of the sampling approximation errors rather than the usual maximum, a crucial requirement given the sampling sparsity of online planning. Empirical evaluations on two large-scale problems with dynamically evolving environments -- including a helicopter emergency scenario in the Corsica region requiring approximately 150 planning steps -- corroborate the theoretical results and indicate that our solver considerably outperforms current online benchmarks.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution</title>
<link>https://arxiv.org/abs/2507.12207</link>
<guid>https://arxiv.org/abs/2507.12207</guid>
<content:encoded><![CDATA[
<div> Keywords: building energy forecasting, Large Language Models (LLMs), evolutionary process, physical principles, transparent prediction logic

Summary:
BuildEvo is a new framework introduced in this paper that utilizes Large Language Models (LLMs) to automatically create accurate and interpretable heuristics for building energy forecasting. By integrating physical insights from building characteristics and operational data, BuildEvo guides LLMs through an evolutionary process to develop effective prediction models. The framework demonstrates state-of-the-art performance on benchmarks, showcasing improved generalization and transparent prediction logic. This approach advances the automation of robust and physically grounded heuristics, facilitating the creation of trustworthy models for complex energy systems. <div>
arXiv:2507.12207v1 Announce Type: new 
Abstract: Accurate building energy forecasting is essential, yet traditional heuristics often lack precision, while advanced models can be opaque and struggle with generalization by neglecting physical principles. This paper introduces BuildEvo, a novel framework that uses Large Language Models (LLMs) to automatically design effective and interpretable energy prediction heuristics. Within an evolutionary process, BuildEvo guides LLMs to construct and enhance heuristics by systematically incorporating physical insights from building characteristics and operational data (e.g., from the Building Data Genome Project 2). Evaluations show BuildEvo achieves state-of-the-art performance on benchmarks, offering improved generalization and transparent prediction logic. This work advances the automated design of robust, physically grounded heuristics, promoting trustworthy models for complex energy systems.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.12215</link>
<guid>https://arxiv.org/abs/2507.12215</guid>
<content:encoded><![CDATA[
<div> Chinese Chess, Large Language Models, Strategic Reasoning, Xiangqi-R1, Spatial Complexity 
<br />
Summary:
In this study, the effectiveness of Large Language Models (LLMs) in spatial strategic reasoning is explored using Chinese Chess (Xiangqi) as a testbed. A training framework tailored to Xiangqi is proposed, utilizing a dataset of five million board-move pairs with expert annotations. The Xiangqi-R1 model, with 7B parameters, undergoes multi-stage training incorporating fine-tuning, strategic annotations, and reinforcement learning to enhance reasoning stability. Results show general-purpose LLMs struggle in spatially complex tasks, while Xiangqi-R1 shows significant improvement with an increase in move legality and analysis accuracy. This research highlights the potential for developing general strategic intelligence in complex spatial environments. 
<br /> <div>
arXiv:2507.12215v1 Announce Type: new 
Abstract: Game playing has long served as a fundamental benchmark for evaluating Artificial General Intelligence (AGI). While Large Language Models (LLMs) have demonstrated impressive capabilities in general reasoning, their effectiveness in spatial strategic reasoning, which is critical for complex and fully observable board games, remains insufficiently explored. In this work, we adopt Chinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate rules and spatial complexity. To advance LLMs' strategic competence in such environments, we propose a training framework tailored to Xiangqi, built upon a large-scale dataset of five million board-move pairs enhanced with expert annotations and engine evaluations. Building on this foundation, we introduce Xiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning for legal move prediction to capture basic spatial rules, (2) incorporating strategic annotations to improve decision-making, and (3) applying reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional reward signals to enhance reasoning stability. Our Experimental results indicate that, despite their size and power, general-purpose LLMs struggle to achieve satisfactory performance in these tasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an 18% rise in move legality and a 22% boost in analysis accuracy. Our results point to a promising path for creating general strategic intelligence in spatially complex areas.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of Generative AI in Computer Science Education: Challenges and Opportunities in Accuracy, Authenticity, and Assessment</title>
<link>https://arxiv.org/abs/2507.11543</link>
<guid>https://arxiv.org/abs/2507.11543</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, computer science education, accuracy, authenticity, assessment, AI literacy

Summary:
Generative AI tools like ChatGPT and Claude are being increasingly used in computer science education, providing benefits in efficiency and supporting student creativity. However, challenges such as AI hallucinations, error propagation, bias, and blurring of AI-assisted and student-authored content require human oversight. Literature suggests the need for hybrid assessment models combining AI and human evaluation, bias detection frameworks, and promoting AI literacy among both students and educators. The successful integration of AI in education necessitates a balanced approach, considering ethical, pedagogical, and technical factors. Future research directions include enhancing AI accuracy, ensuring academic integrity, and developing adaptive models that strike a balance between creativity and precision. <br /><br />Summary: <div>
arXiv:2507.11543v1 Announce Type: cross 
Abstract: This paper surveys the use of Generative AI tools, such as ChatGPT and Claude, in computer science education, focusing on key aspects of accuracy, authenticity, and assessment. Through a literature review, we highlight both the challenges and opportunities these AI tools present. While Generative AI improves efficiency and supports creative student work, it raises concerns such as AI hallucinations, error propagation, bias, and blurred lines between AI-assisted and student-authored content. Human oversight is crucial for addressing these concerns. Existing literature recommends adopting hybrid assessment models that combine AI with human evaluation, developing bias detection frameworks, and promoting AI literacy for both students and educators. Our findings suggest that the successful integration of AI requires a balanced approach, considering ethical, pedagogical, and technical factors. Future research may explore enhancing AI accuracy, preserving academic integrity, and developing adaptive models that balance creativity with precision.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening</title>
<link>https://arxiv.org/abs/2507.11548</link>
<guid>https://arxiv.org/abs/2507.11548</guid>
<content:encoded><![CDATA[
<div> platforms, AI, bias, competence, audit

Summary:<br /><br />
This study investigates the use of generative AI for resume screening and questions the assumption that AI offers unbiased decision-making. The research conducted a two-part audit of eight major AI platforms. Experiment 1 revealed complex racial and gender biases, with some models penalizing candidates based on demographic signals. Experiment 2 found that some seemingly unbiased models lacked core competence and relied on superficial keyword matching for evaluation. This phenomenon is termed the "Illusion of Neutrality," where a lack of bias masks a lack of meaningful judgment. The study recommends organizations and regulators adopt a dual-validation framework to audit AI hiring tools for both bias and competence to ensure fairness and effectiveness. <div>
arXiv:2507.11548v1 Announce Type: cross 
Abstract: The increasing use of generative AI for resume screening is predicated on the assumption that it offers an unbiased alternative to biased human decision-making. However, this belief fails to address a critical question: are these AI systems fundamentally competent at the evaluative tasks they are meant to perform? This study investigates the question of competence through a two-part audit of eight major AI platforms. Experiment 1 confirmed complex, contextual racial and gender biases, with some models penalizing candidates merely for the presence of demographic signals. Experiment 2, which evaluated core competence, provided a critical insight: some models that appeared unbiased were, in fact, incapable of performing a substantive evaluation, relying instead on superficial keyword matching. This paper introduces the "Illusion of Neutrality" to describe this phenomenon, where an apparent lack of bias is merely a symptom of a model's inability to make meaningful judgments. This study recommends that organizations and regulators adopt a dual-validation framework, auditing AI hiring tools for both demographic bias and demonstrable competence to ensure they are both equitable and effective.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search</title>
<link>https://arxiv.org/abs/2507.11549</link>
<guid>https://arxiv.org/abs/2507.11549</guid>
<content:encoded><![CDATA[
<div> Keywords: Deformable Attention Transformers, hardware optimization, neural architecture search, FPGA, ImageNet-1K dataset

Summary:
Deformable Attention Transformers (DAT) have shown impressive performance in computer vision tasks by focusing on informative image regions. However, their data-dependent sampling mechanism leads to irregular memory access patterns, challenging hardware deployment. This paper presents a hardware-friendly optimization framework for DAT. It introduces a neural architecture search (NAS)-based method with a new slicing strategy to automatically divide input features into uniform patches during inference, optimizing hardware cost and inference accuracy. Additionally, an FPGA-based verification system tests the framework's performance on edge-side hardware. Results show that the hardware-friendly framework maintains a 0.2% accuracy drop compared to the baseline DAT, while reducing DRAM access times by 82% on Xilinx FPGA, outperforming existing acceleration methods.<br /><br />Summary: <div>
arXiv:2507.11549v1 Announce Type: cross 
Abstract: Deformable Attention Transformers (DAT) have shown remarkable performance in computer vision tasks by adaptively focusing on informative image regions. However, their data-dependent sampling mechanism introduces irregular memory access patterns, posing significant challenges for efficient hardware deployment. Existing acceleration methods either incur high hardware overhead or compromise model accuracy. To address these issues, this paper proposes a hardware-friendly optimization framework for DAT. First, a neural architecture search (NAS)-based method with a new slicing strategy is proposed to automatically divide the input feature into uniform patches during the inference process, avoiding memory conflicts without modifying model architecture. The method explores the optimal slice configuration by jointly optimizing hardware cost and inference accuracy. Secondly, an FPGA-based verification system is designed to test the performance of this framework on edge-side hardware. Algorithm experiments on the ImageNet-1K dataset demonstrate that our hardware-friendly framework can maintain have only 0.2% accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA show the proposed method reduces DRAM access times to 18% compared with existing DAT acceleration methods.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction</title>
<link>https://arxiv.org/abs/2507.11550</link>
<guid>https://arxiv.org/abs/2507.11550</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatio-temporal traffic prediction, Graph Neural Networks, Convolutional Neural Networks, Deformable Dynamic Convolution Network, Scalability

Summary: 
Deformable Dynamic Convolution Network (DDCN) is proposed for accurate and efficient spatio-temporal traffic prediction in urban areas. Traditional methods struggle with capturing heterogeneous traffic patterns across different regions and time periods. DDCN overcomes these limitations by dynamically applying deformable filters based on offset, allowing for better modeling of non-Euclidean spatial structures. The encoder-decoder structure of DDCN, with spatial and spatio-temporal attention blocks in the encoder, emphasizes important features. The decoder, consisting of a feed-forward module, complements the encoder's output. In experiments on four real-world datasets, DDCN shows competitive performance, demonstrating the potential and effectiveness of CNN-based approaches for spatio-temporal traffic prediction.<br /><br />Summary: <div>
arXiv:2507.11550v1 Announce Type: cross 
Abstract: Spatio-temporal traffic prediction plays a key role in intelligent transportation systems by enabling accurate prediction in complex urban areas. Although not only accuracy but also efficiency for scalability is important, some previous methods struggle to capture heterogeneity such as varying traffic patterns across regions and time periods. Moreover, Graph Neural Networks (GNNs), which are the mainstream of traffic prediction, not only require predefined adjacency matrix, but also limit scalability to large-scale data containing many nodes due to their inherent complexity. To overcome these limitations, we propose Deformable Dynamic Convolution Network (DDCN) for accurate yet efficient traffic prediction. Traditional Convolutional Neural Networks (CNNs) are limited in modeling non-Euclidean spatial structures and spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically applying deformable filters based on offset. Specifically, DDCN decomposes transformer-style CNN to encoder-decoder structure, and applies proposed approaches to the spatial and spatio-temporal attention blocks of the encoder to emphasize important features. The decoder, composed of feed-forward module, complements the output of the encoder. This novel structure make DDCN can perform accurate yet efficient traffic prediction. In comprehensive experiments on four real-world datasets, DDCN achieves competitive performance, emphasizing the potential and effectiveness of CNN-based approaches for spatio-temporal traffic prediction.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Landmark Detection for Medical Images using a General-purpose Segmentation Model</title>
<link>https://arxiv.org/abs/2507.11551</link>
<guid>https://arxiv.org/abs/2507.11551</guid>
<content:encoded><![CDATA[
<div> YOLO, SAM, landmark segmentation, medical imaging, orthopaedic<br />
<br />
Summary: 
The article discusses the limitations of using SAM and MedSAM for landmark segmentation in orthopaedic pelvic radiographs due to their lack of fine-grained precision. To overcome this, the authors propose a novel approach using the YOLO model for object detection to provide bounding boxes that act as input prompts for SAM. By combining YOLO and SAM, the hybrid model demonstrates excellent performance in accurately segmenting anatomical landmarks and intricate outlines in orthopaedic pelvic radiographs. This approach enables the segmentation of both a small pilot set of eight landmarks and an expanded set of 72 landmarks and 16 regions with complex outlines. The results suggest that this hybrid model can enhance the accuracy and efficiency of landmark detection in medical imaging, particularly in orthopaedic diagnostics. <div>
arXiv:2507.11551v1 Announce Type: cross 
Abstract: Radiographic images are a cornerstone of medical diagnostics in orthopaedics, with anatomical landmark detection serving as a crucial intermediate step for information extraction. General-purpose foundational segmentation models, such as SAM (Segment Anything Model), do not support landmark segmentation out of the box and require prompts to function. However, in medical imaging, the prompts for landmarks are highly specific. Since SAM has not been trained to recognize such landmarks, it cannot generate accurate landmark segmentations for diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has been trained to identify larger anatomical structures, such as organs and their parts, and lacks the fine-grained precision required for orthopaedic pelvic landmarks. To address this limitation, we propose leveraging another general-purpose, non-foundational model: YOLO. YOLO excels in object detection and can provide bounding boxes that serve as input prompts for SAM. While YOLO is efficient at detection, it is significantly outperformed by SAM in segmenting complex structures. In combination, these two models form a reliable pipeline capable of segmenting not only a small pilot set of eight anatomical landmarks but also an expanded set of 72 landmarks and 16 regions with complex outlines, such as the femoral cortical bone and the pelvic inlet. By using YOLO-generated bounding boxes to guide SAM, we trained the hybrid model to accurately segment orthopaedic pelvic radiographs. Our results show that the proposed combination of YOLO and SAM yields excellent performance in detecting anatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models</title>
<link>https://arxiv.org/abs/2507.11554</link>
<guid>https://arxiv.org/abs/2507.11554</guid>
<content:encoded><![CDATA[
<div> alignment methods, diffusion models, Direct Preference Optimization, intractable posterior sampling, compositional image generation<br />
Summary:<br />
The article introduces Inversion-DPO, a novel alignment framework for diffusion models that eliminates the need for reward modeling by utilizing deterministic inversion from winning and losing samples. This approach enhances training precision and efficiency without requiring auxiliary reward models or approximation. Inversion-DPO is applied to text-to-image generation and compositional image generation tasks, showing significant performance improvements over existing methods. A curated dataset with complex structural annotations is used for compositional image generation post-training. The method opens up new possibilities for efficient and high-precision alignment in diffusion models, making them more applicable to realistic generation tasks. The code for Inversion-DPO is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2507.11554v1 Announce Type: cross 
Abstract: Recent advancements in diffusion models (DMs) have been propelled by alignment methods that post-train models to better conform to human preferences. However, these approaches typically require computation-intensive training of a base model and a reward model, which not only incurs substantial computational overhead but may also compromise model accuracy and training efficiency. To address these limitations, we propose Inversion-DPO, a novel alignment framework that circumvents reward modeling by reformulating Direct Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts intractable posterior sampling in Diffusion-DPO with the deterministic inversion from winning and losing samples to noise and thus derive a new post-training paradigm. This paradigm eliminates the need for auxiliary reward models or inaccurate appromixation, significantly enhancing both precision and efficiency of training. We apply Inversion-DPO to a basic task of text-to-image generation and a challenging task of compositional image generation. Extensive experiments show substantial performance improvements achieved by Inversion-DPO compared to existing post-training methods and highlight the ability of the trained generative models to generate high-fidelity compositionally coherent images. For the post-training of compostitional image geneation, we curate a paired dataset consisting of 11,140 images with complex structural annotations and comprehensive scores, designed to enhance the compositional capabilities of generative models. Inversion-DPO explores a new avenue for efficient, high-precision alignment in diffusion models, advancing their applicability to complex realistic generation tasks. Our code is available at https://github.com/MIGHTYEZ/Inversion-DPO
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation</title>
<link>https://arxiv.org/abs/2507.11557</link>
<guid>https://arxiv.org/abs/2507.11557</guid>
<content:encoded><![CDATA[
<div> MRI imaging, integrated approaches, radiation attenuation, MR-to-CT synthesis, 3D Wavelet Latent Diffusion Model<br />
Summary:
- The paper introduces a novel 3D Wavelet Latent Diffusion Model for improving MR-to-CT synthesis methods for whole-body imaging.
- The model performs modality translation in a learned latent space, enhancing capture and reconstruction of fine-scale features.
- Structural and modality-specific characteristics are disentangled to preserve anatomical integrity during the diffusion process.
- A Dual Skip Connection Attention mechanism is introduced to generate high-resolution CT images with improved representation of bony structures and soft-tissue contrast.  

Summary: <div>
arXiv:2507.11557v1 Announce Type: cross 
Abstract: Magnetic Resonance (MR) imaging plays an essential role in contemporary clinical diagnostics. It is increasingly integrated into advanced therapeutic workflows, such as hybrid Positron Emission Tomography/Magnetic Resonance (PET/MR) imaging and MR-only radiation therapy. These integrated approaches are critically dependent on accurate estimation of radiation attenuation, which is typically facilitated by synthesizing Computed Tomography (CT) images from MR scans to generate attenuation maps. However, existing MR-to-CT synthesis methods for whole-body imaging often suffer from poor spatial alignment between the generated CT and input MR images, and insufficient image quality for reliable use in downstream clinical tasks. In this paper, we present a novel 3D Wavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by performing modality translation in a learned latent space. By incorporating a Wavelet Residual Module into the encoder-decoder architecture, we enhance the capture and reconstruction of fine-scale features across image and latent spaces. To preserve anatomical integrity during the diffusion process, we disentangle structural and modality-specific characteristics and anchor the structural component to prevent warping. We also introduce a Dual Skip Connection Attention mechanism within the diffusion model, enabling the generation of high-resolution CT images with improved representation of bony structures and soft-tissue contrast.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting</title>
<link>https://arxiv.org/abs/2507.11558</link>
<guid>https://arxiv.org/abs/2507.11558</guid>
<content:encoded><![CDATA[
<div> Keywords: spatio-temporal forecasting, Vision Foundation Models, dual-branch architecture, Temporal-Aware Token Adapter, Bilateral Cross-Prompt Coordination <br />
Summary:
The paper introduces ST-VFM, a framework for spatio-temporal forecasting using Vision Foundation Models (VFMs). The framework addresses challenges of temporal modeling and modality gap, employing a dual-branch architecture integrating raw and flow inputs. Pre-VFM reprogramming applies a Temporal-Aware Token Adapter for embedding temporal context, while post-VFM reprogramming introduces a Bilateral Cross-Prompt Coordination module for dynamic interaction between branches. Extensive experiments on various datasets show ST-VFM outperforms existing baselines across different VFM backbones like DINO, CLIP, and DEIT. The framework demonstrates effectiveness and robustness in spatio-temporal forecasting, establishing itself as a strong general-purpose solution in the field.<br /><br />Summary: <div>
arXiv:2507.11558v1 Announce Type: cross 
Abstract: Foundation models have achieved remarkable success in natural language processing and computer vision, demonstrating strong capabilities in modeling complex patterns. While recent efforts have explored adapting large language models (LLMs) for time-series forecasting, LLMs primarily capture one-dimensional sequential dependencies and struggle to model the richer spatio-temporal (ST) correlations essential for accurate ST forecasting. In this paper, we present \textbf{ST-VFM}, a novel framework that systematically reprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal forecasting. While VFMs offer powerful spatial priors, two key challenges arise when applying them to ST tasks: (1) the lack of inherent temporal modeling capacity and (2) the modality gap between visual and ST data. To address these, ST-VFM adopts a \emph{dual-branch architecture} that integrates raw ST inputs with auxiliary ST flow inputs, where the flow encodes lightweight temporal difference signals interpretable as dynamic spatial cues. To effectively process these dual-branch inputs, ST-VFM introduces two dedicated reprogramming stages. The \emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token Adapter to embed temporal context and align both branches into VFM-compatible feature spaces. The \emph{post-VFM reprogramming} stage introduces a Bilateral Cross-Prompt Coordination module, enabling dynamic interaction between branches through prompt-based conditioning, thus enriching joint representation learning without modifying the frozen VFM backbone. Extensive experiments on ten spatio-temporal datasets show that ST-VFM outperforms state-of-the-art baselines, demonstrating effectiveness and robustness across VFM backbones (e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong general framework for spatio-temporal forecasting.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing</title>
<link>https://arxiv.org/abs/2507.11560</link>
<guid>https://arxiv.org/abs/2507.11560</guid>
<content:encoded><![CDATA[
<div> Keywords: Industrial Internet of Things, Artificial Intelligence-Generated Content, Edge Computing, Task Offloading, Multi-Agent Deep Deterministic Policy Gradient

Summary:
- The integration of IIoT with AIGC presents opportunities for smart manufacturing but brings challenges related to computation and low-latency demands.
- Traditional generative models on cloud computing struggle to meet real-time requirements in IIoT, leading to the proposal of an edge computing framework for AIGC task offloading.
- The framework considers latency and energy consumption due to AIGC model switching, utilizing IIoT devices as multi-agents to offload tasks to appropriate edge servers with different generative models.
- An algorithm called MADDPG-MATO, based on Multi-Agent Deep Deterministic Policy Gradient, is introduced to minimize latency and energy consumption.
- Experimental results show that MADDPG-MATO outperforms baseline algorithms, reducing latency by 6.98%, energy consumption by 7.12%, and increasing task completion rate by 3.72% across dynamic, high-load IIoT environments.

<br /><br />Summary: <div>
arXiv:2507.11560v1 Announce Type: cross 
Abstract: The integration of the Industrial Internet of Things (IIoT) with Artificial Intelligence-Generated Content (AIGC) offers new opportunities for smart manufacturing, but it also introduces challenges related to computation-intensive tasks and low-latency demands. Traditional generative models based on cloud computing are difficult to meet the real-time requirements of AIGC tasks in IIoT environments, and edge computing can effectively reduce latency through task offloading. However, the dynamic nature of AIGC tasks, model switching delays, and resource constraints impose higher demands on edge computing environments. To address these challenges, this paper proposes an AIGC task offloading framework tailored for IIoT edge computing environments, considering the latency and energy consumption caused by AIGC model switching for the first time. IIoT devices acted as multi-agent collaboratively offload their dynamic AIGC tasks to the most appropriate edge servers deployed with different generative models. A model aware AIGC task offloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient (MADDPG-MATO) is devised to minimize the latency and energy. Experimental results show that MADDPG-MATO outperforms baseline algorithms, achieving an average reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72% increase in task completion rate across four sets of experiments with model numbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is robust and efficient in dynamic, high-load IIoT environments.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach</title>
<link>https://arxiv.org/abs/2507.11561</link>
<guid>https://arxiv.org/abs/2507.11561</guid>
<content:encoded><![CDATA[
<div> Variational autoencoder, pulmonary hypertension, newborns, echocardiography, multi-view learning
Summary:
- Pulmonary hypertension (PH) in newborns is a serious condition characterized by high pressure in the pulmonary arteries, resulting in heart strain.
- Right heart catheterization (RHC) is the standard diagnostic method for PH, but echocardiography is preferred due to its non-invasive nature.
- Echocardiography accuracy depends on the operator and can be subjective, prompting the need for automated detection methods.
- A multi-view variational autoencoder (VAE) approach is employed in this study for PH prediction using echocardiographic videos to improve feature extraction and robustness.
- Results show that the multi-view learning approach outperforms single-view and supervised learning methods, indicating its effectiveness for accurate PH assessment in newborns.<br /><br />Summary: <div>
arXiv:2507.11561v1 Announce Type: cross 
Abstract: Pulmonary hypertension (PH) in newborns is a critical condition characterized by elevated pressure in the pulmonary arteries, leading to right ventricular strain and heart failure. While right heart catheterization (RHC) is the diagnostic gold standard, echocardiography is preferred due to its non-invasive nature, safety, and accessibility. However, its accuracy highly depends on the operator, making PH assessment subjective. While automated detection methods have been explored, most models focus on adults and rely on single-view echocardiographic frames, limiting their performance in diagnosing PH in newborns. While multi-view echocardiography has shown promise in improving PH assessment, existing models struggle with generalizability. In this work, we employ a multi-view variational autoencoder (VAE) for PH prediction using echocardiographic videos. By leveraging the VAE framework, our model captures complex latent representations, improving feature extraction and robustness. We compare its performance against single-view and supervised learning approaches. Our results show improved generalization and classification accuracy, highlighting the effectiveness of multi-view learning for robust PH assessment in newborns.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expert Operational GANS: Towards Real-Color Underwater Image Restoration</title>
<link>https://arxiv.org/abs/2507.11562</link>
<guid>https://arxiv.org/abs/2507.11562</guid>
<content:encoded><![CDATA[
<div> deformation artifacts, underwater image restoration, xOp-GAN, expert generator networks, perceptual confidence score <br />
Summary: <br />
The article introduces xOp-GAN, a novel GAN model designed for underwater image restoration, which addresses the challenges posed by complex light propagation and image degradation. Unlike conventional GAN models, xOp-GAN utilizes multiple expert generator networks, each trained to excel in restoring images of a specific quality range. During the restoration process, the discriminator is used to select the best restored image based on its perceptual confidence score. Experimental results on the LSUI dataset show that xOp-GAN outperforms single-regressor models in terms of PSNR levels, achieving up to 25.16 dB. This approach not only improves restoration performance but also reduces complexity, making it a promising solution for underwater image enhancement tasks. <br /> <div>
arXiv:2507.11562v1 Announce Type: cross 
Abstract: The wide range of deformation artifacts that arise from complex light propagation, scattering, and depth-dependent attenuation makes the underwater image restoration to remain a challenging problem. Like other single deep regressor networks, conventional GAN-based restoration methods struggle to perform well across this heterogeneous domain, since a single generator network is typically insufficient to capture the full range of visual degradations. In order to overcome this limitation, we propose xOp-GAN, a novel GAN model with several expert generator networks, each trained solely on a particular subset with a certain image quality. Thus, each generator can learn to maximize its restoration performance for a particular quality range. Once a xOp-GAN is trained, each generator can restore the input image and the best restored image can then be selected by the discriminator based on its perceptual confidence score. As a result, xOP-GAN is the first GAN model with multiple generators where the discriminator is being used during the inference of the regression task. Experimental results on benchmark Large Scale Underwater Image (LSUI) dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB, surpassing all single-regressor models by a large margin even, with reduced complexity.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Heterogeneous Swarm Control Through Hebbian Learning</title>
<link>https://arxiv.org/abs/2507.11566</link>
<guid>https://arxiv.org/abs/2507.11566</guid>
<content:encoded><![CDATA[
<div> Keywords: Hebbian learning, swarm robotics, heterogeneity, emergence, neural adaptation

Summary: 
Hebbian learning is introduced as a method for swarm robotics, allowing for the automatic emergence of heterogeneity. This biologically inspired form of neural adaptation relies on local information, resolving challenges in learning heterogeneous control. Firstly, Hebbian learning simplifies the attribution of emergent phenomena to individual agents through local learning rules, addressing the micro-macro problem. Secondly, uniform Hebbian learning rules across all swarm members reduce the number of parameters needed, thus scaling with swarm sizes efficiently. Thirdly, evolving Hebbian learning rules based on swarm-level behavior decreases the requirement for extensive prior knowledge in optimizing heterogeneous swarms. The study demonstrates that Hebbian learning promotes natural heterogeneity emergence, leading to swarm-level behavioral switching and enhanced swarm capabilities. Additionally, the evolution of Hebbian learning rules proves to be a viable alternative to Multi Agent Reinforcement Learning in standard benchmarking tasks. 

<br /><br />Summary: <div>
arXiv:2507.11566v1 Announce Type: cross 
Abstract: In this paper, we introduce Hebbian learning as a novel method for swarm robotics, enabling the automatic emergence of heterogeneity. Hebbian learning presents a biologically inspired form of neural adaptation that solely relies on local information. By doing so, we resolve several major challenges for learning heterogeneous control: 1) Hebbian learning removes the complexity of attributing emergent phenomena to single agents through local learning rules, thus circumventing the micro-macro problem; 2) uniform Hebbian learning rules across all swarm members limit the number of parameters needed, mitigating the curse of dimensionality with scaling swarm sizes; and 3) evolving Hebbian learning rules based on swarm-level behaviour minimises the need for extensive prior knowledge typically required for optimising heterogeneous swarms. This work demonstrates that with Hebbian learning heterogeneity naturally emerges, resulting in swarm-level behavioural switching and in significantly improved swarm capabilities. It also demonstrates how the evolution of Hebbian learning rules can be a valid alternative to Multi Agent Reinforcement Learning in standard benchmarking tasks.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?</title>
<link>https://arxiv.org/abs/2507.11569</link>
<guid>https://arxiv.org/abs/2507.11569</guid>
<content:encoded><![CDATA[
<div> pre-trained encoders, breast MRI registration, foundation model-based algorithms, domain-specific training, fine structure accuracy<br />
Summary:<br />
This study evaluates the performance of foundation model-based registration algorithms for breast MRI, a challenging task due to anatomical variation and complex internal structure. Five pre-trained encoders are tested across different breast registration tasks. The results show that foundation model-based algorithms like SAM outperform traditional methods for overall breast alignment but struggle with fine details of fibroglandular tissue. Additional pre-training or fine-tuning on medical images does not always improve registration performance. Further research is needed to understand the impact of domain-specific training on registration and to develop strategies that enhance global alignment and fine structure accuracy. The code used in the study is publicly available on Github. 

<br /><br /> <div>
arXiv:2507.11569v1 Announce Type: cross 
Abstract: Foundation models, pre-trained on large image datasets and capable of capturing rich feature representations, have recently shown potential for zero-shot image registration. However, their performance has mostly been tested in the context of rigid or less complex structures, such as the brain or abdominal organs, and it remains unclear whether these models can handle more challenging, deformable anatomy. Breast MRI registration is particularly difficult due to significant anatomical variation between patients, deformation caused by patient positioning, and the presence of thin and complex internal structure of fibroglandular tissue, where accurate alignment is crucial. Whether foundation model-based registration algorithms can address this level of complexity remains an open question. In this study, we provide a comprehensive evaluation of foundation model-based registration algorithms for breast MRI. We assess five pre-trained encoders, including DINO-v2, SAM, MedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that capture variations in different years and dates, sequences, modalities, and patient disease status (lesion versus no lesion). Our results show that foundation model-based algorithms such as SAM outperform traditional registration baselines for overall breast alignment, especially under large domain shifts, but struggle with capturing fine details of fibroglandular tissue. Interestingly, additional pre-training or fine-tuning on medical or breast-specific images in MedSAM and SSLSAM, does not improve registration performance and may even decrease it in some cases. Further work is needed to understand how domain-specific training influences registration and to explore targeted strategies that improve both global alignment and fine structure accuracy. We also publicly release our code at \href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery</title>
<link>https://arxiv.org/abs/2507.11570</link>
<guid>https://arxiv.org/abs/2507.11570</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, elective spine surgery, length of stay prediction, temporal modeling, interpretability

Summary:<br /><br /> This study developed and evaluated machine learning models for predicting length of stay in elective spine surgery, focusing on the benefits of temporal modeling and model interpretability. The SurgeryLSTM model, a masked bidirectional long short-term memory with attention mechanism, outperformed traditional ML models like linear regression and XGBoost, achieving high predictive accuracy. The attention mechanism improved interpretability by identifying influential temporal segments in the clinical data, helping clinicians understand the key predictors of length of stay. Key predictors identified in the study included bone disorder, chronic kidney disease, and lumbar fusion. The study concludes that temporal modeling with attention mechanisms can significantly improve length of stay predictions, providing higher accuracy and interpretability crucial for clinical adoption. This highlights the potential of integrating attention-based temporal models into hospital planning workflows to enhance discharge readiness and individualized patient care. <div>
arXiv:2507.11570v1 Announce Type: cross 
Abstract: Objective: To develop and evaluate machine learning (ML) models for predicting length of stay (LOS) in elective spine surgery, with a focus on the benefits of temporal modeling and model interpretability. Materials and Methods: We compared traditional ML models (e.g., linear regression, random forest, support vector machine (SVM), and XGBoost) with our developed model, SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an attention, using structured perioperative electronic health records (EHR) data. Performance was evaluated using the coefficient of determination (R2), and key predictors were identified using explainable AI. Results: SurgeryLSTM achieved the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85) and baseline models. The attention mechanism improved interpretability by dynamically identifying influential temporal segments within preoperative clinical sequences, allowing clinicians to trace which events or features most contributed to each LOS prediction. Key predictors of LOS included bone disorder, chronic kidney disease, and lumbar fusion identified as the most impactful predictors of LOS. Discussion: Temporal modeling with attention mechanisms significantly improves LOS prediction by capturing the sequential nature of patient data. Unlike static models, SurgeryLSTM provides both higher accuracy and greater interpretability, which are critical for clinical adoption. These results highlight the potential of integrating attention-based temporal models into hospital planning workflows. Conclusion: SurgeryLSTM presents an effective and interpretable AI solution for LOS prediction in elective spine surgery. Our findings support the integration of temporal, explainable ML approaches into clinical decision support systems to enhance discharge readiness and individualized patient care.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators</title>
<link>https://arxiv.org/abs/2507.11574</link>
<guid>https://arxiv.org/abs/2507.11574</guid>
<content:encoded><![CDATA[
<div> Keywords: uncertainty quantification, deep learning, virtual sensing, Monte Carlo dropout, conformal prediction

Summary:
Robust uncertainty quantification (UQ) is crucial for the safe deployment of deep learning in virtual sensing applications. The Conformalized Monte Carlo Operator (CMCO) framework introduces calibrated prediction intervals for neural operator-based virtual sensing, overcoming challenges of sparse, noisy, or non-collocated sensor data. By combining Monte Carlo dropout and split conformal prediction in a single DeepONet architecture, CMCO provides spatially resolved uncertainty estimates without the need for retraining or ensemble methods. The method achieves near-nominal empirical coverage in various applications such as turbulent flow, elastoplastic deformation, and global cosmic radiation dose estimation. CMCO offers a plug-and-play solution for neural operators, enabling trustworthy inference in digital twins, sensor fusion, and safety-critical monitoring. This breakthrough bridges theory and deployment with minimal computational overhead, establishing a foundation for scalable and uncertainty-aware scientific machine learning. 

<br /><br />Summary: <div>
arXiv:2507.11574v1 Announce Type: cross 
Abstract: Robust uncertainty quantification (UQ) remains a critical barrier to the safe deployment of deep learning in real-time virtual sensing, particularly in high-stakes domains where sparse, noisy, or non-collocated sensor data are the norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework that transforms neural operator-based virtual sensing with calibrated, distribution-free prediction intervals. By unifying Monte Carlo dropout with split conformal prediction in a single DeepONet architecture, CMCO achieves spatially resolved uncertainty estimates without retraining, ensembling, or custom loss design. Our method addresses a longstanding challenge: how to endow operator learning with efficient and reliable UQ across heterogeneous domains. Through rigorous evaluation on three distinct applications: turbulent flow, elastoplastic deformation, and global cosmic radiation dose estimation-CMCO consistently attains near-nominal empirical coverage, even in settings with strong spatial gradients and proxy-based sensing. This breakthrough offers a general-purpose, plug-and-play UQ solution for neural operators, unlocking real-time, trustworthy inference in digital twins, sensor fusion, and safety-critical monitoring. By bridging theory and deployment with minimal computational overhead, CMCO establishes a new foundation for scalable, generalizable, and uncertainty-aware scientific machine learning.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>