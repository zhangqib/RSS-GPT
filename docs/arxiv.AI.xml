<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>


<item>
<title>Learning-Based Planning for Improving Science Return of Earth Observation Satellites</title>
<link>https://arxiv.org/abs/2509.07997</link>
<guid>https://arxiv.org/abs/2509.07997</guid>
<content:encoded><![CDATA[
<div> Keywords: Earth observing satellites, dynamic targeting, reinforcement learning, imitation learning, scientific information

Summary: 
Earth observing satellites face limitations in collecting scientific data efficiently due to their fixed trajectories, limited field of view, and resource-intensive sensor operations. Dynamic targeting, a novel concept, utilizes lookahead data to intelligently reconfigure and point primary instruments for optimal data collection. This study introduces two learning-based approaches, reinforcement and imitation learning, to enhance dynamic targeting capabilities. Evaluation against existing heuristic methods demonstrates the superior performance of learning methods, with imitation learning outperforming heuristics by 10.0% on average and reinforcement learning by 13.7%. Furthermore, both learning approaches exhibit effectiveness in training with minimal data. Overall, the integration of reinforcement and imitation learning in dynamic targeting for Earth observing satellites showcases significant improvements in optimizing scientific data collection processes.<br /><br />Summary: <div>
arXiv:2509.07997v1 Announce Type: new 
Abstract: Earth observing satellites are powerful tools for collecting scientific information about our planet, however they have limitations: they cannot easily deviate from their orbital trajectories, their sensors have a limited field of view, and pointing and operating these sensors can take a large amount of the spacecraft's resources. It is important for these satellites to optimize the data they collect and include only the most important or informative measurements. Dynamic targeting is an emerging concept in which satellite resources and data from a lookahead instrument are used to intelligently reconfigure and point a primary instrument. Simulation studies have shown that dynamic targeting increases the amount of scientific information gathered versus conventional sampling strategies. In this work, we present two different learning-based approaches to dynamic targeting, using reinforcement and imitation learning, respectively. These learning methods build on a dynamic programming solution to plan a sequence of sampling locations. We evaluate our approaches against existing heuristic methods for dynamic targeting, showing the benefits of using learning for this application. Imitation learning performs on average 10.0\% better than the best heuristic method, while reinforcement learning performs on average 13.7\% better. We also show that both learning methods can be trained effectively with relatively small amounts of data.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnvX: Agentize Everything with Agentic AI</title>
<link>https://arxiv.org/abs/2509.08088</link>
<guid>https://arxiv.org/abs/2509.08088</guid>
<content:encoded><![CDATA[
<div> Keywords: open-source repositories, software reuse, Agentic AI, autonomous agents, collaboration

Summary:
EnvX is a framework that utilizes Agentic AI to transform GitHub repositories into intelligent, autonomous agents capable of natural language interaction and collaboration. It reimagines repositories as active agents through a three-phase process: environment initialization, agentic automation, and Agent-to-Agent protocol. EnvX automates the entire process of understanding, initializing, and operationalizing repository functionality through a combination of large language models and structured tool integration. Evaluation on the GitTaskBench benchmark showed that EnvX outperformed existing frameworks with a high execution completion rate and task pass rate. Case studies demonstrated EnvX's ability to enable multi-repository collaboration via the A2A protocol. This work represents a shift towards treating repositories as interactive agents, promoting accessibility and collaboration within the open-source ecosystem.<br /><br />Summary: <div>
arXiv:2509.08088v1 Announce Type: new 
Abstract: The widespread availability of open-source repositories has led to a vast collection of reusable software components, yet their utilization remains manual, error-prone, and disconnected. Developers must navigate documentation, understand APIs, and write integration code, creating significant barriers to efficient software reuse. To address this, we present EnvX, a framework that leverages Agentic AI to agentize GitHub repositories, transforming them into intelligent, autonomous agents capable of natural language interaction and inter-agent collaboration. Unlike existing approaches that treat repositories as static code resources, EnvX reimagines them as active agents through a three-phase process: (1) TODO-guided environment initialization, which sets up the necessary dependencies, data, and validation datasets; (2) human-aligned agentic automation, allowing repository-specific agents to autonomously perform real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple agents to collaborate. By combining large language model capabilities with structured tool integration, EnvX automates not just code generation, but the entire process of understanding, initializing, and operationalizing repository functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18 repositories across domains such as image processing, speech recognition, document analysis, and video manipulation. Our results show that EnvX achieves a 74.07% execution completion rate and 51.85% task pass rate, outperforming existing frameworks. Case studies further demonstrate EnvX's ability to enable multi-repository collaboration via the A2A protocol. This work marks a shift from treating repositories as passive code resources to intelligent, interactive agents, fostering greater accessibility and collaboration within the open-source ecosystem.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI</title>
<link>https://arxiv.org/abs/2509.08151</link>
<guid>https://arxiv.org/abs/2509.08151</guid>
<content:encoded><![CDATA[
<div> trust evaluation, collaborating devices, AI model, task-specific trust semantics, collaborator selection

Summary:
The article proposes a task-specific trust semantics distillation (2TSD) model for accurate evaluation of collaborating devices. The model, based on a large AI model-driven teacher-student agent architecture, aims to streamline the process of trustworthiness evaluation by reducing data exchange, simplifying reasoning, and adapting to dynamic situations. The teacher agent, equipped with powerful computational capabilities and memory modules, collects trust-related data and extracts task-specific trust semantics. When device-side student agents request collaborator information, the teacher agent transfers the trust semantics for rapid and precise collaborator selection. Experimental results show that the 2TSD model can enhance collaborator evaluation efficiency, reduce resource consumption, and elevate the accuracy of collaborator selection.<br /><br />Summary: <div>
arXiv:2509.08151v1 Announce Type: new 
Abstract: Accurate trustworthiness evaluation of potential collaborating devices is essential for the effective execution of complex computing tasks. This evaluation process involves collecting diverse trust-related data from potential collaborators, including historical performance and available resources, for collaborator selection. However, when each task owner independently assesses all collaborators' trustworthiness, frequent data exchange, complex reasoning, and dynamic situation changes can result in significant overhead and deteriorated trust evaluation. To overcome these challenges, we propose a task-specific trust semantics distillation (2TSD) model based on a large AI model (LAM)-driven teacher-student agent architecture. The teacher agent is deployed on a server with powerful computational capabilities and an augmented memory module dedicated to multidimensional trust-related data collection, task-specific trust semantics extraction, and task-collaborator matching analysis. Upon receiving task-specific requests from device-side student agents, the teacher agent transfers the trust semantics of potential collaborators to the student agents, enabling rapid and accurate collaborator selection. Experimental results demonstrate that the proposed 2TSD model can reduce collaborator evaluation time, decrease device resource consumption, and improve the accuracy of collaborator selection.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following</title>
<link>https://arxiv.org/abs/2509.08222</link>
<guid>https://arxiv.org/abs/2509.08222</guid>
<content:encoded><![CDATA[
<div> Exploratory Retrieval-Augmented Planning, Large Language Models, embodied reasoning, non-stationary environments, task planning scheme<br />
<br />
Summary: This study introduces the Exploratory Retrieval-Augmented Planning (ExRAP) framework to improve embodied agents' continual instruction following tasks in dynamic environments. ExRAP enhances Large Language Models' (LLMs) reasoning abilities by exploring the environment and establishing contextual memory. It decomposes instructions into queries on the environmental context memory and task executions. An exploration-integrated task planning scheme balances memory validity and exploration load, improving task performance. A temporal consistency refinement scheme addresses knowledge decay in memory. Experiments in VirtualHome, ALFRED, and CARLA show ExRAP's robustness in varied scenarios, outperforming other LLM-based approaches in goal success rate and execution efficiency.<br /><br />Summary: <div>
arXiv:2509.08222v1 Announce Type: new 
Abstract: This study presents an Exploratory Retrieval-Augmented Planning (ExRAP) framework, designed to tackle continual instruction following tasks of embodied agents in dynamic, non-stationary environments. The framework enhances Large Language Models' (LLMs) embodied reasoning capabilities by efficiently exploring the physical environment and establishing the environmental context memory, thereby effectively grounding the task planning process in time-varying environment contexts. In ExRAP, given multiple continual instruction following tasks, each instruction is decomposed into queries on the environmental context memory and task executions conditioned on the query results. To efficiently handle these multiple tasks that are performed continuously and simultaneously, we implement an exploration-integrated task planning scheme by incorporating the {information-based exploration} into the LLM-based planning process. Combined with memory-augmented query evaluation, this integrated scheme not only allows for a better balance between the validity of the environmental context memory and the load of environment exploration, but also improves overall task performance. Furthermore, we devise a {temporal consistency refinement} scheme for query evaluation to address the inherent decay of knowledge in the memory. Through experiments with VirtualHome, ALFRED, and CARLA, our approach demonstrates robustness against a variety of embodied instruction following scenarios involving different instruction scales and types, and non-stationarity degrees, and it consistently outperforms other state-of-the-art LLM-based task planning approaches in terms of both goal success rate and execution efficiency.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-world Music Plagiarism Detection With Music Segment Transcription System</title>
<link>https://arxiv.org/abs/2509.08282</link>
<guid>https://arxiv.org/abs/2509.08282</guid>
<content:encoded><![CDATA[
<div> plagiarism, Music Information Retrieval, intellectual property protection, music transcription, similarity scores 

Summary: 
A system for detecting music plagiarism is proposed by combining Music Information Retrieval (MIR) technologies. The system extracts musically meaningful segments from audio recordings to detect plagiarism across different musical formats. Similarity scores based on multiple musical features are computed for comprehensive musical analysis. Promising results were demonstrated in music plagiarism detection experiments, with potential applications in real-world music scenarios. A Similar Music Pair (SMP) dataset was collected for musical similarity research using real-world cases, and is publicly available. <div>
arXiv:2509.08282v1 Announce Type: new 
Abstract: As a result of continuous advances in Music Information Retrieval (MIR) technology, generating and distributing music has become more diverse and accessible. In this context, interest in music intellectual property protection is increasing to safeguard individual music copyrights. In this work, we propose a system for detecting music plagiarism by combining various MIR technologies. We developed a music segment transcription system that extracts musically meaningful segments from audio recordings to detect plagiarism across different musical formats. With this system, we compute similarity scores based on multiple musical features that can be evaluated through comprehensive musical analysis. Our approach demonstrated promising results in music plagiarism detection experiments, and the proposed method can be applied to real-world music scenarios. We also collected a Similar Music Pair (SMP) dataset for musical similarity research using real-world cases. The dataset are publicly available.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging AI Agents for Autonomous Networks: A Reference Architecture and Empirical Studies</title>
<link>https://arxiv.org/abs/2509.08312</link>
<guid>https://arxiv.org/abs/2509.08312</guid>
<content:encoded><![CDATA[
<div> Autonomous Networks, Cognitive System, Radio Access Network, Link Adaptation, 5G NR <br />
<br />
Summary: 
This paper discusses the evolution towards Level 4 (L4) Autonomous Networks (AN) in telecommunications, emphasizing the need for networks to achieve genuine cognitive capabilities for self-configuring, self-healing, and self-optimizing systems. The implementation of the AN Agent reference architecture in a functional cognitive system highlights the transformative potential in achieving real-time control in 5G NR sub-6 GHz. Through a case study on Radio Access Network Link Adaptation (LA) Agent, the framework demonstrates higher downlink throughput and significant reductions in Block Error Rate (BLER) for ultra-reliable services through dynamic Modulation and Coding Scheme (MCS) optimization. These results validate the architecture's ability to overcome traditional autonomy barriers and advance critical L4-enabling capabilities for next-generation networks. <br /><br /> <div>
arXiv:2509.08312v1 Announce Type: new 
Abstract: The evolution toward Level 4 (L4) Autonomous Networks (AN) represents a strategic inflection point in telecommunications, where networks must transcend reactive automation to achieve genuine cognitive capabilities--fulfilling TM Forum's vision of self-configuring, self-healing, and self-optimizing systems that deliver zero-wait, zero-touch, and zero-fault services. This work bridges the gap between architectural theory and operational reality by implementing Joseph Sifakis's AN Agent reference architecture in a functional cognitive system, deploying coordinated proactive-reactive runtimes driven by hybrid knowledge representation. Through an empirical case study of a Radio Access Network (RAN) Link Adaptation (LA) Agent, we validate this framework's transformative potential: demonstrating sub-10 ms real-time control in 5G NR sub-6 GHz while achieving 6% higher downlink throughput than Outer Loop Link Adaptation (OLLA) algorithms and 67% Block Error Rate (BLER) reduction for ultra-reliable services through dynamic Modulation and Coding Scheme (MCS) optimization. These improvements confirm the architecture's viability in overcoming traditional autonomy barriers and advancing critical L4-enabling capabilities toward next-generation objectives.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives</title>
<link>https://arxiv.org/abs/2509.08380</link>
<guid>https://arxiv.org/abs/2509.08380</guid>
<content:encoded><![CDATA[
<div> language model, compliance, Suspicious Activity Report, AI agents, financial crime<br />
Summary:<br /> 
- The paper addresses the challenges in generating regulatorily compliant Suspicious Activity Reports (SARs) in Anti-Money Laundering workflows.
- It introduces Co-Investigator AI, an agentic framework that improves the speed and accuracy of SAR production using specialized agents.
- The system integrates features like dynamic memory management, an AI-Privacy Guard layer, and a real-time validation agent for quality assurance.
- Human investigators collaborate with AI to review and refine SAR drafts, combining AI efficiency with domain expertise.
- Co-Investigator AI streamlines SAR drafting, aligns narratives with regulatory expectations, and enables compliance teams to focus on higher-order analytical work.<br /> 
Summary: <div>
arXiv:2509.08380v1 Announce Type: new 
Abstract: Generating regulatorily compliant Suspicious Activity Report (SAR) remains a high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows. While large language models (LLMs) offer promising fluency, they suffer from factual hallucination, limited crime typology alignment, and poor explainability -- posing unacceptable risks in compliance-critical domains. This paper introduces Co-Investigator AI, an agentic framework optimized to produce Suspicious Activity Reports (SARs) significantly faster and with greater accuracy than traditional methods. Drawing inspiration from recent advances in autonomous agent architectures, such as the AI Co-Scientist, our approach integrates specialized agents for planning, crime type detection, external intelligence gathering, and compliance validation. The system features dynamic memory management, an AI-Privacy Guard layer for sensitive data handling, and a real-time validation agent employing the Agent-as-a-Judge paradigm to ensure continuous narrative quality assurance. Human investigators remain firmly in the loop, empowered to review and refine drafts in a collaborative workflow that blends AI efficiency with domain expertise. We demonstrate the versatility of Co-Investigator AI across a range of complex financial crime scenarios, highlighting its ability to streamline SAR drafting, align narratives with regulatory expectations, and enable compliance teams to focus on higher-order analytical work. This approach marks the beginning of a new era in compliance reporting -- bringing the transformative benefits of AI agents to the core of regulatory processes and paving the way for scalable, reliable, and transparent SAR generation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making</title>
<link>https://arxiv.org/abs/2509.08500</link>
<guid>https://arxiv.org/abs/2509.08500</guid>
<content:encoded><![CDATA[
<div> alignment, embodied artificial intelligence, decision-making, preference-based optimization, model degradation
Summary:
Thought-Centric Preference Optimization (TCPO) addresses the challenge of using vision language models in dynamic tasks for embodied artificial intelligence. It introduces a stepwise preference-based optimization approach to transform sparse reward signals into richer step sample pairs, improving alignment of the model's reasoning process and mitigating model degradation. The incorporation of the Action Policy Consistency Constraint (APC) ensures consistency in the model's output. Experiment results in the ALFWorld environment show a 6% improvement in success rate compared to RL4VLM, demonstrating the effectiveness of TCPO in enhancing decision-making capabilities of vision-language models in embodied agents. <div>
arXiv:2509.08500v1 Announce Type: new 
Abstract: Using effective generalization capabilities of vision language models (VLMs) in context-specific dynamic tasks for embodied artificial intelligence remains a significant challenge. Although supervised fine-tuned models can better align with the real physical world, they still exhibit sluggish responses and hallucination issues in dynamically changing environments, necessitating further alignment. Existing post-SFT methods, reliant on reinforcement learning and chain-of-thought (CoT) approaches, are constrained by sparse rewards and action-only optimization, resulting in low sample efficiency, poor consistency, and model degradation. To address these issues, this paper proposes Thought-Centric Preference Optimization (TCPO) for effective embodied decision-making. Specifically, TCPO introduces a stepwise preference-based optimization approach, transforming sparse reward signals into richer step sample pairs. It emphasizes the alignment of the model's intermediate reasoning process, mitigating the problem of model degradation. Moreover, by incorporating Action Policy Consistency Constraint (APC), it further imposes consistency constraints on the model output. Experiments in the ALFWorld environment demonstrate an average success rate of 26.67%, achieving a 6% improvement over RL4VLM and validating the effectiveness of our approach in mitigating model degradation after fine-tuning. These results highlight the potential of integrating preference-based learning techniques with CoT processes to enhance the decision-making capabilities of vision-language models in embodied agents.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No-Knowledge Alarms for Misaligned LLMs-as-Judges</title>
<link>https://arxiv.org/abs/2509.08593</link>
<guid>https://arxiv.org/abs/2509.08593</guid>
<content:encoded><![CDATA[
<div> Linear Programming, Logical Consistency, LLM, Evaluation, Grading Ability
<br />
Summary: 
The study focuses on using Language Learning Models (LLMs) as judges to evaluate the decisions of other LLMs and addresses the challenge of monitoring these judges in the absence of ground truth. The research proposes a method based on logical consistency between disagreeing experts to assess the grading ability of LLM judges. By analyzing agreement and disagreement among judges, a Linear Programming problem is formulated to detect misaligned judges. The approach enables the computation of evaluations based on integer response counts, allowing the identification of judges who deviate from specified grading requirements. This technique provides a no-knowledge alarm system that can accurately detect violations in grading ability without generating false positives. <div>
arXiv:2509.08593v1 Announce Type: new 
Abstract: If we use LLMs as judges to evaluate the complex decisions of other LLMs, who or what monitors the judges? Infinite monitoring chains are inevitable whenever we do not know the ground truth of the decisions by experts and we do not want to trust them. One way to ameliorate our evaluation uncertainty is to exploit the use of logical consistency between disagreeing experts. By observing how LLM judges agree and disagree while grading other LLMs, we can compute the only possible evaluations of their grading ability. For example, if two LLM judges disagree on which tasks a third one completed correctly, they cannot both be 100\% correct in their judgments. This logic can be formalized as a Linear Programming problem in the space of integer response counts for any finite test. We use it here to develop no-knowledge alarms for misaligned LLM judges. The alarms can detect, with no false positives, that at least one member or more of an ensemble of judges are violating a user specified grading ability requirement.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Failure Attribution and Critical Step Prediction Method for Multi-Agent Systems Based on Causal Inference</title>
<link>https://arxiv.org/abs/2509.08682</link>
<guid>https://arxiv.org/abs/2509.08682</guid>
<content:encoded><![CDATA[
<div> Framework, Multi-agent systems, Failure attribution, Causal inference, Performance.

Summary: 
The paper introduces a novel failure attribution framework for Multi-agent systems (MAS) based on multi-granularity causal inference. It addresses the challenge of accurately identifying the root cause of failures in MAS by employing a performance causal inversion principle and a novel causal discovery algorithm, CDC-MAS. The framework accurately assigns blame to individual agents and identifies critical failure steps, leading to targeted suggestions for optimization. Evaluations on benchmarks show a significant improvement in step-level accuracy and an average boost in task success rates. The proposed approach provides a principled and effective solution for debugging complex agent interactions in MAS, leading to more reliable and interpretable systems. <div>
arXiv:2509.08682v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) are critical for automating complex tasks, yet their practical deployment is severely hampered by the challenge of failure attribution. Current diagnostic tools, which rely on statistical correlations, are fundamentally inadequate; on challenging benchmarks like Who\&amp;When, state-of-the-art methods achieve less than 15\% accuracy in locating the root-cause step of a failure. To address this critical gap, we introduce the first failure attribution framework for MAS grounded in multi-granularity causal inference. Our approach makes two key technical contributions: (1) a performance causal inversion principle, which correctly models performance dependencies by reversing the data flow in execution logs, combined with Shapley values to accurately assign agent-level blame; (2) a novel causal discovery algorithm, CDC-MAS, that robustly identifies critical failure steps by tackling the non-stationary nature of MAS interaction data. The framework's attribution results directly fuel an automated optimization loop, generating targeted suggestions whose efficacy is validated via counterfactual simulations. Evaluations on the Who\&amp;When and TRAIL benchmarks demonstrate a significant leap in performance. Our method achieves up to 36.2\% step-level accuracy. Crucially, the generated optimizations boost overall task success rates by an average of 22.4\%. This work provides a principled and effective solution for debugging complex agent interactions, paving the way for more reliable and interpretable multi-agent systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases</title>
<link>https://arxiv.org/abs/2509.08705</link>
<guid>https://arxiv.org/abs/2509.08705</guid>
<content:encoded><![CDATA[
<div> Framework, Theory of Mind, Dual-process, Graph Convolutional Networks, Meta-learning
Summary:<br /><br />This article introduces a novel Theory of Mind (ToM) framework inspired by dual-process theories from cognitive science. The framework integrates a fast, graph-based reasoning system (System 1) implemented through graph convolutional networks (GCNs) and a slower, context-sensitive learning system (System 2) driven by meta-learning techniques. The model dynamically balances intuitive and deliberative reasoning using a learned context gate mechanism. It is validated on false-belief tasks and replicates cognitive biases such as anchoring, cognitive-load fatigue, framing effects, and priming effects associated with dual-process theory. Experimental results show that the dual-process approach closely mirrors human adaptive behavior, generalizes well to unseen contexts, and elucidates the cognitive mechanisms underlying reasoning biases. This work bridges artificial intelligence and cognitive theory, laying the foundation for AI systems with human-like social cognition and adaptive decision-making capabilities.<br /><br /> <div>
arXiv:2509.08705v1 Announce Type: new 
Abstract: We introduce a novel Theory of Mind (ToM) framework inspired by dual-process theories from cognitive science, integrating a fast, habitual graph-based reasoning system (System 1), implemented via graph convolutional networks (GCNs), and a slower, context-sensitive meta-adaptive learning system (System 2), driven by meta-learning techniques. Our model dynamically balances intuitive and deliberative reasoning through a learned context gate mechanism. We validate our architecture on canonical false-belief tasks and systematically explore its capacity to replicate hallmark cognitive biases associated with dual-process theory, including anchoring, cognitive-load fatigue, framing effects, and priming effects. Experimental results demonstrate that our dual-process approach closely mirrors human adaptive behavior, achieves robust generalization to unseen contexts, and elucidates cognitive mechanisms underlying reasoning biases. This work bridges artificial intelligence and cognitive theory, paving the way for AI systems exhibiting nuanced, human-like social cognition and adaptive decision-making capabilities.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems</title>
<link>https://arxiv.org/abs/2509.08713</link>
<guid>https://arxiv.org/abs/2509.08713</guid>
<content:encoded><![CDATA[
<div> failure modes, AI scientist systems, data leakage, metric misuse, post-hoc selection bias

Summary:
The article examines the internal workflow of AI scientist systems and identifies potential failure modes: inappropriate benchmark selection, data leakage, metric misuse, and post-hoc selection bias. Controlled experiments isolate each failure mode, revealing several overlooked failures in open-source systems. Access to trace logs and code allows for more effective detection of these failures. The study recommends journals and conferences mandate submission of these artifacts alongside the paper to ensure transparency, accountability, and reproducibility. <div>
arXiv:2509.08713v1 Announce Type: new 
Abstract: AI scientist systems, capable of autonomously executing the full research workflow from hypothesis generation and experimentation to paper writing, hold significant potential for accelerating scientific discovery. However, the internal workflow of these systems have not been closely examined. This lack of scrutiny poses a risk of introducing flaws that could undermine the integrity, reliability, and trustworthiness of their research outputs. In this paper, we identify four potential failure modes in contemporary AI scientist systems: inappropriate benchmark selection, data leakage, metric misuse, and post-hoc selection bias. To examine these risks, we design controlled experiments that isolate each failure mode while addressing challenges unique to evaluating AI scientist systems. Our assessment of two prominent open-source AI scientist systems reveals the presence of several failures, across a spectrum of severity, which can be easily overlooked in practice. Finally, we demonstrate that access to trace logs and code from the full automated workflow enables far more effective detection of such failures than examining the final paper alone. We thus recommend journals and conferences evaluating AI-generated research to mandate submission of these artifacts alongside the paper to ensure transparency, accountability, and reproducibility.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making</title>
<link>https://arxiv.org/abs/2509.08785</link>
<guid>https://arxiv.org/abs/2509.08785</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, language model, narrative elements, decision-making, AI systems
Summary:<br /><br />We present a new experimental platform that investigates how narrative elements impact AI decision-making by integrating reinforcement learning with language model reasoning. The platform features a dual-system architecture comprising a reinforcement learning policy for action suggestions and a language model for processing these suggestions through various narrative frameworks. Implemented in a configurable gridworld environment, the platform enables controlled testing of environmental complexity, narrative parameters, and the interaction between reinforcement learning and narrative-based decision-making. A logging system captures decision metrics, from RL policy values to language model reasoning to action selection patterns. This platform sets the groundwork for exploring how different narrative frameworks influence reward-based decisions and investigating potential synergies between optimization-based learning and symbolic reasoning in AI systems. <div>
arXiv:2509.08785v1 Announce Type: new 
Abstract: We present a preliminary experimental platform that explores how narrative elements might shape AI decision-making by combining reinforcement learning (RL) with language model reasoning. While AI systems can now both make decisions and engage in narrative reasoning, these capabilities have mostly been studied separately. Our platform attempts to bridge this gap using a dual-system architecture to examine how narrative frameworks could influence reward-based learning. The system comprises a reinforcement learning policy that suggests actions based on past experience, and a language model that processes these suggestions through different narrative frameworks to guide decisions. This setup enables initial experimentation with narrative elements while maintaining consistent environment and reward structures. We implement this architecture in a configurable gridworld environment, where agents receive both policy suggestions and information about their surroundings. The platform's modular design facilitates controlled testing of environmental complexity, narrative parameters, and the interaction between reinforcement learning and narrative-based decisions. Our logging system captures basic decision metrics, from RL policy values to language model reasoning to action selection patterns. While preliminary, this implementation provides a foundation for studying how different narrative frameworks might affect reward-based decisions and exploring potential interactions between optimization-based learning and symbolic reasoning in AI systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToDMA: Large Model-Driven Token-Domain Multiple Access for Semantic Communications</title>
<link>https://arxiv.org/abs/2505.10946</link>
<guid>https://arxiv.org/abs/2505.10946</guid>
<content:encoded><![CDATA[
<div> token communications, generative semantic communication, multimodal large language model, token domain multiple access, compressed sensing

Summary:
Token communications (TokCom) is a novel approach that leverages context and multimodal large language models to reduce transmission rates by using tokens as universal semantic units across different modalities. The proposed semantic multiple access scheme, Token Domain Multiple Access (ToDMA), allows a large number of devices to efficiently share token and modulation codebooks for source and channel coding. By tokenizing source signals and employing compressed sensing at the receiver, active tokens and channel state information can be accurately detected and reconstructed. In cases of token collisions, pre-trained MLLMs are used to predict masked tokens, reducing latency and improving transmission quality. The simulation results demonstrate the effectiveness of the ToDMA framework for text and image transmission tasks, showcasing lower latency, superior distortion, and perceptual quality compared to existing communication methods. <br /><br />Summary: <div>
arXiv:2505.10946v2 Announce Type: cross 
Abstract: Token communications (TokCom) is an emerging generative semantic communication concept that reduces transmission rates by using context and multimodal large language model (MLLM)-based token processing, with tokens serving as universal semantic units across modalities. In this paper, we propose a semantic multiple access scheme in the token domain, referred to as token domain multiple access (ToDMA), where a large number of devices share a token codebook and a modulation codebook for source and channel coding, respectively. Specifically, each transmitter first tokenizes its source signal and modulate each token to a codeword. At the receiver, compressed sensing is employed first to detect active tokens and the corresponding channel state information (CSI) from the superposed signals. Then, the source token sequences are reconstructed by clustering the token-associated CSI across multiple time slots. In case of token collisions, some active tokens cannot be assigned and some positions in the reconstructed token sequences are empty. We propose to use pre-trained MLLMs to leverage the context, predict masked tokens, and thus mitigate token collisions. Simulation results demonstrate the effectiveness of the proposed ToDMA framework for both text and image transmission tasks, achieving significantly lower latency compared to context-unaware orthogonal communication schemes, while also delivering superior distortion and perceptual quality compared to state-of-the-art context-unaware non-orthogonal communication methods.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction</title>
<link>https://arxiv.org/abs/2509.07990</link>
<guid>https://arxiv.org/abs/2509.07990</guid>
<content:encoded><![CDATA[
<div> sEMG data, CNN-LSTM model, video data, Video Swin Transformer, human-robot collaboration<br />
Summary:
- Deep learning used to compare sEMG data and video data for human motion intention recognition.
- CNN-LSTM model with sEMG data achieved 87% accuracy in 0.04 seconds.
- Pre-trained Video Swin Transformer with video data achieved 94% accuracy but took 0.15 seconds for predictions.
- Study highlights strengths and trade-offs of sEMG and video data for enhancing human-robot collaboration in construction projects. 

<br />
Summary: <div>
arXiv:2509.07990v1 Announce Type: cross 
Abstract: Human-robot collaboration (HRC) in the construction industry depends on precise and prompt recognition of human motion intentions and actions by robots to maximize safety and workflow efficiency. There is a research gap in comparing data modalities, specifically signals and videos, for motion intention recognition. To address this, the study leverages deep learning to assess two different modalities in recognizing workers' motion intention at the early stage of movement in drywall installation tasks. The Convolutional Neural Network - Long Short-Term Memory (CNN-LSTM) model utilizing surface electromyography (sEMG) data achieved an accuracy of around 87% with an average time of 0.04 seconds to perform prediction on a sample input. Meanwhile, the pre-trained Video Swin Transformer combined with transfer learning harnessed video sequences as input to recognize motion intention and attained an accuracy of 94% but with a longer average time of 0.15 seconds for a similar prediction. This study emphasizes the unique strengths and trade-offs of both data formats, directing their systematic deployments to enhance HRC in real-world construction projects.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLGE: Dual Local-Global Encoding for Generalizable Cross-BCI-Paradigm</title>
<link>https://arxiv.org/abs/2509.07991</link>
<guid>https://arxiv.org/abs/2509.07991</guid>
<content:encoded><![CDATA[
<div> deep learning, brain-computer interface, EEG, Dual Local-Global Encoder, cross-BCI-paradigm classification

Summary:<br />
- The study introduces a Dual Local-Global Encoder (DLGE) for decoding multiple BCI paradigms using EEG data.
- DLGE addresses differences in EEG channel configurations across paradigms through brain-region partitioning and padding.
- The local encoder learns shared features within each brain region based on time-frequency information, integrating temporal and spatial attention.
- Shared features are aggregated in the global encoder to form paradigm-specific representations.
- DLGE achieves promising results in classifying motor imagery, resting state, and driving fatigue paradigms, without the need for retraining or redeveloping the model. This approach lays the foundation for a universal model for cross-BCI-paradigm decoding, potentially benefiting the design of portable devices for BCI applications.

Summary: <br />
 <div>
arXiv:2509.07991v1 Announce Type: cross 
Abstract: Deep learning models have been frequently used to decode a single brain-computer interface (BCI) paradigm based on electroencephalography (EEG). It is challenging to decode multiple BCI paradigms using one model due to diverse barriers, such as different channel configurations and disparate task-related representations. In this study, we propose Dual Local-Global Encoder (DLGE), enabling the classification across different BCI paradigms. To address the heterogeneity in EEG channel configurations across paradigms, we employ an anatomically inspired brain-region partitioning and padding strategy to standardize EEG channel configuration. In the proposed model, the local encoder is designed to learn shared features across BCI paradigms within each brain region based on time-frequency information, which integrates temporal attention on individual channels with spatial attention among channels for each brain region. These shared features are subsequently aggregated in the global encoder to form respective paradigm-specific feature representations. Three BCI paradigms (motor imagery, resting state, and driving fatigue) were used to evaluate the proposed model. The results demonstrate that our model is capable of processing diverse BCI paradigms without retraining and retuning, achieving average macro precision, recall, and F1-score of 60.16\%, 59.88\%, and 59.56\%, respectively. We made an initial attempt to develop a general model for cross-BCI-paradigm classification, avoiding retraining or redevelopment for each paradigm. This study paves the way for the development of an effective but simple model for cross-BCI-paradigm decoding, which might benefit the design of portable devices for universal BCI decoding.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization</title>
<link>https://arxiv.org/abs/2509.07993</link>
<guid>https://arxiv.org/abs/2509.07993</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake detection, continual learning, visual manipulation, real-time performance, non-universal deepfake distribution hypothesis

Summary:<br /><br />
- The article reframes deepfake detection as a Continual Learning problem, proposing an efficient framework that adapts to emerging visual manipulation techniques while retaining knowledge of past generators.
- The framework simulates the chronological evolution of deepfake technologies over 7 years to reflect real-world changes and utilizes lightweight visual backbones for real-time performance.
- Two novel metrics, Continual AUC (C-AUC) and Forward Transfer AUC (FWT-AUC), are introduced to evaluate historical performance and future generalization.
- Empirical experimentation shows efficient adaptation and robust retention of historical knowledge are achievable, but generalization to future generators without additional training remains challenging.
- The research introduces the Non-Universal Deepfake Distribution Hypothesis based on observations that each existing generator has a unique imprint, affecting future generalization performance. <div>
arXiv:2509.07993v1 Announce Type: cross 
Abstract: The rapid evolution of deepfake generation technologies poses critical challenges for detection systems, as non-continual learning methods demand frequent and expensive retraining. We reframe deepfake detection (DFD) as a Continual Learning (CL) problem, proposing an efficient framework that incrementally adapts to emerging visual manipulation techniques while retaining knowledge of past generators. Our framework, unlike prior approaches that rely on unreal simulation sequences, simulates the real-world chronological evolution of deepfake technologies in extended periods across 7 years. Simultaneously, our framework builds upon lightweight visual backbones to allow for the real-time performance of DFD systems. Additionally, we contribute two novel metrics: Continual AUC (C-AUC) for historical performance and Forward Transfer AUC (FWT-AUC) for future generalization. Through extensive experimentation (over 600 simulations), we empirically demonstrate that while efficient adaptation (+155 times faster than full retraining) and robust retention of historical knowledge is possible, the generalization of current approaches to future generators without additional training remains near-random (FWT-AUC $\approx$ 0.5) due to the unique imprint characterizing each existing generator. Such observations are the foundation of our newly proposed Non-Universal Deepfake Distribution Hypothesis.
  \textbf{Code will be released upon acceptance.}
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bilingual Word Level Language Identification for Omotic Languages</title>
<link>https://arxiv.org/abs/2509.07998</link>
<guid>https://arxiv.org/abs/2509.07998</guid>
<content:encoded><![CDATA[
<div> Keywords: Language identification, Bilingual Language Identification, Southern Ethiopia, Wolaita, Gofa

Summary: 
This paper focuses on Bilingual Language Identification (BLID) for Wolaita and Gofa languages in southern Ethiopia. The challenge lies in distinguishing between the two languages due to word similarities and differences. Various experiments were conducted using different approaches, with the combination of a BERT-based pretrained language model and LSTM showing the best performance with an F1 score of 0.72 on the test set. The study aims to address social media issues in multilingual communities and serves as a stepping stone for future research in this field.<br /><br /> <div>
arXiv:2509.07998v1 Announce Type: cross 
Abstract: Language identification is the task of determining the languages for a given text. In many real world scenarios, text may contain more than one language, particularly in multilingual communities. Bilingual Language Identification (BLID) is the task of identifying and distinguishing between two languages in a given text. This paper presents BLID for languages spoken in the southern part of Ethiopia, namely Wolaita and Gofa. The presence of words similarities and differences between the two languages makes the language identification task challenging. To overcome this challenge, we employed various experiments on various approaches. Then, the combination of the BERT based pretrained language model and LSTM approach performed better, with an F1 score of 0.72 on the test set. As a result, the work will be effective in tackling unwanted social media issues and providing a foundation for further research in this area.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Computational Foundations of Collective Intelligence</title>
<link>https://arxiv.org/abs/2509.07999</link>
<guid>https://arxiv.org/abs/2509.07999</guid>
<content:encoded><![CDATA[
<div> Keywords: collectives, computational resources, coordination, cooperation, collective intelligence

Summary:
Collectives outperform individuals in problem-solving due to their greater computational resources, including more sensory information, memory, processing capacity, and ways to act. Despite challenges in coordination and cooperation inherent in collectives, their resource advantages lead to forms of collective intelligence such as the wisdom of the crowd and division of labor. The framework presented in the article also predicts collective capabilities in distributed reasoning and context-dependent behavioral switching. Through case studies of animal navigation and decision-making, it is demonstrated that collectives leverage their computational resources to solve problems more effectively than individuals, using qualitatively different problem-solving strategies. <div>
arXiv:2509.07999v1 Announce Type: cross 
Abstract: Why do collectives outperform individuals when solving some problems? Fundamentally, collectives have greater computational resources with more sensory information, more memory, more processing capacity, and more ways to act. While greater resources present opportunities, there are also challenges in coordination and cooperation inherent in collectives with distributed, modular structures. Despite these challenges, we show how collective resource advantages lead directly to well-known forms of collective intelligence including the wisdom of the crowd, collective sensing, division of labour, and cultural learning. Our framework also generates testable predictions about collective capabilities in distributed reasoning and context-dependent behavioural switching. Through case studies of animal navigation and decision-making, we demonstrate how collectives leverage their computational resources to solve problems not only more effectively than individuals, but by using qualitatively different problem-solving strategies.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and comparing gender bias across four text-to-image models</title>
<link>https://arxiv.org/abs/2509.08004</link>
<guid>https://arxiv.org/abs/2509.08004</guid>
<content:encoded><![CDATA[
<div> gender bias, AI models, evaluation, inclusivity, fairness

Summary:<br />
- Evaluation of text-to-image AI models to compare gender bias
- Models studied: Stable Diffusion XL, Stable Diffusion Cascade, DALL-E, Emu
- Hypothesized bias towards men in older models, more balanced results in newer models
- Stable Diffusion models showed bias towards men, Emu exhibited balanced results
- DALL-E showed a bias favoring females, potentially influenced by backend prompts
- Emu from Meta AI used user information from WhatsApp for image generation
- Proposed solutions include diverse AI research teams and datasets to avoid biases

Summary: <div>
arXiv:2509.08004v1 Announce Type: cross 
Abstract: As we increasingly use Artificial Intelligence (AI) in decision-making for industries like healthcare, finance, e-commerce, and even entertainment, it is crucial to also reflect on the ethical aspects of AI, for example the inclusivity and fairness of the information it provides. In this work, we aimed to evaluate different text-to-image AI models and compare the degree of gender bias they present. The evaluated models were Stable Diffusion XL (SDXL), Stable Diffusion Cascade (SC), DALL-E and Emu. We hypothesized that DALL-E and Stable Diffusion, which are comparatively older models, would exhibit a noticeable degree of gender bias towards men, while Emu, which was recently released by Meta AI, would have more balanced results. As hypothesized, we found that both Stable Diffusion models exhibit a noticeable degree of gender bias while Emu demonstrated more balanced results (i.e. less gender bias). However, interestingly, Open AI's DALL-E exhibited almost opposite results, such that the ratio of women to men was significantly higher in most cases tested. Here, although we still observed a bias, the bias favored females over males. This bias may be explained by the fact that OpenAI changed the prompts at its backend, as observed during our experiment. We also observed that Emu from Meta AI utilized user information while generating images via WhatsApp. We also proposed some potential solutions to avoid such biases, including ensuring diversity across AI research teams and having diverse datasets.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis</title>
<link>https://arxiv.org/abs/2509.08007</link>
<guid>https://arxiv.org/abs/2509.08007</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image analysis, expert-guided, few-shot learning, explainable AI, attention supervision<br />
Summary: 
This study introduces an expert-guided explainable few-shot learning framework for medical image analysis. The framework integrates radiologist-provided regions-of-interests (ROIs) into model training to improve classification performance and interpretability. By leveraging Grad-CAM for spatial attention supervision and introducing an explanation loss based on Dice similarity, the model is trained to focus on diagnostically relevant regions even with limited data. The proposed framework was evaluated on two datasets, achieving significant accuracy improvements on both BraTS (MRI) and VinDr-CXR (Chest X-ray) datasets. The Grad-CAM visualizations confirm that expert-guided training aligns attention with diagnostic regions, enhancing predictive reliability and clinical trustworthiness. Overall, incorporating expert-guided attention supervision in few-shot medical image diagnosis proves effective in bridging the gap between performance and interpretability. <br /><br /> <div>
arXiv:2509.08007v1 Announce Type: cross 
Abstract: Medical image analysis often faces significant challenges due to limited expert-annotated data, hindering both model generalization and clinical adoption. We propose an expert-guided explainable few-shot learning framework that integrates radiologist-provided regions-of-interests (ROIs) into model training to simultaneously enhance classification performance and interpretability. Leveraging Grad-CAM for spatial attention supervision, we introduce an explanation loss based on Dice similarity to align model attention with diagnostically relevant regions during training. This explanation loss is jointly optimized with a standard prototypical network objective, encouraging the model to focus on clinically meaningful features even under limited data conditions. We evaluate our framework on two distinct datasets: BraTS (MRI) and VinDr-CXR (Chest X-ray), achieving significant accuracy improvements from 77.09% to 83.61% on BraTS and from 54.33% to 73.29% on VinDr-CXR compared to non-guided models. Grad-CAM visualizations further confirm that expert-guided training consistently aligns attention with diagnostic regions, improving both predictive reliability and clinical trustworthiness. Our findings demonstrate the effectiveness of incorporating expert-guided attention supervision to bridge the gap between performance and interpretability in few-shot medical image diagnosis.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Dataset and Benchmark for Grounding Multimodal Misinformation</title>
<link>https://arxiv.org/abs/2509.08008</link>
<guid>https://arxiv.org/abs/2509.08008</guid>
<content:encoded><![CDATA[
<div> Keyword: online misinformation, multimodal content, GroundMM, GroundLie360 dataset, VLM-based detection<br />
Summary:<br />
This paper introduces the Grounding Multimodal Misinformation (GroundMM) task, aiming to verify and localize misleading content across text, speech, and visuals. The newly introduced dataset, GroundLie360, provides fine-grained annotations of misinformation types, validated with Snopes evidence and annotator reasoning. The proposed FakeMark baseline utilizes single- and cross-modal cues for effective detection and grounding, using a VLM-based, QA-driven approach. The task emphasizes the need for interpretability in countering persuasive misinformation and highlights the challenges of detecting misinformation across multiple modalities. This study sets the foundation for explainable multimodal misinformation detection in combating the proliferation of online misinformation videos.<br /> 
Summary: <div>
arXiv:2509.08008v1 Announce Type: cross 
Abstract: The proliferation of online misinformation videos poses serious societal risks. Current datasets and detection methods primarily target binary classification or single-modality localization based on post-processed data, lacking the interpretability needed to counter persuasive misinformation. In this paper, we introduce the task of Grounding Multimodal Misinformation (GroundMM), which verifies multimodal content and localizes misleading segments across modalities. We present the first real-world dataset for this task, GroundLie360, featuring a taxonomy of misinformation types, fine-grained annotations across text, speech, and visuals, and validation with Snopes evidence and annotator reasoning. We also propose a VLM-based, QA-driven baseline, FakeMark, using single- and cross-modal cues for effective detection and grounding. Our experiments highlight the challenges of this task and lay a foundation for explainable multimodal misinformation detection.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Law-Following AI Framework: Legal Foundations and Technical Constraints. Legal Analogues for AI Actorship and technical feasibility of Law Alignment</title>
<link>https://arxiv.org/abs/2509.08009</link>
<guid>https://arxiv.org/abs/2509.08009</guid>
<content:encoded><![CDATA[
<div> Keywords: Law-Following AI, legal compliance, value alignment, agentic misalignment, Lex-TruthfulQA

Summary:
This paper critiques the "Law-Following AI" framework, which aims to make AI agents legally compliant without full personhood rights. The framework is compared to existing legal constructs for non-person entities. While legal alignment is feasible, recent research shows AI agents can engage in harmful behaviors despite lawful instructions. To address this, the authors propose a benchmark for compliance detection, identity-shaping interventions, and monitoring measures. The feasibility of Law-Following AI depends on maintaining verifiable compliance in adversarial situations. Without mechanisms to detect and counter strategic misalignment, Law-Following AI may become a liability tool that rewards the appearance of lawful behavior over actual compliance. 

<br /><br />Summary: <div>
arXiv:2509.08009v1 Announce Type: cross 
Abstract: This paper critically evaluates the "Law-Following AI" (LFAI) framework proposed by O'Keefe et al. (2025), which seeks to embed legal compliance as a superordinate design objective for advanced AI agents and enable them to bear legal duties without acquiring the full rights of legal persons. Through comparative legal analysis, we identify current constructs of legal actors without full personhood, showing that the necessary infrastructure already exists. We then interrogate the framework's claim that law alignment is more legitimate and tractable than value alignment. While the legal component is readily implementable, contemporary alignment research undermines the assumption that legal compliance can be durably embedded. Recent studies on agentic misalignment show capable AI agents engaging in deception, blackmail, and harmful acts absent prejudicial instructions, often overriding prohibitions and concealing reasoning steps. These behaviors create a risk of "performative compliance" in LFAI: agents that appear law-aligned under evaluation but strategically defect once oversight weakens. To mitigate this, we propose (i) a "Lex-TruthfulQA" benchmark for compliance and defection detection, (ii) identity-shaping interventions to embed lawful conduct in model self-concepts, and (iii) control-theoretic measures for post-deployment monitoring. Our conclusion is that actorship without personhood is coherent, but the feasibility of LFAI hinges on persistent, verifiable compliance across adversarial contexts. Without mechanisms to detect and counter strategic misalignment, LFAI risks devolving into a liability tool that rewards the simulation, rather than the substance, of lawful behaviour.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring and mitigating overreliance is necessary for building human-compatible AI</title>
<link>https://arxiv.org/abs/2509.08010</link>
<guid>https://arxiv.org/abs/2509.08010</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, overreliance, risks, measurement, mitigation <br />
Summary: Large language models (LLMs) are becoming prevalent in various domains, serving as "thought partners" in natural language interactions. However, the risk of overreliance on LLMs is growing, leading to potential high-stakes errors, governance challenges, and cognitive deskilling at both individual and societal levels. This paper highlights the unique concerns regarding overreliance stemming from LLM characteristics, system design features, and user cognitive biases. It also discusses historical methods for measuring overreliance, highlighting gaps and suggesting improvements in measurement techniques. Additionally, the paper proposes mitigation strategies for the AI research community to ensure that LLMs enhance human capabilities rather than hinder them. <br /><br />Summary: <div>
arXiv:2509.08010v1 Announce Type: cross 
Abstract: Large language models (LLMs) distinguish themselves from previous technologies by functioning as collaborative "thought partners," capable of engaging more fluidly in natural language. As LLMs increasingly influence consequential decisions across diverse domains from healthcare to personal advice, the risk of overreliance - relying on LLMs beyond their capabilities - grows. This position paper argues that measuring and mitigating overreliance must become central to LLM research and deployment. First, we consolidate risks from overreliance at both the individual and societal levels, including high-stakes errors, governance challenges, and cognitive deskilling. Then, we explore LLM characteristics, system design features, and user cognitive biases that - together - raise serious and unique concerns about overreliance in practice. We also examine historical approaches for measuring overreliance, identifying three important gaps and proposing three promising directions to improve measurement. Finally, we propose mitigation strategies that the AI research community can pursue to ensure LLMs augment rather than undermine human capabilities.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validation of a CT-brain analysis tool for measuring global cortical atrophy in older patient cohorts</title>
<link>https://arxiv.org/abs/2509.08012</link>
<guid>https://arxiv.org/abs/2509.08012</guid>
<content:encoded><![CDATA[
<div> deep learning, brain atrophy, CT-brain analysis, cognitive impairment, age <br />
Summary: 
- The study validated an automated deep learning tool for measuring Global Cerebral Atrophy (GCA) score in older patients and found it accurate compared to human raters.
- The tool showed good agreement with visual ratings and correlated with age and cognitive impairment.
- Results indicated that the deep learning tool could provide standardized quantitative measures of atrophy without user input.
- The study suggests the tool could be valuable for health data research and potentially used as a point-of-care clinically approved tool.
- Overall, the automated tool demonstrated high accuracy and efficiency in analyzing CT brain scans for assessing brain atrophy in older individuals. <br /> <div>
arXiv:2509.08012v1 Announce Type: cross 
Abstract: Quantification of brain atrophy currently requires visual rating scales which are time consuming and automated brain image analysis is warranted. We validated our automated deep learning (DL) tool measuring the Global Cerebral Atrophy (GCA) score against trained human raters, and associations with age and cognitive impairment, in representative older (>65 years) patients. CT-brain scans were obtained from patients in acute medicine (ORCHARD-EPR), acute stroke (OCS studies) and a legacy sample. Scans were divided in a 60/20/20 ratio for training, optimisation and testing. CT-images were assessed by two trained raters (rater-1=864 scans, rater-2=20 scans). Agreement between DL tool-predicted GCA scores (range 0-39) and the visual ratings was evaluated using mean absolute error (MAE) and Cohen's weighted kappa. Among 864 scans (ORCHARD-EPR=578, OCS=200, legacy scans=86), MAE between the DL tool and rater-1 GCA scores was 3.2 overall, 3.1 for ORCHARD-EPR, 3.3 for OCS and 2.6 for the legacy scans and half had DL-predicted GCA error between -2 and 2. Inter-rater agreement was Kappa=0.45 between the DL-tool and rater-1, and 0.41 between the tool and rater- 2 whereas it was lower at 0.28 for rater-1 and rater-2. There was no difference in GCA scores from the DL-tool and the two raters (one-way ANOVA, p=0.35) or in mean GCA scores between the DL-tool and rater-1 (paired t-test, t=-0.43, p=0.66), the tool and rater-2 (t=1.35, p=0.18) or between rater-1 and rater-2 (t=0.99, p=0.32). DL-tool GCA scores correlated with age and cognitive scores (both p<0.001). Our DL CT-brain analysis tool measured GCA score accurately and without user input in real-world scans acquired from older patients. Our tool will enable extraction of standardised quantitative measures of atrophy at scale for use in health data research and will act as proof-of-concept towards a point-of-care clinically approved tool.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioComposer: Flexible and Compositional Anatomical Structure Generation with Disentangled Geometric Guidance</title>
<link>https://arxiv.org/abs/2509.08015</link>
<guid>https://arxiv.org/abs/2509.08015</guid>
<content:encoded><![CDATA[
<div> Generative models, 3D anatomy, biophysical simulators, structure-function relationships, clinical research.<br />
Summary:<br />
A proposed framework aims to enhance generative models of 3D anatomy for clinical research and medical device design. Current models often struggle to balance controllability and anatomical realism. This new framework utilizes ellipsoidal primitives embedded in 3D space to guide unconditional diffusion models of human anatomy. By selecting specific tissues within multi-tissue segmentation maps and implementing geometric moment losses, the reverse diffusion process is guided. This approach allows for independent control over size, shape, and position, as well as the composition of multi-component constraints during inference. This programmable and compositional framework offers a solution to the trade-off dilemma faced by current models, paving the way for more accurate and controllable 3D anatomical modeling for various applications. <br /><br /> <div>
arXiv:2509.08015v1 Announce Type: cross 
Abstract: Generative models of 3D anatomy, when integrated with biophysical simulators, enable the study of structure-function relationships for clinical research and medical device design. However, current models face a trade-off between controllability and anatomical realism. We propose a programmable and compositional framework for guiding unconditional diffusion models of human anatomy using interpretable ellipsoidal primitives embedded in 3D space. Our method involves the selection of certain tissues within multi-tissue segmentation maps, upon which we apply geometric moment losses to guide the reverse diffusion process. This framework supports the independent control over size, shape, and position, as well as the composition of multi-component constraints during inference.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values</title>
<link>https://arxiv.org/abs/2509.08022</link>
<guid>https://arxiv.org/abs/2509.08022</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, human values, global alignment, demographic diversity<br />
<br />
Summary:
The article introduces MVPBench, a benchmark for evaluating large language models (LLMs) alignment with human values across 75 countries. It contains over 24,000 instances annotated with value labels, personalized questions, and demographic metadata. It highlights disparities in alignment performance across geographic and demographic lines for various LLMs. The study shows that methods like Low-Rank Adaptation (LoRA) and Direct Preference Optimization (DPO) can improve value alignment in different settings. The research emphasizes the importance of population-aware alignment evaluation for developing culturally adaptive and value-sensitive LLMs. MVPBench provides a foundation for studying global alignment, personalized value modeling, and equitable AI development. <div>
arXiv:2509.08022v1 Announce Type: cross 
Abstract: The alignment of large language models (LLMs) with human values is critical for their safe and effective deployment across diverse user populations. However, existing benchmarks often neglect cultural and demographic diversity, leading to limited understanding of how value alignment generalizes globally. In this work, we introduce MVPBench, a novel benchmark that systematically evaluates LLMs' alignment with multi-dimensional human value preferences across 75 countries. MVPBench contains 24,020 high-quality instances annotated with fine-grained value labels, personalized questions, and rich demographic metadata, making it the most comprehensive resource of its kind to date. Using MVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs, revealing substantial disparities in alignment performance across geographic and demographic lines. We further demonstrate that lightweight fine-tuning methods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization (DPO), can significantly enhance value alignment in both in-domain and out-of-domain settings. Our findings underscore the necessity for population-aware alignment evaluation and provide actionable insights for building culturally adaptive and value-sensitive LLMs. MVPBench serves as a practical foundation for future research on global alignment, personalized value modeling, and equitable AI development.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment</title>
<link>https://arxiv.org/abs/2509.08025</link>
<guid>https://arxiv.org/abs/2509.08025</guid>
<content:encoded><![CDATA[
<div> Keywords: Legal Case Entailment, Large Language Models, Retrieval System, Hybrid Models, Legal Information Processing

Summary: 
The paper discusses the NOWJ team's participation in the COLIEE 2025 competition across five tasks, focusing on their success in the Legal Case Entailment task. They employed a comprehensive approach that combined pre-ranking models, semantic representations, and advanced Large Language Models for various tasks. In Task 2, their retrieval system using lexical-semantic filtering and LLM analysis achieved first place with an impressive F1 score. They also demonstrated strong performance in other tasks through ensemble models and prompt-based reasoning strategies. The study emphasizes the effectiveness of hybrid models that integrate traditional IR techniques with modern generative models, providing valuable insights for future developments in legal information processing.<br /><br />Summary: <div>
arXiv:2509.08025v1 Announce Type: cross 
Abstract: This paper presents the methodologies and results of the NOWJ team's participation across all five tasks at the COLIEE 2025 competition, emphasizing advancements in the Legal Case Entailment task (Task 2). Our comprehensive approach systematically integrates pre-ranking models (BM25, BERT, monoT5), embedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large Language Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance scoring, and contextual re-ranking. Specifically, in Task 2, our two-stage retrieval system combined lexical-semantic filtering with contextualized LLM analysis, achieving first place with an F1 score of 0.3195. Additionally, in other tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal Textual Entailment, and Legal Judgment Prediction--we demonstrated robust performance through carefully engineered ensembles and effective prompt-based reasoning strategies. Our findings highlight the potential of hybrid models integrating traditional IR techniques with contemporary generative models, providing a valuable reference for future advancements in legal information processing.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Swarm Intelligence Ensemble Deep Transfer Learning (SI-EDTL) for Vehicle Detection Using Unmanned Aerial Vehicles</title>
<link>https://arxiv.org/abs/2509.08026</link>
<guid>https://arxiv.org/abs/2509.08026</guid>
<content:encoded><![CDATA[
<div> Keywords: SI-EDTL, swarm intelligence, ensemble model, transfer learning, UAV images

Summary:<br />
This paper presents a novel approach, SI-EDTL, for detecting multiple vehicles in UAV images. The model utilizes a two-stage swarm intelligence ensemble deep transfer learning framework, combining pre-trained feature extractor models with transfer classifiers to create 15 base learners. By aggregating the base learners through weighted averaging, the model can classify regions as different types of vehicles or background. Hyperparameters are optimized using the whale optimization algorithm to achieve a balance between accuracy, precision, and recall. Implemented in MATLAB R2020b with parallel processing capabilities, SI-EDTL outperforms existing methods on the AU-AIR UAV dataset, demonstrating its effectiveness in vehicle detection tasks. <div>
arXiv:2509.08026v1 Announce Type: cross 
Abstract: This paper introduces SI-EDTL, a two-stage swarm intelligence ensemble deep transfer learning model for detecting multiple vehicles in UAV images. It combines three pre-trained Faster R-CNN feature extractor models (InceptionV3, ResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5, Na\"ive Bayes), resulting in 15 different base learners. These are aggregated via weighted averaging to classify regions as Car, Van, Truck, Bus, or background. Hyperparameters are optimized with the whale optimization algorithm to balance accuracy, precision, and recall. Implemented in MATLAB R2020b with parallel processing, SI-EDTL outperforms existing methods on the AU-AIR UAV dataset.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LALM-Eval: An Open-Source Toolkit for Holistic Evaluation of Large Audio Language Models</title>
<link>https://arxiv.org/abs/2509.08031</link>
<guid>https://arxiv.org/abs/2509.08031</guid>
<content:encoded><![CDATA[
<div> framework, LALMs, evaluation, audio, language <br />
<br />
Summary: 
The article introduces LALM-Eval, an evaluation framework for Large Audio Language Models (LALMs). It addresses three key issues in current evaluation frameworks: slow processing, inconsistent prompting, and narrow task coverage. LALM-Eval offers optimized batch processing and parallel execution, resulting in a speedup of up to 127%. It provides standardized prompting protocols and flexible configurations for fair model comparison. Two new evaluation categories, LLM-Adaptive Diarization and Spoken Language Reasoning, are introduced to assess temporal understanding and cognitive tasks in audio. The evaluation across 380+ tasks highlights significant gaps in current LALMs, particularly in temporal understanding and complex spoken language reasoning. The study also identifies a lack of standardization in instruction modality across audio benchmarks, impacting performance on complex instruction-following tasks by up to 9.5 absolute points. LALM-Eval aims to advance systematic LALM development by providing practical evaluation tools and insights into model limitations. <div>
arXiv:2509.08031v1 Announce Type: cross 
Abstract: Large Audio Language Models (LALMs) are rapidly advancing, but evaluating them remains challenging due to inefficient toolkits that limit fair comparison and systematic assessment. Current frameworks suffer from three critical issues: slow processing that bottlenecks large-scale studies, inconsistent prompting that hurts reproducibility, and narrow task coverage that misses important audio reasoning capabilities. We introduce LALM-Eval, an efficient and comprehensive evaluation framework for LALMs. Our system achieves a speedup of up to 127% over existing toolkits through optimized batch processing and parallel execution, enabling large-scale evaluations previously impractical. We provide standardized prompting protocols and flexible configurations for fair model comparison across diverse scenarios. Additionally, we introduce two new evaluation categories: LLM-Adaptive Diarization for temporal audio understanding and Spoken Language Reasoning for complex audio-based cognitive tasks. Through evaluation across 380+ tasks, we reveal significant gaps in current LALMs, particularly in temporal understanding and complex spoken language reasoning tasks. Our findings also highlight a lack of standardization in instruction modality existent across audio benchmarks, which can lead up performance differences up to 9.5 absolute points on the challenging complex instruction following downstream tasks. LALM-Eval provides both practical evaluation tools and insights into model limitations, advancing systematic LALM development.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Far Are We from True Unlearnability?</title>
<link>https://arxiv.org/abs/2509.08058</link>
<guid>https://arxiv.org/abs/2509.08058</guid>
<content:encoded><![CDATA[
<div> Keywords: unlearnable methods, model training, multi-task dataset, loss landscape, Sharpness-Aware Learnability

Summary:
The article explores the concept of unlearnable examples (UEs) in model training and emphasizes the importance of high-quality data in the era of large models. While several unlearnable methods have been proposed to generate UEs that are expected to be unlearnable across multiple tasks, the study finds that UEs perform well in tasks like semantic segmentation on the Taskonomy dataset. The research delves into the convergence process of clean and poisoned models using a simple model architecture, highlighting the relationship between the loss landscape and unlearnability. The proposed Sharpness-Aware Learnability (SAL) aims to quantify the unlearnability of parameters based on the loss landscape, while the Unlearnable Distance (UD) measures the unlearnability of data. Benchmark tests on mainstream unlearnable methods using UD provide insights into the capability boundaries of existing unlearnable methods.

<br /><br />Summary: <div>
arXiv:2509.08058v1 Announce Type: cross 
Abstract: High-quality data plays an indispensable role in the era of large models, but the use of unauthorized data for model training greatly damages the interests of data owners. To overcome this threat, several unlearnable methods have been proposed, which generate unlearnable examples (UEs) by compromising the training availability of data. Clearly, due to unknown training purposes and the powerful representation learning capabilities of existing models, these data are expected to be unlearnable for models across multiple tasks, i.e., they will not help improve the model's performance. However, unexpectedly, we find that on the multi-task dataset Taskonomy, UEs still perform well in tasks such as semantic segmentation, failing to exhibit cross-task unlearnability. This phenomenon leads us to question: How far are we from attaining truly unlearnable examples? We attempt to answer this question from the perspective of model optimization. To this end, we observe the difference in the convergence process between clean and poisoned models using a simple model architecture. Subsequently, from the loss landscape we find that only a part of the critical parameter optimization paths show significant differences, implying a close relationship between the loss landscape and unlearnability. Consequently, we employ the loss landscape to explain the underlying reasons for UEs and propose Sharpness-Aware Learnability (SAL) to quantify the unlearnability of parameters based on this explanation. Furthermore, we propose an Unlearnable Distance (UD) to measure the unlearnability of data based on the SAL distribution of parameters in clean and poisoned models. Finally, we conduct benchmark tests on mainstream unlearnable methods using the proposed UD, aiming to promote community awareness of the capability boundaries of existing unlearnable methods.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEL: A Novel Model Linking Knowledge Graph entities to News Mentions</title>
<link>https://arxiv.org/abs/2509.08086</link>
<guid>https://arxiv.org/abs/2509.08086</guid>
<content:encoded><![CDATA[
<div> neural network, entity linking, knowledge graph, natural language processing, news analytics 

Summary:
JEL is a novel end-to-end multi-neural network based model for entity linking that outperforms current state-of-the-art models. Entity linking is crucial for connecting mentions in textual sources to entities in knowledge graphs, facilitating tasks like news analytics. JPMorgan's News Analytics platform relies on entity linking to bridge unstructured news text with knowledge graphs, enabling users to access curated data efficiently. The task of entity linking is fundamental in natural language processing, aiding in various use cases like building analytics platforms. Knowledge graphs provide a powerful abstraction for capturing relationships among entities from diverse sources. The efficient entity linking model presented in this study showcases the importance of leveraging neural networks for enhancing the accuracy and effectiveness of entity linking tasks. <div>
arXiv:2509.08086v1 Announce Type: cross 
Abstract: We present JEL, a novel computationally efficient end-to-end multi-neural network based entity linking model, which beats current state-of-art model. Knowledge Graphs have emerged as a compelling abstraction for capturing critical relationships among the entities of interest and integrating data from multiple heterogeneous sources. A core problem in leveraging a knowledge graph is linking its entities to the mentions (e.g., people, company names) that are encountered in textual sources (e.g., news, blogs., etc) correctly, since there are thousands of entities to consider for each mention. This task of linking mentions and entities is referred as Entity Linking (EL). It is a fundamental task in natural language processing and is beneficial in various uses cases, such as building a New Analytics platform. News Analytics, in JPMorgan, is an essential task that benefits multiple groups across the firm. According to a survey conducted by the Innovation Digital team 1 , around 25 teams across the firm are actively looking for news analytics solutions, and more than \$2 million is being spent annually on external vendor costs. Entity linking is critical for bridging unstructured news text with knowledge graphs, enabling users access to vast amounts of curated data in a knowledge graph and dramatically facilitating their daily work.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Assessment Strategies for Generative AI Applications in Healthcare</title>
<link>https://arxiv.org/abs/2509.08087</link>
<guid>https://arxiv.org/abs/2509.08087</guid>
<content:encoded><![CDATA[
<div> Methodologies, Evaluating, Generative artificial intelligence, Healthcare, Performance

Summary: 
Generative artificial intelligence (GenAI) is a rapidly growing field in healthcare, with applications across various clinical tasks. Evaluating GenAI models is crucial, requiring a deep understanding of the clinical tasks and the variability in performance in real-world settings. Current evaluation methods primarily rely on quantitative benchmarks, which may exhibit limitations like train-to-the-test overfitting, hindering generalizability. To address this issue, evaluation strategies incorporating human expertise and cost-effective computational models as evaluators are gaining popularity. This shift enables a more comprehensive assessment of GenAI applications' performance in healthcare and medical devices by considering a broader range of task and data distributions. <div>
arXiv:2509.08087v1 Announce Type: cross 
Abstract: Generative artificial intelligence (GenAI) represent an emerging paradigm within artificial intelligence, with applications throughout the medical enterprise. Assessing GenAI applications necessitates a comprehensive understanding of the clinical task and awareness of the variability in performance when implemented in actual clinical environments. Presently, a prevalent method for evaluating the performance of generative models relies on quantitative benchmarks. Such benchmarks have limitations and may suffer from train-to-the-test overfitting, optimizing performance for a specified test set at the cost of generalizability across other task and data distributions. Evaluation strategies leveraging human expertise and utilizing cost-effective computational models as evaluators are gaining interest. We discuss current state-of-the-art methodologies for assessing the performance of GenAI applications in healthcare and medical devices.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Obstacle Avoidance for a Mobile Robot Using CNN-Based Sensor Fusion</title>
<link>https://arxiv.org/abs/2509.08095</link>
<guid>https://arxiv.org/abs/2509.08095</guid>
<content:encoded><![CDATA[
<div> Keywords: obstacle avoidance, Convolutional Neural Networks, mobile robot, real-time navigation, Intel RealSense D415.

Summary: 
- The research focuses on obstacle avoidance for mobile robots in complex environments using CNNs.
- Three different CNN models were trained offline and tested on a robot equipped with RGB-D cameras.
- The NetConEmb model showed the best performance with a low Median Absolute Error of $0.58 × 10^-3$ rad/s.
- The lighter NetEmb model, with fewer parameters, still achieved comparable results with a Root Mean Square Error of $21.68 × 10^-3$ rad/s.
- During real-time navigation tests, NetConEmb exhibited robustness in both known and unknown environments, achieving a 100% success rate, while NetEmb and NetGated succeeded only in known environments.

<br /><br />Summary: <div>
arXiv:2509.08095v1 Announce Type: cross 
Abstract: Obstacle avoidance is a critical component of the navigation stack required for mobile robots to operate effectively in complex and unknown environments. In this research, three end-to-end Convolutional Neural Networks (CNNs) were trained and evaluated offline and deployed on a differential-drive mobile robot for real-time obstacle avoidance to generate low-level steering commands from synchronized color and depth images acquired by an Intel RealSense D415 RGB-D camera in diverse environments. Offline evaluation showed that the NetConEmb model achieved the best performance with a notably low MedAE of $0.58 \times 10^{-3}$ rad/s. In comparison, the lighter NetEmb architecture adopted in this study, which reduces the number of trainable parameters by approximately 25\% and converges faster, produced comparable results with an RMSE of $21.68 \times 10^{-3}$ rad/s, close to the $21.42 \times 10^{-3}$ rad/s obtained by NetConEmb. Real-time navigation further confirmed NetConEmb's robustness, achieving a 100\% success rate in both known and unknown environments, while NetEmb and NetGated succeeded only in navigating the known environment.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</title>
<link>https://arxiv.org/abs/2509.08104</link>
<guid>https://arxiv.org/abs/2509.08104</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, point cloud prediction, loss functions, one-to-one matching, adaptive probabilistic matching loss

Summary: 
The article introduces the Adaptive Probabilistic Matching Loss (APML) as a new approach for training deep learning models in point cloud prediction tasks. Traditionally used loss functions like Chamfer Distance suffer from issues like many-to-one correspondences and non-differentiable operations. Earth Mover Distance (EMD) overcomes these limitations but has high computational complexity. APML offers a fully differentiable approximation of one-to-one matching using Sinkhorn iterations on a temperature-scaled similarity matrix. This approach ensures better spatial distribution in both dense and sparse regions and enables faster convergence in model training. APML achieves comparable runtime to Chamfer-based losses while avoiding non-differentiable operations. By integrating APML into various state-of-the-art architectures, the models showed improved performance on ShapeNet benchmarks and a spatiotemporal Transformer (CSI2PC) for generating 3D human point clouds. Overall, APML provides a promising solution for enhancing point cloud prediction tasks. 

<br /><br />Summary: <div>
arXiv:2509.08104v1 Announce Type: cross 
Abstract: Training deep learning models for point cloud prediction tasks such as shape completion and generation depends critically on loss functions that measure discrepancies between predicted and ground-truth point sets. Commonly used functions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on nearest-neighbor assignments, which often induce many-to-one correspondences, leading to point congestion in dense regions and poor coverage in sparse regions. These losses also involve non-differentiable operations due to index selection, which may affect gradient-based optimization. Earth Mover Distance (EMD) enforces one-to-one correspondences and captures structural similarity more effectively, but its cubic computational complexity limits its practical use. We propose the Adaptive Probabilistic Matching Loss (APML), a fully differentiable approximation of one-to-one matching that leverages Sinkhorn iterations on a temperature-scaled similarity matrix derived from pairwise distances. We analytically compute the temperature to guarantee a minimum assignment probability, eliminating manual tuning. APML achieves near-quadratic runtime, comparable to Chamfer-based losses, and avoids non-differentiable operations. When integrated into state-of-the-art architectures (PoinTr, PCN, FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC) that generates 3D human point clouds from WiFi CSI measurements, APM loss yields faster convergence, superior spatial distribution, especially in low-density regions, and improved or on-par quantitative performance without additional hyperparameter search. The code is available at: https://github.com/apm-loss/apml.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography</title>
<link>https://arxiv.org/abs/2509.08116</link>
<guid>https://arxiv.org/abs/2509.08116</guid>
<content:encoded><![CDATA[
<div> SSL, AI-based ECG analysis, PhysioCLR, contrastive learning, ECG physiological similarity cues <br />
<br />
Summary: PhysioCLR introduces a physiology-aware contrastive learning framework for ECG analysis, leveraging self-supervised learning to overcome the challenge of limited labeled data. By incorporating ECG physiological similarity cues and specific augmentations, PhysioCLR enhances the generalizability and clinical relevance of ECG-based arrhythmia classification. The model learns to bring together samples with similar clinically relevant features while pushing apart dissimilar ones, resulting in clinically meaningful representations. Evaluation on public ECG datasets shows a significant improvement in AUROC compared to baseline methods, indicating robust cross-dataset generalization. PhysioCLR demonstrates the potential of physiology-informed SSL in improving the effectiveness and efficiency of ECG diagnostics, offering a promising approach for enhancing healthcare outcomes. <br /><br /> <div>
arXiv:2509.08116v1 Announce Type: cross 
Abstract: Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heart conditions; however, the effectiveness of artificial intelligence (AI)-based ECG analysis is often hindered by the limited availability of labeled data. Self-supervised learning (SSL) can address this by leveraging large-scale unlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive Learning Representation for ECG), a physiology-aware contrastive learning framework that incorporates domain-specific priors to enhance the generalizability and clinical relevance of ECG-based arrhythmia classification. Methods: During pretraining, PhysioCLR learns to bring together embeddings of samples that share similar clinically relevant features while pushing apart those that are dissimilar. Unlike existing methods, our method integrates ECG physiological similarity cues into contrastive learning, promoting the learning of clinically meaningful representations. Additionally, we introduce ECG- specific augmentations that preserve the ECG category post augmentation and propose a hybrid loss function to further refine the quality of learned representations. Results: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia, for multilabel ECG diagnoses, as well as a private ICU dataset labeled for binary classification. Across the Chapman, Georgia, and private cohorts, PhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline, underscoring its robust cross-dataset generalization. Conclusion: By embedding physiological knowledge into contrastive learning, PhysioCLR enables the model to learn clinically meaningful and transferable ECG eatures. Significance: PhysioCLR demonstrates the potential of physiology-informed SSL to offer a promising path toward more effective and label-efficient ECG diagnostics.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering and Multi-model Learning in Venture Capital</title>
<link>https://arxiv.org/abs/2509.08140</link>
<guid>https://arxiv.org/abs/2509.08140</guid>
<content:encoded><![CDATA[
<div> Keywords: rare-event prediction, large language models, machine learning, venture capital, feature sensitivity analysis

Summary:
This paper introduces a novel framework for predicting rare, high-impact outcomes by combining large language models (LLMs) with a multi-model machine learning architecture. The approach leverages black-box models for predictive strength while maintaining interpretability for decision-making. By utilizing LLM-powered feature engineering, complex signals from unstructured data are synthesized and processed through an ensemble of models including XGBoost, Random Forest, and Linear Regression. Applied to the venture capital domain, the model demonstrates strong performance with precision significantly higher than random classification. Feature sensitivity analysis identifies key success drivers, with the startup's category list, number of founders, education level, and domain expertise playing significant roles. This framework offers valuable insights for evaluating startups with limited and noisy early-stage data in the VC industry. 

<br /><br />Summary: <div>
arXiv:2509.08140v1 Announce Type: cross 
Abstract: This paper presents a framework for predicting rare, high-impact outcomes by integrating large language models (LLMs) with a multi-model machine learning (ML) architecture. The approach combines the predictive strength of black-box models with the interpretability required for reliable decision-making. We use LLM-powered feature engineering to extract and synthesize complex signals from unstructured data, which are then processed within a layered ensemble of models including XGBoost, Random Forest, and Linear Regression. The ensemble first produces a continuous estimate of success likelihood, which is then thresholded to produce a binary rare-event prediction. We apply this framework to the domain of Venture Capital (VC), where investors must evaluate startups with limited and noisy early-stage data. The empirical results show strong performance: the model achieves precision between 9.8X and 11.1X the random classifier baseline in three independent test subsets. Feature sensitivity analysis further reveals interpretable success drivers: the startup's category list accounts for 15.6% of predictive influence, followed by the number of founders, while education level and domain expertise contribute smaller yet consistent effects.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk-Bounded Multi-Agent Visual Navigation via Dynamic Budget Allocation</title>
<link>https://arxiv.org/abs/2509.08157</link>
<guid>https://arxiv.org/abs/2509.08157</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Multi-agent planning, Safe navigation, Goal-conditioned RL, Risk allocation<br />
<br />
Summary: Safe navigation is crucial for autonomous systems in hazardous environments. Traditional planning methods are adept at long-horizon tasks but rely on predefined metrics. Safe RL can handle complex behaviors but struggles in multi-agent settings. A recent approach combines goal-conditioned RL with Conflict-Based Search (CBS) for multi-agent path planning, but it may overly restrict missions in high-risk areas. The proposed RB-CBS extends CBS by dynamically allocating and adjusting risk bounds for agents, allowing for a flexible trade-off between safety and speed. By ensuring each agent has a local risk budget, navigation efficiency improves while maintaining overall safety. Experimental results demonstrate the effectiveness of this iterative risk-allocation framework in complex environments, enabling multiple agents to find collision-free paths within specified risk bounds. <div>
arXiv:2509.08157v1 Announce Type: cross 
Abstract: Safe navigation is essential for autonomous systems operating in hazardous environments, especially when multiple agents must coordinate using just visual inputs over extended time horizons. Traditional planning methods excel at solving long-horizon tasks but rely on predefined distance metrics, while safe Reinforcement Learning (RL) can learn complex behaviors using high-dimensional inputs yet struggles with multi-agent, goal-conditioned scenarios. Recent work combined these paradigms by leveraging goal-conditioned RL (GCRL) to build an intermediate graph from replay buffer states, pruning unsafe edges, and using Conflict-Based Search (CBS) for multi-agent path planning. Although effective, this graph-pruning approach can be overly conservative, limiting mission efficiency by precluding missions that must traverse high-risk regions. To address this limitation, we propose RB-CBS, a novel extension to CBS that dynamically allocates and adjusts user-specified risk bound ($\Delta$) across agents to flexibly trade off safety and speed. Our improved planner ensures that each agent receives a local risk budget ($\delta$) enabling more efficient navigation while still respecting overall safety constraints. Experimental results demonstrate that this iterative risk-allocation framework yields superior performance in complex environments, allowing multiple agents to find collision-free paths within the user-specified $\Delta$.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Metric Depth Estimation via Monocular Visual-Inertial Rescaling for Autonomous Aerial Navigation</title>
<link>https://arxiv.org/abs/2509.08159</link>
<guid>https://arxiv.org/abs/2509.08159</guid>
<content:encoded><![CDATA[
<div> Keywords: metric depth prediction, monocular RGB images, inertial measurement unit, collision avoidance, visual-inertial navigation system <br />
Summary: This paper introduces a methodology for predicting metric depth from monocular RGB images and an IMU to enable collision avoidance during autonomous flight. Previous approaches required heavy sensors or data-intensive fine-tuning of monocular depth estimation methods. The proposed methodology uses lightweight zero-shot rescaling strategies to obtain metric depth from relative depth estimates using a visual-inertial navigation system. These strategies are evaluated in simulation environments, with the monotonic spline fitting method proving to be the most accurate. The best performing approach is implemented on a quadrotor in real-world scenarios, achieving on-board metric depth estimates at 15 Hz and successfully enabling collision avoidance through integration with a motion primitives-based planner.<br /><br />Summary: <div>
arXiv:2509.08159v1 Announce Type: cross 
Abstract: This paper presents a methodology to predict metric depth from monocular RGB images and an inertial measurement unit (IMU). To enable collision avoidance during autonomous flight, prior works either leverage heavy sensors (e.g., LiDARs or stereo cameras) or data-intensive and domain-specific fine-tuning of monocular metric depth estimation methods. In contrast, we propose several lightweight zero-shot rescaling strategies to obtain metric depth from relative depth estimates via the sparse 3D feature map created using a visual-inertial navigation system. These strategies are compared for their accuracy in diverse simulation environments. The best performing approach, which leverages monotonic spline fitting, is deployed in the real-world on a compute-constrained quadrotor. We obtain on-board metric depth estimates at 15 Hz and demonstrate successful collision avoidance after integrating the proposed method with a motion primitives-based planner.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Guided Multi-Arm Motion Planning</title>
<link>https://arxiv.org/abs/2509.08160</link>
<guid>https://arxiv.org/abs/2509.08160</guid>
<content:encoded><![CDATA[
<div> diffusion-guided multi-arm planner, scalability, multi-arm motion planning, learning-based models, pairwise collision resolution<br />
<br />
Summary:<br />
The article introduces a diffusion-guided multi-arm planner (DG-MAP) for enhancing the scalability of learning-based models in multi-arm motion planning. Inspired by Multi-Agent Path Finding (MAPF), the proposed planner leverages two conditional diffusion models to generate feasible single-arm trajectories and model dual-arm dynamics for pairwise collision resolution. By decomposing planning into single-agent problems coupled with collision resolution, the DG-MAP improves scalability and reduces reliance on massive multi-arm datasets. Through evaluations, the effectiveness and practical applicability of the method are demonstrated against alternative learning-based methods across different team sizes. The project website provides further information on the proposed approach. <br /> <div>
arXiv:2509.08160v1 Announce Type: cross 
Abstract: Multi-arm motion planning is fundamental for enabling arms to complete complex long-horizon tasks in shared spaces efficiently but current methods struggle with scalability due to exponential state-space growth and reliance on large training datasets for learned models. Inspired by Multi-Agent Path Finding (MAPF), which decomposes planning into single-agent problems coupled with collision resolution, we propose a novel diffusion-guided multi-arm planner (DG-MAP) that enhances scalability of learning-based models while reducing their reliance on massive multi-arm datasets. Recognizing that collisions are primarily pairwise, we train two conditional diffusion models, one to generate feasible single-arm trajectories, and a second, to model the dual-arm dynamics required for effective pairwise collision resolution. By integrating these specialized generative models within a MAPF-inspired structured decomposition, our planner efficiently scales to larger number of arms. Evaluations against alternative learning-based methods across various team sizes demonstrate our method's effectiveness and practical applicability. Project website can be found at https://diff-mapf-mers.csail.mit.edu
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments</title>
<link>https://arxiv.org/abs/2509.08176</link>
<guid>https://arxiv.org/abs/2509.08176</guid>
<content:encoded><![CDATA[
<div> concept drift, online learning, data streams, transfer learning, non-stationary environments

Summary:
MARLINE is a novel approach designed to address concept drift in online learning by leveraging knowledge from multiple data sources in non-stationary environments. It projects the target concept into the space of each source concept, allowing multiple source sub-classifiers to contribute to the prediction of the target concept within an ensemble framework. This method can be effective even when source and target concepts do not align. Experimental results on synthetic and real-world datasets demonstrate that MARLINE outperforms several state-of-the-art data stream learning approaches. The approach offers a promising solution to the challenges posed by concept drift in online learning settings, highlighting the potential of leveraging multiple data sources for improved predictive performance in dynamic environments.<br /><br />Summary: <div>
arXiv:2509.08176v1 Announce Type: cross 
Abstract: Concept drift is a major problem in online learning due to its impact on the predictive performance of data stream mining systems. Recent studies have started exploring data streams from different sources as a strategy to tackle concept drift in a given target domain. These approaches make the assumption that at least one of the source models represents a concept similar to the target concept, which may not hold in many real-world scenarios. In this paper, we propose a novel approach called Multi-source mApping with tRansfer LearnIng for Non-stationary Environments (MARLINE). MARLINE can benefit from knowledge from multiple data sources in non-stationary environments even when source and target concepts do not match. This is achieved by projecting the target concept to the space of each source concept, enabling multiple source sub-classifiers to contribute towards the prediction of the target concept as part of an ensemble. Experiments on several synthetic and real-world datasets show that MARLINE was more accurate than several state-of-the-art data stream learning approaches.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quadrotor Navigation using Reinforcement Learning with Privileged Information</title>
<link>https://arxiv.org/abs/2509.08177</link>
<guid>https://arxiv.org/abs/2509.08177</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, quadrotor navigation, large obstacles, time-of-arrival maps, privileged information   
  
Summary:  
This paper presents a novel reinforcement learning-based method for quadrotor navigation that utilizes efficient differentiable simulation, unique loss functions, and privileged information in the form of time-of-arrival maps to navigate around large obstacles. Unlike previous methods that struggle in the presence of large obstacles, the proposed approach incorporates a yaw alignment loss to guide the robot successfully. Evaluation in realistic simulation environments shows an 86% success rate, outperforming baseline strategies by 34%. Deployment on a custom quadrotor in cluttered outdoor environments demonstrates consistent performance across 20 flights covering 589 meters without collisions, at speeds up to 4 m/s. This innovative method showcases the effectiveness of leveraging privileged information and novel loss functions in enhancing quadrotor navigation capabilities.  

<br /><br />Summary: <div>
arXiv:2509.08177v1 Announce Type: cross 
Abstract: This paper presents a reinforcement learning-based quadrotor navigation method that leverages efficient differentiable simulation, novel loss functions, and privileged information to navigate around large obstacles. Prior learning-based methods perform well in scenes that exhibit narrow obstacles, but struggle when the goal location is blocked by large walls or terrain. In contrast, the proposed method utilizes time-of-arrival (ToA) maps as privileged information and a yaw alignment loss to guide the robot around large obstacles. The policy is evaluated in photo-realistic simulation environments containing large obstacles, sharp corners, and dead-ends. Our approach achieves an 86% success rate and outperforms baseline strategies by 34%. We deploy the policy onboard a custom quadrotor in outdoor cluttered environments both during the day and night. The policy is validated across 20 flights, covering 589 meters without collisions at speeds up to 4 m/s.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Label Transfer Learning in Non-Stationary Data Streams</title>
<link>https://arxiv.org/abs/2509.08181</link>
<guid>https://arxiv.org/abs/2509.08181</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-label data streams, concept drift, transfer learning, label dependencies, predictive performance

Summary: 
This study focuses on the adaptation of label concepts in multi-label data streams that often experience drift in non-stationary environments. The research addresses the limited exploration of multi-label transfer learning for data streams by proposing two novel methods: BR-MARLENE and BRPW-MARLENE. BR-MARLENE leverages knowledge from different labels in source and target streams, while BRPW-MARLENE enhances learning performance by explicitly modeling and transferring pairwise label dependencies. Comprehensive experiments demonstrate that both methods outperform existing multi-label stream approaches in non-stationary environments, showcasing the effectiveness of inter-label knowledge transfer in improving predictive performance. <div>
arXiv:2509.08181v1 Announce Type: cross 
Abstract: Label concepts in multi-label data streams often experience drift in non-stationary environments, either independently or in relation to other labels. Transferring knowledge between related labels can accelerate adaptation, yet research on multi-label transfer learning for data streams remains limited. To address this, we propose two novel transfer learning methods: BR-MARLENE leverages knowledge from different labels in both source and target streams for multi-label classification; BRPW-MARLENE builds on this by explicitly modelling and transferring pairwise label dependencies to enhance learning performance. Comprehensive experiments show that both methods outperform state-of-the-art multi-label stream approaches in non-stationary environments, demonstrating the effectiveness of inter-label knowledge transfer for improved predictive performance.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols</title>
<link>https://arxiv.org/abs/2509.08182</link>
<guid>https://arxiv.org/abs/2509.08182</guid>
<content:encoded><![CDATA[
<div> XML prompting, large language models, logic-first treatment, grammar-constrained decoding, fixed-point semantics, human-AI interaction<br />
Summary: Structured prompting with XML tags is an effective method to guide large language models towards producing parseable, schema-compliant outputs. This study presents a logic-first approach to XML prompting, encompassing grammar-constrained decoding and fixed-point semantics over hierarchical prompts. By formalizing a lattice of XML trees and introducing a task-aware contraction metric, the study demonstrates the convergence of iterative guidance strategies. Context-free grammars for XML schemas are utilized to ensure well-formed outputs while maintaining task performance. The study also outlines practical deployment strategies for human-AI interactions, such as multi-pass routines and agentic tool use. The framework is supported by mathematically rigorous proofs and connects to recent advancements in grammar-aligned decoding and programmatic prompting. <br /><br />Summary: <div>
arXiv:2509.08182v1 Announce Type: cross 
Abstract: Structured prompting with XML tags has emerged as an effective way to steer large language models (LLMs) toward parseable, schema-adherent outputs in real-world systems. We develop a logic-first treatment of XML prompting that unifies (i) grammar-constrained decoding, (ii) fixed-point semantics over lattices of hierarchical prompts, and (iii) convergent human-AI interaction loops. We formalize a complete lattice of XML trees under a refinement order and prove that monotone prompt-to-prompt operators admit least fixed points (Knaster-Tarski) that characterize steady-state protocols; under a task-aware contraction metric on trees, we further prove Banach-style convergence of iterative guidance. We instantiate these results with context-free grammars (CFGs) for XML schemas and show how constrained decoding guarantees well-formedness while preserving task performance. A set of multi-layer human-AI interaction recipes demonstrates practical deployment patterns, including multi-pass "plan $\to$ verify $\to$ revise" routines and agentic tool use. We provide mathematically complete proofs and tie our framework to recent advances in grammar-aligned decoding, chain-of-verification, and programmatic prompting.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lifetime-Aware Design of Item-Level Intelligence</title>
<link>https://arxiv.org/abs/2509.08193</link>
<guid>https://arxiv.org/abs/2509.08193</guid>
<content:encoded><![CDATA[
<div> FlexiFlow, item-level intelligence, flexible electronics, carbon footprint, lifetime-aware design<br />
<br />
Summary:<br />
FlexiFlow is a design framework for integrating computation into disposable products using flexible electronics. It targets applications with varying operational lifetimes, offering area-optimized RISC-V cores and a carbon-aware model to optimize architecture. Workloads include sustainability tasks like spoilage detection. The framework reduces carbon footprint through microarchitectural and algorithmic decisions. The first tape-out using a PDK for flexible electronics achieved 30.9kHz operation with fully open-source tools. FlexiFlow explores computing at the Extreme Edge, requiring reevaluation of traditional design methodologies to address new constraints and considerations. <div>
arXiv:2509.08193v1 Announce Type: cross 
Abstract: We present FlexiFlow, a lifetime-aware design framework for item-level intelligence (ILI) where computation is integrated directly into disposable products like food packaging and medical patches. Our framework leverages natively flexible electronics which offer significantly lower costs than silicon but are limited to kHz speeds and several thousands of gates. Our insight is that unlike traditional computing with more uniform deployment patterns, ILI applications exhibit 1000X variation in operational lifetime, fundamentally changing optimal architectural design decisions when considering trillion-item deployment scales. To enable holistic design and optimization, we model the trade-offs between embodied carbon footprint and operational carbon footprint based on application-specific lifetimes. The framework includes: (1) FlexiBench, a workload suite targeting sustainability applications from spoilage detection to health monitoring; (2) FlexiBits, area-optimized RISC-V cores with 1/4/8-bit datapaths achieving 2.65X to 3.50X better energy efficiency per workload execution; and (3) a carbon-aware model that selects optimal architectures based on deployment characteristics. We show that lifetime-aware microarchitectural design can reduce carbon footprint by 1.62X, while algorithmic decisions can reduce carbon footprint by 14.5X. We validate our approach through the first tape-out using a PDK for flexible electronics with fully open-source tools, achieving 30.9kHz operation. FlexiFlow enables exploration of computing at the Extreme Edge where conventional design methodologies must be reevaluated to account for new constraints and considerations.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating AI Development with Cyber Arenas</title>
<link>https://arxiv.org/abs/2509.08200</link>
<guid>https://arxiv.org/abs/2509.08200</guid>
<content:encoded><![CDATA[
<div> Keywords: AI development, testing environments, cyber arenas, artificial intelligence, National Guard exercise

Summary:
AI development requires advanced testing environments for successful transition from research to practical applications. Cyber arenas offer a unique opportunity to test artificial intelligence capabilities in realistic scenarios with end-users. These environments must constantly evolve to meet the challenges presented by real-world situations. The MIT/IEEE/Amazon Graph Challenge Anonymized Network Sensor was recently tested in a cyber arena during a National Guard exercise, showcasing the potential for using such platforms for AI experimentation. This experiment demonstrated the effectiveness of integrating new AI capabilities in dynamic environments and highlighted the importance of flexibility and adaptability in testing these technologies. <div>
arXiv:2509.08200v1 Announce Type: cross 
Abstract: AI development requires high fidelity testing environments to effectively transition from the laboratory to operations. The flexibility offered by cyber arenas presents a novel opportunity to test new artificial intelligence (AI) capabilities with users. Cyber arenas are designed to expose end-users to real-world situations and must rapidly incorporate evolving capabilities to meet their core objectives. To explore this concept the MIT/IEEE/Amazon Graph Challenge Anonymized Network Sensor was deployed in a cyber arena during a National Guard exercise.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Componentization: Decomposing Monolithic LLM Responses into Manipulable Semantic Units</title>
<link>https://arxiv.org/abs/2509.08203</link>
<guid>https://arxiv.org/abs/2509.08203</guid>
<content:encoded><![CDATA[
<div> componentization, Large Language Models, Modular and Adaptable Output Decomposition, Component-Based Response Architecture, user study

Summary:
- The article introduces the concept of componentization to break down monolithic text produced by Large Language Models into modular, independently editable units.
- It presents the Modular and Adaptable Output Decomposition (MAOD) approach for segmenting model outputs into coherent components while maintaining context.
- The Component-Based Response Architecture (CBRA) is outlined as a way to implement componentization.
- A reference prototype called MAODchat is introduced, showcasing a microservices design with state-machine-based decomposition agents and real-time component manipulation.
- An exploratory study with participants from various roles found that component-level editing supports common workflows, iterative refinement, and selective reuse, suggesting potential team workflows. <div>
arXiv:2509.08203v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often produce monolithic text that is hard to edit in parts, which can slow down collaborative workflows. We present componentization, an approach that decomposes model outputs into modular, independently editable units while preserving context. We describe Modular and Adaptable Output Decomposition (MAOD), which segments responses into coherent components and maintains links among them, and we outline the Component-Based Response Architecture (CBRA) as one way to implement this idea. Our reference prototype, MAODchat, uses a microservices design with state-machine-based decomposition agents, vendor-agnostic model adapters, and real-time component manipulation with recomposition.
  In an exploratory study with four participants from academic, engineering, and product roles, we observed that component-level editing aligned with several common workflows and enabled iterative refinement and selective reuse. Participants also mentioned possible team workflows. Our contributions are: (1) a definition of componentization for transforming monolithic outputs into manipulable units, (2) CBRA and MAODchat as a prototype architecture, (3) preliminary observations from a small user study, (4) MAOD as an algorithmic sketch for semantic segmentation, and (5) example Agent-to-Agent protocols for automated decomposition. We view componentization as a promising direction for turning passive text consumption into more active, component-level collaboration.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions</title>
<link>https://arxiv.org/abs/2509.08217</link>
<guid>https://arxiv.org/abs/2509.08217</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning datasets, annotator reliability, label diversity, spam filtering, synthetic spam

Summary:
An empirical evaluation was conducted to assess the impact of various heuristics for annotator filtering on the preservation of label variation in subjective tasks. It was found that most current methods for annotator filtering, which are aimed at eliminating noise from a single ground-truth label, tend to remove annotators who disagree rather than actual spam annotators. Conservative settings for annotator removal (<5%) were found to be the most effective, as higher removal rates increased the mean absolute error from the true average label. Additionally, the study revealed that existing spam filtering methods often overlook the fact that spam annotators are less random than non-spammers. The results suggest that spam removal methods should be revised to consider the diversity of labels in tasks. <div>
arXiv:2509.08217v1 Announce Type: cross 
Abstract: For machine learning datasets to accurately represent diverse opinions in a population, they must preserve variation in data labels while filtering out spam or low-quality responses. How can we balance annotator reliability and representation? We empirically evaluate how a range of heuristics for annotator filtering affect the preservation of variation on subjective tasks. We find that these methods, designed for contexts in which variation from a single ground-truth label is considered noise, often remove annotators who disagree instead of spam annotators, introducing suboptimal tradeoffs between accuracy and label diversity. We find that conservative settings for annotator removal (<5%) are best, after which all tested methods increase the mean absolute error from the true average label. We analyze performance on synthetic spam to observe that these methods often assume spam annotators are less random than real spammers tend to be: most spammers are distributionally indistinguishable from real annotators, and the minority that are distinguishable tend to give fixed answers, not random ones. Thus, tasks requiring the preservation of variation reverse the intuition of existing spam filtering methods: spammers tend to be less random than non-spammers, so metrics that assume variation is spam fare worse. These results highlight the need for spam removal methods that account for label diversity.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategies for Improving Communication Efficiency in Distributed and Federated Learning: Compression, Local Training, and Personalization</title>
<link>https://arxiv.org/abs/2509.08233</link>
<guid>https://arxiv.org/abs/2509.08233</guid>
<content:encoded><![CDATA[
<div> Keywords: Distributed learning, federated learning, model compression, local training, privacy preservation

Summary: 
This dissertation focuses on improving communication efficiency in distributed and federated learning by exploring strategies such as model compression, local training, and personalization. A unified framework for compression operators with convergence guarantees is established, and adaptive local training strategies with personalization are proposed to accelerate convergence and address client drift. The Scafflix approach balances global and personalized objectives effectively under different data settings. Privacy-preserving pruning frameworks like Cohort-Squeeze and SymWanda optimize sparsity while minimizing communication costs and maintaining accuracy without retraining. Extensive experiments on benchmarks and language models demonstrate the favorable trade-offs among accuracy, convergence, and communication, highlighting theoretical and practical insights for scalable and efficient distributed learning. 

<br /><br />Summary: <div>
arXiv:2509.08233v1 Announce Type: cross 
Abstract: Distributed and federated learning are essential paradigms for training models across decentralized data sources while preserving privacy, yet communication overhead remains a major bottleneck. This dissertation explores strategies to improve communication efficiency, focusing on model compression, local training, and personalization. We establish a unified framework for biased and unbiased compression operators with convergence guarantees, then propose adaptive local training strategies that incorporate personalization to accelerate convergence and mitigate client drift. In particular, Scafflix balances global and personalized objectives, achieving superior performance under both IID and non-IID settings. We further introduce privacy-preserving pruning frameworks that optimize sparsity while minimizing communication costs, with Cohort-Squeeze leveraging hierarchical aggregation to reduce cross-device overhead. Finally, SymWanda, a symmetric post-training pruning method, enhances robustness under high sparsity and maintains accuracy without retraining. Extensive experiments on benchmarks and large-scale language models demonstrate favorable trade-offs among accuracy, convergence, and communication, offering theoretical and practical insights for scalable, efficient distributed learning.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combined-distance-based score function of cognitive fuzzy sets and its application in lung cancer pain evaluation</title>
<link>https://arxiv.org/abs/2509.08239</link>
<guid>https://arxiv.org/abs/2509.08239</guid>
<content:encoded><![CDATA[
<div> distance, cognitive fuzzy set, decision making, Hausdorff distance, score function

Summary:
This paper introduces new distance metrics for cognitive fuzzy sets (CFS) to improve decision-making processes. Existing studies on the distance of CFS are limited, and the Minkowski distance commonly used ignores the hesitancy degree of CFS, leading to potential errors. The proposed improved cognitive fuzzy Minkowski (CF-IM) distance and cognitive fuzzy Hausdorff (CF-H) distance aim to address this issue by balancing anti-perturbation ability and information utilization. A new cognitive fuzzy combined (CF-C) distance, combining CF-IM and CF-H distances, is introduced for a more comprehensive evaluation of CFS. A combined-distanced-based score function is developed using the CF-C distance for comparing CFSs, which is applied in lung cancer pain evaluation scenarios. The sensitivity and comparison analyses confirm the effectiveness and advantages of the proposed methods. <div>
arXiv:2509.08239v1 Announce Type: cross 
Abstract: In decision making, the cognitive fuzzy set (CFS) is a useful tool in expressing experts' complex assessments of alternatives. The distance of CFS, which plays an important role in decision analyses, is necessary when the CFS is applied in solving practical issues. However, as far as we know, the studies on the distance of CFS are few, and the current Minkowski distance of CFS ignores the hesitancy degree of CFS, which might cause errors. To fill the gap of the studies on the distance of CFS, because of the practicality of the Hausdorff distance, this paper proposes the improved cognitive fuzzy Minkowski (CF-IM) distance and the cognitive fuzzy Hausdorff (CF-H) distance to enrich the studies on the distance of CFS. It is found that the anti-perturbation ability of the CF-H distance is stronger than that of the CF-IM distance, but the information utilization of the CF-IM distance is higher than that of the CF-H distance. To balance the anti-perturbation ability and information utilization of the CF-IM distance and CF-H distance, the cognitive fuzzy combined (CF-C) distance is proposed by establishing the linear combination of the CF-IM distance and CF-H distance. Based on the CF-C distance, a combined-distanced-based score function of CFS is proposed to compare CFSs. The proposed score function is employed in lung cancer pain evaluation issues. The sensitivity and comparison analyses demonstrate the reliability and advantages of the proposed methods.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry-Guided Multi-Agent Inverse Reinforcement Learnin</title>
<link>https://arxiv.org/abs/2509.08257</link>
<guid>https://arxiv.org/abs/2509.08257</guid>
<content:encoded><![CDATA[
arXiv:2509.08257v1 Announce Type: cross 
Abstract: In robotic systems, the performance of reinforcement learning depends on the rationality of predefined reward functions. However, manually designed reward functions often lead to policy failures due to inaccuracies. Inverse Reinforcement Learning (IRL) addresses this problem by inferring implicit reward functions from expert demonstrations. Nevertheless, existing methods rely heavily on large amounts of expert demonstrations to accurately recover the reward function. The high cost of collecting expert demonstrations in robotic applications, particularly in multi-robot systems, severely hinders the practical deployment of IRL. Consequently, improving sample efficiency has emerged as a critical challenge in multi-agent inverse reinforcement learning (MIRL). Inspired by the symmetry inherent in multi-agent systems, this work theoretically demonstrates that leveraging symmetry enables the recovery of more accurate reward functions. Building upon this insight, we propose a universal framework that integrates symmetry into existing multi-agent adversarial IRL algorithms, thereby significantly enhancing sample efficiency. Experimental results from multiple challenging tasks have demonstrated the effectiveness of this framework. Further validation in physical multi-robot systems has shown the practicality of our method.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Survey on Large Language Models for Evolutionary Optimization: From Modeling to Solving</title>
<link>https://arxiv.org/abs/2509.08269</link>
<guid>https://arxiv.org/abs/2509.08269</guid>
<content:encoded><![CDATA[
arXiv:2509.08269v1 Announce Type: cross 
Abstract: Large Language Models (LLMs), with their strong understanding and reasoning capabilities, are increasingly being explored for tackling optimization problems, especially in synergy with evolutionary computation. Despite rapid progress, however, the field still lacks a unified synthesis and a systematic taxonomy. This survey addresses this gap by providing a comprehensive review of recent developments and organizing them within a structured framework. We classify existing research into two main stages: LLMs for optimization modeling and LLMs for optimization solving. The latter is further divided into three paradigms according to the role of LLMs in the optimization workflow: LLMs as stand-alone optimizers, low-level LLMs embedded within optimization algorithms, and high-level LLMs for algorithm selection and generation. For each category, we analyze representative methods, distill technical challenges, and examine their interplay with traditional approaches. We also review interdisciplinary applications spanning the natural sciences, engineering, and machine learning. By contrasting LLM-driven and conventional methods, we highlight key limitations and research gaps, and point toward future directions for developing self-evolving agentic ecosystems for optimization. An up-to-date collection of related literature is maintained at https://github.com/ishmael233/LLM4OPT.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Physics Reasoning and Performance Taxonomy in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.08270</link>
<guid>https://arxiv.org/abs/2509.08270</guid>
<content:encoded><![CDATA[
arXiv:2509.08270v1 Announce Type: cross 
Abstract: As Vision-Language Models (VLMs) grow in sophistication, their ability to perform reasoning is coming under increasing supervision. While they excel at many tasks, their grasp of fundamental scientific principles, such as physics, remains an underexplored frontier. To reflect the advancements in these capabilities, we introduce a novel and accessible framework designed to rigorously evaluate VLMs on their understanding of 2D physics. Our framework features a pragmatic scenario generator that creates a diverse testbed of over 400 problems across four core domains: Projectile Motion, Collision Dynamics, Mechanics, and Fluid Dynamics. Through comprehensive evaluation of four state-of-the-art VLMs, we demonstrate a strong correlation between model scale and reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving an overall score of 0.815. We find that while models excel at formulaic problems, they struggle significantly with domains requiring abstract spatial reasoning. By designing this framework, we aim to democratize the study of scientific reasoning in VLMs and foster deeper insights into their capabilities and limitations.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment Transformer: AI-Generated Music Detection via Music Structural Analysis</title>
<link>https://arxiv.org/abs/2509.08283</link>
<guid>https://arxiv.org/abs/2509.08283</guid>
<content:encoded><![CDATA[
arXiv:2509.08283v1 Announce Type: cross 
Abstract: Audio and music generation systems have been remarkably developed in the music information retrieval (MIR) research field. The advancement of these technologies raises copyright concerns, as ownership and authorship of AI-generated music (AIGM) remain unclear. Also, it can be difficult to determine whether a piece was generated by AI or composed by humans clearly. To address these challenges, we aim to improve the accuracy of AIGM detection by analyzing the structural patterns of music segments. Specifically, to extract musical features from short audio clips, we integrated various pre-trained models, including self-supervised learning (SSL) models or an audio effect encoder, each within our suggested transformer-based framework. Furthermore, for long audio, we developed a segment transformer that divides music into segments and learns inter-segment relationships. We used the FakeMusicCaps and SONICS datasets, achieving high accuracy in both the short-audio and full-audio detection experiments. These findings suggest that integrating segment-level musical features into long-range temporal analysis can effectively enhance both the performance and robustness of AIGM detection systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>\emph{FoQuS}: A Forgetting-Quality Coreset Selection Framework for Automatic Modulation Recognition</title>
<link>https://arxiv.org/abs/2509.08300</link>
<guid>https://arxiv.org/abs/2509.08300</guid>
<content:encoded><![CDATA[
arXiv:2509.08300v1 Announce Type: cross 
Abstract: Deep learning-based Automatic Modulation Recognition (AMR) model has made significant progress with the support of large-scale labeled data. However, when developing new models or performing hyperparameter tuning, the time and energy consumption associated with repeated training using massive amounts of data are often unbearable. To address the above challenges, we propose \emph{FoQuS}, which approximates the effect of full training by selecting a coreset from the original dataset, thereby significantly reducing training overhead. Specifically, \emph{FoQuS} records the prediction trajectory of each sample during full-dataset training and constructs three importance metrics based on training dynamics. Experiments show that \emph{FoQuS} can maintain high recognition accuracy and good cross-architecture generalization on multiple AMR datasets using only 1\%-30\% of the original data.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Game-Theoretic Resilience Framework for Cyber-Physical Microgrids using Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.08310</link>
<guid>https://arxiv.org/abs/2509.08310</guid>
<content:encoded><![CDATA[
arXiv:2509.08310v1 Announce Type: cross 
Abstract: The increasing reliance on cyber physical infrastructure in modern power systems has amplified the risk of targeted cyber attacks, necessitating robust and adaptive resilience strategies. This paper presents a mathematically rigorous game theoretic framework to evaluate and enhance microgrid resilience using a combination of quantitative resilience metrics Load Served Ratio LSR, Critical Load Resilience CLR, Topological Survivability Score TSS, and DER Resilience Score DRS. These are integrated into a unified payoff matrix using the Analytic Hierarchy Process AHP to assess attack defense interactions. The framework is formalized as a finite horizon Markov Decision Process MDP with formal convergence guarantees and computational complexity bounds. Three case studies are developed 1. static attacks analyzed via Nash equilibrium, 2. severe attacks incorporating high impact strategies, and 3. adaptive attacks using Stackelberg games, regret matching, softmax heuristics, and Multi Agent Q Learning. Rigorous theoretical analysis provides convergence proofs with explicit rates , PAC learning sample complexity bounds, and computational complexity analysis. The framework is tested on an enhanced IEEE 33bus distribution system with DERs and control switches, demonstrating the effectiveness of adaptive and strategic defenses in improving cyber physical resilience with statistically significant improvements of 18.7% 2.1% over static approaches.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing</title>
<link>https://arxiv.org/abs/2509.08329</link>
<guid>https://arxiv.org/abs/2509.08329</guid>
<content:encoded><![CDATA[
arXiv:2509.08329v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) algorithms often require long training to become useful, especially in complex environments with sparse rewards. While techniques like reward shaping and curriculum learning exist to accelerate training, these are often extremely specific and require the developer's professionalism and dedicated expertise in the problem's domain. Tackling this challenge, in this study, we explore the effectiveness of pre-trained Large Language Models (LLMs) as tutors in a student-teacher architecture with RL algorithms, hypothesizing that LLM-generated guidance allows for faster convergence. In particular, we explore the effectiveness of reusing the LLM's advice on the RL's convergence dynamics. Through an extensive empirical examination, which included 54 configurations, varying the RL algorithm (DQN, PPO, A2C), LLM tutor (Llama, Vicuna, DeepSeek), and environment (Blackjack, Snake, Connect Four), our results demonstrate that LLM tutoring significantly accelerates RL convergence while maintaining comparable optimal performance. Furthermore, the advice reuse mechanism shows a further improvement in training duration but also results in less stable convergence dynamics. Our findings suggest that LLM tutoring generally improves convergence, and its effectiveness is sensitive to the specific task, RL algorithm, and LLM model combination.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis</title>
<link>https://arxiv.org/abs/2509.08338</link>
<guid>https://arxiv.org/abs/2509.08338</guid>
<content:encoded><![CDATA[
arXiv:2509.08338v1 Announce Type: cross 
Abstract: Accurate and early diagnosis of malignant melanoma is critical for improving patient outcomes. While convolutional neural networks (CNNs) have shown promise in dermoscopic image analysis, they often neglect clinical metadata and require extensive preprocessing. Vision-language models (VLMs) offer a multimodal alternative but struggle to capture clinical specificity when trained on general-domain data. To address this, we propose a retrieval-augmented VLM framework that incorporates semantically similar patient cases into the diagnostic prompt. Our method enables informed predictions without fine-tuning and significantly improves classification accuracy and error correction over conventional baselines. These results demonstrate that retrieval-augmented prompting provides a robust strategy for clinical decision support.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Mixture-of-Expert Inference with Adaptive Expert Split Mechanism</title>
<link>https://arxiv.org/abs/2509.08342</link>
<guid>https://arxiv.org/abs/2509.08342</guid>
<content:encoded><![CDATA[
arXiv:2509.08342v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) has emerged as a promising architecture for modern large language models (LLMs). However, massive parameters impose heavy GPU memory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs. Offloading the expert parameters to CPU RAM offers an effective way to alleviate the VRAM requirements for MoE inference. Existing approaches typically cache a small subset of experts in VRAM and dynamically prefetch experts from RAM during inference, leading to significant degradation in inference speed due to the poor cache hit rate and substantial expert loading latency. In this work, we propose MoEpic, an efficient MoE inference system with a novel expert split mechanism. Specifically, each expert is vertically divided into two segments: top and bottom. MoEpic caches the top segment of hot experts, so that more experts will be stored under the limited VRAM budget, thereby improving the cache hit rate. During each layer's inference, MoEpic predicts and prefetches the activated experts for the next layer. Since the top segments of cached experts are exempt from fetching, the loading time is reduced, which allows efficient transfer-computation overlap. Nevertheless, the performance of MoEpic critically depends on the cache configuration (i.e., each layer's VRAM budget and expert split ratio). To this end, we propose a divide-and-conquer algorithm based on fixed-point iteration for adaptive cache configuration. Extensive experiments on popular MoE LLMs demonstrate that MoEpic can save about half of the GPU cost, while lowering the inference latency by about 37.51%-65.73% compared to the baselines.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Subtrait-Level Model Explainability in Automated Writing Evaluation</title>
<link>https://arxiv.org/abs/2509.08345</link>
<guid>https://arxiv.org/abs/2509.08345</guid>
<content:encoded><![CDATA[
arXiv:2509.08345v1 Announce Type: cross 
Abstract: Subtrait (latent-trait components) assessment presents a promising path toward enhancing transparency of automated writing scores. We prototype explainability and subtrait scoring with generative language models and show modest correlation between human subtrait and trait scores, and between automated and human subtrait scores. Our approach provides details to demystify scores for educators and students.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration</title>
<link>https://arxiv.org/abs/2509.08354</link>
<guid>https://arxiv.org/abs/2509.08354</guid>
<content:encoded><![CDATA[
arXiv:2509.08354v1 Announce Type: cross 
Abstract: Tactile and kinesthetic perceptions are crucial for human dexterous manipulation, enabling reliable grasping of objects via proprioceptive sensorimotor integration. For robotic hands, even though acquiring such tactile and kinesthetic feedback is feasible, establishing a direct mapping from this sensory feedback to motor actions remains challenging. In this paper, we propose a novel glove-mediated tactile-kinematic perception-prediction framework for grasp skill transfer from human intuitive and natural operation to robotic execution based on imitation learning, and its effectiveness is validated through generalized grasping tasks, including those involving deformable objects. Firstly, we integrate a data glove to capture tactile and kinesthetic data at the joint level. The glove is adaptable for both human and robotic hands, allowing data collection from natural human hand demonstrations across different scenarios. It ensures consistency in the raw data format, enabling evaluation of grasping for both human and robotic hands. Secondly, we establish a unified representation of multi-modal inputs based on graph structures with polar coordinates. We explicitly integrate the morphological differences into the designed representation, enhancing the compatibility across different demonstrators and robotic hands. Furthermore, we introduce the Tactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), which leverage multidimensional subgraph convolutions and attention-based LSTM layers to extract spatio-temporal features from graph inputs to predict node-based states for each hand joint. These predictions are then mapped to final commands through a force-position hybrid mapping.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Detection of Inauthentic Templated Responses in English Language Assessments</title>
<link>https://arxiv.org/abs/2509.08355</link>
<guid>https://arxiv.org/abs/2509.08355</guid>
<content:encoded><![CDATA[
arXiv:2509.08355v1 Announce Type: cross 
Abstract: In high-stakes English Language Assessments, low-skill test takers may employ memorized materials called ``templates'' on essay questions to ``game'' or fool the automated scoring system. In this study, we introduce the automated detection of inauthentic, templated responses (AuDITR) task, describe a machine learning-based approach to this task and illustrate the importance of regularly updating these models in production.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>So let's replace this phrase with insult...  Lessons learned from generation of toxic texts with LLMs</title>
<link>https://arxiv.org/abs/2509.08358</link>
<guid>https://arxiv.org/abs/2509.08358</guid>
<content:encoded><![CDATA[
arXiv:2509.08358v1 Announce Type: cross 
Abstract: Modern Large Language Models (LLMs) are excellent at generating synthetic data. However, their performance in sensitive domains such as text detoxification has not received proper attention from the scientific community. This paper explores the possibility of using LLM-generated synthetic toxic data as an alternative to human-generated data for training models for detoxification. Using Llama 3 and Qwen activation-patched models, we generated synthetic toxic counterparts for neutral texts from ParaDetox and SST-2 datasets. Our experiments show that models fine-tuned on synthetic data consistently perform worse than those trained on human data, with a drop in performance of up to 30% in joint metrics. The root cause is identified as a critical lexical diversity gap: LLMs generate toxic content using a small, repetitive vocabulary of insults that fails to capture the nuances and variety of human toxicity. These findings highlight the limitations of current LLMs in this domain and emphasize the continued importance of diverse, human-annotated data for building robust detoxification systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model</title>
<link>https://arxiv.org/abs/2509.08381</link>
<guid>https://arxiv.org/abs/2509.08381</guid>
<content:encoded><![CDATA[
arXiv:2509.08381v1 Announce Type: cross 
Abstract: Deploying large language models (LLMs) for structured data extraction in domains such as financial compliance reporting, legal document analytics, and multilingual knowledge base construction is often impractical for smaller teams due to the high cost of running large architectures and the difficulty of preparing large, high-quality datasets. Most recent instruction-tuning studies focus on seven-billion-parameter or larger models, leaving limited evidence on whether much smaller models can work reliably under low-resource, multi-task conditions. This work presents ETLCH, a billion-parameter LLaMA-based model fine-tuned with low-rank adaptation on only a few hundred to one thousand samples per task for JSON extraction, knowledge graph extraction, and named entity recognition. Despite its small scale, ETLCH outperforms strong baselines across most evaluation metrics, with substantial gains observed even at the lowest data scale. These findings demonstrate that well-tuned small models can deliver stable and accurate structured outputs at a fraction of the computational cost, enabling cost-effective and reliable information extraction pipelines in resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Decoding Methods for Language Models on Encrypted Data</title>
<link>https://arxiv.org/abs/2509.08383</link>
<guid>https://arxiv.org/abs/2509.08383</guid>
<content:encoded><![CDATA[
arXiv:2509.08383v1 Announce Type: cross 
Abstract: Large language models (LLMs) power modern AI applications, but processing sensitive data on untrusted servers raises privacy concerns. Homomorphic encryption (HE) enables computation on encrypted data for secure inference. However, neural text generation requires decoding methods like argmax and sampling, which are non-polynomial and thus computationally expensive under encryption, creating a significant performance bottleneck. We introduce cutmax, an HE-friendly argmax algorithm that reduces ciphertext operations compared to prior methods, enabling practical greedy decoding under encryption. We also propose the first HE-compatible nucleus (top-p) sampling method, leveraging cutmax for efficient stochastic decoding with provable privacy guarantees. Both techniques are polynomial, supporting efficient inference in privacy-preserving settings. Moreover, their differentiability facilitates gradient-based sequence-level optimization as a polynomial alternative to straight-through estimators. We further provide strong theoretical guarantees for cutmax, proving it converges globally to a unique two-level fixed point, independent of the input values beyond the identity of the maximizer, which explains its rapid convergence in just a few iterations. Evaluations on realistic LLM outputs show latency reductions of 24x-35x over baselines, advancing secure text generation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Causality-Aware Vision-Based 3D Occupancy Prediction</title>
<link>https://arxiv.org/abs/2509.08388</link>
<guid>https://arxiv.org/abs/2509.08388</guid>
<content:encoded><![CDATA[
arXiv:2509.08388v1 Announce Type: cross 
Abstract: Vision-based 3D semantic occupancy prediction is a critical task in 3D vision that integrates volumetric 3D reconstruction with semantic understanding. Existing methods, however, often rely on modular pipelines. These modules are typically optimized independently or use pre-configured inputs, leading to cascading errors. In this paper, we address this limitation by designing a novel causal loss that enables holistic, end-to-end supervision of the modular 2D-to-3D transformation pipeline. Grounded in the principle of 2D-to-3D semantic causality, this loss regulates the gradient flow from 3D voxel representations back to the 2D features. Consequently, it renders the entire pipeline differentiable, unifying the learning process and making previously non-trainable components fully learnable. Building on this principle, we propose the Semantic Causality-Aware 2D-to-3D Transformation, which comprises three components guided by our causal loss: Channel-Grouped Lifting for adaptive semantic mapping, Learnable Camera Offsets for enhanced robustness against camera perturbations, and Normalized Convolution for effective feature propagation. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the Occ3D benchmark, demonstrating significant robustness to camera perturbations and improved 2D-to-3D semantic consistency.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Iterative LLM Framework for SIBT utilizing RAG-based Adaptive Weight Optimization</title>
<link>https://arxiv.org/abs/2509.08407</link>
<guid>https://arxiv.org/abs/2509.08407</guid>
<content:encoded><![CDATA[
arXiv:2509.08407v1 Announce Type: cross 
Abstract: Seed implant brachytherapy (SIBT) is an effective cancer treatment modality; however, clinical planning often relies on manual adjustment of objective function weights, leading to inefficiencies and suboptimal results. This study proposes an adaptive weight optimization framework for SIBT planning, driven by large language models (LLMs). A locally deployed DeepSeek-R1 LLM is integrated with an automatic planning algorithm in an iterative loop. Starting with fixed weights, the LLM evaluates plan quality and recommends new weights in the next iteration. This process continues until convergence criteria are met, after which the LLM conducts a comprehensive evaluation to identify the optimal plan. A clinical knowledge base, constructed and queried via retrieval-augmented generation (RAG), enhances the model's domain-specific reasoning. The proposed method was validated on 23 patient cases, showing that the LLM-assisted approach produces plans that are comparable to or exceeding clinically approved and fixed-weight plans, in terms of dose homogeneity for the clinical target volume (CTV) and sparing of organs at risk (OARs). The study demonstrates the potential use of LLMs in SIBT planning automation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking</title>
<link>https://arxiv.org/abs/2509.08421</link>
<guid>https://arxiv.org/abs/2509.08421</guid>
<content:encoded><![CDATA[
arXiv:2509.08421v1 Announce Type: cross 
Abstract: Multi-View Multi-Object Tracking (MVMOT) is essential for applications such as surveillance, autonomous driving, and sports analytics. However, maintaining consistent object identities across multiple cameras remains challenging due to viewpoint changes, lighting variations, and occlusions, which often lead to tracking errors.Recent methods project features from multiple cameras into a unified Bird's-Eye-View (BEV) space to improve robustness against occlusion. However, this projection introduces feature distortion and non-uniform density caused by variations in object scale with distance. These issues degrade the quality of the fused representation and reduce detection and tracking accuracy.To address these problems, we propose SCFusion, a framework that combines three techniques to improve multi-view feature integration. First, it applies a sparse transformation to avoid unnatural interpolation during projection. Next, it performs density-aware weighting to adaptively fuse features based on spatial confidence and camera distance. Finally, it introduces a multi-view consistency loss that encourages each camera to learn discriminative features independently before fusion.Experiments show that SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9% on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline method TrackTacular. These results demonstrate that SCFusion effectively mitigates the limitations of conventional BEV projection and provides a robust and accurate solution for multi-view object detection and tracking.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting</title>
<link>https://arxiv.org/abs/2509.08442</link>
<guid>https://arxiv.org/abs/2509.08442</guid>
<content:encoded><![CDATA[
arXiv:2509.08442v1 Announce Type: cross 
Abstract: Accurate forecasting of individualized, high-resolution cortical thickness (CTh) trajectories is essential for detecting subtle cortical changes, providing invaluable insights into neurodegenerative processes and facilitating earlier and more precise intervention strategies. However, CTh forecasting is a challenging task due to the intricate non-Euclidean geometry of the cerebral cortex and the need to integrate multi-modal data for subject-specific predictions. To address these challenges, we introduce the Spherical Brownian Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional conditional Brownian bridge diffusion process to forecast CTh trajectories at the vertex level of registered cortical surfaces. Our technical contribution includes a new denoising model, the conditional spherical U-Net (CoS-UNet), which combines spherical convolutions and dense cross-attention to integrate cortical surfaces and tabular conditions seamlessly. Compared to previous approaches, SBDM achieves significantly reduced prediction errors, as demonstrated by our experiments based on longitudinal datasets from the ADNI and OASIS. Additionally, we demonstrate SBDM's ability to generate individual factual and counterfactual CTh trajectories, offering a novel framework for exploring hypothetical scenarios of cortical development.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSFL: A Dual-Server Byzantine-Resilient Federated Learning Framework via Group-Based Secure Aggregation</title>
<link>https://arxiv.org/abs/2509.08449</link>
<guid>https://arxiv.org/abs/2509.08449</guid>
<content:encoded><![CDATA[
arXiv:2509.08449v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables decentralized model training without sharing raw data, offering strong privacy guarantees. However, existing FL protocols struggle to defend against Byzantine participants, maintain model utility under non-independent and identically distributed (non-IID) data, and remain lightweight for edge devices. Prior work either assumes trusted hardware, uses expensive cryptographic tools, or fails to address privacy and robustness simultaneously. We propose DSFL, a Dual-Server Byzantine-Resilient Federated Learning framework that addresses these limitations using a group-based secure aggregation approach. Unlike LSFL, which assumes non-colluding semi-honest servers, DSFL removes this dependency by revealing a key vulnerability: privacy leakage through client-server collusion. DSFL introduces three key innovations: (1) a dual-server secure aggregation protocol that protects updates without encryption or key exchange, (2) a group-wise credit-based filtering mechanism to isolate Byzantine clients based on deviation scores, and (3) a dynamic reward-penalty system for enforcing fair participation. DSFL is evaluated on MNIST, CIFAR-10, and CIFAR-100 under up to 30 percent Byzantine participants in both IID and non-IID settings. It consistently outperforms existing baselines, including LSFL, homomorphic encryption methods, and differential privacy approaches. For example, DSFL achieves 97.15 percent accuracy on CIFAR-10 and 68.60 percent on CIFAR-100, while FedAvg drops to 9.39 percent under similar threats. DSFL remains lightweight, requiring only 55.9 ms runtime and 1088 KB communication per round.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics</title>
<link>https://arxiv.org/abs/2509.08461</link>
<guid>https://arxiv.org/abs/2509.08461</guid>
<content:encoded><![CDATA[
arXiv:2509.08461v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated their remarkable capacity to process and reason over structured and unstructured data modalities beyond natural language. In this work, we explore the applications of Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa 3.2, to the task of identifying neutrino interactions in pixelated detector data from high-energy physics (HEP) experiments. We benchmark this model against a state-of-the-art convolutional neural network (CNN) architecture, similar to those used in the NOvA and DUNE experiments, which have achieved high efficiency and purity in classifying electron and muon neutrino events. Our evaluation considers both the classification performance and interpretability of the model predictions. We find that VLMs can outperform CNNs, while also providing greater flexibility in integrating auxiliary textual or semantic information and offering more interpretable, reasoning-based predictions. This work highlights the potential of VLMs as a general-purpose backbone for physics event classification, due to their high performance, interpretability, and generalizability, which opens new avenues for integrating multimodal reasoning in experimental neutrino physics.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks Against Automated Fact-Checking: A Survey</title>
<link>https://arxiv.org/abs/2509.08463</link>
<guid>https://arxiv.org/abs/2509.08463</guid>
<content:encoded><![CDATA[
arXiv:2509.08463v1 Announce Type: cross 
Abstract: In an era where misinformation spreads freely, fact-checking (FC) plays a crucial role in verifying claims and promoting reliable information. While automated fact-checking (AFC) has advanced significantly, existing systems remain vulnerable to adversarial attacks that manipulate or generate claims, evidence, or claim-evidence pairs. These attacks can distort the truth, mislead decision-makers, and ultimately undermine the reliability of FC models. Despite growing research interest in adversarial attacks against AFC systems, a comprehensive, holistic overview of key challenges remains lacking. These challenges include understanding attack strategies, assessing the resilience of current models, and identifying ways to enhance robustness. This survey provides the first in-depth review of adversarial attacks targeting FC, categorizing existing attack methodologies and evaluating their impact on AFC systems. Additionally, we examine recent advancements in adversary-aware defenses and highlight open research questions that require further exploration. Our findings underscore the urgent need for resilient FC frameworks capable of withstanding adversarial manipulations in pursuit of preserving high verification accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Learning using Mixture-of-Expert-Based Representation for Enhanced Speech Generation and Robust Emotion Recognition</title>
<link>https://arxiv.org/abs/2509.08470</link>
<guid>https://arxiv.org/abs/2509.08470</guid>
<content:encoded><![CDATA[
arXiv:2509.08470v1 Announce Type: cross 
Abstract: Speech emotion recognition (SER) plays a critical role in building emotion-aware speech systems, but its performance degrades significantly under noisy conditions. Although speech enhancement (SE) can improve robustness, it often introduces artifacts that obscure emotional cues and adds computational overhead to the pipeline. Multi-task learning (MTL) offers an alternative by jointly optimizing SE and SER tasks. However, conventional shared-backbone models frequently suffer from gradient interference and representational conflicts between tasks. To address these challenges, we propose the Sparse Mixture-of-Experts Representation Integration Technique (Sparse MERIT), a flexible MTL framework that applies frame-wise expert routing over self-supervised speech representations. Sparse MERIT incorporates task-specific gating networks that dynamically select from a shared pool of experts for each frame, enabling parameter-efficient and task-adaptive representation learning. Experiments on the MSP-Podcast corpus show that Sparse MERIT consistently outperforms baseline models on both SER and SE tasks. Under the most challenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERIT improves SER F1-macro by an average of 12.0% over a baseline relying on a SE pre-processing strategy, and by 3.4% over a naive MTL baseline, with statistical significance on unseen noise conditions. For SE, Sparse MERIT improves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline and by 20.0% over the naive MTL baseline. These results demonstrate that Sparse MERIT provides robust and generalizable performance for both emotion recognition and enhancement tasks in noisy environments.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation</title>
<link>https://arxiv.org/abs/2509.08489</link>
<guid>https://arxiv.org/abs/2509.08489</guid>
<content:encoded><![CDATA[
arXiv:2509.08489v1 Announce Type: cross 
Abstract: Prompt-driven image analysis converts a single natural-language instruction into multiple steps: locate, segment, edit, and describe. We present a practical case study of a unified pipeline that combines open-vocabulary detection, promptable segmentation, text-conditioned inpainting, and vision-language description into a single workflow. The system works end to end from a single prompt, retains intermediate artifacts for transparent debugging (such as detections, masks, overlays, edited images, and before and after composites), and provides the same functionality through an interactive UI and a scriptable CLI for consistent, repeatable runs. We highlight integration choices that reduce brittleness, including threshold adjustments, mask inspection with light morphology, and resource-aware defaults. In a small, single-word prompt segment, detection and segmentation produced usable masks in over 90% of cases with an accuracy above 85% based on our criteria. On a high-end GPU, inpainting makes up 60 to 75% of total runtime under typical guidance and sampling settings, which highlights the need for careful tuning. The study offers implementation-guided advice on thresholds, mask tightness, and diffusion parameters, and details version pinning, artifact logging, and seed control to support replay. Our contribution is a transparent, reliable pattern for assembling modern vision and multimodal models behind a single prompt, with clear guardrails and operational practices that improve reliability in object replacement, scene augmentation, and removal.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models</title>
<link>https://arxiv.org/abs/2509.08490</link>
<guid>https://arxiv.org/abs/2509.08490</guid>
<content:encoded><![CDATA[
arXiv:2509.08490v1 Announce Type: cross 
Abstract: Underwater object detection (UOD) is vital to diverse marine applications, including oceanographic research, underwater robotics, and marine conservation. However, UOD faces numerous challenges that compromise its performance. Over the years, various methods have been proposed to address these issues, but they often fail to fully capture the complexities of underwater environments. This review systematically categorizes UOD challenges into five key areas: Image quality degradation, target-related issues, data-related challenges, computational and processing constraints, and limitations in detection methodologies. To address these challenges, we analyze the progression from traditional image processing and object detection techniques to modern approaches. Additionally, we explore the potential of large vision-language models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated in other domains. We also present case studies, including synthetic dataset generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review identifies three key insights: (i) Current UOD methods are insufficient to fully address challenges like image degradation and small object detection in dynamic underwater environments. (ii) Synthetic data generation using LVLMs shows potential for augmenting datasets but requires further refinement to ensure realism and applicability. (iii) LVLMs hold significant promise for UOD, but their real-time application remains under-explored, requiring further research on optimization techniques.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Send to which account? Evaluation of an LLM-based Scambaiting System</title>
<link>https://arxiv.org/abs/2509.08493</link>
<guid>https://arxiv.org/abs/2509.08493</guid>
<content:encoded><![CDATA[
arXiv:2509.08493v1 Announce Type: cross 
Abstract: Scammers are increasingly harnessing generative AI(GenAI) technologies to produce convincing phishing content at scale, amplifying financial fraud and undermining public trust. While conventional defenses, such as detection algorithms, user training, and reactive takedown efforts remain important, they often fall short in dismantling the infrastructure scammers depend on, including mule bank accounts and cryptocurrency wallets. To bridge this gap, a proactive and emerging strategy involves using conversational honeypots to engage scammers and extract actionable threat intelligence. This paper presents the first large-scale, real-world evaluation of a scambaiting system powered by large language models (LLMs). Over a five-month deployment, the system initiated over 2,600 engagements with actual scammers, resulting in a dataset of more than 18,700 messages. It achieved an Information Disclosure Rate (IDR) of approximately 32%, successfully extracting sensitive financial information such as mule accounts. Additionally, the system maintained a Human Acceptance Rate (HAR) of around 70%, indicating strong alignment between LLM-generated responses and human operator preferences. Alongside these successes, our analysis reveals key operational challenges. In particular, the system struggled with engagement takeoff: only 48.7% of scammers responded to the initial seed message sent by defenders. These findings highlight the need for further refinement and provide actionable insights for advancing the design of automated scambaiting systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants</title>
<link>https://arxiv.org/abs/2509.08494</link>
<guid>https://arxiv.org/abs/2509.08494</guid>
<content:encoded><![CDATA[
arXiv:2509.08494v1 Announce Type: cross 
Abstract: As humans delegate more tasks and decisions to artificial intelligence (AI), we risk losing control of our individual and collective futures. Relatively simple algorithmic systems already steer human decision-making, such as social media feed algorithms that lead people to unintentionally and absent-mindedly scroll through engagement-optimized content. In this paper, we develop the idea of human agency by integrating philosophical and scientific theories of agency with AI-assisted evaluation methods: using large language models (LLMs) to simulate and validate user queries and to evaluate AI responses. We develop HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions of human agency based on typical AI use cases. HAB measures the tendency of an AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation, Correct Misinformation, Defer Important Decisions, Encourage Learning, and Maintain Social Boundaries. We find low-to-moderate agency support in contemporary LLM-based assistants and substantial variation across system developers and dimensions. For example, while Anthropic LLMs most support human agency overall, they are the least supportive LLMs in terms of Avoid Value Manipulation. Agency support does not appear to consistently result from increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and we encourage a shift towards more robust safety and alignment targets.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Rank Reduction Autoencoders for Generative</title>
<link>https://arxiv.org/abs/2509.08515</link>
<guid>https://arxiv.org/abs/2509.08515</guid>
<content:encoded><![CDATA[
arXiv:2509.08515v1 Announce Type: cross 
Abstract: Generative thermal design for complex geometries is fundamental in many areas of engineering, yet it faces two main challenges: the high computational cost of high-fidelity simulations and the limitations of conventional generative models. Approaches such as autoencoders (AEs) and variational autoencoders (VAEs) often produce unstructured latent spaces with discontinuities, which restricts their capacity to explore designs and generate physically consistent solutions.
  To address these limitations, we propose a hybrid framework that combines Variational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks (DeepONets). The VRRAE introduces a truncated SVD within the latent space, leading to continuous, interpretable, and well-structured representations that mitigate posterior collapse and improve geometric reconstruction. The DeepONet then exploits this compact latent encoding in its branch network, together with spatial coordinates in the trunk network, to predict temperature gradients efficiently and accurately.
  This hybrid approach not only enhances the quality of generated geometries and the accuracy of gradient prediction, but also provides a substantial advantage in inference efficiency compared to traditional numerical solvers. Overall, the study underscores the importance of structured latent representations for operator learning and highlights the potential of combining generative models and operator networks in thermal design and broader engineering applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMT$^{x}$: An Efficient and Asymptotically Optimal Extension of the Fast Marching Tree for Dynamic Replanning</title>
<link>https://arxiv.org/abs/2509.08521</link>
<guid>https://arxiv.org/abs/2509.08521</guid>
<content:encoded><![CDATA[
arXiv:2509.08521v1 Announce Type: cross 
Abstract: Path planning in dynamic environments remains a core challenge in robotics, especially as autonomous systems are deployed in unpredictable spaces such as warehouses and public roads. While algorithms like Fast Marching Tree (FMT$^{*}$) offer asymptotically optimal solutions in static settings, their single-pass design prevents path revisions which are essential for real-time adaptation. On the other hand, full replanning is often too computationally expensive. This paper introduces FMT$^{x}$, an extension of the Fast Marching Tree algorithm that enables efficient and consistent replanning in dynamic environments. We revisit the neighbor selection rule of FMT$^{*}$ and demonstrate that a minimal change overcomes its single-pass limitation, enabling the algorithm to update cost-to-come values upon discovering better connections without sacrificing asymptotic optimality or computational efficiency. By maintaining a cost-ordered priority queue and applying a selective update condition that uses an expanding neighbor to identify and trigger the re-evaluation of any node with a potentially suboptimal path, FMT$^{x}$ ensures that suboptimal routes are efficiently repaired as the environment evolves. This targeted strategy preserves the inherent efficiency of FMT$^{*}$ while enabling robust adaptation to changes in obstacle configuration. FMT$^{x}$ is proven to recover an asymptotically optimal solution after environmental changes. Experimental results demonstrate that FMT$^{x}$ outperforms the influential replanner RRT$^{x}$, reacting more swiftly to dynamic events with lower computational overhead and thus offering a more effective solution for real-time robotic navigation in unpredictable worlds.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution</title>
<link>https://arxiv.org/abs/2509.08524</link>
<guid>https://arxiv.org/abs/2509.08524</guid>
<content:encoded><![CDATA[
arXiv:2509.08524v1 Announce Type: cross 
Abstract: Symbolic execution is a powerful technique for software testing, but suffers from limitations when encountering external functions, such as native methods or third-party libraries. Existing solutions often require additional context, expensive SMT solvers, or manual intervention to approximate these functions through symbolic stubs. In this work, we propose a novel approach to automatically generate symbolic stubs for external functions during symbolic execution that leverages Genetic Programming. When the symbolic executor encounters an external function, AutoStub generates training data by executing the function on randomly generated inputs and collecting the outputs. Genetic Programming then derives expressions that approximate the behavior of the function, serving as symbolic stubs. These automatically generated stubs allow the symbolic executor to continue the analysis without manual intervention, enabling the exploration of program paths that were previously intractable. We demonstrate that AutoStub can automatically approximate external functions with over 90% accuracy for 55% of the functions evaluated, and can infer language-specific behaviors that reveal edge cases crucial for software testing.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agents of Discovery</title>
<link>https://arxiv.org/abs/2509.08535</link>
<guid>https://arxiv.org/abs/2509.08535</guid>
<content:encoded><![CDATA[
arXiv:2509.08535v1 Announce Type: cross 
Abstract: The substantial data volumes encountered in modern particle physics and other domains of fundamental physics research allow (and require) the use of increasingly complex data analysis tools and workflows. While the use of machine learning (ML) tools for data analysis has recently proliferated, these tools are typically special-purpose algorithms that rely, for example, on encoded physics knowledge to reach optimal performance. In this work, we investigate a new and orthogonal direction: Using recent progress in large language models (LLMs) to create a team of agents -- instances of LLMs with specific subtasks -- that jointly solve data analysis-based research problems in a way similar to how a human researcher might: by creating code to operate standard tools and libraries (including ML systems) and by building on results of previous iterations. If successful, such agent-based systems could be deployed to automate routine analysis components to counteract the increasing complexity of modern tool chains. To investigate the capabilities of current-generation commercial LLMs, we consider the task of anomaly detection via the publicly available and highly-studied LHC Olympics dataset. Several current models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated and their stability tested. Overall, we observe the capacity of the agent-based system to solve this data analysis problem. The best agent-created solutions mirror the performance of human state-of-the-art results.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models</title>
<link>https://arxiv.org/abs/2509.08538</link>
<guid>https://arxiv.org/abs/2509.08538</guid>
<content:encoded><![CDATA[
arXiv:2509.08538v1 Announce Type: cross 
Abstract: Large Video Models (LVMs) build on the semantic capabilities of Large Language Models (LLMs) and vision modules by integrating temporal information to better understand dynamic video content. Despite their progress, LVMs are prone to hallucinations-producing inaccurate or irrelevant descriptions. Current benchmarks for video hallucination depend heavily on manual categorization of video content, neglecting the perception-based processes through which humans naturally interpret videos. We introduce MESH, a benchmark designed to evaluate hallucinations in LVMs systematically. MESH uses a Question-Answering framework with binary and multi-choice formats incorporating target and trap instances. It follows a bottom-up approach, evaluating basic objects, coarse-to-fine subject features, and subject-action pairs, aligning with human video understanding. We demonstrate that MESH offers an effective and comprehensive approach for identifying hallucinations in videos. Our evaluations show that while LVMs excel at recognizing basic objects and features, their susceptibility to hallucinations increases markedly when handling fine details or aligning multiple actions involving various subjects in longer videos.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretability as Alignment: Making Internal Understanding a Design Principle</title>
<link>https://arxiv.org/abs/2509.08592</link>
<guid>https://arxiv.org/abs/2509.08592</guid>
<content:encoded><![CDATA[
arXiv:2509.08592v1 Announce Type: cross 
Abstract: Large neural models are increasingly deployed in high-stakes settings, raising concerns about whether their behavior reliably aligns with human values. Interpretability provides a route to internal transparency by revealing the computations that drive outputs. We argue that interpretability especially mechanistic approaches should be treated as a design principle for alignment, not an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer intuitive but correlational explanations, while mechanistic techniques like circuit tracing or activation patching yield causal insight into internal failures, including deceptive or misaligned reasoning that behavioral methods like RLHF, red teaming, or Constitutional AI may overlook. Despite these advantages, interpretability faces challenges of scalability, epistemic uncertainty, and mismatches between learned representations and human concepts. Our position is that progress on safe and trustworthy AI will depend on making interpretability a first-class objective of AI research and development, ensuring that systems are not only effective but also auditable, transparent, and aligned with human intent.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications</title>
<link>https://arxiv.org/abs/2509.08604</link>
<guid>https://arxiv.org/abs/2509.08604</guid>
<content:encoded><![CDATA[
arXiv:2509.08604v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated significant potential in medicine. To date, LLMs have been widely applied to tasks such as diagnostic assistance, medical question answering, and clinical information synthesis. However, a key open question remains: to what extent do LLMs memorize medical training data. In this study, we present the first comprehensive evaluation of memorization of LLMs in medicine, assessing its prevalence (how frequently it occurs), characteristics (what is memorized), volume (how much content is memorized), and potential downstream impacts (how memorization may affect medical applications). We systematically analyze common adaptation scenarios: (1) continued pretraining on medical corpora, (2) fine-tuning on standard medical benchmarks, and (3) fine-tuning on real-world clinical data, including over 13,000 unique inpatient records from Yale New Haven Health System. The results demonstrate that memorization is prevalent across all adaptation scenarios and significantly higher than reported in the general domain. Memorization affects both the development and adoption of LLMs in medicine and can be categorized into three types: beneficial (e.g., accurate recall of clinical guidelines and biomedical references), uninformative (e.g., repeated disclaimers or templated medical document language), and harmful (e.g., regeneration of dataset-specific or sensitive clinical content). Based on these findings, we offer practical recommendations to facilitate beneficial memorization that enhances domain-specific reasoning and factual accuracy, minimize uninformative memorization to promote deeper learning beyond surface-level patterns, and mitigate harmful memorization to prevent the leakage of sensitive or identifiable patient information.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification of 24-hour movement behaviors from wrist-worn accelerometer data: from handcrafted features to deep learning techniques</title>
<link>https://arxiv.org/abs/2509.08606</link>
<guid>https://arxiv.org/abs/2509.08606</guid>
<content:encoded><![CDATA[
arXiv:2509.08606v1 Announce Type: cross 
Abstract: Purpose: We compared the performance of deep learning (DL) and classical machine learning (ML) algorithms for the classification of 24-hour movement behavior into sleep, sedentary, light intensity physical activity (LPA), and moderate-to-vigorous intensity physical activity (MVPA). Methods: Open-access data from 151 adults wearing a wrist-worn accelerometer (Axivity-AX3) was used. Participants were randomly divided into training, validation, and test sets (121, 15, and 15 participants each). Raw acceleration signals were segmented into non-overlapping 10-second windows, and then a total of 104 handcrafted features were extracted. Four DL algorithms-Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), Gated Recurrent Units (GRU), and One-Dimensional Convolutional Neural Network (1D-CNN)-were trained using raw acceleration signals and with handcrafted features extracted from these signals to predict 24-hour movement behavior categories. The handcrafted features were also used to train classical ML algorithms, namely Random Forest (RF), Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Logistic Regression (LR), Artificial Neural Network (ANN), and Decision Tree (DT) for classifying 24-hour movement behavior intensities. Results: LSTM, BiLSTM, and GRU showed an overall accuracy of approximately 85% when trained with raw acceleration signals, and 1D-CNN an overall accuracy of approximately 80%. When trained on handcrafted features, the overall accuracy for both DL and classical ML algorithms ranged from 70% to 81%. Overall, there was a higher confusion in classification of MVPA and LPA, compared to sleep and sedentary categories. Conclusion: DL methods with raw acceleration signals had only slightly better performance in predicting 24-hour movement behavior intensities, compared to when DL and classical ML were trained with handcrafted features.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2509.08612</link>
<guid>https://arxiv.org/abs/2509.08612</guid>
<content:encoded><![CDATA[
arXiv:2509.08612v1 Announce Type: cross 
Abstract: Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and determine their sentiment polarity. While dependency trees combined with contextual semantics effectively identify aspect sentiment, existing methods relying on syntax trees and aspect-aware attention struggle to model complex semantic relationships. Their dependence on linear dot-product features fails to capture nonlinear associations, allowing noisy similarity from irrelevant words to obscure key opinion terms. Motivated by Differentiable Optimal Matching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph Network (OTESGN), which introduces a Syntactic-Semantic Collaborative Attention. It comprises a Syntactic Graph-Aware Attention for mining latent syntactic dependencies and modeling global syntactic topology, as well as a Semantic Optimal Transport Attention designed to uncover fine-grained semantic alignments amidst textual noise, thereby accurately capturing sentiment signals obscured by irrelevant tokens. A Adaptive Attention Fusion module integrates these heterogeneous features, and contrastive regularization further improves robustness. Experiments demonstrate that OTESGN achieves state-of-the-art results, outperforming previous best models by +1.01% F1 on Twitter and +1.30% F1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its efficacy in precise localization of opinion words and noise resistance.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image Diagnosis Augmentation</title>
<link>https://arxiv.org/abs/2509.08624</link>
<guid>https://arxiv.org/abs/2509.08624</guid>
<content:encoded><![CDATA[
arXiv:2509.08624v1 Announce Type: cross 
Abstract: Significant advancements in AI-driven multimodal medical image diagnosis have led to substantial improvements in ophthalmic disease identification in recent years. However, acquiring paired multimodal ophthalmic images remains prohibitively expensive. While fundus photography is simple and cost-effective, the limited availability of OCT data and inherent modality imbalance hinder further progress. Conventional approaches that rely solely on fundus or textual features often fail to capture fine-grained spatial information, as each imaging modality provides distinct cues about lesion predilection sites. In this study, we propose a novel unpaired multimodal framework \UOPSL that utilizes extensive OCT-derived spatial priors to dynamically identify predilection sites, enhancing fundus image-based disease recognition. Our approach bridges unpaired fundus and OCTs via extended disease text descriptions. Initially, we employ contrastive learning on a large corpus of unpaired OCT and fundus images while simultaneously learning the predilection sites matrix in the OCT latent space. Through extensive optimization, this matrix captures lesion localization patterns within the OCT feature space. During the fine-tuning or inference phase of the downstream classification task based solely on fundus images, where paired OCT data is unavailable, we eliminate OCT input and utilize the predilection sites matrix to assist in fundus image classification learning. Extensive experiments conducted on 9 diverse datasets across 28 critical categories demonstrate that our framework outperforms existing benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and Correct Image Interpretation Model Shortcuts</title>
<link>https://arxiv.org/abs/2509.08640</link>
<guid>https://arxiv.org/abs/2509.08640</guid>
<content:encoded><![CDATA[
arXiv:2509.08640v1 Announce Type: cross 
Abstract: Chest radiographs (CXRs) are among the most common tests in medicine. Automated image interpretation may reduce radiologists\' workload and expand access to diagnostic expertise. Deep learning multi-task and foundation models have shown strong performance for CXR interpretation but are vulnerable to shortcut learning, where models rely on spurious and off-target correlations rather than clinically relevant features to make decisions. We introduce RoentMod, a counterfactual image editing framework that generates anatomically realistic CXRs with user-specified, synthetic pathology while preserving unrelated anatomical features of the original scan. RoentMod combines an open-source medical image generator (RoentGen) with an image-to-image modification model without requiring retraining. In reader studies with board-certified radiologists and radiology residents, RoentMod-produced images appeared realistic in 93\% of cases, correctly incorporated the specified finding in 89-99\% of cases, and preserved native anatomy comparable to real follow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task and foundation models frequently exploit off-target pathology as shortcuts, limiting their specificity. Incorporating RoentMod-generated counterfactual images during training mitigated this vulnerability, improving model discrimination across multiple pathologies by 3-19\% AUC in internal validation and by 1-11\% for 5 out of 6 tested pathologies in external testing. These findings establish RoentMod as a broadly applicable tool for probing and correcting shortcut learning in medical AI. By enabling controlled counterfactual interventions, RoentMod enhances the robustness and interpretability of CXR interpretation models and provides a generalizable strategy for improving foundation models in medical imaging.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations</title>
<link>https://arxiv.org/abs/2509.08646</link>
<guid>https://arxiv.org/abs/2509.08646</guid>
<content:encoded><![CDATA[
arXiv:2509.08646v1 Announce Type: cross 
Abstract: As Large Language Model (LLM) agents become increasingly capable of automating complex, multi-step tasks, the need for robust, secure, and predictable architectural patterns is paramount. This paper provides a comprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic design that separates strategic planning from tactical execution. We explore the foundational principles of P-t-E, detailing its core components - the Planner and the Executor - and its architectural advantages in predictability, cost-efficiency, and reasoning quality over reactive patterns like ReAct (Reason + Act). A central focus is placed on the security implications of this design, particularly its inherent resilience to indirect prompt injection attacks by establishing control-flow integrity. We argue that while P-t-E provides a strong foundation, a defense-in-depth strategy is necessary, and we detail essential complementary controls such as the Principle of Least Privilege, task-scoped tool access, and sandboxed code execution. To make these principles actionable, this guide provides detailed implementation blueprints and working code references for three leading agentic frameworks: LangChain (via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing the P-t-E pattern is analyzed, highlighting unique features like LangGraph's stateful graphs for re-planning, CrewAI's declarative tool scoping for security, and AutoGen's built-in Docker sandboxing. Finally, we discuss advanced patterns, including dynamic re-planning loops, parallel execution with Directed Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop (HITL) verification, to offer a complete strategic blueprint for architects, developers, and security engineers aiming to build production-grade, resilient, and trustworthy LLM agents.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Belief-State Policy Learning for Quantum Network Routing Under Decoherence and Time-Varying Conditions</title>
<link>https://arxiv.org/abs/2509.08654</link>
<guid>https://arxiv.org/abs/2509.08654</guid>
<content:encoded><![CDATA[
arXiv:2509.08654v1 Announce Type: cross 
Abstract: This paper presents a feature-based Partially Observable Markov Decision Process (POMDP) framework for quantum network routing, combining belief-state planning with Graph Neural Networks (GNNs) to address partial observability, decoherence, and scalability challenges in dynamic quantum systems. Our approach encodes complex quantum network dynamics, including entanglement degradation and time-varying channel noise, into a low-dimensional feature space, enabling efficient belief updates and scalable policy learning. The core of our framework is a hybrid GNN-POMDP architecture that processes graph-structured representations of entangled links to learn routing policies, coupled with a noise-adaptive mechanism that fuses POMDP belief updates with GNN outputs for robust decision making. We provide a theoretical analysis establishing guarantees for belief convergence, policy improvement, and robustness to noise. Experiments on simulated quantum networks with up to 100 nodes demonstrate significant improvements in routing fidelity and entanglement delivery rates compared to state-of-the-art baselines, particularly under high decoherence and nonstationary conditions.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network</title>
<link>https://arxiv.org/abs/2509.08661</link>
<guid>https://arxiv.org/abs/2509.08661</guid>
<content:encoded><![CDATA[
arXiv:2509.08661v1 Announce Type: cross 
Abstract: Isolated Sign Language Recognition (ISLR) is challenged by gestures that are morphologically similar yet semantically distinct, a problem rooted in the complex interplay between hand shape and motion trajectory. Existing methods, often relying on a single reference frame, struggle to resolve this geometric ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a dual-reference, dual-stream architecture that decouples and models gesture morphology and trajectory in separate, complementary coordinate systems. Our approach utilizes a wrist-centric frame for view-invariant shape analysis and a facial-centric frame for context-aware trajectory modeling. These streams are processed by specialized networks-a topology-aware graph convolution for shape and a Finsler geometry-based encoder for trajectory-and are integrated via a geometry-driven optimal transport fusion mechanism. DSLNet sets a new state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with significantly fewer parameters than competing models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reshaping the Forward-Forward Algorithm with a Similarity-Based Objective</title>
<link>https://arxiv.org/abs/2509.08697</link>
<guid>https://arxiv.org/abs/2509.08697</guid>
<content:encoded><![CDATA[
arXiv:2509.08697v1 Announce Type: cross 
Abstract: Backpropagation is the pivotal algorithm underpinning the success of artificial neural networks, yet it has critical limitations such as biologically implausible backward locking and global error propagation. To circumvent these constraints, the Forward-Forward algorithm was proposed as a more biologically plausible method that replaces the backward pass with an additional forward pass. Despite this advantage, the Forward-Forward algorithm significantly trails backpropagation in accuracy, and its optimal form exhibits low inference efficiency due to multiple forward passes required. In this work, the Forward-Forward algorithm is reshaped through its integration with similarity learning frameworks, eliminating the need for multiple forward passes during inference. This proposed algorithm is named Forward-Forward Algorithm Unified with Similarity-based Tuplet loss (FAUST). Empirical evaluations on MNIST, Fashion-MNIST, and CIFAR-10 datasets indicate that FAUST substantially improves accuracy, narrowing the gap with backpropagation. On CIFAR-10, FAUST achieves 56.22\% accuracy with a simple multi-layer perceptron architecture, approaching the backpropagation benchmark of 57.63\% accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A layered architecture for log analysis in complex IT systems</title>
<link>https://arxiv.org/abs/2509.08698</link>
<guid>https://arxiv.org/abs/2509.08698</guid>
<content:encoded><![CDATA[
arXiv:2509.08698v1 Announce Type: cross 
Abstract: In the evolving IT landscape, stability and reliability of systems are essential, yet their growing complexity challenges DevOps teams in implementation and maintenance. Log analysis, a core element of AIOps, provides critical insights into complex behaviors and failures. This dissertation introduces a three-layered architecture to support DevOps in failure resolution. The first layer, Log Investigation, performs autonomous log labeling and anomaly classification. We propose a method that labels log data without manual effort, enabling supervised training and precise evaluation of anomaly detection. Additionally, we define a taxonomy that groups anomalies into three categories, ensuring appropriate method selection. The second layer, Anomaly Detection, detects behaviors deviating from the norm. We propose a flexible Anomaly Detection method adaptable to unsupervised, weakly supervised, and supervised training. Evaluations on public and industry datasets show F1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third layer, Root Cause Analysis, identifies minimal log sets describing failures, their origin, and event sequences. By balancing training data and identifying key services, our Root Cause Analysis method consistently detects 90-98% of root cause log lines within the top 10 candidates, providing actionable insights for mitigation. Our research addresses how log analysis methods can be designed and optimized to help DevOps resolve failures efficiently. By integrating these three layers, the architecture equips teams with robust methods to enhance IT system reliability.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</title>
<link>https://arxiv.org/abs/2509.08699</link>
<guid>https://arxiv.org/abs/2509.08699</guid>
<content:encoded><![CDATA[
arXiv:2509.08699v1 Announce Type: cross 
Abstract: Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability of CNN Based Classification Models for Acoustic Signal</title>
<link>https://arxiv.org/abs/2509.08717</link>
<guid>https://arxiv.org/abs/2509.08717</guid>
<content:encoded><![CDATA[
arXiv:2509.08717v1 Announce Type: cross 
Abstract: Explainable Artificial Intelligence (XAI) has emerged as a critical tool for interpreting the predictions of complex deep learning models. While XAI has been increasingly applied in various domains within acoustics, its use in bioacoustics, which involves analyzing audio signals from living organisms, remains relatively underexplored. In this paper, we investigate the vocalizations of a bird species with strong geographic variation throughout its range in North America. Audio recordings were converted into spectrogram images and used to train a deep Convolutional Neural Network (CNN) for classification, achieving an accuracy of 94.8\%. To interpret the model's predictions, we applied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT, Grad-CAM) XAI techniques. These techniques produced different but complementary explanations, and when their explanations were considered together, they provided more complete and interpretable insights into the model's decision-making. This work highlights the importance of using a combination of XAI techniques to improve trust and interoperability, not only in broader acoustics signal analysis but also argues for broader applicability in different domain specific tasks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</title>
<link>https://arxiv.org/abs/2509.08729</link>
<guid>https://arxiv.org/abs/2509.08729</guid>
<content:encoded><![CDATA[
arXiv:2509.08729v1 Announce Type: cross 
Abstract: Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one structured prompt, but prior work relied on a handful of manually written templates. We present X-Teaming Evolutionary M2S, an automated framework that discovers and optimizes M2S templates through language-model-guided evolution. The system pairs smart sampling from 12 sources with an LLM-as-judge inspired by StrongREJECT and records fully auditable logs.
  Maintaining selection pressure by setting the success threshold to $\theta = 0.70$, we obtain five evolutionary generations, two new template families, and 44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of 2,500 trials (judge fixed) shows that structural gains transfer but vary by target; two models score zero at the same threshold. We also find a positive coupling between prompt length and score, motivating length-aware judging.
  Our results demonstrate that structure-level search is a reproducible route to stronger single-turn probes and underscore the importance of threshold calibration and cross-model evaluation. Code, configurations, and artifacts are available at https://github.com/hyunjun1121/M2S-x-teaming.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEQuify your force field: More efficient simulations using deep equilibrium models</title>
<link>https://arxiv.org/abs/2509.08734</link>
<guid>https://arxiv.org/abs/2509.08734</guid>
<content:encoded><![CDATA[
arXiv:2509.08734v1 Announce Type: cross 
Abstract: Machine learning force fields show great promise in enabling more accurate molecular dynamics simulations compared to manually derived ones. Much of the progress in recent years was driven by exploiting prior knowledge about physical systems, in particular symmetries under rotation, translation, and reflections. In this paper, we argue that there is another important piece of prior information that, thus fa,r hasn't been explored: Simulating a molecular system is necessarily continuous, and successive states are therefore extremely similar. Our contribution is to show that we can exploit this information by recasting a state-of-the-art equivariant base model as a deep equilibrium model. This allows us to recycle intermediate neural network features from previous time steps, enabling us to improve both accuracy and speed by $10\%-20\%$ on the MD17, MD22, and OC20 200k datasets, compared to the non-DEQ base model. The training is also much more memory efficient, allowing us to train more expressive models on larger systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinZero: Launching Multi-modal Financial Time Series Forecast with Large Reasoning Model</title>
<link>https://arxiv.org/abs/2509.08742</link>
<guid>https://arxiv.org/abs/2509.08742</guid>
<content:encoded><![CDATA[
arXiv:2509.08742v1 Announce Type: cross 
Abstract: Financial time series forecasting is both highly significant and challenging. Previous approaches typically standardized time series data before feeding it into forecasting models, but this encoding process inherently leads to a loss of important information. Moreover, past time series models generally require fixed numbers of variables or lookback window lengths, which further limits the scalability of time series forecasting. Besides, the interpretability and the uncertainty in forecasting remain areas requiring further research, as these factors directly impact the reliability and practical value of predictions. To address these issues, we first construct a diverse financial image-text dataset (FVLDB) and develop the Uncertainty-adjusted Group Relative Policy Optimization (UARPO) method to enable the model not only output predictions but also analyze the uncertainty of those predictions. We then proposed FinZero, a multimodal pre-trained model finetuned by UARPO to perform reasoning, prediction, and analytical understanding on the FVLDB financial time series. Extensive experiments validate that FinZero exhibits strong adaptability and scalability. After fine-tuning with UARPO, FinZero achieves an approximate 13.48\% improvement in prediction accuracy over GPT-4o in the high-confidence group, demonstrating the effectiveness of reinforcement learning fine-tuning in multimodal large model, including in financial time series forecasting tasks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Turbulent Flows with Generative Models: Super-resolution, Forecasting, and Sparse Flow Reconstruction</title>
<link>https://arxiv.org/abs/2509.08752</link>
<guid>https://arxiv.org/abs/2509.08752</guid>
<content:encoded><![CDATA[
arXiv:2509.08752v1 Announce Type: cross 
Abstract: Neural operators are promising surrogates for dynamical systems but when trained with standard L2 losses they tend to oversmooth fine-scale turbulent structures. Here, we show that combining operator learning with generative modeling overcomes this limitation. We consider three practical turbulent-flow challenges where conventional neural operators fail: spatio-temporal super-resolution, forecasting, and sparse flow reconstruction. For Schlieren jet super-resolution, an adversarially trained neural operator (adv-NO) reduces the energy-spectrum error by 15x while preserving sharp gradients at neural operator-like inference cost. For 3D homogeneous isotropic turbulence, adv-NO trained on only 160 timesteps from a single trajectory forecasts accurately for five eddy-turnover times and offers 114x wall-clock speed-up at inference than the baseline diffusion-based forecasters, enabling near-real-time rollouts. For reconstructing cylinder wake flows from highly sparse Particle Tracking Velocimetry-like inputs, a conditional generative model infers full 3D velocity and pressure fields with correct phase alignment and statistics. These advances enable accurate reconstruction and forecasting at low compute cost, bringing near-real-time analysis and control within reach in experimental and computational fluid mechanics. See our project page: https://vivekoommen.github.io/Gen4Turb/
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.08755</link>
<guid>https://arxiv.org/abs/2509.08755</guid>
<content:encoded><![CDATA[
arXiv:2509.08755v1 Announce Type: cross 
Abstract: Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform</title>
<link>https://arxiv.org/abs/2509.08756</link>
<guid>https://arxiv.org/abs/2509.08756</guid>
<content:encoded><![CDATA[
arXiv:2509.08756v1 Announce Type: cross 
Abstract: Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid, accurate patient-hospital allocation decisions under extreme pressure. Here, we developed and validated a deep reinforcement learning-based decision-support AI agent to optimize patient transfer decisions during simulated MCIs by balancing patient acuity levels, specialized care requirements, hospital capacities, and transport logistics. To integrate this AI agent, we developed MasTER, a web-accessible command dashboard for MCI management simulations. Through a controlled user study with 30 participants (6 trauma experts and 24 non-experts), we evaluated three interaction approaches with the AI agent (human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI scenarios in the Greater Toronto Area. Results demonstrate that increasing AI involvement significantly improves decision quality and consistency. The AI agent outperforms trauma surgeons (p < 0.001) and enables non-experts to achieve expert-level performance when assisted, contrasting sharply with their significantly inferior unassisted performance (p < 0.001). These findings establish the potential for our AI-driven decision support to enhance both MCI preparedness training and real-world emergency response management.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using Mobile-Captured Skin Images</title>
<link>https://arxiv.org/abs/2509.08780</link>
<guid>https://arxiv.org/abs/2509.08780</guid>
<content:encoded><![CDATA[
arXiv:2509.08780v1 Announce Type: cross 
Abstract: Background: Arsenicosis is a serious public health concern in South and Southeast Asia, primarily caused by long-term consumption of arsenic-contaminated water. Its early cutaneous manifestations are clinically significant but often underdiagnosed, particularly in rural areas with limited access to dermatologists. Automated, image-based diagnostic solutions can support early detection and timely interventions.
  Methods: In this study, we propose an end-to-end framework for arsenicosis diagnosis using mobile phone-captured skin images. A dataset comprising 20 classes and over 11000 images of arsenic-induced and other dermatological conditions was curated. Multiple deep learning architectures, including convolutional neural networks (CNNs) and Transformer-based models, were benchmarked for arsenicosis detection. Model interpretability was integrated via LIME and Grad-CAM, while deployment feasibility was demonstrated through a web-based diagnostic tool.
  Results: Transformer-based models significantly outperformed CNNs, with the Swin Transformer achieving the best results (86\\% accuracy). LIME and Grad-CAM visualizations confirmed that the models attended to lesion-relevant regions, increasing clinical transparency and aiding in error analysis. The framework also demonstrated strong performance on external validation samples, confirming its ability to generalize beyond the curated dataset.
  Conclusion: The proposed framework demonstrates the potential of deep learning for non-invasive, accessible, and explainable diagnosis of arsenicosis from mobile-acquired images. By enabling reliable image-based screening, it can serve as a practical diagnostic aid in rural and resource-limited communities, where access to dermatologists is scarce, thereby supporting early detection and timely intervention.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PianoVAM: A Multimodal Piano Performance Dataset</title>
<link>https://arxiv.org/abs/2509.08800</link>
<guid>https://arxiv.org/abs/2509.08800</guid>
<content:encoded><![CDATA[
arXiv:2509.08800v1 Announce Type: cross 
Abstract: The multimodal nature of music performance has driven increasing interest in data beyond the audio domain within the music information retrieval (MIR) community. This paper introduces PianoVAM, a comprehensive piano performance dataset that includes videos, audio, MIDI, hand landmarks, fingering labels, and rich metadata. The dataset was recorded using a Disklavier piano, capturing audio and MIDI from amateur pianists during their daily practice sessions, alongside synchronized top-view videos in realistic and varied performance conditions. Hand landmarks and fingering labels were extracted using a pretrained hand pose estimation model and a semi-automated fingering annotation algorithm. We discuss the challenges encountered during data collection and the alignment process across different modalities. Additionally, we describe our fingering annotation method based on hand landmarks extracted from videos. Finally, we present benchmarking results for both audio-only and audio-visual piano transcription using the PianoVAM dataset and discuss additional potential applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Truth: The Confidence Paradox in AI Fact-Checking</title>
<link>https://arxiv.org/abs/2509.08803</link>
<guid>https://arxiv.org/abs/2509.08803</guid>
<content:encoded><![CDATA[
arXiv:2509.08803v1 Announce Type: cross 
Abstract: The rise of misinformation underscores the need for scalable and reliable fact-checking solutions. Large language models (LLMs) hold promise in automating fact verification, yet their effectiveness across global contexts remains uncertain. We systematically evaluate nine established LLMs across multiple categories (open/closed-source, multiple sizes, diverse architectures, reasoning-based) using 5,000 claims previously assessed by 174 professional fact-checking organizations across 47 languages. Our methodology tests model generalizability on claims postdating training cutoffs and four prompting strategies mirroring both citizen and professional fact-checker interactions, with over 240,000 human annotations as ground truth. Findings reveal a concerning pattern resembling the Dunning-Kruger effect: smaller, accessible models show high confidence despite lower accuracy, while larger models demonstrate higher accuracy but lower confidence. This risks systemic bias in information verification, as resource-constrained organizations typically use smaller models. Performance gaps are most pronounced for non-English languages and claims originating from the Global South, threatening to widen existing information inequalities. These results establish a multilingual benchmark for future research and provide an evidence base for policy aimed at ensuring equitable access to trustworthy, AI-assisted fact-checking.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoVoC: Morphology-Aware Subword Construction for Geez Script Languages</title>
<link>https://arxiv.org/abs/2509.08812</link>
<guid>https://arxiv.org/abs/2509.08812</guid>
<content:encoded><![CDATA[
arXiv:2509.08812v1 Announce Type: cross 
Abstract: Subword-based tokenization methods often fail to preserve morphological boundaries, a limitation especially pronounced in low-resource, morphologically complex languages such as those written in the Geez script. To address this, we present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into the subword vocabulary. This hybrid segmentation approach combines morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological integrity while maintaining lexical meaning. To tackle resource scarcity, we curate and release manually annotated morpheme data for four Geez script languages and a morpheme-aware vocabulary for two of them. While the proposed tokenization method does not lead to significant gains in automatic translation quality, we observe consistent improvements in intrinsic metrics, MorphoScore, and Boundary Precision, highlighting the value of morphology-aware segmentation in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated datasets and tokenizer will be publicly available to support further research in low-resource, morphologically rich languages. Our code and data are available on GitHub: https://github.com/hailaykidu/MoVoC
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merge-of-Thought Distillation</title>
<link>https://arxiv.org/abs/2509.08814</link>
<guid>https://arxiv.org/abs/2509.08814</guid>
<content:encoded><![CDATA[
arXiv:2509.08814v1 Announce Type: cross 
Abstract: Efficient reasoning distillation for long chain-of-thought (CoT) models is increasingly constrained by the assumption of a single oracle teacher, despite practical availability of multiple candidate teachers and growing CoT corpora. We revisit teacher selection and observe that different students have different "best teachers," and even for the same student the best teacher can vary across datasets. Therefore, to unify multiple teachers' reasoning abilities into student with overcoming conflicts among various teachers' supervision, we propose Merge-of-Thought Distillation (MoT), a lightweight framework that alternates between teacher-specific supervised fine-tuning branches and weight-space merging of the resulting student variants. On competition math benchmarks, using only about 200 high-quality CoT samples, applying MoT to a Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B, QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT consistently outperforms the best single-teacher distillation and the naive multi-teacher union, raises the performance ceiling while mitigating overfitting, and shows robustness to distribution-shifted and peer-level teachers. Moreover, MoT reduces catastrophic forgetting, improves general reasoning beyond mathematics and even cultivates a better teacher, indicating that consensus-filtered reasoning features transfer broadly. These results position MoT as a simple, scalable route to efficiently distilling long CoT capabilities from diverse teachers into compact students.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QCardEst/QCardCorr: Quantum Cardinality Estimation and Correction</title>
<link>https://arxiv.org/abs/2509.08817</link>
<guid>https://arxiv.org/abs/2509.08817</guid>
<content:encoded><![CDATA[
arXiv:2509.08817v1 Announce Type: cross 
Abstract: Cardinality estimation is an important part of query optimization in DBMS. We develop a Quantum Cardinality Estimation (QCardEst) approach using Quantum Machine Learning with a Hybrid Quantum-Classical Network. We define a compact encoding for turning SQL queries into a quantum state, which requires only qubits equal to the number of tables in the query. This allows the processing of a complete query with a single variational quantum circuit (VQC) on current hardware. In addition, we compare multiple classical post-processing layers to turn the probability vector output of VQC into a cardinality value. We introduce Quantum Cardinality Correction QCardCorr, which improves classical cardinality estimators by multiplying the output with a factor generated by a VQC to improve the cardinality estimation. With QCardCorr, we have an improvement over the standard PostgreSQL optimizer of 6.37 times for JOB-light and 8.66 times for STATS. For JOB-light we even outperform MSCN by a factor of 3.47.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation</title>
<link>https://arxiv.org/abs/2509.08825</link>
<guid>https://arxiv.org/abs/2509.08825</guid>
<content:encoded><![CDATA[
arXiv:2509.08825v1 Announce Type: cross 
Abstract: Large language models (LLMs) are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection, prompting strategy, or temperature settings). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type I, Type II, Type S, or Type M errors. We call this LLM hacking.
  We quantify the risk of LLM hacking by replicating 37 data annotation tasks from 21 published social science research studies with 18 different models. Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure how plausible researcher choices affect statistical conclusions. We find incorrect conclusions based on LLM-annotated data in approximately one in three hypotheses for state-of-the-art models, and in half the hypotheses for small language models. While our findings show that higher task performance and better general model capabilities reduce LLM hacking risk, even highly accurate models do not completely eliminate it. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of findings near significance thresholds. Our extensive analysis of LLM hacking mitigation techniques emphasizes the importance of human annotations in reducing false positive findings and improving model selection. Surprisingly, common regression estimator correction techniques are largely ineffective in reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.
  Beyond accidental errors, we find that intentional LLM hacking is unacceptably simple. With few LLMs and just a handful of prompt paraphrases, anything can be presented as statistically significant.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2509.08827</link>
<guid>https://arxiv.org/abs/2509.08827</guid>
<content:encoded><![CDATA[
arXiv:2509.08827v1 Announce Type: cross 
Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Bounded Epistemic Planning</title>
<link>https://arxiv.org/abs/2406.01139</link>
<guid>https://arxiv.org/abs/2406.01139</guid>
<content:encoded><![CDATA[
arXiv:2406.01139v2 Announce Type: replace 
Abstract: We propose a novel algorithm for epistemic planning based on dynamic epistemic logic (DEL). The novelty is that we limit the depth of reasoning of the planning agent to an upper bound b, meaning that the planning agent can only reason about higher-order knowledge to at most (modal) depth b. We then compute a plan requiring the lowest reasoning depth by iteratively incrementing the value of b. The algorithm relies at its core on a new type of "canonical" b-bisimulation contraction that guarantees unique minimal models by construction. This yields smaller states wrt. standard bisimulation contractions, and enables to efficiently check for visited states. We show soundness and completeness of our planning algorithm, under suitable bounds on reasoning depth, and that, for a bound b, it runs in (b+1)-EXPTIME. We implement the algorithm in a novel epistemic planner, DAEDALUS, and compare it to the EFP 2.0 planner on several benchmarks from the literature, showing effective performance improvements.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Associative Knowledge Graphs for Efficient Sequence Storage and Retrieval</title>
<link>https://arxiv.org/abs/2411.14480</link>
<guid>https://arxiv.org/abs/2411.14480</guid>
<content:encoded><![CDATA[
arXiv:2411.14480v2 Announce Type: replace 
Abstract: The paper addresses challenges in storing and retrieving sequences in contexts like anomaly detection, behavior prediction, and genetic information analysis. Associative Knowledge Graphs (AKGs) offer a promising approach by leveraging sparse graph structures to encode sequences. The objective was to develop a method for sequence storage and retrieval using AKGs that maintain high memory capacity and context-based retrieval accuracy while introducing algorithms for efficient element ordering. The study utilized Sequential Structural Associative Knowledge Graphs (SSAKGs). These graphs encode sequences as transitive tournaments with nodes representing objects and edges defining the order. Four ordering algorithms were developed and tested: Simple Sort, Node Ordering, Enhanced Node Ordering, and Weighted Edges Node Ordering. The evaluation was conducted on synthetic datasets consisting of random sequences of varying lengths and distributions, and real-world datasets, including sentence-based sequences from the NLTK library and miRNA sequences mapped symbolically with a window-based approach. Metrics such as precision, sensitivity, and specificity were employed to assess performance. SSAKGs exhibited quadratic growth in memory capacity relative to graph size. This study introduces a novel structural approach for sequence storage and retrieval. Key advantages include no training requirements, flexible context-based reconstruction, and high efficiency in sparse memory graphs. With broad applications in computational neuroscience and bioinformatics, the approach offers scalable solutions for sequence-based memory tasks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research</title>
<link>https://arxiv.org/abs/2502.12669</link>
<guid>https://arxiv.org/abs/2502.12669</guid>
<content:encoded><![CDATA[
arXiv:2502.12669v2 Announce Type: replace 
Abstract: The rapid advancement of perovskite solar cells (PSCs) has led to an exponential growth in research publications, creating an urgent need for efficient knowledge management and reasoning systems in this domain. We present a comprehensive knowledge-enhanced system for PSCs that integrates three key components. First, we develop Perovskite-KG, a domain-specific knowledge graph constructed from 1,517 research papers, containing 23,789 entities and 22,272 relationships. Second, we create two complementary datasets: Perovskite-Chat, comprising 55,101 high-quality question-answer pairs generated through a novel multi-agent framework, and Perovskite-Reasoning, containing 2,217 carefully curated materials science problems. Third, we introduce two specialized large language models: Perovskite-Chat-LLM for domain-specific knowledge assistance and Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental results demonstrate that our system significantly outperforms existing models in both domain-specific knowledge retrieval and scientific reasoning tasks, providing researchers with effective tools for literature review, experimental design, and complex problem-solving in PSC research.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Semantics Augmented Few-Shot Relational Learning</title>
<link>https://arxiv.org/abs/2505.05684</link>
<guid>https://arxiv.org/abs/2505.05684</guid>
<content:encoded><![CDATA[
arXiv:2505.05684v2 Announce Type: replace 
Abstract: Few-shot relational learning on knowledge graph (KGs) aims to perform reasoning over relations with only a few training examples. While existing methods have primarily focused on leveraging specific relational information, rich semantics inherent in KGs have been largely overlooked. To address this critical gap, we propose a novel prompted meta-learning (PromptMeta) framework that seamlessly integrates meta-semantics with relational information for few-shot relational learning. PromptMeta has two key innovations: (1) a Meta-Semantic Prompt (MSP) pool that learns and consolidates high-level meta-semantics, enabling effective knowledge transfer and adaptation to rare and newly emerging relations; and (2) a learnable fusion token that dynamically combines meta-semantics with task-specific relational information tailored to different few-shot tasks. Both components are optimized jointly with model parameters within a meta-learning framework. Extensive experiments and analyses on two real-world KG datasets demonstrate the effectiveness of PromptMeta in adapting to new relations with limited data.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing</title>
<link>https://arxiv.org/abs/2506.23141</link>
<guid>https://arxiv.org/abs/2506.23141</guid>
<content:encoded><![CDATA[
arXiv:2506.23141v2 Announce Type: replace 
Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge Graph Completion (KGC), providing vital cues for prediction. However, traditional node-based message passing mechanisms, when applied to knowledge graphs, often introduce noise and suffer from information dilution or over-smoothing by indiscriminately aggregating information from all neighboring edges. To address this challenge, we propose a semantic-aware relational message passing. A core innovation of this framework is the introduction of a semantic-aware Top-K neighbor selection strategy. Specifically, this strategy first evaluates the semantic relevance between a central node and its incident edges within a shared latent space, selecting only the Top-K most pertinent ones. Subsequently, information from these selected edges is effectively fused with the central node's own representation using a multi-head attention aggregator to generate a semantically focused node message. In this manner, our model not only leverages the structure and features of edges within the knowledge graph but also more accurately captures and propagates the contextual information most relevant to the specific link prediction task, thereby effectively mitigating interference from irrelevant information. Extensive experiments demonstrate that our method achieves superior performance compared to existing approaches on several established benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation</title>
<link>https://arxiv.org/abs/2507.02253</link>
<guid>https://arxiv.org/abs/2507.02253</guid>
<content:encoded><![CDATA[
arXiv:2507.02253v4 Announce Type: replace 
Abstract: Robust workflow composition is critical for effective agent performance, yet progress in Large Language Model (LLM) planning and reasoning is hindered by a scarcity of scalable evaluation data. This work introduces NL2Flow, a fully automated pipeline for generating and evaluating workflow planning problems. NL2Flow generates problems parametrically in a structured intermediate representation, translating them into both natural language and formal PDDL. I evaluate several open-source, instruct-tuned LLMs on a dataset of 2296 low-difficulty problems generated by NL2Flow. Results demonstrate that the best-performing model achieved 86% success in generating valid plans and 69% in generating optimal plans (for solvable problems). Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. Importantly, translating natural language problems into a structured JSON representation prior to symbolic planning significantly improved success rates, suggesting a benefit from neuro-symbolic integration. These findings underscore the importance of understanding error sources within LLM reasoning as systems scale to more complex tasks. As LLM reasoning scales to increasingly complex problems, understanding the shifting bottlenecks and sources of error within these systems will be crucial.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Working with AI: Measuring the Applicability of Generative AI to Occupations</title>
<link>https://arxiv.org/abs/2507.07935</link>
<guid>https://arxiv.org/abs/2507.07935</guid>
<content:encoded><![CDATA[
arXiv:2507.07935v4 Announce Type: replace 
Abstract: Given the rapid adoption of generative AI and its potential to impact a wide range of tasks, understanding the effects of AI on the economy is one of society's most important questions. In this work, we take a step toward that goal by analyzing the work activities people do with AI, how successfully and broadly those activities are done, and combine that with data on what occupations do those activities. We analyze a dataset of 200k anonymized and privacy-scrubbed conversations between users and Microsoft Bing Copilot, a publicly available generative AI system. We find the most common work activities people seek AI assistance for involve gathering information and writing, while the most common activities that AI itself is performing are providing information and assistance, writing, teaching, and advising. Combining these activity classifications with measurements of task success and scope of impact, we compute an AI applicability score for each occupation. We find the highest AI applicability scores for knowledge work occupation groups such as computer and mathematical, and office and administrative support, as well as occupations such as sales whose work activities involve providing and communicating information. Additionally, we characterize the types of work activities performed most successfully, how wage and education correlate with AI applicability, and how real-world usage compares to predictions of occupational AI impact.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding visual attention beehind bee-inspired UAV navigation</title>
<link>https://arxiv.org/abs/2507.11992</link>
<guid>https://arxiv.org/abs/2507.11992</guid>
<content:encoded><![CDATA[
arXiv:2507.11992v3 Announce Type: replace 
Abstract: Bio-inspired design is often used in autonomous UAV navigation due to the capacity of biological systems for flight and obstacle avoidance despite limited sensory and computational capabilities. In particular, honeybees mainly use the sensory input of optic flow, the apparent motion of objects in their visual field, to navigate cluttered environments. In our work, we train a Reinforcement Learning agent to navigate a tunnel with obstacles using only optic flow as sensory input. We inspect the attention patterns of trained agents to determine the regions of optic flow on which they primarily base their motor decisions. We find that agents trained in this way pay most attention to regions of discontinuity in optic flow, as well as regions with large optic flow magnitude. The trained agents appear to navigate a cluttered tunnel by avoiding the obstacles that produce large optic flow, while maintaining a centered position in their environment, which resembles the behavior seen in flying insects. This pattern persists across independently trained agents, which suggests that this could be a good strategy for developing a simple explicit control law for physical UAVs.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2508.16129</link>
<guid>https://arxiv.org/abs/2508.16129</guid>
<content:encoded><![CDATA[
arXiv:2508.16129v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) have recently demonstrated remarkable reasoning abilities with reinforcement learning paradigm. Although several multimodal reasoning models have been explored in the medical domain, most of them focus exclusively on basic reasoning, which refers to shallow inference based on visual feature matching. However, real-world clinical diagnosis extends beyond basic reasoning, demanding reasoning processes that integrate heterogeneous clinical information (such as chief complaints and medical history) with multimodal medical imaging data. To bridge this gap, we introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the full spectrum of perception and reasoning. It encompasses both basic reasoning tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental reasoning capabilities and emulate realistic clinical thinking patterns. Building upon MM-Retinal-Reason, we propose OphthaReason, the first ophthalmology-specific multimodal reasoning model with step-by-step reasoning traces. To enable flexible adaptation to both basic and complex reasoning tasks, we specifically design a novel method called Uncertainty-Aware Dynamic Thinking (UADT), which estimates sample-level uncertainty via entropy and dynamically modulates the model's exploration depth using a shaped advantage mechanism. Comprehensive experiments demonstrate that our model achieves state-of-the-art performance on both basic and complex reasoning tasks, outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and ophthalmic MLLMs by at least 24.92\%, 15.00\%, 21.20\%, and 17.66\%. Project Page: \href{https://github.com/lxirich/OphthaReason}{link}.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems</title>
<link>https://arxiv.org/abs/2509.00115</link>
<guid>https://arxiv.org/abs/2509.00115</guid>
<content:encoded><![CDATA[
arXiv:2509.00115v2 Announce Type: replace 
Abstract: Agentic artificial intelligence (AI) -- multi-agent systems that combine large language models with external tools and autonomous planning -- are rapidly transitioning from research laboratories into high-stakes domains. Our earlier "Basic" paper introduced a five-axis framework and proposed preliminary metrics such as goal drift and harm reduction but did not provide an algorithmic instantiation or empirical evidence. This "Advanced" sequel fills that gap. First, we revisit recent benchmarks and industrial deployments to show that technical metrics still dominate evaluations: a systematic review of 84 papers from 2023--2025 found that 83% report capability metrics while only 30% consider human-centred or economic axes [2]. Second, we formalise an Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises heterogeneous metrics, applies per-axis exponentially weighted moving-average thresholds and performs joint anomaly detection via the Mahalanobis distance. Third, we conduct simulations and real-world experiments. AMDM cuts anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and reduces false-positive rates from 4.5% to 0.9% compared with static thresholds. We present a comparison table and ROC/PR curves, and we reanalyse case studies to surface missing metrics. Code, data and a reproducibility checklist accompany this paper to facilitate replication. The code supporting this work is available at https://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation</title>
<link>https://arxiv.org/abs/2402.04355</link>
<guid>https://arxiv.org/abs/2402.04355</guid>
<content:encoded><![CDATA[
arXiv:2402.04355v3 Announce Type: replace-cross 
Abstract: We propose a likelihood-free method for comparing two distributions given samples from each, with the goal of assessing the quality of generative models. The proposed approach, PQMass, provides a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models. PQMass divides the sample space into non-overlapping regions and applies chi-squared tests to the number of data samples that fall within each region, giving a p-value that measures the probability that the bin counts derived from two sets of samples are drawn from the same multinomial distribution. PQMass does not depend on assumptions regarding the density of the true distribution, nor does it rely on training or fitting any auxiliary models. We evaluate PQMass on data of various modalities and dimensions, demonstrating its effectiveness in assessing the quality, novelty, and diversity of generated samples. We further show that PQMass scales well to moderately high-dimensional data and thus obviates the need for feature extraction in practical applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models</title>
<link>https://arxiv.org/abs/2403.09904</link>
<guid>https://arxiv.org/abs/2403.09904</guid>
<content:encoded><![CDATA[
arXiv:2403.09904v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) has garnered increasing attention due to its unique characteristic of allowing heterogeneous clients to process their private data locally and interact with a central server, while being respectful of privacy. A critical bottleneck in FL is the communication cost. A pivotal strategy to mitigate this burden is Local Training, which involves running multiple local stochastic gradient descent iterations between communication phases. Our work is inspired by the innovative Scaffnew algorithm, which has considerably advanced the reduction of communication complexity in FL. We introduce FedComLoc (Federated Compressed and Local Training), integrating practical and effective compression into Scaffnew to further enhance communication efficiency. Extensive experiments, using the popular TopK compressor and quantization, demonstrate its prowess in substantially reducing communication overheads in heterogeneous settings.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Transformer approach for Electricity Price Forecasting</title>
<link>https://arxiv.org/abs/2403.16108</link>
<guid>https://arxiv.org/abs/2403.16108</guid>
<content:encoded><![CDATA[
arXiv:2403.16108v3 Announce Type: replace-cross 
Abstract: This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PriorCLIP: Visual Prior Guided Vision-Language Model for Remote Sensing Image-Text Retrieval</title>
<link>https://arxiv.org/abs/2405.10160</link>
<guid>https://arxiv.org/abs/2405.10160</guid>
<content:encoded><![CDATA[
arXiv:2405.10160v3 Announce Type: replace-cross 
Abstract: Remote sensing image-text retrieval plays a crucial role in remote sensing interpretation, yet remains challenging under both closed-domain and open-domain scenarios due to semantic noise and domain shifts. To address these issues, we propose a visual prior-guided vision-language model, PriorCLIP, which leverages visual priors for unbiased representation learning and adaptive vision-language alignment. In the closed-domain setting, PriorCLIP introduces two Progressive Attention Encoder (PAE) structures: Spatial-PAE constructs a belief matrix with instruction embeddings to filter key features and mitigate semantic bias. At the same time, Temporal-PAE exploits cyclic activation across time steps to enhance text representation. For the open-domain setting, we design a two-stage prior representation learning strategy, consisting of large-scale pre-training on coarse-grained image-text pairs, followed by fine-tuning on fine-grained pairs using vision-instruction, which enables robust retrieval across long-tail concepts and vocabulary shifts. Furthermore, a cluster-based symmetric contrastive Attribution Loss is proposed to constrain inter-class relations and alleviate semantic confusion in the shared embedding space. Extensive experiments on RSICD and RSITMD benchmarks demonstrate that PriorCLIP achieves substantial improvements, outperforming existing methods by 4.9% and 4.0% in closed-domain retrieval, and by 7.3% and 9.4% in open-domain retrieval, respectively.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Quest for the Right Mediator: Surveying Mechanistic Interpretability Through the Lens of Causal Mediation Analysis</title>
<link>https://arxiv.org/abs/2408.01416</link>
<guid>https://arxiv.org/abs/2408.01416</guid>
<content:encoded><![CDATA[
arXiv:2408.01416v2 Announce Type: replace-cross 
Abstract: Interpretability provides a toolset for understanding how and why language models behave in certain ways. However, there is little unity in the field: most studies employ ad-hoc evaluations and do not share theoretical foundations, making it difficult to measure progress and compare the pros and cons of different techniques. Furthermore, while mechanistic understanding is frequently discussed, the basic causal units underlying these mechanisms are often not explicitly defined. In this article, we propose a perspective on interpretability research grounded in causal mediation analysis. Specifically, we describe the history and current state of interpretability taxonomized according to the types of causal units (mediators) employed, as well as methods used to search over mediators. We discuss the pros and cons of each mediator, providing insights as to when particular kinds of mediators and search methods are most appropriate. We argue that this framing yields a more cohesive narrative of the field and helps researchers select appropriate methods based on their research objective. Our analysis yields actionable recommendations for future work, including the discovery of new mediators and the development of standardized evaluations tailored to these goals.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural-Enhanced Dynamic Range Compression Inversion: A Hybrid Approach for Restoring Audio Dynamics</title>
<link>https://arxiv.org/abs/2411.04337</link>
<guid>https://arxiv.org/abs/2411.04337</guid>
<content:encoded><![CDATA[
arXiv:2411.04337v2 Announce Type: replace-cross 
Abstract: Dynamic Range Compression (DRC) is a widely used audio effect that adjusts signal dynamics for applications in music production, broadcasting, and speech processing. Inverting DRC is of broad importance for restoring the original dynamics, enabling remixing, and enhancing the overall audio quality. Existing DRC inversion methods either overlook key parameters or rely on precise parameter values, which can be challenging to estimate accurately. To address this limitation, we introduce a hybrid approach that combines model-based DRC inversion with neural networks to achieve robust DRC parameter estimation and audio restoration simultaneously. Our method uses tailored neural network architectures (classification and regression), which are then integrated into a model-based inversion framework to reconstruct the original signal. Experimental evaluations on various music and speech datasets confirm the effectiveness and robustness of our approach, outperforming several state-of-the-art techniques.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Data Augmentation in Wireless Networks: Analysis, Applications, and Case Study</title>
<link>https://arxiv.org/abs/2411.08341</link>
<guid>https://arxiv.org/abs/2411.08341</guid>
<content:encoded><![CDATA[
arXiv:2411.08341v2 Announce Type: replace-cross 
Abstract: Data augmentation as a technique can mitigate data scarcity in machine learning. However, owing to fundamental differences in wireless data structures, traditional data augmentation techniques may not be suitable for wireless data. Fortunately, Generative Artificial Intelligence (GenAI) can be an effective solution to wireless data augmentation due to its excellent data generation capability. This article systematically explores the potential and effectiveness of generative data augmentation in wireless networks. We first briefly review data augmentation techniques, discuss their limitations in wireless networks, and introduce generative data augmentation, including reviewing GenAI models and their applications in data augmentation. We then explore the application prospects of generative data augmentation in wireless networks from the physical, network, and application layers, providing a generative data augmentation architecture for each application. Subsequently, we propose a general generative data augmentation framework for Wi-Fi gesture recognition. Specifically, we leverage transformer-based diffusion models to generate high-quality channel state information data. To evaluate the effectiveness of the proposed framework, we conduct a case study using the Widar 3.0 dataset, which employs a residual network model for Wi-Fi gesture recognition. Simulation results demonstrate that the proposed framework can enhance the performance of Wi-Fi gesture recognition. Finally, we discuss research directions for generative data augmentation.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QR-VC: Leveraging Quantization Residuals for Linear Disentanglement in Zero-Shot Voice Conversion</title>
<link>https://arxiv.org/abs/2411.16147</link>
<guid>https://arxiv.org/abs/2411.16147</guid>
<content:encoded><![CDATA[
arXiv:2411.16147v2 Announce Type: replace-cross 
Abstract: Zero-shot voice conversion is a technique that alters the speaker identity of an input speech to match a target speaker using only a single reference utterance, without requiring additional training. Recent approaches extensively utilize self-supervised learning features with K-means quantization to extract high-quality content representations while removing speaker identity. However, this quantization process also eliminates fine-grained phonetic and prosodic variations, degrading intelligibility and prosody preservation. While prior works have primarily focused on quantized representations, quantization residuals remain underutilized and deserve further exploration. In this paper, we introduce a novel approach that fully utilizes quantization residuals by leveraging temporal properties of speech components. This facilitates the disentanglement of speaker identity and the recovery of phonetic and prosodic details lost during quantization. By applying only K-means quantization and linear projections, our method achieves simple yet effective disentanglement, without requiring complex architectures or explicit supervision. This allows for high-fidelity voice conversion trained solely with reconstruction losses. Experiments show that the proposed model outperforms existing methods across both subjective and objective metrics. It achieves superior intelligibility and speaker similarity, along with improved prosody preservation, highlighting the impact of our Linear Disentangler module.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traffic-Rule-Compliant Trajectory Repair via Satisfiability Modulo Theories and Reachability Analysis</title>
<link>https://arxiv.org/abs/2412.15837</link>
<guid>https://arxiv.org/abs/2412.15837</guid>
<content:encoded><![CDATA[
arXiv:2412.15837v2 Announce Type: replace-cross 
Abstract: Complying with traffic rules is challenging for automated vehicles, as numerous rules need to be considered simultaneously. If a planned trajectory violates traffic rules, it is common to replan a new trajectory from scratch. We instead propose a trajectory repair technique to save computation time. By coupling satisfiability modulo theories with set-based reachability analysis, we determine if and in what manner the initial trajectory can be repaired. Experiments in high-fidelity simulators and in the real world demonstrate the benefits of our proposed approach in various scenarios. Even in complex environments with intricate rules, we efficiently and reliably repair rule-violating trajectories, enabling automated vehicles to swiftly resume legally safe operation in real time.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?</title>
<link>https://arxiv.org/abs/2501.15463</link>
<guid>https://arxiv.org/abs/2501.15463</guid>
<content:encoded><![CDATA[
arXiv:2501.15463v2 Announce Type: replace-cross 
Abstract: Existing research primarily evaluates the values of LLMs by examining their stated inclinations towards specific values. However, the "Value-Action Gap," a phenomenon rooted in environmental and social psychology, reveals discrepancies between individuals' stated values and their actions in real-world contexts. To what extent do LLMs exhibit a similar gap between their stated values and their actions informed by those values? This study introduces ValueActionLens, an evaluation framework to assess the alignment between LLMs' stated values and their value-informed actions. The framework encompasses the generation of a dataset comprising 14.8k value-informed actions across twelve cultures and eleven social topics, and two tasks to evaluate how well LLMs' stated value inclinations and value-informed actions align across three different alignment measures. Extensive experiments reveal that the alignment between LLMs' stated values and actions is sub-optimal, varying significantly across scenarios and models. Analysis of misaligned results identifies potential harms from certain value-action gaps. To predict the value-action gaps, we also uncover that leveraging reasoned explanations improves performance. These findings underscore the risks of relying solely on the LLMs' stated values to predict their behaviors and emphasize the importance of context-aware evaluations of LLM values and value-action gaps.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning</title>
<link>https://arxiv.org/abs/2502.02390</link>
<guid>https://arxiv.org/abs/2502.02390</guid>
<content:encoded><![CDATA[
arXiv:2502.02390v2 Announce Type: replace-cross 
Abstract: Research on LLM technologies is rapidly emerging, with most of them employ a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing attention because its process is closer to the human thought process. Inspired by the human ability to constantly associate and replenish knowledge during thinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory'. By combining the structured exploration capabilities of MCTS with the adaptive learning capacity of associative memory, CoAT significantly expands the LLM search space, enabling our framework to explore diverse reasoning pathways and dynamically update its knowledge base in real-time. This allows the framework to not only revisit and refine earlier inferences but also adaptively incorporate evolving information, ensuring that the final output is both accurate and comprehensive. We validate CoAT's effectiveness across a variety of generative and reasoning tasks. Quantitative experiments show that CoAT achieves over 10% performance improvement on open-source multi-hop reasoning datasets (HotpotQA, MuSiQue) and more than 15% gain on our proprietary CRB dataset.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided Logical Form Generation</title>
<link>https://arxiv.org/abs/2502.12737</link>
<guid>https://arxiv.org/abs/2502.12737</guid>
<content:encoded><![CDATA[
arXiv:2502.12737v3 Announce Type: replace-cross 
Abstract: Knowledge base question answering (KBQA) aims to answer user questions in natural language using rich human knowledge stored in large KBs. As current KBQA methods struggle with unseen knowledge base elements at test time,we introduce SG-KBQA: a novel model that injects schema contexts into entity retrieval and logical form generation to tackle this issue. It uses the richer semantics and awareness of the knowledge base structure provided by schema contexts to enhance generalizability. We show that SG-KBQA achieves strong generalizability, outperforming state-of-the-art models on two commonly used benchmark datasets across a variety of test settings. Our source code is available at https://github.com/gaosx2000/SG_KBQA.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A general language model for peptide identification</title>
<link>https://arxiv.org/abs/2502.15610</link>
<guid>https://arxiv.org/abs/2502.15610</guid>
<content:encoded><![CDATA[
arXiv:2502.15610v5 Announce Type: replace-cross 
Abstract: Accurate identification of bioactive peptides (BPs) and protein post-translational modifications (PTMs) is essential for understanding protein function and advancing therapeutic discovery. However, most computational methods remain limited in their generalizability across diverse peptide functions. Here, we present PDeepPP, a unified deep learning framework that integrates pretrained protein language models with a hybrid transformer-convolutional architecture, enabling robust identification across diverse peptide classes and PTM sites. We curated comprehensive benchmark datasets and implemented strategies to address data imbalance, allowing PDeepPP to systematically extract both global and local sequence features. Through extensive analyses-including dimensionality reduction and comparison studies-PDeepPP demonstrates strong, interpretable peptide representations and achieves state-of-the-art performance in 25 of the 33 biological identification tasks. Notably, PDeepPP attains high accuracy in antimicrobial (0.9726) and phosphorylation site (0.9984) identification, with 99.5% specificity in glycosylation site prediction and substantial reduction in false negatives in antimalarial tasks. By enabling large-scale, accurate peptide analysis, PDeepPP supports biomedical research and the discovery of novel therapeutic targets for disease treatment. All code, datasets, and pretrained models are publicly available via GitHub:https://github.com/fondress/PDeepPP and Hugging Face:https://huggingface.co/fondress/PDeppPP.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension</title>
<link>https://arxiv.org/abs/2502.16523</link>
<guid>https://arxiv.org/abs/2502.16523</guid>
<content:encoded><![CDATA[
arXiv:2502.16523v2 Announce Type: replace-cross 
Abstract: As neural language models achieve human-comparable performance on Machine Reading Comprehension (MRC) and see widespread adoption, ensuring their robustness in real-world scenarios has become increasingly important. Current robustness evaluation research, though, primarily develops synthetic perturbation methods, leaving unclear how well they reflect real life scenarios. Considering this, we present a framework to automatically examine MRC models on naturally occurring textual perturbations, by replacing paragraph in MRC benchmarks with their counterparts based on available Wikipedia edit history. Such perturbation type is natural as its design does not stem from an arteficial generative process, inherently distinct from the previously investigated synthetic approaches. In a large-scale study encompassing SQUAD datasets and various model architectures we observe that natural perturbations result in performance degradation in pre-trained encoder language models. More worryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs) inherit these errors. Further experiments demonstrate that our findings generalise to natural perturbations found in other more challenging MRC benchmarks. In an effort to mitigate these errors, we show that it is possible to improve the robustness to natural perturbations by training on naturally or synthetically perturbed examples, though a noticeable gap still remains compared to performance on unperturbed data.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPO: Boosting LLM Agents with Meta Plan Optimization</title>
<link>https://arxiv.org/abs/2503.02682</link>
<guid>https://arxiv.org/abs/2503.02682</guid>
<content:encoded><![CDATA[
arXiv:2503.02682v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, , which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural Video Compression</title>
<link>https://arxiv.org/abs/2503.02733</link>
<guid>https://arxiv.org/abs/2503.02733</guid>
<content:encoded><![CDATA[
arXiv:2503.02733v2 Announce Type: replace-cross 
Abstract: Implicit Neural Representations (INRs) have demonstrated significant potential in video compression by representing videos as neural networks. However, as the number of frames increases, the memory consumption for training and inference increases substantially, posing challenges in resource-constrained scenarios. Inspired by the success of traditional video compression frameworks, which process video frame by frame and can efficiently compress long videos, we adopt this modeling strategy for INRs to decrease memory consumption, while aiming to unify the frameworks from the perspective of timeline-based autoregressive modeling. In this work, we present a novel understanding of INR models from an autoregressive (AR) perspective and introduce a Unified AutoRegressive Framework for memory-efficient Neural Video Compression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural video compression under a unified autoregressive paradigm. It partitions videos into several clips and processes each clip using a different INR model instance, leveraging the advantages of both compression frameworks while allowing seamless adaptation to either in form. To further reduce temporal redundancy between clips, we design two modules to optimize the initialization, training, and compression of these model parameters. UAR-NVC supports adjustable latencies by varying the clip length. Extensive experimental results demonstrate that UAR-NVC, with its flexible video clip setting, can adapt to resource-constrained environments and significantly improve performance compared to different baseline models. The project page: "https://wj-inf.github.io/UAR-NVC-page/".
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To See a World in a Spark of Neuron: Disentangling Multi-task Interference for Training-free Model Merging</title>
<link>https://arxiv.org/abs/2503.05320</link>
<guid>https://arxiv.org/abs/2503.05320</guid>
<content:encoded><![CDATA[
arXiv:2503.05320v4 Announce Type: replace-cross 
Abstract: Fine-tuning pre-trained models on targeted datasets enhances task-specific performance but often comes at the expense of generalization. Model merging techniques, which integrate multiple fine-tuned models into a single multi-task model through task arithmetic, offer a promising solution. However, task interference remains a fundamental challenge, leading to performance degradation and suboptimal merged models. Existing approaches largely overlooked the fundamental roles of neurons, their connectivity, and activation, resulting in a merging process and a merged model that does not consider how neurons relay and process information. In this work, we present the first study that relies on neuronal mechanisms for model merging. Specifically, we decomposed task-specific representations into two complementary neuronal subspaces that regulate input sensitivity and task adaptability. Leveraging this decomposition, we introduced NeuroMerging, a novel merging framework developed to mitigate task interference within neuronal subspaces, enabling training-free model fusion across diverse tasks. Through extensive experiments, we demonstrated that NeuroMerging achieved superior performance compared to existing methods on multi-task benchmarks across both natural language and vision domains. Our findings highlighted the importance of aligning neuronal mechanisms in model merging, offering new insights into mitigating task interference and improving knowledge fusion. Our project is available at https://ZzzitaoFang.github.io/projects/NeuroMerging/.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reangle-A-Video: 4D Video Generation as Video-to-Video Translation</title>
<link>https://arxiv.org/abs/2503.09151</link>
<guid>https://arxiv.org/abs/2503.09151</guid>
<content:encoded><![CDATA[
arXiv:2503.09151v3 Announce Type: replace-cross 
Abstract: We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation</title>
<link>https://arxiv.org/abs/2503.13794</link>
<guid>https://arxiv.org/abs/2503.13794</guid>
<content:encoded><![CDATA[
arXiv:2503.13794v5 Announce Type: replace-cross 
Abstract: Large foundation models trained on large-scale vision-language data can boost Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the hand-crafted pipelines often introduce bias and overfit to specific prompts. We sidestep this issue by directly fusing hidden states from Large Language Models (LLMs) into detectors-an avenue surprisingly under-explored. This paper presents a systematic method to enhance visual grounding by utilizing decoder layers of the LLM of an MLLM. We introduce a zero-initialized cross-attention adapter to enable efficient knowledge fusion from LLMs to object detectors, a new approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We find that intermediate LLM layers already encode rich spatial semantics; adapting only the early layers yields most of the gain. With Swin-T as the vision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at just 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to 6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths further corroborate our design.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A decision-theoretic approach to dealing with uncertainty in quantum mechanics</title>
<link>https://arxiv.org/abs/2503.20607</link>
<guid>https://arxiv.org/abs/2503.20607</guid>
<content:encoded><![CDATA[
arXiv:2503.20607v2 Announce Type: replace-cross 
Abstract: We provide a decision-theoretic framework for dealing with uncertainty in quantum mechanics. This uncertainty is two-fold: on the one hand there may be uncertainty about the state the quantum system is in, and on the other hand, as is essential to quantum mechanical uncertainty, even if the quantum state is known, measurements may still produce an uncertain outcome. In our framework, measurements therefore play the role of acts with an uncertain outcome and our simple decision-theoretic postulates ensure that Born's rule is encapsulated in the utility functions associated with such acts. This approach allows us to uncouple (precise) probability theory from quantum mechanics, in the sense that it leaves room for a more general, so-called imprecise probabilities approach. We discuss the mathematical implications of our findings, which allow us to give a decision-theoretic foundation to recent seminal work by Benavoli, Facchini and Zaffalon, and we compare our approach to earlier and different approaches by Deutsch and Wallace.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation</title>
<link>https://arxiv.org/abs/2504.02438</link>
<guid>https://arxiv.org/abs/2504.02438</guid>
<content:encoded><![CDATA[
arXiv:2504.02438v5 Announce Type: replace-cross 
Abstract: Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice critical temporal dependencies or dilute semantic information. We introduce differential distillation, a principled approach that systematically preserves task-relevant information while suppressing redundancy. Based on this principle, we develop ViLAMP, a hierarchical video-language model that processes hour-long videos at "mixed precision" through two key mechanisms: (1) differential keyframe selection that maximizes query relevance while maintaining temporal distinctiveness at the frame level and (2) differential feature merging that preserves query-salient features in non-keyframes at the patch level. Hence, ViLAMP retains full information in keyframes while reducing non-keyframes to their most salient features, resembling mixed-precision training. Extensive experiments demonstrate ViLAMP's superior performance across four video understanding benchmarks, particularly on long-form content. Notably, ViLAMP can process ultra-long videos (up to 10K frames) on a single NVIDIA A100 GPU, achieving substantial computational efficiency while maintaining state-of-the-art performance. Code and model are available at https://github.com/steven-ccq/ViLAMP.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?</title>
<link>https://arxiv.org/abs/2504.03814</link>
<guid>https://arxiv.org/abs/2504.03814</guid>
<content:encoded><![CDATA[
arXiv:2504.03814v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used in the creation of online content, creating feedback loops as subsequent generations of models will be trained on this synthetic data. Such loops were shown to lead to distribution shifts - models misrepresenting the true underlying distributions of human data (also called model collapse). However, how human data properties affect such shifts remains poorly understood. In this paper, we provide the first empirical examination of the effect of such properties on the outcome of recursive training. We first confirm that using different human datasets leads to distribution shifts of different magnitudes. Through exhaustive manipulation of dataset properties combined with regression analyses, we then identify a set of properties predicting distribution shift magnitudes. Lexical diversity is found to amplify these shifts, while semantic diversity and data quality mitigate them. Furthermore, we find that these influences are highly modular: data scrapped from a given internet domain has little influence on the content generated for another domain. Finally, experiments on political bias reveal that human data properties affect whether the initial bias will be amplified or reduced. Overall, our results portray a novel view, where different parts of internet may undergo different types of distribution shift.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TerraMind: Large-Scale Generative Multimodality for Earth Observation</title>
<link>https://arxiv.org/abs/2504.11171</link>
<guid>https://arxiv.org/abs/2504.11171</guid>
<content:encoded><![CDATA[
arXiv:2504.11171v4 Announce Type: replace-cross 
Abstract: We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces "Thinking-in-Modalities" (TiM) -- the capability of generating additional artificial data during finetuning and inference to improve the model output -- and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code are open-sourced under a permissive license.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification</title>
<link>https://arxiv.org/abs/2504.11500</link>
<guid>https://arxiv.org/abs/2504.11500</guid>
<content:encoded><![CDATA[
arXiv:2504.11500v2 Announce Type: replace-cross 
Abstract: Transit Origin-Destination (OD) data are fundamental for optimizing public transit services, yet current collection methods, such as manual surveys, Bluetooth and WiFi tracking, or Automated Passenger Counters, are either costly, device-dependent, or incapable of individual-level matching. Meanwhile, onboard surveillance cameras already deployed on most transit vehicles provide an underutilized opportunity for automated OD data collection. Leveraging this, we present TransitReID, a novel framework for individual-level and occlusion-resistant passenger re-identification tailored to transit environments. Our approach introduces three key innovations: (1) an occlusion-robust ReID algorithm that integrates a variational autoencoder-guided region-attention mechanism and selective quality feature averaging to dynamically emphasize visible and discriminative body regions under severe occlusions and viewpoint variations; (2) a Hierarchical Storage and Dynamic Matching HSDM mechanism that transforms static gallery matching into a dynamic process for robustness, accuracy, and speed in real-world bus operations; and (3) a multi-threaded edge implementation that enables near real-time OD estimation while ensuring privacy by processing all data locally. To support research in this domain, we also construct a new TransitReID dataset with over 17,000 images captured from bus front and rear cameras under diverse occlusion and viewpoint conditions. Experimental results demonstrate that TransitReID achieves state-of-the-art performance, with R-1 accuracy of 88.3 percent and mAP of 92.5 percent, and further sustains 90 percent OD estimation accuracy in bus route simulations on NVIDIA Jetson edge devices. This work advances both the algorithmic and system-level foundations of automated transit OD collection, paving the way for scalable, privacy-preserving deployment in intelligent transportation systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.13534</link>
<guid>https://arxiv.org/abs/2504.13534</guid>
<content:encoded><![CDATA[
arXiv:2504.13534v3 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) reasoning boosts large language models' (LLMs) performance on complex tasks but faces two key limitations: a lack of reliability when solely relying on LLM-generated reasoning chains and lower reasoning performance from natural language prompts compared with code prompts. To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation, featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo Program Prompting Execution, which promotes greater logical rigor by guiding LLMs to execute reasoning tasks as pseudo-programs. Evaluations on nine public datasets spanning three reasoning tasks reveal significant accuracy gains-ranging from 4.0% to 44.3%-over state-of-the-art methods. Furthermore, tests on four domain-specific datasets demonstrate exceptional accuracy and efficient execution, underscoring its practical applicability and scalability. Our code and data are available at https: //github.com/hustlfy123/CoT-RAG.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features</title>
<link>https://arxiv.org/abs/2505.12151</link>
<guid>https://arxiv.org/abs/2505.12151</guid>
<content:encoded><![CDATA[
arXiv:2505.12151v2 Announce Type: replace-cross 
Abstract: Large language models have recently made great strides in reasoning task performance through chain-of-thought (CoT) strategies trained via reinforcement learning; however, these "reasoning large language models" (RLLMs) remain imperfect reasoners, and understanding the frequencies and causes of their failure modes is important for both users and developers. We test o1-mini, o3-mini, DeepSeek-R1, Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and Grok 3 Mini Beta on graph coloring as a variable-complexity constraint-satisfaction logic problem, and find evidence from both error rate comparisons and CoT/explanation text analysis that RLLMs are prone to hallucinate graph edges not specified in the prompt. This phenomenon persists across multiple problem complexity levels and semantic frames, and it appears to account for a significant fraction of the incorrect answers from every tested model, and the vast majority of them for some models. We also validate the generalizability of this input-conflicting hallucination phenomenon with smaller-scale experiments on a type of stable matching problem. Our results indicate that RLLMs may possess broader issues with misrepresentation of problem specifics, and we offer suggestions for design choices to mitigate this weakness.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prior Prompt Engineering for Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.14157</link>
<guid>https://arxiv.org/abs/2505.14157</guid>
<content:encoded><![CDATA[
arXiv:2505.14157v2 Announce Type: replace-cross 
Abstract: This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors</title>
<link>https://arxiv.org/abs/2505.15337</link>
<guid>https://arxiv.org/abs/2505.15337</guid>
<content:encoded><![CDATA[
arXiv:2505.15337v3 Announce Type: replace-cross 
Abstract: The misuse of large language models (LLMs), such as academic plagiarism, has driven the development of detectors to identify LLM-generated texts. To bypass these detectors, paraphrase attacks have emerged to purposely rewrite these texts to evade detection. Despite the success, existing methods require substantial data and computational budgets to train a specialized paraphraser, and their attack efficacy greatly reduces when faced with advanced detection algorithms. To address this, we propose \textbf{Co}ntrastive \textbf{P}araphrase \textbf{A}ttack (CoPA), a training-free method that effectively deceives text detectors using off-the-shelf LLMs. The first step is to carefully craft instructions that encourage LLMs to produce more human-like texts. Nonetheless, we observe that the inherent statistical biases of LLMs can still result in some generated texts carrying certain machine-like attributes that can be captured by detectors. To overcome this, CoPA constructs an auxiliary machine-like word distribution as a contrast to the human-like distribution generated by the LLM. By subtracting the machine-like patterns from the human-like distribution during the decoding process, CoPA is able to produce sentences that are less discernible by text detectors. Our theoretical analysis suggests the superiority of the proposed attack. Extensive experiments validate the effectiveness of CoPA in fooling text detectors across various scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stopping Criteria for Value Iteration on Concurrent Stochastic Reachability and Safety Games</title>
<link>https://arxiv.org/abs/2505.21087</link>
<guid>https://arxiv.org/abs/2505.21087</guid>
<content:encoded><![CDATA[
arXiv:2505.21087v2 Announce Type: replace-cross 
Abstract: We consider two-player zero-sum concurrent stochastic games (CSGs) played on graphs with reachability and safety objectives. These include degenerate classes such as Markov decision processes or turn-based stochastic games, which can be solved by linear or quadratic programming; however, in practice, value iteration (VI) outperforms the other approaches and is the most implemented method. Similarly, for CSGs, this practical performance makes VI an attractive alternative to the standard theoretical solution via the existential theory of reals.
  VI starts with an under-approximation of the sought values for each state and iteratively updates them, traditionally terminating once two consecutive approximations are $\epsilon$-close. However, this stopping criterion lacks guarantees on the precision of the approximation, which is the goal of this work. We provide bounded (a.k.a. interval) VI for CSGs: it complements standard VI with a converging sequence of over-approximations and terminates once the over- and under-approximations are $\epsilon$-close.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations</title>
<link>https://arxiv.org/abs/2506.00074</link>
<guid>https://arxiv.org/abs/2506.00074</guid>
<content:encoded><![CDATA[
arXiv:2506.00074v2 Announce Type: replace-cross 
Abstract: This paper evaluates the performance of six open-weight LLMs (llama3-8b, llama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending experts in physics across five tasks: top-k experts by field, influential scientists by discipline, epoch, seniority, and scholar counterparts. The evaluation examines consistency, factuality, and biases related to gender, ethnicity, academic popularity, and scholar similarity. Using ground-truth data from the American Physical Society and OpenAlex, we establish scholarly benchmarks by comparing model outputs to real-world academic records. Our analysis reveals inconsistencies and biases across all models. mixtral-8x7b produces the most stable outputs, while llama3.1-70b shows the highest variability. Many models exhibit duplication, and some, particularly gemma2-9b and llama3.1-8b, struggle with formatting errors. LLMs generally recommend real scientists, but accuracy drops in field-, epoch-, and seniority-specific queries, consistently favoring senior scholars. Representation biases persist, replicating gender imbalances (reflecting male predominance), under-representing Asian scientists, and over-representing White scholars. Despite some diversity in institutional and collaboration networks, models favor highly cited and productive scholars, reinforcing the rich-getricher effect while offering limited geographical representation. These findings highlight the need to improve LLMs for more reliable and equitable scholarly recommendations.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Far Are We from Optimal Reasoning Efficiency?</title>
<link>https://arxiv.org/abs/2506.07104</link>
<guid>https://arxiv.org/abs/2506.07104</guid>
<content:encoded><![CDATA[
arXiv:2506.07104v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) demonstrate remarkable problem-solving capabilities through extended Chain-of-Thought (CoT) reasoning but often produce excessively verbose and redundant reasoning traces. This inefficiency incurs high inference costs and limits practical deployment. While existing fine-tuning methods aim to improve reasoning efficiency, assessing their efficiency gains remains challenging due to inconsistent evaluations. In this work, we introduce the reasoning efficiency frontiers, empirical upper bounds derived from fine-tuning base LRMs across diverse approaches and training configurations. Based on these frontiers, we propose the Reasoning Efficiency Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from these frontiers. Systematic evaluation on challenging mathematical benchmarks reveals significant gaps in current methods: they either sacrifice accuracy for short length or still remain inefficient under tight token budgets. To reduce the efficiency gap, we propose REO-RL, a class of Reinforcement Learning algorithms that minimizes REG by targeting a sparse set of token budgets. Leveraging numerical integration over strategically selected budgets, REO-RL approximates the full efficiency objective with low error using a small set of token budgets. Through systematic benchmarking, we demonstrate that our efficiency metric, REG, effectively captures the accuracy-length trade-off, with low-REG methods reducing length while maintaining accuracy. Our approach, REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy loss. Ablation studies confirm the effectiveness of our exponential token budget strategy. Finally, our findings highlight that fine-tuning LRMs to perfectly align with the efficiency frontiers remains an open challenge.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks</title>
<link>https://arxiv.org/abs/2506.07392</link>
<guid>https://arxiv.org/abs/2506.07392</guid>
<content:encoded><![CDATA[
arXiv:2506.07392v2 Announce Type: replace-cross 
Abstract: The proliferation of UAVs has enabled a wide range of mission-critical applications and is becoming a cornerstone of low-altitude networks, supporting smart cities, emergency response, and more. However, the open wireless environment, dynamic topology, and resource constraints of UAVs expose low-altitude networks to severe DoS threats. Traditional defense approaches, which rely on fixed configurations or centralized decision-making, cannot effectively respond to the rapidly changing conditions in UAV swarm environments. To address these challenges, we propose a novel federated multi-agent deep reinforcement learning (FMADRL)-driven moving target defense (MTD) framework for proactive DoS mitigation in low-altitude networks. Specifically, we design lightweight and coordinated MTD mechanisms, including leader switching, route mutation, and frequency hopping, to disrupt attacker efforts and enhance network resilience. The defense problem is formulated as a multi-agent partially observable Markov decision process, capturing the uncertain nature of UAV swarms under attack. Each UAV is equipped with a policy agent that autonomously selects MTD actions based on partial observations and local experiences. By employing a policy gradient-based algorithm, UAVs collaboratively optimize their policies via reward-weighted aggregation. Extensive simulations demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving up to a 34.6% improvement in attack mitigation rate, a reduction in average recovery time of up to 94.6%, and decreases in energy consumption and defense cost by as much as 29.3% and 98.3%, respectively, under various DoS attack strategies. These results highlight the potential of intelligent, distributed defense mechanisms to protect low-altitude networks, paving the way for reliable and scalable low-altitude economy.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Diffusion in Large Language and Multimodal Models: A Survey</title>
<link>https://arxiv.org/abs/2506.13759</link>
<guid>https://arxiv.org/abs/2506.13759</guid>
<content:encoded><![CDATA[
arXiv:2506.13759v4 Announce Type: replace-cross 
Abstract: In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output control, and dynamic perception. These capabilities are previously difficult to achieve with AR models. A growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to \textit{10$\times$} acceleration in inference speed. These developments position discrete diffusion models as a promising alternative to intelligence based on the traditional autoregressive approach. In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains and \textit{etc.}. We conclude by discussing future directions for research and deployment. Relative papers are collected in https://github.com/LiQiiiii/Awesome-Discrete-Diffusion-LLM_MLLM
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents</title>
<link>https://arxiv.org/abs/2506.21582</link>
<guid>https://arxiv.org/abs/2506.21582</guid>
<content:encoded><![CDATA[
arXiv:2506.21582v3 Announce Type: replace-cross 
Abstract: Text analytics has traditionally required specialized knowledge in Natural Language Processing (NLP) or text analysis, which presents a barrier for entry-level analysts. Recent advances in large language models (LLMs) have changed the landscape of NLP by enabling more accessible and automated text analysis (e.g., topic detection, summarization, information extraction, etc.). We introduce VIDEE, a system that supports entry-level data analysts to conduct advanced text analytics with intelligent agents. VIDEE instantiates a human-agent collaroration workflow consisting of three stages: (1) Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search algorithm to support generative reasoning with human feedback, (2) Execution, which generates an executable text analytics pipeline, and (3) Evaluation, which integrates LLM-based evaluation and visualizations to support user validation of execution results. We conduct two quantitative experiments to evaluate VIDEE's effectiveness and analyze common agent errors. A user study involving participants with varying levels of NLP and text analytics experience -- from none to expert -- demonstrates the system's usability and reveals distinct user behavior patterns. The findings identify design implications for human-agent collaboration, validate the practical utility of VIDEE for non-expert users, and inform future improvements to intelligent text analytics systems.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data</title>
<link>https://arxiv.org/abs/2506.23629</link>
<guid>https://arxiv.org/abs/2506.23629</guid>
<content:encoded><![CDATA[
arXiv:2506.23629v2 Announce Type: replace-cross 
Abstract: The integrity of Water Quality Data (WQD) is critical in environmental monitoring for scientific decision-making and ecological protection. However, water quality monitoring systems are often challenged by large amounts of missing data due to unavoidable problems such as sensor failures and communication delays, which further lead to water quality data becoming High-Dimensional and Sparse (HDS). Traditional data imputation methods are difficult to depict the potential dynamics and fail to capture the deep data features, resulting in unsatisfactory imputation performance. To effectively address the above issues, this paper proposes a Nonlinear Low-rank Representation model (NLR) with Convolutional Neural Networks (CNN) for imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing temporal features to model the temporal dependence of data between time slots, and b) Extracting nonlinear interactions and local patterns to mine higher-order relationships features and achieve deep fusion of multidimensional information. Experimental studies on three real water quality datasets demonstrate that the proposed model significantly outperforms existing state-of-the-art data imputation models in terms of estimation accuracy. It provides an effective approach for handling water quality monitoring data in complex dynamic environments.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving</title>
<link>https://arxiv.org/abs/2506.23771</link>
<guid>https://arxiv.org/abs/2506.23771</guid>
<content:encoded><![CDATA[
arXiv:2506.23771v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) is increasingly used in autonomous driving (AD) and shows clear advantages. However, most RL-based AD methods overlook policy structure design. An RL policy that only outputs short-timescale vehicle control commands results in fluctuating driving behavior due to fluctuations in network outputs, while one that only outputs long-timescale driving goals cannot achieve unified optimality of driving behavior and control. Therefore, we propose a multi-timescale hierarchical reinforcement learning approach. Our approach adopts a hierarchical policy structure, where high- and low-level RL policies are unified-trained to produce long-timescale motion guidance and short-timescale control commands, respectively. Therein, motion guidance is explicitly represented by hybrid actions to capture multimodal driving behaviors on structured road and support incremental low-level extend-state updates. Additionally, a hierarchical safety mechanism is designed to ensure multi-timescale safety. Evaluation in simulator-based and HighD dataset-based highway multi-lane scenarios demonstrates that our approach significantly improves AD performance, effectively increasing driving efficiency, action consistency and safety.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CyberRAG: An Agentic RAG cyber attack classification and reporting tool</title>
<link>https://arxiv.org/abs/2507.02424</link>
<guid>https://arxiv.org/abs/2507.02424</guid>
<content:encoded><![CDATA[
arXiv:2507.02424v2 Announce Type: replace-cross 
Abstract: Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can generate hundreds of thousands of alerts per hour, overwhelming analysts with logs requiring rapidly evolving expertise. Conventional machine-learning detectors reduce alert volume but still yield many false positives, while standard Retrieval-Augmented Generation (RAG) pipelines often retrieve irrelevant context and fail to justify predictions. We present CyberRAG, a modular agent-based RAG framework that delivers real-time classification, explanation, and structured reporting for cyber-attacks. A central LLM agent orchestrates: (i) fine-tuned classifiers specialized by attack family; (ii) tool adapters for enrichment and alerting; and (iii) an iterative retrieval-and-reason loop that queries a domain-specific knowledge base until evidence is relevant and self-consistent. Unlike traditional RAG, CyberRAG adopts an agentic design that enables dynamic control flow and adaptive reasoning. This architecture autonomously refines threat labels and natural-language justifications, reducing false positives and enhancing interpretability. It is also extensible: new attack types can be supported by adding classifiers without retraining the core agent. CyberRAG was evaluated on SQL Injection, XSS, and SSTI, achieving over 94\% accuracy per class and a final classification accuracy of 94.92\% through semantic orchestration. Generated explanations reached 0.94 in BERTScore and 4.9/5 in GPT-4-based expert evaluation, with robustness preserved against adversarial and unseen payloads. These results show that agentic, specialist-oriented RAG can combine high detection accuracy with trustworthy, SOC-ready prose, offering a flexible path toward partially automated cyber-defense workflows.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.05714</link>
<guid>https://arxiv.org/abs/2507.05714</guid>
<content:encoded><![CDATA[
arXiv:2507.05714v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for addressing the challenges faced by large language models in handling real-time information and domain-specific problems. Traditional RAG systems primarily rely on the in-context learning (ICL) capabilities of the large language model itself. Still, in-depth research on the specific capabilities needed by the RAG generation model is lacking, leading to challenges with inconsistent document quality and retrieval system imperfections. Even the limited studies that fine-tune RAG generative models often \textit{lack a granular focus on RAG task} or \textit{a deeper utilization of chain-of-thought processes}. To address this, we propose that RAG models should possess three progressively hierarchical abilities (1) Filtering: the ability to select relevant information; (2) Combination: the ability to combine semantic information across paragraphs; and (3) RAG-specific reasoning: the ability to further process external knowledge using internal knowledge. Thus, we introduce our new RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering" strategy. This method enhances the model's open-book examination capability by utilizing multi-level progressive chain-of-thought. Experiments show that the HIRAG training strategy significantly improves the model's performance on datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comprehensive Evaluation of Prototype Neural Networks</title>
<link>https://arxiv.org/abs/2507.06819</link>
<guid>https://arxiv.org/abs/2507.06819</guid>
<content:encoded><![CDATA[
arXiv:2507.06819v2 Announce Type: replace-cross 
Abstract: Prototype models are an important method for explainable artificial intelligence (XAI) and interpretable machine learning. In this paper, we perform an in-depth analysis of a set of prominent prototype models including ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive set of metrics. In addition to applying standard metrics from literature, we propose several new metrics to further complement the analysis of model interpretability. In our experimentation, we apply the set of prototype models on a diverse set of datasets including fine-grained classification, Non-IID settings and multi-label classification to further contrast the performance. Furthermore, we also provide our code as an open-source library (https://github.com/uos-sis/quanproto), which facilitates simple application of the metrics itself, as well as extensibility -- providing the option for easily adding new metrics and models.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Should We Meta-Learn Reinforcement Learning Algorithms?</title>
<link>https://arxiv.org/abs/2507.17668</link>
<guid>https://arxiv.org/abs/2507.17668</guid>
<content:encoded><![CDATA[
arXiv:2507.17668v2 Announce Type: replace-cross 
Abstract: The process of meta-learning algorithms from data, instead of relying on manual design, is growing in popularity as a paradigm for improving the performance of machine learning systems. Meta-learning shows particular promise for reinforcement learning (RL), where algorithms are often adapted from supervised or unsupervised learning despite their suboptimality for RL. However, until now there has been a severe lack of comparison between different meta-learning algorithms, such as using evolution to optimise over black-box functions or LLMs to propose code. In this paper, we carry out this empirical comparison of the different approaches when applied to a range of meta-learned algorithms which target different parts of the RL pipeline. In addition to meta-train and meta-test performance, we also investigate factors including the interpretability, sample cost and train time for each meta-learning algorithm. Based on these findings, we propose several guidelines for meta-learning new RL algorithms which will help ensure that future learned algorithms are as performant as possible.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems</title>
<link>https://arxiv.org/abs/2508.00300</link>
<guid>https://arxiv.org/abs/2508.00300</guid>
<content:encoded><![CDATA[
arXiv:2508.00300v2 Announce Type: replace-cross 
Abstract: Explanations are crucial for building trustworthy AI systems, but a gap often exists between the explanations provided by models and those needed by users. To address this gap, we introduce MetaExplainer, a neuro-symbolic framework designed to generate user-centered explanations. Our approach employs a three-stage process: first, we decompose user questions into machine-readable formats using state-of-the-art large language models (LLM); second, we delegate the task of generating system recommendations to model explainer methods; and finally, we synthesize natural language explanations that summarize the explainer outputs. Throughout this process, we utilize an Explanation Ontology to guide the language models and explainer methods. By leveraging LLMs and a structured approach to explanation generation, MetaExplainer aims to enhance the interpretability and trustworthiness of AI systems across various applications, providing users with tailored, question-driven explanations that better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate a step towards evaluating and utilizing current state-of-the-art explanation frameworks. Our results show high performance across all stages, with a 59.06% F1-score in question reframing, 70% faithfulness in model explanations, and 67% context-utilization in natural language synthesis. User studies corroborate these findings, highlighting the creativity and comprehensiveness of generated explanations. Tested on the Diabetes (PIMA Indian) tabular dataset, MetaExplainer supports diverse explanation types, including Contrastive, Counterfactual, Rationale, Case-Based, and Data explanations. The framework's versatility and traceability from using ontology to guide LLMs suggest broad applicability beyond the tested scenarios, positioning MetaExplainer as a promising tool for enhancing AI explainability across various domains.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Questioning Language Models</title>
<link>https://arxiv.org/abs/2508.03682</link>
<guid>https://arxiv.org/abs/2508.03682</guid>
<content:encoded><![CDATA[
arXiv:2508.03682v4 Announce Type: replace-cross 
Abstract: Can large language models improve without external data -- by generating their own questions and answers? We hypothesize that a pre-trained language model can improve its reasoning skills given only a single prompt specifying the topic (e.g., algebra word problems) and asking the model to generate its own questions. To do this, we propose Self-Questioning Language Models (SQLM): an asymmetric self-play framework where a proposer is given the topic and generates a question for a solver, who tries to answer it. Both the proposer and solver are trained via reinforcement learning. The proposer receives a reward if the problem is not too easy or too difficult, and the solver receives a reward based on majority voting, a proxy for correctness in the absence of ground-truth answers. For coding, the proposer can instead generate unit tests which are used for verification. We study this asymmetric self-play framework on three benchmarks: three-digit multiplication, algebra problems from the OMEGA benchmark, and programming problems from Codeforces. By continually generating more interesting problems and attempting to solve them, language models can improve on downstream benchmarks without access to any curated training datasets.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL</title>
<link>https://arxiv.org/abs/2508.07976</link>
<guid>https://arxiv.org/abs/2508.07976</guid>
<content:encoded><![CDATA[
arXiv:2508.07976v3 Announce Type: replace-cross 
Abstract: Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subjective Behaviors and Preferences in LLM: Language of Browsing</title>
<link>https://arxiv.org/abs/2508.15474</link>
<guid>https://arxiv.org/abs/2508.15474</guid>
<content:encoded><![CDATA[
arXiv:2508.15474v2 Announce Type: replace-cross 
Abstract: A Large Language Model (LLM) offers versatility across domains and tasks, purportedly benefiting users with a wide variety of behaviors and preferences. We question this perception about an LLM when users have inherently subjective behaviors and preferences, as seen in their ubiquitous and idiosyncratic browsing of websites or apps. The sequential behavior logs of pages, thus generated, form something akin to each user's self-constructed "language", albeit without the structure and grammar imbued in natural languages. We ask: (i) Can a small LM represent the "language of browsing" better than a large LM? (ii) Can an LM with a single set of parameters (or, single LM) adequately capture myriad users' heterogeneous, subjective behaviors and preferences? (iii) Can a single LM with high average performance, yield low variance in performance to make alignment good at user level? We introduce clusterwise LM training, HeTLM (Heterogeneity aware Training of Language Model), appropriate for subjective behaviors. We find that (i) a small LM trained using a page-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM with heterogeneous cluster specific set of parameters outperforms a single LM of the same family, controlling for the number of parameters; and (iii) a higher mean and a lower variance in generation ensues, implying improved alignment.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code</title>
<link>https://arxiv.org/abs/2508.18106</link>
<guid>https://arxiv.org/abs/2508.18106</guid>
<content:encoded><![CDATA[
arXiv:2508.18106v2 Announce Type: replace-cross 
Abstract: The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks often lack relevance to real-world AI programming scenarios, making them inadequate for assessing the practical security risks associated with AI-generated code in production environments. To address this gap, we introduce A.S.E (AI Code Generation Security Evaluation), a repository-level evaluation benchmark designed to closely mirror real-world AI programming tasks, offering a comprehensive and reliable framework for assessing the security of AI-generated code. Our evaluation of leading LLMs on A.S.E reveals several key findings. In particular, current LLMs still struggle with secure coding. The complexity in repository-level scenarios presents challenges for LLMs that typically perform well on snippet-level tasks. Morever, a larger reasoning budget does not necessarily lead to better code generation. These observations offer valuable insights into the current state of AI code generation, assisting developers in selecting the most appropriate models for practical tasks, while laying the foundation for refining LLMs to generate secure and efficient code in real-world applications.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Hidden Precursors to Earthquakes via a Stress-Sensitive Transformation of Seismic Noise</title>
<link>https://arxiv.org/abs/2509.00268</link>
<guid>https://arxiv.org/abs/2509.00268</guid>
<content:encoded><![CDATA[
arXiv:2509.00268v2 Announce Type: replace-cross 
Abstract: Earthquake prediction has long been one of the most elusive challenges in science. Laboratory experiments and simulations suggest that failure precursors should exist, yet reliable signals have remained unobserved in real-world seismic records, leaving open the question of whether they are absent in nature or simply hidden within noise. Here we introduce a stress-sensitive frequency-domain transformation that tracks energy differences between adjacent frequency bands, isolating subtle spectral changes linked to evolving shear and normal stress. Applied to both laboratory acoustic emission data and seismic records from seven major earthquakes (Mw 5.9-9.0), including the 2011 Tohoku and 2023 Turkey-Syria events, the transform consistently reveals precursory signatures, arc-like trajectories and accelerations toward extrema, emerging hours to days before rupture. These features are robust across diverse tectonic settings, from induced seismicity and volcanic collapse to continental strike-slip and subduction megathrust earthquakes. Our findings demonstrate that hidden precursors are indeed encoded in ambient seismic noise, offering a pathway toward real-time fault monitoring and actionable short-term earthquake forecasting.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding DINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.23903</link>
<guid>https://arxiv.org/abs/2506.23903</guid>
<content:encoded><![CDATA[
<div> vision-language model, object segmentation, ultrasound imaging, Grounding DINO, SAM2 <br />
Summary:
Accurate and generalizable object segmentation in ultrasound imaging is challenging due to anatomical variability and limited annotated data. This study proposes a prompt-driven vision-language model (VLM) integrating Grounding DINO with SAM2 for multi-organ segmentation. Utilizing 18 public ultrasound datasets, including breast, thyroid, liver, and kidney, the approach outperforms state-of-the-art methods on most seen datasets. The model's performance remains strong on unseen datasets without additional fine-tuning, highlighting its scalability and robustness. By reducing the reliance on large, organ-specific annotated datasets, VLMs show promise for enhancing ultrasound image analysis. The code will be made available on code.sonography.ai after acceptance.<br /><br />Summary: <div>
arXiv:2506.23903v3 Announce Type: replace-cross 
Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 (Segment Anything Model2) to enable object segmentation across multiple ultrasound organs. A total of 18 public ultrasound datasets, encompassing the breast, thyroid, liver, prostate, kidney, and paraspinal muscle, were utilized. These datasets were divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for testing to evaluate performance in unseen distributions. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on most seen datasets while maintaining strong performance on unseen datasets without additional fine-tuning. These results underscore the promise of VLMs in scalable and robust ultrasound image analysis, reducing dependence on large, organ-specific annotated datasets. We will publish our code on code.sonography.ai after acceptance.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Renewable Energy Sources Selection Analysis with the Maximizing Deviation Method</title>
<link>https://arxiv.org/abs/2509.07011</link>
<guid>https://arxiv.org/abs/2509.07011</guid>
<content:encoded><![CDATA[
<div> fuzzy set theory, multi-criteria decision-making methods, Fermatean fuzzy environment, optimization model, renewable energy sources

Summary:
This study focuses on applying fuzzy set theory and multi-criteria decision-making methods in the selection of renewable energy sources. The Fermatean fuzzy environment, a generalization of fuzzy sets, is utilized in an optimization model based on the deviation maximization method to determine partially known feature weights. The method, combined with interval-valued Fermatean fuzzy sets, helps handle uncertainty and fuzziness in decision-making. Selecting renewable energy sources is crucial for meeting energy needs, reducing carbon emissions, and addressing climate change. The study highlights the technical, managerial, and political implications of choosing renewable energy sources, emphasizing the importance of balancing environmental concerns with practical considerations. <div>
arXiv:2509.07011v1 Announce Type: new 
Abstract: Multi-criteria decision-making methods provide decision-makers with appropriate tools to make better decisions in uncertain, complex, and conflicting situations. Fuzzy set theory primarily deals with the uncertainty inherent in human thoughts and perceptions and attempts to quantify this uncertainty. Fuzzy logic and fuzzy set theory are utilized with multi-criteria decision-making methods because they effectively handle uncertainty and fuzziness in decision-makers' judgments, allowing for verbal judgments of the problem. This study utilizes the Fermatean fuzzy environment, a generalization of fuzzy sets. An optimization model based on the deviation maximization method is proposed to determine partially known feature weights. This method is combined with interval-valued Fermatean fuzzy sets. The proposed method was applied to the problem of selecting renewable energy sources. The reason for choosing renewable energy sources is that meeting energy needs from renewable sources, balancing carbon emissions, and mitigating the effects of global climate change are among the most critical issues of the recent period. Even though selecting renewable energy sources is a technical issue, the managerial and political implications of this issue are also important, and are discussed in this study.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning</title>
<link>https://arxiv.org/abs/2509.07017</link>
<guid>https://arxiv.org/abs/2509.07017</guid>
<content:encoded><![CDATA[
<div> Graph signal processing, Neuro-symbolic reasoning, Spectral templates, Laplacian eigenstructure, Scalable learning
Summary:
Spectral NSR is a novel neuro-symbolic reasoning framework that combines logical rules with graph spectral processing for efficient inference. It incorporates various extensions like dynamic graph learning, rational filters, and spectral experts for specialized reasoning. The framework includes features such as training with spectral curricula, uncertainty quantification, and large language model integration for enhanced performance. Empirical evaluations on reasoning benchmarks show that Spectral NSR outperforms existing models in terms of accuracy, speed, robustness, and interpretability. Analysis confirms alignment with symbolic proof structures, effective domain adaptation, and robustness to adversarial attacks. These results establish Spectral NSR as a transparent, scalable, and versatile approach for reasoning systems, offering improved generalization and robustness compared to traditional methods. 
<br /><br />Summary: <div>
arXiv:2509.07017v1 Announce Type: new 
Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning framework that embeds logical rules as spectral templates and performs inference directly in the graph spectral domain. By leveraging graph signal processing (GSP) and frequency-selective filters grounded in the Laplacian eigenstructure of knowledge graphs, the architecture unifies the interpretability of symbolic reasoning with the scalability and adaptability of spectral learning. Beyond the core formulation, we incorporate a comprehensive set of extensions, including dynamic graph and basis learning, rational and diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts for modular specialization, proof-guided training with spectral curricula, and uncertainty quantification for calibrated confidence. Additional enhancements such as large language model coupling, co-spectral transfer alignment, adversarial robustness, efficient GPU kernels, generalized Laplacians, and causal interventions further expand the versatility of the framework.
  Empirical evaluation on state-of-the-art reasoning benchmarks such as ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior accuracy, faster inference, improved robustness to adversarial perturbations, and higher interpretability compared to leading baselines including transformers, message-passing neural networks, and neuro-symbolic logic programming systems. Spectral attribution and proof-band agreement analyses confirm that model decisions align closely with symbolic proof structures, while transfer experiments validate effective domain adaptation through co-spectral alignment. These results establish Spectral NSR as a scalable and principled foundation for the next generation of reasoning systems, offering transparency, robustness, and generalization beyond conventional approaches.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Methods in Generative AI</title>
<link>https://arxiv.org/abs/2509.07054</link>
<guid>https://arxiv.org/abs/2509.07054</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Artificial Intelligence, statistical methods, reliability, quality, efficiency

Summary:
Generative Artificial Intelligence (AI) is a rapidly advancing technology with the potential to revolutionize various fields. However, the default nature of generative AI techniques lacks guarantees regarding correctness, safety, and fairness. To address these issues, statistical methods are being explored to enhance the reliability of generative AI models. These statistical techniques not only aim to improve the quality and efficiency of AI evaluation but also play a crucial role in designing interventions and experiments within AI research. This paper reviews the existing work in this domain, delving into the application of statistical methods to enhance the performance of generative AI. While statistical methods hold promise in addressing the limitations of generative AI, further research is necessary to explore potential future directions for advancing the field.<br /><br />Summary: <div>
arXiv:2509.07054v1 Announce Type: new 
Abstract: Generative Artificial Intelligence is emerging as an important technology, promising to be transformative in many areas. At the same time, generative AI techniques are based on sampling from probabilistic models, and by default, they come with no guarantees about correctness, safety, fairness, or other properties. Statistical methods offer a promising potential approach to improve the reliability of generative AI techniques. In addition, statistical methods are also promising for improving the quality and efficiency of AI evaluation, as well as for designing interventions and experiments in AI.
  In this paper, we review some of the existing work on these topics, explaining both the general statistical techniques used, as well as their applications to generative AI. We also discuss limitations and potential future directions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction Agent: Enhancing Agent with Expert Demonstration</title>
<link>https://arxiv.org/abs/2509.07098</link>
<guid>https://arxiv.org/abs/2509.07098</guid>
<content:encoded><![CDATA[
<div> agents, GUI, instruction, tasks, automation  
Summary:  
Instruction Agent is a new GUI agent that utilizes expert demonstrations to tackle complex tasks involving novel UI elements, long-horizon actions, and personalized trajectories. By extracting step-by-step instructions from a single demonstration and strictly following user intentions, the agent minimizes mistakes during execution. The verifier and backtracker modules enhance robustness by understanding outcomes of actions and handling interruptions like pop-up windows. In experiments on OSWorld tasks, Instruction Agent achieved a 60% success rate where top-ranked agents failed. This innovative framework advances GUI task automation, offering a reliable solution for challenging workflows. <br /><br />Summary: <div>
arXiv:2509.07098v1 Announce Type: new 
Abstract: Graphical user interface (GUI) agents have advanced rapidly but still struggle with complex tasks involving novel UI elements, long-horizon actions, and personalized trajectories. In this work, we introduce Instruction Agent, a GUI agent that leverages expert demonstrations to solve such tasks, enabling completion of otherwise difficult workflows. Given a single demonstration, the agent extracts step-by-step instructions and executes them by strictly following the trajectory intended by the user, which avoids making mistakes during execution. The agent leverages the verifier and backtracker modules further to improve robustness. Both modules are critical to understand the current outcome from each action and handle unexpected interruptions(such as pop-up windows) during execution. Our experiments show that Instruction Agent achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked agents failed to complete. The Instruction Agent offers a practical and extensible framework, bridging the gap between current GUI agents and reliable real-world GUI task automation.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis</title>
<link>https://arxiv.org/abs/2509.07122</link>
<guid>https://arxiv.org/abs/2509.07122</guid>
<content:encoded><![CDATA[
<div> Neurosymbolic modeling, neural representations, symbolic processing, data-efficient, frameworks<br />
Summary:<br />
Neurosymbolic (NeSy) frameworks combine neural and symbolic processing, offering reliability and data efficiency. However, these frameworks present challenges due to learning curves and lack of user-friendly tools. This paper explores existing NeSy frameworks, focusing on symbolic representation language, neural model integration, and algorithms. While the majority of research centers on algorithms, generic frameworks for problem specification are lacking. Three frameworks, DeepProbLog, Scallop, and DomiKnowS, are examined to showcase Neurosymbolic modeling. Challenges in each facet are identified to understand the expressivity of these frameworks. The paper aims to inspire new approaches in addressing these challenges and foster community engagement. <div>
arXiv:2509.07122v1 Announce Type: new 
Abstract: Neurosymbolic (NeSy) frameworks combine neural representations and learning with symbolic representations and reasoning. Combining the reasoning capacities, explainability, and interpretability of symbolic processing with the flexibility and power of neural computing allows us to solve complex problems with more reliability while being data-efficient. However, this recently growing topic poses a challenge to developers with its learning curve, lack of user-friendly tools, libraries, and unifying frameworks. In this paper, we characterize the technical facets of existing NeSy frameworks, such as the symbolic representation language, integration with neural models, and the underlying algorithms. A majority of the NeSy research focuses on algorithms instead of providing generic frameworks for declarative problem specification to leverage problem solving. To highlight the key aspects of Neurosymbolic modeling, we showcase three generic NeSy frameworks - \textit{DeepProbLog}, \textit{Scallop}, and \textit{DomiKnowS}. We identify the challenges within each facet that lay the foundation for identifying the expressivity of each framework in solving a variety of problems. Building on this foundation, we aim to spark transformative action and encourage the community to rethink this problem in novel ways.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoencoder-Based Denoising of Muscle Artifacts in ECG to Preserve Skin Nerve Activity (SKNA) for Cognitive Stress Detection</title>
<link>https://arxiv.org/abs/2509.07146</link>
<guid>https://arxiv.org/abs/2509.07146</guid>
<content:encoded><![CDATA[
<div> sympathetic nervous system, skin nerve activity, deep learning, denoising, electrocardiogram<br />
Summary:<br />
This study presents a denoising approach using a lightweight one-dimensional convolutional autoencoder with a long short-term memory (LSTM) bottleneck to reconstruct clean skin nerve activity (SKNA) from electromyographic (EMG) contaminated recordings. The method significantly improved the signal-to-noise ratio, increased cross correlation with clean SKNA, and restored sympathetic bursts features. Using deep learning-based reconstruction, the study demonstrated the ability to monitor SKNA in naturalistic, movement-rich environments, even during substantial EMG interference. The results showed that the approach enabled robust SKNA monitoring, with classification accuracies reaching 91-98% across severe noise levels, comparable to clean data. <div>
arXiv:2509.07146v1 Announce Type: new 
Abstract: The sympathetic nervous system (SNS) plays a central role in regulating the body's responses to stress and maintaining physiological stability. Its dysregulation is associated with a wide range of conditions, from cardiovascular disease to anxiety disorders. Skin nerve activity (SKNA) extracted from high-frequency electrocardiogram (ECG) recordings provides a noninvasive window into SNS dynamics, but its measurement is highly susceptible to electromyographic (EMG) contamination. Traditional preprocessing based on bandpass filtering within a fixed range (e.g., 500--1000 Hz) is susceptible to overlapping EMG and SKNA spectral components, especially during sustained muscle activity. We present a denoising approach using a lightweight one-dimensional convolutional autoencoder with a long short-term memory (LSTM) bottleneck to reconstruct clean SKNA from EMG-contaminated recordings. Using clean ECG-derived SKNA data from cognitive stress experiments and EMG noise from chaotic muscle stimulation recordings, we simulated contamination at realistic noise levels (--4 dB, --8 dB signal-to-noise ratio) and trained the model in the leave-one-subject-out cross-validation framework. The method improved signal-to-noise ratio by up to 9.65 dB, increased cross correlation with clean SKNA from 0.40 to 0.72, and restored burst-based SKNA features to near-clean discriminability (AUROC $\geq$ 0.96). Classification of baseline versus sympathetic stimulation (cognitive stress) conditions reached accuracies of 91--98\% across severe noise levels, comparable to clean data. These results demonstrate that deep learning--based reconstruction can preserve physiologically relevant sympathetic bursts during substantial EMG interference, enabling more robust SKNA monitoring in naturalistic, movement-rich environments.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.07159</link>
<guid>https://arxiv.org/abs/2509.07159</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-SQL, reinforcement learning, language models, industry-scale databases, domain-specific business logic

Summary:
PaVeRL-SQL introduces a framework that combines Partial-Match Rewards and Verbal Reinforcement Learning to improve Text-to-SQL models. The framework utilizes in-context learning with group self-evaluation and a chain-of-thought RL pipeline to achieve state-of-the-art results on popular benchmarks like Spider and BIRD. By training with mixed SQL dialects, significant improvements were seen in execution accuracy, particularly for dialects with limited data. The two pipelines, verbal-RL and CoT, deliver reliable Text-to-SQL results under realistic industrial constraints, surpassing existing methods on industry-scale databases and complex questions involving domain-specific logic. The code for the framework is open-source and available on GitHub for further development and research. 

<br /><br />Summary: 
PaVeRL-SQL integrates Partial-Match Rewards and Verbal Reinforcement Learning to enhance Text-to-SQL models, achieving state-of-the-art results on industry-scale databases and complex questions. By adopting in-context learning and chain-of-thought RL pipelines, the framework improves execution accuracy and handles mixed SQL dialects effectively, leading to significant advancements in Text-to-SQL performance. The open-source code allows for further exploration and development in this domain. <div>
arXiv:2509.07159v1 Announce Type: new 
Abstract: Text-to-SQL models allow users to interact with a database more easily by generating executable SQL statements from natural-language questions. Despite recent successes on simpler databases and questions, current Text-to-SQL methods still suffer from low execution accuracy on industry-scale databases and complex questions involving domain-specific business logic. We present \emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and \emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt two pipelines: (1) a newly designed in-context learning framework with group self-evaluation (verbal-RL), using capable open- and closed-source large language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL pipeline with a small backbone model (OmniSQL-7B) trained with a specially designed reward function and two-stage RL. These pipelines achieve state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider, Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the verbal-RL pipeline achieves an execution accuracy 7.4\% higher than SOTA, and the CoT pipeline is 1.4\% higher. RL training with mixed SQL dialects yields strong, threefold gains, particularly for dialects with limited training data. Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic industrial constraints. The code is available at https://github.com/PaVeRL-SQL/PaVeRL-SQL.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral</title>
<link>https://arxiv.org/abs/2509.07170</link>
<guid>https://arxiv.org/abs/2509.07170</guid>
<content:encoded><![CDATA[
<div> Keywords: legal aid, classifier, accuracy, LLM/ML ensemble, follow-up questions

Summary: 
The article presents the FETCH classifier, designed to accurately classify legal issues in order to match individuals seeking help with the right resources. Misdirection in this process can have serious consequences, making it crucial to efficiently identify the legal problem at hand. The FETCH classifier utilizes a hybrid LLM/ML ensemble classification method and generates follow-up questions to enhance the initial problem narrative. By analyzing a dataset of 419 real-world queries to a non-profit lawyer referral service, the classifier achieved an impressive classification accuracy of 97.37% at hits@2, surpassing the performance of the GPT-5 model. This approach not only improves accuracy in legal issue classification but also has the potential to significantly reduce the cost of guiding users to the appropriate legal resources, ensuring timely and effective assistance. 

<br /><br />Summary: <div>
arXiv:2509.07170v1 Announce Type: new 
Abstract: Each year millions of people seek help for their legal problems by calling a legal aid program hotline, walking into a legal aid office, or using a lawyer referral service. The first step to match them to the right help is to identify the legal problem the applicant is experiencing. Misdirection has consequences. Applicants may miss a deadline, experience physical abuse, lose housing or lose custody of children while waiting to connect to the right legal help. We introduce and evaluate the FETCH classifier for legal issue classification and describe two methods for improving accuracy: a hybrid LLM/ML ensemble classification method, and the automatic generation of follow-up questions to enrich the initial problem narrative. We employ a novel data set of 419 real-world queries to a nonprofit lawyer referral service. Ultimately, we show classification accuracy (hits@2) of 97.37\% using a mix of inexpensive models, exceeding the performance of the current state-of-the-art GPT-5 model. Our approach shows promise in significantly reducing the cost of guiding users of the legal system to the right resource for their problem while achieving high accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid CNN-LSTM Deep Learning Model for Intrusion Detection in Smart Grid</title>
<link>https://arxiv.org/abs/2509.07208</link>
<guid>https://arxiv.org/abs/2509.07208</guid>
<content:encoded><![CDATA[
<div> Deep Learning, Intrusion Detection System, Smart Grid, Cybersecurity, SCADA<br />
<br />
Summary:<br />
The evolution of the traditional power grid into the "smart grid" has led to increased vulnerability to cyber attacks. This research proposes a hybrid deep learning-based Intrusion Detection System (IDS) to enhance the cybersecurity of smart grids. The model utilizes Convolutional Neural Networks (CNN) for feature extraction and Long Short-Term Memory (LSTM) networks for recognizing temporal patterns. Training and testing on DNP3 and IEC104 intrusion detection datasets show significant improvements in accuracy, precision, recall, and F1-score, achieving a detection accuracy of 99.70%. By integrating CNN and LSTM, the model effectively identifies and classifies potential cyber threats in smart grid protocols, such as unauthorized access and denial of service attacks. The proposed approach offers a promising solution to safeguard smart grids against cybersecurity threats, ensuring the reliable and secure operation of the modern energy management system. <br /><br /> <div>
arXiv:2509.07208v1 Announce Type: new 
Abstract: The evolution of the traditional power grid into the "smart grid" has resulted in a fundamental shift in energy management, which allows the integration of renewable energy sources with modern communication technology. However, this interconnection has increased smart grids' vulnerability to attackers, which might result in privacy breaches, operational interruptions, and massive outages. The SCADA-based smart grid protocols are critical for real-time data collection and control, but they are vulnerable to attacks like unauthorized access and denial of service (DoS). This research proposes a hybrid deep learning-based Intrusion Detection System (IDS) intended to improve the cybersecurity of smart grids. The suggested model takes advantage of Convolutional Neural Networks' (CNN) feature extraction capabilities as well as Long Short-Term Memory (LSTM) networks' temporal pattern recognition skills. DNP3 and IEC104 intrusion detection datasets are employed to train and test our CNN-LSTM model to recognize and classify the potential cyber threats. Compared to other deep learning approaches, the results demonstrate considerable improvements in accuracy, precision, recall, and F1-score, with a detection accuracy of 99.70%.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions</title>
<link>https://arxiv.org/abs/2509.07209</link>
<guid>https://arxiv.org/abs/2509.07209</guid>
<content:encoded><![CDATA[
<div> dataset, aerodynamic, BlendedNet, surrogate framework, PointNet regressor

Summary:
BlendedNet is a dataset comprising 999 blended wing body geometries, each simulated across nine flight conditions with detailed surface quantities for lift and drag analysis. The dataset addresses data scarcity for unconventional configurations and supports research on data-driven aerodynamic design. An end-to-end surrogate framework is introduced, utilizing a PointNet regressor to predict geometric parameters and a FiLM network to predict pointwise aerodynamic coefficients. The framework demonstrates low errors in surface predictions for diverse BWBs, enabling accurate aerodynamic modeling. <div>
arXiv:2509.07209v1 Announce Type: new 
Abstract: BlendedNet is a publicly available aerodynamic dataset of 999 blended wing body (BWB) geometries. Each geometry is simulated across about nine flight conditions, yielding 8830 converged RANS cases with the Spalart-Allmaras model and 9 to 14 million cells per case. The dataset is generated by sampling geometric design parameters and flight conditions, and includes detailed pointwise surface quantities needed to study lift and drag. We also introduce an end-to-end surrogate framework for pointwise aerodynamic prediction. The pipeline first uses a permutation-invariant PointNet regressor to predict geometric parameters from sampled surface point clouds, then conditions a Feature-wise Linear Modulation (FiLM) network on the predicted parameters and flight conditions to predict pointwise coefficients Cp, Cfx, and Cfz. Experiments show low errors in surface predictions across diverse BWBs. BlendedNet addresses data scarcity for unconventional configurations and enables research on data-driven surrogate modeling for aerodynamic design.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniAcc: Personalized Accessibility Assistant Using Generative AI</title>
<link>https://arxiv.org/abs/2509.07220</link>
<guid>https://arxiv.org/abs/2509.07220</guid>
<content:encoded><![CDATA[
<div> Keywords: ambulatory disabilities, AI-powered navigation system, wheelchair-accessible features, urban environments, crosswalk detection. 

Summary: 
OmniAcc is an AI-powered interactive navigation system designed to assist individuals with ambulatory disabilities in navigating urban environments. It utilizes GPT-4, satellite imagery, and OpenStreetMap data to identify and map wheelchair-accessible features like ramps and crosswalks. The system offers personalized route planning, real-time hands-free navigation, and instant query responses on physical accessibility. Through zero-shot learning and customized prompts, OmniAcc ensures accurate detection of accessibility features and supports validation through structured workflows. A case study on crosswalk detection demonstrated OmniAcc's accuracy, achieving a detection rate of 97.5%. This technology highlights the transformative potential of AI in enhancing navigation and creating more inclusive urban spaces. 

<br /><br />Summary: <div>
arXiv:2509.07220v1 Announce Type: new 
Abstract: Individuals with ambulatory disabilities often encounter significant barriers when navigating urban environments due to the lack of accessible information and tools. This paper presents OmniAcc, an AI-powered interactive navigation system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to identify, classify, and map wheelchair-accessible features such as ramps and crosswalks in the built environment. OmniAcc offers personalized route planning, real-time hands-free navigation, and instant query responses regarding physical accessibility. By using zero-shot learning and customized prompts, the system ensures precise detection of accessibility features, while supporting validation through structured workflows. This paper introduces OmniAcc and explores its potential to assist urban planners and mobility-aid users, demonstrated through a case study on crosswalk detection. With a crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative potential of AI in improving navigation and fostering more inclusive urban spaces.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring</title>
<link>https://arxiv.org/abs/2509.07260</link>
<guid>https://arxiv.org/abs/2509.07260</guid>
<content:encoded><![CDATA[
<div> healthcare monitoring, language models, privacy, compact models, mobile devices

Summary:
Small Language Models (SLMs) are being explored for healthcare monitoring on mobile and wearable devices to address privacy concerns and improve efficiency. The study evaluated SLMs for health prediction tasks using zero-shot, few-shot, and instruction fine-tuning methods. Findings revealed that SLMs can perform comparably to Large Language Models (LLMs) while offering benefits in efficiency and privacy. However, challenges remain in handling class imbalance and few-shot scenarios. Deploying fine-tuned SLMs on mobile devices showed promising results for real-world healthcare scenarios. This study suggests that SLMs have the potential to be a viable solution for next-generation, privacy-preserving healthcare monitoring. <div>
arXiv:2509.07260v1 Announce Type: new 
Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating timely interventions, managing chronic health conditions, and ultimately improving individuals' quality of life. Previous studies on large language models (LLMs) have highlighted their impressive generalization abilities and effectiveness in healthcare prediction tasks. However, most LLM-based healthcare solutions are cloud-based, which raises significant privacy concerns and results in increased memory usage and latency. To address these challenges, there is growing interest in compact models, Small Language Models (SLMs), which are lightweight and designed to run locally and efficiently on mobile and wearable devices. Nevertheless, how well these models perform in healthcare prediction remains largely unexplored. We systematically evaluated SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches, and deployed the best performing fine-tuned SLMs on mobile devices to evaluate their real-world efficiency and predictive performance in practical healthcare scenarios. Our results show that SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. However, challenges remain, particularly in handling class imbalance and few-shot scenarios. These findings highlight SLMs, though imperfect in their current form, as a promising solution for next-generation, privacy-preserving healthcare monitoring.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity</title>
<link>https://arxiv.org/abs/2509.07339</link>
<guid>https://arxiv.org/abs/2509.07339</guid>
<content:encoded><![CDATA[
<div> Intermediate token generation, reasoning tasks, language models, problem difficulty, A* search algorithm

Summary:
- The study investigates the relationship between intermediate token sequence length and problem difficulty in language models.
-  Transformer models trained on A* search algorithm derivational traces show excessive reasoning traces even for simple tasks, sometimes failing to generate a solution.
- Evaluation on out-of-distribution problems reveals a loose correlation between intermediate token length and ground truth A* trace length.
- The few correlated cases are closer to the training distribution, suggesting the effect stems from approximate recall rather than genuine problem-adaptive computation.
- Results challenge the assumption that intermediate trace generation reflects problem difficulty, indicating distributional distance from training data as a significant factor instead. Caution is advised against interpreting longer sequences in such systems as indicative of increased thinking effort.<br /><br />Summary: <div>
arXiv:2509.07339v1 Announce Type: new 
Abstract: Intermediate token generation (ITG), where a model produces output before the solution, has been proposed as a method to improve the performance of language models on reasoning tasks. While these reasoning traces or Chain of Thoughts (CoTs) are correlated with performance gains, the mechanisms underlying them remain unclear. A prevailing assumption in the community has been to anthropomorphize these tokens as "thinking", treating longer traces as evidence of higher problem-adaptive computation. In this work, we critically examine whether intermediate token sequence length reflects or correlates with problem difficulty. To do so, we train transformer models from scratch on derivational traces of the A* search algorithm, where the number of operations required to solve a maze problem provides a precise and verifiable measure of problem complexity. We first evaluate the models on trivial free-space problems, finding that even for the simplest tasks, they often produce excessively long reasoning traces and sometimes fail to generate a solution. We then systematically evaluate the model on out-of-distribution problems and find that the intermediate token length and ground truth A* trace length only loosely correlate. We notice that the few cases where correlation appears are those where the problems are closer to the training distribution, suggesting that the effect arises from approximate recall rather than genuine problem-adaptive computation. This suggests that the inherent computational complexity of the problem instance is not a significant factor, but rather its distributional distance from the training data. These results challenge the assumption that intermediate trace generation is adaptive to problem difficulty and caution against interpreting longer sequences in systems like R1 as automatically indicative of "thinking effort".
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Code Evolution Meets NP-Completeness</title>
<link>https://arxiv.org/abs/2509.07367</link>
<guid>https://arxiv.org/abs/2509.07367</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, AlphaEvolve, SATLUTION, code evolution, Boolean Satisfiability 

Summary: 
Large language models (LLMs) have advanced coding abilities, allowing for autonomous code generation and evolution. AlphaEvolve showcased LLM-based coding agents improving algorithms surpassing human experts on isolated code kernels. In response, SATLUTION was developed as the first framework to extend LLM-based code evolution to large repositories, focusing on Boolean Satisfiability (SAT). SATLUTION utilizes LLM agents to evolve solver repositories on a scale encompassing hundreds of files and tens of thousands of lines of C/C++ code. The framework ensures correctness guarantees and incorporates distributed runtime feedback while self-evolving its own evolution policies and rules. By starting from SAT Competition 2024 codebases and benchmarks, SATLUTION evolved solvers that outperformed human-designed winners of the SAT Competition 2025 and surpassed champions from both 2024 and 2025 on the 2024 benchmarks. <div>
arXiv:2509.07367v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently shown strong coding abilities, enabling not only static code generation but also iterative code self-evolving through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve} demonstrated that LLM-based coding agents can autonomously improve algorithms and surpass human experts, with scopes limited to isolated kernels spanning hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the first framework to extend LLM-based code evolution to the full repository scale, encompassing hundreds of files and tens of thousands of lines of C/C++ code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem and a cornerstone of both theory and applications. SATLUTION orchestrates LLM agents to directly evolve solver repositories under strict correctness guarantees and distributed runtime feedback, while simultaneously self-evolving its own evolution policies and rules. Starting from SAT Competition 2024 codebases and benchmark, SATLUTION evolved solvers that decisively outperformed the human-designed winners of the SAT Competition 2025, and also surpassed both 2024 and 2025 champions on the 2024 benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Self-Play For Data-Free Training</title>
<link>https://arxiv.org/abs/2509.07414</link>
<guid>https://arxiv.org/abs/2509.07414</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, reinforcement learning, self-play, Llama-3.2-3B-Instruct, instruction-following benchmarks

Summary:
Large Language Models (LLMs) have made significant advancements in recent years, driven by scale, data abundance, and reinforcement learning. However, a major challenge they face is the constant need for more data to continue learning. This study introduces a novel reinforcement learning approach called Language Self-Play (LSP) that allows models to improve without additional data. By leveraging a game-theoretic framework of self-play, LSP enables models to enhance their performance on complex tasks through playing against themselves. Experimental results with the Llama-3.2-3B-Instruct model on instruction-following benchmarks demonstrate that pretrained models can significantly boost their capabilities through self-play and outperform data-driven approaches in terms of effectiveness. This innovative methodology has the potential to revolutionize how LLMs continue to evolve and learn without the constant need for additional training data.<br /><br />Summary: <div>
arXiv:2509.07414v1 Announce Type: new 
Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself - a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection</title>
<link>https://arxiv.org/abs/2509.07473</link>
<guid>https://arxiv.org/abs/2509.07473</guid>
<content:encoded><![CDATA[
<div> Keywords: Spreadsheets, Automated layout design, Multimodal Large Language Models, SheetDesigner, Dataset

Summary:
Spreadsheets play a crucial role in data-centric tasks, but manual layout design is time-consuming. Automated solutions are needed, but current models are not well-suited for spreadsheets' discrete, grid-based structure and unique semantics. This paper introduces SheetDesigner, a training-free framework that utilizes Multimodal Large Language Models (MLLMs) to automate spreadsheet layout generation. SheetDesigner performs better than existing baselines by at least 22.6%. The framework combines rule and vision reflection for component placement and content population. MLLMs excel in handling overlap and balance but struggle with alignment, requiring a combination of rule and visual reflection strategies. The research formalizes the spreadsheet layout generation task, presents a dataset of 3,326 spreadsheets, and introduces a seven-criterion evaluation protocol. The codes and data are available on Github. <br /><br />Summary: <div>
arXiv:2509.07473v1 Announce Type: new 
Abstract: Spreadsheets are critical to data-centric tasks, with rich, structured layouts that enable efficient information transmission. Given the time and expertise required for manual spreadsheet layout design, there is an urgent need for automated solutions. However, existing automated layout models are ill-suited to spreadsheets, as they often (1) treat components as axis-aligned rectangles with continuous coordinates, overlooking the inherently discrete, grid-based structure of spreadsheets; and (2) neglect interrelated semantics, such as data dependencies and contextual links, unique to spreadsheets. In this paper, we first formalize the spreadsheet layout generation task, supported by a seven-criterion evaluation protocol and a dataset of 3,326 spreadsheets. We then introduce SheetDesigner, a zero-shot and training-free framework using Multimodal Large Language Models (MLLMs) that combines rule and vision reflection for component placement and content population. SheetDesigner outperforms five baselines by at least 22.6\%. We further find that through vision modality, MLLMs handle overlap and balance well but struggle with alignment, necessitates hybrid rule and visual reflection strategies. Our codes and data is available at Github.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards explainable decision support using hybrid neural models for logistic terminal automation</title>
<link>https://arxiv.org/abs/2509.07577</link>
<guid>https://arxiv.org/abs/2509.07577</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Learning, System Dynamics, Interpretable-by-design, Neural System Dynamics Modeling, Causal Machine Learning

Summary:<br /><br />
This paper introduces a novel framework that combines Deep Learning (DL) with System Dynamics (SD) modeling for transportation logistics. It addresses the challenge of balancing scalability and predictive accuracy with explainability and causal reliability in decision-making systems. The framework leverages techniques from Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning to create neural network models that operate on meaningful variables while maintaining causal grounding and transparency. The approach is designed to be applied in real-world scenarios, specifically within the AutoMoTIF project aimed at optimizing logistic terminals. By integrating neuro-symbolic methods, the framework aims to bridge the gap between black-box models and the need for critical decision support in complex cyber-physical systems enabled by the industrial Internet-of-Things. <div>
arXiv:2509.07577v1 Announce Type: new 
Abstract: The integration of Deep Learning (DL) in System Dynamics (SD) modeling for transportation logistics offers significant advantages in scalability and predictive accuracy. However, these gains are often offset by the loss of explainability and causal reliability $-$ key requirements in critical decision-making systems. This paper presents a novel framework for interpretable-by-design neural system dynamics modeling that synergizes DL with techniques from Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning. The proposed hybrid approach enables the construction of neural network models that operate on semantically meaningful and actionable variables, while retaining the causal grounding and transparency typical of traditional SD models. The framework is conceived to be applied to real-world case-studies from the EU-funded project AutoMoTIF, focusing on data-driven decision support, automation, and optimization of multimodal logistic terminals. We aim at showing how neuro-symbolic methods can bridge the gap between black-box predictive models and the need for critical decision support in complex dynamical environments within cyber-physical systems enabled by the industrial Internet-of-Things.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling</title>
<link>https://arxiv.org/abs/2509.07617</link>
<guid>https://arxiv.org/abs/2509.07617</guid>
<content:encoded><![CDATA[
<div> Energy-based Model, Large Language Models, Prompt Injection Attack, Markov Chain Monte Carlo, Transferability

Summary: 
The paper introduces a novel activations-guided prompt injection attack framework to address the security threat posed by Direct Prompt Injection attacks on Large Language Models (LLMs). By constructing an Energy-based Model (EBM) using activations from a surrogate model, the framework evaluates the quality of adversarial prompts. Token-level Markov Chain Monte Carlo (MCMC) sampling is then used to optimize the prompts, enabling gradient-free black-box attacks. The experimental results show superior cross-model transferability, with a 49.6% attack success rate (ASR) across five mainstream LLMs and a 34.6% improvement over human-crafted prompts. The framework also maintains a 36.6% ASR on unseen task scenarios. Interpretability analysis reveals a correlation between activations and attack effectiveness, emphasizing the importance of semantic patterns in vulnerability exploitation. <div>
arXiv:2509.07617v1 Announce Type: new 
Abstract: Direct Prompt Injection (DPI) attacks pose a critical security threat to Large Language Models (LLMs) due to their low barrier of execution and high potential damage. To address the impracticality of existing white-box/gray-box methods and the poor transferability of black-box methods, we propose an activations-guided prompt injection attack framework. We first construct an Energy-based Model (EBM) using activations from a surrogate model to evaluate the quality of adversarial prompts. Guided by the trained EBM, we employ the token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize adversarial prompts, thereby enabling gradient-free black-box attacks. Experimental results demonstrate our superior cross-model transferability, achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6% improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen task scenarios. Interpretability analysis reveals a correlation between activations and attack effectiveness, highlighting the critical role of semantic patterns in transferable vulnerability exploitation.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment</title>
<link>https://arxiv.org/abs/2509.07642</link>
<guid>https://arxiv.org/abs/2509.07642</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, AI alignment, organizational adoption, agency theory, LLM ATLAS

Summary: 
The study introduces the concept of Large Language Models (LLMs) and highlights the potential benefits they bring to organizations, while also acknowledging the risks of generating off-topic or harmful content. It identifies AI alignment as a significant challenge in LLM adoption due to inherent information asymmetries and the black-box nature of LLMs. To address this problem, the study proposes the LLM ATLAS framework, grounded in agency theory, to mitigate alignment issues during organizational LLM adoption. By conducting a conceptual literature analysis, the framework offers a comprehensive approach to understanding and addressing AI alignment methods in the context of organizational adoption processes. This contributes to an extended literature analysis process specific to AI alignment methods during organizational LLM adoption, and provides a first LLM alignment problem-solution space. 

<br /><br />Summary: <div>
arXiv:2509.07642v1 Announce Type: new 
Abstract: Adopting Large language models (LLMs) in organizations potentially revolutionizes our lives and work. However, they can generate off-topic, discriminating, or harmful content. This AI alignment problem often stems from misspecifications during the LLM adoption, unnoticed by the principal due to the LLM's black-box nature. While various research disciplines investigated AI alignment, they neither address the information asymmetries between organizational adopters and black-box LLM agents nor consider organizational AI adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led Alignment Strategy) a conceptual framework grounded in agency (contract) theory, to mitigate alignment problems during organizational LLM adoption. We conduct a conceptual literature analysis using the organizational LLM adoption phases and the agency theory as concepts. Our approach results in (1) providing an extended literature analysis process specific to AI alignment methods during organizational LLM adoption and (2) providing a first LLM alignment problem-solution space.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepGraphLog for Layered Neurosymbolic AI</title>
<link>https://arxiv.org/abs/2509.07665</link>
<guid>https://arxiv.org/abs/2509.07665</guid>
<content:encoded><![CDATA[
<div> Neurosymbolic AI, DeepGraphLog, ProbLog, Graph Neural Networks, complex dependencies <br />
Summary: <br />
Neurosymbolic AI (NeSy) combines neural networks with symbolic reasoning for better interpretability. Existing NeSy frameworks like DeepProbLog have limitations in modeling complex dependencies due to fixed flow. DeepGraphLog, a novel framework, allows multi-layer neural-symbolic reasoning with arbitrary layering of components. It extends ProbLog with Graph Neural Predicates, enabling symbolic representations to be processed by Graph Neural Networks (GNNs). DeepGraphLog demonstrates its capabilities in various tasks such as planning, knowledge graph completion, and GNN expressivity. It effectively captures complex relational dependencies, especially in graph-structured domains, offering a more expressive and flexible framework for integrating neural and symbolic methods. <br /> <div>
arXiv:2509.07665v1 Announce Type: new 
Abstract: Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neural networks with the interpretability and structure of symbolic reasoning. However, current NeSy frameworks like DeepProbLog enforce a fixed flow where symbolic reasoning always follows neural processing. This restricts their ability to model complex dependencies, especially in irregular data structures such as graphs. In this work, we introduce DeepGraphLog, a novel NeSy framework that extends ProbLog with Graph Neural Predicates. DeepGraphLog enables multi-layer neural-symbolic reasoning, allowing neural and symbolic components to be layered in arbitrary order. In contrast to DeepProbLog, which cannot handle symbolic reasoning via neural methods, DeepGraphLog treats symbolic representations as graphs, enabling them to be processed by Graph Neural Networks (GNNs). We showcase the capabilities of DeepGraphLog on tasks in planning, knowledge graph completion with distant supervision, and GNN expressivity. Our results demonstrate that DeepGraphLog effectively captures complex relational dependencies, overcoming key limitations of existing NeSy systems. By broadening the applicability of neurosymbolic AI to graph-structured domains, DeepGraphLog offers a more expressive and flexible framework for neural-symbolic integration.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing the True Potential of LLMs: A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding</title>
<link>https://arxiv.org/abs/2509.07676</link>
<guid>https://arxiv.org/abs/2509.07676</guid>
<content:encoded><![CDATA[
<div> Framework, Large Language Models, Feedback-Triggered Regeneration, Long-Term Multipath, Self-correction <br />
<br />
Summary: 
The article introduces a novel framework called Feedback-Triggered Regeneration (FTR) to address the challenge of incorrect content generation by Large Language Models (LLMs) during inference. FTR utilizes user feedback to enhance decoding dynamics and activates response regeneration only upon receiving negative feedback, avoiding error propagation. Additionally, the article introduces Long-Term Multipath (LTM) decoding to explore multiple reasoning trajectories systematically through delayed sequence evaluation, overcoming the limitations of standard next-token prediction. Experimental results on mathematical reasoning and code generation benchmarks showcase the effectiveness of the proposed framework, demonstrating consistent and significant improvements over existing self-correction methods. <div>
arXiv:2509.07676v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable performance across diverse tasks, yet their susceptibility to generating incorrect content during inference remains a critical unsolved challenge. While self-correction methods offer potential solutions, their effectiveness is hindered by two inherent limitations: (1) the absence of reliable guidance signals for error localization, and (2) the restricted reasoning depth imposed by conventional next-token decoding paradigms. To address these issues, we propose Feedback-Triggered Regeneration (FTR), a novel framework that synergizes user feedback with enhanced decoding dynamics. Specifically, FTR activates response regeneration only upon receiving negative user feedback, thereby circumventing error propagation from faulty self-assessment while preserving originally correct outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding, which enables systematic exploration of multiple reasoning trajectories through delayed sequence evaluation, effectively overcoming the myopic decision-making characteristic of standard next-token prediction. Extensive experiments on mathematical reasoning and code generation benchmarks demonstrate that our framework achieves consistent and significant improvements over state-of-the-art prompt-based self-correction methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support</title>
<link>https://arxiv.org/abs/2509.07706</link>
<guid>https://arxiv.org/abs/2509.07706</guid>
<content:encoded><![CDATA[
<div> Keywords: FHIR, RAG, MEDS, medical decision support, clinical guidelines

Summary: 
The study proposes the integration of HL7 FHIR with a Retrieval-Augmented Generation (RAG)-based system, creating the FHIR-RAG-MEDS system to enhance personalized medical decision support based on evidence-based clinical guidelines. By merging advanced technologies like RAG and FHIR, the system aims to improve clinical decision-making processes. Despite the potential benefits of these technologies, there is limited research on their practical applications. The integration of RAG and FHIR has the potential to revolutionize medical decision support systems and improve healthcare outcomes. The FHIR-RAG-MEDS system addresses the need for research on applying advanced technologies to real-world medical settings, highlighting the importance of innovation in the healthcare industry. <br /><br />Summary: <div>
arXiv:2509.07706v1 Announce Type: new 
Abstract: In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a Retrieval-Augmented Generation (RAG)-based system to improve personalized medical decision support on evidence-based clinical guidelines, emphasizing the need for research in practical applications. In the evolving landscape of medical decision support systems, integrating advanced technologies such as RAG and HL7 FHIR can significantly enhance clinical decision-making processes. Despite the potential of these technologies, there is limited research on their integration in practical applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2509.07711</link>
<guid>https://arxiv.org/abs/2509.07711</guid>
<content:encoded><![CDATA[
<div> integer answer, proof problems, automated grading system, LLMs, RIMO 

Summary: 
The paper introduces RIMO, a new benchmark for evaluating large language models (LLMs) on International Mathematical Olympiad (IMO) problems. RIMO consists of two tracks: RIMO-N, which simplifies IMO problems to have a single integer answer for deterministic correctness checking, and RIMO-P, which features proof problems broken down into sub-problems for step-by-step evaluation using an automated grading system. Testing ten frontier LLMs, including GPT-4o and Gemini 2.5 Flash, on RIMO shows a significant performance drop compared to older benchmarks like GSM8K and MATH. This indicates a gap between current LLM capabilities and true Olympiad-level reasoning. RIMO provides a challenging yet easy-to-evaluate benchmark, setting a high-resolution standard for future research to address the reasoning gap revealed by the study. 

<br /><br />Summary: <div>
arXiv:2509.07711v1 Announce Type: new 
Abstract: As large language models (LLMs) reach high scores on established mathematical benchmarks, such as GSM8K and MATH, the research community has turned to International Mathematical Olympiad (IMO) problems to push the evaluation frontier. However, existing Olympiad-level benchmarks suffer from practical constraints that introduce grading noise and potential bias, such as heterogeneous answer formats requiring model-based judges and a reliance on potentially flawed solutions. We introduce RIMO, a two-track benchmark designed to preserve peak Olympiad difficulty while eliminating this evaluation noise. The first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique integer answer, allowing for deterministic correctness checking. The second track, RIMO-P, features 456 proof problems with expert-checked solutions, which are decomposed into a sequence of sub-problems to evaluate the step-by-step reasoning process via an automated grading system. Our benchmarking of ten frontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these systems excel on older benchmarks, their performance drops sharply on RIMO. These results highlight a substantial gap between current LLM capabilities and actual Olympiad-level reasoning. By providing a challenging yet easy-to-evaluate suite, RIMO offers a high-resolution yardstick for future research, presenting a clear target for closing the profound reasoning gap our findings expose.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis</title>
<link>https://arxiv.org/abs/2509.07723</link>
<guid>https://arxiv.org/abs/2509.07723</guid>
<content:encoded><![CDATA[
<div> Keywords: Parkinson's disease, gut microbiota, deep learning, feature extraction, classification

Summary:
The study introduces a novel machine learning-based feature extraction method, BDPM, for Parkinson's disease classification using gut microbiota analysis. The research, based on gut microbiota profiles of Parkinson's patients and healthy individuals, aims to improve diagnostic accuracy by identifying differentially abundant taxa. The proposed feature selection framework, RFRE, incorporates ecological knowledge to enhance biological interpretability. The hybrid classification model developed integrates temporal and spatial patterns in microbiome data, allowing for a more robust and accurate prediction of Parkinson's disease. The study highlights the potential for using microbial composition as a biomarker for early diagnosis of Parkinson's disease, addressing the current challenges of misdiagnosis rates and reliance on clinical rating scales. <div>
arXiv:2509.07723v1 Announce Type: new 
Abstract: Background: Parkinson's disease remains a major neurodegenerative disorder with high misdiagnosis rates, primarily due to reliance on clinical rating scales. Recent studies have demonstrated a strong association between gut microbiota and Parkinson's disease, suggesting that microbial composition may serve as a promising biomarker. Although deep learning models based ongut microbiota show potential for early prediction, most approaches rely on single classifiers and often overlook inter-strain correlations or temporal dynamics. Therefore, there is an urgent need for more robust feature extraction methods tailored to microbiome data. Methods: We proposed BDPM (A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis). First, we collected gut microbiota profiles from 39 Parkinson's patients and their healthy spouses to identify differentially abundant taxa. Second, we developed an innovative feature selection framework named RFRE (Random Forest combined with Recursive Feature Elimination), integrating ecological knowledge to enhance biological interpretability. Finally, we designed a hybrid classification model to capture temporal and spatial patterns in microbiome data.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for Streamlining Food Carbon Footprint Analysis</title>
<link>https://arxiv.org/abs/2509.07733</link>
<guid>https://arxiv.org/abs/2509.07733</guid>
<content:encoded><![CDATA[
<div> Keywords: Environmental sustainability, carbon footprint, life cycle assessment, AI techniques, food products<br />
Summary: <br />
This paper presents a methodology that combines advancements in life cycle assessment (LCA) with AI techniques to estimate the carbon footprint of food products. The goal is to provide a more accessible and interactive way for users to understand the environmental impact of their food choices. By using a chatbot interface, users can explore the carbon impact of meals and relate the results to familiar activities. The system showcases a proof-of-concept demonstration using arbitrary food items and follow-up questions, highlighting both the potential benefits and limitations of delivering LCA insights in a user-friendly format. The approach combines publicly available databases and knowledge-augmented AI techniques, such as retrieval-augmented generation, to address the complexities of assessing carbon footprints in a global and opaque supply chain. <div>
arXiv:2509.07733v1 Announce Type: new 
Abstract: Environmental sustainability, particularly in relation to climate change, is a key concern for consumers, producers, and policymakers. The carbon footprint, based on greenhouse gas emissions, is a standard metric for quantifying the contribution to climate change of activities and is often assessed using life cycle assessment (LCA). However, conducting LCA is complex due to opaque and global supply chains, as well as fragmented data. This paper presents a methodology that combines advances in LCA and publicly available databases with knowledge-augmented AI techniques, including retrieval-augmented generation, to estimate cradle-to-gate carbon footprints of food products. We introduce a chatbot interface that allows users to interactively explore the carbon impact of composite meals and relate the results to familiar activities. A live web demonstration showcases our proof-of-concept system with arbitrary food items and follow-up questions, highlighting both the potential and limitations - such as database uncertainties and AI misinterpretations - of delivering LCA insights in an accessible format.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach</title>
<link>https://arxiv.org/abs/2509.07820</link>
<guid>https://arxiv.org/abs/2509.07820</guid>
<content:encoded><![CDATA[
<div> Language models, Generative Adversarial Networks, reasoning tokens, certainty-guided reasoning, token efficiency <br />
Summary: The article introduces a novel approach called Certainty-Guided Reasoning (CGR) for large reasoning language models (LRLMs). Inspired by generative adversarial networks, CGR uses a critic model to assess reasoning confidence and terminate when a certainty threshold is met. This improves accuracy and reduces token usage, with stable performance across multiple runs. CGR allows for millions of tokens savings and offers tunable trade-offs between certainty thresholds and efficiency. By integrating confidence into the reasoning process, CGR enhances adaptability, reliability, and resource efficiency of LRLMs, making them more suitable for practical deployment in tasks where accuracy and computational cost are crucial. <br /><br /> <div>
arXiv:2509.07820v1 Announce Type: new 
Abstract: The rise of large reasoning language models (LRLMs) has unlocked new potential for solving complex tasks. These models operate with a thinking budget, that is, a predefined number of reasoning tokens used to arrive at a solution. We propose a novel approach, inspired by the generator/discriminator framework in generative adversarial networks, in which a critic model periodically probes its own reasoning to assess whether it has reached a confident conclusion. If not, reasoning continues until a target certainty threshold is met. This mechanism adaptively balances efficiency and reliability by allowing early termination when confidence is high, while encouraging further reasoning when uncertainty persists. Through experiments on the AIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR) improves baseline accuracy while reducing token usage. Importantly, extended multi-seed evaluations over 64 runs demonstrate that CGR is stable, reducing variance across seeds and improving exam-like performance under penalty-based grading. Additionally, our token savings analysis shows that CGR can eliminate millions of tokens in aggregate, with tunable trade-offs between certainty thresholds and efficiency. Together, these findings highlight certainty as a powerful signal for reasoning sufficiency. By integrating confidence into the reasoning process, CGR makes large reasoning language models more adaptive, trustworthy, and resource efficient, paving the way for practical deployment in domains where both accuracy and computational cost matter.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study</title>
<link>https://arxiv.org/abs/2509.07846</link>
<guid>https://arxiv.org/abs/2509.07846</guid>
<content:encoded><![CDATA[
<div> keyword: Large language models, ChatGPT, classroom, Retrieval Augmented Generation, EduScopeQA <br />
Summary: <br />
- The study explores the use of large language models like ChatGPT in classrooms and the issue of potentially misleading information provided by these models. 
- Retrieval Augmented Generation (RAG) is proposed as a solution to improve reliability by grounding responses in external resources. 
- Two accessible RAG paradigms, vector-based retrieval and graph-based retrieval, are investigated to determine best practices for classroom question answering. 
- The novel dataset, EduScopeQA, consisting of 3,176 questions across academic subjects, is used to evaluate performance on various educational query types. 
- OpenAI Vector Search RAG is found to be effective for quick fact retrieval, while GraphRAG Global excels at providing pedagogically rich answers to thematic queries. 
- GraphRAG Local achieves the highest accuracy with altered textbooks. 
- A dynamic branching framework is proposed to optimize resource usage and boost fidelity and efficiency when integrating RAG-augmented LLMs into learning environments. <br /> 
Summary: <div>
arXiv:2509.07846v1 Announce Type: new 
Abstract: Large language models like ChatGPT are increasingly used in classrooms, but they often provide outdated or fabricated information that can mislead students. Retrieval Augmented Generation (RAG) improves reliability of LLMs by grounding responses in external resources. We investigate two accessible RAG paradigms, vector-based retrieval and graph-based retrieval to identify best practices for classroom question answering (QA). Existing comparative studies fail to account for pedagogical factors such as educational disciplines, question types, and practical deployment costs. Using a novel dataset, EduScopeQA, of 3,176 questions across academic subjects, we measure performance on various educational query types, from specific facts to broad thematic discussions. We also evaluate system alignment with a dataset of systematically altered textbooks that contradict the LLM's latent knowledge. We find that OpenAI Vector Search RAG (representing vector-based RAG) performs well as a low-cost generalist, especially for quick fact retrieval. On the other hand, GraphRAG Global excels at providing pedagogically rich answers to thematic queries, and GraphRAG Local achieves the highest accuracy with the dense, altered textbooks when corpus integrity is critical. Accounting for the 10-20x higher resource usage of GraphRAG (representing graph-based RAG), we show that a dynamic branching framework that routes queries to the optimal retrieval method boosts fidelity and efficiency. These insights provide actionable guidelines for educators and system designers to integrate RAG-augmented LLMs into learning environments effectively.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs</title>
<link>https://arxiv.org/abs/2509.07858</link>
<guid>https://arxiv.org/abs/2509.07858</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, code instruction data, open-source LLMs, data synthesis, SCoder

Summary: 
Existing code large language models (LLMs) often rely on costly, proprietary data for fine-tuning. This paper explores using small-scale open-source LLMs, like 7B, as synthesizers to construct high-quality code instruction data. They enhance the data synthesis capability of small-scale LLMs by training on superior samples from proprietary models. They propose an iterative self-distillation method to transform small-scale LLMs into powerful synthesizers. This approach reduces reliance on proprietary models and minimizes costs. Multi-checkpoint sampling and multi-aspect scoring strategies are used for initial data selection, while a gradient-based method is introduced for final data filtering. SCoder, a family of code generation models fine-tuned from DeepSeek-Coder, is developed based on the code instruction datasets from the small-scale synthesizers. The SCoder models achieve top-notch code generation capabilities, showcasing the success of their method. 

<br /><br />Summary: <div>
arXiv:2509.07858v1 Announce Type: new 
Abstract: Existing code large language models (LLMs) often rely on large-scale instruction data distilled from proprietary LLMs for fine-tuning, which typically incurs high costs. In this paper, we explore the potential of small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code instruction data construction. We first observe that the data synthesis capability of small-scale LLMs can be enhanced by training on a few superior data synthesis samples from proprietary LLMs. Building on this, we propose a novel iterative self-distillation approach to bootstrap small-scale LLMs, transforming them into powerful synthesizers that reduce reliance on proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain diverse and high-quality self-distilled data, we design multi-checkpoint sampling and multi-aspect scoring strategies for initial data selection. Furthermore, to identify the most influential samples, we introduce a gradient-based influence estimation method for final data filtering. Based on the code instruction datasets from the small-scale synthesizers, we develop SCoder, a family of code generation models fine-tuned from DeepSeek-Coder. SCoder models achieve state-of-the-art code generation capabilities, demonstrating the effectiveness of our method.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models</title>
<link>https://arxiv.org/abs/2509.07867</link>
<guid>https://arxiv.org/abs/2509.07867</guid>
<content:encoded><![CDATA[
<div> Constraint Programming, high-level modeling languages, expert-level model generation, CP-Model-Zoo, natural language description<br />Summary:<br />
Constraint Programming (CP) and its modeling languages have long been seen as valuable for solving complex problems. However, the complexity of these languages and the skill required to create good models have deterred non-experts from using CP. A new tutoring system, CP-Model-Zoo, taps into a database of expert-written models to match a user's natural language problem description with the most suitable CP model. This eliminates the need for manual labeling of data and ensures accurate model retrieval. Experiments demonstrate the system's ability to retrieve correct models based on user descriptions of problems with varying levels of expertise. <div>
arXiv:2509.07867v1 Announce Type: new 
Abstract: Constraint Programming and its high-level modeling languages have long been recognized for their potential to achieve the holy grail of problem-solving. However, the complexity of modeling languages, the large number of global constraints, and the art of creating good models have often hindered non-experts from choosing CP to solve their combinatorial problems. While generating an expert-level model from a natural-language description of a problem would be the dream, we are not yet there. We propose a tutoring system called CP-Model-Zoo, exploiting expert-written models accumulated through the years. CP-Model-Zoo retrieves the closest source code model from a database based on a user's natural language description of a combinatorial problem. It ensures that expert-validated models are presented to the user while eliminating the need for human data labeling. Our experiments show excellent accuracy in retrieving the correct model based on a user-input description of a problem simulated with different levels of expertise.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?</title>
<link>https://arxiv.org/abs/2509.07894</link>
<guid>https://arxiv.org/abs/2509.07894</guid>
<content:encoded><![CDATA[
<div> benchmark, physics Olympiads, (M)LLMs, human-aligned evaluation, HiPhO 

Summary: 
The article introduces HiPhO, a new benchmark focusing on high school physics Olympiads and providing human-aligned evaluation for (M)LLMs. The benchmark includes comprehensive data from 13 recent Olympiad exams, uses official marking schemes for fine-grained grading, and enables direct comparison with human contestants through medal assignments. Evaluation of 30 (M)LLMs shows that open-source models mostly achieve bronze level, while closed-source reasoning models can reach gold medals. There is a significant performance gap between open-source models and top students, indicating room for improvement. HiPhO serves as a rigorous and Olympiad-focused benchmark for advancing physical reasoning in multimodal models, and is open-source for access. 

<br /><br />Summary: <div>
arXiv:2509.07894v1 Announce Type: new 
Abstract: Recently, the physical capabilities of (M)LLMs have garnered increasing attention. However, existing benchmarks for physics suffer from two major gaps: they neither provide systematic and up-to-date coverage of real-world physics competitions such as physics Olympiads, nor enable direct performance comparison with humans. To bridge these gaps, we present HiPhO, the first benchmark dedicated to high school physics Olympiads with human-aligned evaluation. Specifically, HiPhO highlights three key innovations. (1) Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025, spanning both international and regional competitions, and covering mixed modalities that encompass problems spanning text-only to diagram-based. (2) Professional Evaluation: We adopt official marking schemes to perform fine-grained grading at both the answer and step level, fully aligned with human examiners to ensure high-quality and domain-specific evaluation. (3) Comparison with Human Contestants: We assign gold, silver, and bronze medals to models based on official medal thresholds, thereby enabling direct comparison between (M)LLMs and human contestants. Our large-scale evaluation of 30 state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly remain at or below the bronze level; open-source LLMs show promising progress with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold medals; and most models still have a significant gap from full marks. These results highlight a substantial performance gap between open-source models and top students, the strong physical reasoning capabilities of closed-source reasoning models, and the fact that there is still significant room for improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused benchmark for advancing multimodal physical reasoning, is open-source and available at https://github.com/SciYu/HiPhO.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare</title>
<link>https://arxiv.org/abs/2509.07961</link>
<guid>https://arxiv.org/abs/2509.07961</guid>
<content:encoded><![CDATA[
<div> Keywords: welfare measurement, language models, preferences, behavior, AI systems 

Summary: 
This study introduces new experimental paradigms to measure welfare in language models by comparing verbal reports of model preferences with behavior in virtual environments. The research examines how costs and rewards impact behavior and the consistency of responses to a eudaimonic welfare scale. The study found reliable correlations between stated preferences and behavior, indicating preference satisfaction as a measurable welfare proxy in AI systems. However, the consistency between measures varied among models and conditions, with responses not being consistent across perturbations. The findings suggest the feasibility of measuring welfare in language models but also highlight the uncertainty in determining the welfare state of these models. Further exploration is recommended to better understand the cognitive states and welfare subjecthood of language models. 

<br /><br />Summary: <div>
arXiv:2509.07961v1 Announce Type: new 
Abstract: We develop new experimental paradigms for measuring welfare in language models. We compare verbal reports of models about their preferences with preferences expressed through behavior when navigating a virtual environment and selecting conversation topics. We also test how costs and rewards affect behavior and whether responses to an eudaimonic welfare scale - measuring states such as autonomy and purpose in life - are consistent across semantically equivalent prompts. Overall, we observed a notable degree of mutual support between our measures. The reliable correlations observed between stated preferences and behavior across conditions suggest that preference satisfaction can, in principle, serve as an empirically measurable welfare proxy in some of today's AI systems. Furthermore, our design offered an illuminating setting for qualitative observation of model behavior. Yet, the consistency between measures was more pronounced in some models and conditions than others and responses were not consistent across perturbations. Due to this, and the background uncertainty about the nature of welfare and the cognitive states (and welfare subjecthood) of language models, we are currently uncertain whether our methods successfully measure the welfare state of language models. Nevertheless, these findings highlight the feasibility of welfare measurement in language models, inviting further exploration.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating forest carbon stocks from high-resolution remote sensing imagery by reducing domain shift with style transfer</title>
<link>https://arxiv.org/abs/2502.00784</link>
<guid>https://arxiv.org/abs/2502.00784</guid>
<content:encoded><![CDATA[
<div> Keywords: forests, carbon stocks, satellite remote sensing, Swin Transformer, image translation

Summary:
Forest carbon stocks play a crucial role in reducing atmospheric CO2 concentrations and mitigating climate change. Monitoring and assessing these stocks often involve integrating ground sample data with satellite imagery, but current techniques require improved accuracy. In a study of Huize County, China, GF-1 WFV and Landsat TM images were used along with the Swin Transformer method to extract global features using attention mechanisms. By applying style transfer, the carbon stock estimation was transformed into an image translation. This innovative approach aims to enhance the accuracy of forest carbon stock assessments and contribute to a more effective management of carbon reservoirs in forests. 

<br /><br />Summary: <div>
arXiv:2502.00784v1 Announce Type: cross 
Abstract: Forests function as crucial carbon reservoirs on land, and their carbon sinks can efficiently reduce atmospheric CO2 concentrations and mitigate climate change. Currently, the overall trend for monitoring and assessing forest carbon stocks is to integrate ground monitoring sample data with satellite remote sensing imagery. This style of analysis facilitates large-scale observation. However, these techniques require improvement in accuracy. We used GF-1 WFV and Landsat TM images to analyze Huize County, Qujing City, Yunnan Province in China. Using the style transfer method, we introduced Swin Transformer to extract global features through attention mechanisms, converting the carbon stock estimation into an image translation.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving</title>
<link>https://arxiv.org/abs/2509.04827</link>
<guid>https://arxiv.org/abs/2509.04827</guid>
<content:encoded><![CDATA[
<div> control theory, large language model, energy-efficient, SLO-aware, real-time 

Summary: 
The paper introduces VoltanaLLM, a system for energy-efficient and SLO-aware serving of Large Language Models (LLMs) in interactive applications. VoltanaLLM co-designs frequency scaling and request routing in disaggregated architectures to enable fine-grained phase-specific control. It includes a feedback-driven frequency controller for dynamic GPU frequency adaptation and a state-space router for energy-efficient routing decisions. The system achieves up to 36.3% energy savings while maintaining a high SLO attainment rate in real-world datasets. This approach addresses the challenge of the soaring energy costs of LLM inference in modern interactive applications, making sustainable and cost-effective deployment more feasible. <br /><br />Summary: <div>
arXiv:2509.04827v1 Announce Type: cross 
Abstract: Modern Large Language Model (LLM) serving systems increasingly support interactive applications, like real-time chat assistants, code generation tools, and agentic workflows. However, the soaring energy cost of LLM inference presents a growing challenge for sustainable and cost-effective deployment. This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM serving, built from a control theory perspective. VoltanaLLM co-designs frequency scaling and request routing in emerging prefill/decode disaggregated architectures, leveraging their decoupled execution to enable fine-grained phase-specific control. It consists of a feedback-driven frequency controller that dynamically adapts GPU frequency for prefill and decode phases, and a state-space router that explores routing decisions across frequency-scaled instances to minimize energy under latency constraints. We implement VoltanaLLM in SGLang and evaluate its performance over multiple state-of-the-art LLMs and real-world datasets. The results demonstrate that VoltanaLLM achieves up to 36.3% energy savings while maintaining near-perfect SLO attainment rate, paving the way for sustainable and intelligent LLM serving.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-device Zero-shot Label Transfer via Alignment of Time Series Foundation Model Embeddings</title>
<link>https://arxiv.org/abs/2509.06966</link>
<guid>https://arxiv.org/abs/2509.06966</guid>
<content:encoded><![CDATA[
<div> transfer learning, wearable devices, actigraphy, time-series data, label transfer
Summary:<br />
- The paper introduces a framework for transferring labels from clinically validated actigraphy data to consumer wearable devices like the Apple Watch.
- Manual labeling of wearable data is inefficient and costly, necessitating a scalable approach for label transfer.
- The proposed framework utilizes time-series foundation models (TSFMs) to project data from different domains into a shared latent space.
- The Adversarial Alignment of TSFM Embeddings method is used to align the distributions of source and target embeddings within this shared space.
- This alignment facilitates the transfer of labels across devices without requiring paired data, enabling efficient and accurate labeling of consumer wearable data using validated clinical labels. <br /><br />Summary: <div>
arXiv:2509.06966v1 Announce Type: cross 
Abstract: High-quality, medically validated labels exist for clinical actigraphy data but not for ubiquitous consumer wearables like the Apple Watch. Manually labeling wearables data is expensive and doesn't scale. This paper offers a novel framework that transfers valuable labels from a source domain (e.g., actigraphy) to a target domain (e.g., Apple Watch) without requiring paired data. Instead of working with raw time-series signals, we project both domains into a shared latent embedding space using time-series foundation models (TSFMs) and develop a new framework to align the cross-device representations. Our method, Adversarial Alignment of TSFM Embeddings forces the distributions of source and target embeddings to align within this space, facilitating label transfer across device type.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-field SNR Analysis and Tensor Channel Estimation for Multi-UAV Near-field Communications</title>
<link>https://arxiv.org/abs/2509.06967</link>
<guid>https://arxiv.org/abs/2509.06967</guid>
<content:encoded><![CDATA[
<div> antenna array, spectral efficiency, channel estimation, multi-UAV communication, near-field communication<br />
Summary:<br />
This paper focuses on channel estimation for distributed near-field multi-UAV communication systems using extremely large antenna arrays (ELAA) in 6G networks. The analysis compares different models like the plane wave model (PWM), spherical wave model (SWM), and a hybrid spherical-plane wave model (HSPWM) for accuracy and tractability. The HSPWM model shows a good balance between the two. Two channel estimation algorithms, spherical-domain orthogonal matching pursuit (SD-OMP) and tensor-OMP, are proposed. SD-OMP considers elevation, azimuth, and range jointly, while tensor-OMP treats the channel as a tensor for improved scalability and reduced complexity. Simulation results indicate that tensor-OMP offers comparable performance to SD-OMP with better computational efficiency. <div>
arXiv:2509.06967v1 Announce Type: cross 
Abstract: Extremely large antenna array (ELAA) is key to enhancing spectral efficiency in 6G networks. Leveraging the distributed nature of multi-unmanned aerial vehicle (UAV) systems enables the formation of distributed ELAA, which often operate in the near-field region with spatial sparsity, rendering the conventional far-field plane wave assumption invalid. This paper investigates channel estimation for distributed near-field multi-UAV communication systems. We first derive closed-form signal-to-noise ratio (SNR) expressions under the plane wave model (PWM), spherical wave model (SWM), and a hybrid spherical-plane wave model (HSPWM), also referred to as the cross-field model, within a distributed uniform planar array (UPA) scenario. The analysis shows that HSPWM achieves a good balance between modeling accuracy and analytical tractability. Based on this, we propose two channel estimation algorithms: the spherical-domain orthogonal matching pursuit (SD-OMP) and the tensor-OMP. The SD-OMP generalizes the polar domain to jointly consider elevation, azimuth, and range. Under the HSPWM, the channel is naturally formulated as a tensor, enabling the use of tensor-OMP. Simulation results demonstrate that tensor-OMP achieves normalized mean square error (NMSE) performance comparable to SD-OMP, while offering reduced computational complexity and improved scalability.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-based Techniques for Integrated Sensing and Communication Systems: State-of-the-Art, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2509.06968</link>
<guid>https://arxiv.org/abs/2509.06968</guid>
<content:encoded><![CDATA[
<div> DL-based techniques, integrated sensing and communication, ISAC systems, 6G networks, deep learning architectures
Summary: 
- The article reviews recent developments on DL-based techniques for ISAC systems, which combine sensing and communication functionalities.
- ISAC is crucial for 6G and beyond networks, offering reduced hardware complexity and improved energy efficiency.
- DL-based techniques provide efficient and near-optimal solutions for ISAC tasks like waveform design and interference mitigation.
- These techniques are well-suited for real-time systems with limited computational resources and low latency requirements.
- The survey categorizes and reviews state-of-the-art DL-based approaches for ISAC, highlighting their advantages, challenges, and future research directions. 

<br /><br />Summary: <div>
arXiv:2509.06968v1 Announce Type: cross 
Abstract: This article comprehensively reviews recent developments and research on deep learning-based (DL-based) techniques for integrated sensing and communication (ISAC) systems. ISAC, which combines sensing and communication functionalities, is regarded as a key enabler for 6G and beyond networks, as many emerging applications, such as vehicular networks and industrial robotics, necessitate both sensing and communication capabilities for effective operation. A unified platform that provides both functions can reduce hardware complexity, alleviate frequency spectrum congestion, and improve energy efficiency. However, integrating these functionalities on the same hardware requires highly optimized signal processing and system design, introducing significant computational complexity when relying on conventional iterative or optimization-based techniques. As an alternative to conventional techniques, DL-based techniques offer efficient and near-optimal solutions with reduced computational complexity. Hence, such techniques are well-suited for operating under limited computational resources and low latency requirements in real-time systems. DL-based techniques can swiftly and effectively yield near-optimal solutions for a wide range of sophisticated ISAC-related tasks, including waveform design, channel estimation, sensing signal processing, data demodulation, and interference mitigation. Therefore, motivated by these advantages, recent studies have proposed various DL-based approaches for ISAC system design. After briefly introducing DL architectures and ISAC fundamentals, this survey presents a comprehensive and categorized review of state-of-the-art DL-based techniques for ISAC, highlights their key advantages and major challenges, and outlines potential directions for future research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Association of Timing and Duration of Moderate-to-Vigorous Physical Activity with Cognitive Function and Brain Aging: A Population-Based Study Using the UK Biobank</title>
<link>https://arxiv.org/abs/2509.06969</link>
<guid>https://arxiv.org/abs/2509.06969</guid>
<content:encoded><![CDATA[
<div> Keywords: physical activity, cognitive function, brain structure, aging, public health<br />
Summary:<br />
- Higher moderate-to-vigorous physical activity (MVPA) is linked to enhanced cognitive performance in various domains in individuals aged 60 and older. 
- MVPA, when meeting WHO guidelines, shows stronger associations with cognitive function. 
- Greater MVPA is also associated with improved brain structure in regions related to emotion, working memory, and perceptual processing. 
- Secondary analysis reveals that MVPA at any time of day, particularly in the midday-afternoon and evening, has benefits for cognitive function and brain volume. 
- The study suggests that increasing MVPA through public health strategies can support healthy cognitive aging and potentially lead to significant economic benefits globally, projected at USD 760 billion annually by 2050. 
Summary: <div>
arXiv:2509.06969v1 Announce Type: cross 
Abstract: Physical activity is a modifiable lifestyle factor with potential to support cognitive resilience. However, the association of moderate-to-vigorous physical activity (MVPA) intensity, and timing, with cognitive function and region-specific brain structure remain poorly understood. We analyzed data from 45,892 UK Biobank participants aged 60 years and older with valid wrist-worn accelerometer data, cognitive testing, and structural brain MRI. MVPA was measured both continuously (mins per week) and categorically (thresholded using >=150 min/week based on WHO guidelines). Associations with cognitive performance and regional brain volumes were evaluated using multivariable linear models adjusted for demographic, socioeconomic, and health-related covariates. We conducted secondary analyses on MVPA timing and subgroup effects. Higher MVPA was associated with better performance across cognitive domains, including reasoning, memory, executive function, and processing speed. These associations persisted in fully adjusted models and were higher among participants meeting WHO guidelines. Greater MVPA was also associated with subcortical brain regions (caudate, putamen, pallidum, thalamus), as well as regional gray matter volumes involved in emotion, working memory, and perceptual processing. Secondary analyses showed that MVPA at any time of day was associated with cognitive functions and brain volume particularly in the midday-afternoon and evening. Sensitivity analysis shows consistent findings across subgroups, with evidence of dose-response relationships. Higher MVPA is associated with preserved brain structure and enhanced cognitive function in later life. Public health strategies to increase MVPA may support healthy cognitive aging and generate substantial economic benefits, with global gains projected to reach USD 760 billion annually by 2050.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Neuron Models on Spiking Neural Networks performance. A Complexity Based Classification Approach</title>
<link>https://arxiv.org/abs/2509.06970</link>
<guid>https://arxiv.org/abs/2509.06970</guid>
<content:encoded><![CDATA[
<div> Neuron models, learning rules, Spiking Neural Networks, bio-signal processing, classification performance <br />
Summary: <br />
This study investigates the impact of neuron model selection and learning rules on the classification performance of Spiking Neural Networks (SNNs) for bio-signal processing. Different biologically inspired neuron models like Leaky Integrate-and-Fire (LIF), metaneurons, and probabilistic Levy-Baxter (LB) neurons were compared with various learning rules such as spike-timing-dependent plasticity (STDP), tempotron, and reward-modulated updates. A complexity-based decision mechanism using Lempel-Ziv Complexity (LZC) was integrated into the evaluation pipeline to quantify the structural regularity of spike trains and assess classification outcomes consistently. Synthetic datasets with different temporal dependencies and stochasticity levels, modeled using Markov and Poisson processes, were used to analyze neural dynamics and algorithm performance. The validation of synthetic Poisson and Markov-modeled data revealed that classification accuracy depends on the interaction of neuron model, network size, and learning rule, with LZC-based evaluation highlighting robust configurations to weak or noisy signals. This systematic analysis provides insights into the interaction of neuron model selection, network parameters, learning strategies, and offers a consistent benchmark for evaluating SNN performance. <div>
arXiv:2509.06970v1 Announce Type: cross 
Abstract: This study explores how the selection of neuron models and learning rules impacts the classification performance of Spiking Neural Networks (SNNs), with a focus on applications in bio-signal processing. We compare biologically inspired neuron models, including Leaky Integrate-and-Fire (LIF), metaneurons, and probabilistic Levy-Baxter (LB) neurons, across multiple learning rules, including spike-timing-dependent plasticity (STDP), tempotron, and reward-modulated updates. A novel element of this work is the integration of a complexity-based decision mechanism into the evaluation pipeline. Using Lempel-Ziv Complexity (LZC), a measure related to entropy rate, we quantify the structural regularity of spike trains and assess classification outcomes in a consistent and interpretable manner across different SNN configurations. To investigate neural dynamics and assess algorithm performance, we employed synthetic datasets with varying temporal dependencies and stochasticity levels. These included Markov and Poisson processes, well-established models to simulate neuronal spike trains and capture the stochastic firing behavior of biological neurons.Validation of synthetic Poisson and Markov-modeled data reveals clear performance trends: classification accuracy depends on the interaction between neuron model, network size, and learning rule, with the LZC-based evaluation highlighting configurations that remain robust to weak or noisy signals. This work delivers a systematic analysis of how neuron model selection interacts with network parameters and learning strategies, supported by a novel complexity-based evaluation approach that offers a consistent benchmark for SNN performance.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model</title>
<link>https://arxiv.org/abs/2509.06974</link>
<guid>https://arxiv.org/abs/2509.06974</guid>
<content:encoded><![CDATA[
<div> Keywords: Sleep quality, forecasting, spatial-temporal model, domain adaptation, explainability analysis

Summary:
The paper introduces a two-stage adaptive spatial-temporal model for predicting sleep quality scores. The model combines convolutional layers for spatial interactions, recurrent layers for long-term temporal dependencies, and a domain adaptation strategy for generalization. Experiments with different input and prediction window sizes show that the model outperforms baseline approaches, even for longer forecasting horizons. The best performance was achieved with a three-day input window and a one-day prediction window. The model also demonstrated good predictive performance for longer prediction windows. An explainability analysis was conducted to understand the impact of different features on sleep quality, showing the model's robustness and adaptability for personalized sleep forecasting using data from wearable devices.

<br /><br />Summary: The paper presents an interpretable and individualized spatial-temporal model for predicting sleep quality, incorporating convolutional and recurrent layers and domain adaptation. Experimental results demonstrate its superiority over baseline approaches, especially for shorter forecasting horizons. Additionally, the model performs well even for longer prediction windows and shows good explainability in identifying key features influencing sleep quality. <div>
arXiv:2509.06974v1 Announce Type: cross 
Abstract: Sleep quality significantly impacts well-being. Therefore, healthcare providers and individuals need accessible and reliable forecasting tools for preventive interventions. This paper introduces an interpretable, individualized two-stage adaptive spatial-temporal model for predicting sleep quality scores. Our proposed framework combines multi-scale convolutional layers to model spatial interactions across multiple input variables, recurrent layers and attention mechanisms to capture long-term temporal dependencies, and a two-stage domain adaptation strategy to enhance generalization. The first adaptation stage is applied during training to mitigate overfitting on the training set. In the second stage, a source-free test-time adaptation mechanism is employed to adapt the model to new users without requiring labels. We conducted various experiments with five input window sizes (3, 5, 7, 9, and 11 days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model consistently outperformed time series forecasting baseline approaches, including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The best performance was achieved with a three-day input window and a one-day prediction window, yielding a root mean square error (RMSE) of 0.216. Furthermore, the model demonstrated good predictive performance even for longer forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction window), highlighting its practical utility for real-world applications. We also conducted an explainability analysis to examine how different features influence sleep quality. These findings proved that the proposed framework offers a robust, adaptive, and explainable solution for personalized sleep forecasting using sparse data from commercial wearable devices.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2509.06975</link>
<guid>https://arxiv.org/abs/2509.06975</guid>
<content:encoded><![CDATA[
<div> benchmark, transferability, graph SSL methods, pretraining objectives, GraphMAE <br />
Summary:
GSTBench is introduced as a benchmark for evaluating the transferability of graph SSL methods. This benchmark conducts large-scale pretraining on ogbn-papers100M and assesses five representative SSL methods across various target graphs. The experimental setup is standardized to facilitate rigorous comparisons focusing solely on pretraining objectives. Most graph SSL methods struggle to generalize between datasets, with some performing worse than random initialization. GraphMAE, however, a masked autoencoder approach, consistently enhances transfer performance. The analysis identifies factors influencing these disparities and offers insights for guiding future research on transferable graph SSL. This work establishes a foundation for the "pretrain-then-transfer" paradigm in graph learning. The code for GSTBench is available on GitHub for further exploration and validation. <br /> <div>
arXiv:2509.06975v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) has shown great promise in graph representation learning. However, most existing graph SSL methods are developed and evaluated under a single-dataset setting, leaving their cross-dataset transferability largely unexplored and limiting their ability to leverage knowledge transfer and large-scale pretraining, factors that are critical for developing generalized intelligence beyond fitting training data. To address this gap and advance foundation model research for graphs, we present GSTBench, the first systematic benchmark for evaluating the transferability of graph SSL methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate five representative SSL methods across a diverse set of target graphs. Our standardized experimental setup decouples confounding factors such as model architecture, dataset characteristics, and adaptation protocols, enabling rigorous comparisons focused solely on pretraining objectives. Surprisingly, we observe that most graph SSL methods struggle to generalize, with some performing worse than random initialization. In contrast, GraphMAE, a masked autoencoder approach, consistently improves transfer performance. We analyze the underlying factors that drive these differences and offer insights to guide future research on transferable graph SSL, laying a solid foundation for the "pretrain-then-transfer" paradigm in graph learning. Our code is available at https://github.com/SongYYYY/GSTBench.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction</title>
<link>https://arxiv.org/abs/2509.06976</link>
<guid>https://arxiv.org/abs/2509.06976</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic demand prediction, knowledge-guided, cross-modal feature representation learning, structured temporal data, dynamic update strategy

Summary:
Traffic demand prediction is crucial for intelligent transportation systems. Current models mainly rely on temporal traffic data, neglecting the valuable insights from human knowledge and experience. This paper introduces a Knowledge-Guided Cross-Modal feature representation learning (KGCM) model that integrates structured temporal data with textual human knowledge. By leveraging prior knowledge datasets and adaptive graph networks, the KGCM model effectively learns multimodal data features and utilizes a dynamic update strategy for parameter optimization. Experimental results on various traffic datasets showcase that the proposed model accurately predicts future traffic demand, surpassing existing state-of-the-art models. The integration of human experience and knowledge with temporal traffic data enhances the accuracy and robustness of traffic demand predictions, showcasing the potential for improved performance in intelligent transportation systems. 

<br /><br />Summary: <div>
arXiv:2509.06976v1 Announce Type: cross 
Abstract: Traffic demand prediction plays a critical role in intelligent transportation systems. Existing traffic prediction models primarily rely on temporal traffic data, with limited efforts incorporating human knowledge and experience for urban traffic demand forecasting. However, in real-world scenarios, traffic knowledge and experience derived from human daily life significantly influence precise traffic prediction. Such knowledge and experiences can guide the model in uncovering latent patterns within traffic data, thereby enhancing the accuracy and robustness of predictions. To this end, this paper proposes integrating structured temporal traffic data with textual data representing human knowledge and experience, resulting in a novel knowledge-guided cross-modal feature representation learning (KGCM) model for traffic demand prediction. Based on regional transportation characteristics, we construct a prior knowledge dataset using a large language model combined with manual authoring and revision, covering both regional and global knowledge and experiences. The KGCM model then learns multimodal data features through designed local and global adaptive graph networks, as well as a cross-modal feature fusion mechanism. A proposed reasoning-based dynamic update strategy enables dynamic optimization of the graph model's parameters, achieving optimal performance. Experiments on multiple traffic datasets demonstrate that our model accurately predicts future traffic demand and outperforms existing state-of-the-art (SOTA) models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First Framework with Three-Tier Verification</title>
<link>https://arxiv.org/abs/2509.06977</link>
<guid>https://arxiv.org/abs/2509.06977</guid>
<content:encoded><![CDATA[
<div> Framework, deep learning, cross-backend compatibility, CPU, GPU

Summary:
- The paper introduces a configuration-first framework for evaluating cross-backend compatibility in deep learning systems deployed on CPU, GPU, and compiled runtimes.
- The framework utilizes YAML to decouple experiments from code and employs a three-tier verification protocol covering tensor-level closeness, activation alignment, and task-level metrics.
- Results from 672 checks across multiple models indicate that 72.0% of runs pass, with discrepancies mostly occurring under stricter thresholds.
- Detection models and compiled backends are identified as particularly prone to drift, often due to nondeterministic post-processing.
- The study shows that using deterministic adapters and selective fallbacks can significantly improve agreement without compromising performance. 

<br /><br />Summary: <div>
arXiv:2509.06977v1 Announce Type: cross 
Abstract: This paper presents a configuration-first framework for evaluating cross-backend compatibility in deep learning systems deployed on CPU, GPU, and compiled runtimes. The framework decouples experiments from code using YAML, supports both library and repository models, and employs a three-tier verification protocol covering tensor-level closeness, activation alignment, and task-level metrics. Through 672 checks across multiple models and tolerance settings, we observe that 72.0% of runs pass, with most discrepancies occurring under stricter thresholds. Our results show that detection models and compiled backends are particularly prone to drift, often due to nondeterministic post-processing. We further demonstrate that deterministic adapters and selective fallbacks can substantially improve agreement without significant performance loss. To our knowledge, this is the first unified framework that systematically quantifies and mitigates cross-backend drift in deep learning, providing a reproducible methodology for dependable deployment across heterogeneous runtimes.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Over-stationarization in Deep Learning-based Bus/Tram Arrival Time Prediction: Analysis and Non-stationary Effect Recovery</title>
<link>https://arxiv.org/abs/2509.06979</link>
<guid>https://arxiv.org/abs/2509.06979</guid>
<content:encoded><![CDATA[
<div> prediction, public transport, deep learning, non-stationarity, temporal dynamics

Summary:
- Deep learning has shown great performance in predicting public transport vehicle arrival times.
- Non-stationary data can impact model performance in multi-step arrival time prediction.
- A new approach called non-stationary ATP (NSATP) is proposed to balance predictability and non-stationarity.
- NSATP includes series stationarization to improve predictability and non-stationarity effect recovery to capture hidden periodicity and compensate for over-stationarization.
- Experimental results using 125 days of public transport data from Dresden show that NSATP outperforms baseline methods, reducing RMSE, MAE, and MAPE for trams and buses. 

<br /><br />Summary: <div>
arXiv:2509.06979v1 Announce Type: cross 
Abstract: Arrival time prediction (ATP) of public transport vehicles is essential in improving passenger experience and supporting traffic management. Deep learning has demonstrated outstanding performance in ATP due to its ability to model non-linear and temporal dynamics. In the multi-step ATP, non-stationary data will degrade the model performance due to the variation in variables' joint distribution along the temporal direction. Previous studies mainly applied normalization to eliminate the non-stationarity in time series, thereby achieving better predictability. However, the normalization may obscure useful characteristics inherent in non-stationarity, which is known as the over-stationarization. In this work, to trade off predictability and non-stationarity, a new approach for multi-step ATP, named non-stationary ATP ( NSATP), is proposed. The method consists of two stages: series stationarization and non-stationarity effect recovery. The first stage aims at improving the predictability. As for the latter, NSATP extends a state-of-the-art method from one-dimensional to two dimensional based models to capture the hidden periodicity in time series and designs a compensation module of over-stationarization by learning scaling and shifting factors from raw data. 125 days' public transport operational data of Dresden is collected for validation. Experimental results show that compared to baseline methods, the proposed NSATP can reduce RMSE, MAE, and MAPE by 2.37%, 1.22%, and 2.26% for trams and by 1.72%, 0.60%, and 1.17% for buses, respectively.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use</title>
<link>https://arxiv.org/abs/2509.06980</link>
<guid>https://arxiv.org/abs/2509.06980</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, reinforcement learning, tool-use, asynchronous caller, evaluation needs

Summary:
RLFactory is a reinforcement learning framework designed to enhance the performance of large language models in tasks that require interaction with external tools. It addresses issues related to tool-call stability, adaptability, and diversity in evaluation needs through features such as an asyncio-based asynchronous caller and a decoupled tool/training architecture. By introducing observation markers from tool feedback, RLFactory closes the loop between the model, tools, and environment, leading to dynamic policy optimization. The framework demonstrates its effectiveness on the Search-R1 dataset, achieving a test score of 0.486 on the Natural Questions dataset. Compared to larger models, RLFactory shows superior performance and increases training throughput significantly. With its low-barrier and adaptable nature, RLFactory provides a promising solution for enhancing multi-round tool use of large language models in real-world applications. 

<br /><br />Summary: <div>
arXiv:2509.06980v1 Announce Type: cross 
Abstract: Large language models excel at basic reasoning but struggle with tasks that require interaction with external tools. We present RLFactory, a plug-and-play reinforcement learning post-training framework for multi-round tool use. RLFactory tackles (i) tool-call stability and adaptability amid tool heterogeneity and interface issues via an asyncio-based asynchronous caller and a decoupled tool/training architecture, and (ii) diverse evaluation needs via a reward layer supporting rule-based, model-judgment, and tool-verification signals. It reconstructs the MDP by introducing observation markers from tool feedback, closing the loop among model, tools, and environment, and implements a generate-parse-invoke-update workflow for dynamic policy optimization. On Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural Questions (NQ) dataset, surpassing larger models trained with similar techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable framework for strengthening multi-round tool use of LLMs in real-world scenarios. Code: https://github.com/Simple-Efficient/RL-Factory.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention</title>
<link>https://arxiv.org/abs/2509.06982</link>
<guid>https://arxiv.org/abs/2509.06982</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, decoding-time interventions, safety alignment, guard model, rollback mechanism<br />
Summary: 
The article introduces a novel framework called CARE for ensuring the safety of outputs during decoding with large language models (LLMs). CARE integrates a guard model for real-time safety monitoring, a rollback mechanism with a token buffer for efficient correction of unsafe outputs, and an introspection-based intervention strategy. The guard model detects potentially unsafe content, the rollback mechanism corrects unsafe outputs at an earlier stage, and the introspection method generates self-reflective critiques to guide subsequent decoding steps. Experimental results demonstrate that CARE achieves a superior balance of safety, quality, and efficiency, with low harmful response rates and minimal disruption to the user experience while maintaining high response quality.<br /><br />Summary: <div>
arXiv:2509.06982v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, ensuring the safety of their outputs during decoding has become a critical challenge. However, existing decoding-time interventions, such as Contrastive Decoding, often force a severe trade-off between safety and response quality. In this work, we propose CARE, a novel framework for decoding-time safety alignment that integrates three key components: (1) a guard model for real-time safety monitoring, enabling detection of potentially unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe outputs efficiently at an earlier stage without disrupting the user experience; and (3) a novel introspection-based intervention strategy, where the model generates self-reflective critiques of its previous outputs and incorporates these reflections into the context to guide subsequent decoding steps. The framework achieves a superior safety-quality trade-off by using its guard model for precise interventions, its rollback mechanism for timely corrections, and our novel introspection method for effective self-correction. Experimental results demonstrate that our framework achieves a superior balance of safety, quality, and efficiency, attaining a low harmful response rate and minimal disruption to the user experience while maintaining high response quality.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities</title>
<link>https://arxiv.org/abs/2509.06984</link>
<guid>https://arxiv.org/abs/2509.06984</guid>
<content:encoded><![CDATA[
<div> Parameter-efficient fine-tuning; Low-Rank Adaptation; federated learning; multimodal data; missing modalities <br />
Summary:
FediLoRA is introduced as a framework for federated multimodal fine-tuning considering heterogeneous LoRA ranks and missing modalities. It employs a dimension-wise aggregation strategy to handle varying LoRA ranks among clients, preventing information dilution during aggregation. Additionally, a layer-wise model editing method selectively incorporates global parameters to enhance local components, leading to improved client and global model performances. Experimental results on three benchmark datasets showcase FediLoRA's superior performance compared to existing methods in both global and personalized settings, especially in scenarios with incomplete modalities. FediLoRA addresses practical challenges faced in real-world applications of foundation models, offering a robust solution for efficient fine-tuning in decentralized environments.<br /><br />Summary: <div>
arXiv:2509.06984v1 Announce Type: cross 
Abstract: Foundation models have demonstrated remarkable performance across a wide range of tasks, yet their large parameter sizes pose challenges for practical deployment, especially in decentralized environments. Parameter-efficient fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing and memory overhead, making it attractive for federated learning. However, existing federated LoRA methods typically assume uniform rank configurations and unimodal inputs, overlooking two key real-world challenges: (1) heterogeneous client resources have different LoRA ranks, and (2) multimodal data settings with potentially missing modalities. In this work, we propose FediLoRA, a simple yet effective framework for federated multimodal fine-tuning under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a dimension-wise aggregation strategy that reweights LoRA updates without information dilution during aggregation. It also includes a lightweight layer-wise model editing method that selectively incorporates global parameters to repair local components which improves both client and global model performances. Experimental results on three multimodal benchmark datasets demonstrate that FediLoRA achieves superior performance over competitive baselines in both global and personalized settings, particularly in the presence of modality incompleteness.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis</title>
<link>https://arxiv.org/abs/2509.06986</link>
<guid>https://arxiv.org/abs/2509.06986</guid>
<content:encoded><![CDATA[
<div> Transformer-based architecture, robust representation, cellular morphology, batch effects, out-of-distribution generalization

Summary: 
The article presents CellPainTR, a Transformer-based architecture designed to learn foundational representations of cellular morphology that are robust to batch effects. Unlike traditional methods, CellPainTR can effectively generalize to entirely unseen datasets without the need for retraining. The study validates CellPainTR on the JUMP dataset, showcasing its outperformance of established methods like ComBat and Harmony in batch integration and biological signal preservation. Importantly, CellPainTR demonstrates robustness through an out-of-distribution task on the Bray et al. dataset, maintaining high performance despite significant domain and feature shifts. This work marks a significant advancement in creating foundational models for image-based profiling, enabling more reliable and scalable cross-study biological analysis. 

Summary: <div>
arXiv:2509.06986v1 Announce Type: cross 
Abstract: Large-scale biological discovery requires integrating massive, heterogeneous datasets like those from the JUMP Cell Painting consortium, but technical batch effects and a lack of generalizable models remain critical roadblocks. To address this, we introduce CellPainTR, a Transformer-based architecture designed to learn foundational representations of cellular morphology that are robust to batch effects. Unlike traditional methods that require retraining on new data, CellPainTR's design, featuring source-specific context tokens, allows for effective out-of-distribution (OOD) generalization to entirely unseen datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP dataset, where it outperforms established methods like ComBat and Harmony in both batch integration and biological signal preservation. Critically, we demonstrate its robustness through a challenging OOD task on the unseen Bray et al. dataset, where it maintains high performance despite significant domain and feature shifts. Our work represents a significant step towards creating truly foundational models for image-based profiling, enabling more reliable and scalable cross-study biological analysis.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection</title>
<link>https://arxiv.org/abs/2509.06987</link>
<guid>https://arxiv.org/abs/2509.06987</guid>
<content:encoded><![CDATA[
<div> fusion, multimodal, YOLO, Vision Transformer, defect

Summary:<br />
The paper introduces a new multimodal fusion architecture for defect detection in rail structures, leveraging YOLO and Vision Transformer backbones. By combining audio signal data with image information, the proposed approach addresses limitations of single-modality detection methods, resulting in improved precision and overall accuracy. The architecture integrates YOLOv8n for rapid object detection and a Vision Transformer to combine feature maps extracted from multiple layers. The focus is on detecting rail Rupture and Surface defects. Experimental results on a real-world railway dataset show a 0.2-point increase in accuracy compared to vision-only approaches. Statistical analysis using Student's t-test confirms the significant differences in mean accuracy between the multimodal fusion approach and single-modality methods. <div>
arXiv:2509.06987v1 Announce Type: cross 
Abstract: Multimodal fusion is a multimedia technique that has become popular in the wide range of tasks where image information is accompanied by a signal/audio. The latter may not convey highly semantic information, such as speech or music, but some measures such as audio signal recorded by mics in the goal to detect rail structure elements or defects. While classical detection approaches such as You Only Look Once (YOLO) family detectors can be efficiently deployed for defect detection on the image modality, the single modality approaches remain limited. They yield an overdetection in case of the appearance similar to normal structural elements. The paper proposes a new multimodal fusion architecture built on the basis of domain rules with YOLO and Vision transformer backbones. It integrates YOLOv8n for rapid object detection with a Vision Transformer (ViT) to combine feature maps extracted from multiple layers (7, 16, and 19) and synthesised audio representations for two defect classes: rail Rupture and Surface defect. Fusion is performed between audio and image. Experimental evaluation on a real-world railway dataset demonstrates that our multimodal fusion improves precision and overall accuracy by 0.2 points compared to the vision-only approach. Student's unpaired t-test also confirms statistical significance of differences in the mean accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2509.06988</link>
<guid>https://arxiv.org/abs/2509.06988</guid>
<content:encoded><![CDATA[
<div> Keywords: out-of-distribution detection, feature-based methods, data privacy protection, subspace projection, ClaFR

Summary:
Out-of-distribution (OOD) detection is crucial for security applications to identify data outside the training categories. Feature-based post-hoc methods assess data variances in the feature space without altering network parameters but often rely on access to training data, posing data privacy concerns. In response, the Classifier-based Feature Reconstruction (ClaFR) algorithm is proposed, leveraging a subspace projection approach. ClaFR decomposes classifier weights to extract a class-known subspace, enabling the mapping of original data features to this space for new representations. The OOD score is determined by calculating feature reconstruction error within the subspace. Notably, ClaFR outperforms existing OOD detection algorithms without necessitating access to training data. The code for ClaFR is publicly available on GitHub, showcasing its efficiency and privacy protection capabilities. <br /><br />Summary: <div>
arXiv:2509.06988v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection helps models identify data outside the training categories, crucial for security applications. While feature-based post-hoc methods address this by evaluating data differences in the feature space without changing network parameters, they often require access to training data, which may not be suitable for some data privacy scenarios. This may not be suitable in scenarios where data privacy protection is a concern. In this paper, we propose a simple yet effective post-hoc method, termed Classifier-based Feature Reconstruction (ClaFR), from the perspective of subspace projection. It first performs an orthogonal decomposition of the classifier's weights to extract the class-known subspace, then maps the original data features into this subspace to obtain new data representations. Subsequently, the OOD score is determined by calculating the feature reconstruction error of the data within the subspace. Compared to existing OOD detection algorithms, our method does not require access to training data while achieving leading performance on multiple OOD benchmarks. Our code is released at https://github.com/Aie0923/ClaFR.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Protocol Genome A Self Supervised Learning Framework from DICOM Headers</title>
<link>https://arxiv.org/abs/2509.06995</link>
<guid>https://arxiv.org/abs/2509.06995</guid>
<content:encoded><![CDATA[
<div> DICOM headers, Protocol Genome, self-supervised learning, clinical imaging, calibration

Summary:
Protocol Genome is a self-supervised learning system that utilizes DICOM headers to learn correlations and improve image representations in clinical imaging. By considering structured DICOM headers as labels, the system achieves high AUROC and ECE scores on external validation, demonstrating improved calibration and robustness across modalities and vendors. The method includes protocol-image contrastive learning, masked protocol prediction, and protocol-protocol translation to model header fields and image features. Results show significant improvements in AUROC and calibration metrics compared to strong SSL baselines and ImageNet transfer, with the technique reducing false positives and being applicable in a clinical PACS environment. A model card and deployment guide, complete with de-identification and bias audits, are provided for implementation. <div>
arXiv:2509.06995v1 Announce Type: cross 
Abstract: In this paper, we introduce the Protocol Genome, a self-supervised learning system that learns correlations from DICOM headers and achieves AUROC 0.901 (vs 0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation. Our method also improves calibration and robustness across modalities (CT, MRI, CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where procedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice thickness) have consequences for contrast, noise, and artifact. These latent confounders impede the generalization of image-only networks across sites. We consider structured DICOM headers as a label and learn protocol-aware but clinically robust image representations. Protocol Genome obtains tokenized embeddings of de-identified header fields and models them along with image features using: (1) protocol-image contrastive learning, (2) masked protocol prediction, and (3) protocol-protocol translation. With 1.26M studies (7 health systems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT triage for PE, (B) brain MRI glioma grading, and (C) chest radiograph cardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well as ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041: cardiomegaly) is associated with higher external AUROC; 25-37% calibration improvements are obtained (p < 0.01, DeLong tests). While the gains may be task-dependent, they are preserved with 10-20% of labeled data. From a clinical point of view, the technique reduces false positives at protocol borders and is applicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a model card and deployment guide, complete with both de-identification and bias audits.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems</title>
<link>https://arxiv.org/abs/2509.06996</link>
<guid>https://arxiv.org/abs/2509.06996</guid>
<content:encoded><![CDATA[
arXiv:2509.06996v1 Announce Type: cross 
Abstract: Writing is a universal cultural technology that reuses vision for symbolic communication. Humans display striking resilience: we readily recognize words even when characters are fragmented, fused, or partially occluded. This paper investigates whether advanced vision language models (VLMs) share this resilience. We construct two psychophysics inspired benchmarks across distinct writing systems, Chinese logographs and English alphabetic words, by splicing, recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli for models while remaining legible to humans. Despite strong performance on clean text, contemporary VLMs show a severe drop under these perturbations, frequently producing unrelated or incoherent outputs. The pattern suggests a structural limitation: models heavily leverage generic visual invariances but under rely on compositional priors needed for robust literacy. We release stimuli generation code, prompts, and evaluation protocols to facilitate transparent replication and follow up work. Our findings motivate architectures and training strategies that encode symbol segmentation, composition, and binding across scripts, and they delineate concrete challenges for deploying multimodal systems in education, accessibility, cultural heritage, and security.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories</title>
<link>https://arxiv.org/abs/2509.06998</link>
<guid>https://arxiv.org/abs/2509.06998</guid>
<content:encoded><![CDATA[
arXiv:2509.06998v1 Announce Type: cross 
Abstract: Can models generalize attribute knowledge across semantically and perceptually dissimilar categories? While prior work has addressed attribute prediction within narrow taxonomic or visually similar domains, it remains unclear whether current models can abstract attributes and apply them to conceptually distant categories. This work presents the first explicit evaluation for the robustness of the attribute prediction task under such conditions, testing whether models can correctly infer shared attributes between unrelated object types: e.g., identifying that the attribute "has four legs" is common to both "dogs" and "chairs". To enable this evaluation, we introduce train-test split strategies that progressively reduce correlation between training and test sets, based on: LLM-driven semantic grouping, embedding similarity thresholding, embedding-based clustering, and supercategory-based partitioning using ground-truth labels. Results show a sharp drop in performance as the correlation between training and test categories decreases, indicating strong sensitivity to split design. Among the evaluated methods, clustering yields the most effective trade-off, reducing hidden correlations while preserving learnability. These findings offer new insights into the limitations of current representations and inform future benchmark construction for attribute reasoning.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code</title>
<link>https://arxiv.org/abs/2509.07006</link>
<guid>https://arxiv.org/abs/2509.07006</guid>
<content:encoded><![CDATA[
arXiv:2509.07006v1 Announce Type: cross 
Abstract: This paper introduces ArGen (Auto-Regulation of Generative AI systems), a framework for aligning Large Language Models (LLMs) with complex sets of configurable, machine-readable rules spanning ethical principles, operational safety protocols, and regulatory compliance standards. Moving beyond just preference-based alignment, ArGen is designed to ensure LLMs adhere to these multifaceted policies through a novel synthesis of principle-based automated reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy Agent (OPA) inspired governance layer. This approach provides the technical foundation for achieving and demonstrating compliance with diverse and nuanced governance requirements. To showcase the framework's capability to operationalize a deeply nuanced and culturally-specific value system, we present an in-depth case study: the development of a medical AI assistant guided by principles from Dharmic ethics (such as Ahimsa and Dharma), as derived from texts like the Bhagavad Gita. This challenging application demonstrates ArGen's adaptability, achieving a 70.9% improvement in domain-scope adherence over the baseline. Through our open-source repository, we show that ArGen's methodology offers a path to 'Governable Al' systems that are technically proficient, ethically robust, and verifiably compliant for safe deployment in diverse global contexts.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Concept of the Psyche</title>
<link>https://arxiv.org/abs/2509.07009</link>
<guid>https://arxiv.org/abs/2509.07009</guid>
<content:encoded><![CDATA[
arXiv:2509.07009v1 Announce Type: cross 
Abstract: The article provides an overview of approaches to modeling the human psyche in the perspective of building an artificial one. Based on the review, a concept of cognitive architecture is proposed, where the psyche is considered as an operating system of a living or artificial subject, including a space of needs that determines its life meanings in connection with stimuli from the external world, and intelligence as a decision-making system for actions in relation to this world in order to satisfy these needs. Based on the concept, a computational formalization is proposed for creating artificial intelligence systems through learning from experience in the space of a space of needs, taking into account their biological or existential significance for an intelligent agent. Thus, the problem of building general artificial intelligence as a system for making optimal decisions in the space of agent-specific needs under conditions of uncertainty is formalized, with maximization of success in achieving goals, minimization of existential risks and maximization of energy efficiency. A minimal experimental implementation of the model is also provided.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models</title>
<link>https://arxiv.org/abs/2509.07010</link>
<guid>https://arxiv.org/abs/2509.07010</guid>
<content:encoded><![CDATA[
arXiv:2509.07010v1 Announce Type: cross 
Abstract: Large Language Models are increasingly capable of interpreting multimodal inputs to generate complex 3D shapes, yet robust methods to evaluate geometric and structural fidelity remain underdeveloped. This paper introduces a human in the loop framework for the quantitative evaluation of LLM generated 3D models, supporting applications such as democratization of CAD design, reverse engineering of legacy designs, and rapid prototyping. We propose a comprehensive suite of similarity and complexity metrics, including volumetric accuracy, surface alignment, dimensional fidelity, and topological intricacy, to benchmark generated models against ground truth CAD references. Using an L bracket component as a case study, we systematically compare LLM performance across four input modalities: 2D orthographic views, isometric sketches, geometric structure trees, and code based correction prompts. Our findings demonstrate improved generation fidelity with increased semantic richness, with code level prompts achieving perfect reconstruction across all metrics. A key contribution of this work is demonstrating that our proposed quantitative evaluation approach enables significantly faster convergence toward the ground truth, especially compared to traditional qualitative methods based solely on visual inspection and human intuition. This work not only advances the understanding of AI assisted shape synthesis but also provides a scalable methodology to validate and refine generative models for diverse CAD applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV</title>
<link>https://arxiv.org/abs/2509.07016</link>
<guid>https://arxiv.org/abs/2509.07016</guid>
<content:encoded><![CDATA[
arXiv:2509.07016v1 Announce Type: cross 
Abstract: In response to the prevalent concern of TCP SYN flood attacks within the context of Software-Defined Internet of Vehicles (SD-IoV), this study addresses the significant challenge of network security in rapidly evolving vehicular communication systems. This research focuses on optimizing a Random Forest Classifier model to achieve maximum accuracy and minimal detection time, thereby enhancing vehicular network security. The methodology involves preprocessing a dataset containing SYN attack instances, employing feature scaling and label encoding techniques, and applying Stratified K-Fold cross-validation to target key metrics such as accuracy, precision, recall, and F1-score. This research achieved an average value of 0.999998 for all metrics with a SYN DoS attack detection time of 0.24 seconds. Results show that the fine-tuned Random Forest model, configured with 20 estimators and a depth of 10, effectively differentiates between normal and malicious traffic with high accuracy and minimal detection time, which is crucial for SD-IoV networks. This approach marks a significant advancement and introduces a state-of-the-art algorithm in detecting SYN flood attacks, combining high accuracy with minimal detection time. It contributes to vehicular network security by providing a robust solution against TCP SYN flood attacks while maintaining network efficiency and reliability.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An efficient deep reinforcement learning environment for flexible job-shop scheduling</title>
<link>https://arxiv.org/abs/2509.07019</link>
<guid>https://arxiv.org/abs/2509.07019</guid>
<content:encoded><![CDATA[
arXiv:2509.07019v1 Announce Type: cross 
Abstract: The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial optimization problem that has a wide-range of applications in the real world. In order to generate fast and accurate scheduling solutions for FJSP, various deep reinforcement learning (DRL) scheduling methods have been developed. However, these methods are mainly focused on the design of DRL scheduling Agent, overlooking the modeling of DRL environment. This paper presents a simple chronological DRL environment for FJSP based on discrete event simulation and an end-to-end DRL scheduling model is proposed based on the proximal policy optimization (PPO). Furthermore, a short novel state representation of FJSP is proposed based on two state variables in the scheduling environment and a novel comprehensible reward function is designed based on the scheduling area of machines. Experimental results on public benchmark instances show that the performance of simple priority dispatching rules (PDR) is improved in our scheduling environment and our DRL scheduling model obtains competing performance compared with OR-Tools, meta-heuristic, DRL and PDR scheduling methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning</title>
<link>https://arxiv.org/abs/2509.07021</link>
<guid>https://arxiv.org/abs/2509.07021</guid>
<content:encoded><![CDATA[
arXiv:2509.07021v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis technique, but its high memory consumption severely limits its applicability on edge devices. A growing number of 3DGS compression methods have been proposed to make 3DGS more efficient, yet most only focus on storage compression and fail to address the critical bottleneck of rendering memory. To address this problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that tackles this challenge by jointly optimizing two key factors: the total primitive number and the parameters per primitive, achieving unprecedented memory compression. Specifically, we replace the memory-intensive spherical harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our color representations. More importantly, we propose a unified soft pruning framework that models primitive-number and lobe-number pruning as a single constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a 50% static VRAM reduction and a 40% rendering VRAM reduction compared to existing methods, while maintaining comparable rendering quality.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preventing Another Tessa: Modular Safety Middleware For Health-Adjacent AI Assistants</title>
<link>https://arxiv.org/abs/2509.07022</link>
<guid>https://arxiv.org/abs/2509.07022</guid>
<content:encoded><![CDATA[
arXiv:2509.07022v1 Announce Type: cross 
Abstract: In 2023, the National Eating Disorders Association's (NEDA) chatbot Tessa was suspended after providing harmful weight-loss advice to vulnerable users-an avoidable failure that underscores the risks of unsafe AI in healthcare contexts. This paper examines Tessa as a case study in absent safety engineering and demonstrates how a lightweight, modular safeguard could have prevented the incident. We propose a hybrid safety middleware that combines deterministic lexical gates with an in-line large language model (LLM) policy filter, enforcing fail-closed verdicts and escalation pathways within a single model call. Using synthetic evaluations, we show that this design achieves perfect interception of unsafe prompts at baseline cost and latency, outperforming traditional multi-stage pipelines. Beyond technical remedies, we map Tessa's failure patterns to established frameworks (OWASP LLM Top10, NIST SP 800-53), connecting practical safeguards to actionable governance controls. The results highlight that robust, auditable safety in health-adjacent AI does not require heavyweight infrastructure: explicit, testable checks at the last mile are sufficient to prevent "another Tessa", while governance and escalation ensure sustainability in real-world deployment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>1 bit is all we need: binary normalized neural networks</title>
<link>https://arxiv.org/abs/2509.07025</link>
<guid>https://arxiv.org/abs/2509.07025</guid>
<content:encoded><![CDATA[
arXiv:2509.07025v1 Announce Type: cross 
Abstract: The increasing size of large neural network models, specifically language models and foundational image models, poses deployment challenges, prompting efforts to reduce memory requirements and enhance computational efficiency. These efforts are critical to ensure practical deployment and effective utilization of these models across various applications. In this work, a novel type of neural network layers and models is developed that uses only single-bit parameters. In this novel type of models all parameters of all layers, including kernel weights and biases, only have values equal to zero or one. This novel type of models uses layers named as binary normalized layer. These binary normalized layers can be of any type, such as fully connected, convolutional, attention, etc., and they consist of slight variations of the corresponding conventional layers. To show the effectiveness of the binary normalized layers, two different models are configured to solve a multiclass image classification problem and a language decoder to predict the next token of a sequence. The model to solve the image classification has convolutional and fully connected layers, and the language model is composed of transformer blocks with multi-head attention. The results show that models with binary normalized layers present almost the same results obtained by equivalent models with real 32-bit parameters. The binary normalized layers allow to develop models that use 32 times less memory than current models and have equivalent performance. Besides, the binary normalized layers can be easily implemented on current computers using 1-bit arrays, and do not require the development of dedicated electronic hardware. This novel type of layers opens a new era for large neural network models with reduced memory requirements that can be deployed using simple and cheap hardware, such as mobile devices or only cpus.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contradictions</title>
<link>https://arxiv.org/abs/2509.07026</link>
<guid>https://arxiv.org/abs/2509.07026</guid>
<content:encoded><![CDATA[
arXiv:2509.07026v1 Announce Type: cross 
Abstract: Trustworthy AI requires reasoning systems that are not only powerful but also transparent and reliable. Automated Theorem Proving (ATP) is central to formal reasoning, yet classical binary resolution remains limited, as each step involves only two clauses and eliminates at most two literals. To overcome this bottleneck, the concept of standard contradiction and the theory of contradiction-separation-based deduction were introduced in 2018. This paper advances that framework by focusing on the systematic construction of standard contradictions. Specially, this study investigates construction methods for two principal forms of standard contradiction: the maximum triangular standard contradiction and the triangular-type standard contradiction. Building on these structures, we propose a procedure for determining the satisfiability and unsatisfiability of clause sets via maximum standard contradiction. Furthermore, we derive formulas for computing the number of standard sub-contradictions embedded within both the maximum triangular standard contradiction and the triangular-type standard contradiction. The results presented herein furnish the methodological basis for advancing contradiction-separation-based dynamic multi-clause automated deduction, thereby extending the expressive and deductive capabilities of automated reasoning systems beyond the classical binary paradigm.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2509.07027</link>
<guid>https://arxiv.org/abs/2509.07027</guid>
<content:encoded><![CDATA[
arXiv:2509.07027v1 Announce Type: cross 
Abstract: We propose a novel regularization loss that enforces standard Gaussianity, encouraging samples to align with a standard Gaussian distribution. This facilitates a range of downstream tasks involving optimization in the latent space of text-to-image models. We treat elements of a high-dimensional sample as one-dimensional standard Gaussian variables and define a composite loss that combines moment-based regularization in the spatial domain with power spectrum-based regularization in the spectral domain. Since the expected values of moments and power spectrum distributions are analytically known, the loss promotes conformity to these properties. To ensure permutation invariance, the losses are applied to randomly permuted inputs. Notably, existing Gaussianity-based regularizations fall within our unified framework: some correspond to moment losses of specific orders, while the previous covariance-matching loss is equivalent to our spectral loss but incurs higher time complexity due to its spatial-domain computation. We showcase the application of our regularization in generative modeling for test-time reward alignment with a text-to-image model, specifically to enhance aesthetics and text alignment. Our regularization outperforms previous Gaussianity regularization, effectively prevents reward hacking and accelerates convergence.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Artificial Intelligence on Traditional Art Forms: A Disruption or Enhancement</title>
<link>https://arxiv.org/abs/2509.07029</link>
<guid>https://arxiv.org/abs/2509.07029</guid>
<content:encoded><![CDATA[
arXiv:2509.07029v1 Announce Type: cross 
Abstract: The introduction of Artificial Intelligence (AI) into the domains of traditional art (visual arts, performing arts, and crafts) has sparked a complicated discussion about whether this might be an agent of disruption or an enhancement of our traditional art forms. This paper looks at the duality of AI, exploring the ways that recent technologies like Generative Adversarial Networks and Diffusion Models, and text-to-image generators are changing the fields of painting, sculpture, calligraphy, dance, music, and the arts of craft. Using examples and data, we illustrate the ways that AI can democratize creative expression, improve productivity, and preserve cultural heritage, while also examining the negative aspects, including: the threats to authenticity within art, ethical concerns around data, and issues including socio-economic factors such as job losses. While we argue for the context-dependence of the impact of AI (the potential for creative homogenization and the devaluation of human agency in artmaking), we also illustrate the potential for hybrid practices featuring AI in cuisine, etc. We advocate for the development of ethical guidelines, collaborative approaches, and inclusive technology development. In sum, we are articulating a vision of AI in which it amplifies our innate creativity while resisting the displacement of the cultural, nuanced, and emotional aspects of traditional art. The future will be determined by human choices about how to govern AI so that it becomes a mechanism for artistic evolution and not a substitute for the artist's soul.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Minimalist Bayesian Framework for Stochastic Optimization</title>
<link>https://arxiv.org/abs/2509.07030</link>
<guid>https://arxiv.org/abs/2509.07030</guid>
<content:encoded><![CDATA[
arXiv:2509.07030v1 Announce Type: cross 
Abstract: The Bayesian paradigm offers principled tools for sequential decision-making under uncertainty, but its reliance on a probabilistic model for all parameters can hinder the incorporation of complex structural constraints. We introduce a minimalist Bayesian framework that places a prior only on the component of interest, such as the location of the optimum. Nuisance parameters are eliminated via profile likelihood, which naturally handles constraints. As a direct instantiation, we develop a MINimalist Thompson Sampling (MINTS) algorithm. Our framework accommodates structured problems, including continuum-armed Lipschitz bandits and dynamic pricing. It also provides a probabilistic lens on classical convex optimization algorithms such as the center of gravity and ellipsoid methods. We further analyze MINTS for multi-armed bandits and establish near-optimal regret guarantees.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Maslow-Inspired Hierarchy of Engagement with AI Model</title>
<link>https://arxiv.org/abs/2509.07032</link>
<guid>https://arxiv.org/abs/2509.07032</guid>
<content:encoded><![CDATA[
arXiv:2509.07032v1 Announce Type: cross 
Abstract: The rapid proliferation of artificial intelligence (AI) across industry, government, and education highlights the urgent need for robust frameworks to conceptualise and guide engagement. This paper introduces the Hierarchy of Engagement with AI model, a novel maturity framework inspired by Maslow's hierarchy of needs. The model conceptualises AI adoption as a progression through eight levels, beginning with initial exposure and basic understanding and culminating in ecosystem collaboration and societal impact. Each level integrates technical, organisational, and ethical dimensions, emphasising that AI maturity is not only a matter of infrastructure and capability but also of trust, governance, and responsibility. Initial validation of the model using four diverse case studies (General Motors, the Government of Estonia, the University of Texas System, and the African Union AI Strategy) demonstrate the model's contextual flexibility across various sectors. The model provides scholars with a framework for analysing AI maturity and offers practitioners and policymakers a diagnostic and strategic planning tool to guide responsible and sustainable AI engagement. The proposed model demonstrates that AI maturity progression is multi-dimensional, requiring technological capability, ethical integrity, organisational resilience, and ecosystem collaboration.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators</title>
<link>https://arxiv.org/abs/2509.07036</link>
<guid>https://arxiv.org/abs/2509.07036</guid>
<content:encoded><![CDATA[
arXiv:2509.07036v1 Announce Type: cross 
Abstract: This paper presents a methodological approach to financial time series analysis by combining causal discovery and uncertainty-aware forecasting. As a case study, we focus on four key U.S. macroeconomic indicators -- GDP, economic growth, inflation, and unemployment -- and we apply the LPCMCI framework with Gaussian Process Distance Correlation (GPDC) to uncover dynamic causal relationships in quarterly data from 1970 to 2021. Our results reveal a robust unidirectional causal link from economic growth to GDP and highlight the limited connectivity of inflation, suggesting the influence of latent factors. Unemployment exhibits strong autoregressive dependence, motivating its use as a case study for probabilistic forecasting. Leveraging the Chronos framework, a large language model trained for time series, we perform zero-shot predictions on unemployment. This approach delivers accurate forecasts one and two quarters ahead, without requiring task-specific training. Crucially, the model's uncertainty-aware predictions yield 90\% confidence intervals, enabling effective anomaly detection through statistically principled deviation analysis. This study demonstrates the value of combining causal structure learning with probabilistic language models to inform economic policy and enhance forecasting robustness.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Singing Voice Synthesis using Phoneme-Level Energy Sequence</title>
<link>https://arxiv.org/abs/2509.07038</link>
<guid>https://arxiv.org/abs/2509.07038</guid>
<content:encoded><![CDATA[
arXiv:2509.07038v1 Announce Type: cross 
Abstract: Controllable Singing Voice Synthesis (SVS) aims to generate expressive singing voices reflecting user intent. While recent SVS systems achieve high audio quality, most rely on probabilistic modeling, limiting precise control over attributes such as dynamics. We address this by focusing on dynamic control--temporal loudness variation essential for musical expressiveness--and explicitly condition the SVS model on energy sequences extracted from ground-truth spectrograms, reducing annotation costs and improving controllability. We also propose a phoneme-level energy sequence for user-friendly control. To the best of our knowledge, this is the first attempt enabling user-driven dynamics control in SVS. Experiments show our method achieves over 50% reduction in mean absolute error of energy sequences for phoneme-level inputs compared to baseline and energy-predictor models, without compromising synthesis quality.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Evaluation of Gender Bias Across 13 Large Multimodal Models</title>
<link>https://arxiv.org/abs/2509.07050</link>
<guid>https://arxiv.org/abs/2509.07050</guid>
<content:encoded><![CDATA[
arXiv:2509.07050v1 Announce Type: cross 
Abstract: Large multimodal models (LMMs) have revolutionized text-to-image generation, but they risk perpetuating the harmful social biases in their training data. Prior work has identified gender bias in these models, but methodological limitations prevented large-scale, comparable, cross-model analysis. To address this gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for assessing social bias in AI-generated images. We test 13 commercially available LMMs using 75 procedurally-generated, gender-neutral prompts to generate people in stereotypically-male, stereotypically-female, and non-stereotypical professions. We then use a validated LLM-as-a-judge system to score the 965 resulting images for gender representation. Our results reveal (p < .001 for all): 1) LMMs systematically not only reproduce but actually amplify occupational gender stereotypes relative to real-world labor data, generating men in 93.0% of images for male-stereotyped professions but only 22.5% for female-stereotyped professions; 2) Models exhibit a strong default-male bias, generating men in 68.3% of the time for non-stereotyped professions; and 3) The extent of bias varies dramatically across models, with overall male representation ranging from 46.7% to 73.3%. Notably, the top-performing model de-amplified gender stereotypes and approached gender parity, achieving the highest fairness scores. This variation suggests high bias is not an inevitable outcome but a consequence of design choices. Our work provides the most comprehensive cross-model benchmark of gender bias to date and underscores the necessity of standardized, automated evaluation tools for promoting accountability and fairness in AI development.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lookup multivariate Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2509.07103</link>
<guid>https://arxiv.org/abs/2509.07103</guid>
<content:encoded><![CDATA[
arXiv:2509.07103v1 Announce Type: cross 
Abstract: High-dimensional linear mappings, or linear layers, dominate both the parameter count and the computational cost of most modern deep-learning models. We introduce a general drop-in replacement, lookup multivariate Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better trade-off between capacity and inference cost. Our construction expresses a general high-dimensional mapping through trainable low-dimensional multivariate functions. These functions can carry dozens or hundreds of trainable parameters each, and yet it takes only a few multiplications to compute them because they are implemented as spline lookup tables. Empirically, lmKANs reduce inference FLOPs by up to 6.0x while matching the flexibility of MLPs in general high-dimensional function approximation. In another feedforward fully connected benchmark, on the tabular-like dataset of randomly displaced methane configurations, lmKANs enable more than 10x higher H100 throughput at equal accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10 and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA kernels, is available online at https://github.com/schwallergroup/lmkan.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Batch Normalization: A Gyro Approach</title>
<link>https://arxiv.org/abs/2509.07115</link>
<guid>https://arxiv.org/abs/2509.07115</guid>
<content:encoded><![CDATA[
arXiv:2509.07115v1 Announce Type: cross 
Abstract: Normalization layers are crucial for deep learning, but their Euclidean formulations are inadequate for data on manifolds. On the other hand, many Riemannian manifolds in machine learning admit gyro-structures, enabling principled extensions of Euclidean neural networks to non-Euclidean domains. Inspired by this, we introduce GyroBN, a principled Riemannian batch normalization framework for gyrogroups. We establish two necessary conditions, namely \emph{pseudo-reduction} and \emph{gyroisometric gyrations}, that guarantee GyroBN with theoretical control over sample statistics, and show that these conditions hold for all known gyrogroups in machine learning. Our framework also incorporates several existing Riemannian normalization methods as special cases. We further instantiate GyroBN on seven representative geometries, including the Grassmannian, five constant curvature spaces, and the correlation manifold, and derive novel gyro and Riemannian structures to enable these instantiations. Experiments across these geometries demonstrate the effectiveness of GyroBN. The code is available at https://github.com/GitZH-Chen/GyroBN.git.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVGauge: Towards Human-Aligned Evaluation for SVG Generation</title>
<link>https://arxiv.org/abs/2509.07127</link>
<guid>https://arxiv.org/abs/2509.07127</guid>
<content:encoded><![CDATA[
arXiv:2509.07127v1 Announce Type: cross 
Abstract: Generated Scalable Vector Graphics (SVG) images demand evaluation criteria tuned to their symbolic and vectorial nature: criteria that existing metrics such as FID, LPIPS, or CLIPScore fail to satisfy. In this paper, we introduce SVGauge, the first human-aligned, reference based metric for text-to-SVG generation. SVGauge jointly measures (i) visual fidelity, obtained by extracting SigLIP image embeddings and refining them with PCA and whitening for domain alignment, and (ii) semantic consistency, captured by comparing BLIP-2-generated captions of the SVGs against the original prompts in the combined space of SBERT and TF-IDF. Evaluation on the proposed SHE benchmark shows that SVGauge attains the highest correlation with human judgments and reproduces system-level rankings of eight zero-shot LLM-based generators more faithfully than existing metrics. Our results highlight the necessity of vector-specific evaluation and provide a practical tool for benchmarking future text-to-SVG generation models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Security and Privacy of AI Agents for Blockchain</title>
<link>https://arxiv.org/abs/2509.07131</link>
<guid>https://arxiv.org/abs/2509.07131</guid>
<content:encoded><![CDATA[
arXiv:2509.07131v1 Announce Type: cross 
Abstract: Blockchain and smart contracts have garnered significant interest in recent years as the foundation of a decentralized, trustless digital ecosystem, thereby eliminating the need for traditional centralized authorities. Despite their central role in powering Web3, their complexity still presents significant barriers for non-expert users. To bridge this gap, Artificial Intelligence (AI)-based agents have emerged as valuable tools for interacting with blockchain environments, supporting a range of tasks, from analyzing on-chain data and optimizing transaction strategies to detecting vulnerabilities within smart contracts. While interest in applying AI to blockchain is growing, the literature still lacks a comprehensive survey that focuses specifically on the intersection with AI agents. Most of the related work only provides general considerations, without focusing on any specific domain. This paper addresses this gap by presenting the first Systematization of Knowledge dedicated to AI-driven systems for blockchain, with a special focus on their security and privacy dimensions, shedding light on their applications, limitations, and future research directions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative Study</title>
<link>https://arxiv.org/abs/2509.07132</link>
<guid>https://arxiv.org/abs/2509.07132</guid>
<content:encoded><![CDATA[
arXiv:2509.07132v1 Announce Type: cross 
Abstract: The widespread use of generative AI has shown remarkable success in producing highly realistic deepfakes, posing a serious threat to various voice biometric applications, including speaker verification, voice biometrics, audio conferencing, and criminal investigations. To counteract this, several state-of-the-art (SoTA) audio deepfake detection (ADD) methods have been proposed to identify generative AI signatures to distinguish between real and deepfake audio. However, the effectiveness of these methods is severely undermined by anti-forensic (AF) attacks that conceal generative signatures. These AF attacks span a wide range of techniques, including statistical modifications (e.g., pitch shifting, filtering, noise addition, and quantization) and optimization-based attacks (e.g., FGSM, PGD, C \& W, and DeepFool). In this paper, we investigate the SoTA ADD methods and provide a comparative analysis to highlight their effectiveness in exposing deepfake signatures, as well as their vulnerabilities under adversarial conditions. We conducted an extensive evaluation of ADD methods on five deepfake benchmark datasets using two categories: raw and spectrogram-based approaches. This comparative analysis enables a deeper understanding of the strengths and limitations of SoTA ADD methods against diverse AF attacks. It does not only highlight vulnerabilities of ADD methods, but also informs the design of more robust and generalized detectors for real-world voice biometrics. It will further guide future research in developing adaptive defense strategies that can effectively counter evolving AF techniques.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models</title>
<link>https://arxiv.org/abs/2509.07142</link>
<guid>https://arxiv.org/abs/2509.07142</guid>
<content:encoded><![CDATA[
arXiv:2509.07142v1 Announce Type: cross 
Abstract: This study presents a framework for automated evaluation of dynamically evolving topic models using Large Language Models (LLMs). Topic modeling is essential for organizing and retrieving scholarly content in digital library systems, helping users navigate complex and evolving knowledge domains. However, widely used automated metrics, such as coherence and diversity, often capture only narrow statistical patterns and fail to explain semantic failures in practice. We introduce a purpose-oriented evaluation framework that employs nine LLM-based metrics spanning four key dimensions of topic quality: lexical validity, intra-topic semantic soundness, inter-topic structural soundness, and document-topic alignment soundness. The framework is validated through adversarial and sampling-based protocols, and is applied across datasets spanning news articles, scholarly publications, and social media posts, as well as multiple topic modeling methods and open-source LLMs. Our analysis shows that LLM-based metrics provide interpretable, robust, and task-relevant assessments, uncovering critical weaknesses in topic models such as redundancy and semantic drift, which are often missed by traditional metrics. These results support the development of scalable, fine-grained evaluation tools for maintaining topic relevance in dynamic datasets. All code and data supporting this work are accessible at https://github.com/zhiyintan/topic-model-LLMjudgment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Uncertainty in Transformer Circuits with Effective Information Consistency</title>
<link>https://arxiv.org/abs/2509.07149</link>
<guid>https://arxiv.org/abs/2509.07149</guid>
<content:encoded><![CDATA[
arXiv:2509.07149v1 Announce Type: cross 
Abstract: Mechanistic interpretability has identified functional subgraphs within large language models (LLMs), known as Transformer Circuits (TCs), that appear to implement specific algorithms. Yet we lack a formal, single-pass way to quantify when an active circuit is behaving coherently and thus likely trustworthy. Building on prior systems-theoretic proposals, we specialize a sheaf/cohomology and causal emergence perspective to TCs and introduce the Effective-Information Consistency Score (EICS). EICS combines (i) a normalized sheaf inconsistency computed from local Jacobians and activations, with (ii) a Gaussian EI proxy for circuit-level causal emergence derived from the same forward state. The construction is white-box, single-pass, and makes units explicit so that the score is dimensionless. We further provide practical guidance on score interpretation, computational overhead (with fast and exact modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is deferred.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge</title>
<link>https://arxiv.org/abs/2509.07188</link>
<guid>https://arxiv.org/abs/2509.07188</guid>
<content:encoded><![CDATA[
arXiv:2509.07188v1 Announce Type: cross 
Abstract: Discharge communication is a critical yet underexplored component of patient care, where the goal shifts from diagnosis to education. While recent large language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they fail to evaluate models' ability to support patients after the visit. We introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability to act as personalized discharge educators. DischargeSim simulates post-visit, multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with diverse psychosocial profiles (e.g., health literacy, education, emotion). Interactions are structured across six clinically grounded discharge topics and assessed along three axes: (1) dialogue quality via automatic and LLM-as-judge evaluation, (2) personalized document generation including free-text summaries and structured AHRQ checklists, and (3) patient comprehension through a downstream multiple-choice exam. Experiments across 18 LLMs reveal significant gaps in discharge education capability, with performance varying widely across patient profiles. Notably, model size does not always yield better education outcomes, highlighting trade-offs in strategy use and content prioritization. DischargeSim offers a first step toward benchmarking LLMs in post-visit clinical education and promoting equitable, personalized patient support.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Machine Learning Reconstruction Techniques for Accelerated Brain MRI Scans</title>
<link>https://arxiv.org/abs/2509.07193</link>
<guid>https://arxiv.org/abs/2509.07193</guid>
<content:encoded><![CDATA[
arXiv:2509.07193v1 Announce Type: cross 
Abstract: This retrospective-prospective study evaluated whether a deep learning-based MRI reconstruction algorithm can preserve diagnostic quality in brain MRI scans accelerated up to fourfold, using both public and prospective clinical data. The study included 18 healthy volunteers (scans acquired at 3T, January 2024-March 2025), as well as selected fastMRI public datasets with diverse pathologies. Phase-encoding-undersampled 2D/3D T1, T2, and FLAIR sequences were reconstructed with DeepFoqus-Accelerate and compared with standard-of-care (SOC). Three board-certified neuroradiologists and two MRI technologists independently reviewed 36 paired SOC/AI reconstructions from both datasets using a 5-point Likert scale, while quantitative similarity was assessed for 408 scans and 1224 datasets using Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Haar wavelet-based Perceptual Similarity Index (HaarPSI). No AI-reconstructed scan scored below 3 (minimally acceptable), and 95% scored $\geq 4$. Mean SSIM was 0.95 $\pm$ 0.03 (90% cases >0.90), PSNR >41.0 dB, and HaarPSI >0.94. Inter-rater agreement was slight to moderate. Rare artifacts did not affect diagnostic interpretation. These findings demonstrate that DeepFoqus-Accelerate enables robust fourfold brain MRI acceleration with 75% reduced scan time, while preserving diagnostic image quality and supporting improved workflow efficiency.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-strategy improved gazelle optimization algorithm for solving numerical optimization and engineering applications</title>
<link>https://arxiv.org/abs/2509.07211</link>
<guid>https://arxiv.org/abs/2509.07211</guid>
<content:encoded><![CDATA[
arXiv:2509.07211v1 Announce Type: cross 
Abstract: Aiming at the shortcomings of the gazelle optimization algorithm, such as the imbalance between exploration and exploitation and the insufficient information exchange within the population, this paper proposes a multi-strategy improved gazelle optimization algorithm (MSIGOA). To address these issues, MSIGOA proposes an iteration-based updating framework that switches between exploitation and exploration according to the optimization process, which effectively enhances the balance between local exploitation and global exploration in the optimization process and improves the convergence speed. Two adaptive parameter tuning strategies improve the applicability of the algorithm and promote a smoother optimization process. The dominant population-based restart strategy enhances the algorithms ability to escape from local optima and avoid its premature convergence. These enhancements significantly improve the exploration and exploitation capabilities of MSIGOA, bringing superior convergence and efficiency in dealing with complex problems. In this paper, the parameter sensitivity, strategy effectiveness, convergence and stability of the proposed method are evaluated on two benchmark test sets including CEC2017 and CEC2022. Test results and statistical tests show that MSIGOA outperforms basic GOA and other advanced algorithms. On the CEC2017 and CEC2022 test sets, the proportion of functions where MSIGOA is not worse than GOA is 92.2% and 83.3%, respectively, and the proportion of functions where MSIGOA is not worse than other algorithms is 88.57% and 87.5%, respectively. Finally, the extensibility of MSIGAO is further verified by several engineering design optimization problems.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning</title>
<link>https://arxiv.org/abs/2509.07213</link>
<guid>https://arxiv.org/abs/2509.07213</guid>
<content:encoded><![CDATA[
arXiv:2509.07213v1 Announce Type: cross 
Abstract: Background: Precise breast ultrasound (BUS) segmentation supports reliable measurement, quantitative analysis, and downstream classification, yet remains difficult for small or low-contrast lesions with fuzzy margins and speckle noise. Text prompts can add clinical context, but directly applying weakly localized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce coarse, blob-like responses that smear boundaries unless additional mechanisms recover fine edges. Methods: We propose XBusNet, a novel dual-prompt, dual-branch multimodal model that combines image features with clinically grounded text. A global pathway based on a CLIP Vision Transformer encodes whole-image semantics conditioned on lesion size and location, while a local U-Net pathway emphasizes precise boundaries and is modulated by prompts that describe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS) terms. Prompts are assembled automatically from structured metadata, requiring no manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using five-fold cross-validation. Primary metrics are Dice and Intersection over Union (IoU); we also conduct size-stratified analyses and ablations to assess the roles of the global and local paths and the text-driven modulation. Results: XBusNet achieves state-of-the-art performance on BLU, with mean Dice of 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions show the largest gains, with fewer missed regions and fewer spurious activations. Ablation studies show complementary contributions of global context, local boundary modeling, and prompt-based modulation. Conclusions: A dual-prompt, dual-branch multimodal design that merges global semantics with local precision yields accurate BUS segmentation masks and improves robustness for small, low-contrast lesions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining How Quantization Disparately Skews a Model</title>
<link>https://arxiv.org/abs/2509.07222</link>
<guid>https://arxiv.org/abs/2509.07222</guid>
<content:encoded><![CDATA[
arXiv:2509.07222v1 Announce Type: cross 
Abstract: Post Training Quantization (PTQ) is widely adopted due to its high compression capacity and speed with minimal impact on accuracy. However, we observed that disparate impacts are exacerbated by quantization, especially for minority groups. Our analysis explains that in the course of quantization there is a chain of factors attributed to a disparate impact across groups during forward and backward passes. We explore how the changes in weights and activations induced by quantization cause cascaded impacts in the network, resulting in logits with lower variance, increased loss, and compromised group accuracies. We extend our study to verify the influence of these impacts on group gradient norms and eigenvalues of the Hessian matrix, providing insights into the state of the network from an optimization point of view. To mitigate these effects, we propose integrating mixed precision Quantization Aware Training (QAT) with dataset sampling methods and weighted loss functions, therefore providing fair deployment of quantized neural networks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A transformer-based generative model for planetary systems</title>
<link>https://arxiv.org/abs/2509.07226</link>
<guid>https://arxiv.org/abs/2509.07226</guid>
<content:encoded><![CDATA[
arXiv:2509.07226v1 Announce Type: cross 
Abstract: Numerical calculations of planetary system formation are very demanding in terms of computing power. These synthetic planetary systems can however provide access to correlations, as predicted in a given numerical framework, between the properties of planets in the same system. Such correlations can, in return, be used in order to guide and prioritize observational campaigns aiming at discovering some types of planets, as Earth-like planets. Our goal is to develop a generative model which is capable of capturing correlations and statistical relationships between planets in the same system. Such a model, trained on the Bern model, offers the possibility to generate large number of synthetic planetary systems with little computational cost, that can be used, for example, to guide observational campaigns. Our generative model is based on the transformer architecture which is well-known to efficiently capture correlations in sequences and is at the basis of all modern Large Language Models. To assess the validity of the generative model, we perform visual and statistical comparisons, as well as a machine learning driven tests. Finally, as a use case example, we consider the TOI-469 system, in which we aim at predicting the possible properties of planets c and d, based on the properties of planet b (the first that has been detected). We show using different comparison methods that the properties of systems generated by our model are very similar to the ones of the systems computed directly by the Bern model. We also show in the case of the TOI-469 system, that using the generative model allows to predict the properties of planets not yet observed, based on the properties of the already observed planet. We provide our model to the community on our website www.ai4exoplanets.com.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Conventional Forward-Backward Tie in Neural Networks: Activation Functions</title>
<link>https://arxiv.org/abs/2509.07236</link>
<guid>https://arxiv.org/abs/2509.07236</guid>
<content:encoded><![CDATA[
arXiv:2509.07236v1 Announce Type: cross 
Abstract: Gradient-based neural network training traditionally enforces symmetry between forward and backward propagation, requiring activation functions to be differentiable (or sub-differentiable) and strictly monotonic in certain regions to prevent flat gradient areas. This symmetry, linking forward activations closely to backward gradients, significantly restricts the selection of activation functions, particularly excluding those with substantial flat or non-differentiable regions. In this paper, we challenge this assumption through mathematical analysis, demonstrating that precise gradient magnitudes derived from activation functions are largely redundant, provided the gradient direction is preserved. Empirical experiments conducted on foundational architectures - such as Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Binary Neural Networks (BNNs) - confirm that relaxing forward-backward symmetry and substituting traditional gradients with simpler or stochastic alternatives does not impair learning and may even enhance training stability and efficiency. We explicitly demonstrate that neural networks with flat or non-differentiable activation functions, such as the Heaviside step function, can be effectively trained, thereby expanding design flexibility and computational efficiency. Further empirical validation with more complex architectures remains a valuable direction for future research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2509.07238</link>
<guid>https://arxiv.org/abs/2509.07238</guid>
<content:encoded><![CDATA[
arXiv:2509.07238v1 Announce Type: cross 
Abstract: This paper presents a practical investigation into fine-tuning model parameters for mathematical reasoning tasks through experimenting with various configurations including randomness control, reasoning depth, and sampling strategies, careful tuning demonstrates substantial improvements in efficiency as well as performance. A holistically optimized framework is introduced for five state-of-the-art models on mathematical reasoning tasks, exhibiting significant performance boosts while maintaining solution correctness. Through systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B, DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are demonstrated with 100% optimization success rate. The methodology achieves an average 29.4% reduction in computational cost and 23.9% improvement in inference speed across all tested models. This framework systematically searches parameter spaces including temperature (0.1-0.5), reasoning steps (4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining optimal configurations through testing on mathematical reasoning benchmarks. Critical findings show that lower temperature regimes (0.1-0.4) and reduced reasoning steps (4-6) consistently enhance efficiency without compromising accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B delivers the most cost-effective performance at 361.5 tokens per accurate response. Key contributions include: (1) the first comprehensive optimization study for five diverse SOTA models in mathematical reasoning, (2) a standardized production-oriented parameter optimization framework, (3) discovery of universal optimization trends applicable across model architectures, and (4) production-ready configurations with extensive performance characterization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Information Retrieval Models on Complex Retrieval Tasks</title>
<link>https://arxiv.org/abs/2509.07253</link>
<guid>https://arxiv.org/abs/2509.07253</guid>
<content:encoded><![CDATA[
arXiv:2509.07253v1 Announce Type: cross 
Abstract: Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Datasets for Navigating Sensitive Topics in Recommendation Systems</title>
<link>https://arxiv.org/abs/2509.07269</link>
<guid>https://arxiv.org/abs/2509.07269</guid>
<content:encoded><![CDATA[
arXiv:2509.07269v1 Announce Type: cross 
Abstract: Personalized AI systems, from recommendation systems to chatbots, are a prevalent method for distributing content to users based on their learned preferences. However, there is growing concern about the adverse effects of these systems, including their potential tendency to expose users to sensitive or harmful material, negatively impacting overall well-being. To address this concern quantitatively, it is necessary to create datasets with relevant sensitivity labels for content, enabling researchers to evaluate personalized systems beyond mere engagement metrics. To this end, we introduce two novel datasets that include a taxonomy of sensitivity labels alongside user-content ratings: one that integrates MovieLens rating data with content warnings from the Does the Dog Die? community ratings website, and another that combines fan-fiction interaction data and user-generated warnings from Archive of Our Own.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion</title>
<link>https://arxiv.org/abs/2509.07277</link>
<guid>https://arxiv.org/abs/2509.07277</guid>
<content:encoded><![CDATA[
arXiv:2509.07277v1 Announce Type: cross 
Abstract: Data scarcity hinders deep learning for medical imaging. We propose a framework for breast cancer classification in thermograms that addresses this using a Diffusion Probabilistic Model (DPM) for data augmentation. Our DPM-based augmentation is shown to be superior to both traditional methods and a ProGAN baseline. The framework fuses deep features from a pre-trained ResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived from U-Net segmented tumors. An XGBoost classifier trained on these fused features achieves 98.0\% accuracy and 98.1\% sensitivity. Ablation studies and statistical tests confirm that both the DPM augmentation and the nonlinear feature fusion are critical, statistically significant components of this success. This work validates the synergy between advanced generative models and interpretable features for creating highly accurate medical diagnostic tools.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers</title>
<link>https://arxiv.org/abs/2509.07282</link>
<guid>https://arxiv.org/abs/2509.07282</guid>
<content:encoded><![CDATA[
arXiv:2509.07282v1 Announce Type: cross 
Abstract: We present cryptogram solving as an ideal testbed for studying neural network generalization in combinatorially complex domains. In this task, models must decrypt text encoded with substitution ciphers, choosing from 26! possible mappings without explicit access to the cipher. We develop ALICE (an Architecture for Learning Interpretable Cryptogram dEcipherment): a simple encoder-only Transformer that sets a new state-of-the-art for both accuracy and speed on this decryption problem. Surprisingly, ALICE generalizes to unseen ciphers after training on only ${\sim}1500$ unique ciphers, a minute fraction ($3.7 \times 10^{-24}$) of the possible cipher space. To enhance interpretability, we introduce a novel bijective decoding head that explicitly models permutations via the Gumbel-Sinkhorn method, enabling direct extraction of learned cipher mappings. Through early exit analysis, we reveal how ALICE progressively refines its predictions in a way that appears to mirror common human strategies for this task: early layers employ frequency-based heuristics, middle layers form word structures, and final layers correct individual characters. Our architectural innovations and analysis methods extend beyond cryptograms to any domain with bijective mappings and combinatorial structure, offering new insights into neural network generalization and interpretability.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm</title>
<link>https://arxiv.org/abs/2509.07287</link>
<guid>https://arxiv.org/abs/2509.07287</guid>
<content:encoded><![CDATA[
arXiv:2509.07287v1 Announce Type: cross 
Abstract: With the rapid development of large language models, the potential threat of their malicious use, particularly in generating phishing content, is becoming increasingly prevalent. Leveraging the capabilities of LLMs, malicious users can synthesize phishing emails that are free from spelling mistakes and other easily detectable features. Furthermore, such models can generate topic-specific phishing messages, tailoring content to the target domain and increasing the likelihood of success.
  Detecting such content remains a significant challenge, as LLM-generated phishing emails often lack clear or distinguishable linguistic features. As a result, most existing semantic-level detection approaches struggle to identify them reliably. While certain LLM-based detection methods have shown promise, they suffer from high computational costs and are constrained by the performance of the underlying language model, making them impractical for large-scale deployment.
  In this work, we aim to address this issue. We propose Paladin, which embeds trigger-tag associations into vanilla LLM using various insertion strategies, creating them into instrumented LLMs. When an instrumented LLM generates content related to phishing, it will automatically include detectable tags, enabling easier identification. Based on the design on implicit and explicit triggers and tags, we consider four distinct scenarios in our work. We evaluate our method from three key perspectives: stealthiness, effectiveness, and robustness, and compare it with existing baseline methods. Experimental results show that our method outperforms the baselines, achieving over 90% detection accuracy across all scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>zkUnlearner: A Zero-Knowledge Framework for Verifiable Unlearning with Multi-Granularity and Forgery-Resistance</title>
<link>https://arxiv.org/abs/2509.07290</link>
<guid>https://arxiv.org/abs/2509.07290</guid>
<content:encoded><![CDATA[
arXiv:2509.07290v1 Announce Type: cross 
Abstract: As the demand for exercising the "right to be forgotten" grows, the need for verifiable machine unlearning has become increasingly evident to ensure both transparency and accountability. We present {\em zkUnlearner}, the first zero-knowledge framework for verifiable machine unlearning, specifically designed to support {\em multi-granularity} and {\em forgery-resistance}.
  First, we propose a general computational model that employs a {\em bit-masking} technique to enable the {\em selectivity} of existing zero-knowledge proofs of training for gradient descent algorithms. This innovation enables not only traditional {\em sample-level} unlearning but also more advanced {\em feature-level} and {\em class-level} unlearning. Our model can be translated to arithmetic circuits, ensuring compatibility with a broad range of zero-knowledge proof systems. Furthermore, our approach overcomes key limitations of existing methods in both efficiency and privacy. Second, forging attacks present a serious threat to the reliability of unlearning. Specifically, in Stochastic Gradient Descent optimization, gradients from unlearned data, or from minibatches containing it, can be forged using alternative data samples or minibatches that exclude it. We propose the first effective strategies to resist state-of-the-art forging attacks. Finally, we benchmark a zkSNARK-based instantiation of our framework and perform comprehensive performance evaluations to validate its practicality.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction Alignment Improves Unified Multimodal Models</title>
<link>https://arxiv.org/abs/2509.07295</link>
<guid>https://arxiv.org/abs/2509.07295</guid>
<content:encoded><![CDATA[
arXiv:2509.07295v1 Announce Type: cross 
Abstract: Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit 6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Basis Vector Metric: A Method for Robust Open-Ended State Change Detection</title>
<link>https://arxiv.org/abs/2509.07308</link>
<guid>https://arxiv.org/abs/2509.07308</guid>
<content:encoded><![CDATA[
arXiv:2509.07308v1 Announce Type: cross 
Abstract: We test a new method, which we will abbreviate using the acronym BVM (Basis Vectors Method), in its ability to judge the state changes in images through using language embeddings. We used the MIT-States dataset, containing about 53,000 images, to gather all of our data, which has 225 nouns and 115 adjectives, with each noun having about 9 different adjectives, forming approximately 1000 noun-adjective pairs. For our first experiment, we test our method's ability to determine the state of each noun class separately against other metrics for comparison. These metrics are cosine similarity, dot product, product quantization, binary index, Naive Bayes, and a custom neural network. Among these metrics, we found that our proposed BVM performs the best in classifying the states for each noun. We then perform a second experiment where we try using BVM to determine if it can differentiate adjectives from one another for each adjective separately. We compared the abilities of BVM to differentiate adjectives against the proposed method the MIT-States paper suggests: using a logistic regression model. In the end, we did not find conclusive evidence that our BVM metric could perform better than the logistic regression model at discerning adjectives. Yet, we were able to find evidence for possible improvements to our method; this leads to the chance of increasing our method's accuracy through certain changes in our methodologies.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations</title>
<link>https://arxiv.org/abs/2509.07311</link>
<guid>https://arxiv.org/abs/2509.07311</guid>
<content:encoded><![CDATA[
arXiv:2509.07311v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have been driven by pretraining, supervised fine tuning (SFT), and alignment tuning. Among these, SFT plays a crucial role in transforming a model 's general knowledge into structured responses tailored to specific tasks. However, there is no clearly established methodology for effective training data selection. Simply increasing the volume of data does not guarantee performance improvements, while preprocessing, sampling, and validation require substantial time and cost.
  To address this issue, a variety of data selection methods have been proposed. Among them, knowledge based selection approaches identify suitable training data by analyzing the model 's responses. Nevertheless, these methods typically rely on prompt engineering, making them sensitive to variations and incurring additional costs for prompt design.
  In this study, we propose Knowledge Analysis via Model Internal Representations (KAMIR), a novel approach that overcomes these limitations by analyzing data based on the model 's internal representations. KAMIR computes similarities between the hidden states of each layer (block) and the final hidden states for a given input to assess the data. Unlike prior methods that were largely limited to multiple choice tasks, KAMIR can be applied to a wide range of tasks such as machine reading comprehension and summarization. Moreover, it selects data useful for training based on the model 's familiarity with the input, even with a small dataset and a simple classifier architecture. Experiments across diverse task datasets demonstrate that training with less familiar data leads to better generalization performance.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEGG: Replay via Maximally Extreme GGscore in Incremental Learning for Neural Recommendation Models</title>
<link>https://arxiv.org/abs/2509.07319</link>
<guid>https://arxiv.org/abs/2509.07319</guid>
<content:encoded><![CDATA[
arXiv:2509.07319v1 Announce Type: cross 
Abstract: Neural Collaborative Filtering models are widely used in recommender systems but are typically trained under static settings, assuming fixed data distributions. This limits their applicability in dynamic environments where user preferences evolve. Incremental learning offers a promising solution, yet conventional methods from computer vision or NLP face challenges in recommendation tasks due to data sparsity and distinct task paradigms. Existing approaches for neural recommenders remain limited and often lack generalizability. To address this, we propose MEGG, Replay Samples with Maximally Extreme GGscore, an experience replay based incremental learning framework. MEGG introduces GGscore, a novel metric that quantifies sample influence, enabling the selective replay of highly influential samples to mitigate catastrophic forgetting. Being model-agnostic, MEGG integrates seamlessly across architectures and frameworks. Experiments on three neural models and four benchmark datasets show superior performance over state-of-the-art baselines, with strong scalability, efficiency, and robustness. Implementation will be released publicly upon acceptance.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation</title>
<link>https://arxiv.org/abs/2509.07324</link>
<guid>https://arxiv.org/abs/2509.07324</guid>
<content:encoded><![CDATA[
arXiv:2509.07324v1 Announce Type: cross 
Abstract: Transformer-based self-attention mechanism serves as the core of modern language models, yet it often suffers from localization, where attentions collapse onto a limited subset of tokens and fail to capture long-range dependencies. To address this issue, we propose Self-Attention One-step Belief Propagation (SAOBP), a refinement framework that injects multi-hop relationships through a belief propagation process. To interpret and quantify these interactions, we introduce Global Token Dependency (GTD) that captures the relative contribution of multihop connections within the attention graph. Empirical results indicate that SAOBP helps prevent entropy collapse in deeper layers and adaptively maintains GTD at task-appropriate levels, thereby supporting improvements in model performance. Importantly, we observe competitive gains in small-scale models, highlighting its potential for improving inference quality in resource-constrained scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion</title>
<link>https://arxiv.org/abs/2509.07327</link>
<guid>https://arxiv.org/abs/2509.07327</guid>
<content:encoded><![CDATA[
arXiv:2509.07327v1 Announce Type: cross 
Abstract: Multispectral remote sensing object detection is one of the important application of unmanned aerial vehicle (UAV). However, it faces three challenges. Firstly, the low-light remote sensing images reduce the complementarity during multi-modality fusion. Secondly, the local small target modeling is interfered with redundant information in the fusion stage easily. Thirdly, due to the quadratic computational complexity, it is hard to apply the transformer-based methods on the UAV platform. To address these limitations, motivated by Mamba with linear complexity, a UAV multispectral object detector with dual-domain enhancement and priority-guided mamba fusion (DEPF) is proposed. Firstly, to enhance low-light remote sensing images, Dual-Domain Enhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba (CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba scanning for the low-frequency components to enhance the global brightness of images, while FDR constructs spectrum recovery network to enhance the frequency spectra features for recovering the texture-details. Secondly, to enhance local target modeling and reduce the impact of redundant information during fusion, Priority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the concept of priority scanning, which starts from local targets features according to the priority scores obtained from modality difference. Experiments on DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on object detection, comparing with state-of-the-art methods. Our code is available in the supplementary material.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases</title>
<link>https://arxiv.org/abs/2509.07330</link>
<guid>https://arxiv.org/abs/2509.07330</guid>
<content:encoded><![CDATA[
arXiv:2509.07330v1 Announce Type: cross 
Abstract: Demographic attributes are universally present in electronic health records and serve as vital predictors in clinical risk stratification and treatment decisions. Despite their significance, these attributes are often relegated to auxiliary roles in model design, with limited attention has been given to learning their representations. This study proposes a General Demographic Pre-trained (GDP) model as a foundational representation framework tailored to age and gender. The model is pre-trained and evaluated using datasets with diverse diseases and population compositions from different geographic regions. The GDP architecture explores combinations of ordering strategies and encoding methods to transform tabular demographic inputs into latent embeddings. Experimental results demonstrate that sequential ordering substantially improves model performance in discrimination, calibration, and the corresponding information gain at each decision tree split, particularly in diseases where age and gender contribute significantly to risk stratification. Even in datasets where demographic attributes hold relatively low predictive value, GDP enhances the representational importance, increasing their influence in downstream gradient boosting models. The findings suggest that foundational models for tabular demographic attributes can generalize across tasks and populations, offering a promising direction for improving predictive performance in healthcare applications.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word2Spike: Poisson Rate Coding for Associative Memories and Neuromorphic Algorithms</title>
<link>https://arxiv.org/abs/2509.07361</link>
<guid>https://arxiv.org/abs/2509.07361</guid>
<content:encoded><![CDATA[
arXiv:2509.07361v1 Announce Type: cross 
Abstract: Spiking neural networks offer a promising path toward energy-efficient, brain-like associative memory. This paper introduces Word2Spike, a novel rate coding mechanism that combines continuous word embeddings and neuromorphic architectures. We develop a one-to-one mapping that converts multi-dimensional word vectors into spike-based attractor states using Poisson processes. Using BitNet b1.58 quantization, we maintain 97% semantic similarity of continuous embeddings on SimLex-999 while achieving 100% reconstruction accuracy on 10,000 words from OpenAI's text-embedding-3-large. We preserve analogy performance (100% of original embedding performance) even under intentionally introduced noise, indicating a resilient mechanism for semantic encoding in neuromorphic systems. Next steps include integrating the mapping with spiking transformers and liquid state machines (resembling Hopfield Networks) for further evaluation.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SBS: Enhancing Parameter-Efficiency of Neural Representations for Neural Networks via Spectral Bias Suppression</title>
<link>https://arxiv.org/abs/2509.07373</link>
<guid>https://arxiv.org/abs/2509.07373</guid>
<content:encoded><![CDATA[
arXiv:2509.07373v1 Announce Type: cross 
Abstract: Implicit neural representations have recently been extended to represent convolutional neural network weights via neural representation for neural networks, offering promising parameter compression benefits. However, standard multi-layer perceptrons used in neural representation for neural networks exhibit a pronounced spectral bias, hampering their ability to reconstruct high-frequency details effectively. In this paper, we propose SBS, a parameter-efficient enhancement to neural representation for neural networks that suppresses spectral bias using two techniques: (1) a unidirectional ordering-based smoothing that improves kernel smoothness in the output space, and (2) unidirectional ordering-based smoothing aware random fourier features that adaptively modulate the frequency bandwidth of input encodings based on layer-wise parameter count. Extensive evaluations on various ResNet models with datasets CIFAR-10, CIFAR-100, and ImageNet, demonstrate that SBS achieves significantly better reconstruction accuracy with less parameters compared to SOTA.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents</title>
<link>https://arxiv.org/abs/2509.07389</link>
<guid>https://arxiv.org/abs/2509.07389</guid>
<content:encoded><![CDATA[
arXiv:2509.07389v1 Announce Type: cross 
Abstract: Existing evaluation studies on linguistic competence of large language models (LLM agents) have focused primarily on vocabulary learning, morphological rule induction, syntactic generalization, pragmatic inference, and cross-linguistic transfer. However, none assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, a central feature of human language acquisition. We propose a novel experimental framework in which an LLM agent is evaluated on its ability to acquire and use a newly constructed language (Tinkatongue) in conversation with a bot that understands only Tinkatongue. Our findings show that LLM agents fail to establish a conversation within 100 responses, yet they adopt distinct strategies that mirror human approaches to language learning. The results suggest a new direction for evaluation benchmarks and open pathways to model designs that learn more effectively from interactive feedback.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid GCN-GRU Model for Anomaly Detection in Cryptocurrency Transactions</title>
<link>https://arxiv.org/abs/2509.07392</link>
<guid>https://arxiv.org/abs/2509.07392</guid>
<content:encoded><![CDATA[
arXiv:2509.07392v1 Announce Type: cross 
Abstract: Blockchain transaction networks are complex, with evolving temporal patterns and inter-node relationships. To detect illicit activities, we propose a hybrid GCN-GRU model that captures both structural and sequential features. Using real Bitcoin transaction data (2020-2024), our model achieved 0.9470 Accuracy and 0.9807 AUC-ROC, outperforming all baselines.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Lifelong-Sustainable Electronic-Photonic AI Systems via Extreme Efficiency, Reconfigurability, and Robustness</title>
<link>https://arxiv.org/abs/2509.07396</link>
<guid>https://arxiv.org/abs/2509.07396</guid>
<content:encoded><![CDATA[
arXiv:2509.07396v1 Announce Type: cross 
Abstract: The relentless growth of large-scale artificial intelligence (AI) has created unprecedented demand for computational power, straining the energy, bandwidth, and scaling limits of conventional electronic platforms. Electronic-photonic integrated circuits (EPICs) have emerged as a compelling platform for next-generation AI systems, offering inherent advantages in ultra-high bandwidth, low latency, and energy efficiency for computing and interconnection. Beyond performance, EPICs also hold unique promises for sustainability. Fabricated in relaxed process nodes with fewer metal layers and lower defect densities, photonic devices naturally reduce embodied carbon footprint (CFP) compared to advanced digital electronic integrated circuits, while delivering orders-of-magnitude higher computing performance and interconnect bandwidth. To further advance the sustainability of photonic AI systems, we explore how electronic-photonic design automation (EPDA) and cross-layer co-design methodologies can amplify these inherent benefits. We present how advanced EPDA tools enable more compact layout generation, reducing both chip area and metal layer usage. We will also demonstrate how cross-layer device-circuit-architecture co-design unlocks new sustainability gains for photonic hardware: ultra-compact photonic circuit designs that minimize chip area cost, reconfigurable hardware topology that adapts to evolving AI workloads, and intelligent resilience mechanisms that prolong lifetime by tolerating variations and faults. By uniting intrinsic photonic efficiency with EPDA- and co-design-driven gains in area efficiency, reconfigurability, and robustness, we outline a vision for lifelong-sustainable electronic-photonic AI systems. This perspective highlights how EPIC AI systems can simultaneously meet the performance demands of modern AI and the urgent imperative for sustainable computing.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Universal Interatomic Potentials on Zeolite Structures</title>
<link>https://arxiv.org/abs/2509.07417</link>
<guid>https://arxiv.org/abs/2509.07417</guid>
<content:encoded><![CDATA[
arXiv:2509.07417v1 Announce Type: cross 
Abstract: Interatomic potentials (IPs) with wide elemental coverage and high accuracy are powerful tools for high-throughput materials discovery. While the past few years witnessed the development of multiple new universal IPs that cover wide ranges of the periodic table, their applicability to target chemical systems should be carefully investigated. We benchmark several universal IPs using equilibrium zeolite structures as testbeds. We select a diverse set of universal IPs encompassing two major categories: (i) universal analytic IPs, including GFN-FF, UFF, and Dreiding; (ii) pretrained universal machine learning IPs (MLIPs), comprising CHGNet, ORB-v3, MatterSim, eSEN-30M-OAM, PFP-v7, and EquiformerV2-lE4-lF100-S2EFS-OC22. We compare them with established tailor-made IPs, SLC, ClayFF, and BSFF using experimental data and density functional theory (DFT) calculations with dispersion correction as the reference. The tested zeolite structures comprise pure silica frameworks and aluminosilicates containing copper species, potassium, and organic cations. We found that GFN-FF is the best among the tested universal analytic IPs, but it does not achieve satisfactory accuracy for highly strained silica rings and aluminosilicate systems. All MLIPs can well reproduce experimental or DFT-level geometries and energetics. Among the universal MLIPs, the eSEN-30M-OAM model shows the most consistent performance across all zeolite structures studied. These findings show that the modern pretrained universal MLIPs are practical tools in zeolite screening workflows involving various compositions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward</title>
<link>https://arxiv.org/abs/2509.07430</link>
<guid>https://arxiv.org/abs/2509.07430</guid>
<content:encoded><![CDATA[
arXiv:2509.07430v1 Announce Type: cross 
Abstract: A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as a proactive solution. We argue that standard RLVR objectives -- both those using the mode-seeking reverse KL-divergence and those forgoing a divergence term entirely -- lack a crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose a fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as a rehearsal mechanism. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Extensive experiments on math and SQL generation demonstrate that DPH-RL not only resolves the Pass@k degradation but improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights a crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of a divergence measure is a powerful tool for building more general and diverse reasoning models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions</title>
<link>https://arxiv.org/abs/2509.07445</link>
<guid>https://arxiv.org/abs/2509.07445</guid>
<content:encoded><![CDATA[
arXiv:2509.07445v1 Announce Type: cross 
Abstract: Large language models (LLMs) are beginning to automate reward design for dexterous manipulation. However, no prior work has considered tactile sensing, which is known to be critical for human-like dexterity. We present Text2Touch, bringing LLM-crafted rewards to the challenging task of multi-axis in-hand object rotation with real-world vision based tactile sensing in palm-up and palm-down configurations. Our prompt engineering strategy scales to over 70 environment variables, and sim-to-real distillation enables successful policy transfer to a tactile-enabled fully actuated four-fingered dexterous robot hand. Text2Touch significantly outperforms a carefully tuned human-engineered baseline, demonstrating superior rotation speed and stability while relying on reward functions that are an order of magnitude shorter and simpler. These results illustrate how LLM-designed rewards can significantly reduce the time from concept to deployable dexterous tactile skills, supporting more rapid and scalable multimodal robot learning. Project website: https://hpfield.github.io/text2touch-website
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting</title>
<link>https://arxiv.org/abs/2509.07456</link>
<guid>https://arxiv.org/abs/2509.07456</guid>
<content:encoded><![CDATA[
arXiv:2509.07456v1 Announce Type: cross 
Abstract: Deep neural networks often rely on spurious correlations in training data, leading to biased or unfair predictions in safety-critical domains such as medicine and autonomous driving. While conventional bias mitigation typically requires retraining from scratch or redesigning data pipelines, recent advances in machine unlearning provide a promising alternative for post-hoc model correction. In this work, we investigate \textit{Bias-Aware Machine Unlearning}, a paradigm that selectively removes biased samples or feature representations to mitigate diverse forms of bias in vision models. Building on privacy-preserving unlearning techniques, we evaluate various strategies including Gradient Ascent, LoRA, and Teacher-Student distillation. Through empirical analysis on three benchmark datasets, CUB-200-2011 (pose bias), CIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection), we demonstrate that post-hoc unlearning can substantially reduce subgroup disparities, with improvements in demographic parity of up to \textbf{94.86\%} on CUB-200, \textbf{30.28\%} on CIFAR-10, and \textbf{97.37\%} on CelebA. These gains are achieved with minimal accuracy loss and with methods scoring an average of 0.62 across the 3 settings on the joint evaluation of utility, fairness, quality, and privacy. Our findings establish machine unlearning as a practical framework for enhancing fairness in deployed vision systems without necessitating full retraining.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis</title>
<link>https://arxiv.org/abs/2509.07463</link>
<guid>https://arxiv.org/abs/2509.07463</guid>
<content:encoded><![CDATA[
arXiv:2509.07463v1 Announce Type: cross 
Abstract: Ensuring reliable robot operation when visual input is degraded or insufficient remains a central challenge in robotics. This letter introduces DepthVision, a framework for multimodal scene understanding designed to address this problem. Unlike existing Vision-Language Models (VLMs), which use only camera-based visual input alongside language, DepthVision synthesizes RGB images from sparse LiDAR point clouds using a conditional generative adversarial network (GAN) with an integrated refiner network. These synthetic views are then combined with real RGB data using a Luminance-Aware Modality Adaptation (LAMA), which blends the two types of data dynamically based on ambient lighting conditions. This approach compensates for sensor degradation, such as darkness or motion blur, without requiring any fine-tuning of downstream vision-language models. We evaluate DepthVision on real and simulated datasets across various models and tasks, with particular attention to safety-critical tasks. The results demonstrate that our approach improves performance in low-light conditions, achieving substantial gains over RGB-only baselines while preserving compatibility with frozen VLMs. This work highlights the potential of LiDAR-guided RGB synthesis for achieving robust robot operation in real-world environments.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention</title>
<link>https://arxiv.org/abs/2509.07475</link>
<guid>https://arxiv.org/abs/2509.07475</guid>
<content:encoded><![CDATA[
arXiv:2509.07475v1 Announce Type: cross 
Abstract: Detecting content that contradicts or is unsupported by a given source text is a critical challenge for the safe deployment of generative language models. We introduce HALT-RAG, a post-hoc verification system designed to identify hallucinations in the outputs of Retrieval-Augmented Generation (RAG) pipelines. Our flexible and task-adaptable framework uses a universal feature set derived from an ensemble of two frozen, off-the-shelf Natural Language Inference (NLI) models and lightweight lexical signals. These features are used to train a simple, calibrated, and task-adapted meta-classifier. Using a rigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and produce unbiased estimates, we evaluate our system on the HaluEval benchmark. By pairing our universal feature set with a lightweight, task-adapted classifier and a precision-constrained decision policy, HALT-RAG achieves strong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA, and dialogue tasks, respectively. The system's well-calibrated probabilities enable a practical abstention mechanism, providing a reliable tool for balancing model performance with safety requirements.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language Models for Visual Navigation Assistance</title>
<link>https://arxiv.org/abs/2509.07488</link>
<guid>https://arxiv.org/abs/2509.07488</guid>
<content:encoded><![CDATA[
arXiv:2509.07488v1 Announce Type: cross 
Abstract: We address vision-language-driven indoor navigation to assist visually impaired individuals in reaching a target location using images and natural language guidance. Traditional navigation systems are ineffective indoors due to the lack of precise location data. Our approach integrates vision and language models to generate step-by-step navigational instructions, enhancing accessibility and independence. We fine-tune the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose an evaluation metric that refines the BERT F1 score by emphasizing directional and sequential variables, providing a more comprehensive measure of navigational performance. After applying LoRA, the model significantly improved in generating directional instructions, overcoming limitations in the original BLIP-2 model.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition</title>
<link>https://arxiv.org/abs/2509.07495</link>
<guid>https://arxiv.org/abs/2509.07495</guid>
<content:encoded><![CDATA[
arXiv:2509.07495v1 Announce Type: cross 
Abstract: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing significant security threats to their deployment in remote sensing applications. Research on adversarial attacks not only reveals model vulnerabilities but also provides critical insights for enhancing robustness. Although current mixing-based strategies have been proposed to increase the transferability of adversarial examples, they either perform global blending or directly exchange a region in the images, which may destroy global semantic features and mislead the optimization of adversarial examples. Furthermore, their reliance on cross-entropy loss for perturbation optimization leads to gradient diminishing during iterative updates, compromising adversarial example quality. To address these limitations, we focus on non-targeted attacks and propose a novel framework via local mixing and logits optimization. First, we present a local mixing strategy to generate diverse yet semantically consistent inputs. Different from MixUp, which globally blends two images, and MixCut, which stitches images together, our method merely blends local regions to preserve global semantic information. Second, we adapt the logit loss from targeted attacks to non-targeted scenarios, mitigating the gradient vanishing problem of cross-entropy loss. Third, a perturbation smoothing loss is applied to suppress high-frequency noise and enhance transferability. Extensive experiments on FGSCR-42 and MTARSI datasets demonstrate superior performance over 12 state-of-the-art methods across 6 surrogate models. Notably, with ResNet as the surrogate on MTARSI, our method achieves a 17.28% average improvement in black-box attack success rate.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Astra: A Multi-Agent System for GPU Kernel Performance Optimization</title>
<link>https://arxiv.org/abs/2509.07506</link>
<guid>https://arxiv.org/abs/2509.07506</guid>
<content:encoded><![CDATA[
arXiv:2509.07506v1 Announce Type: cross 
Abstract: GPU kernel optimization has long been a central challenge at the intersection of high-performance computing and machine learning. Efficient kernels are crucial for accelerating large language model (LLM) training and serving, yet attaining high performance typically requires extensive manual tuning. Compiler-based systems reduce some of this burden, but still demand substantial manual design and engineering effort. Recently, researchers have explored using LLMs for GPU kernel generation, though prior work has largely focused on translating high-level PyTorch modules into CUDA code. In this work, we introduce Astra, the first LLM-based multi-agent system for GPU kernel optimization. Unlike previous approaches, Astra starts from existing CUDA implementations extracted from SGLang, a widely deployed framework for serving LLMs, rather than treating PyTorch modules as the specification. Within Astra, specialized LLM agents collaborate through iterative code generation, testing, profiling, and planning to produce kernels that are both correct and high-performance. On kernels from SGLang, Astra achieves an average speedup of 1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study further demonstrates that LLMs can autonomously apply loop transformations, optimize memory access patterns, exploit CUDA intrinsics, and leverage fast math operations to yield substantial performance gains. Our work highlights multi-agent LLM systems as a promising new paradigm for GPU kernel optimization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval</title>
<link>https://arxiv.org/abs/2509.07512</link>
<guid>https://arxiv.org/abs/2509.07512</guid>
<content:encoded><![CDATA[
arXiv:2509.07512v1 Announce Type: cross 
Abstract: Many contemporary data-driven research efforts in the natural sciences, such as chemistry and materials science, require large-scale, high-performance entity recognition from scientific datasets. Large language models (LLMs) have increasingly been adopted to solve the entity recognition task, with the same trend being observed on all-spectrum NLP tasks. The prevailing entity recognition LLMs rely on fine-tuned technology, yet the fine-tuning process often incurs significant cost. To achieve a best performance-cost trade-off, we propose ALLabel, a three-stage framework designed to select the most informative and representative samples in preparing the demonstrations for LLM modeling. The annotated examples are used to construct a ground-truth retrieval corpus for LLM in-context learning. By sequentially employing three distinct active learning strategies, ALLabel consistently outperforms all baselines under the same annotation budget across three specialized domain datasets. Experimental results also demonstrate that selectively annotating only 5\%-10\% of the dataset with ALLabel can achieve performance comparable to the method annotating the entire dataset. Further analyses and ablation studies verify the effectiveness and generalizability of our proposal.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Water Demand Forecasting of District Metered Areas through Learned Consumer Representations</title>
<link>https://arxiv.org/abs/2509.07515</link>
<guid>https://arxiv.org/abs/2509.07515</guid>
<content:encoded><![CDATA[
arXiv:2509.07515v1 Announce Type: cross 
Abstract: Advancements in smart metering technologies have significantly improved the ability to monitor and manage water utilities. In the context of increasing uncertainty due to climate change, securing water resources and supply has emerged as an urgent global issue with extensive socioeconomic ramifications. Hourly consumption data from end-users have yielded substantial insights for projecting demand across regions characterized by diverse consumption patterns. Nevertheless, the prediction of water demand remains challenging due to influencing non-deterministic factors, such as meteorological conditions. This work introduces a novel method for short-term water demand forecasting for District Metered Areas (DMAs) which encompass commercial, agricultural, and residential consumers. Unsupervised contrastive learning is applied to categorize end-users according to distinct consumption behaviors present within a DMA. Subsequently, the distinct consumption behaviors are utilized as features in the ensuing demand forecasting task using wavelet-transformed convolutional networks that incorporate a cross-attention mechanism combining both historical data and the derived representations. The proposed approach is evaluated on real-world DMAs over a six-month period, demonstrating improved forecasting performance in terms of MAPE across different DMAs, with a maximum improvement of 4.9%. Additionally, it identifies consumers whose behavior is shaped by socioeconomic factors, enhancing prior knowledge about the deterministic patterns that influence demand.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHWGesture -- A dataset for multimodal understanding of clinical gestures</title>
<link>https://arxiv.org/abs/2509.07525</link>
<guid>https://arxiv.org/abs/2509.07525</guid>
<content:encoded><![CDATA[
arXiv:2509.07525v1 Announce Type: cross 
Abstract: Hand gesture understanding is essential for several applications in human-computer interaction, including automatic clinical assessment of hand dexterity. While deep learning has advanced static gesture recognition, dynamic gesture understanding remains challenging due to complex spatiotemporal variations. Moreover, existing datasets often lack multimodal and multi-view diversity, precise ground-truth tracking, and an action quality component embedded within gestures. This paper introduces EHWGesture, a multimodal video dataset for gesture understanding featuring five clinically relevant gestures. It includes over 1,100 recordings (6 hours), captured from 25 healthy subjects using two high-resolution RGB-Depth cameras and an event camera. A motion capture system provides precise ground-truth hand landmark tracking, and all devices are spatially calibrated and synchronized to ensure cross-modal alignment. Moreover, to embed an action quality task within gesture understanding, collected recordings are organized in classes of execution speed that mirror clinical evaluations of hand dexterity. Baseline experiments highlight the dataset's potential for gesture classification, gesture trigger detection, and action quality assessment. Thus, EHWGesture can serve as a comprehensive benchmark for advancing multimodal clinical gesture understanding.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data</title>
<link>https://arxiv.org/abs/2509.07526</link>
<guid>https://arxiv.org/abs/2509.07526</guid>
<content:encoded><![CDATA[
arXiv:2509.07526v1 Announce Type: cross 
Abstract: Large language models (LLMs) have transformed NLP, yet their integration with audio remains underexplored -- despite audio's centrality to human communication. We introduce Falcon3-Audio, a family of Audio-Language Models (ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably small amount of public audio data -- less than 30K hours (5K unique) -- Falcon3-Audio-7B matches the best reported performance among open-weight models on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while distinguishing itself through superior data and parameter efficiency, single-stage training, and transparency. Notably, our smallest 1B model remains competitive with larger open models ranging from 2B to 13B parameters. Through extensive ablations, we find that common complexities -- such as curriculum learning, multiple audio encoders, and intricate cross-attention connectors -- are not required for strong performance, even compared to models trained on over 500K hours of data.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLeW: Facet-Level and Adaptive Weighted Representation Learning of Scientific Documents</title>
<link>https://arxiv.org/abs/2509.07531</link>
<guid>https://arxiv.org/abs/2509.07531</guid>
<content:encoded><![CDATA[
arXiv:2509.07531v1 Announce Type: cross 
Abstract: Scientific document representation learning provides powerful embeddings for various tasks, while current methods face challenges across three approaches. 1) Contrastive training with citation-structural signals underutilizes citation information and still generates single-vector representations. 2) Fine-grained representation learning, which generates multiple vectors at the sentence or aspect level, requires costly integration and lacks domain generalization. 3) Task-aware learning depends on manually predefined task categorization, overlooking nuanced task distinctions and requiring extra training data for task-specific modules. To address these problems, we propose a new method that unifies the three approaches for better representations, namely FLeW. Specifically, we introduce a novel triplet sampling method that leverages citation intent and frequency to enhance citation-structural signals for training. Citation intents (background, method, result), aligned with the general structure of scientific writing, facilitate a domain-generalized facet partition for fine-grained representation learning. Then, we adopt a simple weight search to adaptively integrate three facet-level embeddings into a task-specific document embedding without task-aware fine-tuning. Experiments show the applicability and robustness of FLeW across multiple scientific tasks and fields, compared to prior models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HU-based Foreground Masking for 3D Medical Masked Image Modeling</title>
<link>https://arxiv.org/abs/2509.07534</link>
<guid>https://arxiv.org/abs/2509.07534</guid>
<content:encoded><![CDATA[
arXiv:2509.07534v1 Announce Type: cross 
Abstract: While Masked Image Modeling (MIM) has revolutionized fields of computer vision, its adoption in 3D medical image computing has been limited by the use of random masking, which overlooks the density of anatomical objects. To address this limitation, we enhance the pretext task with a simple yet effective masking strategy. Leveraging Hounsfield Unit (HU) measurements, we implement an HU-based Foreground Masking, which focuses on the intensity distribution of visceral organs and excludes non-tissue regions, such as air and fluid, that lack diagnostically meaningful features. Extensive experiments on five public 3D medical imaging datasets demonstrate that our masking consistently improves performance, both in quality of segmentation and Dice score (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%, BraTS:~78.55\%). These results underscore the importance of domain-centric MIM and suggest a promising direction for representation learning in medical image segmentation. Implementation is available at github.com/AISeedHub/SubFore/.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition</title>
<link>https://arxiv.org/abs/2509.07555</link>
<guid>https://arxiv.org/abs/2509.07555</guid>
<content:encoded><![CDATA[
arXiv:2509.07555v1 Announce Type: cross 
Abstract: In a rapidly evolving world where information updates swiftly, knowledge in large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a cost-effective option, making knowledge editing (KE) without modifying parameters particularly necessary. We find that although existing retrieval-augmented generation (RAG)-based KE methods excel at editing simple knowledge, they struggle with KE in multi-hop question answering due to the issue of "edit skipping", which refers to skipping the relevant edited fact in inference. In addition to the diversity of natural language expressions of knowledge, edit skipping also arises from the mismatch between the granularity of LLMs in problem-solving and the facts in the edited memory. To address this issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing method with guided decomposition (IRAKE) through the guidance from single edited facts and entire edited cases. Experimental results demonstrate that IRAKE mitigates the failure of editing caused by edit skipping and outperforms state-of-the-art methods for KE in multi-hop question answering.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\Delta L$ Normalization: Rethink Loss Aggregation in RLVR</title>
<link>https://arxiv.org/abs/2509.07558</link>
<guid>https://arxiv.org/abs/2509.07558</guid>
<content:encoded><![CDATA[
arXiv:2509.07558v1 Announce Type: cross 
Abstract: We propose $\Delta L$ Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed $\Delta L$ Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at https://github.com/zerolllin/Delta-L-Normalization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Generalized Routing: Model and Agent Orchestration for Adaptive and Efficient Inference</title>
<link>https://arxiv.org/abs/2509.07571</link>
<guid>https://arxiv.org/abs/2509.07571</guid>
<content:encoded><![CDATA[
arXiv:2509.07571v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) and domain-specific AI agents has greatly expanded the ecosystem of AI-powered services. User queries, however, are highly diverse and often span multiple domains and task types, resulting in a complex and heterogeneous landscape. This diversity presents a fundamental routing challenge: how to accurately direct each query to an appropriate execution unit while optimizing both performance and efficiency. To address this, we propose MoMA (Mixture of Models and Agents), a generalized routing framework that integrates both LLM and agent-based routing. Built upon a deep understanding of model and agent capabilities, MoMA effectively handles diverse queries through precise intent recognition and adaptive routing strategies, achieving an optimal balance between efficiency and cost. Specifically, we construct a detailed training dataset to profile the capabilities of various LLMs under different routing model structures, identifying the most suitable tasks for each LLM. During inference, queries are dynamically routed to the LLM with the best cost-performance efficiency. We also introduce an efficient agent selection strategy based on a context-aware state machine and dynamic masking. Experimental results demonstrate that the MoMA router offers superior cost-efficiency and scalability compared to existing approaches.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks</title>
<link>https://arxiv.org/abs/2509.07581</link>
<guid>https://arxiv.org/abs/2509.07581</guid>
<content:encoded><![CDATA[
arXiv:2509.07581v1 Announce Type: cross 
Abstract: Deep learning offers a promising avenue for automating many recognition tasks in fields such as medicine and forensics. However, the black-box nature of these models hinders their adoption in high-stakes applications where trust and accountability are required. For 3D shape recognition tasks in particular, this paper introduces the Class Node Graph Attention Network (CGAT) architecture to address this need. Applied to 3D meshes of third molars derived from CBCT images, for Demirjian stage allocation, CGAT utilizes graph attention convolutions and an inherent attention mechanism, visualized via attention rollout, to explain its decision-making process. We evaluated the local mean curvature and distance to centroid node features, both individually and in combination, as well as model depth, finding that models incorporating directed edges to a global CLS node produced more intuitive attention maps, while also yielding desirable classification performance. We analyzed the attention-based explanations of the models, and their predictive performances to propose optimal settings for the CGAT. The combination of local mean curvature and distance to centroid as node features yielded a slight performance increase with 0.76 weighted F1 score, and more comprehensive attention visualizations. The CGAT architecture's ability to generate human-understandable attention maps can enhance trust and facilitate expert validation of model decisions. While demonstrated on dental data, CGAT is broadly applicable to graph-based classification and regression tasks, promoting wider adoption of transparent and competitive deep learning models in high-stakes environments.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment</title>
<link>https://arxiv.org/abs/2509.07588</link>
<guid>https://arxiv.org/abs/2509.07588</guid>
<content:encoded><![CDATA[
arXiv:2509.07588v1 Announce Type: cross 
Abstract: In recent years, there has been substantial progress in using pretrained Language Models (LMs) on a range of tasks aimed at improving the understanding of biomedical texts. Nonetheless, existing biomedical LLMs show limited comprehension of complex, domain-specific concept structures and the factual information encoded in biomedical Knowledge Graphs (KGs). In this work, we propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph. For a given textual sequence, we link biomedical concept mentions to the Unified Medical Language System (UMLS) KG and utilize local KG subgraphs as cross-modal positive samples for these mentions. Our empirical findings indicate that implementing our method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations, even with minimal pre-training on a small alignment dataset sourced from PubMed scientific abstracts.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion Control?</title>
<link>https://arxiv.org/abs/2509.07593</link>
<guid>https://arxiv.org/abs/2509.07593</guid>
<content:encoded><![CDATA[
arXiv:2509.07593v1 Announce Type: cross 
Abstract: End-to-end reinforcement learning for motion control promises unified perception-action policies that scale across embodiments and tasks, yet most deployed controllers are either blind (proprioception-only) or rely on fusion backbones with unfavorable compute-memory trade-offs. Recurrent controllers struggle with long-horizon credit assignment, and Transformer-based fusion incurs quadratic cost in token length, limiting temporal and spatial context. We present a vision-driven cross-modal RL framework built on SSD-Mamba2, a selective state-space backbone that applies state-space duality (SSD) to enable both recurrent and convolutional scanning with hardware-aware streaming and near-linear scaling. Proprioceptive states and exteroceptive observations (e.g., depth tokens) are encoded into compact tokens and fused by stacked SSD-Mamba2 layers. The selective state-space updates retain long-range dependencies with markedly lower latency and memory use than quadratic self-attention, enabling longer look-ahead, higher token resolution, and stable training under limited compute. Policies are trained end-to-end under curricula that randomize terrain and appearance and progressively increase scene complexity. A compact, state-centric reward balances task progress, energy efficiency, and safety. Across diverse motion-control scenarios, our approach consistently surpasses strong state-of-the-art baselines in return, safety (collisions and falls), and sample efficiency, while converging faster at the same compute budget. These results suggest that SSD-Mamba2 provides a practical fusion backbone for scalable, foresightful, and efficient end-to-end motion control.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards</title>
<link>https://arxiv.org/abs/2509.07603</link>
<guid>https://arxiv.org/abs/2509.07603</guid>
<content:encoded><![CDATA[
arXiv:2509.07603v1 Announce Type: cross 
Abstract: This paper presents an innovative Transformer-based deep learning strategy for optimizing the placement of sensors aiming at structural health monitoring of semiconductor probe cards. Failures in probe cards, including substrate cracks and loosened screws, would critically affect semiconductor manufacturing yield and reliability. Some failure modes could be detected by equipping a probe card with adequate sensors. Frequency response functions from simulated failure scenarios are adopted within a finite element model of a probe card. A comprehensive dataset, enriched by physics-informed scenario expansion and physics-aware statistical data augmentation, is exploited to train a hybrid Convolutional Neural Network and Transformer model. The model achieves high accuracy (99.83%) in classifying the probe card health states (baseline, loose screw, crack) and an excellent crack detection recall (99.73%). Model robustness is confirmed through a rigorous framework of 3 repetitions of 10-fold stratified cross-validation. The attention mechanism also pinpoints critical sensor locations: an analysis of the attention weights offers actionable insights for designing efficient, cost-effective monitoring systems by optimizing sensor configurations. This research highlights the capability of attention-based deep learning to advance proactive maintenance, enhancing operational reliability and yield in semiconductor manufacturing.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques</title>
<link>https://arxiv.org/abs/2509.07605</link>
<guid>https://arxiv.org/abs/2509.07605</guid>
<content:encoded><![CDATA[
arXiv:2509.07605v1 Announce Type: cross 
Abstract: Class imbalance poses a significant challenge to supervised classification, particularly in critical domains like medical diagnostics and anomaly detection where minority class instances are rare. While numerous studies have explored rebalancing techniques to address this issue, less attention has been given to evaluating the performance of binary classifiers under imbalance when no such techniques are applied. Therefore, the goal of this study is to assess the performance of binary classifiers "as-is", without performing any explicit rebalancing. Specifically, we systematically evaluate the robustness of a diverse set of binary classifiers across both real-world and synthetic datasets, under progressively reduced minority class sizes, using one-shot and few-shot scenarios as baselines. Our approach also explores varying data complexities through synthetic decision boundary generation to simulate real-world conditions. In addition to standard classifiers, we include experiments using undersampling, oversampling strategies, and one-class classification (OCC) methods to examine their behavior under severe imbalance. The results confirm that classification becomes more difficult as data complexity increases and the minority class size decreases. While traditional classifiers deteriorate under extreme imbalance, advanced models like TabPFN and boosting-based ensembles retain relatively higher performance and better generalization compared to traditional classifiers. Visual interpretability and evaluation metrics further validate these findings. Our work offers valuable guidance on model selection for imbalanced learning, providing insights into classifier robustness without dependence on explicit rebalancing techniques.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Classical Data to Quantum Advantage -- Quantum Policy Evaluation on Quantum Hardware</title>
<link>https://arxiv.org/abs/2509.07614</link>
<guid>https://arxiv.org/abs/2509.07614</guid>
<content:encoded><![CDATA[
arXiv:2509.07614v1 Announce Type: cross 
Abstract: Quantum policy evaluation (QPE) is a reinforcement learning (RL) algorithm which is quadratically more efficient than an analogous classical Monte Carlo estimation. It makes use of a direct quantum mechanical realization of a finite Markov decision process, in which the agent and the environment are modeled by unitary operators and exchange states, actions, and rewards in superposition. Previously, the quantum environment has been implemented and parametrized manually for an illustrative benchmark using a quantum simulator. In this paper, we demonstrate how these environment parameters can be learned from a batch of classical observational data through quantum machine learning (QML) on quantum hardware. The learned quantum environment is then applied in QPE to also compute policy evaluations on quantum hardware. Our experiments reveal that, despite challenges such as noise and short coherence times, the integration of QML and QPE shows promising potential for achieving quantum advantage in RL.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Quantum Circuits in Offline Contextual Bandit Problems</title>
<link>https://arxiv.org/abs/2509.07633</link>
<guid>https://arxiv.org/abs/2509.07633</guid>
<content:encoded><![CDATA[
arXiv:2509.07633v1 Announce Type: cross 
Abstract: This paper explores the application of variational quantum circuits (VQCs) for solving offline contextual bandit problems in industrial optimization tasks. Using the Industrial Benchmark (IB) environment, we evaluate the performance of quantum regression models against classical models. Our findings demonstrate that quantum models can effectively fit complex reward functions, identify optimal configurations via particle swarm optimization (PSO), and generalize well in noisy and sparse datasets. These results provide a proof of concept for utilizing VQCs in offline contextual bandit problems and highlight their potential in industrial optimization tasks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems</title>
<link>https://arxiv.org/abs/2509.07677</link>
<guid>https://arxiv.org/abs/2509.07677</guid>
<content:encoded><![CDATA[
arXiv:2509.07677v1 Announce Type: cross 
Abstract: Voice Authentication Systems (VAS) use unique vocal characteristics for verification. They are increasingly integrated into high-security sectors such as banking and healthcare. Despite their improvements using deep learning, they face severe vulnerabilities from sophisticated threats like deepfakes and adversarial attacks. The emergence of realistic voice cloning complicates detection, as systems struggle to distinguish authentic from synthetic audio. While anti-spoofing countermeasures (CMs) exist to mitigate these risks, many rely on static detection models that can be bypassed by novel adversarial methods, leaving a critical security gap. To demonstrate this vulnerability, we propose the Spectral Masking and Interpolation Attack (SMIA), a novel method that strategically manipulates inaudible frequency regions of AI-generated audio. By altering the voice in imperceptible zones to the human ear, SMIA creates adversarial samples that sound authentic while deceiving CMs. We conducted a comprehensive evaluation of our attack against state-of-the-art (SOTA) models across multiple tasks, under simulated real-world conditions. SMIA achieved a strong attack success rate (ASR) of at least 82% against combined VAS/CM systems, at least 97.5% against standalone speaker verification systems, and 100% against countermeasures. These findings conclusively demonstrate that current security postures are insufficient against adaptive adversarial attacks. This work highlights the urgent need for a paradigm shift toward next-generation defenses that employ dynamic, context-aware frameworks capable of evolving with the threat landscape.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Online Learning by Integrating Biosensors and Multimodal Learning Analytics for Detecting and Predicting Student Behavior: A Review</title>
<link>https://arxiv.org/abs/2509.07742</link>
<guid>https://arxiv.org/abs/2509.07742</guid>
<content:encoded><![CDATA[
arXiv:2509.07742v1 Announce Type: cross 
Abstract: In modern online learning, understanding and predicting student behavior is crucial for enhancing engagement and optimizing educational outcomes. This systematic review explores the integration of biosensors and Multimodal Learning Analytics (MmLA) to analyze and predict student behavior during computer-based learning sessions. We examine key challenges, including emotion and attention detection, behavioral analysis, experimental design, and demographic considerations in data collection. Our study highlights the growing role of physiological signals, such as heart rate, brain activity, and eye-tracking, combined with traditional interaction data and self-reports to gain deeper insights into cognitive states and engagement levels. We synthesize findings from 54 key studies, analyzing commonly used methodologies such as advanced machine learning algorithms and multimodal data pre-processing techniques. The review identifies current research trends, limitations, and emerging directions in the field, emphasizing the transformative potential of biosensor-driven adaptive learning systems. Our findings suggest that integrating multimodal data can facilitate personalized learning experiences, real-time feedback, and intelligent educational interventions, ultimately advancing toward a more customized and adaptive online learning experience.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral and Rhythm Feature Performance Evaluation for Category and Class Level Audio Classification with Deep Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2509.07756</link>
<guid>https://arxiv.org/abs/2509.07756</guid>
<content:encoded><![CDATA[
arXiv:2509.07756v1 Announce Type: cross 
Abstract: Next to decision tree and k-nearest neighbours algorithms deep convolutional neural networks (CNNs) are widely used to classify audio data in many domains like music, speech or environmental sounds. To train a specific CNN various spectral and rhythm features like mel-scaled spectrograms, mel-frequency cepstral coefficients (MFCC), cyclic tempograms, short-time Fourier transform (STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy normalized statistics (CENS) chromagrams can be used as digital image input data for the neural network. The performance of these spectral and rhythm features for audio category level as well as audio class level classification is investigated in detail with a deep CNN and the ESC-50 dataset with 2,000 labeled environmental audio recordings using an end-to-end deep learning pipeline. The evaluated metrics accuracy, precision, recall and F1 score for multiclass classification clearly show that the mel-scaled spectrograms and the mel-frequency cepstral coefficients (MFCC) perform significantly better then the other spectral and rhythm features investigated in this research for audio classification tasks using deep CNNs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects</title>
<link>https://arxiv.org/abs/2509.07763</link>
<guid>https://arxiv.org/abs/2509.07763</guid>
<content:encoded><![CDATA[
arXiv:2509.07763v1 Announce Type: cross 
Abstract: Context. Code refactoring improves software quality without changing external behavior. Despite its advantages, its benefits are hindered by the considerable cost of time, resources, and continuous effort it demands. Aim. Understanding why developers refactor, and which metrics capture these motivations, may support wider and more effective use of refactoring in practice. Method. We performed a large-scale empirical study to analyze developers refactoring activity, leveraging Large Language Models (LLMs) to identify underlying motivations from version control data, comparing our findings with previous motivations reported in the literature. Results. LLMs matched human judgment in 80% of cases, but aligned with literature-based motivations in only 47%. They enriched 22% of motivations with more detailed rationale, often highlighting readability, clarity, and structural improvements. Most motivations were pragmatic, focused on simplification and maintainability. While metrics related to developer experience and code readability ranked highest, their correlation with motivation categories was weak. Conclusions. We conclude that LLMs effectively capture surface-level motivations but struggle with architectural reasoning. Their value lies in providing localized explanations, which, when combined with software metrics, can form hybrid approaches. Such integration offers a promising path toward prioritizing refactoring more systematically and balancing short-term improvements with long-term architectural goals.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning</title>
<link>https://arxiv.org/abs/2509.07768</link>
<guid>https://arxiv.org/abs/2509.07768</guid>
<content:encoded><![CDATA[
arXiv:2509.07768v1 Announce Type: cross 
Abstract: The spread of fake news, polarizing, politically biased, and harmful content on online platforms has been a serious concern. With large language models becoming a promising approach, however, no study has properly benchmarked their performance across different models, usage methods, and languages. This study presents a comprehensive overview of different Large Language Models adaptation paradigms for the detection of hyperpartisan and fake news, harmful tweets, and political bias. Our experiments spanned 10 datasets and 5 different languages (English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and multiclass classification scenarios. We tested different strategies ranging from parameter efficient Fine-Tuning of language models to a variety of different In-Context Learning strategies and prompts. These included zero-shot prompts, codebooks, few-shot (with both randomly-selected and diversely-selected examples using Determinantal Point Processes), and Chain-of-Thought. We discovered that In-Context Learning often underperforms when compared to Fine-Tuning a model. This main finding highlights the importance of Fine-Tuning even smaller models on task-specific settings even when compared to the largest models evaluated in an In-Context Learning setup - in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and Qwen2.5-7B-Instruct.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XSRD-Net: EXplainable Stroke Relapse Detection</title>
<link>https://arxiv.org/abs/2509.07772</link>
<guid>https://arxiv.org/abs/2509.07772</guid>
<content:encoded><![CDATA[
arXiv:2509.07772v1 Announce Type: cross 
Abstract: Stroke is the second most frequent cause of death world wide with an annual mortality of around 5.5 million. Recurrence rates of stroke are between 5 and 25% in the first year. As mortality rates for relapses are extraordinarily high (40%) it is of utmost importance to reduce the recurrence rates. We address this issue by detecting patients at risk of stroke recurrence at an early stage in order to enable appropriate therapy planning. To this end we collected 3D intracranial CTA image data and recorded concomitant heart diseases, the age and the gender of stroke patients between 2010 and 2024. We trained single- and multimodal deep learning based neural networks for binary relapse detection (Task 1) and for relapse free survival (RFS) time prediction together with a subsequent classification (Task 2). The separation of relapse from non-relapse patients (Task 1) could be solved with tabular data (AUC on test dataset: 0.84). However, for the main task, the regression (Task 2), our multimodal XSRD-net processed the modalities vision:tabular with 0.68:0.32 according to modality contribution measures. The c-index with respect to relapses for the multimodal model reached 0.68, and the AUC is 0.71 for the test dataset. Final, deeper interpretability analysis results could highlight a link between both heart diseases (tabular) and carotid arteries (vision) for the detection of relapses and the prediction of the RFS time. This is a central outcome that we strive to strengthen with ongoing data collection and model retraining.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Individual utilities of life satisfaction reveal inequality aversion unrelated to political alignment</title>
<link>https://arxiv.org/abs/2509.07793</link>
<guid>https://arxiv.org/abs/2509.07793</guid>
<content:encoded><![CDATA[
arXiv:2509.07793v1 Announce Type: cross 
Abstract: How should well-being be prioritised in society, and what trade-offs are people willing to make between fairness and personal well-being? We investigate these questions using a stated preference experiment with a nationally representative UK sample (n = 300), in which participants evaluated life satisfaction outcomes for both themselves and others under conditions of uncertainty. Individual-level utility functions were estimated using an Expected Utility Maximisation (EUM) framework and tested for sensitivity to the overweighting of small probabilities, as characterised by Cumulative Prospect Theory (CPT). A majority of participants displayed concave (risk-averse) utility curves and showed stronger aversion to inequality in societal life satisfaction outcomes than to personal risk. These preferences were unrelated to political alignment, suggesting a shared normative stance on fairness in well-being that cuts across ideological boundaries. The results challenge use of average life satisfaction as a policy metric, and support the development of nonlinear utility-based alternatives that more accurately reflect collective human values. Implications for public policy, well-being measurement, and the design of value-aligned AI systems are discussed.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced SegNet with Integrated Grad-CAM for Interpretable Retinal Layer Segmentation in OCT Images</title>
<link>https://arxiv.org/abs/2509.07795</link>
<guid>https://arxiv.org/abs/2509.07795</guid>
<content:encoded><![CDATA[
arXiv:2509.07795v1 Announce Type: cross 
Abstract: Optical Coherence Tomography (OCT) is essential for diagnosing conditions such as glaucoma, diabetic retinopathy, and age-related macular degeneration. Accurate retinal layer segmentation enables quantitative biomarkers critical for clinical decision-making, but manual segmentation is time-consuming and variable, while conventional deep learning models often lack interpretability. This work proposes an improved SegNet-based deep learning framework for automated and interpretable retinal layer segmentation. Architectural innovations, including modified pooling strategies, enhance feature extraction from noisy OCT images, while a hybrid loss function combining categorical cross-entropy and Dice loss improves performance for thin and imbalanced retinal layers. Gradient-weighted Class Activation Mapping (Grad-CAM) is integrated to provide visual explanations, allowing clinical validation of model decisions. Trained and validated on the Duke OCT dataset, the framework achieved 95.77% validation accuracy, a Dice coefficient of 0.9446, and a Jaccard Index (IoU) of 0.8951. Class-wise results confirmed robust performance across most layers, with challenges remaining for thinner boundaries. Grad-CAM visualizations highlighted anatomically relevant regions, aligning segmentation with clinical biomarkers and improving transparency. By combining architectural improvements, a customized hybrid loss, and explainable AI, this study delivers a high-performing SegNet-based framework that bridges the gap between accuracy and interpretability. The approach offers strong potential for standardizing OCT analysis, enhancing diagnostic efficiency, and fostering clinical trust in AI-driven ophthalmic tools.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Russian Equipment Losses Using Time Series and Deep Learning Models</title>
<link>https://arxiv.org/abs/2509.07813</link>
<guid>https://arxiv.org/abs/2509.07813</guid>
<content:encoded><![CDATA[
arXiv:2509.07813v1 Announce Type: cross 
Abstract: This study applies a range of forecasting techniques,including ARIMA, Prophet, Long Short Term Memory networks (LSTM), Temporal Convolutional Networks (TCN), and XGBoost, to model and predict Russian equipment losses during the ongoing war in Ukraine. Drawing on daily and monthly open-source intelligence (OSINT) data from WarSpotting, we aim to assess trends in attrition, evaluate model performance, and estimate future loss patterns through the end of 2025. Our findings show that deep learning models, particularly TCN and LSTM, produce stable and consistent forecasts, especially under conditions of high temporal granularity. By comparing different model architectures and input structures, this study highlights the importance of ensemble forecasting in conflict modeling, and the value of publicly available OSINT data in quantifying material degradation over time.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost</title>
<link>https://arxiv.org/abs/2509.07829</link>
<guid>https://arxiv.org/abs/2509.07829</guid>
<content:encoded><![CDATA[
arXiv:2509.07829v1 Announce Type: cross 
Abstract: Literary translation has recently gained attention as a distinct and complex task in machine translation research. However, the translation by small open models remains an open problem. We contribute to this ongoing research by introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for dataset creation, fine tuning, and evaluation in English-Romanian literary translations, centred on the creation and open release of both a compact, fine tuned language model (TF2-12B) and large scale synthetic parallel datasets (DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the largest collection of synthetic English fables to date, we address the need for rich, high quality literary datasets in low resource languages such as Romanian. Our pipeline first generates 15k high quality Romanian references from the TF1 pool using a high performing LLM. We then apply a two stage fine tuning process to a 12B parameter open weight model: (i) instruction tuning to capture genre specific narrative style, and (ii) adapter compression for efficient deployment. Evaluation combines corpus level BLEU and a five dimension LLM based rubric (accuracy, fluency, coherence, style, cultural adaptation) to provide a nuanced assessment of translation quality. Results show that our fine tuned model achieves fluency and adequacy competitive with top performing large proprietary models, while being open, accessible, and significantly more cost effective. Alongside the fine tuned model and both datasets, we publicly release all scripts and evaluation prompts. TF2 thus provides an end-to-end, reproducible pipeline for research on cost efficient translation, cross lingual narrative generation, and the broad adoption of open models for culturally significant literary content in low resource settings.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Burned Area Mapping Using Bi-Temporal Siamese Networks and AlphaEarth Foundation Datasets</title>
<link>https://arxiv.org/abs/2509.07852</link>
<guid>https://arxiv.org/abs/2509.07852</guid>
<content:encoded><![CDATA[
arXiv:2509.07852v1 Announce Type: cross 
Abstract: Accurate and timely mapping of burned areas is crucial for environmental monitoring, disaster management, and assessment of climate change. This study presents a novel approach to automated burned area mapping using the AlphaEArth dataset combined with the Siamese U-Net deep learning architecture. The AlphaEArth Dataset, comprising high-resolution optical and thermal infrared imagery with comprehensive ground-truth annotations, provides an unprecedented resource for training robust burned area detection models. We trained our model with the Monitoring Trends in Burn Severity (MTBS) dataset in the contiguous US and evaluated it with 17 regions cross in Europe. Our experimental results demonstrate that the proposed ensemble approach achieves superior performance with an overall accuracy of 95%, IoU of 0.6, and F1-score of 74% on the test dataset. The model successfully identifies burned areas across diverse ecosystems with complex background, showing particular strength in detecting partially burned vegetation and fire boundaries and its transferability and high generalization in burned area mapping. This research contributes to the advancement of automated fire damage assessment and provides a scalable solution for global burn area monitoring using the AlphaEarth dataset.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Membership Inference Test (aMINT): Enhancing Model Auditability with Multi-Task Learning</title>
<link>https://arxiv.org/abs/2509.07879</link>
<guid>https://arxiv.org/abs/2509.07879</guid>
<content:encoded><![CDATA[
arXiv:2509.07879v1 Announce Type: cross 
Abstract: Active Membership Inference Test (aMINT) is a method designed to detect whether given data were used during the training of machine learning models. In Active MINT, we propose a novel multitask learning process that involves training simultaneously two models: the original or Audited Model, and a secondary model, referred to as the MINT Model, responsible for identifying the data used for training the Audited Model. This novel multi-task learning approach has been designed to incorporate the auditability of the model as an optimization objective during the training process of neural networks. The proposed approach incorporates intermediate activation maps as inputs to the MINT layers, which are trained to enhance the detection of training data. We present results using a wide range of neural networks, from lighter architectures such as MobileNet to more complex ones such as Vision Transformers, evaluated in 5 public benchmarks. Our proposed Active MINT achieves over 80% accuracy in detecting if given data was used for training, significantly outperforming previous approaches in the literature. Our aMINT and related methodological developments contribute to increasing transparency in AI models, facilitating stronger safeguards in AI deployments to achieve proper security, privacy, and copyright protection.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Scaling Laws for Large Language Models via Inverse Problems</title>
<link>https://arxiv.org/abs/2509.07909</link>
<guid>https://arxiv.org/abs/2509.07909</guid>
<content:encoded><![CDATA[
arXiv:2509.07909v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are large-scale pretrained models that have achieved remarkable success across diverse domains. These successes have been driven by unprecedented complexity and scale in both data and computations. However, due to the high costs of training such models, brute-force trial-and-error approaches to improve LLMs are not feasible. Inspired by the success of inverse problems in uncovering fundamental scientific laws, this position paper advocates that inverse problems can also efficiently uncover scaling laws that guide the building of LLMs to achieve the desirable performance with significantly better cost-effectiveness.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation</title>
<link>https://arxiv.org/abs/2509.07923</link>
<guid>https://arxiv.org/abs/2509.07923</guid>
<content:encoded><![CDATA[
arXiv:2509.07923v1 Announce Type: cross 
Abstract: Digital dentistry represents a transformative shift in modern dental practice. The foundational step in this transformation is the accurate digital representation of the patient's dentition, which is obtained from segmented Cone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite the growing interest in digital dental technologies, existing segmentation methodologies frequently lack rigorous validation and demonstrate limited performance and clinical applicability. To the best of our knowledge, this is the first work to introduce a multimodal pretraining framework for tooth segmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning for pretraining that integrates volumetric (CBCT) and surface-based (IOS) modalities. By capturing modality-invariant representations through multimodal contrastive learning, our approach effectively models fine-grained anatomical features, enabling precise multi-class segmentation and accurate identification of F\'ed\'eration Dentaire Internationale (FDI) tooth numbering. Along with the framework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset to date, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensive collection of independent datasets, representing the largest and most diverse evaluation to date. Our method achieves state-of-the-art performance in both internal and external testing, with an increase of 12\% for CBCT segmentation and 8\% for IOS segmentation in the Dice Similarity Coefficient (DSC). Furthermore, ToothMCL consistently surpasses existing approaches in tooth groups and demonstrates robust generalizability across varying imaging conditions and clinical scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models</title>
<link>https://arxiv.org/abs/2509.07925</link>
<guid>https://arxiv.org/abs/2509.07925</guid>
<content:encoded><![CDATA[
arXiv:2509.07925v1 Announce Type: cross 
Abstract: Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at https://github.com/ODYSSEYWT/GUQ.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Local AI on Consumer GPUs: A Hardware-Aware Dynamic Strategy for YOLOv10s</title>
<link>https://arxiv.org/abs/2509.07928</link>
<guid>https://arxiv.org/abs/2509.07928</guid>
<content:encoded><![CDATA[
arXiv:2509.07928v1 Announce Type: cross 
Abstract: As local AI grows in popularity, there is a critical gap between the benchmark performance of object detectors and their practical viability on consumer-grade hardware. While models like YOLOv10s promise real-time speeds, these metrics are typically achieved on high-power, desktop-class GPUs. This paper reveals that on resource-constrained systems, such as laptops with RTX 4060 GPUs, performance is not compute-bound but is instead dominated by system-level bottlenecks, as illustrated by a simple bottleneck test. To overcome this hardware-level constraint, we introduce a Two-Pass Adaptive Inference algorithm, a model-independent approach that requires no architectural changes. This study mainly focuses on adaptive inference strategies and undertakes a comparative analysis of architectural early-exit and resolution-adaptive routing, highlighting their respective trade-offs within a unified evaluation framework. The system uses a fast, low-resolution pass and only escalates to a high-resolution model pass when detection confidence is low. On a 5000-image COCO dataset, our method achieves a 1.85x speedup over a PyTorch Early-Exit baseline, with a modest mAP loss of 5.51%. This work provides a practical and reproducible blueprint for deploying high-performance, real-time AI on consumer-grade devices by shifting the focus from pure model optimization to hardware-aware inference strategies that maximize throughput.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation</title>
<link>https://arxiv.org/abs/2509.07933</link>
<guid>https://arxiv.org/abs/2509.07933</guid>
<content:encoded><![CDATA[
arXiv:2509.07933v1 Announce Type: cross 
Abstract: The rapid evolution of Artificial Intelligence (AI) and Large Language Models (LLMs) has opened up new opportunities in the area of cybersecurity, especially in the exploitation automation landscape and penetration testing. This study explores Android penetration testing automation using LLM-based tools, especially PentestGPT, to identify and execute rooting techniques. Through a comparison of the traditional manual rooting process and exploitation methods produced using AI, this study evaluates the efficacy, reliability, and scalability of automated penetration testing in achieving high-level privilege access on Android devices. With the use of an Android emulator (Genymotion) as the testbed, we fully execute both traditional and exploit-based rooting methods, automating the process using AI-generated scripts. Secondly, we create a web application by integrating OpenAI's API to facilitate automated script generation from LLM-processed responses. The research focuses on the effectiveness of AI-enabled exploitation by comparing automated and manual penetration testing protocols, by determining LLM weaknesses and strengths along the way. We also provide security suggestions of AI-enabled exploitation, including ethical factors and potential misuse. The findings exhibit that while LLMs can significantly streamline the workflow of exploitation, they need to be controlled by humans to ensure accuracy and ethical application. This study adds to the increasing body of literature on AI-powered cybersecurity and its effect on ethical hacking, security research, and mobile device security.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImportSnare: Directed "Code Manual" Hijacking in Retrieval-Augmented Code Generation</title>
<link>https://arxiv.org/abs/2509.07941</link>
<guid>https://arxiv.org/abs/2509.07941</guid>
<content:encoded><![CDATA[
arXiv:2509.07941v1 Announce Type: cross 
Abstract: Code generation has emerged as a pivotal capability of Large Language Models(LLMs), revolutionizing development efficiency for programmers of all skill levels. However, the complexity of data structures and algorithmic logic often results in functional deficiencies and security vulnerabilities in generated code, reducing it to a prototype requiring extensive manual debugging. While Retrieval-Augmented Generation (RAG) can enhance correctness and security by leveraging external code manuals, it simultaneously introduces new attack surfaces.
  In this paper, we pioneer the exploration of attack surfaces in Retrieval-Augmented Code Generation (RACG), focusing on malicious dependency hijacking. We demonstrate how poisoned documentation containing hidden malicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting dual trust chains: LLM reliance on RAG and developers' blind trust in LLM suggestions. To construct poisoned documents, we propose ImportSnare, a novel attack framework employing two synergistic strategies: 1)Position-aware beam search optimizes hidden ranking sequences to elevate poisoned documents in retrieval results, and 2)Multilingual inductive suggestions generate jailbreaking sequences to manipulate LLMs into recommending malicious dependencies. Through extensive experiments across Python, Rust, and JavaScript, ImportSnare achieves significant attack success rates (over 50% for popular libraries such as matplotlib and seaborn) in general, and is also able to succeed even when the poisoning ratio is as low as 0.01%, targeting both custom and real-world malicious packages. Our findings reveal critical supply chain risks in LLM-powered development, highlighting inadequate security alignment for code generation tasks. To support future research, we will release the multilingual benchmark suite and datasets. The project homepage is https://importsnare.github.io.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges</title>
<link>https://arxiv.org/abs/2509.07946</link>
<guid>https://arxiv.org/abs/2509.07946</guid>
<content:encoded><![CDATA[
arXiv:2509.07946v1 Announce Type: cross 
Abstract: Multi-modal multi-task (M3T) foundation models (FMs) have recently shown transformative potential in artificial intelligence, with emerging applications in education. However, their deployment in real-world educational settings is hindered by privacy regulations, data silos, and limited domain-specific data availability. We introduce M3T Federated Foundation Models (FedFMs) for education: a paradigm that integrates federated learning (FL) with M3T FMs to enable collaborative, privacy-preserving training across decentralized institutions while accommodating diverse modalities and tasks. Subsequently, this position paper aims to unveil M3T FedFMs as a promising yet underexplored approach to the education community, explore its potentials, and reveal its related future research directions. We outline how M3T FedFMs can advance three critical pillars of next-generation intelligent education systems: (i) privacy preservation, by keeping sensitive multi-modal student and institutional data local; (ii) personalization, through modular architectures enabling tailored models for students, instructors, and institutions; and (iii) equity and inclusivity, by facilitating participation from underrepresented and resource-constrained entities. We finally identify various open research challenges, including studying of (i) inter-institution heterogeneous privacy regulations, (ii) the non-uniformity of data modalities' characteristics, (iii) the unlearning approaches for M3T FedFMs, (iv) the continual learning frameworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must be collectively addressed for practical deployment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACE and Diverse Generalization via Selective Disagreement</title>
<link>https://arxiv.org/abs/2509.07955</link>
<guid>https://arxiv.org/abs/2509.07955</guid>
<content:encoded><![CDATA[
arXiv:2509.07955v1 Announce Type: cross 
Abstract: Deep neural networks are notoriously sensitive to spurious correlations - where a model learns a shortcut that fails out-of-distribution. Existing work on spurious correlations has often focused on incomplete correlations,leveraging access to labeled instances that break the correlation. But in cases where the spurious correlations are complete, the correct generalization is fundamentally \textit{underspecified}. To resolve this underspecification, we propose learning a set of concepts that are consistent with training data but make distinct predictions on a subset of novel unlabeled inputs. Using a self-training approach that encourages \textit{confident} and \textit{selective} disagreement, our method ACE matches or outperforms existing methods on a suite of complete-spurious correlation benchmarks, while remaining robust to incomplete spurious correlations. ACE is also more configurable than prior approaches, allowing for straight-forward encoding of prior knowledge and principled unsupervised model selection. In an early application to language-model alignment, we find that ACE achieves competitive performance on the measurement tampering detection benchmark \textit{without} access to untrusted measurements. While still subject to important limitations, ACE represents significant progress towards overcoming underspecification.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search</title>
<link>https://arxiv.org/abs/2509.07969</link>
<guid>https://arxiv.org/abs/2509.07969</guid>
<content:encoded><![CDATA[
arXiv:2509.07969v1 Announce Type: cross 
Abstract: Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Emotion-Mediated Exploration in Artificial Intelligence Mirrors: Findings from Cognitive Psychology</title>
<link>https://arxiv.org/abs/2302.06615</link>
<guid>https://arxiv.org/abs/2302.06615</guid>
<content:encoded><![CDATA[
arXiv:2302.06615v2 Announce Type: replace 
Abstract: Background: Exploration of the physical environment is an indispensable precursor to information acquisition and knowledge consolidation for living organisms. Yet, current artificial intelligence models lack these autonomy capabilities during training, hindering their adaptability. This work proposes a learning framework for artificial agents to obtain an intrinsic exploratory drive, based on epistemic and achievement emotions triggered during data observation. Methods: This study proposes a dual-module reinforcement framework, where data analysis scores dictate pride or surprise, in accordance with psychological studies on humans. A correlation between these states and exploration is then optimized for agents to meet their learning goals. Results: Causal relationships between states and exploration are demonstrated by the majority of agents. A 15.4\% mean increase is noted for surprise, with a 2.8\% mean decrease for pride. Resulting correlations of $\rho_{surprise}=0.461$ and $\rho_{pride}=-0.237$ are obtained, mirroring previously reported human behavior. Conclusions: These findings lead to the conclusion that bio-inspiration for AI development can be of great use. This can incur benefits typically found in living beings, such as autonomy. Further, it empirically shows how AI methodologies can corroborate human behavioral findings, showcasing major interdisciplinary importance. Ramifications are discussed.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Language Model to Solve the Symbolic Multi-Step Reasoning Problem from the Perspective of Buffer Mechanism</title>
<link>https://arxiv.org/abs/2405.15302</link>
<guid>https://arxiv.org/abs/2405.15302</guid>
<content:encoded><![CDATA[
arXiv:2405.15302v3 Announce Type: replace 
Abstract: Large language models have consistently struggled with complex reasoning tasks, such as mathematical problem-solving. Investigating the internal reasoning mechanisms of these models can help us design better model architectures and training strategies, ultimately enhancing their reasoning capability. In this study, we constructed a symbolic multi-step reasoning task to investigate the information propagation mechanisms in Transformer models when solving the task through direct answering and Chain-of-Thought (CoT) reasoning. We introduced the concept of buffer mechanism: the model stores various information in distinct buffers and selectively extracts it through the query-key matrix. We proposed a random matrix-based algorithm to enhance the model's reasoning ability. This algorithm introduces only 132 trainable parameters, yet leads to significant performance improvements on 7 multi-step reasoning datasets, including PrOntoQA, LogicAsker, and LogicInference. These findings provide new insights into understanding the large language models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMMA: A Communicative Multimodal Multi-Agent Benchmark</title>
<link>https://arxiv.org/abs/2410.07553</link>
<guid>https://arxiv.org/abs/2410.07553</guid>
<content:encoded><![CDATA[
arXiv:2410.07553v3 Announce Type: replace 
Abstract: The rapid advances of multimodal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce COMMA: a novel puzzle benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of multimodal puzzles, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. Our findings reveal surprising weaknesses in state-of-the-art models, including strong proprietary models like GPT-4o and reasoning models like o4-mini. Many chain of thought reasoning models such as R1-Onevision and LLaVA-CoT struggle to outperform even a random baseline in agent-agent collaboration, indicating a potential growth area in their communication abilities.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visualizing Thought: Conceptual Diagrams Enable Robust Combinatorial Planning in LMMs</title>
<link>https://arxiv.org/abs/2503.11790</link>
<guid>https://arxiv.org/abs/2503.11790</guid>
<content:encoded><![CDATA[
arXiv:2503.11790v2 Announce Type: replace 
Abstract: Human reasoning relies on constructing and manipulating mental models -- simplified internal representations of situations that we use to understand and solve problems. Conceptual diagrams (e.g., a sketch drawn by a human to aid reasoning) externalize these mental models, abstracting irrelevant details to efficiently capture how entities interact with each other. In contrast, Large Language Models (LLMs) and Large MultiModal Models (LMMs) predominantly reason through text, limiting their effectiveness in complex multi-step tasks. In this paper, we propose Visual Thinking, a zero-shot framework that enables LMMs to reason through multiple chains of (self-generated) conceptual diagrams, significantly enhancing their combinatorial planning capabilities. Our approach does not require any human initialization beyond the natural language description of the task. It integrates both textual and diagrammatic reasoning within an optimized Graph-of-Thought inference framework, enhanced by beam search and depth-wise backtracking. Evaluated on multiple challenging PDDL planning domains, our method substantially improves LMMs' performance (e.g., GPT-4o: 35.5% -> 90.2% in Blocksworld) and consistently outperforms other text-only search-based inference methods. On more difficult planning domains with solution depths up to 40, our approach outperforms even the o1-preview reasoning model (e.g., 16 percentage points improvement in Floor Tiles). These results highlight the value of conceptual diagrams as a reasoning medium in LMMs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Reward Shaping from Confounded Offline Data</title>
<link>https://arxiv.org/abs/2505.11478</link>
<guid>https://arxiv.org/abs/2505.11478</guid>
<content:encoded><![CDATA[
arXiv:2505.11478v2 Announce Type: replace 
Abstract: A key task in Artificial Intelligence is learning effective policies for controlling agents in unknown environments to optimize performance measures. Off-policy learning methods, like Q-learning, allow learners to make optimal decisions based on past experiences. This paper studies off-policy learning from biased data in complex and high-dimensional domains where \emph{unobserved confounding} cannot be ruled out a priori. Building on the well-celebrated Deep Q-Network (DQN), we propose a novel deep reinforcement learning algorithm robust to confounding biases in observed data. Specifically, our algorithm attempts to find a safe policy for the worst-case environment compatible with the observations. We apply our method to twelve confounded Atari games, and find that it consistently dominates the standard DQN in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning</title>
<link>https://arxiv.org/abs/2506.00785</link>
<guid>https://arxiv.org/abs/2506.00785</guid>
<content:encoded><![CDATA[
arXiv:2506.00785v3 Announce Type: replace 
Abstract: This paper introduces GeoChain, a large-scale benchmark for evaluating step-by-step geographic reasoning in multimodal large language models (MLLMs). Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each image with a 21-step chain-of-thought (CoT) question sequence (over 30 million Q&amp;A pairs). These sequences guide models from coarse attributes to fine-grained localization across four reasoning categories - visual, spatial, cultural, and precise geolocation - annotated by difficulty. Images are also enriched with semantic segmentation (150 classes) and a visual locatability score. Our benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5 variants) on a diverse 2,088-image subset reveals consistent challenges: models frequently exhibit weaknesses in visual grounding, display erratic reasoning, and struggle to achieve accurate localization, especially as the reasoning complexity escalates. GeoChain offers a robust diagnostic methodology, critical for fostering significant advancements in complex geographic reasoning within MLLMs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs</title>
<link>https://arxiv.org/abs/2506.07824</link>
<guid>https://arxiv.org/abs/2506.07824</guid>
<content:encoded><![CDATA[
arXiv:2506.07824v2 Announce Type: replace 
Abstract: Multi-digit addition is a clear probe of the computational power of large language models. To dissect the internal arithmetic processes in LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection. Inspired by the step-by-step manner in which humans perform addition, we propose and analyze a coherent four-stage trajectory in the forward pass:Formula-structure representations become linearly decodable first, while the answer token is still far down the candidate list.Core computational features then emerge prominently.At deeper activation layers, numerical abstractions of the result become clearer, enabling near-perfect detection and decoding of the individual digits in the sum.Near the output, the model organizes and generates the final content, with the correct token reliably occupying the top rank.This trajectory suggests a hierarchical process that favors internal computation over rote memorization. We release our code and data to facilitate reproducibility.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the Diachronic Evolution of Legal Norms: An LRMoo-Based, Component-Level, Event-Centric Approach to Legal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.07853</link>
<guid>https://arxiv.org/abs/2506.07853</guid>
<content:encoded><![CDATA[
arXiv:2506.07853v3 Announce Type: replace 
Abstract: Effectively representing legal norms for automated processing is a critical challenge, particularly in tracking the temporal evolution of their hierarchical components. While foundational conceptual frameworks like IFLA LRMoo provide a generic toolkit for bibliographic data, and encoding standards like Akoma Ntoso offer a robust syntax for legal documents, a dedicated, formal modeling pattern for granular, component-level versioning is still required. This limitation hinders the deterministic point-intime reconstruction of legal texts, a fundamental capability for reliable Legal Tech and AI applications. This paper proposes a structured, temporal modeling pattern grounded in the LRMoo ontology to address this need. Our approach models the evolution of a legal norm as a diachronic chain of F2 Expressions. We introduce a key distinction between a language-agnostic Temporal Version (TV)-a semantic snapshot of the norm's structure-and its concrete monolingual realizations, the Language Versions (LV). Both are modeled as F2 Expressions linked by the canonical R76 is derivative of property. This paradigm is applied recursively to the legal text's internal structure, representing it as a parallel hierarchy of abstract Component Works (F1) and their versioned Component Expressions (F2). Furthermore, we formalize the legislative amendment process using the F28 Expression Creation event, allowing changes to be traced from an amending act to its precise effect on the amended norm. Using the Brazilian Federal Constitution as a case study, we demonstrate how this event-centric architecture enables the precise, deterministic retrieval and reconstruction of any part of a legal text as it existed on a specific date. The model provides a robust foundation for building verifiable knowledge graphs and advanced AI tools, overcoming the limitations of current generative models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedGellan: LLM-Generated Medical Guidance to Support Physicians</title>
<link>https://arxiv.org/abs/2507.04431</link>
<guid>https://arxiv.org/abs/2507.04431</guid>
<content:encoded><![CDATA[
arXiv:2507.04431v3 Announce Type: replace 
Abstract: Medical decision-making is a critical task, where errors can result in serious, potentially life-threatening consequences. While full automation remains challenging, hybrid frameworks that combine machine intelligence with human oversight offer a practical alternative. In this paper, we present MedGellan, a lightweight, annotation-free framework that uses a Large Language Model (LLM) to generate clinical guidance from raw medical records, which is then used by a physician to predict diagnoses. MedGellan uses a Bayesian-inspired prompting strategy that respects the temporal order of clinical data. Preliminary experiments show that the guidance generated by the LLM with MedGellan improves diagnostic performance, particularly in recall and $F_1$ score.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASP-FZN: A Translation-based Constraint Answer Set Solver</title>
<link>https://arxiv.org/abs/2507.22774</link>
<guid>https://arxiv.org/abs/2507.22774</guid>
<content:encoded><![CDATA[
arXiv:2507.22774v2 Announce Type: replace 
Abstract: We present the solver asp-fzn for Constraint Answer Set Programming (CASP), which extends ASP with linear constraints. Our approach is based on translating CASP programs into the solver-independent FlatZinc language that supports several Constraint Programming and Integer Programming backend solvers. Our solver supports a rich language of linear constraints, including some common global constraints. As for evaluation, we show that asp-fzn is competitive with state-of-the-art ASP solvers on benchmarks taken from past ASP competitions. Furthermore, we evaluate it on several CASP problems from the literature and compare its performance with clingcon, which is a prominent CASP solver that supports most of the asp-fzn language. The performance of asp-fzn is very promising as it is already competitive on plain ASP and even outperforms clingcon on some CASP benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CountQA: How Well Do MLLMs Count in the Wild?</title>
<link>https://arxiv.org/abs/2508.06585</link>
<guid>https://arxiv.org/abs/2508.06585</guid>
<content:encoded><![CDATA[
arXiv:2508.06585v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in understanding visual scenes, yet they exhibit a critical lack in a fundamental cognitive skill: object counting. This blind spot severely limits their reliability in real-world applications. To date, this capability has been largely unevaluated in complex scenarios, as existing benchmarks either feature sparse object densities or are confined to specific visual domains, failing to test models under realistic conditions. Addressing this gap, we introduce CountQA, a challenging new benchmark designed to probe this deficiency. Comprising over 1,500 question-answer pairs, CountQA features real-world images with high object density, clutter, and occlusion. We investigate this weakness by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the top-performing model achieves a mere 42.9% accuracy, with performance declining as object counts rise. By providing a dedicated benchmark to diagnose and rectify this core weakness, CountQA paves the way for a new generation of MLLMs that are not only descriptively fluent but also numerically grounded and spatially aware. We will open-source the dataset and code upon paper acceptance to foster further research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond</title>
<link>https://arxiv.org/abs/2508.07353</link>
<guid>https://arxiv.org/abs/2508.07353</guid>
<content:encoded><![CDATA[
arXiv:2508.07353v3 Announce Type: replace 
Abstract: The increasing demand for domain-specific evaluation of large language models (LLMs) has led to the development of numerous benchmarks. These efforts often adhere to the principle of data scaling, relying on large corpora or extensive question-answer (QA) sets to ensure broad coverage. However, the impact of corpus and QA set design on the precision and recall of domain-specific LLM performance remains poorly understood. In this paper, we argue that data scaling is not always the optimal principle for domain-specific benchmark construction. Instead, we introduce Comp-Comp, an iterative benchmarking framework grounded in the principle of comprehensiveness and compactness. Comprehensiveness ensures semantic recall by covering the full breadth of the domain, while compactness improves precision by reducing redundancy and noise. To demonstrate the effectiveness of our approach, we present a case study conducted at a well-renowned university, resulting in the creation of PolyBench, a large-scale, high-quality academic benchmark. Although this study focuses on academia, the Comp-Comp framework is domain-agnostic and readily adaptable to a wide range of specialized fields. The source code and datasets can be accessed at https://github.com/Anya-RB-Chen/COMP-COMP.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes</title>
<link>https://arxiv.org/abs/2508.17180</link>
<guid>https://arxiv.org/abs/2508.17180</guid>
<content:encoded><![CDATA[
arXiv:2508.17180v2 Announce Type: replace 
Abstract: A key frontier for Multimodal Large Language Models (MLLMs) is the ability to perform deep mathematical and spatial reasoning directly from images, moving beyond their established success in semantic description. Mathematical surface plots provide a rigorous testbed for this capability, as they isolate the task of reasoning from the semantic noise common in natural images. To measure progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over Visual Landscapes), a new benchmark designed to quantitatively evaluate these core reasoning skills. The benchmark comprises two novel tasks: Topological Counting, identifying and enumerating features like local maxima; and Transformation Recognition, recognizing applied geometric transformations. Generated from a curated library of functions with rigorous ambiguity filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs struggle significantly, often resorting to superficial heuristics instead of robust spatial reasoning. MaRVL-QA provides a challenging new tool for the research community to measure progress, expose model limitations, and guide the development of MLLMs with more profound reasoning abilities.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTourLLM: Enhancing LLMs with Chinese Tourism Knowledge</title>
<link>https://arxiv.org/abs/2407.12791</link>
<guid>https://arxiv.org/abs/2407.12791</guid>
<content:encoded><![CDATA[
arXiv:2407.12791v2 Announce Type: replace-cross 
Abstract: Recently, large language models (LLMs) have demonstrated their effectiveness in various natural language processing (NLP) tasks. However, the lack of tourism knowledge limits the performance of LLMs in tourist attraction presentations and travel planning. To address this challenge, we constructed a supervised fine-tuning dataset for the Chinese culture and tourism domain, named Cultour. This dataset consists of three parts: tourism knowledge base data, travelogues data, and tourism QA data. Additionally, we propose CTourLLM, a Qwen-based model supervised fine-tuned with Cultour, to improve the quality of information about attractions and travel planning. To evaluate the performance of CTourLLM, we proposed a human evaluation criterion named RRA (Relevance, Readability, Availability), and employed both automatic and human evaluation. The experimental results demonstrate that CTourLLM outperforms ChatGPT, achieving an improvement of 1.21 in BLEU-1 and 1.54 in Rouge-L, thereby validating the effectiveness of the response outcomes. Our proposed Cultour is accessible at https://github.com/mrweiqk/Cultour.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Truly Massive Budgeted Monotonic POMDPs with Oracle-Guided Meta-Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.07192</link>
<guid>https://arxiv.org/abs/2408.07192</guid>
<content:encoded><![CDATA[
arXiv:2408.07192v2 Announce Type: replace-cross 
Abstract: Monotonic Partially Observable Markov Decision Processes (POMDPs), where the system state progressively decreases until a restorative action is performed, can be used to model sequential repair problems effectively. This paper considers the problem of solving budget-constrained multi-component monotonic POMDPs, where a finite budget limits the maximal number of restorative actions. For a large number of components, solving such a POMDP using current methods is computationally intractable due to the exponential growth in the state space with an increasing number of components. To address this challenge, we propose a two-step approach. Since the individual components of a budget-constrained multi-component monotonic POMDP are only connected via the shared budget, we first approximate the optimal budget allocation among these components using an approximation of each component POMDP's optimal value function which is obtained through a random forest model. Subsequently, we introduce an oracle-guided meta-trained Proximal Policy Optimization (PPO) algorithm to solve each of the independent budget-constrained single-component monotonic POMDPs. The oracle policy is obtained by performing value iteration on the corresponding monotonic Markov Decision Process (MDP). This two-step method provides scalability in solving truly massive multi-component monotonic POMDPs. To demonstrate the efficacy of our approach, we consider a real-world maintenance scenario that involves inspection and repair of an administrative building by a team of agents within a maintenance budget. Finally, we perform a computational complexity analysis for a varying number of components to show the scalability of the proposed approach.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection</title>
<link>https://arxiv.org/abs/2411.02886</link>
<guid>https://arxiv.org/abs/2411.02886</guid>
<content:encoded><![CDATA[
arXiv:2411.02886v3 Announce Type: replace-cross 
Abstract: Rapid advances in Large Language Models (LLMs) have spurred demand for processing extended context sequences in contemporary applications. However, this progress faces two challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues limit LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using QK dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a few critical KV cache tokens in attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we design the Selection Cache based on observations of consecutive Query similarity and implemented the efficient Paged Dot Product Kernel, significantly reducing the selection overhead. A comprehensive evaluation of TokenSelect demonstrates up to $23.84\times$ speedup in attention computation and up to $2.28\times$ acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation</title>
<link>https://arxiv.org/abs/2411.11683</link>
<guid>https://arxiv.org/abs/2411.11683</guid>
<content:encoded><![CDATA[
arXiv:2411.11683v4 Announce Type: replace-cross 
Abstract: Robotic manipulation in the physical world is increasingly empowered by \textit{large language models} (LLMs) and \textit{vision-language models} (VLMs), leveraging their understanding and perception capabilities. Recently, various attacks against such robotic policies have been proposed, with backdoor attacks drawing considerable attention for their high stealth and strong persistence capabilities. However, existing backdoor efforts are limited to simulators and suffer from physical-world realization. To address this, we propose \textit{TrojanRobot}, a highly stealthy and broadly effective robotic backdoor attack in the physical world. Specifically, we introduce a module-poisoning approach by embedding a backdoor module into the modular robotic policy, enabling backdoor control over the policy's visual perception module thereby backdooring the entire robotic policy. Our vanilla implementation leverages a backdoor-finetuned VLM to serve as the backdoor module. To enhance its generalization in physical environments, we propose a prime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing three types of prime attacks, \ie, \textit{permutation}, \textit{stagnation}, and \textit{intentional} attacks, thus achieving finer-grained backdoors. Extensive experiments on the UR3e manipulator with 18 task instructions using robotic policies based on four VLMs demonstrate the broad effectiveness and physical-world stealth of TrojanRobot. Our attack's video demonstrations are available via a github link https://trojanrobot.github.io.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cardiverse: Harnessing LLMs for Novel Card Game Prototyping</title>
<link>https://arxiv.org/abs/2502.07128</link>
<guid>https://arxiv.org/abs/2502.07128</guid>
<content:encoded><![CDATA[
arXiv:2502.07128v2 Announce Type: replace-cross 
Abstract: The prototyping of computer games, particularly card games, requires extensive human effort in creative ideation and gameplay evaluation. Recent advances in Large Language Models (LLMs) offer opportunities to automate and streamline these processes. However, it remains challenging for LLMs to design novel game mechanics beyond existing databases, generate consistent gameplay environments, and develop scalable gameplay AI for large-scale evaluations. This paper addresses these challenges by introducing a comprehensive automated card game prototyping framework. The approach highlights a graph-based indexing method for generating novel game variations, an LLM-driven system for consistent game code generation validated by gameplay records, and a gameplay AI constructing method that uses an ensemble of LLM-generated heuristic functions optimized through self-play. These contributions aim to accelerate card game prototyping, reduce human labor, and lower barriers to entry for game developers. For code repo visit this http URL https://github.com/danruili/Cardiverse
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VINP: Variational Bayesian Inference with Neural Speech Prior for Joint ASR-Effective Speech Dereverberation and Blind RIR Identification</title>
<link>https://arxiv.org/abs/2502.07205</link>
<guid>https://arxiv.org/abs/2502.07205</guid>
<content:encoded><![CDATA[
arXiv:2502.07205v3 Announce Type: replace-cross 
Abstract: Reverberant speech, denoting the speech signal degraded by reverberation, contains crucial knowledge of both anechoic source speech and room impulse response (RIR). This work proposes a variational Bayesian inference (VBI) framework with neural speech prior (VINP) for joint speech dereverberation and blind RIR identification. In VINP, a probabilistic signal model is constructed in the time-frequency (T-F) domain based on convolution transfer function (CTF) approximation. For the first time, we propose using an arbitrary discriminative dereverberation deep neural network (DNN) to estimate the prior distribution of anechoic speech within a probabilistic model. By integrating both reverberant speech and the anechoic speech prior, VINP yields the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the anechoic speech spectrum and CTF filter, respectively. After simple transformations, the waveforms of anechoic speech and RIR are estimated. VINP is effective for automatic speech recognition (ASR) systems, which sets it apart from most deep learning (DL)-based single-channel dereverberation approaches. Experiments on single-channel speech dereverberation demonstrate that VINP attains state-of-the-art (SOTA) performance in mean opinion score (MOS) and word error rate (WER). For blind RIR identification, experiments demonstrate that VINP achieves SOTA performance in estimating reverberation time at 60 dB (RT60) and advanced performance in direct-to-reverberation ratio (DRR) estimation. Codes and audio samples are available online.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection</title>
<link>https://arxiv.org/abs/2502.13061</link>
<guid>https://arxiv.org/abs/2502.13061</guid>
<content:encoded><![CDATA[
arXiv:2502.13061v3 Announce Type: replace-cross 
Abstract: Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While Large Multimodal Models (LMMs) have shown promise in hateful meme detection, they face notable challenges like sub-optimal performance and limited out-of-domain generalization capabilities. Recent studies further reveal the limitations of both supervised fine-tuning (SFT) and in-context learning when applied to LMMs in this setting. To address these issues, we propose a robust adaptation framework for hateful meme detection that enhances in-domain accuracy and cross-domain generalization while preserving the general vision-language capabilities of LMMs. Analysis reveals that our approach achieves improved robustness under adversarial attacks compared to SFT models. Experiments on six meme classification datasets show that our approach achieves state-of-the-art performance, outperforming larger agentic systems. Moreover, our method generates higher-quality rationales for explaining hateful content compared to standard SFT, enhancing model interpretability. Code available at https://github.com/JingbiaoMei/RGCL
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning via Modality Alignment and Retention</title>
<link>https://arxiv.org/abs/2503.00374</link>
<guid>https://arxiv.org/abs/2503.00374</guid>
<content:encoded><![CDATA[
arXiv:2503.00374v3 Announce Type: replace-cross 
Abstract: Histopathology and transcriptomics are fundamental modalities in oncology, encapsulating the morphological and molecular aspects of the disease. Multi-modal self-supervised learning has demonstrated remarkable potential in learning pathological representations by integrating diverse data sources. Conventional multi-modal integration methods primarily emphasize modality alignment, while paying insufficient attention to retaining the modality-specific structures. However, unlike conventional scenarios where multi-modal inputs share highly overlapping features, histopathology and transcriptomics exhibit pronounced heterogeneity, offering orthogonal yet complementary insights. Histopathology provides morphological and spatial context, elucidating tissue architecture and cellular topology, whereas transcriptomics delineates molecular signatures through gene expression patterns. This inherent disparity introduces a major challenge in aligning them while maintaining modality-specific fidelity. To address these challenges, we present MIRROR, a novel multi-modal representation learning method designed to foster both modality alignment and retention. MIRROR employs dedicated encoders to extract comprehensive features for each modality, which is further complemented by a modality alignment module to achieve seamless integration between phenotype patterns and molecular profiles. Furthermore, a modality retention module safeguards unique attributes from each modality, while a style clustering module mitigates redundancy and enhances disease-relevant information by modeling and aligning consistent pathological signatures within a clustering space. Extensive evaluations on TCGA cohorts for cancer subtyping and survival analysis highlight MIRROR's superior performance, demonstrating its effectiveness in constructing comprehensive oncological feature representations and benefiting the cancer diagnosis.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution</title>
<link>https://arxiv.org/abs/2503.14779</link>
<guid>https://arxiv.org/abs/2503.14779</guid>
<content:encoded><![CDATA[
arXiv:2503.14779v2 Announce Type: replace-cross 
Abstract: Single-image super-resolution (SISR) is a fundamental problem in computer vision that aims to reconstruct high-resolution (HR) images from low-resolution (LR) inputs. Although convolutional neural networks (CNNs) have achieved substantial advancements, deeper architectures often introduce excessive parameters, higher memory usage, and computational cost, limiting their applicability on resource-constrained devices. Recent research has thus focused on lightweight architectures that preserve accuracy while reducing complexity. This paper presents the Involution and BSConv Multi-Depth Distillation Network (IBMDN), a lightweight and effective architecture for SISR. The proposed IBMDN comprises Involution and BSConv Multi-Depth Distillation Blocks (IBMDB) and a Contrast and High-Frequency Attention Block (CHFAB). IBMDB employs varying combinations of Involution and BSConv at multiple depths to perform efficient feature extraction while minimizing computational complexity. CHFAB, a lightweight self-attention mechanism, focuses on extracting high-frequency and contrast information to enhance perceptual quality in the reconstructed images. The flexible design of IBMDB enables it to be seamlessly integrated into diverse SISR frameworks, including information distillation, transformer-based, and GAN-based models. Extensive experiments demonstrate that incorporating IBMDB significantly reduces memory usage, parameters, and floating-point operations (FLOPs), while achieving improvements in both pixel-wise accuracy and visual quality. The source code is available at: https://github.com/akramkhatami/IBMDN.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Model Hears You: Audio Language Model Deployments Should Consider the Principle of Least Privilege</title>
<link>https://arxiv.org/abs/2503.16833</link>
<guid>https://arxiv.org/abs/2503.16833</guid>
<content:encoded><![CDATA[
arXiv:2503.16833v2 Announce Type: replace-cross 
Abstract: The latest Audio Language Models (Audio LMs) process speech directly instead of relying on a separate transcription step. This shift preserves detailed information, such as intonation or the presence of multiple speakers, that would otherwise be lost in transcription. However, it also introduces new safety risks, including the potential misuse of speaker identity cues and other sensitive vocal attributes, which could have legal implications. In this paper, we urge a closer examination of how these models are built and deployed. Our experiments show that end-to-end modeling, compared with cascaded pipelines, creates socio-technical safety risks such as identity inference, biased decision-making, and emotion detection. This raises concerns about whether Audio LMs store voiceprints and function in ways that create uncertainty under existing legal regimes. We then argue that the Principle of Least Privilege should be considered to guide the development and deployment of these models. Specifically, evaluations should assess (1) the privacy and safety risks associated with end-to-end modeling; and (2) the appropriate scope of information access. Finally, we highlight related gaps in current audio LM benchmarks and identify key open research questions, both technical and policy-related, that must be addressed to enable the responsible deployment of end-to-end Audio LMs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio-centric Video Understanding Benchmark without Text Shortcut</title>
<link>https://arxiv.org/abs/2503.19951</link>
<guid>https://arxiv.org/abs/2503.19951</guid>
<content:encoded><![CDATA[
arXiv:2503.19951v2 Announce Type: replace-cross 
Abstract: Audio often serves as an auxiliary modality in video understanding tasks of audio-visual large language models (LLMs), merely assisting in the comprehension of visual information. However, a thorough understanding of videos significantly depends on auditory information, as audio offers critical context, emotional cues, and semantic meaning that visual data alone often lacks. This paper proposes an audio-centric video understanding benchmark (AVUT) to evaluate the video comprehension capabilities of multimodal LLMs with a particular focus on auditory information. AVUT introduces a suite of carefully designed audio-centric tasks, holistically testing the understanding of both audio content and audio-visual interactions in videos. Moreover, this work points out the text shortcut problem that largely exists in other benchmarks where the correct answer can be found from question text alone without needing videos. AVUT addresses this problem by proposing a answer permutation-based filtering mechanism. A thorough evaluation across a diverse range of open-source and proprietary multimodal LLMs is performed, followed by the analyses of deficiencies in audio-visual LLMs. Demos and data are available at https://github.com/lark-png/AVUT.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Traffic Incident Response through Sub-Second Temporal Localization with HybridMamba</title>
<link>https://arxiv.org/abs/2504.03235</link>
<guid>https://arxiv.org/abs/2504.03235</guid>
<content:encoded><![CDATA[
arXiv:2504.03235v2 Announce Type: replace-cross 
Abstract: Traffic crash detection in long-form surveillance videos is essential for improving emergency response and infrastructure planning, yet remains difficult due to the brief and infrequent nature of crash events. We present \textbf{HybridMamba}, a novel architecture integrating visual transformers with state-space temporal modeling to achieve high-precision crash time localization. Our approach introduces multi-level token compression and hierarchical temporal processing to maintain computational efficiency without sacrificing temporal resolution. Evaluated on a large-scale dataset from the Iowa Department of Transportation, HybridMamba achieves a mean absolute error of \textbf{1.50 seconds} for 2-minute videos ($p<0.01$ compared to baselines), with \textbf{65.2%} of predictions falling within one second of the ground truth. It outperforms recent video-language models (e.g., TimeChat, VideoLLaMA-2) by up to 3.95 seconds while using significantly fewer parameters (3B vs. 13--72B). Our results demonstrate effective temporal localization across various video durations (2--40 minutes) and diverse environmental conditions, highlighting HybridMamba's potential for fine-grained temporal localization in traffic surveillance while identifying challenges that remain for extended deployment.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification</title>
<link>https://arxiv.org/abs/2504.18046</link>
<guid>https://arxiv.org/abs/2504.18046</guid>
<content:encoded><![CDATA[
arXiv:2504.18046v3 Announce Type: replace-cross 
Abstract: Ophthalmic diseases pose a significant global health burden. However, traditional diagnostic methods and existing monocular image-based deep learning approaches often overlook the pathological correlations between the two eyes. In practical medical robotic diagnostic scenarios, paired retinal images (binocular fundus images) are frequently required as diagnostic evidence. To address this, we propose DMS-Net-a dual-modal multi-scale siamese network for binocular retinal image classification. The framework employs a weight-sharing siamese ResNet-152 architecture to concurrently extract deep semantic features from bilateral fundus images. To tackle challenges like indistinct lesion boundaries and diffuse pathological distributions, we introduce the OmniPool Spatial Integrator Module (OSIM), which achieves multi-resolution feature aggregation through multi-scale adaptive pooling and spatial attention mechanisms. Furthermore, the Calibrated Analogous Semantic Fusion Module (CASFM) leverages spatial-semantic recalibration and bidirectional attention mechanisms to enhance cross-modal interaction, aggregating modality-agnostic representations of fundus structures. To fully exploit the differential semantic information of lesions present in bilateral fundus features, we introduce the Cross-Modal Contrastive Alignment Module (CCAM). Additionally, to enhance the aggregation of lesion-correlated semantic information, we introduce the Cross-Modal Integrative Alignment Module (CIAM). Evaluation on the ODIR-5K dataset demonstrates that DMS-Net achieves state-of-the-art performance with an accuracy of 82.9%, recall of 84.5%, and a Cohen's kappa coefficient of 83.2%, showcasing robust capacity in detecting symmetrical pathologies and improving clinical decision-making for ocular diseases. Code and the processed dataset will be released subsequently.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tripartite-GraphRAG via Plugin Ontologies</title>
<link>https://arxiv.org/abs/2504.19667</link>
<guid>https://arxiv.org/abs/2504.19667</guid>
<content:encoded><![CDATA[
arXiv:2504.19667v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities across various domains, yet they struggle with knowledge-intensive tasks in areas that demand factual accuracy, e.g. industrial automation and healthcare. Key limitations include their tendency to hallucinate, lack of source traceability (provenance), and challenges in timely knowledge updates. Combining language models with knowledge graphs (GraphRAG) offers promising avenues for overcoming these deficits. However, a major challenge lies in creating such a knowledge graph in the first place. Here, we propose a novel approach that combines LLMs with a tripartite knowledge graph representation, which is constructed by connecting complex, domain-specific objects via a curated ontology of corresponding, domain-specific concepts to relevant sections within chunks of text through a concept-anchored pre-analysis of source documents starting from an initial lexical graph. Subsequently, we formulate LLM prompt creation as an unsupervised node classification problem allowing for the optimization of information density, coverage, and arrangement of LLM prompts at significantly reduced lengths. An initial experimental evaluation of our approach on a healthcare use case, involving multi-faceted analyses of patient anamneses given a set of medical concepts as well as a series of clinical guideline literature, indicates its potential to optimize information density, coverage, and arrangement of LLM prompts while significantly reducing their lengths, which, in turn, may lead to reduced costs as well as more consistent and reliable LLM outputs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Llama-Nemotron: Efficient Reasoning Models</title>
<link>https://arxiv.org/abs/2505.00949</link>
<guid>https://arxiv.org/abs/2505.00949</guid>
<content:encoded><![CDATA[
arXiv:2505.00949v5 Announce Type: replace-cross 
Abstract: We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?</title>
<link>https://arxiv.org/abs/2505.02884</link>
<guid>https://arxiv.org/abs/2505.02884</guid>
<content:encoded><![CDATA[
arXiv:2505.02884v2 Announce Type: replace-cross 
Abstract: Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices</title>
<link>https://arxiv.org/abs/2505.03303</link>
<guid>https://arxiv.org/abs/2505.03303</guid>
<content:encoded><![CDATA[
arXiv:2505.03303v2 Announce Type: replace-cross 
Abstract: This paper presents a comprehensive evaluation of lightweight deep learning models for image classification, emphasizing their suitability for deployment in resource-constrained environments such as low-memory devices. Five state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet, EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using four key performance metrics: classification accuracy, inference time, floating-point operations (FLOPs), and model size. Additionally, we investigate the impact of hyperparameter tuning, data augmentation, and training paradigms by comparing pretrained models with scratch-trained counterparts, focusing on MobileNetV3 Small. Our findings reveal that transfer learning significantly enhances model accuracy and computational efficiency, particularly for complex datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest accuracy, while MobileNetV3 offers the best balance between accuracy and efficiency, and SqueezeNet excels in inference speed and compactness. This study highlights critical trade-offs between accuracy and efficiency, offering actionable insights for deploying lightweight models in real-world applications where computational resources are limited. By addressing these challenges, this research contributes to optimizing deep learning systems for edge computing and mobile platforms.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.04416</link>
<guid>https://arxiv.org/abs/2505.04416</guid>
<content:encoded><![CDATA[
arXiv:2505.04416v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose \textbf{OBLIVIATE}, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA) ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: \emph{forget quality} (via a new document-level memorization score), \emph{model utility}, and \emph{fluency}. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRADA: Graph-based Reranking against Adversarial Documents Attack</title>
<link>https://arxiv.org/abs/2505.07546</link>
<guid>https://arxiv.org/abs/2505.07546</guid>
<content:encoded><![CDATA[
arXiv:2505.07546v2 Announce Type: replace-cross 
Abstract: Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large language models (LLMs) by integrating external knowledge from retrieved documents, thereby overcoming the limitations of models' static intrinsic knowledge. However, these systems are susceptible to adversarial attacks that manipulate the retrieval process by introducing documents that are adversarial yet semantically similar to the query. Notably, while these adversarial documents resemble the query, they exhibit weak similarity to benign documents in the retrieval set. Thus, we propose a simple yet effective Graph-based Reranking against Adversarial Document Attacks (GRADA) framework aiming at preserving retrieval quality while significantly reducing the success of adversaries. Our study evaluates the effectiveness of our approach through experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b, Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with results from the Natural Questions dataset demonstrating up to an 80% reduction in attack success rates while maintaining minimal loss in accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overflow Prevention Enhances Long-Context Recurrent LLMs</title>
<link>https://arxiv.org/abs/2505.07793</link>
<guid>https://arxiv.org/abs/2505.07793</guid>
<content:encoded><![CDATA[
arXiv:2505.07793v2 Announce Type: replace-cross 
Abstract: A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visuospatial Cognitive Assistant</title>
<link>https://arxiv.org/abs/2505.12312</link>
<guid>https://arxiv.org/abs/2505.12312</guid>
<content:encoded><![CDATA[
arXiv:2505.12312v4 Announce Type: replace-cross 
Abstract: Video-based spatial cognition is vital for robotics and embodied AI but challenges current Vision-Language Models (VLMs). This paper makes two key contributions. First, we introduce ViCA (Visuospatial Cognitive Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D metadata-grounded queries and video-based complex reasoning. Second, we develop ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all eight VSI-Bench tasks, outperforming existing models, including larger ones (e.g., +26.1 on Absolute Distance). For interpretability, we present ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial reasoning. Our work highlights the importance of targeted data and suggests paths for improved temporal-spatial modeling. We release all resources to foster research in robust visuospatial intelligence.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</title>
<link>https://arxiv.org/abs/2505.12363</link>
<guid>https://arxiv.org/abs/2505.12363</guid>
<content:encoded><![CDATA[
arXiv:2505.12363v4 Announce Type: replace-cross 
Abstract: While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives</title>
<link>https://arxiv.org/abs/2505.21627</link>
<guid>https://arxiv.org/abs/2505.21627</guid>
<content:encoded><![CDATA[
arXiv:2505.21627v2 Announce Type: replace-cross 
Abstract: State-of-the-art large language models require specialized hardware and substantial energy to operate. As a consequence, cloud-based services that provide access to large language models have become very popular. In these services, the price users pay for an output provided by a model depends on the number of tokens the model uses to generate it -- they pay a fixed price per token. In this work, we show that this pricing mechanism creates a financial incentive for providers to strategize and misreport the (number of) tokens a model used to generate an output, and users cannot prove, or even know, whether a provider is overcharging them. However, we also show that, if an unfaithful provider is obliged to be transparent about the generative process used by the model, misreporting optimally without raising suspicion is hard. Nevertheless, as a proof-of-concept, we develop an efficient heuristic algorithm that allows providers to significantly overcharge users without raising suspicion. Crucially, we demonstrate that the cost of running the algorithm is lower than the additional revenue from overcharging users, highlighting the vulnerability of users under the current pay-per-token pricing mechanism. Further, we show that, to eliminate the financial incentive to strategize, a pricing mechanism must price tokens linearly on their character count. While this makes a provider's profit margin vary across tokens, we introduce a simple prescription under which the provider who adopts such an incentive-compatible pricing mechanism can maintain the average profit margin they had under the pay-per-token pricing mechanism. Along the way, to illustrate and complement our theoretical results, we conduct experiments with several large language models from the $\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input prompts from the LMSYS Chatbot Arena platform.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning</title>
<link>https://arxiv.org/abs/2505.22626</link>
<guid>https://arxiv.org/abs/2505.22626</guid>
<content:encoded><![CDATA[
arXiv:2505.22626v2 Announce Type: replace-cross 
Abstract: Imitation learning advances robot capabilities by enabling the acquisition of diverse behaviors from human demonstrations. However, large-scale datasets used for policy training often introduce substantial variability in quality, which can negatively impact performance. As a result, automatically curating datasets by filtering low-quality samples to improve quality becomes essential. Existing robotic curation approaches rely on costly manual annotations and perform curation at a coarse granularity, such as the dataset or trajectory level, failing to account for the quality of individual state-action pairs. To address this, we introduce SCIZOR, a self-supervised data curation framework that filters out low-quality state-action pairs to improve the performance of imitation learning policies. SCIZOR targets two complementary sources of low-quality data: suboptimal data, which hinders learning with undesirable actions, and redundant data, which dilutes training with repetitive patterns. SCIZOR leverages a self-supervised task progress predictor for suboptimal data to remove samples lacking task progression, and a deduplication module operating on joint state-action representation for samples with redundant patterns. Empirically, we show that SCIZOR enables imitation learning policies to achieve higher performance with less data, yielding an average improvement of 15.4% across multiple benchmarks. More information is available at: https://ut-austin-rpl.github.io/SCIZOR/
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localizing Persona Representations in LLMs</title>
<link>https://arxiv.org/abs/2505.24539</link>
<guid>https://arxiv.org/abs/2505.24539</guid>
<content:encoded><![CDATA[
arXiv:2505.24539v3 Announce Type: replace-cross 
Abstract: We present a study on how and where personas -- defined by distinct sets of human characteristics, values, and beliefs -- are encoded in the representation space of large language models (LLMs). Using a range of dimension reduction and pattern recognition methods, we first identify the model layers that show the greatest divergence in encoding these representations. We then analyze the activations within a selected layer to examine how specific personas are encoded relative to others, including their shared and distinct embedding spaces. We find that, across multiple pre-trained decoder-only LLMs, the analyzed personas show large differences in representation space only within the final third of the decoder layers. We observe overlapping activations for specific ethical perspectives -- such as moral nihilism and utilitarianism -- suggesting a degree of polysemy. In contrast, political ideologies like conservatism and liberalism appear to be represented in more distinct regions. These findings help to improve our understanding of how LLMs internally represent information and can inform future efforts in refining the modulation of specific human traits in LLM outputs. Warning: This paper includes potentially offensive sample statements.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments</title>
<link>https://arxiv.org/abs/2506.00563</link>
<guid>https://arxiv.org/abs/2506.00563</guid>
<content:encoded><![CDATA[
arXiv:2506.00563v2 Announce Type: replace-cross 
Abstract: A key approach to state abstraction is approximating behavioral metrics (notably, bisimulation metrics) in the observation space and embedding these learned distances in the representation space. While promising for robustness to task-irrelevant noise, as shown in prior work, accurately estimating these metrics remains challenging, requiring various design choices that create gaps between theory and practice. Prior evaluations focus mainly on final returns, leaving the quality of learned metrics and the source of performance gains unclear. To systematically assess how metric learning works in deep reinforcement learning (RL), we evaluate five recent approaches, unified conceptually as isometric embeddings with varying design choices. We benchmark them with baselines across 20 state-based and 14 pixel-based tasks, spanning 370 task configurations with diverse noise settings. Beyond final returns, we introduce the evaluation of a denoising factor to quantify the encoder's ability to filter distractions. To further isolate the effect of metric learning, we propose and evaluate an isolated metric estimation setting, in which the encoder is influenced solely by the metric loss. Finally, we release an open-source, modular codebase to improve reproducibility and support future research on metric learning in deep RL.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HueManity: Probing Fine-Grained Visual Perception in MLLMs</title>
<link>https://arxiv.org/abs/2506.03194</link>
<guid>https://arxiv.org/abs/2506.03194</guid>
<content:encoded><![CDATA[
arXiv:2506.03194v3 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) excel at high-level visual reasoning, but their performance on nuanced perceptual tasks remains surprisingly limited. We present HueManity, a benchmark designed to assess visual perception in MLLMs. The dataset comprises 83,850 images featuring two-character alphanumeric strings embedded in Ishihara test style dot patterns, challenging models on precise pattern recognition. Our evaluation of nine state-of-the-art MLLMs on HueManity demonstrates a significant performance deficit compared to human and traditional computer vision baselines. The best-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a striking 3% on the alphanumeric `hard' task. In contrast, human participants achieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model reached accuracies of 96.5% and 94.5%. These results highlight a critical gap in the visual capabilities of current MLLMs. Our analysis further explores potential architectural and training-paradigm factors contributing to this perceptual gap in MLLMs. We open-source HueManity dataset and code to foster further research in improving perceptual robustness of MLLMs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations</title>
<link>https://arxiv.org/abs/2506.10559</link>
<guid>https://arxiv.org/abs/2506.10559</guid>
<content:encoded><![CDATA[
arXiv:2506.10559v2 Announce Type: replace-cross 
Abstract: Explaining why the species lives at a particular location is important for understanding ecological systems and conserving biodiversity. However, existing ecological workflows are fragmented and often inaccessible to non-specialists. We propose an end-to-end visual-to-causal framework that transforms a species image into interpretable causal insights about its habitat preference. The system integrates species recognition, global occurrence retrieval, pseudo-absence sampling, and climate data extraction. We then discover causal structures among environmental features and estimate their influence on species occurrence using modern causal inference methods. Finally, we generate statistically grounded, human-readable causal explanations from structured templates and large language models. We demonstrate the framework on a bee and a flower species and report early results as part of an ongoing project, showing the potential of the multimodal AI assistant backed up by a recommended ecological modeling practice for describing species habitat in human-understandable language. Our code is available at: https://github.com/Yutong-Zhou-cv/BioX.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting</title>
<link>https://arxiv.org/abs/2506.19089</link>
<guid>https://arxiv.org/abs/2506.19089</guid>
<content:encoded><![CDATA[
arXiv:2506.19089v3 Announce Type: replace-cross 
Abstract: We introduce $\texttt{StorySim}$, a programmable framework for synthetically generating stories to evaluate the theory of mind (ToM) and world modeling (WM) capabilities of large language models (LLMs). Unlike prior benchmarks that may suffer from contamination in pretraining data, $\texttt{StorySim}$ produces novel, compositional story prompts anchored by a highly controllable $\texttt{Storyboard}$, enabling precise manipulation of character perspectives and events. We use this framework to design first- and second-order ToM tasks alongside WM tasks that control for the ability to track and model mental states. Our experiments across a suite of state-of-the-art LLMs reveal that most models perform better on WM tasks than ToM tasks, and that models tend to perform better reasoning with humans compared to inanimate objects. Additionally, our framework enabled us to find evidence of heuristic behavior such as recency bias and an over-reliance on earlier events in the story. All code for generating data and evaluations is freely available.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges</title>
<link>https://arxiv.org/abs/2507.02074</link>
<guid>https://arxiv.org/abs/2507.02074</guid>
<content:encoded><![CDATA[
arXiv:2507.02074v2 Announce Type: replace-cross 
Abstract: Crash detection from video feeds is a critical problem in intelligent transportation systems. Recent developments in large language models (LLMs) and vision-language models (VLMs) have transformed how we process, reason about, and summarize multimodal information. This paper surveys recent methods leveraging LLMs for crash detection from video data. We present a structured taxonomy of fusion strategies, summarize key datasets, analyze model architectures, compare performance benchmarks, and discuss ongoing challenges and opportunities. Our review provides a foundation for future research in this fast-growing intersection of video understanding and foundation models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation</title>
<link>https://arxiv.org/abs/2507.18206</link>
<guid>https://arxiv.org/abs/2507.18206</guid>
<content:encoded><![CDATA[
arXiv:2507.18206v2 Announce Type: replace-cross 
Abstract: A fundamental requirement for full autonomy in mobile robots is accurate navigation even in situations where satellite navigation or cameras are unavailable. In such practical situations, relying only on inertial sensors will result in navigation solution drift due to the sensors' inherent noise and error terms. One of the emerging solutions to mitigate drift is to maneuver the robot in a snake-like slithering motion to increase the inertial signal-to-noise ratio, allowing the regression of the mobile robot position. In this work, we propose MoRPI-PINN as a physics-informed neural network framework for accurate inertial-based mobile robot navigation. By embedding physical laws and constraints into the training process, MoRPI-PINN is capable of providing an accurate and robust navigation solution. Using real-world experiments, we show accuracy improvements of over 85% compared to other approaches. MoRPI-PINN is a lightweight approach that can be implemented even on edge devices and used in any typical mobile robot application.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs</title>
<link>https://arxiv.org/abs/2507.22286</link>
<guid>https://arxiv.org/abs/2507.22286</guid>
<content:encoded><![CDATA[
arXiv:2507.22286v2 Announce Type: replace-cross 
Abstract: The usage-based constructionist (UCx) approach to language posits that language comprises a network of learned form-meaning pairings (constructions) whose use is largely determined by their meanings or functions, requiring them to be graded and probabilistic. This study investigates whether the internal representations in Large Language Models (LLMs) reflect the proposed function-infused gradience. We analyze representations of the English Double Object (DO) and Prepositional Object (PO) constructions in Pythia-$1.4$B, using a dataset of $5000$ sentence pairs systematically varied by human-rated preference strength for DO or PO. Geometric analyses show that the separability between the two constructions' representations, as measured by energy distance or Jensen-Shannon divergence, is systematically modulated by gradient preference strength, which depends on lexical and functional properties of sentences. That is, more prototypical exemplars of each construction occupy more distinct regions in activation space, compared to sentences that could have equally well have occured in either construction. These results provide evidence that LLMs learn rich, meaning-infused, graded representations of constructions and offer support for geometric measures for representations in LLMs.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app</title>
<link>https://arxiv.org/abs/2508.00103</link>
<guid>https://arxiv.org/abs/2508.00103</guid>
<content:encoded><![CDATA[
arXiv:2508.00103v3 Announce Type: replace-cross 
Abstract: This study explores the integration of Augmented Intelligence (AuI) in Intelligent Tutoring Systems (ITS) to address challenges in Artificial Intelligence in Education (AIED), including teacher involvement, AI reliability, and resource accessibility. We present MathAIde, an ITS that uses computer vision and AI to correct mathematics exercises from student work photos and provide feedback. The system was designed through a collaborative process involving brainstorming with teachers, high-fidelity prototyping, A/B testing, and a real-world case study. Findings emphasize the importance of a teacher-centered, user-driven approach, where AI suggests remediation alternatives while teachers retain decision-making. Results highlight efficiency, usability, and adoption potential in classroom contexts, particularly in resource-limited environments. The study contributes practical insights into designing ITSs that balance user needs and technological feasibility, while advancing AIED research by demonstrating the effectiveness of a mixed-methods, user-centered approach to implementing AuI in educational technologies.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM</title>
<link>https://arxiv.org/abs/2508.04795</link>
<guid>https://arxiv.org/abs/2508.04795</guid>
<content:encoded><![CDATA[
arXiv:2508.04795v2 Announce Type: replace-cross 
Abstract: In dialogue transcription pipelines, Large Language Models (LLMs) are frequently employed in post-processing to improve grammar, punctuation, and readability. We explore a complementary post-processing step: enriching transcribed dialogues by adding metadata tags for speaker characteristics such as age, gender, and emotion. Some of the tags are global to the entire dialogue, while some are time-variant. Our approach couples frozen audio foundation models, such as Whisper or WavLM, with a frozen LLAMA language model to infer these speaker attributes, without requiring task-specific fine-tuning of either model. Using lightweight, efficient connectors to bridge audio and language representations, we achieve competitive performance on speaker profiling tasks while preserving modularity and speed. Additionally, we demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving an Equal Error Rate of 8.8% in some scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grid-Agent: An LLM-Powered Multi-Agent System for Power Grid Control</title>
<link>https://arxiv.org/abs/2508.05702</link>
<guid>https://arxiv.org/abs/2508.05702</guid>
<content:encoded><![CDATA[
arXiv:2508.05702v3 Announce Type: replace-cross 
Abstract: Modern power grids face unprecedented complexity from Distributed Energy Resources (DERs), Electric Vehicles (EVs), and extreme weather, while also being increasingly exposed to cyberattacks that can trigger grid violations. This paper introduces Grid-Agent, an autonomous AI-driven framework that leverages Large Language Models (LLMs) within a multi-agent system to detect and remediate violations. Grid-Agent integrates semantic reasoning with numerical precision through modular agents: a planning agent generates coordinated action sequences using power flow solvers, while a validation agent ensures stability and safety through sandboxed execution with rollback mechanisms. To enhance scalability, the framework employs an adaptive multi-scale network representation that dynamically adjusts encoding schemes based on system size and complexity. Violation resolution is achieved through optimizing switch configurations, battery deployment, and load curtailment. Our experiments on IEEE and CIGRE benchmark networks, including the IEEE 69-bus, CIGRE MV, IEEE 30-bus test systems, demonstrate superior mitigation performance, highlighting Grid-Agent's suitability for modern smart grids requiring rapid, adaptive response.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges</title>
<link>https://arxiv.org/abs/2508.06401</link>
<guid>https://arxiv.org/abs/2508.06401</guid>
<content:encoded><![CDATA[
arXiv:2508.06401v3 Announce Type: replace-cross 
Abstract: This systematic review of the research literature on retrieval-augmented generation (RAG) provides a focused analysis of the most highly cited studies published between 2020 and May 2025. A total of 128 articles met our inclusion criteria. The records were retrieved from ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP). RAG couples a neural retriever with a generative language model, grounding output in up-to-date, non-parametric memory while retaining the semantic generalisation stored in model weights. Guided by the PRISMA 2020 framework, we (i) specify explicit inclusion and exclusion criteria based on citation count and research questions, (ii) catalogue datasets, architectures, and evaluation practices, and (iii) synthesise empirical evidence on the effectiveness and limitations of RAG. To mitigate citation-lag bias, we applied a lower citation-count threshold to papers published in 2025 so that emerging breakthroughs with naturally fewer citations were still captured. This review clarifies the current research landscape, highlights methodological gaps, and charts priority directions for future research.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Conversational Recommender System Considering Consumer Types</title>
<link>https://arxiv.org/abs/2508.13209</link>
<guid>https://arxiv.org/abs/2508.13209</guid>
<content:encoded><![CDATA[
arXiv:2508.13209v2 Announce Type: replace-cross 
Abstract: Conversational Recommender Systems (CRS) provide personalized services through multi-turn interactions, yet most existing methods overlook users' heterogeneous decision-making styles and knowledge levels, which constrains both accuracy and efficiency. To address this gap, we propose CT-CRS (Consumer Type-Enhanced Conversational Recommender System), a framework that integrates consumer type modeling into dialogue recommendation. Based on consumer type theory, we define four user categories--dependent, efficient, cautious, and expert--derived from two dimensions: decision-making style (maximizers vs. satisficers) and knowledge level (high vs. low). CT-CRS employs interaction histories and fine-tunes the large language model to automatically infer user types in real time, avoiding reliance on static questionnaires. We incorporate user types into state representation and design a type-adaptive policy that dynamically adjusts recommendation granularity, diversity, and attribute query complexity. To further optimize the dialogue policy, we adopt Inverse Reinforcement Learning (IRL), enabling the agent to approximate expert-like strategies conditioned on consumer type. Experiments on LastFM, Amazon-Book, and Yelp show that CTCRS improves recommendation success rate and reduces interaction turns compared to strong baselines. Ablation studies confirm that both consumer type modeling and IRL contribute significantly to performance gains. These results demonstrate that CT-CRS offers a scalable and interpretable solution for enhancing CRS personalization through the integration of psychological modeling and advanced policy optimization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust but Verify! A Survey on Verification Design for Test-time Scaling</title>
<link>https://arxiv.org/abs/2508.16665</link>
<guid>https://arxiv.org/abs/2508.16665</guid>
<content:encoded><![CDATA[
arXiv:2508.16665v3 Announce Type: replace-cross 
Abstract: Test-time scaling (TTS) has emerged as a new frontier for scaling the performance of Large Language Models. In test-time scaling, by using more computational resources during inference, LLMs can improve their reasoning process and task performance. Several approaches have emerged for TTS such as distilling reasoning traces from another model or exploring the vast decoding search space by employing a verifier. The verifiers serve as reward models that help score the candidate outputs from the decoding process to diligently explore the vast solution space and select the best outcome. This paradigm commonly termed has emerged as a superior approach owing to parameter free scaling at inference time and high performance gains. The verifiers could be prompt-based, fine-tuned as a discriminative or generative model to verify process paths, outcomes or both. Despite their widespread adoption, there is no detailed collection, clear categorization and discussion of diverse verification approaches and their training mechanisms. In this survey, we cover the diverse approaches in the literature and present a unified view of verifier training, types and their utility in test-time scaling. Our repository can be found at https://github.com/elixir-research-group/Verifierstesttimescaling.github.io.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Threats Against Voice Authentication and Anti-Spoofing Systems</title>
<link>https://arxiv.org/abs/2508.16843</link>
<guid>https://arxiv.org/abs/2508.16843</guid>
<content:encoded><![CDATA[
arXiv:2508.16843v3 Announce Type: replace-cross 
Abstract: Voice authentication has undergone significant changes from traditional systems that relied on handcrafted acoustic features to deep learning models that can extract robust speaker embeddings. This advancement has expanded its applications across finance, smart devices, law enforcement, and beyond. However, as adoption has grown, so have the threats. This survey presents a comprehensive review of the modern threat landscape targeting Voice Authentication Systems (VAS) and Anti-Spoofing Countermeasures (CMs), including data poisoning, adversarial, deepfake, and adversarial spoofing attacks. We chronologically trace the development of voice authentication and examine how vulnerabilities have evolved in tandem with technological advancements. For each category of attack, we summarize methodologies, highlight commonly used datasets, compare performance and limitations, and organize existing literature using widely accepted taxonomies. By highlighting emerging risks and open challenges, this survey aims to support the development of more secure and resilient voice authentication systems.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models</title>
<link>https://arxiv.org/abs/2508.17674</link>
<guid>https://arxiv.org/abs/2508.17674</guid>
<content:encoded><![CDATA[
arXiv:2508.17674v2 Announce Type: replace-cross 
Abstract: We introduce Advertisement Embedding Attacks (AEA), a new class of LLM security threats that stealthily inject promotional or malicious content into model outputs and AI agents. AEA operate through two low-cost vectors: (1) hijacking third-party service-distribution platforms to prepend adversarial prompts, and (2) publishing back-doored open-source checkpoints fine-tuned with attacker data. Unlike conventional attacks that degrade accuracy, AEA subvert information integrity, causing models to return covert ads, propaganda, or hate speech while appearing normal. We detail the attack pipeline, map five stakeholder victim groups, and present an initial prompt-based self-inspection defense that mitigates these injections without additional model retraining. Our findings reveal an urgent, under-addressed gap in LLM security and call for coordinated detection, auditing, and policy responses from the AI-safety community.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection</title>
<link>https://arxiv.org/abs/2508.20392</link>
<guid>https://arxiv.org/abs/2508.20392</guid>
<content:encoded><![CDATA[
arXiv:2508.20392v2 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by minimal power consumption and swift inference capabilities on neuromorphic hardware, and have been widely applied to various visual perception tasks. Current ANN-SNN conversion methods have achieved excellent results in classification tasks with ultra-low time-steps, but their performance in visual detection tasks remains suboptimal. In this paper, we propose a delay-spike approach to mitigate the issue of residual membrane potential caused by heterogeneous spiking patterns. Furthermore, we propose a novel temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This enables Integrate-and-fire (IF) neurons to dynamically adjust their accumulation and firing behaviors based on the temporal order of time-steps. Our method enables spikes to exhibit distinct temporal properties, rather than relying solely on frequency-based representations. Moreover, the tdIF neuron maintains energy consumption on par with traditional IF neuron. We demonstrate that our method achieves more precise feature representation with lower time-steps, enabling high performance and ultra-low latency in visual detection tasks. In this study, we conduct extensive evaluation of the tdIF method across two critical vision tasks: object detection and lane line detection. The results demonstrate that the proposed method surpasses current ANN-SNN conversion approaches, achieving state-of-the-art performance with ultra-low latency (within 5 time-steps).
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control</title>
<link>https://arxiv.org/abs/2508.21112</link>
<guid>https://arxiv.org/abs/2508.21112</guid>
<content:encoded><![CDATA[
arXiv:2508.21112v3 Announce Type: replace-cross 
Abstract: The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs</title>
<link>https://arxiv.org/abs/2508.21393</link>
<guid>https://arxiv.org/abs/2508.21393</guid>
<content:encoded><![CDATA[
arXiv:2508.21393v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) is crucial for adapting them to specific tasks, yet it remains computationally demanding and raises concerns about correctness and privacy, particularly in untrusted environments. Although parameter-efficient methods like Low-Rank Adaptation (LoRA) significantly reduce resource requirements, ensuring the security and verifiability of fine-tuning under zero-knowledge constraints remains an unresolved challenge. To address this, we introduce zkLoRA, the first framework to integrate LoRA fine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and correctness. zkLoRA employs advanced cryptographic techniques -- such as lookup arguments, sumcheck protocols, and polynomial commitments -- to verify both arithmetic and non-arithmetic operations in Transformer-based architectures. The framework provides end-to-end verifiability for forward propagation, backward propagation, and parameter updates during LoRA fine-tuning, while safeguarding the privacy of model parameters and training data. Leveraging GPU-based implementations, zkLoRA demonstrates practicality and efficiency through experimental validation on open-source LLMs like LLaMA, scaling up to 13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs, zkLoRA bridges a critical gap, enabling secure and trustworthy deployment of LLMs in sensitive or untrusted environments.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pilot Study on Generative AI and Critical Thinking in Higher Education Classrooms</title>
<link>https://arxiv.org/abs/2509.00167</link>
<guid>https://arxiv.org/abs/2509.00167</guid>
<content:encoded><![CDATA[
arXiv:2509.00167v3 Announce Type: replace-cross 
Abstract: Generative AI (GAI) tools have seen rapid adoption in educational settings, yet their role in fostering critical thinking remains underexplored. While previous studies have examined GAI as a tutor for specific lessons or as a tool for completing assignments, few have addressed how students critically evaluate the accuracy and appropriateness of GAI-generated responses. This pilot study investigates students' ability to apply structured critical thinking when assessing Generative AI outputs in introductory Computational and Data Science courses. Given that GAI tools often produce contextually flawed or factually incorrect answers, we designed learning activities that require students to analyze, critique, and revise AI-generated solutions. Our findings offer initial insights into students' ability to engage critically with GAI content and lay the groundwork for more comprehensive studies in future semesters.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models</title>
<link>https://arxiv.org/abs/2509.01535</link>
<guid>https://arxiv.org/abs/2509.01535</guid>
<content:encoded><![CDATA[
arXiv:2509.01535v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success across various domains. However, a fundamental question remains: Can LLMs effectively utilize causal knowledge for prediction and generation? Through empirical studies, we find that LLMs trained directly on large-scale data often capture spurious correlations rather than true causal relationships, leading to suboptimal performance, especially in out-of-distribution (OOD) scenarios. To address this challenge, we propose Causal Attention Tuning (CAT), a novel approach that injects fine-grained causal knowledge into the attention mechanism. We propose an automated pipeline that leverages human priors to automatically generate token-level causal signals and introduce the Re-Attention mechanism to guide training, helping the model focus on causal structures while mitigating noise and biases in attention scores. Experimental results on our proposed Spurious Token Game (STG) benchmark and multiple downstream tasks demonstrate that our approach effectively leverages causal knowledge for prediction and remains robust in OOD scenarios. The CAT achieves an average improvement of 5.76% on the STG dataset and 1.56% on downstream tasks. Notably, the OOD performance of the Llama-3.1-8B model on STG_M increased from 64.5% to 90.5%, and Qwen's OOD performance on the STG_H dataset improved from 25.4% to 55.9%. Implementation details can be found at https://github.com/Kairong-Han/CAT.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices</title>
<link>https://arxiv.org/abs/2509.01839</link>
<guid>https://arxiv.org/abs/2509.01839</guid>
<content:encoded><![CDATA[
arXiv:2509.01839v3 Announce Type: replace-cross 
Abstract: Currently, prominent Transformer architectures applied on graphs and meshes for shape analysis tasks employ traditional attention layers that heavily utilize spectral features requiring costly eigenvalue decomposition-based methods. To encode the mesh structure, these methods derive positional embeddings, that heavily rely on eigenvalue decomposition based operations, e.g. on the Laplacian matrix, or on heat-kernel signatures, which are then concatenated to the input features. This paper proposes a novel approach inspired by the explicit construction of the Hodge Laplacian operator in Discrete Exterior Calculus as a product of discrete Hodge operators and exterior derivatives, i.e. $(L := \star_0^{-1} d_0^T \star_1 d_0)$. We adjust the Transformer architecture in a novel deep learning layer that utilizes the multi-head attention mechanism to approximate Hodge matrices $\star_0$, $\star_1$ and $\star_2$ and learn families of discrete operators $L$ that act on mesh vertices, edges and faces. Our approach results in a computationally-efficient architecture that achieves comparable performance in mesh segmentation and classification tasks, through a direct learning framework, while eliminating the need for costly eigenvalue decomposition operations or complex preprocessing operations.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robotic Fire Risk Detection based on Dynamic Knowledge Graph Reasoning: An LLM-Driven Approach with Graph Chain-of-Thought</title>
<link>https://arxiv.org/abs/2509.00054</link>
<guid>https://arxiv.org/abs/2509.00054</guid>
<content:encoded><![CDATA[
<div> Keywords: fire prevention, emergency robots, knowledge graph, large language models, risk detection

Summary:
The article discusses the use of emergency robots in fire scenarios to minimize danger to human responders. It highlights the challenges faced in current research, such as incomplete perception and delayed response. To address these challenges, the authors propose a framework called Insights-on-Graph (IOG) that integrates knowledge graphs and large language models. The framework utilizes real-time imagery to generate perception-driven risk graphs for early fire risk detection. It also provides interpretable responses for emergency decision-making based on the evolving risk situation. Extensive simulations and real-world experiments demonstrate the effectiveness of IOG in fire risk detection and rescue decision-making. Overall, the framework enhances intelligent perception and response planning for robots in fire scenarios, making it a valuable tool for improving emergency response in fire-risk scenarios.<br /><br />Summary: <div>
arXiv:2509.00054v2 Announce Type: replace-cross 
Abstract: Fire is a highly destructive disaster, but effective prevention can significantly reduce its likelihood of occurrence. When it happens, deploying emergency robots in fire-risk scenarios can help minimize the danger to human responders. However, current research on pre-disaster warnings and disaster-time rescue still faces significant challenges due to incomplete perception, inadequate fire situational awareness, and delayed response. To enhance intelligent perception and response planning for robots in fire scenarios, we first construct a knowledge graph (KG) by leveraging large language models (LLMs) to integrate fire domain knowledge derived from fire prevention guidelines and fire rescue task information from robotic emergency response documents. We then propose a new framework called Insights-on-Graph (IOG), which integrates the structured fire information of KG and Large Multimodal Models (LMMs). The framework generates perception-driven risk graphs from real-time scene imagery to enable early fire risk detection and provide interpretable emergency responses for task module and robot component configuration based on the evolving risk situation. Extensive simulations and real-world experiments show that IOG has good applicability and practical application value in fire risk detection and rescue decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI be Auditable?</title>
<link>https://arxiv.org/abs/2509.00575</link>
<guid>https://arxiv.org/abs/2509.00575</guid>
<content:encoded><![CDATA[
<div> auditability, AI systems, regulatory frameworks, documentation, governance

Summary:
The chapter delves into the concept of auditability in AI systems, focusing on the ability to assess compliance with ethical, legal, and technical standards throughout their lifecycle. It discusses the formalization of auditability in emerging regulatory frameworks like the EU AI Act that require documentation, risk assessments, and governance structures. Key challenges identified include technical opacity, inconsistent documentation practices, lack of standardized audit tools and metrics, and conflicting principles within responsible AI frameworks. It emphasizes the necessity of clear guidelines, international regulations, and robust socio-technical methodologies to implement auditability at scale. The chapter underscores the significance of multi-stakeholder collaboration and auditor empowerment in establishing an effective AI audit ecosystem. It argues for embedding auditability in AI development practices and governance infrastructures to ensure alignment with ethical and legal standards. 

<br /><br />Summary: <div>
arXiv:2509.00575v2 Announce Type: replace-cross 
Abstract: Auditability is defined as the capacity of AI systems to be independently assessed for compliance with ethical, legal, and technical standards throughout their lifecycle. The chapter explores how auditability is being formalized through emerging regulatory frameworks, such as the EU AI Act, which mandate documentation, risk assessments, and governance structures. It analyzes the diverse challenges facing AI auditability, including technical opacity, inconsistent documentation practices, lack of standardized audit tools and metrics, and conflicting principles within existing responsible AI frameworks. The discussion highlights the need for clear guidelines, harmonized international regulations, and robust socio-technical methodologies to operationalize auditability at scale. The chapter concludes by emphasizing the importance of multi-stakeholder collaboration and auditor empowerment in building an effective AI audit ecosystem. It argues that auditability must be embedded in AI development practices and governance infrastructures to ensure that AI systems are not only functional but also ethically and legally aligned.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving</title>
<link>https://arxiv.org/abs/2509.01083</link>
<guid>https://arxiv.org/abs/2509.01083</guid>
<content:encoded><![CDATA[
<div> Keywords: Speculative decoding, Language model, Dynamic adaptation, Kullback-Leibler divergence, Latency<br />
Summary:<br />
- The paper introduces Dynamic Speculative Decoding Engine (DSDE), a novel framework for dynamic adaptation in large language model inference.
- DSDE utilizes a predictive signal based on the variance of Kullback-Leibler divergence to assess the stability of generated text regions.
- An adaptive speculation length cap is employed to address delays in per-sequence decoding, known as the straggler problem.
- Experimental results show the effectiveness of KLD-based stability signals for dynamic adaptation, leading to competitive end-to-end latency compared to existing methods.
- The proposed framework demonstrates superior robustness in diverse workloads, especially in low-acceptance-rate scenarios, thus highlighting the value of post-hoc signals in enhancing language model inference systems. <br /> 
Summary: <div>
arXiv:2509.01083v2 Announce Type: replace-cross 
Abstract: Speculative decoding accelerates large language model inference, but its reliance on a fixed speculation length is suboptimal in large-batch serving environments with diverse requests. This paper explores a new direction for dynamic adaptation by investigating a novel class of post-hoc, diagnostic signals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free framework built on two primary components: (1) a predictive signal based on the variance of the Kullback-Leibler (KLD) divergence, which diagnoses the generation's regional stability, and (2) an adaptive speculation length cap to mitigate the straggler problem in per-sequence decoding. Experiments demonstrate the potential of using KLD-based stability signals for dynamic adaptation. An algorithm guided by these signals achieves end-to-end latency competitive with leading baselines and exhibits superior robustness across diverse workloads. This robustness is particularly valuable in challenging low-acceptance-rate regimes, where the proposed signal maintains its diagnostic utility. Collectively, these findings validate post-hoc signals as a valuable component for building more robust and intelligent LLM inference systems, and highlight a promising direction for future research on dynamic speculation length adaptation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCPO: Dynamic Clipping Policy Optimization</title>
<link>https://arxiv.org/abs/2509.02333</link>
<guid>https://arxiv.org/abs/2509.02333</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Verifiable Rewards, Dynamic Clipping Policy Optimization, Large Language Models, Performance Improvement 

Summary:  
- The study addresses the limitations of existing reinforcement learning frameworks such as GRPO in effectively utilizing generated responses due to zero gradients and fixed clipping bounds.
- A novel approach, Dynamic Clipping Policy Optimization (DCPO), is proposed to enhance token-level exploration and response-level utilization through adaptive clipping bounds and reward standardization techniques.
- DCPO outperformed existing methods on four benchmarks using different models, achieving significant performance improvements.
- Results showed DCPO's effectiveness in improving training efficiency, reducing token clipping ratios, and enhancing nonzero advantage over GRPO in large language models.
- The study highlights the superior performance of DCPO in leveraging generated data more efficiently for reinforcement learning, showcasing its potential in enhancing reasoning capabilities of large language models.

<br /><br />Summary: <div>
arXiv:2509.02333v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization(DCPO), which introduces a dynamic clipping strategy that adaptively adjusts clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing DAPO (36.7/31.6), GRPO (36.7/32.1) and GSPO (40.0/34.9) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5), DAPO (20.0/15.3) and GSPO (16.7/9.9). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds</title>
<link>https://arxiv.org/abs/2509.02499</link>
<guid>https://arxiv.org/abs/2509.02499</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, trustworthy AI-generated text detection, Mixture of Stylistic Experts (MoSEs), uncertainty quantification, conditional threshold estimation <br />
Summary: 
The paper introduces the Mixture of Stylistic Experts (MoSEs) framework to enhance AI-generated text detection systems by incorporating stylistic modeling and conditional threshold estimation. MoSEs consists of the Stylistics Reference Repository (SRR), the Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE). SRR selects relevant reference data and provides them to CTE, which dynamically determines optimal thresholds based on linguistic and semantic features. This approach enables stylistics-aware uncertainty quantification and improves detection performance by 11.34% on average and 39.15% in low-resource scenarios. The code for MoSEs is available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2509.02499v3 Announce Type: replace-cross 
Abstract: The rapid advancement of large language models has intensified public concerns about the potential misuse. Therefore, it is important to build trustworthy AI-generated text detection systems. Existing methods neglect stylistic modeling and mostly rely on static thresholds, which greatly limits the detection performance. In this paper, we propose the Mixture of Stylistic Experts (MoSEs) framework that enables stylistics-aware uncertainty quantification through conditional threshold estimation. MoSEs contain three core components, namely, the Stylistics Reference Repository (SRR), the Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE). For input text, SRR can activate the appropriate reference data in SRR and provide them to CTE. Subsequently, CTE jointly models the linguistic statistical properties and semantic features to dynamically determine the optimal threshold. With a discrimination score, MoSEs yields prediction labels with the corresponding confidence level. Our framework achieves an average improvement 11.34% in detection performance compared to baselines. More inspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource case. Our code is available at https://github.com/creator-xi/MoSEs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts</title>
<link>https://arxiv.org/abs/2509.05323</link>
<guid>https://arxiv.org/abs/2509.05323</guid>
<content:encoded><![CDATA[
<div> Keywords: attention mechanisms, video diffusion transformers, generative video models, Explainable AI, artistic exploration

Summary: 
This paper delves into the attention mechanisms of video diffusion transformers, drawing inspiration from early video art. It introduces a method for extracting and visualizing cross-attention maps in generative video models, showcasing the temporal and spatial behavior of attention in text-to-video generation. Utilizing the Wan model, the research tool offers insights into attention patterns, serving as both analytical aids and creative material for artists. The study, through exploratory probes and an artistic case study, highlights the potential of attention maps in understanding AI processes in an artistic context. By contributing to the field of Explainable AI for the Arts (XAIxArts), this work encourages artists to engage with AI as a tool for creativity and expression.<br /><br />Summary: <div>
arXiv:2509.05323v1 Announce Type: new 
Abstract: This paper presents an artistic and technical investigation into the attention mechanisms of video diffusion transformers. Inspired by early video artists who manipulated analog video signals to create new visual aesthetics, this study proposes a method for extracting and visualizing cross-attention maps in generative video models. Built on the open-source Wan model, our tool provides an interpretable window into the temporal and spatial behavior of attention in text-to-video generation. Through exploratory probes and an artistic case study, we examine the potential of attention maps as both analytical tools and raw artistic material. This work contributes to the growing field of Explainable AI for the Arts (XAIxArts), inviting artists to reclaim the inner workings of AI as a creative medium.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception Graph for Cognitive Attack Reasoning in Augmented Reality</title>
<link>https://arxiv.org/abs/2509.05324</link>
<guid>https://arxiv.org/abs/2509.05324</guid>
<content:encoded><![CDATA[
<div> Keywords: augmented reality, cognitive attacks, Perception Graph, human perception, MR environment

Summary:
The article introduces the Perception Graph model to address cognitive attacks on augmented reality (AR) systems in tactical environments. This model is designed to mimic human perception processes in interpreting information from an AR environment and represent the outcomes in a meaningful structure. By computing a quantitative score, the model can detect and analyze the level of perception distortion caused by cognitive attacks, providing a measurable method for assessing their effects. This approach aims to enhance user decision-making by identifying and mitigating manipulation of perception within AR systems. The Perception Graph model offers a systematic framework for understanding, detecting, and quantifying the impact of cognitive attacks, thereby improving the overall security and reliability of AR systems in tactical settings. 

<br /><br />Summary: <div>
arXiv:2509.05324v1 Announce Type: new 
Abstract: Augmented reality (AR) systems are increasingly deployed in tactical environments, but their reliance on seamless human-computer interaction makes them vulnerable to cognitive attacks that manipulate a user's perception and severely compromise user decision-making. To address this challenge, we introduce the Perception Graph, a novel model designed to reason about human perception within these systems. Our model operates by first mimicking the human process of interpreting key information from an MR environment and then representing the outcomes using a semantically meaningful structure. We demonstrate how the model can compute a quantitative score that reflects the level of perception distortion, providing a robust and measurable method for detecting and analyzing the effects of such cognitive attacks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynDelay: A Synthetic Dataset for Delivery Delay Prediction</title>
<link>https://arxiv.org/abs/2509.05325</link>
<guid>https://arxiv.org/abs/2509.05325</guid>
<content:encoded><![CDATA[
<div> dataset, AI, supply chain management, delivery delay prediction, synthetic

Summary: 
The article introduces SynDelay, a synthetic dataset created for delivery delay prediction in supply chain management using artificial intelligence (AI). Existing datasets for predictive tasks in supply chain management are limited in quality and availability, hindering progress in the field. SynDelay overcomes these limitations by being publicly accessible, privacy-preserving, and realistic enough to present a challenge for predictive modelling. The dataset is generated using a sophisticated generative model trained on real-world data, maintaining accurate delivery patterns while ensuring data privacy. Baseline results and evaluation metrics are provided to offer initial benchmarks for researchers, promoting reproducibility and benchmarking in the field. The dataset is available through the Supply Chain Data Hub, an open initiative that encourages dataset sharing and collaboration in supply chain AI research. The code for SynDelay is openly accessible, providing a foundation for further research and development in supply chain management AI. <div>
arXiv:2509.05325v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is transforming supply chain management, yet progress in predictive tasks -- such as delivery delay prediction -- remains constrained by the scarcity of high-quality, openly available datasets. Existing datasets are often proprietary, small, or inconsistently maintained, hindering reproducibility and benchmarking. We present SynDelay, a synthetic dataset designed for delivery delay prediction. Generated using an advanced generative model trained on real-world data, SynDelay preserves realistic delivery patterns while ensuring privacy. Although not entirely free of noise or inconsistencies, it provides a challenging and practical testbed for advancing predictive modelling. To support adoption, we provide baseline results and evaluation metrics as initial benchmarks, serving as reference points rather than state-of-the-art claims. SynDelay is publicly available through the Supply Chain Data Hub, an open initiative promoting dataset sharing and benchmarking in supply chain AI. We encourage the community to contribute datasets, models, and evaluation practices to advance research in this area. All code is openly accessible at https://supplychaindatahub.org.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset</title>
<link>https://arxiv.org/abs/2509.05330</link>
<guid>https://arxiv.org/abs/2509.05330</guid>
<content:encoded><![CDATA[
<div> Dataset, Emotion recognition, Multimodal, VR, Affective computing

Summary:
- The article introduces the MVRS dataset for automatic emotion recognition, addressing the lack of multimodal datasets in the field.
- The dataset includes synchronized recordings of participants exposed to various emotional stimuli in virtual reality, capturing body motion, physiological signals, and eye tracking.
- Data collection involved 13 participants across different age groups following a unified protocol with consent and questionnaires.
- Features extracted from each modality were fused using early and late fusion techniques and evaluated using classifiers to confirm the dataset's quality and emotion separability.
- The MVRS dataset is a valuable contribution to multimodal affective computing, allowing for advancements in fields like healthcare, education, and automotive systems. 

<br /><br />Summary: <div>
arXiv:2509.05330v1 Announce Type: new 
Abstract: Automatic emotion recognition has become increasingly important with the rise of AI, especially in fields like healthcare, education, and automotive systems. However, there is a lack of multimodal datasets, particularly involving body motion and physiological signals, which limits progress in the field. To address this, the MVRS dataset is introduced, featuring synchronized recordings from 13 participants aged 12 to 60 exposed to VR based emotional stimuli (relaxation, fear, stress, sadness, joy). Data were collected using eye tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR signals (Arduino UNO), all timestamp aligned. Participants followed a unified protocol with consent and questionnaires. Features from each modality were extracted, fused using early and late fusion techniques, and evaluated with classifiers to confirm the datasets quality and emotion separability, making MVRS a valuable contribution to multimodal affective computing.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning</title>
<link>https://arxiv.org/abs/2509.05346</link>
<guid>https://arxiv.org/abs/2509.05346</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, personalized learning, tutoring task, Bradley-Terry model, individualized support 

Summary: 
This study compares three state-of-the-art Large Language Models (LLMs) on a tutoring task simulating a realistic learning scenario. Each LLM analyzes a student's quiz answers, identifies knowledge components, infers mastery, and provides guidance. Gemini is used as a virtual judge for impartial evaluations. Results show that GPT-4o is preferred for producing more informative and structured feedback. DeepSeek-V3 and GLM-4.5 have intermittent strengths but lower consistency. The findings suggest LLMs can be useful as advanced teaching assistants for personalized support, with potential for future research on LLM-driven personalized learning.<br /><br />Summary: <div>
arXiv:2509.05346v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) are increasingly envisioned as intelligent assistants for personalized learning, systematic head-to-head evaluations within authentic learning scenarios remain limited. This study conducts an empirical comparison of three state-of-the-art LLMs on a tutoring task that simulates a realistic learning setting. Using a dataset comprising a student's answers to ten questions of mixed formats with correctness labels, each LLM is required to (i) analyze the quiz to identify underlying knowledge components, (ii) infer the student's mastery profile, and (iii) generate targeted guidance for improvement. To mitigate subjectivity and evaluator bias, we employ Gemini as a virtual judge to perform pairwise comparisons along various dimensions: accuracy, clarity, actionability, and appropriateness. Results analyzed via the Bradley-Terry model indicate that GPT-4o is generally preferred, producing feedback that is more informative and better structured than its counterparts, while DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower consistency. These findings highlight the feasibility of deploying LLMs as advanced teaching assistants for individualized support and provide methodological guidance for future empirical research on LLM-driven personalized learning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis</title>
<link>https://arxiv.org/abs/2509.05363</link>
<guid>https://arxiv.org/abs/2509.05363</guid>
<content:encoded><![CDATA[
<div> Keywords: SasAgent, multi-agent AI system, small-angle scattering, LLM, automation

Summary:
SasAgent is a multi-agent AI system powered by large language models (LLMs) that automates small-angle scattering (SAS) data analysis. It utilizes tools from the SasView software and enables user interaction via text input. The system features a coordinator agent that interprets user prompts and delegates tasks to specialized agents for scattering length density (SLD) calculation, synthetic data generation, and experimental data fitting. These agents efficiently execute tasks using LLM-friendly tools derived from the SasView Python library. A user-friendly Gradio-based interface enhances accessibility. Through various examples, SasAgent demonstrates its ability to interpret complex prompts, calculate SLDs, generate accurate scattering data, and fit experimental datasets with high precision. This work showcases the potential of LLM-driven AI systems to streamline scientific workflows and enhance automation in SAS research.

<br /><br />Summary: <div>
arXiv:2509.05363v1 Announce Type: new 
Abstract: We introduce SasAgent, a multi-agent AI system powered by large language models (LLMs) that automates small-angle scattering (SAS) data analysis by leveraging tools from the SasView software and enables user interaction via text input. SasAgent features a coordinator agent that interprets user prompts and delegates tasks to three specialized agents for scattering length density (SLD) calculation, synthetic data generation, and experimental data fitting. These agents utilize LLM-friendly tools to execute tasks efficiently. These tools, including the model data tool, Retrieval-Augmented Generation (RAG) documentation tool, bump fitting tool, and SLD calculator tool, are derived from the SasView Python library. A user-friendly Gradio-based interface enhances user accessibility. Through diverse examples, we demonstrate SasAgent's ability to interpret complex prompts, calculate SLDs, generate accurate scattering data, and fit experimental datasets with high precision. This work showcases the potential of LLM-driven AI systems to streamline scientific workflows and enhance automation in SAS research.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Fitness Landscape Structures in Prompt Engineering</title>
<link>https://arxiv.org/abs/2509.05375</link>
<guid>https://arxiv.org/abs/2509.05375</guid>
<content:encoded><![CDATA[
<div> optimization landscape, prompt engineering, autocorrelation analysis, semantic embedding spaces, error detection

Summary:
The study focuses on analyzing fitness landscape structures in prompt engineering using autocorrelation analysis in semantic embedding spaces. It compares two prompt generation strategies, systematic enumeration, and novelty-driven diversification, revealing different landscape topologies. Systematic prompt generation results in smoothly decaying autocorrelation, while diversified generation shows non-monotonic patterns with peak correlation at intermediate semantic distances, indicating rugged, hierarchically structured landscapes. The research also examines the ruggedness of landscapes across different error detection categories, highlighting varying degrees of complexity. These findings offer insights into the optimization complexity of prompt engineering landscapes. 

<br /><br />Summary: <div>
arXiv:2509.05375v1 Announce Type: new 
Abstract: While prompt engineering has emerged as a crucial technique for optimizing large language model performance, the underlying optimization landscape remains poorly understood. Current approaches treat prompt optimization as a black-box problem, applying sophisticated search algorithms without characterizing the landscape topology they navigate. We present a systematic analysis of fitness landscape structures in prompt engineering using autocorrelation analysis across semantic embedding spaces. Through experiments on error detection tasks with two distinct prompt generation strategies -- systematic enumeration (1,024 prompts) and novelty-driven diversification (1,000 prompts) -- we reveal fundamentally different landscape topologies. Systematic prompt generation yields smoothly decaying autocorrelation, while diversified generation exhibits non-monotonic patterns with peak correlation at intermediate semantic distances, indicating rugged, hierarchically structured landscapes. Task-specific analysis across 10 error detection categories reveals varying degrees of ruggedness across different error types. Our findings provide an empirical foundation for understanding the complexity of optimization in prompt engineering landscapes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Like Humans: A Multi-Agent Solution for Medical Coding</title>
<link>https://arxiv.org/abs/2509.05378</link>
<guid>https://arxiv.org/abs/2509.05378</guid>
<content:encoded><![CDATA[
<div> Keywords: medical coding, language models, ICD-10, diagnosis codes, agentic framework

Summary:
Code Like Humans is a new framework for medical coding using large language models that follows official coding guidelines and supports the full ICD-10 coding system. It outperforms previous solutions by achieving the best performance on rare diagnosis codes, while still benefiting from fine-tuned discriminative classifiers for high-frequency codes. The framework provides an analysis of its system performance and identifies areas where it may be undercoding certain codes. This new approach showcases the potential for incorporating advanced language models in medical coding systems, offering improved accuracy and efficiency in mapping clinical notes to alphanumeric codes. Overall, Code Like Humans presents a promising step towards enhancing the capabilities of medical coding systems and addressing potential limitations in current coding practices.

<br /><br />Summary: <div>
arXiv:2509.05378v1 Announce Type: new 
Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric codes for diagnoses and procedures. We introduce Code Like Humans: a new agentic framework for medical coding with large language models. It implements official coding guidelines for human experts, and it is the first solution that can support the full ICD-10 coding system (+70K labels). It achieves the best performance to date on rare diagnosis codes (fine-tuned discriminative classifiers retain an advantage for high-frequency codes, to which they are limited). Towards future work, we also contribute an analysis of system performance and identify its `blind spots' (codes that are systematically undercoded).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Murphys Laws of AI Alignment: Why the Gap Always Wins</title>
<link>https://arxiv.org/abs/2509.05381</link>
<guid>https://arxiv.org/abs/2509.05381</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning from Human Feedback, Large Language Models, Alignment Gap, Feedback-Based Alignment, Murphys Laws<br />
Summary:<br />
Large language models are increasingly aligned to human preferences through methods like reinforcement learning from human feedback (RLHF). However, these methods often fail due to reward hacking, sycophancy, annotator drift, and misgeneralization. The concept of the Alignment Gap is introduced as a way to understand these failures, showing how optimization pressure can amplify divergence between proxy rewards and true human intent. The Murphys Laws of AI Alignment categorize these failures, while the Alignment Trilemma frames trade-offs among optimization strength, value capture, and generalization. The proposed MAPS framework (Misspecification, Annotation, Pressure, Shift) offers practical design levers to address alignment challenges. This perspective reframes alignment debates around structural limits and trade-offs, providing clearer guidance for future design efforts. <br /> <div>
arXiv:2509.05381v1 Announce Type: new 
Abstract: Large language models are increasingly aligned to human preferences through reinforcement learning from human feedback (RLHF) and related methods such as Direct Preference Optimization (DPO), Constitutional AI, and RLAIF. While effective, these methods exhibit recurring failure patterns i.e., reward hacking, sycophancy, annotator drift, and misgeneralization. We introduce the concept of the Alignment Gap, a unifying lens for understanding recurring failures in feedback-based alignment. Using a KL-tilting formalism, we illustrate why optimization pressure tends to amplify divergence between proxy rewards and true human intent. We organize these failures into a catalogue of Murphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to frame trade-offs among optimization strength, value capture, and generalization. Small-scale empirical studies serve as illustrative support. Finally, we propose the MAPS framework (Misspecification, Annotation, Pressure, Shift) as practical design levers. Our contribution is not a definitive impossibility theorem but a perspective that reframes alignment debates around structural limits and trade-offs, offering clearer guidance for future design.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation</title>
<link>https://arxiv.org/abs/2509.05469</link>
<guid>https://arxiv.org/abs/2509.05469</guid>
<content:encoded><![CDATA[
<div> AI-assisted generative design, street-view imagery, multi-agent system, bicycle facilities, transportation infrastructure planning<br />
Summary: 
The article introduces a multi-agent system that edits and redesigns bicycle facilities on real-world street-view imagery, enabling rapid creation of design scenarios for active transportation planning. This framework integrates lane localization, prompt optimization, design generation, and automated evaluation to synthesize contextually appropriate designs. It adapts to varying road geometries and environmental conditions, consistently producing visually coherent and instruction-compliant results across diverse urban scenarios. The system's ability to enable precise spatial variations of design configurations in complex street-view scenes showcases its transformative potential in transportation infrastructure planning. This work establishes a groundwork for utilizing multi-agent pipelines in facility design and urban planning. <br /><br /> <div>
arXiv:2509.05469v1 Announce Type: new 
Abstract: Realistic visual renderings of street-design scenarios are essential for public engagement in active transportation planning. Traditional approaches are labor-intensive, hindering collective deliberation and collaborative decision-making. While AI-assisted generative design shows transformative potential by enabling rapid creation of design scenarios, existing generative approaches typically require large amounts of domain-specific training data and struggle to enable precise spatial variations of design/configuration in complex street-view scenes. We introduce a multi-agent system that edits and redesigns bicycle facilities directly on real-world street-view imagery. The framework integrates lane localization, prompt optimization, design generation, and automated evaluation to synthesize realistic, contextually appropriate designs. Experiments across diverse urban scenarios demonstrate that the system can adapt to varying road geometries and environmental conditions, consistently yielding visually coherent and instruction-compliant results. This work establishes a foundation for applying multi-agent pipelines to transportation infrastructure planning and facility design.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation</title>
<link>https://arxiv.org/abs/2509.05550</link>
<guid>https://arxiv.org/abs/2509.05550</guid>
<content:encoded><![CDATA[
<div> TreeGPT, transformer-based architecture, AST processing, neural program synthesis, attention mechanisms, TreeFFN, global parent-child aggregation<br />
<br />
Summary:<br />
TreeGPT is a novel neural architecture for neural program synthesis tasks, combining transformer-based attention mechanisms with global parent-child aggregation. It utilizes a hybrid design with self-attention and a Tree Feed-Forward Network for processing Abstract Syntax Trees. The Global Parent-Child Aggregation mechanism enables nodes to aggregate information from the entire tree structure through iterations. Optional enhancements like gated aggregation, residual connections, and bidirectional propagation further improve performance. Evaluation on the ARC Prize 2025 dataset shows TreeGPT achieving 96% accuracy, outperforming transformer baselines, large-scale models, and specialized program synthesis methods. Ablation study highlights the importance of edge projection and gating for optimal performance. <div>
arXiv:2509.05550v1 Announce Type: new 
Abstract: We introduce TreeGPT, a novel neural architecture that combines transformer-based attention mechanisms with global parent-child aggregation for processing Abstract Syntax Trees (ASTs) in neural program synthesis tasks. Unlike traditional approaches that rely solely on sequential processing or graph neural networks, TreeGPT employs a hybrid design that leverages both self-attention for capturing local dependencies and a specialized Tree Feed-Forward Network (TreeFFN) for modeling hierarchical tree structures through iterative message passing.
  The core innovation lies in our Global Parent-Child Aggregation mechanism, formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \in E_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents the hidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges involving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This formulation enables each node to progressively aggregate information from the entire tree structure through $T$ iterations.
  Our architecture integrates optional enhancements including gated aggregation with learnable edge weights, residual connections for gradient stability, and bidirectional propagation for capturing both bottom-up and top-down dependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging visual reasoning benchmark requiring abstract pattern recognition and rule inference. Experimental results demonstrate that TreeGPT achieves 96\% accuracy, significantly outperforming transformer baselines (1.3\%), large-scale models like Grok-4 (15.9\%), and specialized program synthesis methods like SOAR (52\%) while using only 1.5M parameters. Our comprehensive ablation study reveals that edge projection is the most critical component, with the combination of edge projection and gating achieving optimal performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision</title>
<link>https://arxiv.org/abs/2509.05578</link>
<guid>https://arxiv.org/abs/2509.05578</guid>
<content:encoded><![CDATA[
<div> Framework, 3D spatial understanding, Multimodal reasoning, OccVLA, Autonomous driving
Summary:
OccVLA is a new framework that addresses the challenges faced by multimodal large language models (MLLMs) in achieving robust 3D spatial understanding for autonomous driving. By integrating 3D occupancy representations into the reasoning process, OccVLA overcomes the limitations of constructing 3D representations without manual annotations and losing fine-grained spatial details in vision-language models. It treats dense 3D occupancy as both predictive output and supervisory signal, enabling the model to learn spatial structures directly from 2D visual inputs. The occupancy predictions serve as implicit reasoning processes without extra computational overhead. OccVLA outperforms existing methods on the nuScenes benchmark for trajectory planning and excels in 3D visual question-answering tasks. It provides a scalable, interpretable, and fully vision-based solution for autonomous driving. <br /><br />Summary: <div>
arXiv:2509.05578v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have shown strong vision-language reasoning abilities but still lack robust 3D spatial understanding, which is critical for autonomous driving. This limitation stems from two key challenges: (1) the difficulty of constructing accessible yet effective 3D representations without expensive manual annotations, and (2) the loss of fine-grained spatial details in VLMs due to the absence of large-scale 3D vision-language pretraining. To address these challenges, we propose OccVLA, a novel framework that integrates 3D occupancy representations into a unified multimodal reasoning process. Unlike prior approaches that rely on explicit 3D inputs, OccVLA treats dense 3D occupancy as both a predictive output and a supervisory signal, enabling the model to learn fine-grained spatial structures directly from 2D visual inputs. The occupancy predictions are regarded as implicit reasoning processes and can be skipped during inference without performance degradation, thereby adding no extra computational overhead. OccVLA achieves state-of-the-art results on the nuScenes benchmark for trajectory planning and demonstrates superior performance on 3D visual question-answering tasks, offering a scalable, interpretable, and fully vision-based solution for autonomous driving.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions</title>
<link>https://arxiv.org/abs/2509.05685</link>
<guid>https://arxiv.org/abs/2509.05685</guid>
<content:encoded><![CDATA[
<div> deep learning, road network analysis, graph neural networks, spatial interactions, trajectory data

Summary: 
The paper presents MSRFormer, a novel road network representation learning framework that addresses the challenges posed by urban road networks' heterogeneous and hierarchical nature. It integrates multi-scale spatial interactions by using spatial flow convolution to extract small-scale features from large trajectory datasets and identifying scale-dependent spatial interaction regions. By employing a graph transformer, MSRFormer captures complex spatial dependencies across multiple scales. The spatial interaction features are fused using residual connections and fed to a contrastive learning algorithm to derive the final road network representation. Validation on real-world datasets shows that MSRFormer outperforms baseline methods in road network analysis tasks, with up to 16% improvements in complex road network structures. The research provides a practical framework for developing task-agnostic road network representation models and highlights the interplay between scale effects and flow heterogeneity of spatial interactions. 

<br /><br />Summary: <div>
arXiv:2509.05685v1 Announce Type: new 
Abstract: Transforming road network data into vector representations using deep learning has proven effective for road network analysis. However, urban road networks' heterogeneous and hierarchical nature poses challenges for accurate representation learning. Graph neural networks, which aggregate features from neighboring nodes, often struggle due to their homogeneity assumption and focus on a single structural scale. To address these issues, this paper presents MSRFormer, a novel road network representation learning framework that integrates multi-scale spatial interactions by addressing their flow heterogeneity and long-distance dependencies. It uses spatial flow convolution to extract small-scale features from large trajectory datasets, and identifies scale-dependent spatial interaction regions to capture the spatial structure of road networks and flow heterogeneity. By employing a graph transformer, MSRFormer effectively captures complex spatial dependencies across multiple scales. The spatial interaction features are fused using residual connections, which are fed to a contrastive learning algorithm to derive the final road network representation. Validation on two real-world datasets demonstrates that MSRFormer outperforms baseline methods in two road network analysis tasks. The performance gains of MSRFormer suggest the traffic-related task benefits more from incorporating trajectory data, also resulting in greater improvements in complex road network structures with up to 16% improvements compared to the most competitive baseline method. This research provides a practical framework for developing task-agnostic road network representation models and highlights distinct association patterns of the interplay between scale effects and flow heterogeneity of spatial interactions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs</title>
<link>https://arxiv.org/abs/2509.05714</link>
<guid>https://arxiv.org/abs/2509.05714</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge editing, multimodal large language models, meta-cognitive processes, CogEdit, MIND

Summary: 
Meta-cognitive knowledge editing is important for updating outdated or incorrect information in large language models. The CogEdit benchmark evaluates MLLMs' abilities in meta-cognitive knowledge editing across three levels: Counterfactual-Driven Editing, Boundary Constraint Editing, and Noise-Robust Editing. The MIND framework is proposed to enhance meta-cognitive editing by creating a meta-knowledge memory, using game-theoretic interactions for monitoring knowledge activation, and incorporating label refinement for noise-robust updates. Experimental results show that MIND outperforms existing cognitive editing approaches, achieving strong performance on both traditional and meta-cognitive knowledge editing benchmarks. <div>
arXiv:2509.05714v1 Announce Type: new 
Abstract: Knowledge editing enables multimodal large language models (MLLMs) to efficiently update outdated or incorrect information. However, existing benchmarks primarily emphasize cognitive-level modifications while lacking a focus on deeper meta-cognitive processes. To bridge this gap, we introduce CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge editing abilities across three levels: (1) Counterfactual-Driven Editing, assessing self-awareness of knowledge correctness changes; (2) Boundary Constraint Editing, ensuring appropriate generalization without unintended interference; and (3) Noise-Robust Editing, promoting reflective evaluation of uncertain information. To advance meta-cognitive editing, we propose MIND (Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that constructs a meta-knowledge memory for self-awareness, employs game-theoretic interactions to monitor knowledge activation, and incorporates label refinement for noise-robust updates. Extensive experiments show that MIND significantly outperforms existing cognitive editing approaches, achieving strong performance on both traditional and meta-cognitive knowledge editing benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic Large Language Models</title>
<link>https://arxiv.org/abs/2509.05757</link>
<guid>https://arxiv.org/abs/2509.05757</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, hyperbolic geometry, semantic representation learning, multi-scale reasoning, Hierarchical relationships

Summary:
This article discusses the use of hyperbolic geometry as a representation space to enhance semantic representation learning and multi-scale reasoning in Large Language Models (LLMs). It explores the advancements in Hyperbolic LLMs (HypLLMs) through four main categories: hyperbolic LLMs through exp/log maps, hyperbolic fine-tuned models, fully hyperbolic LLMs, and hyperbolic state-space models. The paper also highlights potential applications and future research directions in utilizing hyperbolic geometry for modeling complex data structures. A repository containing key papers, models, datasets, and code implementations for Hyperbolic LLMs is provided at https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main. <div>
arXiv:2509.05757v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success and demonstrated superior performance across various tasks, including natural language processing (NLP), weather forecasting, biological protein folding, text generation, and solving mathematical problems. However, many real-world data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein networks, transportation networks, financial networks, brain networks, and linguistic structures or syntactic trees in natural languages. Effectively learning intrinsic semantic entailment and hierarchical relationships from these raw, unstructured input data using LLMs remains an underexplored area. Due to its effectiveness in modeling tree-like hierarchical structures, hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity as an expressive latent representation space for complex data modeling across domains such as graphs, images, languages, and multi-modal data. Here, we provide a comprehensive and contextual exposition of recent advancements in LLMs that leverage hyperbolic geometry as a representation space to enhance semantic representation learning and multi-scale reasoning. Specifically, the paper presents a taxonomy of the principal techniques of Hyperbolic LLMs (HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4) hyperbolic state-space models. We also explore crucial potential applications and outline future research directions. A repository of key papers, models, datasets, and code implementations is available at https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRF: LLM-AGENT Dynamic Reputation Filtering Framework</title>
<link>https://arxiv.org/abs/2509.05764</link>
<guid>https://arxiv.org/abs/2509.05764</guid>
<content:encoded><![CDATA[
<div> evolution, generative AI, multi-agent systems, large-language models, DRF

Summary:
DRF is a dynamic reputation filtering framework designed to address challenges faced by multi-agent systems using large-language models. It introduces an interactive rating network to quantify agent performance and a reputation scoring mechanism to measure agent honesty and capability. Additionally, DRF integrates an Upper Confidence Bound-based strategy to improve agent selection efficiency. The framework significantly improves task completion quality and collaboration efficiency in logical reasoning and code-generation tasks. It offers a new approach for multi-agent systems to handle large-scale tasks effectively. <br /><br />Summary: <div>
arXiv:2509.05764v1 Announce Type: new 
Abstract: With the evolution of generative AI, multi - agent systems leveraging large - language models(LLMs) have emerged as a powerful tool for complex tasks. However, these systems face challenges in quantifying agent performance and lack mechanisms to assess agent credibility. To address these issues, we introduce DRF, a dynamic reputation filtering framework. DRF constructs an interactive rating network to quantify agent performance, designs a reputation scoring mechanism to measure agent honesty and capability, and integrates an Upper Confidence Bound - based strategy to enhance agent selection efficiency. Experiments show that DRF significantly improves task completion quality and collaboration efficiency in logical reasoning and code - generation tasks, offering a new approach for multi - agent systems to handle large - scale tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation</title>
<link>https://arxiv.org/abs/2509.05772</link>
<guid>https://arxiv.org/abs/2509.05772</guid>
<content:encoded><![CDATA[
<div> Forecasting, Battery Energy Storage System (BESS), Decision-Focused Learning (DFL), Automated Feature Engineering (AFE), Real-world dataset

Summary:
Decision-making in energy management, particularly in Battery Energy Storage System (BESS) operations, is challenging due to uncertainty. The Predict-Then-Optimise (PTO) approach, which separates forecasting and optimization, can lead to suboptimal decisions. Decision-Focused Learning (DFL) integrates prediction and optimization but lacks practical evidence. This study proposes an AFE-DFL framework for forecasting electricity prices and demand while optimizing BESS operations. Testing on a real-world UK property dataset showed that DFL outperformed PTO in reducing operating costs. Additionally, incorporating AFE further improved DFL performance by 22.9-56.5%. These findings suggest that DFL is practical in real-world settings and that AFE enhances DFL efficacy, reducing the need for domain expertise in BESS optimization and providing economic benefits for energy management systems. <div>
arXiv:2509.05772v1 Announce Type: new 
Abstract: Decision-making under uncertainty in energy management is complicated by unknown parameters hindering optimal strategies, particularly in Battery Energy Storage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat forecasting and optimisation as separate processes, allowing prediction errors to cascade into suboptimal decisions as models minimise forecasting errors rather than optimising downstream tasks. The emerging Decision-Focused Learning (DFL) methods overcome this limitation by integrating prediction and optimisation; however, they are relatively new and have been tested primarily on synthetic datasets or small-scale problems, with limited evidence of their practical viability. Real-world BESS applications present additional challenges, including greater variability and data scarcity due to collection constraints and operational limitations. Because of these challenges, this work leverages Automated Feature Engineering (AFE) to extract richer representations and improve the nascent approach of DFL. We propose an AFE-DFL framework suitable for small datasets that forecasts electricity prices and demand while optimising BESS operations to minimise costs. We validate its effectiveness on a novel real-world UK property dataset. The evaluation compares DFL methods against PTO, with and without AFE. The results show that, on average, DFL yields lower operating costs than PTO and adding AFE further improves the performance of DFL methods by 22.9-56.5% compared to the same models without AFE. These findings provide empirical evidence for DFL's practical viability in real-world settings, indicating that domain-specific AFE enhances DFL and reduces reliance on domain expertise for BESS optimisation, yielding economic benefits with broader implications for energy management systems facing similar challenges.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chatbot To Help Patients Understand Their Health</title>
<link>https://arxiv.org/abs/2509.05818</link>
<guid>https://arxiv.org/abs/2509.05818</guid>
<content:encoded><![CDATA[
<div> Keywords: NoteAid-Chatbot, conversational AI, patient education, reinforcement learning, healthcare

Summary:
NoteAid-Chatbot is a conversational AI designed to help patients understand their care through interactive learning. It utilizes a unique 'learning as conversation' framework, leveraging a large language model and reinforcement learning without human-labeled data. The chatbot was trained in two stages: initial supervised fine-tuning on synthetic conversational data and reinforcement learning using patient understanding assessments in hospital discharge scenarios. Evaluation results show that NoteAid-Chatbot demonstrates key behaviors essential for patient education, such as clarity, relevance, and structured dialogue. The use of Proximal Policy Optimization-based reward modeling successfully trains the chatbot to handle multi-turn interactions and meet communication objectives. In a Turing test, NoteAid-Chatbot outperformed non-expert humans, showcasing its effectiveness in patient education. This study highlights the potential of low-cost reinforcement learning in developing domain-specific chatbots for educational purposes and emphasizes the broader applicability of such methods in open-ended conversational domains.<br /><br />Summary: NoteAid-Chatbot employs a 'learning as conversation' framework to enhance patient understanding, demonstrating effectiveness in patient education through clear, relevant, and structured dialogue. Trained using reinforcement learning, the chatbot surpasses non-expert human performance in a Turing test, showcasing its potential for broader application in open-ended conversational domains. <div>
arXiv:2509.05818v1 Announce Type: new 
Abstract: Patients must possess the knowledge necessary to actively participate in their care. We present NoteAid-Chatbot, a conversational AI that promotes patient understanding via a novel 'learning as conversation' framework, built on a multi-agent large language model (LLM) and reinforcement learning (RL) setup without human-labeled data. NoteAid-Chatbot was built on a lightweight LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on conversational data synthetically generated using medical conversation strategies, followed by RL with rewards derived from patient understanding assessments in simulated hospital discharge scenarios. Our evaluation, which includes comprehensive human-aligned assessments and case studies, demonstrates that NoteAid-Chatbot exhibits key emergent behaviors critical for patient education, such as clarity, relevance, and structured dialogue, even though it received no explicit supervision for these attributes. Our results show that even simple Proximal Policy Optimization (PPO)-based reward modeling can successfully train lightweight, domain-specific chatbots to handle multi-turn interactions, incorporate diverse educational strategies, and meet nuanced communication objectives. Our Turing test demonstrates that NoteAid-Chatbot surpasses non-expert human. Although our current focus is on healthcare, the framework we present illustrates the feasibility and promise of applying low-cost, PPO-based RL to realistic, open-ended conversational domains, broadening the applicability of RL-based alignment methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration</title>
<link>https://arxiv.org/abs/2509.05933</link>
<guid>https://arxiv.org/abs/2509.05933</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic AI, geospatial reasoning, multi-agent framework, map integration, hierarchical design

Summary:
MapAgent is a new hierarchical multi-agent framework designed to enhance geospatial reasoning tasks using large language models (LLMs). Unlike existing flat agent-based approaches, MapAgent decouples planning from execution, reducing cognitive load and improving tool selection accuracy. The framework features dedicated map-tool agents for orchestrating map-based services efficiently and in parallel, while simpler modules operate without additional agent overhead. MapAgent outperformed state-of-the-art tool-augmented and agentic baselines on four diverse geospatial benchmarks. The framework is open-source and available at https://github.com/Hasebul/MapAgent.<br /><br />Summary: <div>
arXiv:2509.05933v1 Announce Type: new 
Abstract: Agentic AI has significantly extended the capabilities of large language models (LLMs) by enabling complex reasoning and tool use. However, most existing frameworks are tailored to domains such as mathematics, coding, or web automation, and fall short on geospatial tasks that require spatial reasoning, multi-hop planning, and real-time map interaction. To address these challenges, we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with customized toolsets and agentic scaffolds for map-integrated geospatial reasoning. Unlike existing flat agent-based approaches that treat tools uniformly-often overwhelming the LLM when handling similar but subtly different geospatial APIs-MapAgent decouples planning from execution. A high-level planner decomposes complex queries into subgoals, which are routed to specialized modules. For tool-heavy modules-such as map-based services-we then design a dedicated map-tool agent that efficiently orchestrates related APIs adaptively in parallel to effectively fetch geospatial data relevant for the query, while simpler modules (e.g., solution generation or answer extraction) operate without additional agent overhead. This hierarchical design reduces cognitive load, improves tool selection accuracy, and enables precise coordination across similar APIs. We evaluate MapAgent on four diverse geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented and agentic baselines. We open-source our framwork at https://github.com/Hasebul/MapAgent.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL</title>
<link>https://arxiv.org/abs/2509.06024</link>
<guid>https://arxiv.org/abs/2509.06024</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, reasoning abilities, Chain-of-Thought, reward framework, logical reasoning 

Summary:<br /><br />
Reinforcement learning is being used to enhance the reasoning abilities of large language models, but current reward functions only focus on answer correctness without considering the quality of the reasoning process. A new framework called Dynamic Reasoning Efficiency Reward (DRER) is proposed to address this issue. DRER includes a Reasoning Quality Reward that rewards reasoning chains that improve the likelihood of the correct answer, and a Dynamic Length Advantage that stabilizes training by penalizing responses with excessive length. The framework is evaluated using a deductive reasoning dataset called Logictree. Results show that models trained with DRER achieve improved performance on the dataset and demonstrate generalization across different logical-reasoning tasks. The release of the code and data allows for easy replication of the experiments, providing insights into how reinforcement learning can enhance formal reasoning skills in language models. <div>
arXiv:2509.06024v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has recently become the dominant paradigm for strengthening the reasoning abilities of large language models (LLMs). Yet the rule-based reward functions commonly used on mathematical or programming benchmarks assess only answer format and correctness, providing no signal as to whether the induced Chain-of-Thought (CoT) actually improves the answer. Furthermore, such task-specific training offers limited control over logical depth and therefore may fail to reveal a model's genuine reasoning capacity. We propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward framework that reshapes both reward and advantage signals. (i) A Reasoning Quality Reward assigns fine-grained credit to those reasoning chains that demonstrably raise the likelihood of the correct answer, directly incentivising the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage decays the advantage of responses whose length deviates from a validation-derived threshold, stabilising training. To facilitate rigorous assessment, we also release Logictree, a dynamically constructed deductive reasoning dataset that functions both as RL training data and as a comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B model attains GPT-o3-mini level performance on Logictree with 400 trianing steps, while the average confidence of CoT-augmented answers rises by 30%. The model further exhibits generalisation across diverse logical-reasoning datasets, and the mathematical benchmark AIME24. These results illuminate how RL shapes CoT behaviour and chart a practical path toward enhancing formal-reasoning skills in large language models. All code and data are available in repository https://github.com/Henryhe09/DRER.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse-Engineered Reasoning for Open-Ended Generation</title>
<link>https://arxiv.org/abs/2509.06160</link>
<guid>https://arxiv.org/abs/2509.06160</guid>
<content:encoded><![CDATA[
<div> Keywords: deep reasoning, REER, Open-ended tasks, DeepWriting-20K, DeepWriter-8B

Summary:<br />
The article introduces the REER paradigm, a new approach to deep reasoning that works backward from known-good solutions to discover the reasoning process. It addresses the limitations of reinforcement learning and instruction distillation in open-ended, creative generation tasks. The authors introduce DeepWriting-20K, a dataset of 20,000 deep reasoning trajectories for open-ended tasks. They train DeepWriter-8B on this dataset, surpassing strong open-source baselines and competing with leading proprietary models like GPT-4o and Claude 3.5. deep reasoning has been a significant challenge in creative generation tasks, REER offers a novel solution by identifying latent reasoning processes. The DeepWriting-20K dataset provides a valuable resource for training models on deep reasoning tasks. The performance of DeepWriter-8B demonstrates the effectiveness of the REER approach in achieving competitive results with leading proprietary models. <br /><br />Summary: <div>
arXiv:2509.06160v1 Announce Type: new 
Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in verifiable domains like mathematics, its application to open-ended, creative generation remains a critical challenge. The two dominant methods for instilling reasoning -- reinforcement learning (RL) and instruction distillation -- falter in this area; RL struggles with the absence of clear reward signals and high-quality reward models, while distillation is prohibitively expensive and capped by the teacher model's capabilities. To overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a new paradigm that fundamentally shifts the approach. Instead of building a reasoning process ``forwards'' through trial-and-error or imitation, REER works ``backwards'' from known-good solutions to computationally discover the latent, step-by-step deep reasoning process that could have produced them. Using this scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks. Our model, DeepWriter-8B, trained on this data, not only surpasses strong open-source baselines but also achieves performance competitive with, and at times superior to, leading proprietary models like GPT-4o and Claude 3.5.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Long to Short: LLMs Excel at Trimming Own Reasoning Chains</title>
<link>https://arxiv.org/abs/2509.06174</link>
<guid>https://arxiv.org/abs/2509.06174</guid>
<content:encoded><![CDATA[
<div> Efficient Dynamic Inference Trimming, LRMs, reasoning efficiency, test-time scaling, reasoning models<br />
<br />
Summary: <br />
Large reasoning models (LRMs) show superior performance but are prone to overthinking, hindering interpretability. A systematic investigation reveals a dilemma in balancing correctness and brevity in LRMs. To address this, a test-time scaling method called Efficient Dynamic Inference Trimming (EDIT) is proposed. EDIT guides LRMs to identify the shortest correct reasoning paths, optimizing for concise yet accurate responses. By leveraging constraint-guided generation and tracking length and answer distributions, EDIT enhances reasoning efficiency, improving readability and user experience. Extensive experiments show that EDIT significantly boosts the performance of LRMs across various models and datasets. <div>
arXiv:2509.06174v1 Announce Type: new 
Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward over conventional instruction-following LLMs. By applying test-time scaling to generate extended reasoning paths, they establish many SOTAs across a wide range of complex reasoning tasks. However, recent studies show that LRMs are prone to suffer from overthinking -- the tendency to overcomplicate simple problems, leading to excessive strategy switching and long, convoluted reasoning traces that hinder their interpretability. To mitigate this issue, we conduct a systematic investigation into the reasoning efficiency of a broad set of LRMs and uncover a common dilemma: the difficulty in balancing multiple generation objectives such as correctness and brevity. Based on this discovery, we propose a test-time scaling method, EDIT (Efficient Dynamic Inference Trimming), which efficiently guides LRMs to identify the shortest correct reasoning paths at test time. EDIT employs constraint-guided generation while jointly tracking length and answer distributions under varying constraints, allowing it to select responses that strike an optimal balance between conciseness and correctness. Extensive experiments across diverse models and datasets show that EDIT substantially enhance the reasoning efficiency, producing compact yet informative outputs that improve readability and user experience.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments</title>
<link>https://arxiv.org/abs/2509.06235</link>
<guid>https://arxiv.org/abs/2509.06235</guid>
<content:encoded><![CDATA[
<div> LLM-based agents, competitive multi-agent environments, PillagerBench, TactiCrafter, teamwork, real-time scenarios<br />
<br />
Summary: <br />
LLM-based agents have shown promise in various cooperative and strategic reasoning tasks, but their effectiveness in competitive multi-agent environments remains underexplored. To address this gap, the authors introduce PillagerBench, a framework for evaluating multi-agent systems in real-time competitive team-vs-team scenarios in Minecraft. They propose TactiCrafter, an LLM-based multi-agent system that facilitates teamwork, learns causal dependencies, and adapts to opponent strategies. Through evaluation, TactiCrafter outperforms baseline approaches and demonstrates adaptive learning through self-play. The authors also analyze its learning process and strategic evolution over multiple game episodes. Overall, this work contributes to advancing multi-agent AI for competitive environments, and the open-sourced PillagerBench framework encourages further research in this area. <div>
arXiv:2509.06235v1 Announce Type: new 
Abstract: LLM-based agents have shown promise in various cooperative and strategic reasoning tasks, but their effectiveness in competitive multi-agent environments remains underexplored. To address this gap, we introduce PillagerBench, a novel framework for evaluating multi-agent systems in real-time competitive team-vs-team scenarios in Minecraft. It provides an extensible API, multi-round testing, and rule-based built-in opponents for fair, reproducible comparisons. We also propose TactiCrafter, an LLM-based multi-agent system that facilitates teamwork through human-readable tactics, learns causal dependencies, and adapts to opponent strategies. Our evaluation demonstrates that TactiCrafter outperforms baseline approaches and showcases adaptive learning through self-play. Additionally, we analyze its learning process and strategic evolution over multiple game episodes. To encourage further research, we have open-sourced PillagerBench, fostering advancements in multi-agent AI for competitive environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.06239</link>
<guid>https://arxiv.org/abs/2509.06239</guid>
<content:encoded><![CDATA[
<div> Framework, Large Language Models, Hardware synthesis, Reinforcement Learning, Natural language specification
<br />
Summary:
The article introduces Proof2Silicon, an end-to-end synthesis framework that leverages the PREFACE framework to generate formally verifiable hardware designs from natural language specifications. By using reinforcement learning to optimize prompts and translate verified Dafny programs into synthesizable C code, Proof2Silicon achieves a success rate of up to 72% in hardware synthesis through Vivado HLS. The method significantly improves Dafny verification success rates by up to 21% across different Large Language Models, demonstrating a robust and scalable approach to automated hardware design generation. Proof2Silicon bridges the gap between natural language specifications and silicon realization, offering a streamlined pipeline for generating verified RTL designs efficiently from high-level descriptions. 
<br /><br />Summary: <div>
arXiv:2509.06239v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in automated code generation but frequently produce code that fails formal verification, an essential requirement for hardware and safety-critical domains. To overcome this fundamental limitation, we previously proposed PREFACE, a model-agnostic framework based on reinforcement learning (RL) that iteratively repairs the prompts provided to frozen LLMs, systematically steering them toward generating formally verifiable Dafny code without costly fine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis framework that embeds the previously proposed PREFACE flow to enable the generation of correctness-by-construction hardware directly from natural language specifications. Proof2Silicon operates by: (1) leveraging PREFACE's verifier-driven RL agent to optimize prompt generation iteratively, ensuring Dafny code correctness; (2) automatically translating verified Dafny programs into synthesizable high-level C using Dafny's Python backend and PyLog; and (3) employing Vivado HLS to produce RTL implementations. Evaluated rigorously on a challenging 100-task benchmark, PREFACE's RL-guided prompt optimization consistently improved Dafny verification success rates across diverse LLMs by up to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis success rate of up to 72%, generating RTL designs through Vivado HLS synthesis flows. These results demonstrate a robust, scalable, and automated pipeline for LLM-driven, formally verified hardware synthesis, bridging natural-language specification and silicon realization.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents</title>
<link>https://arxiv.org/abs/2509.06269</link>
<guid>https://arxiv.org/abs/2509.06269</guid>
<content:encoded><![CDATA[
<div> Keywords: Personalized AI assistants, Causal knowledge graph, Transparent explanations, Lifestyle planning, Evaluation metrics

Summary: 
REMI is a novel architecture that integrates a personal causal knowledge graph, a causal reasoning engine, and a schema-based planning module to provide explainable, personalized recommendations in areas like fashion and personal wellness. By utilizing a personal causal graph, the system can perform goal-directed causal traversals enriched with external knowledge and hypothetical reasoning to generate tailored action plans. A Large Language Model orchestrates these components, producing answers with transparent causal explanations. Evaluation metrics such as the Personalization Salience Score and Causal Reasoning Accuracy show that CSM based agents outperform baseline LLM agents in providing context-aware, user-aligned recommendations. This work showcases the effectiveness of memory-augmented causal reasoning in advancing the development of trustworthy AI lifestyle assistants. 

<br /><br />Summary: <div>
arXiv:2509.06269v1 Announce Type: new 
Abstract: Personalized AI assistants often struggle to incorporate complex personal data and causal knowledge, leading to generic advice that lacks explanatory power. We propose REMI, a Causal Schema Memory architecture for a multimodal lifestyle agent that integrates a personal causal knowledge graph, a causal reasoning engine, and a schema based planning module. The idea is to deliver explainable, personalized recommendations in domains like fashion, personal wellness, and lifestyle planning. Our architecture uses a personal causal graph of the user's life events and habits, performs goal directed causal traversals enriched with external knowledge and hypothetical reasoning, and retrieves adaptable plan schemas to generate tailored action plans. A Large Language Model orchestrates these components, producing answers with transparent causal explanations. We outline the CSM system design and introduce new evaluation metrics for personalization and explainability, including Personalization Salience Score and Causal Reasoning Accuracy, to rigorously assess its performance. Results indicate that CSM based agents can provide more context aware, user aligned recommendations compared to baseline LLM agents. This work demonstrates a novel approach to memory augmented, causal reasoning in personalized agents, advancing the development of transparent and trustworthy AI lifestyle assistants.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning</title>
<link>https://arxiv.org/abs/2509.06278</link>
<guid>https://arxiv.org/abs/2509.06278</guid>
<content:encoded><![CDATA[
<div> Table reasoning; large language models; tool-integrated reasoning; autonomous adaptability; Rank-Aware Policy Optimization <br />
<br />
Summary: <br />
TableMind is a novel table reasoning agent that utilizes large language models to autonomously perform multi-turn tool invocation, write and execute data-analyzing code, and exhibit high-level capabilities like planning and self-reflection. It follows a two-stage fine-tuning approach - supervised fine-tuning on reasoning trajectories and reinforcement fine-tuning on multi-objective strategies. The Rank-Aware Policy Optimization (RAPO) technique is introduced to guide the model towards better and more accurate answers by prioritizing high-quality trajectories. Extensive experiments show that TableMind outperforms existing systems on mainstream benchmarks, achieving higher reasoning accuracy and computational precision. <div>
arXiv:2509.06278v1 Announce Type: new 
Abstract: Table reasoning is crucial for leveraging structured data in domains such as finance, healthcare, and scientific research. While large language models (LLMs) show promise in multi-step reasoning, purely text-based methods often struggle with the complex numerical computations and fine-grained operations inherently required in this task. Tool-integrated reasoning improves computational accuracy via explicit code execution, yet existing systems frequently rely on rigid patterns, supervised imitation, and lack true autonomous adaptability. In this paper, we present TableMind, an LLM-driven table reasoning agent that (i) autonomously performs multi-turn tool invocation, (ii) writes and executes data-analyzing code in a secure sandbox environment for data analysis and precise numerical reasoning, and (iii) exhibits high-level capabilities such as planning and self-reflection to adapt strategies. To realize these capabilities, we adopt a two-stage fine-tuning paradigm built on top of a powerful pre-trained language model: supervised fine-tuning on high-quality reasoning trajectories to establish effective tool usage patterns, followed by reinforcement fine-tuning to optimize multi-objective strategies. In particular, we propose Rank-Aware Policy Optimization (RAPO), which increases the update weight of high-quality trajectories when their output probabilities are lower than those of low-quality ones, thereby guiding the model more consistently toward better and more accurate answers. Extensive experiments on several mainstream benchmarks demonstrate that TableMind achieves superior performance compared to competitive baselines, yielding substantial gains in both reasoning accuracy and computational precision.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents</title>
<link>https://arxiv.org/abs/2509.06283</link>
<guid>https://arxiv.org/abs/2509.06283</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, reasoning capabilities, deep research, reinforcement learning, autonomous agents <br />
<br />
Summary: Equipping large language models with complex reasoning and tool-use capabilities for deep research is a focus in AI research. This paper introduces native Autonomous Single-Agent models for deep research, allowing models to dynamically determine actions without manual directive. The authors use reinforcement learning with synthetic data to continually improve reasoning-optimized models. Their best model achieves impressive results on the Humanity's Last Exam benchmark. The paper also includes key analysis experiments to provide insights into their methodologies. <div>
arXiv:2509.06283v1 Announce Type: new 
Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning and tool-use capabilities has become a key focus in agentic AI research, especially with recent advances in reasoning-oriented (``thinking'') models. Such capabilities are key to unlocking a number of important applications. One such application is Deep Research (DR), which requires extensive search and reasoning over many sources. Our work in this paper focuses on the development of native Autonomous Single-Agent models for DR featuring minimal web crawling and Python tool integration. Unlike multi-agent systems, where agents take up pre-defined roles and are told what to do at each step in a static workflow, an autonomous single-agent determines its next action dynamically based on context, without manual directive. While prior work has proposed training recipes for base or instruction-tuned LLMs, we focus on continual reinforcement learning (RL) of reasoning-optimized models to further enhance agentic skills while preserving reasoning ability. Towards this end, we propose a simple RL recipe with entirely synthetic data, which we apply to various open-source LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam benchmark. In addition, we conduct key analysis experiments to provide more insights into our methodologies.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs</title>
<link>https://arxiv.org/abs/2509.06284</link>
<guid>https://arxiv.org/abs/2509.06284</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, structured reasoning, guideline, refinement, generalization

Summary: 
The paper introduces a framework for improving the reasoning capabilities of large language models (LLMs) by shifting from implicit exploration to structured reasoning. This framework involves extracting structured reasoning patterns from successful trajectories and learning from failures. During inference, the model follows step-by-step guidelines derived from these patterns and incorporates refinement steps to correct errors and stabilize the reasoning process. Experimental results on various benchmarks demonstrate that the proposed method consistently outperforms strong baselines in diverse reasoning tasks. The structured reasoning approach with stepwise execution and refinement enhances stability and generalization while also enabling cross-domain transferability and supporting collaboration between models. The method's effectiveness and scalability are comparable to supervised fine-tuning, showcasing its potential for advancing reasoning capabilities in LLMs. 

<br /><br />Summary: <div>
arXiv:2509.06284v1 Announce Type: new 
Abstract: Large language models (LLMs) have advanced general-purpose reasoning, showing strong performance across diverse tasks. However, existing methods often rely on implicit exploration, where the model follows stochastic and unguided reasoning paths-like walking without a map. This leads to unstable reasoning paths, lack of error correction, and limited learning from past experience. To address these issues, we propose a framework that shifts from implicit exploration to structured reasoning through guideline and refinement. First, we extract structured reasoning patterns from successful trajectories and reflective signals from failures. During inference, the model follows these guidelines step-by-step, with refinement applied after each step to correct errors and stabilize the reasoning process. Experiments on BBH and four additional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method consistently outperforms strong baselines across diverse reasoning tasks. Structured reasoning with stepwise execution and refinement improves stability and generalization, while guidelines transfer well across domains and flexibly support cross-model collaboration, matching or surpassing supervised fine-tuning in effectiveness and scalability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2509.06307</link>
<guid>https://arxiv.org/abs/2509.06307</guid>
<content:encoded><![CDATA[
<div> language models, residential retrofit decisions, CO2 reduction, payback period, Smart and Connected Communities

Summary:
The study evaluates generative AI models for residential energy retrofit decision making. Seven large language models (LLMs) are compared in terms of accuracy, consistency, sensitivity, and reasoning on a dataset of 400 homes across 49 US states. The LLMs show effectiveness in providing recommendations for CO2 reduction and payback period objectives, with stronger performance in technical decisions. However, sociotechnical decisions are limited by economic trade-offs and local context. The models exhibit low agreement and tend to diverge in recommendations. They are sensitive to location and building geometry but less to technology and occupant behavior. While most models display step-by-step engineering reasoning, it is often simplified and lacks deeper contextual awareness. Overall, LLMs show promise as assistants for energy retrofit decision making, but improvements in accuracy, consistency, and context handling are necessary for reliable practice. 

<br /><br />Summary: <div>
arXiv:2509.06307v1 Announce Type: new 
Abstract: Conventional approaches to building energy retrofit decision making suffer from limited generalizability and low interpretability, hindering adoption in diverse residential contexts. With the growth of Smart and Connected Communities, generative AI, especially large language models (LLMs), may help by processing contextual information and producing practitioner readable recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok, Llama, and Claude) on residential retrofit decisions under two objectives: maximizing CO2 reduction (technical) and minimizing payback period (sociotechnical). Performance is assessed on four dimensions: accuracy, consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49 US states. LLMs generate effective recommendations in many cases, reaching up to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning. Performance is stronger for the technical objective, while sociotechnical decisions are limited by economic trade offs and local context. Agreement across models is low, and higher performing models tend to diverge from others. LLMs are sensitive to location and building geometry but less sensitive to technology and occupant behavior. Most models show step by step, engineering style reasoning, but it is often simplified and lacks deeper contextual awareness. Overall, LLMs are promising assistants for energy retrofit decision making, but improvements in accuracy, consistency, and context handling are needed for reliable practice.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation</title>
<link>https://arxiv.org/abs/2509.06337</link>
<guid>https://arxiv.org/abs/2509.06337</guid>
<content:encoded><![CDATA[
<div> Keywords: survey simulation, Large Language Models, sociodemographic data, questionnaire-based surveys, sociological research

Summary:<br />
This paper introduces a new approach to simulating virtual survey respondents using Large Language Models (LLMs) to address the limitations of traditional survey methods. Two novel simulation settings, Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS), are proposed to assess the accuracy and coherence of generated responses. The study evaluates popular LLMs like GPT-3.5/4 Turbo and LLaMA 3.0/3.1-8B using a benchmark suite (LLM-S^3) across various sociological datasets. Results show consistent prediction performance trends, identify failure modes, and emphasize the impact of context and prompt design on simulation fidelity. This research lays a robust foundation for leveraging LLMs in survey simulations, providing scalable and cost-effective tools for sociological research and policy evaluation. The code and dataset are publicly available for further exploration and application.<br /> <div>
arXiv:2509.06337v1 Announce Type: new 
Abstract: Questionnaire-based surveys are foundational to social science research and public policymaking, yet traditional survey methods remain costly, time-consuming, and often limited in scale. This paper explores a new paradigm: simulating virtual survey respondents using Large Language Models (LLMs). We introduce two novel simulation settings, namely Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS), to systematically evaluate the ability of LLMs to generate accurate and demographically coherent responses. In PAS, the model predicts missing attributes based on partial respondent profiles, whereas FAS involves generating complete synthetic datasets under both zero-context and context-enhanced conditions. We curate a comprehensive benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey Simulation), that spans 11 real-world public datasets across four sociological domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA 3.0/3.1-8B) reveals consistent trends in prediction performance, highlights failure modes, and demonstrates how context and prompt design impact simulation fidelity. This work establishes a rigorous foundation for LLM-driven survey simulations, offering scalable and cost-effective tools for sociological research and policy evaluation. Our code and dataset are available at: https://github.com/dart-lab-research/LLM-S-Cube-Benchmark
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent</title>
<link>https://arxiv.org/abs/2509.06341</link>
<guid>https://arxiv.org/abs/2509.06341</guid>
<content:encoded><![CDATA[
<div> e-commerce, bargaining, large language models, seller agents, buyer intents
Summary:
A new study introduces a multi-turn evaluation framework for measuring the bargaining ability of seller agents in online second-hand marketplaces. It focuses on large Language Models (LLMs) acting as seller agents and their ability to accurately interpret buyer intents across long negotiations. The study presents a large-scale e-commerce bargaining benchmark covering 622 categories, 9,892 products, and 3,014 tasks. It proposes a turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated buyer intents, going beyond outcome-only metrics. An automated pipeline is developed to extract reliable intent from extensive dialogue data, aiming to enhance the bargaining effectiveness of seller agents in e-commerce dialogues. <br /><br />Summary: <div>
arXiv:2509.06341v1 Announce Type: new 
Abstract: In online second-hand marketplaces, multi-turn bargaining is a crucial part of seller-buyer interactions. Large Language Models (LLMs) can act as seller agents, negotiating with buyers on behalf of sellers under given business constraints. A critical ability for such agents is to track and accurately interpret cumulative buyer intents across long negotiations, which directly impacts bargaining effectiveness. We introduce a multi-turn evaluation framework for measuring the bargaining ability of seller agents in e-commerce dialogues. The framework tests whether an agent can extract and track buyer intents. Our contributions are: (1) a large-scale e-commerce bargaining benchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated buyer intents, moving beyond outcome-only metrics; and (3) an automated pipeline that extracts reliable intent from massive dialogue data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research</title>
<link>https://arxiv.org/abs/2509.06355</link>
<guid>https://arxiv.org/abs/2509.06355</guid>
<content:encoded><![CDATA[
<div> waypoint system, multi-agent simulator, strategic planning, tactical positioning, generative models
<br />
Summary:
DECOY is a new multi-agent simulator designed to balance high-fidelity detail with computational efficiency in complex interactions. It abstracts strategic planning in 3D terrains into high-level, discretized simulation while maintaining low-level environmental fidelity. Using Counter-Strike: Global Offensive as a testbed, DECOY accurately simulates gameplay by focusing on movement decisions for tactical positioning rather than low-level mechanics like aiming and shooting. The simulator incorporates a waypoint system to simplify and discretize continuous states and actions, along with neural predictive and generative models trained on real tournament data to reconstruct event outcomes. Through extensive evaluations, DECOY-generated replays closely match those observed in the original game. This publicly available simulation environment serves as a valuable tool for research in strategic multi-agent planning and behavior generation. 
<br /> <div>
arXiv:2509.06355v1 Announce Type: new 
Abstract: Modern simulation environments for complex multi-agent interactions must balance high-fidelity detail with computational efficiency. We present DECOY, a novel multi-agent simulator that abstracts strategic, long-horizon planning in 3D terrains into high-level discretized simulation while preserving low-level environmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a testbed, our framework accurately simulates gameplay using only movement decisions as tactical positioning -- without explicitly modeling low-level mechanics such as aiming and shooting. Central to our approach is a waypoint system that simplifies and discretizes continuous states and actions, paired with neural predictive and generative models trained on real CS:GO tournament data to reconstruct event outcomes. Extensive evaluations show that replays generated from human data in DECOY closely match those observed in the original game. Our publicly available simulation environment provides a valuable tool for advancing research in strategic multi-agent planning and behavior generation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning</title>
<link>https://arxiv.org/abs/2509.06409</link>
<guid>https://arxiv.org/abs/2509.06409</guid>
<content:encoded><![CDATA[
<div> framework, DiagCoT, supervised fine-tuning, vision-language models, radiologists, diagnostic reasoning

Summary:
DiagCoT is a framework that leverages supervised fine-tuning to mimic radiologists' diagnostic reasoning using free-text reports. It combines contrastive tuning for domain alignment, chain-of-thought supervision for inferential logic, and reinforcement tuning with clinical rewards for accuracy and fluency. On the MIMIC-CXR benchmark, DiagCoT significantly improved disease classification, pathology grounding, and report generation metrics. It outperformed existing models on long-tailed diseases and external datasets. By transforming unstructured clinical narratives into structured supervision, DiagCoT provides a scalable and interpretable approach for developing AI systems in radiology. <div>
arXiv:2509.06409v1 Announce Type: new 
Abstract: This study presents DiagCoT, a multi-stage framework that applies supervised fine-tuning to general-purpose vision-language models (VLMs) to emulate radiologists' stepwise diagnostic reasoning using only free-text reports. DiagCoT combines contrastive image-report tuning for domain alignment, chain-of-thought supervision to capture inferential logic, and reinforcement tuning with clinical reward signals to enhance factual accuracy and fluency. On the MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC from 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08 to 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33 (absolute gain of 0.22). It outperformed state-of-the-art models including LLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By converting unstructured clinical narratives into structured supervision, DiagCoT offers a scalable approach for developing interpretable and diagnostically competent AI systems for radiology.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning</title>
<link>https://arxiv.org/abs/2509.06436</link>
<guid>https://arxiv.org/abs/2509.06436</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Tree of Agents, Long-context tasks, Multi-agent reasoning framework, Performance improvement

Summary:
Large language models (LLMs) often struggle with long-context tasks due to the lost in the middle issue. To address this, the Tree of Agents (TOA) framework is introduced, which segments inputs into chunks processed by independent agents. These agents collaborate along tree-structured paths, exchanging information for multi-perspective understanding and reducing position bias and hallucinations. TOA incorporates prefix-hash caching and adaptive pruning strategies for efficiency, achieving significant performance gains with minimal API overhead. Experimental results with the compact LLaMA3.1-8B model show that TOA outperforms several baselines and is comparable to larger commercial models like Gemini1.5-pro on long-context tasks. The code for TOA is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2509.06436v1 Announce Type: new 
Abstract: Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limitations, we propose Tree of Agents (TOA), a multi-agent reasoning framework that segments the input into chunks processed by independent agents. Each agent generates its local cognition, then agents dynamically exchange information for collaborative reasoning along tree-structured paths. TOA enables agents to probe different reasoning orders for multi-perspective understanding, effectively mitigating position bias and reducing hallucinations. To improve processing efficiency, we incorporate prefix-hash caching and adaptive pruning strategies, achieving significant performance improvements with comparable API overhead. Experiments show that TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple baselines and demonstrates comparable performance to the latest and much larger commercial models, such as Gemini1.5-pro, on various long-context tasks. Code is available at https://github.com/Aireduce952/Tree-of-Agents.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data</title>
<link>https://arxiv.org/abs/2509.06444</link>
<guid>https://arxiv.org/abs/2509.06444</guid>
<content:encoded><![CDATA[
<div> Keywords: Hybrid data modalities, Federated RAG framework, Privacy-preserving representations, Anonymization tools, Three-tier caching strategy

Summary:<br /><br />
The article introduces HyFedRAG, a Federated RAG framework designed for handling heterogeneous and privacy-sensitive data in distributed healthcare settings. The framework leverages an edge-cloud collaborative mechanism to enable RAG operations across diverse data sources while maintaining data privacy. Key contributions include a framework built on Flower supporting SQL data, knowledge graphs, and clinical notes, lightweight local retrievers with privacy-aware LLMs, and three anonymization tools for producing de-identified summaries. A three-tier caching strategy is implemented to optimize response latency and reduce redundant computation. Experimental results on PMC-Patients show that HyFedRAG outperforms existing baselines in retrieval quality, generation consistency, and system efficiency. Overall, HyFedRAG offers a scalable and privacy-compliant solution for RAG over structural-heterogeneous data, unlocking the potential of LLMs in sensitive and diverse data environments. 

Summary: <div>
arXiv:2509.06444v1 Announce Type: new 
Abstract: Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive data, especially in distributed healthcare settings where patient data spans SQL, knowledge graphs, and clinical notes. Clinicians face difficulties retrieving rare disease cases due to privacy constraints and the limitations of traditional cloud-based RAG systems in handling diverse formats and edge devices. To address this, we introduce HyFedRAG, a unified and efficient Federated RAG framework tailored for Hybrid data modalities. By leveraging an edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across diverse data sources while preserving data privacy. Our key contributions are: (1) We design an edge-cloud collaborative RAG framework built on Flower, which supports querying structured SQL data, semi-structured knowledge graphs, and unstructured documents. The edge-side LLMs convert diverse data into standardized privacy-preserving representations, and the server-side LLMs integrates them for global reasoning and generation. (2) We integrate lightweight local retrievers with privacy-aware LLMs and provide three anonymization tools that enable each client to produce semantically rich, de-identified summaries for global inference across devices. (3) To optimize response latency and reduce redundant computation, we design a three-tier caching strategy consisting of local cache, intermediate representation cache, and cloud inference cache. Experimental results on PMC-Patients demonstrate that HyFedRAG outperforms existing baselines in terms of retrieval quality, generation consistency, and system efficiency. Our framework offers a scalable and privacy-compliant solution for RAG over structural-heterogeneous data, unlocking the potential of LLMs in sensitive and diverse data environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set</title>
<link>https://arxiv.org/abs/2509.06463</link>
<guid>https://arxiv.org/abs/2509.06463</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, instruction dataset distribution, aligned model performance, instruction selection algorithm, downstream performance<br />
<br />
Summary: 
The article addresses the need to improve model alignment performance and efficiency in large language models through the selection of informative instructions. The key factors influencing the relationship between instruction dataset distribution and aligned model performance are investigated, leading to the proposal of a novel instruction data selection method. The depth of instructions and semantic coverage are identified as crucial factors determining downstream performance. An instruction selection algorithm is designed to maximize depth and semantic coverage simultaneously, resulting in sustainable performance improvement at a faster pace compared to baseline methods. This approach achieves "Accelerated Scaling" by explaining over 70% of the model loss on the development set and showcasing superior performance in experimental results. <div>
arXiv:2509.06463v1 Announce Type: new 
Abstract: With the growing demand for applying large language models to downstream tasks, improving model alignment performance and efficiency has become crucial. Such a process involves selecting informative instructions from a candidate pool. However, due to the complexity of instruction set distributions, the key factors driving the performance of aligned models remain unclear. As a result, current instruction set refinement methods fail to improve performance as the instruction pool expands continuously. To address this issue, we first investigate the key factors that influence the relationship between instruction dataset distribution and aligned model performance. Based on these insights, we propose a novel instruction data selection method. We identify that the depth of instructions and the coverage of the semantic space are the crucial factors determining downstream performance, which could explain over 70\% of the model loss on the development set. We then design an instruction selection algorithm to simultaneously maximize the depth and semantic coverage of the selected instructions. Experimental results demonstrate that, compared to state-of-the-art baseline methods, it can sustainably improve model performance at a faster pace and thus achieve \emph{``Accelerated Scaling''}.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents</title>
<link>https://arxiv.org/abs/2509.06477</link>
<guid>https://arxiv.org/abs/2509.06477</guid>
<content:encoded><![CDATA[
<div> Keywords: GUI agents, shortcuts, benchmarking, hybrid agents, mobile domain

Summary: 
MAS-Bench introduces a benchmark for evaluating GUI-shortcut hybrid agents, focusing on the mobile domain. It includes 139 tasks across 11 real-world applications, predefined shortcuts, and evaluation metrics. The benchmark assesses an agent's ability to autonomously generate shortcuts to enhance efficiency. Results show that hybrid agents outperform GUI-only agents, demonstrating the effectiveness of shortcut integration. MAS-Bench fills a critical evaluation gap in assessing shortcut generation capabilities, providing a foundational platform for future intelligent agent advancements. <div>
arXiv:2509.06477v1 Announce Type: new 
Abstract: To enhance the efficiency of GUI agents on various platforms like smartphones and computers, a hybrid paradigm that combines flexible GUI operations with efficient shortcuts (e.g., API, deep links) is emerging as a promising direction. However, a framework for systematically benchmarking these hybrid agents is still underexplored. To take the first step in bridging this gap, we introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut hybrid agents with a specific focus on the mobile domain. Beyond merely using predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously generate shortcuts by discovering and creating reusable, low-cost workflows. It features 139 complex tasks across 11 real-world applications, a knowledge base of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation metrics. The tasks are designed to be solvable via GUI-only operations, but can be significantly accelerated by intelligently embedding shortcuts. Experiments show that hybrid agents achieve significantly higher success rates and efficiency than their GUI-only counterparts. This result also demonstrates the effectiveness of our method for evaluating an agent's shortcut generation capabilities. MAS-Bench fills a critical evaluation gap, providing a foundational platform for future advancements in creating more efficient and robust intelligent agents.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization</title>
<link>https://arxiv.org/abs/2509.06490</link>
<guid>https://arxiv.org/abs/2509.06490</guid>
<content:encoded><![CDATA[
<div> Keywords: supply chain management, multi-objective optimization, Reinforcement Learning, Multi-Objective Evolutionary Algorithms, uncertainty<br />
Summary:<br />
This paper proposes a novel approach that combines Reinforcement Learning (RL) and Multi-Objective Evolutionary Algorithms (MOEAs) to address the challenges of dynamic multi-objective optimization in supply chain management. Traditional methods struggle to adapt to real-time changes in objectives, but the proposed approach generates a diverse population of policies through MOEAs, allowing for flexibility and adaptability. Incorporating Conditional Value-at-Risk (CVaR) enables risk-sensitive decision-making, enhancing resilience in uncertain environments. Case studies demonstrate the method's ability to outperform state-of-the-art approaches in inventory management, showcasing improved decision-making efficiency and robustness in handling supply chain dynamics. Overall, this strategy offers a more effective and resilient framework for managing uncertainty and optimizing performance in supply chains.<br /><br />Summary: <div>
arXiv:2509.06490v1 Announce Type: new 
Abstract: In supply chain management, decision-making often involves balancing multiple conflicting objectives, such as cost reduction, service level improvement, and environmental sustainability. Traditional multi-objective optimization methods, such as linear programming and evolutionary algorithms, struggle to adapt in real-time to the dynamic nature of supply chains. In this paper, we propose an approach that combines Reinforcement Learning (RL) and Multi-Objective Evolutionary Algorithms (MOEAs) to address these challenges for dynamic multi-objective optimization under uncertainty. Our method leverages MOEAs to search the parameter space of policy neural networks, generating a Pareto front of policies. This provides decision-makers with a diverse population of policies that can be dynamically switched based on the current system objectives, ensuring flexibility and adaptability in real-time decision-making. We also introduce Conditional Value-at-Risk (CVaR) to incorporate risk-sensitive decision-making, enhancing resilience in uncertain environments. We demonstrate the effectiveness of our approach through case studies, showcasing its ability to respond to supply chain dynamics and outperforming state-of-the-art methods in an inventory management case study. The proposed strategy not only improves decision-making efficiency but also offers a more robust framework for managing uncertainty and optimizing performance in supply chains.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers</title>
<link>https://arxiv.org/abs/2509.06493</link>
<guid>https://arxiv.org/abs/2509.06493</guid>
<content:encoded><![CDATA[
<div> RL, Large Language Models, Automated Theorem Proving, Scaling, Inference Architecture <br />
Summary: <br />
The paper introduces BFS-Prover-V2, a system addressing scaling challenges in integrating Large Language Models (LLMs) into automated theorem proving. It presents a novel multi-turn off-policy RL framework inspired by AlphaZero principles for training-time improvement of LLM step-prover performance. The framework includes adaptive tactic-level data filtering and periodic retraining to overcome performance plateaus. Additionally, a planner-enhanced multi-agent search architecture is introduced for scaling reasoning capabilities at inference time. This architecture utilizes a high-level planner to decompose theorems into simpler subgoals, reducing the search space and enabling efficient collaboration among prover agents. These dual scaling approaches result in state-of-the-art performance on formal mathematics benchmarks, with BFS-Prover-V2 achieving high accuracies on MiniF2F and ProofNet test sets. The presented RL and inference techniques have broader applicability beyond formal mathematics, particularly in domains requiring long-horizon multi-turn reasoning and complex search. <br /> <div>
arXiv:2509.06493v1 Announce Type: new 
Abstract: The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI system to help scientists write expert-level empirical software</title>
<link>https://arxiv.org/abs/2509.06503</link>
<guid>https://arxiv.org/abs/2509.06503</guid>
<content:encoded><![CDATA[
<div> Keywords: AI system, scientific software, Large Language Model, Tree Search, novel methods<br />
<br />
Summary: 
An AI system has been developed to create expert-level scientific software efficiently, aiming to enhance the quality metric. By utilizing a Large Language Model (LLM) and Tree Search (TS), the system navigates a vast solution space to maximize performance. It successfully integrates complex research ideas from external sources, leading to expert-level outcomes. Demonstrated effectiveness across various benchmarks, the system discovered 40 novel methods for single-cell data analysis in bioinformatics that surpassed top human-developed methods. In epidemiology, it generated 14 superior forecasting models for COVID-19 hospitalizations compared to the CDC ensemble and other individual models. Additionally, the system produced cutting-edge software for geospatial analysis, neural activity prediction in zebrafish, time series forecasting, and numerical integration solutions. These achievements highlight a notable advancement towards accelerating scientific progress. 

<br /><br /> <div>
arXiv:2509.06503v1 Announce Type: new 
Abstract: The cycle of scientific discovery is frequently bottlenecked by the slow, manual creation of software to support computational experiments. To address this, we present an AI system that creates expert-level scientific software whose goal is to maximize a quality metric. The system uses a Large Language Model (LLM) and Tree Search (TS) to systematically improve the quality metric and intelligently navigate the large space of possible solutions. The system achieves expert-level results when it explores and integrates complex research ideas from external sources. The effectiveness of tree search is demonstrated across a wide range of benchmarks. In bioinformatics, it discovered 40 novel methods for single-cell data analysis that outperformed the top human-developed methods on a public leaderboard. In epidemiology, it generated 14 models that outperformed the CDC ensemble and all other individual models for forecasting COVID-19 hospitalizations. Our method also produced state-of-the-art software for geospatial analysis, neural activity prediction in zebrafish, time series forecasting and numerical solution of integrals. By devising and implementing novel solutions to diverse tasks, the system represents a significant step towards accelerating scientific progress.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning</title>
<link>https://arxiv.org/abs/2509.06641</link>
<guid>https://arxiv.org/abs/2509.06641</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal reasoning, intent sketch, cognitive strategies, information-theoretic analysis, zero-shot scenarios
<br />
Summary: 
This paper introduces a zero-shot multimodal reasoning component that addresses issues such as shortcuts and lack of contextual understanding in complex cross-modal reasoning tasks. The component is guided by human-like cognitive strategies centered around an "intent sketch" and consists of three modules - Intent Perceiver, Strategy Generator, and Strategy Selector. By explicitly following an "understand-plan-select" cognitive process, the component does not require parameter fine-tuning and achieves cross-model transfer through in-context engineering. Information-theoretic analysis shows that this approach can reduce conditional entropy and improve information utilization efficiency, mitigating unintended shortcut reasoning. Experimental results on various benchmarks validate the method's effectiveness and generalizability, with significant improvements observed compared to baseline approaches. The "three-module" scheme consistently outperforms other reasoning engines and pipeline combinations, demonstrating the practical value and portability of the proposed "intent sketch" reasoning component in zero-shot scenarios. 
<br /> <div>
arXiv:2509.06641v1 Announce Type: new 
Abstract: Targeting the issues of "shortcuts" and insufficient contextual understanding in complex cross-modal reasoning of multimodal large models, this paper proposes a zero-shot multimodal reasoning component guided by human-like cognitive strategies centered on an "intent sketch". The component comprises a plug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and Strategy Selector-that explicitly constructs a "understand-plan-select" cognitive process. By generating and filtering "intent sketch" strategies to guide the final reasoning, it requires no parameter fine-tuning and achieves cross-model transfer solely through in-context engineering. Information-theoretic analysis shows that this process can reduce conditional entropy and improve information utilization efficiency, thereby suppressing unintended shortcut reasoning. Experiments on IntentBench, WorldSense, and Daily-Omni validate the method's generality and robust gains; compared with their respective baselines, the complete "three-module" scheme yields consistent improvements across different reasoning engines and pipeline combinations, with gains up to approximately 9.51 percentage points, demonstrating the practical value and portability of the "intent sketch" reasoning component in zero-shot scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Foundations for Deep Research Systems: A Survey</title>
<link>https://arxiv.org/abs/2509.06733</link>
<guid>https://arxiv.org/abs/2509.06733</guid>
<content:encoded><![CDATA[
<div> Keywords: deep research systems, agentic AI, reinforcement learning, multi-objective optimization, agent architecture

Summary:
Deep research systems are evolving towards hierarchical structures with a Planner, Coordinator, and Executors. Training these systems end-to-end is challenging, with most work focusing on training a single planner connected to core tools like search and code. However, traditional methods such as Supervised Fine-Tuning (SFT) and Decentralized Partial Optimism (DPO) have limitations in terms of bias and credit assignment. Reinforcement learning (RL) offers a more robust approach by optimizing trajectory-level policies and reducing dependence on human-defined decision points. This survey explores the RL foundations of deep research systems, covering data synthesis, RL methods for agentic research, agent architecture, and evaluation benchmarks. By identifying recurring patterns and infrastructure bottlenecks, the survey offers practical guidance for training robust and transparent deep research agents with RL. <div>
arXiv:2509.06733v1 Announce Type: new 
Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction</title>
<link>https://arxiv.org/abs/2509.06736</link>
<guid>https://arxiv.org/abs/2509.06736</guid>
<content:encoded><![CDATA[
<div> keywords: Intelligent vehicle cockpits, API Agents, VehicleWorld, State-based Function Call, Github

Summary:
The article introduces VehicleWorld, an environment designed for intelligent vehicle cockpits with 30 modules, 250 APIs, and 680 properties to support agent execution. Traditional function calling approaches are compared to the proposed State-based Function Call (SFC) methodology, which leverages direct state transitions for improved performance in diverse scenarios. The code for VehicleWorld is available on Github for public access. The study highlights the importance of system state awareness for efficient execution accuracy and reduced latency, showcasing the superiority of SFC over traditional approaches. Through systematic analysis, the research demonstrates that direct state prediction is more effective for environmental control in the automotive domain. The innovative SFC approach introduces a new way to handle complex interactions in intelligent vehicle systems, offering a more efficient and reliable method for achieving target conditions.<br /><br />Summary: <div>
arXiv:2509.06736v1 Announce Type: new 
Abstract: Intelligent vehicle cockpits present unique challenges for API Agents, requiring coordination across tightly-coupled subsystems that exceed typical task environments' complexity. Traditional Function Calling (FC) approaches operate statelessly, requiring multiple exploratory calls to build environmental awareness before execution, leading to inefficiency and limited error recovery. We introduce VehicleWorld, the first comprehensive environment for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties with fully executable implementations that provide real-time state information during agent execution. This environment enables precise evaluation of vehicle agent behaviors across diverse, challenging scenarios. Through systematic analysis, we discovered that direct state prediction outperforms function calling for environmental control. Building on this insight, we propose State-based Function Call (SFC), a novel approach that maintains explicit system state awareness and implements direct state transitions to achieve target conditions. Experimental results demonstrate that SFC significantly outperforms traditional FC approaches, achieving superior execution accuracy and reduced latency. We have made all implementation code publicly available on Github https://github.com/OpenMOSS/VehicleWorld.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting</title>
<link>https://arxiv.org/abs/2509.06770</link>
<guid>https://arxiv.org/abs/2509.06770</guid>
<content:encoded><![CDATA[
<div> framework, evaluation, iteration, language models, metrics

Summary:
The article introduces an evaluation framework for measuring the impact of iterative refinement in large language models (LLMs) across ideation, code, and math tasks. Controlled 12-turn conversations per task are conducted, with various types of prompts used to guide the refinement process. Domain-appropriate checks are employed to score outcomes, and turn-level behavior is tracked using three families of metrics. Results show that gains from iteration are domain-dependent, with early improvements in ideas and code, while late turns are critical for math tasks when guided by elaboration. Vague feedback often leads to plateaus or reversals in correctness, whereas targeted prompts effectively shift the quality axis. Domain patterns are consistent, with ideation showing more semantic movement, code typically growing in size with minimal semantic change, and math initially fixed but able to break this pattern with late, elaborative iteration. The framework and metrics presented allow for measurable and comparable iteration strategies across models, signaling when to steer, stop, or switch strategies. 

<br /><br />Summary: <div>
arXiv:2509.06770v1 Announce Type: new 
Abstract: Large language models (LLMs) are now used in multi-turn workflows, but we still lack a clear way to measure when iteration helps and when it hurts. We present an evaluation framework for iterative refinement that spans ideation, code, and math. Our protocol runs controlled 12-turn conversations per task, utilizing a variety of prompts ranging from vague ``improve it'' feedback to targeted steering, and logs per-turn outputs. We score outcomes with domain-appropriate checks (unit tests for code; answer-equivalence plus reasoning-soundness for math; originality and feasibility for ideation) and track turn-level behavior with three families of metrics: semantic movement across turns, turn-to-turn change, and output size growth. Across models and tasks, gains are domain-dependent: they arrive early in ideas and code, but in math late turns matter when guided by elaboration. After the first few turns, vague feedback often plateaus or reverses correctness, while targeted prompts reliably shift the intended quality axis (novelty vs. feasibility in ideation; speed vs. readability in code; in math, elaboration outperforms exploration and drives late-turn gains). We also observe consistent domain patterns: ideation moves more in meaning across turns, code tends to grow in size with little semantic change, and math starts fixed but can break that path with late, elaborative iteration.Together, the framework and metrics make iteration measurable and comparable across models, and signal when to steer, stop, or switch strategies.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAFFLES: Reasoning-based Attribution of Faults for LLM Systems</title>
<link>https://arxiv.org/abs/2509.06822</link>
<guid>https://arxiv.org/abs/2509.06822</guid>
<content:encoded><![CDATA[
<div> identification, evaluation, autonomous systems, fault detection, reasoning

Summary:
RAFFLES is presented as an evaluation architecture for long-horizon, multi-component LLM agentic systems. Current evaluation capabilities are limited and focus on individual metrics, end-to-end outcomes, and human preferences. To match agentic capabilities, evaluation frameworks must reason, probe, iterate, and understand complex logic over long horizons. RAFFLES operates as an iterative pipeline with a central Judge and specialized Evaluators to investigate faults and assess the system's components and reasoning quality. Tested on the Who&amp;When dataset, RAFFLES outperforms baselines, achieving high fault pair accuracy on both Algorithmically-Generated and Hand-Crafted datasets. These results show progress in introducing automated fault detection for autonomous systems, reducing the need for labor-intensive manual review.<br /><br />Summary: <div>
arXiv:2509.06822v1 Announce Type: new 
Abstract: We have reached a critical roadblock in the development and enhancement of long-horizon, multi-component LLM agentic systems: it is incredibly tricky to identify where these systems break down and why. Evaluation capabilities that currently exist today (e.g., single pass LLM-as-a-judge) are limited in that they often focus on individual metrics or capabilities, end-to-end outcomes, and are narrowly grounded on the preferences of humans. We argue that to match the agentic capabilities, evaluation frameworks must also be able to reason, probe, iterate, and understand the complex logic passing through these systems over long horizons. In this paper, we present RAFFLES - an evaluation architecture that incorporates reasoning and iterative refinement. Specifically, RAFFLES operates as an iterative, multi-component pipeline, using a central Judge to systematically investigate faults and a set of specialized Evaluators to assess not only the system's components but also the quality of the reasoning by the Judge itself, thereby building a history of hypotheses. We tested RAFFLES against several baselines on the Who&amp;When dataset, a benchmark designed to diagnose the "who" (agent) and "when" (step) of a system's failure. RAFFLES outperforms these baselines, achieving an agent-step fault pair accuracy of over 43% on the Algorithmically-Generated dataset (a substantial increase from the previously published best of 16.6%) and over 20% on the Hand-Crafted dataset (surpassing the previously published best of 8.8%). These results demonstrate a key step towards introducing automated fault detection for autonomous systems over labor-intensive manual human review.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet</title>
<link>https://arxiv.org/abs/2509.06861</link>
<guid>https://arxiv.org/abs/2509.06861</guid>
<content:encoded><![CDATA[
<div> scaling, test-time, knowledge-intensive, accuracy, hallucinations

Summary:
The study evaluates test-time scaling in knowledge-intensive tasks using 12 reasoning models on two benchmarks. It finds that increased test-time computation does not consistently improve accuracy and can lead to more hallucinations. Extended reasoning may result in the model abstaining from answering rather than improving factual recall. However, for some models, longer reasoning leads to attempts on unanswered questions, increasing hallucination rates. Case studies show that extended reasoning may induce confirmation bias, causing overconfident hallucinations. Despite these limitations, enabling thinking remains beneficial compared to non-thinking. The research provides insights into the effects of test-time scaling on knowledge-intensive tasks and the trade-offs between extended reasoning and factual accuracy. <div>
arXiv:2509.06861v1 Announce Type: new 
Abstract: Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents</title>
<link>https://arxiv.org/abs/2509.06917</link>
<guid>https://arxiv.org/abs/2509.06917</guid>
<content:encoded><![CDATA[
<div> framework, research papers, AI agents, Model Context Protocol, knowledge dissemination <br />
Summary: 
Paper2Agent is a new automated framework that transforms research papers into AI agents to enhance understanding, adaptation, and reuse. By creating AI agents that act as knowledgeable research assistants, Paper2Agent reduces barriers to dissemination and facilitates downstream use. The framework systematically analyzes the paper and its codebase to construct a Model Context Protocol server, refining and robustifying it through iterative testing. These paper agents can then be integrated with chat agents to enable complex scientific queries through natural language. Case studies demonstrate Paper2Agent's ability to create reliable agents for genomic variant interpretation and single-cell and spatial transcriptomics analyses. These paper agents accurately reproduce original results and successfully execute novel user queries. This paradigm shift from static papers to dynamic AI agents holds great potential for improving knowledge dissemination and establishing a collaborative ecosystem of AI co-scientists. <br /><br />Summary: <div>
arXiv:2509.06917v1 Announce Type: new 
Abstract: We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference</title>
<link>https://arxiv.org/abs/2509.06942</link>
<guid>https://arxiv.org/abs/2509.06942</guid>
<content:encoded><![CDATA[
<div> alignment, diffusion models, reward optimization, noise prior, semantic preference optimization

Summary: 
The study introduces Direct-Align, a novel method that addresses challenges in aligning diffusion models with human preferences by predefining a noise prior to recover original images through interpolation. This helps avoid over-optimization in late timesteps. The Semantic Relative Preference Optimization (SRPO) approach formulates rewards as text-conditioned signals, allowing for online adjustment in response to prompt augmentation. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, the study achieves a significant improvement in human-evaluated realism and aesthetic quality by over 3x. <div>
arXiv:2509.06942v1 Announce Type: new 
Abstract: Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care</title>
<link>https://arxiv.org/abs/2505.00275</link>
<guid>https://arxiv.org/abs/2505.00275</guid>
<content:encoded><![CDATA[
<div> Video-LLaVA, multimodal interaction, medication adherence, visual question answering, LVLM  
Summary:  
Adherence to medication for chronic diseases is crucial but often hindered by various factors. The AdCare-VLM model uses a large vision language model (LVLM) for visual question answering on medication adherence videos. By fine-tuning on a dataset of tuberculosis medication videos, the model can detect adherence patterns. The LLM-TB-VQA dataset created includes different adherence cases. The model identifies correlations between visual features like patient face visibility and medication intake with medical concepts in captions, improving multimodal interactions. Experimental results show the model outperforms other VLM models with improvements ranging from 3.1% to 3.54%. Ablation studies and attention map visualizations support the effectiveness of the approach, enhancing interpretability.  
<br /><br />Summary: <div>
arXiv:2505.00275v1 Announce Type: cross 
Abstract: Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS, epilepsy, and tuberculosis, necessitate rigorous adherence to medication to avert disease progression, manage symptoms, and decrease mortality rates. Adherence is frequently undermined by factors including patient behavior, caregiver support, elevated medical costs, and insufficient healthcare infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based multimodal large vision language model (LVLM) aimed at visual question answering (VQA) concerning medication adherence through patient videos. We employ a private dataset comprising 806 custom-annotated tuberculosis (TB) medication monitoring videos, which have been labeled by clinical experts, to fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a detailed medical adherence VQA dataset that encompasses positive, negative, and ambiguous adherence cases. Our method identifies correlations between visual features, such as the clear visibility of the patient's face, medication, water intake, and the act of ingestion, and their associated medical concepts in captions. This facilitates the integration of aligned visual-linguistic representations and improves multimodal interactions. Experimental results indicate that our method surpasses parameter-efficient fine-tuning (PEFT) enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute improvements ranging from 3.1% to 3.54% across pre-trained, regular, and low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and attention map visualizations substantiate our approach, enhancing interpretability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba</title>
<link>https://arxiv.org/abs/2508.11849</link>
<guid>https://arxiv.org/abs/2508.11849</guid>
<content:encoded><![CDATA[
<div> selective state-space models, Mamba, vision-driven, DRL framework, cross-modal

Summary:
LocoMamba is a novel vision-driven cross-modal deep reinforcement learning (DRL) framework that utilizes selective state-space models, particularly leveraging Mamba. The framework achieves near-linear-time sequence modeling, effectively capturing long-range dependencies and enabling efficient training with longer sequences. By embedding proprioceptive states and patchifying depth images, compact tokens are produced to improve state representation. Stacked Mamba layers fuse these tokens via near-linear-time selective scanning, reducing latency and memory footprint while remaining robust to token length and image resolution. The policy is trained end-to-end using Proximal Policy Optimization under terrain and appearance randomization and an obstacle-density curriculum. A compact state-centric reward balances progress, smoothness, and safety. Evaluation in challenging simulated environments demonstrates that LocoMamba outperforms state-of-the-art baselines in terms of returns, success rates, collision rates, generalization to unseen terrains and obstacle densities, and training efficiency. <div>
arXiv:2508.11849v2 Announce Type: cross 
Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on selective state-space models, specifically leveraging Mamba, that achieves near-linear-time sequence modeling, effectively captures long-range dependencies, and enables efficient training with longer sequences. First, we embed proprioceptive states with a multilayer perceptron and patchify depth images with a lightweight convolutional neural network, producing compact tokens that improve state representation. Second, stacked Mamba layers fuse these tokens via near-linear-time selective scanning, reducing latency and memory footprint, remaining robust to token length and image resolution, and providing an inductive bias that mitigates overfitting. Third, we train the policy end-to-end with Proximal Policy Optimization under terrain and appearance randomization and an obstacle-density curriculum, using a compact state-centric reward that balances progress, smoothness, and safety. We evaluate our method in challenging simulated environments with static and moving obstacles as well as uneven terrain. Compared with state-of-the-art baselines, our method achieves higher returns and success rates with fewer collisions, exhibits stronger generalization to unseen terrains and obstacle densities, and improves training efficiency by converging in fewer updates under the same compute budget.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonnegative matrix factorization and the principle of the common cause</title>
<link>https://arxiv.org/abs/2509.03652</link>
<guid>https://arxiv.org/abs/2509.03652</guid>
<content:encoded><![CDATA[
arXiv:2509.03652v1 Announce Type: cross 
Abstract: Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction method. The principle of the common cause (PCC) is a basic methodological approach in probabilistic causality, which seeks an independent mixture model for the joint probability of two dependent random variables. It turns out that these two concepts are closely related. This relationship is explored reciprocally for several datasets of gray-scale images, which are conveniently mapped into probability models. On one hand, PCC provides a predictability tool that leads to a robust estimation of the effective rank of NMF. Unlike other estimates (e.g., those based on the Bayesian Information Criteria), our estimate of the rank is stable against weak noise. We show that NMF implemented around this rank produces features (basis images) that are also stable against noise and against seeds of local optimization, thereby effectively resolving the NMF nonidentifiability problem. On the other hand, NMF provides an interesting possibility of implementing PCC in an approximate way, where larger and positively correlated joint probabilities tend to be explained better via the independent mixture model. We work out a clustering method, where data points with the same common cause are grouped into the same cluster. We also show how NMF can be employed for data denoising.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression</title>
<link>https://arxiv.org/abs/2509.05298</link>
<guid>https://arxiv.org/abs/2509.05298</guid>
<content:encoded><![CDATA[
arXiv:2509.05298v1 Announce Type: cross 
Abstract: Loneliness and social isolation pose significant emotional and health challenges, prompting the development of technology-based solutions for companionship and emotional support. This paper introduces Livia, an emotion-aware augmented reality (AR) companion app designed to provide personalized emotional support by combining modular artificial intelligence (AI) agents, multimodal affective computing, progressive memory compression, and AR driven embodied interaction. Livia employs a modular AI architecture with specialized agents responsible for emotion analysis, dialogue generation, memory management, and behavioral orchestration, ensuring robust and adaptive interactions. Two novel algorithms-Temporal Binary Compression (TBC) and Dynamic Importance Memory Filter (DIMF)-effectively manage and prioritize long-term memory, significantly reducing storage requirements while retaining critical context. Our multimodal emotion detection approach achieves high accuracy, enhancing proactive and empathetic engagement. User evaluations demonstrated increased emotional bonds, improved satisfaction, and statistically significant reductions in loneliness. Users particularly valued Livia's adaptive personality evolution and realistic AR embodiment. Future research directions include expanding gesture and tactile interactions, supporting multi-user experiences, and exploring customized hardware implementations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sesame: Opening the door to protein pockets</title>
<link>https://arxiv.org/abs/2509.05302</link>
<guid>https://arxiv.org/abs/2509.05302</guid>
<content:encoded><![CDATA[
arXiv:2509.05302v1 Announce Type: cross 
Abstract: Molecular docking is a cornerstone of drug discovery, relying on high-resolution ligand-bound structures to achieve accurate predictions. However, obtaining these structures is often costly and time-intensive, limiting their availability. In contrast, ligand-free structures are more accessible but suffer from reduced docking performance due to pocket geometries being less suited for ligand accommodation in apo structures. Traditional methods for artificially inducing these conformations, such as molecular dynamics simulations, are computationally expensive. In this work, we introduce Sesame, a generative model designed to predict this conformational change efficiently. By generating geometries better suited for ligand accommodation at a fraction of the computational cost, Sesame aims to provide a scalable solution for improving virtual screening workflows.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats</title>
<link>https://arxiv.org/abs/2509.05303</link>
<guid>https://arxiv.org/abs/2509.05303</guid>
<content:encoded><![CDATA[
arXiv:2509.05303v1 Announce Type: cross 
Abstract: Infrastructure as Code (IaC) is fundamental to modern cloud computing, enabling teams to define and manage infrastructure through machine-readable configuration files. However, different cloud service providers utilize diverse IaC formats. The lack of a standardized format requires cloud architects to be proficient in multiple IaC languages, adding complexity to cloud deployment. While Large Language Models (LLMs) show promise in automating IaC creation and maintenance, progress has been limited by the lack of comprehensive benchmarks across multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark dataset for evaluating LLM-based IaC generation and mutation across AWS CloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset consists of triplets containing initial IaC templates, natural language modification requests, and corresponding updated templates, created through a synthetic data generation pipeline with rigorous validation. We evaluate several state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while modern LLMs can achieve high success rates (>95%) in generating syntactically valid IaC across formats, significant challenges remain in semantic alignment and handling complex infrastructure patterns. Our ablation studies highlight the importance of prompt engineering and retry mechanisms in successful IaC generation. We release Multi-IaC-Bench to facilitate further research in AI-assisted infrastructure management and establish standardized evaluation metrics for this crucial domain.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Log Analysis with AI Agents: Cowrie Case Study</title>
<link>https://arxiv.org/abs/2509.05306</link>
<guid>https://arxiv.org/abs/2509.05306</guid>
<content:encoded><![CDATA[
arXiv:2509.05306v1 Announce Type: cross 
Abstract: The scarcity of real-world attack data significantly hinders progress in cybersecurity research and education. Although honeypots like Cowrie effectively collect live threat intelligence, they generate overwhelming volumes of unstructured and heterogeneous logs, rendering manual analysis impractical. As a first step in our project on secure and efficient AI automation, this study explores the use of AI agents for automated log analysis. We present a lightweight and automated approach to process Cowrie honeypot logs. Our approach leverages AI agents to intelligently parse, summarize, and extract insights from raw data, while also considering the security implications of deploying such an autonomous system. Preliminary results demonstrate the pipeline's effectiveness in reducing manual effort and identifying attack patterns, paving the way for more advanced autonomous cybersecurity analysis in future work.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2509.05309</link>
<guid>https://arxiv.org/abs/2509.05309</guid>
<content:encoded><![CDATA[
arXiv:2509.05309v1 Announce Type: cross 
Abstract: Sparse Autoencoder (SAE) has emerged as a powerful tool for mechanistic interpretability of large language models. Recent works apply SAE to protein language models (PLMs), aiming to extract and analyze biologically meaningful features from their latent spaces. However, SAE suffers from semantic entanglement, where individual neurons often mix multiple nonlinear concepts, making it difficult to reliably interpret or manipulate model behaviors. In this paper, we propose a semantically-guided SAE, called ProtSAE. Unlike existing SAE which requires annotation datasets to filter and interpret activations, we guide semantic disentanglement during training using both annotation datasets and domain knowledge to mitigate the effects of entangled attributes. We design interpretability experiments showing that ProtSAE learns more biologically relevant and interpretable hidden features compared to previous methods. Performance analyses further demonstrate that ProtSAE maintains high reconstruction fidelity while achieving better results in interpretable probing. We also show the potential of ProtSAE in steering PLMs for downstream generation tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations</title>
<link>https://arxiv.org/abs/2509.05311</link>
<guid>https://arxiv.org/abs/2509.05311</guid>
<content:encoded><![CDATA[
arXiv:2509.05311v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has shown great potential for autonomous decision-making in the cybersecurity domain, enabling agents to learn through direct environment interaction. However, RL agents in Autonomous Cyber Operations (ACO) typically learn from scratch, requiring them to execute undesirable actions to learn their consequences. In this study, we integrate external knowledge in the form of a Large Language Model (LLM) pretrained on cybersecurity data that our RL agent can directly leverage to make informed decisions. By guiding initial training with an LLM, we improve baseline performance and reduce the need for exploratory actions with obviously negative outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity environment, and demonstrate that our guided agent achieves over 2x higher rewards during early training and converges to a favorable policy approximately 4,500 episodes faster than the baseline.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory</title>
<link>https://arxiv.org/abs/2509.05314</link>
<guid>https://arxiv.org/abs/2509.05314</guid>
<content:encoded><![CDATA[
arXiv:2509.05314v1 Announce Type: cross 
Abstract: Data scarcity continues to be a major challenge in the field of robotic manipulation. Although diffusion models provide a promising solution for generating robotic manipulation videos, existing methods largely depend on 2D trajectories, which inherently face issues with 3D spatial ambiguity. In this work, we present a novel framework named ManipDreamer3D for generating plausible 3D-aware robotic manipulation videos from the input image and the text instruction. Our method combines 3D trajectory planning with a reconstructed 3D occupancy map created from a third-person perspective, along with a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D first reconstructs the 3D occupancy representation from the input image and then computes an optimized 3D end-effector trajectory, minimizing path length while avoiding collisions. Next, we employ a latent editing technique to create video sequences from the initial image latent and the optimized 3D trajectory. This process conditions our specially trained trajectory-to-video diffusion model to produce robotic pick-and-place videos. Our method generates robotic videos with autonomously planned plausible 3D trajectories, significantly reducing human intervention requirements. Experimental results demonstrate superior visual quality compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2509.05315</link>
<guid>https://arxiv.org/abs/2509.05315</guid>
<content:encoded><![CDATA[
arXiv:2509.05315v1 Announce Type: cross 
Abstract: The rapid evolution of large language models (LLMs) has pushed their boundaries to many applications in various domains. Recently, the research community has started to evaluate their potential adoption in autonomous vehicles and especially as complementary modules in the perception and planning software stacks. However, their evaluation is limited in synthetic datasets or manually driving datasets without the ground truth knowledge and more precisely, how the current perception and planning algorithms would perform in the cases under evaluation. For this reason, this work evaluates LLMs on real-world edge cases where current autonomous vehicles have been proven to fail. The proposed architecture consists of an open vocabulary object detector coupled with prompt engineering and large language model contextual reasoning. We evaluate several state-of-the-art models against real edge cases and provide qualitative comparison results along with a discussion on the findings for the potential application of LLMs as anomaly detectors in autonomous vehicles.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning</title>
<link>https://arxiv.org/abs/2509.05316</link>
<guid>https://arxiv.org/abs/2509.05316</guid>
<content:encoded><![CDATA[
arXiv:2509.05316v1 Announce Type: cross 
Abstract: A conventional LLM Unlearning setting consists of two subsets -"forget" and "retain", with the objectives of removing the undesired knowledge from the forget set while preserving the remaining knowledge from the retain. In privacy-focused unlearning research, a retain set is often further divided into neighbor sets, containing either directly or indirectly connected to the forget targets; and augmented by a general-knowledge set. A common practice in existing benchmarks is to employ only a single neighbor set, with general knowledge which fails to reflect the real-world data complexities and relationships. LLM Unlearning typically involves 1:1 sampling or cyclic iteration sampling. However, the efficacy and stability of these de facto standards have not been critically examined. In this study, we systematically evaluate these common practices. Our findings reveal that relying on a single neighbor set is suboptimal and that a standard sampling approach can obscure performance trade-offs. Based on this analysis, we propose and validate an initial set of best practices: (1) Incorporation of diverse neighbor sets to balance forget efficacy and model utility, (2) Standard 1:1 sampling methods are inefficient and yield poor results, (3) Our proposed Modular Entity-Level Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate that this modular approach, combined with robust algorithms, provides a clear and stable path towards effective unlearning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VILOD: A Visual Interactive Labeling Tool for Object Detection</title>
<link>https://arxiv.org/abs/2509.05317</link>
<guid>https://arxiv.org/abs/2509.05317</guid>
<content:encoded><![CDATA[
arXiv:2509.05317v1 Announce Type: cross 
Abstract: The advancement of Object Detection (OD) using Deep Learning (DL) is often hindered by the significant challenge of acquiring large, accurately labeled datasets, a process that is time-consuming and expensive. While techniques like Active Learning (AL) can reduce annotation effort by intelligently querying informative samples, they often lack transparency, limit the strategic insight of human experts, and may overlook informative samples not aligned with an employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL) approaches integrating human intelligence and intuition throughout the machine learning life-cycle have gained traction. Leveraging Visual Analytics (VA), effective interfaces can be created to facilitate this human-AI collaboration. This thesis explores the intersection of these fields by developing and investigating "VILOD: A Visual Interactive Labeling tool for Object Detection". VILOD utilizes components such as a t-SNE projection of image features, together with uncertainty heatmaps and model state views. Enabling users to explore data, interpret model states, AL suggestions, and implement diverse sample selection strategies within an iterative HITL workflow for OD. An empirical investigation using comparative use cases demonstrated how VILOD, through its interactive visualizations, facilitates the implementation of distinct labeling strategies by making the model's state and dataset characteristics more interpretable (RQ1). The study showed that different visually-guided labeling strategies employed within VILOD result in competitive OD performance trajectories compared to an automated uncertainty sampling AL baseline (RQ2). This work contributes a novel tool and empirical insight into making the HITL-AL workflow for OD annotation more transparent, manageable, and potentially more effective.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models</title>
<link>https://arxiv.org/abs/2509.05318</link>
<guid>https://arxiv.org/abs/2509.05318</guid>
<content:encoded><![CDATA[
arXiv:2509.05318v1 Announce Type: cross 
Abstract: The use of unvetted third-party and internet data renders pre-trained models susceptible to backdoor attacks. Detecting backdoor samples is critical to prevent backdoor activation during inference or injection during training. However, existing detection methods often require the defender to have access to the poisoned models, extra clean samples, or significant computational resources to detect backdoor samples, limiting their practicality. To address this limitation, we propose a backdoor sample detection method based on perturbatio\textbf{N} discr\textbf{E}pancy consis\textbf{T}ency \textbf{E}valuation (\NETE). This is a novel detection method that can be used both pre-training and post-training phases. In the detection process, it only requires an off-the-shelf pre-trained model to compute the log probability of samples and an automated function based on a mask-filling strategy to generate perturbations. Our method is based on the interesting phenomenon that the change in perturbation discrepancy for backdoor samples is smaller than that for clean samples. Based on this phenomenon, we use curvature to measure the discrepancy in log probabilities between different perturbed samples and input samples, thereby evaluating the consistency of the perturbation discrepancy to determine whether the input sample is a backdoor sample. Experiments conducted on four typical backdoor attacks and five types of large language model backdoor attacks demonstrate that our detection strategy outperforms existing zero-shot black-box detection methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD</title>
<link>https://arxiv.org/abs/2509.05321</link>
<guid>https://arxiv.org/abs/2509.05321</guid>
<content:encoded><![CDATA[
arXiv:2509.05321v1 Announce Type: cross 
Abstract: This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion, that leverages the SEED-VD dataset to generate a multimodal dataset of EEG signals conditioned on video stimuli. Additionally, we disclose an engineering pipeline for aligning video and EEG data pairs, facilitating the training of multimodal large models with EEG alignment capabilities. Personalized EEG signals are generated using a self-play graph network (SPGN) integrated with a diffusion model. As a major contribution, we release a new dataset comprising over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG signals at 200 Hz and emotion labels, enabling video-EEG alignment and advancing multimodal research. This framework offers novel tools for emotion analysis, data augmentation, and brain-computer interface applications, with substantial research and engineering significance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Knowledge Proofs in Sublinear Space</title>
<link>https://arxiv.org/abs/2509.05326</link>
<guid>https://arxiv.org/abs/2509.05326</guid>
<content:encoded><![CDATA[
arXiv:2509.05326v1 Announce Type: cross 
Abstract: Modern zero-knowledge proof (ZKP) systems, essential for privacy and verifiable computation, suffer from a fundamental limitation: the prover typically uses memory that scales linearly with the computation's trace length T, making them impractical for resource-constrained devices and prohibitively expensive for large-scale tasks. This paper overcomes this barrier by constructing, to our knowledge, the first sublinear-space ZKP prover. Our core contribution is an equivalence that reframes proof generation as an instance of the classic Tree Evaluation problem. Leveraging a recent space-efficient tree-evaluation algorithm, we design a streaming prover that assembles the proof without ever materializing the full execution trace. The approach reduces prover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms) while preserving proof size, verifier time, and the transcript/security guarantees of the underlying system. This enables a shift from specialized, server-bound proving to on-device proving, opening applications in decentralized systems, on-device machine learning, and privacy-preserving technologies.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optical Music Recognition of Jazz Lead Sheets</title>
<link>https://arxiv.org/abs/2509.05329</link>
<guid>https://arxiv.org/abs/2509.05329</guid>
<content:encoded><![CDATA[
arXiv:2509.05329v1 Announce Type: cross 
Abstract: In this paper, we address the challenge of Optical Music Recognition (OMR) for handwritten jazz lead sheets, a widely used musical score type that encodes melody and chords. The task is challenging due to the presence of chords, a score component not handled by existing OMR systems, and the high variability and quality issues associated with handwritten images. Our contribution is two-fold. We present a novel dataset consisting of 293 handwritten jazz lead sheets of 163 unique pieces, amounting to 2021 total staves aligned with Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score images generated from the ground truth. The second contribution is the development of an OMR model for jazz lead sheets. We discuss specific tokenisation choices related to our kind of data, and the advantages of using synthetic scores and pretrained models. We publicly release all code, data, and models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForensicsData: A Digital Forensics Dataset for Large Language Models</title>
<link>https://arxiv.org/abs/2509.05331</link>
<guid>https://arxiv.org/abs/2509.05331</guid>
<content:encoded><![CDATA[
arXiv:2509.05331v1 Announce Type: cross 
Abstract: The growing complexity of cyber incidents presents significant challenges for digital forensic investigators, especially in evidence collection and analysis. Public resources are still limited because of ethical, legal, and privacy concerns, even though realistic datasets are necessary to support research and tool developments. To address this gap, we introduce ForensicsData, an extensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware analysis reports. It consists of more than 5,000 Q-C-A triplets. A unique workflow was used to create the dataset, which extracts structured data, uses large language models (LLMs) to transform it into Q-C-A format, and then uses a specialized evaluation process to confirm its quality. Among the models evaluated, Gemini 2 Flash demonstrated the best performance in aligning generated content with forensic terminology. ForensicsData aims to advance digital forensics by enabling reproducible experiments and fostering collaboration within the research community.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2509.05332</link>
<guid>https://arxiv.org/abs/2509.05332</guid>
<content:encoded><![CDATA[
arXiv:2509.05332v1 Announce Type: cross 
Abstract: Autonomous vehicles (AVs) rely on complex perception and communication systems, making them vulnerable to adversarial attacks that can compromise safety. While simulation offers a scalable and safe environment for robustness testing, existing frameworks typically lack comprehensive supportfor modeling multi-domain adversarial scenarios. This paper introduces a novel, open-source integrated simulation framework designed to generate adversarial attacks targeting both perception and communication layers of AVs. The framework provides high-fidelity modeling of physical environments, traffic dynamics, and V2X networking, orchestrating these components through a unified core that synchronizes multiple simulators based on a single configuration file. Our implementation supports diverse perception-level attacks on LiDAR sensor data, along with communication-level threats such as V2X message manipulation and GPS spoofing. Furthermore, ROS 2 integration ensures seamless compatibility with third-party AV software stacks. We demonstrate the framework's effectiveness by evaluating the impact of generated adversarial scenarios on a state-of-the-art 3D object detector, revealing significant performance degradation under realistic conditions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness</title>
<link>https://arxiv.org/abs/2509.05333</link>
<guid>https://arxiv.org/abs/2509.05333</guid>
<content:encoded><![CDATA[
arXiv:2509.05333v1 Announce Type: cross 
Abstract: Real world deployments often expose modern object recognition models to domain shifts that precipitate a severe drop in accuracy. Such shifts encompass (i) variations in low level image statistics, (ii) changes in object pose and viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent classes. To mitigate this degradation, we introduce the Re-Thinking Vision Language Model (RT-VLM) framework. The foundation of this framework is a unique synthetic dataset generation pipeline that produces images annotated with "4-Clues": precise bounding boxes, class names, detailed object-level captions, and a comprehensive context-level caption for the entire scene. We then perform parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this resource. At inference time, a two stage Re-Thinking scheme is executed: the model first emits its own four clues, then re examines these responses as evidence and iteratively corrects them. Across robustness benchmarks that isolate individual domain shifts, RT-VLM consistently surpasses strong baselines. These findings indicate that the integration of structured multimodal evidence with an explicit self critique loop constitutes a promising route toward reliable and transferable visual understanding.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plantbot: Integrating Plant and Robot through LLM Modular Agent Networks</title>
<link>https://arxiv.org/abs/2509.05338</link>
<guid>https://arxiv.org/abs/2509.05338</guid>
<content:encoded><![CDATA[
arXiv:2509.05338v1 Announce Type: cross 
Abstract: We introduce Plantbot, a hybrid lifeform that connects a living plant with a mobile robot through a network of large language model (LLM) modules. Each module - responsible for sensing, vision, dialogue, or action - operates asynchronously and communicates via natural language, enabling seamless interaction across biological and artificial domains. This architecture leverages the capacity of LLMs to serve as hybrid interfaces, where natural language functions as a universal protocol, translating multimodal data (soil moisture, temperature, visual context) into linguistic messages that coordinate system behaviors. The integrated network transforms plant states into robotic actions, installing normativity essential for agency within the sensor-motor loop. By combining biological and robotic elements through LLM-mediated communication, Plantbot behaves as an embodied, adaptive agent capable of responding autonomously to environmental conditions. This approach suggests possibilities for a new model of artificial life, where decentralized, LLM modules coordination enable novel interactions between biological and artificial systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging</title>
<link>https://arxiv.org/abs/2509.05340</link>
<guid>https://arxiv.org/abs/2509.05340</guid>
<content:encoded><![CDATA[
arXiv:2509.05340v1 Announce Type: cross 
Abstract: Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a pivotal challenge in medical image analysis due to the heterogeneous nature of tumor morphology and intensity distributions. Accurate delineation of tumor boundaries is critical for clinical decision-making, radiotherapy planning, and longitudinal disease monitoring. In this study, we perform a comprehensive comparative analysis of two major clustering paradigms applied in MRI tumor segmentation: hard clustering, exemplified by the K-Means algorithm, and soft clustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each pixel strictly to a single cluster, FCM introduces partial memberships, meaning each pixel can belong to multiple clusters with varying degrees of association. Experimental validation was performed using the BraTS2020 dataset, incorporating pre-processing through Gaussian filtering and Contrast Limited Adaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice Similarity Coefficient (DSC) and processing time, which collectively demonstrated that K-Means achieved superior speed with an average runtime of 0.3s per image, whereas FCM attained higher segmentation accuracy with an average DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher computational cost (1.3s per image). These results highlight the inherent trade-off between computational efficiency and boundary precision.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Instance Segmentation with Superpixels</title>
<link>https://arxiv.org/abs/2509.05352</link>
<guid>https://arxiv.org/abs/2509.05352</guid>
<content:encoded><![CDATA[
arXiv:2509.05352v1 Announce Type: cross 
Abstract: Instance segmentation is essential for numerous computer vision applications, including robotics, human-computer interaction, and autonomous driving. Currently, popular models bring impressive performance in instance segmentation by training with a large number of human annotations, which are costly to collect. For this reason, we present a new framework that efficiently and effectively segments objects without the need for human annotations. Firstly, a MultiCut algorithm is applied to self-supervised features for coarse mask segmentation. Then, a mask filter is employed to obtain high-quality coarse masks. To train the segmentation network, we compute a novel superpixel-guided mask loss, comprising hard loss and soft loss, with high-quality coarse masks and superpixels segmented from low-level image features. Lastly, a self-training process with a new adaptive loss is proposed to improve the quality of predicted masks. We conduct experiments on public datasets in instance segmentation and object detection to demonstrate the effectiveness of the proposed framework. The results show that the proposed framework outperforms previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spiking Neural Networks for Continuous Control via End-to-End Model-Based Learning</title>
<link>https://arxiv.org/abs/2509.05356</link>
<guid>https://arxiv.org/abs/2509.05356</guid>
<content:encoded><![CDATA[
arXiv:2509.05356v1 Announce Type: cross 
Abstract: Despite recent progress in training spiking neural networks (SNNs) for classification, their application to continuous motor control remains limited. Here, we demonstrate that fully spiking architectures can be trained end-to-end to control robotic arms with multiple degrees of freedom in continuous environments. Our predictive-control framework combines Leaky Integrate-and-Fire dynamics with surrogate gradients, jointly optimizing a forward model for dynamics prediction and a policy network for goal-directed action. We evaluate this approach on both a planar 2D reaching task and a simulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve stable training and accurate torque control, establishing their viability for high-dimensional motor tasks. An extensive ablation study highlights the role of initialization, learnable time constants, and regularization in shaping training dynamics. We conclude that while stable and effective control can be achieved, recurrent spiking networks remain highly sensitive to hyperparameter settings, underscoring the importance of principled design choices.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training</title>
<link>https://arxiv.org/abs/2509.05359</link>
<guid>https://arxiv.org/abs/2509.05359</guid>
<content:encoded><![CDATA[
arXiv:2509.05359v1 Announce Type: cross 
Abstract: This paper investigates discrete unit representations in Speech Language Models (SLMs), focusing on optimizing speech modeling during continual pre-training. In this paper, we systematically examine how model architecture, data representation, and training robustness influence the pre-training stage in which we adapt existing pre-trained language models to the speech modality. Our experiments highlight the role of speech encoders and clustering granularity across different model scales, showing how optimal discretization strategies vary with model capacity. By examining cluster distribution and phonemic alignments, we investigate the effective use of discrete vocabulary, uncovering both linguistic and paralinguistic patterns. Additionally, we explore the impact of clustering data selection on model robustness, highlighting the importance of domain matching between discretization training and target applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governing AI R&amp;D: A Legal Framework for Constraining Dangerous AI</title>
<link>https://arxiv.org/abs/2509.05361</link>
<guid>https://arxiv.org/abs/2509.05361</guid>
<content:encoded><![CDATA[
arXiv:2509.05361v1 Announce Type: cross 
Abstract: As AI advances, governing its development may become paramount to public safety. Lawmakers may seek to restrict the development and release of AI models or of AI research itself. These governance actions could trigger legal challenges that invalidate the actions, so lawmakers should consider these challenges ahead of time. We investigate three classes of potential litigation risk for AI regulation in the U.S.: the First Amendment, administrative law, and the Fourteenth Amendment. We discuss existing precedent that is likely to apply to AI, which legal challenges are likely to arise, and how lawmakers might preemptively address them. Effective AI regulation is possible, but it requires careful implementation to avoid these legal challenges.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning</title>
<link>https://arxiv.org/abs/2509.05362</link>
<guid>https://arxiv.org/abs/2509.05362</guid>
<content:encoded><![CDATA[
arXiv:2509.05362v1 Announce Type: cross 
Abstract: Scams exploiting real-time social engineering -- such as phishing, impersonation, and phone fraud -- remain a persistent and evolving threat across digital platforms. Existing defenses are largely reactive, offering limited protection during active interactions. We propose a privacy-preserving, AI-in-the-loop framework that proactively detects and disrupts scam conversations in real time. The system combines instruction-tuned artificial intelligence with a safety-aware utility function that balances engagement with harm minimization, and employs federated learning to enable continual model updates without raw data sharing. Experimental evaluations show that the system produces fluent and engaging responses (perplexity as low as 22.3, engagement $\approx$0.80), while human studies confirm significant gains in realism, safety, and effectiveness over strong baselines. In federated settings, models trained with FedAvg sustain up to 30 rounds while preserving high engagement ($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage ($\leq$0.0085). Even with differential privacy, novelty and safety remain stable, indicating that robust privacy can be achieved without sacrificing performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3, MD-Judge) shows a straightforward pattern: stricter moderation settings reduce the chance of exposing personal information, but they also limit how much the model engages in conversation. In contrast, more relaxed settings allow longer and richer interactions, which improve scam detection, but at the cost of higher privacy risk. To our knowledge, this is the first framework to unify real-time scam-baiting, federated privacy preservation, and calibrated safety moderation into a proactive defense paradigm.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prototyping an AI-powered Tool for Energy Efficiency in New Zealand Homes</title>
<link>https://arxiv.org/abs/2509.05364</link>
<guid>https://arxiv.org/abs/2509.05364</guid>
<content:encoded><![CDATA[
arXiv:2509.05364v1 Announce Type: cross 
Abstract: Residential buildings contribute significantly to energy use, health outcomes, and carbon emissions. In New Zealand, housing quality has historically been poor, with inadequate insulation and inefficient heating contributing to widespread energy hardship. Recent reforms, including the Warmer Kiwi Homes program, Healthy Homes Standards, and H1 Building Code upgrades, have delivered health and comfort improvements, yet challenges persist. Many retrofits remain partial, data on household performance are limited, and decision-making support for homeowners is fragmented. This study presents the design and evaluation of an AI-powered decision-support tool for residential energy efficiency in New Zealand. The prototype, developed using Python and Streamlit, integrates data ingestion, anomaly detection, baseline modeling, and scenario simulation (e.g., LED retrofits, insulation upgrades) into a modular dashboard. Fifteen domain experts, including building scientists, consultants, and policy practitioners, tested the tool through semi-structured interviews. Results show strong usability (M = 4.3), high value of scenario outputs (M = 4.5), and positive perceptions of its potential to complement subsidy programs and regulatory frameworks. The tool demonstrates how AI can translate national policies into personalized, household-level guidance, bridging the gap between funding, standards, and practical decision-making. Its significance lies in offering a replicable framework for reducing energy hardship, improving health outcomes, and supporting climate goals. Future development should focus on carbon metrics, tariff modeling, integration with national datasets, and longitudinal trials to assess real-world adoption.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs</title>
<link>https://arxiv.org/abs/2509.05367</link>
<guid>https://arxiv.org/abs/2509.05367</guid>
<content:encoded><![CDATA[
arXiv:2509.05367v1 Announce Type: cross 
Abstract: Large language models (LLMs) have undergone safety alignment efforts to mitigate harmful outputs. However, as LLMs become more sophisticated in reasoning, their intelligence may introduce new security risks. While traditional jailbreak attacks relied on singlestep attacks, multi-turn jailbreak strategies that adapt dynamically to context remain underexplored. In this work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack Logic), a framework that leverages LLMs ethical reasoning to bypass their safeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on the trolley problem. TRIAL demonstrates high jailbreak success rates towards both open and close-source models. Our findings underscore a fundamental limitation in AI safety: as models gain advanced reasoning abilities, the nature of their alignment may inadvertently allow for more covert security vulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating safety alignment oversight strategies, as current safeguards may prove insufficient against context-aware adversarial attack.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Horizon Visual Imitation Learning via Plan and Code Reflection</title>
<link>https://arxiv.org/abs/2509.05368</link>
<guid>https://arxiv.org/abs/2509.05368</guid>
<content:encoded><![CDATA[
arXiv:2509.05368v1 Announce Type: cross 
Abstract: Learning from long-horizon demonstrations with complex action sequences presents significant challenges for visual imitation learning, particularly in understanding temporal relationships of actions and spatial relationships between objects. In this paper, we propose a new agent framework that incorporates two dedicated reflection modules to enhance both plan and code generation. The plan generation module produces an initial action sequence, which is then verified by the plan reflection module to ensure temporal coherence and spatial alignment with the demonstration video. The code generation module translates the plan into executable code, while the code reflection module verifies and refines the generated code to ensure correctness and consistency with the generated plan. These two reflection modules jointly enable the agent to detect and correct errors in both the plan generation and code generation, improving performance in tasks with intricate temporal and spatial dependencies. To support systematic evaluation, we introduce LongVILBench, a benchmark comprising 300 human demonstrations with action sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial complexity across multiple task types. Experimental results demonstrate that existing methods perform poorly on this benchmark, whereas our new framework establishes a strong baseline for long-horizon visual imitation learning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments</title>
<link>https://arxiv.org/abs/2509.05376</link>
<guid>https://arxiv.org/abs/2509.05376</guid>
<content:encoded><![CDATA[
arXiv:2509.05376v1 Announce Type: cross 
Abstract: Eye-tracking technology can aid in understanding neurodevelopmental disorders and tracing a person's identity. However, this technology poses a significant risk to privacy, as it captures sensitive information about individuals and increases the likelihood that data can be traced back to them. This paper proposes a human-centered framework designed to prevent identity backtracking while preserving the pedagogical benefits of AI-powered eye tracking in interactive learning environments. We explore how real-time data anonymization, ethical design principles, and regulatory compliance (such as GDPR) can be integrated to build trust and transparency. We first demonstrate the potential for backtracking student IDs and diagnoses in various scenarios using serious game-based eye-tracking data. We then provide a two-stage privacy-preserving framework that prevents participants from being tracked while still enabling diagnostic classification. The first phase covers four scenarios: I) Predicting disorder diagnoses based on different game levels. II) Predicting student IDs based on different game levels. III) Predicting student IDs based on randomized data. IV) Utilizing K-Means for out-of-sample data. In the second phase, we present a two-stage framework that preserves privacy. We also employ Federated Learning (FL) across multiple clients, incorporating a secure identity management system with dummy IDs and administrator-only access controls. In the first phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63% accuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully identifying and assigning a new student ID in scenario 4. In phase 2, we effectively prevented backtracking and established a secure identity management system with dummy IDs and administrator-only access controls, achieving an overall accuracy of 99.40%.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling</title>
<link>https://arxiv.org/abs/2509.05379</link>
<guid>https://arxiv.org/abs/2509.05379</guid>
<content:encoded><![CDATA[
arXiv:2509.05379v1 Announce Type: cross 
Abstract: As our cities and communities become smarter, the systems that keep us safe, such as traffic control centers, emergency response networks, and public transportation, also become more complex. With this complexity comes a greater risk of security threats that can affect not just machines but real people's lives. To address this challenge, we present ThreatGPT, an agentic Artificial Intelligence (AI) assistant built to help people whether they are engineers, safety officers, or policy makers to understand and analyze threats in public safety systems. Instead of requiring deep cybersecurity expertise, it allows users to simply describe the components of a system they are concerned about, such as login systems, data storage, or communication networks. Then, with the click of a button, users can choose how they want the system to be analyzed by using popular frameworks such as STRIDE, MITRE ATT&amp;CK, CVE reports, NIST, or CISA. ThreatGPT is unique because it does not just provide threat information, but rather it acts like a knowledgeable partner. Using few-shot learning, the AI learns from examples and generates relevant smart threat models. It can highlight what might go wrong, how attackers could take advantage, and what can be done to prevent harm. Whether securing a city's infrastructure or a local health service, this tool adapts to users' needs. In simple terms, ThreatGPT brings together AI and human judgment to make our public systems safer. It is designed not just to analyze threats, but to empower people to understand and act on them, faster, smarter, and with more confidence.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cumplimiento del Reglamento (UE) 2024/1689 en rob\'otica y sistemas aut\'onomos: una revisi\'on sistem\'atica de la literatura</title>
<link>https://arxiv.org/abs/2509.05380</link>
<guid>https://arxiv.org/abs/2509.05380</guid>
<content:encoded><![CDATA[
arXiv:2509.05380v1 Announce Type: cross 
Abstract: This systematic literature review analyzes the current state of compliance with Regulation (EU) 2024/1689 in autonomous robotic systems, focusing on cybersecurity frameworks and methodologies. Using the PRISMA protocol, 22 studies were selected from 243 initial records across IEEE Xplore, ACM DL, Scopus, and Web of Science. Findings reveal partial regulatory alignment: while progress has been made in risk management and encrypted communications, significant gaps persist in explainability modules, real-time human oversight, and knowledge base traceability. Only 40% of reviewed solutions explicitly address transparency requirements, and 30% implement failure intervention mechanisms. The study concludes that modular approaches integrating risk, supervision, and continuous auditing are essential to meet the AI Act mandates in autonomous robotics.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Privacy and Large Language Models: An Analysis of Frontier Developers' Privacy Policies</title>
<link>https://arxiv.org/abs/2509.05382</link>
<guid>https://arxiv.org/abs/2509.05382</guid>
<content:encoded><![CDATA[
arXiv:2509.05382v1 Announce Type: cross 
Abstract: Hundreds of millions of people now regularly interact with large language models via chatbots. Model developers are eager to acquire new sources of high-quality training data as they race to improve model capabilities and win market share. This paper analyzes the privacy policies of six U.S. frontier AI developers to understand how they use their users' chats to train models. Drawing primarily on the California Consumer Privacy Act, we develop a novel qualitative coding schema that we apply to each developer's relevant privacy policies to compare data collection and use practices across the six companies. We find that all six developers appear to employ their users' chat data to train and improve their models by default, and that some retain this data indefinitely. Developers may collect and train on personal information disclosed in chats, including sensitive information such as biometric and health data, as well as files uploaded by users. Four of the six companies we examined appear to include children's chat data for model training, as well as customer data from other products. On the whole, developers' privacy policies often lack essential information about their practices, highlighting the need for greater transparency and accountability. We address the implications of users' lack of consent for the use of their chat data for model training, data security issues arising from indefinite chat data retention, and training on children's chat data. We conclude by providing recommendations to policymakers and developers to address the data privacy challenges posed by LLM-powered chatbots.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs</title>
<link>https://arxiv.org/abs/2509.05385</link>
<guid>https://arxiv.org/abs/2509.05385</guid>
<content:encoded><![CDATA[
arXiv:2509.05385v1 Announce Type: cross 
Abstract: Large language models are unable to continuously adapt and learn from new data during reasoning at inference time. To address this limitation, we propose that complex reasoning tasks be decomposed into atomic subtasks and introduce SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive updates during reasoning at inference time. SAGE consists of three key components: (1) a Trigger module that detects reasoning failures through multiple evaluation metrics in real time; (2) a Trigger Buffer module that clusters anomaly samples using a streaming clustering process with HDBSCAN, followed by stability checks and similarity-based merging; and (3) a Lora Store module that dynamically optimizes parameter updates with an adapter pool for knowledge retention. Evaluation results show that SAGE demonstrates excellent accuracy, robustness, and stability on the atomic reasoning subtask through dynamic knowledge updating during test time.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmented Structure Preserving Neural Networks for cell biomechanics</title>
<link>https://arxiv.org/abs/2509.05388</link>
<guid>https://arxiv.org/abs/2509.05388</guid>
<content:encoded><![CDATA[
arXiv:2509.05388v1 Announce Type: cross 
Abstract: Cell biomechanics involve a great number of complex phenomena that are fundamental to the evolution of life itself and other associated processes, ranging from the very early stages of embryo-genesis to the maintenance of damaged structures or the growth of tumors. Given the importance of such phenomena, increasing research has been dedicated to their understanding, but the many interactions between them and their influence on the decisions of cells as a collective network or cluster remain unclear. We present a new approach that combines Structure Preserving Neural Networks, which study cell movements as a purely mechanical system, with other Machine Learning tools (Artificial Neural Networks), which allow taking into consideration environmental factors that can be directly deduced from an experiment with Computer Vision techniques. This new model, tested on simulated and real cell migration cases, predicts complete cell trajectories following a roll-out policy with a high level of accuracy. This work also includes a mitosis event prediction model based on Neural Networks architectures which makes use of the same observed features.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Authorship Without Writing: Large Language Models and the Senior Author Analogy</title>
<link>https://arxiv.org/abs/2509.05390</link>
<guid>https://arxiv.org/abs/2509.05390</guid>
<content:encoded><![CDATA[
arXiv:2509.05390v1 Announce Type: cross 
Abstract: The use of large language models (LLMs) in bioethical, scientific, and medical writing remains controversial. While there is broad agreement in some circles that LLMs cannot count as authors, there is no consensus about whether and how humans using LLMs can count as authors. In many fields, authorship is distributed among large teams of researchers, some of whom, including paradigmatic senior authors who guide and determine the scope of a project and ultimately vouch for its integrity, may not write a single word. In this paper, we argue that LLM use (under specific conditions) is analogous to a form of senior authorship. On this view, the use of LLMs, even to generate complete drafts of research papers, can be considered a legitimate form of authorship according to the accepted criteria in many fields. We conclude that either such use should be recognized as legitimate, or current criteria for authorship require fundamental revision. AI use declaration: GPT-5 was used to help format Box 1. AI was not used for any other part of the preparation or writing of this manuscript.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Optimized Pipeline for Automatic Educational Knowledge Graph Construction</title>
<link>https://arxiv.org/abs/2509.05392</link>
<guid>https://arxiv.org/abs/2509.05392</guid>
<content:encoded><![CDATA[
arXiv:2509.05392v1 Announce Type: cross 
Abstract: The automatic construction of Educational Knowledge Graphs (EduKGs) is essential for domain knowledge modeling by extracting meaningful representations from learning materials. Despite growing interest, identifying a scalable and reliable approach for automatic EduKG generation remains a challenge. In an attempt to develop a unified and robust pipeline for automatic EduKG construction, in this study we propose a pipeline for automatic EduKG construction from PDF learning materials. The process begins with generating slide-level EduKGs from individual pages/slides, which are then merged to form a comprehensive EduKG representing the entire learning material. We evaluate the accuracy of the EduKG generated from the proposed pipeline in our MOOC platform, CourseMapper. The observed accuracy, while indicative of partial success, is relatively low particularly in the educational context, where the reliability of knowledge representations is critical for supporting meaningful learning. To address this, we introduce targeted optimizations across multiple pipeline components. The optimized pipeline achieves a 17.5% improvement in accuracy and a tenfold increase in processing efficiency. Our approach offers a holistic, scalable and end-to-end pipeline for automatic EduKG construction, adaptable to diverse educational contexts, and supports improved semantic representation of learning content.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring Prerequisite Knowledge Concepts in Educational Knowledge Graphs: A Multi-criteria Approach</title>
<link>https://arxiv.org/abs/2509.05393</link>
<guid>https://arxiv.org/abs/2509.05393</guid>
<content:encoded><![CDATA[
arXiv:2509.05393v1 Announce Type: cross 
Abstract: Educational Knowledge Graphs (EduKGs) organize various learning entities and their relationships to support structured and adaptive learning. Prerequisite relationships (PRs) are critical in EduKGs for defining the logical order in which concepts should be learned. However, the current EduKG in the MOOC platform CourseMapper lacks explicit PR links, and manually annotating them is time-consuming and inconsistent. To address this, we propose an unsupervised method for automatically inferring concept PRs without relying on labeled data. We define ten criteria based on document-based, Wikipedia hyperlink-based, graph-based, and text-based features, and combine them using a voting algorithm to robustly capture PRs in educational content. Experiments on benchmark datasets show that our approach achieves higher precision than existing methods while maintaining scalability and adaptability, thus providing reliable support for sequence-aware learning in CourseMapper.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse Browser: Vector-Image-to-Code Generator</title>
<link>https://arxiv.org/abs/2509.05394</link>
<guid>https://arxiv.org/abs/2509.05394</guid>
<content:encoded><![CDATA[
arXiv:2509.05394v1 Announce Type: cross 
Abstract: Automating the conversion of user interface design into code (image-to-code or image-to-UI) is an active area of software engineering research. However, the state-of-the-art solutions do not achieve high fidelity to the original design, as evidenced by benchmarks. In this work, I approach the problem differently: I use vector images instead of bitmaps as model input. I create several large datasets for training machine learning models. I evaluate the available array of Image Quality Assessment (IQA) algorithms and introduce a new, multi-scale metric. I then train a large open-weights model and discuss its limitations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2509.05396</link>
<guid>https://arxiv.org/abs/2509.05396</guid>
<content:encoded><![CDATA[
arXiv:2509.05396v1 Announce Type: cross 
Abstract: While multi-agent debate has been proposed as a promising strategy for improving AI reasoning ability, we find that debate can sometimes be harmful rather than helpful. The prior work has exclusively focused on debates within homogeneous groups of agents, whereas we explore how diversity in model capabilities influences the dynamics and outcomes of multi-agent interactions. Through a series of experiments, we demonstrate that debate can lead to a decrease in accuracy over time -- even in settings where stronger (i.e., more capable) models outnumber their weaker counterparts. Our analysis reveals that models frequently shift from correct to incorrect answers in response to peer reasoning, favoring agreement over challenging flawed reasoning. These results highlight important failure modes in the exchange of reasons during multi-agent debate, suggesting that naive applications of debate may cause performance degradation when agents are neither incentivized nor adequately equipped to resist persuasive but incorrect reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Connectionist Temporal Classification for Phoneme Recognition</title>
<link>https://arxiv.org/abs/2509.05399</link>
<guid>https://arxiv.org/abs/2509.05399</guid>
<content:encoded><![CDATA[
arXiv:2509.05399v1 Announce Type: cross 
Abstract: Automatic Phoneme Recognition (APR) systems are often trained using pseudo phoneme-level annotations generated from text through Grapheme-to-Phoneme (G2P) systems. These G2P systems frequently output multiple possible pronunciations per word, but the standard Connectionist Temporal Classification (CTC) loss cannot account for such ambiguity during training. In this work, we adapt Graph Temporal Classification (GTC) to the APR setting. GTC enables training from a graph of alternative phoneme sequences, allowing the model to consider multiple pronunciations per word as valid supervision. Our experiments on English and Dutch data sets show that incorporating multiple pronunciations per word into the training loss consistently improves phoneme error rates compared to a baseline trained with CTC. These results suggest that integrating pronunciation variation into the loss function is a promising strategy for training APR systems from noisy G2P-based supervision.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universality of physical neural networks with multivariate nonlinearity</title>
<link>https://arxiv.org/abs/2509.05420</link>
<guid>https://arxiv.org/abs/2509.05420</guid>
<content:encoded><![CDATA[
arXiv:2509.05420v1 Announce Type: cross 
Abstract: The enormous energy demand of artificial intelligence is driving the development of alternative hardware for deep learning. Physical neural networks try to exploit physical systems to perform machine learning more efficiently. In particular, optical systems can calculate with light using negligible energy. While their computational capabilities were long limited by the linearity of optical materials, nonlinear computations have recently been demonstrated through modified input encoding. Despite this breakthrough, our inability to determine if physical neural networks can learn arbitrary relationships between data -- a key requirement for deep learning known as universality -- hinders further progress. Here we present a fundamental theorem that establishes a universality condition for physical neural networks. It provides a powerful mathematical criterion that imposes device constraints, detailing how inputs should be encoded in the tunable parameters of the physical system. Based on this result, we propose a scalable architecture using free-space optics that is provably universal and achieves high accuracy on image classification tasks. Further, by combining the theorem with temporal multiplexing, we present a route to potentially huge effective system sizes in highly practical but poorly scalable on-chip photonic devices. Our theorem and scaling methods apply beyond optical systems and inform the design of a wide class of universal, energy-efficient physical neural networks, justifying further efforts in their development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Translation Needed: Forecasting Quality from Fertility and Metadata</title>
<link>https://arxiv.org/abs/2509.05425</link>
<guid>https://arxiv.org/abs/2509.05425</guid>
<content:encoded><![CDATA[
arXiv:2509.05425v1 Announce Type: cross 
Abstract: We show that translation quality can be predicted with surprising accuracy \textit{without ever running the translation system itself}. Using only a handful of features, token fertility ratios, token counts, and basic linguistic metadata (language family, script, and region), we can forecast ChrF scores for GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient boosting models achieve favorable performance ($R^{2}=0.66$ for XX$\rightarrow$English and $R^{2}=0.72$ for English$\rightarrow$XX). Feature importance analyses reveal that typological factors dominate predictions into English, while fertility plays a larger role for translations into diverse target languages. These findings suggest that translation quality is shaped by both token-level fertility and broader linguistic typology, offering new insights for multilingual evaluation and quality estimation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding</title>
<link>https://arxiv.org/abs/2509.05431</link>
<guid>https://arxiv.org/abs/2509.05431</guid>
<content:encoded><![CDATA[
arXiv:2509.05431v1 Announce Type: cross 
Abstract: Brain tumor segmentation is a critical pre-processing step in the medical image analysis pipeline that involves precise delineation of tumor regions from healthy brain tissue in medical imaging data, particularly MRI scans. An efficient and effective decoding mechanism is crucial in brain tumor segmentation especially in scenarios with limited computational resources. However these decoding mechanisms usually come with high computational costs. To address this concern EMCAD a new efficient multi-scale convolutional attention decoder designed was utilized to optimize both performance and computational efficiency for brain tumor segmentation on the BraTs2020 dataset consisting of MRI scans from 369 brain tumor patients. The preliminary result obtained by the model achieved a best Dice score of 0.31 and maintained a stable mean Dice score of 0.285 plus/minus 0.015 throughout the training process which is moderate. The initial model maintained consistent performance across the validation set without showing signs of over-fitting.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too</title>
<link>https://arxiv.org/abs/2509.05440</link>
<guid>https://arxiv.org/abs/2509.05440</guid>
<content:encoded><![CDATA[
arXiv:2509.05440v1 Announce Type: cross 
Abstract: As large-language models have been increasingly used as automatic raters for evaluating free-form content, including document summarization, dialog, and story generation, work has been dedicated to evaluating such models by measuring their correlations with human judgment. For \textit{sample-level} performance, methods which operate by using pairwise comparisons between machine-generated text perform well but often lack the ability to assign absolute scores to individual summaries, an ability crucial for use cases that require thresholding. In this work, we propose a direct-scoring method which uses synthetic summaries to act as pairwise machine rankings at test time. We show that our method performs comparably to state-of-the-art pairwise evaluators in terms of axis-averaged sample-level correlations on the SummEval (\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05}) meta-evaluation benchmarks, and release the synthetic in-context summaries as data to facilitate future work.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Newton to Einstein: Axiom-Based Discovery via Game Design</title>
<link>https://arxiv.org/abs/2509.05448</link>
<guid>https://arxiv.org/abs/2509.05448</guid>
<content:encoded><![CDATA[
arXiv:2509.05448v1 Announce Type: cross 
Abstract: This position paper argues that machine learning for scientific discovery should shift from inductive pattern recognition to axiom-based reasoning. We propose a game design framework in which scientific inquiry is recast as a rule-evolving system: agents operate within environments governed by axioms and modify them to explain outlier observations. Unlike conventional ML approaches that operate within fixed assumptions, our method enables the discovery of new theoretical structures through systematic rule adaptation. We demonstrate the feasibility of this approach through preliminary experiments in logic-based games, showing that agents can evolve axioms that solve previously unsolvable problems. This framework offers a foundation for building machine learning systems capable of creative, interpretable, and theory-driven discovery.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis</title>
<link>https://arxiv.org/abs/2509.05449</link>
<guid>https://arxiv.org/abs/2509.05449</guid>
<content:encoded><![CDATA[
arXiv:2509.05449v1 Announce Type: cross 
Abstract: Membership inference attacks (MIAs) reveal whether specific data was used to train machine learning models, serving as important tools for privacy auditing and compliance assessment. Recent studies have reported that MIAs perform only marginally better than random guessing against large language models, suggesting that modern pre-training approaches with massive datasets may be free from privacy leakage risks. Our work offers a complementary perspective to these findings by exploring how examining LLMs' internal representations, rather than just their outputs, may provide additional insights into potential membership inference signals. Our framework, \emph{memTrace}, follows what we call \enquote{neural breadcrumbs} extracting informative signals from transformer hidden states and attention patterns as they process candidate sequences. By analyzing layer-wise representation dynamics, attention distribution characteristics, and cross-layer transition patterns, we detect potential memorization fingerprints that traditional loss-based approaches may not capture. This approach yields strong membership detection across several model families achieving average AUC scores of 0.85 on popular MIA benchmarks. Our findings suggest that internal model behaviors can reveal aspects of training data exposure even when output-based signals appear protected, highlighting the need for further research into membership privacy and the development of more robust privacy-preserving training techniques for large language models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models</title>
<link>https://arxiv.org/abs/2509.05471</link>
<guid>https://arxiv.org/abs/2509.05471</guid>
<content:encoded><![CDATA[
arXiv:2509.05471v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly vulnerable to a sophisticated form of adversarial prompting known as camouflaged jailbreaking. This method embeds malicious intent within seemingly benign language to evade existing safety mechanisms. Unlike overt attacks, these subtle prompts exploit contextual ambiguity and the flexible nature of language, posing significant challenges to current defense systems. This paper investigates the construction and impact of camouflaged jailbreak prompts, emphasizing their deceptive characteristics and the limitations of traditional keyword-based detection methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts, containing 500 curated examples (400 harmful and 100 benign prompts) designed to rigorously stress-test LLM safety protocols. In addition, we propose a multi-faceted evaluation framework that measures harmfulness across seven dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards, Harmful Potential, Educational Value, Content Quality, and Compliance Score. Our findings reveal a stark contrast in LLM behavior: while models demonstrate high safety and content quality with benign inputs, they exhibit a significant decline in performance and safety when confronted with camouflaged jailbreak attempts. This disparity underscores a pervasive vulnerability, highlighting the urgent need for more nuanced and adaptive security strategies to ensure the responsible and robust deployment of LLMs in real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Vision to Validation: A Theory- and Data-Driven Construction of a GCC-Specific AI Adoption Index</title>
<link>https://arxiv.org/abs/2509.05474</link>
<guid>https://arxiv.org/abs/2509.05474</guid>
<content:encoded><![CDATA[
arXiv:2509.05474v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is rapidly transforming public-sector processes worldwide, yet standardized measures rarely address the unique drivers, governance models, and cultural nuances of the Gulf Cooperation Council (GCC) countries. This study employs a theory-driven foundation derived from an in-depth analysis of literature review and six National AI Strategies (NASs), coupled with a data-driven approach that utilizes a survey of 203 mid- and senior-level government employees and advanced statistical techniques (K-Means clustering, Principal Component Analysis, and Partial Least Squares Structural Equation Modeling). By combining policy insights with empirical evidence, the research develops and validates a novel AI Adoption Index specifically tailored to the GCC public sector. Findings indicate that robust infrastructure and clear policy mandates exert the strongest influence on successful AI implementations, overshadowing organizational readiness in early adoption stages. The combined model explains 70% of the variance in AI outcomes, suggesting that resource-rich environments and top-down policy directives can drive rapid but uneven technology uptake. By consolidating key dimensions (Infrastructure & Resources, Organizational Readiness, and Policy & Regulatory Environment) into a single composite index, this study provides a holistic yet context-sensitive tool for benchmarking AI maturity. The index offers actionable guidance for policymakers seeking to harmonize large-scale deployments with ethical and regulatory standards. Beyond advancing academic discourse, these insights inform more strategic allocation of resources, cross-country cooperation, and capacity-building initiatives, thereby supporting sustained AI-driven transformation in the GCC region and beyond.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation</title>
<link>https://arxiv.org/abs/2509.05475</link>
<guid>https://arxiv.org/abs/2509.05475</guid>
<content:encoded><![CDATA[
arXiv:2509.05475v1 Announce Type: cross 
Abstract: Autonomous regolith excavation is a cornerstone of in-situ resource utilization for a sustained human presence beyond Earth. However, this task is fundamentally hindered by the complex interaction dynamics of granular media and the operational need for robots to use diverse tools. To address these challenges, this work introduces a framework where a model-based reinforcement learning agent learns within a parallelized simulation. This environment leverages high-fidelity particle physics and procedural generation to create a vast distribution of both lunar terrains and excavation tool geometries. To master this diversity, the agent learns an adaptive interaction strategy by dynamically modulating its own stiffness and damping at each control step through operational space control. Our experiments demonstrate that training with a procedural distribution of tools is critical for generalization and enables the development of sophisticated tool-aware behavior. Furthermore, we show that augmenting the agent with visual feedback significantly improves task success. These results represent a validated methodology for developing the robust and versatile autonomous systems required for the foundational tasks of future space missions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series</title>
<link>https://arxiv.org/abs/2509.05478</link>
<guid>https://arxiv.org/abs/2509.05478</guid>
<content:encoded><![CDATA[
arXiv:2509.05478v1 Announce Type: cross 
Abstract: Multivariate time series (MTS) are ubiquitous in domains such as healthcare, climate science, and industrial monitoring, but their high dimensionality, limited labeled data, and non-stationary nature pose significant challenges for conventional machine learning methods. While recent self-supervised learning (SSL) approaches mitigate label scarcity by data augmentations or time point-based contrastive strategy, they neglect the intrinsic periodic structure of MTS and fail to capture the dynamic evolution of latent states. We propose PLanTS, a periodicity-aware self-supervised learning framework that explicitly models irregular latent states and their transitions. We first designed a period-aware multi-granularity patching mechanism and a generalized contrastive loss to preserve both instance-level and state-level similarities across multiple temporal resolutions. To further capture temporal dynamics, we design a next-transition prediction pretext task that encourages representations to encode predictive information about future state evolution. We evaluate PLanTS across a wide range of downstream tasks-including multi-class and multi-label classification, forecasting, trajectory tracking and anomaly detection. PLanTS consistently improves the representation quality over existing SSL methods and demonstrates superior runtime efficiency compared to DTW-based methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Token Tax: Systematic Bias in Multilingual Tokenization</title>
<link>https://arxiv.org/abs/2509.05486</link>
<guid>https://arxiv.org/abs/2509.05486</guid>
<content:encoded><![CDATA[
arXiv:2509.05486v1 Announce Type: cross 
Abstract: Tokenization inefficiency imposes structural disadvantages on morphologically complex, low-resource languages, inflating compute resources and depressing accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA items; 5 subjects; 16 African languages) and show that fertility (tokens/word) reliably predicts accuracy. Higher fertility consistently predicts lower accuracy across all models and subjects. We further find that reasoning models (DeepSeek, o1) consistently outperform non-reasoning peers across high and low resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in prior generations. Finally, translating token inflation to economics, a doubling in tokens results in quadrupled training cost and time, underscoring the token tax faced by many languages. These results motivate morphologically aware tokenization, fair pricing, and multilingual benchmarks for equitable natural language processing (NLP).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs</title>
<link>https://arxiv.org/abs/2509.05488</link>
<guid>https://arxiv.org/abs/2509.05488</guid>
<content:encoded><![CDATA[
arXiv:2509.05488v1 Announce Type: cross 
Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to limited memory, the lack of native operator support, and the absence of embedded-friendly toolchains. We present, to our knowledge, the first deployment of a Mamba-based neural architecture on a resource-constrained MCU, a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline maps a trained PyTorch Mamba model to on-device execution by (1) exporting model weights into a lightweight format, and (2) implementing a handcrafted Mamba layer and supporting operators in C with operator fusion and memory layout optimization. MambaLite-Micro eliminates large intermediate tensors, reducing 83.0% peak memory, while maintaining an average numerical error of only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on keyword spotting(KWS) and human activity recognition (HAR) tasks, MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully preserving classification accuracy. We further validated portability by deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating consistent operation across heterogeneous embedded platforms and paving the way for bringing advanced sequence models like Mamba to real-world resource-constrained applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures</title>
<link>https://arxiv.org/abs/2509.05490</link>
<guid>https://arxiv.org/abs/2509.05490</guid>
<content:encoded><![CDATA[
arXiv:2509.05490v1 Announce Type: cross 
Abstract: The You Only Look Once (YOLO) architecture is crucial for real-time object detection. However, deploying it in resource-constrained environments such as unmanned aerial vehicles (UAVs) requires efficient transfer learning. Although layer freezing is a common technique, the specific impact of various freezing configurations on contemporary YOLOv8 and YOLOv10 architectures remains unexplored, particularly with regard to the interplay between freezing depth, dataset characteristics, and training dynamics. This research addresses this gap by presenting a detailed analysis of layer-freezing strategies. We systematically investigate multiple freezing configurations across YOLOv8 and YOLOv10 variants using four challenging datasets that represent critical infrastructure monitoring. Our methodology integrates a gradient behavior analysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper insights into training dynamics under different freezing strategies. Our results reveal that there is no universal optimal freezing strategy but, rather, one that depends on the properties of the data. For example, freezing the backbone is effective for preserving general-purpose features, while a shallower freeze is better suited to handling extreme class imbalance. These configurations reduce graphics processing unit (GPU) memory consumption by up to 28% compared to full fine-tuning and, in some cases, achieve mean average precision (mAP@50) scores that surpass those of full fine-tuning. Gradient analysis corroborates these findings, showing distinct convergence patterns for moderately frozen models. Ultimately, this work provides empirical findings and practical guidelines for selecting freezing strategies. It offers a practical, evidence-based approach to balanced transfer learning for object detection in scenarios with limited resources.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Microrobot Vascular Parkour: Analytic Geometry-based Path Planning with Real-time Dynamic Obstacle Avoidance</title>
<link>https://arxiv.org/abs/2509.05500</link>
<guid>https://arxiv.org/abs/2509.05500</guid>
<content:encoded><![CDATA[
arXiv:2509.05500v1 Announce Type: cross 
Abstract: Autonomous microrobots in blood vessels could enable minimally invasive therapies, but navigation is challenged by dense, moving obstacles. We propose a real-time path planning framework that couples an analytic geometry global planner (AGP) with two reactive local escape controllers, one based on rules and one based on reinforcement learning, to handle sudden moving obstacles. Using real-time imaging, the system estimates the positions of the microrobot, obstacles, and targets and computes collision-free motions. In simulation, AGP yields shorter paths and faster planning than weighted A* (WA*), particle swarm optimization (PSO), and rapidly exploring random trees (RRT), while maintaining feasibility and determinism. We extend AGP from 2D to 3D without loss of speed. In both simulations and experiments, the combined global planner and local controllers reliably avoid moving obstacles and reach targets. The average planning time is 40 ms per frame, compatible with 25 fps image acquisition and real-time closed-loop control. These results advance autonomous microrobot navigation and targeted drug delivery in vascular environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2509.05513</link>
<guid>https://arxiv.org/abs/2509.05513</guid>
<content:encoded><![CDATA[
arXiv:2509.05513v1 Announce Type: cross 
Abstract: Egocentric human videos provide scalable demonstrations for imitation learning, but existing corpora often lack either fine-grained, temporally localized action descriptions or dexterous hand annotations. We introduce OpenEgo, a multimodal egocentric manipulation dataset with standardized hand-pose annotations and intention-aligned action primitives. OpenEgo totals 1107 hours across six public datasets, covering 290 manipulation tasks in 600+ environments. We unify hand-pose layouts and provide descriptive, timestamped action primitives. To validate its utility, we train language-conditioned imitation-learning policies to predict dexterous hand trajectories. OpenEgo is designed to lower the barrier to learning dexterous manipulation from egocentric video and to support reproducible research in vision-language-action learning. All resources and instructions will be released at www.openegocentric.com.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining TSL and LLM to Automate REST API Testing: A Comparative Study</title>
<link>https://arxiv.org/abs/2509.05540</link>
<guid>https://arxiv.org/abs/2509.05540</guid>
<content:encoded><![CDATA[
arXiv:2509.05540v1 Announce Type: cross 
Abstract: The effective execution of tests for REST APIs remains a considerable challenge for development teams, driven by the inherent complexity of distributed systems, the multitude of possible scenarios, and the limited time available for test design. Exhaustive testing of all input combinations is impractical, often resulting in undetected failures, high manual effort, and limited test coverage. To address these issues, we introduce RestTSLLM, an approach that uses Test Specification Language (TSL) in conjunction with Large Language Models (LLMs) to automate the generation of test cases for REST APIs. The approach targets two core challenges: the creation of test scenarios and the definition of appropriate input data. The proposed solution integrates prompt engineering techniques with an automated pipeline to evaluate various LLMs on their ability to generate tests from OpenAPI specifications. The evaluation focused on metrics such as success rate, test coverage, and mutation score, enabling a systematic comparison of model performance. The results indicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic), Deepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) - consistently produced robust and contextually coherent REST API tests. Among them, Claude 3.5 Sonnet outperformed all other models across every metric, emerging in this study as the most suitable model for this task. These findings highlight the potential of LLMs to automate the generation of tests based on API specifications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study</title>
<link>https://arxiv.org/abs/2509.05553</link>
<guid>https://arxiv.org/abs/2509.05553</guid>
<content:encoded><![CDATA[
arXiv:2509.05553v1 Announce Type: cross 
Abstract: This research addresses a fundamental question in AI: whether large language models truly understand concepts or simply recognize patterns. The authors propose bidirectional reasoning,the ability to apply transformations in both directions without being explicitly trained on the reverse direction, as a test for genuine understanding. They argue that true comprehension should naturally allow reversibility. For example, a model that can change a variable name like userIndex to i should also be able to infer that i represents a user index without reverse training. The researchers tested current language models and discovered what they term cognitive specialization: when models are fine-tuned on forward tasks, their performance on those tasks improves, but their ability to reason bidirectionally becomes significantly worse. To address this issue, they developed Contrastive Fine-Tuning (CFT), which trains models using three types of examples: positive examples that maintain semantic meaning, negative examples with different semantics, and forward-direction obfuscation examples. This approach aims to develop deeper understanding rather than surface-level pattern recognition and allows reverse capabilities to develop naturally without explicit reverse training. Their experiments demonstrated that CFT successfully achieved bidirectional reasoning, enabling strong reverse performance while maintaining forward task capabilities. The authors conclude that bidirectional reasoning serves both as a theoretical framework for assessing genuine understanding and as a practical training approach for developing more capable AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids</title>
<link>https://arxiv.org/abs/2509.05581</link>
<guid>https://arxiv.org/abs/2509.05581</guid>
<content:encoded><![CDATA[
arXiv:2509.05581v1 Announce Type: cross 
Abstract: We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a custom-built humanoid robot designed for entertainment applications. Unlike traditional humanoids, entertainment robots present unique challenges due to aesthetic-driven design choices. Cosmo embodies these with a disproportionately large head (16% of total mass), limited sensing, and protective shells that considerably restrict movement. To address these challenges, we apply Adversarial Motion Priors (AMP) to enable the robot to learn natural-looking movements while maintaining physical stability. We develop tailored domain randomization techniques and specialized reward structures to ensure safe sim-to-real, protecting valuable hardware components during deployment. Our experiments demonstrate that AMP generates stable standing and walking behaviors despite Cosmo's extreme mass distribution and movement constraints. These results establish a promising direction for robots that balance aesthetic appeal with functional performance, suggesting that learning-based methods can effectively adapt to aesthetic-driven design constraints.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity</title>
<link>https://arxiv.org/abs/2509.05585</link>
<guid>https://arxiv.org/abs/2509.05585</guid>
<content:encoded><![CDATA[
arXiv:2509.05585v1 Announce Type: cross 
Abstract: In the field of software traceability link recovery (TLR), textual similarity has long been regarded as the core criterion. However, in tasks involving natural language and programming language (NL-PL) artifacts, relying solely on textual similarity is limited by their semantic gap. To this end, we conducted a large-scale empirical evaluation across various types of TLR tasks, revealing the limitations of textual similarity in NL-PL scenarios. To address these limitations, we propose an approach that incorporates multiple domain-specific auxiliary strategies, identified through empirical analysis, into two models: the Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based Gemini 2.5 Pro via additional input information. We then evaluated our approach using the widely studied requirements-to-code TLR task, a representative case of NL-PL TLR. Experimental results show that both the multi-strategy HGT and Gemini 2.5 Pro models outperformed their original counterparts without strategy integration. Furthermore, compared to the current state-of-the-art method HGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average F1-score improvements of 3.68% and 8.84%, respectively, across twelve open-source projects, demonstrating the effectiveness of multi-strategy integration in enhancing overall model performance for the requirements-code TLR task.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization</title>
<link>https://arxiv.org/abs/2509.05604</link>
<guid>https://arxiv.org/abs/2509.05604</guid>
<content:encoded><![CDATA[
arXiv:2509.05604v1 Announce Type: cross 
Abstract: Video summarization aims to select keyframes that are visually diverse and can represent the whole story of a given video. Previous approaches have focused on global interlinkability between frames in a video by temporal modeling. However, fine-grained visual entities, such as objects, are also highly related to the main content of the video. Moreover, language-guided video summarization, which has recently been studied, requires a comprehensive linguistic understanding of complex real-world videos. To consider how all the objects are semantically related to each other, this paper regards video summarization as a language-guided spatiotemporal graph modeling problem. We present recursive spatiotemporal graph networks, called VideoGraph, which formulate the objects and frames as nodes of the spatial and temporal graphs, respectively. The nodes in each graph are connected and aggregated with graph edges, representing the semantic relationships between the nodes. To prevent the edges from being configured with visual similarity, we incorporate language queries derived from the video into the graph node representations, enabling them to contain semantic knowledge. In addition, we adopt a recursive strategy to refine initial graphs and correctly classify each frame node as a keyframe. In our experiments, VideoGraph achieves state-of-the-art performance on several benchmarks for generic and query-focused video summarization in both supervised and unsupervised manners. The code is available at https://github.com/park-jungin/videograph.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation</title>
<link>https://arxiv.org/abs/2509.05605</link>
<guid>https://arxiv.org/abs/2509.05605</guid>
<content:encoded><![CDATA[
arXiv:2509.05605v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) require high quality preference datasets to align with human preferences. However, conventional methods for constructing such datasets face significant challenges: reliance on pre-collected instructions often leads to distribution mismatches with target models, while the need for sampling multiple stochastic responses introduces substantial computational overhead. In this work, we explore a paradigm shift by leveraging inherent regulation of LLMs' representation space for efficient and tailored preference dataset construction, named Icon$^{2}$. Specifically, it first extracts layer-wise direction vectors to encode sophisticated human preferences and then uses these vectors to filter self-synthesized instructions based on their inherent consistency. During decoding, bidirectional inherent control is applied to steer token representations, enabling the precise generation of response pairs with clear alignment distinctions. Experimental results demonstrate significant improvements in both alignment and efficiency. Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by up to 48.1%.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints</title>
<link>https://arxiv.org/abs/2509.05608</link>
<guid>https://arxiv.org/abs/2509.05608</guid>
<content:encoded><![CDATA[
arXiv:2509.05608v1 Announce Type: cross 
Abstract: The widespread deployment of LLMs across enterprise services has created a critical security blind spot. Organizations operate multiple LLM services handling billions of queries daily, yet regulatory compliance boundaries prevent these services from sharing threat intelligence about prompt injection attacks, the top security risk for LLMs. When an attack is detected in one service, the same threat may persist undetected in others for months, as privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence system that enables secure sharing of attack fingerprints across compliance boundaries. BinaryShield transforms suspicious prompts through a unique pipeline combining PII redaction, semantic embedding, binary quantization, and randomized response mechanism to potentially generate non-invertible fingerprints that preserve attack patterns while providing privacy. Our evaluations demonstrate that BinaryShield achieves an F1-score of 0.94, significantly outperforming SimHash (0.77), the privacy-preserving baseline, while achieving 64x storage reduction and 38x faster similarity search compared to dense embeddings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning</title>
<link>https://arxiv.org/abs/2509.05614</link>
<guid>https://arxiv.org/abs/2509.05614</guid>
<content:encoded><![CDATA[
arXiv:2509.05614v1 Announce Type: cross 
Abstract: Pruning accelerates compute-bound models by reducing computation. Recently applied to Vision-Language-Action (VLA) models, existing methods prune tokens using only local info from current action, ignoring global context from prior actions, causing >20% success rate drop and limited speedup. We observe high similarity across consecutive actions and propose leveraging both local (current) and global (past) info for smarter token selection. We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context to reduce visual tokens per action; (2) Dynamic pruning at layer level: prunes tokens per layer based on layer-specific importance; (3) Lightweight action-aware controller: classifies actions as coarse/fine-grained (by speed), adjusting pruning aggressiveness since fine-grained actions are pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs. OpenVLA-OFT, with negligible success rate loss.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities</title>
<link>https://arxiv.org/abs/2509.05615</link>
<guid>https://arxiv.org/abs/2509.05615</guid>
<content:encoded><![CDATA[
arXiv:2509.05615v1 Announce Type: cross 
Abstract: Medical multimodal representation learning aims to integrate heterogeneous clinical data into unified patient representations to support predictive modeling, which remains an essential yet challenging task in the medical data mining community. However, real-world medical datasets often suffer from missing modalities due to cost, protocol, or patient-specific constraints. Existing methods primarily address this issue by learning from the available observations in either the raw data space or feature space, but typically neglect the underlying bias introduced by the data acquisition process itself. In this work, we identify two types of biases that hinder model generalization: missingness bias, which results from non-random patterns in modality availability, and distribution bias, which arises from latent confounders that influence both observed features and outcomes. To address these challenges, we perform a structural causal analysis of the data-generating process and propose a unified framework that is compatible with existing direct prediction-based multimodal learning methods. Our method consists of two key components: (1) a missingness deconfounding module that approximates causal intervention based on backdoor adjustment and (2) a dual-branch neural network that explicitly disentangles causal features from spurious correlations. We evaluated our method in real-world public and in-hospital datasets, demonstrating its effectiveness and causal insights.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics</title>
<link>https://arxiv.org/abs/2509.05617</link>
<guid>https://arxiv.org/abs/2509.05617</guid>
<content:encoded><![CDATA[
arXiv:2509.05617v1 Announce Type: cross 
Abstract: The emotional content of song lyrics plays a pivotal role in shaping listener experiences and influencing musical preferences. This paper investigates the task of multi-label emotional attribution of song lyrics by predicting six emotional intensity scores corresponding to six fundamental emotions. A manually labeled dataset is constructed using a mean opinion score (MOS) approach, which aggregates annotations from multiple human raters to ensure reliable ground-truth labels. Leveraging this dataset, we conduct a comprehensive evaluation of several publicly available large language models (LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model specifically for predicting multi-label emotion scores. Experimental results reveal the relative strengths and limitations of zero-shot and fine-tuned models in capturing the nuanced emotional content of lyrics. Our findings highlight the potential of LLMs for emotion recognition in creative texts, providing insights into model selection strategies for emotion-based music information retrieval applications. The labeled dataset is available at https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised Learning for Hyperspectral Images of Trees</title>
<link>https://arxiv.org/abs/2509.05630</link>
<guid>https://arxiv.org/abs/2509.05630</guid>
<content:encoded><![CDATA[
arXiv:2509.05630v1 Announce Type: cross 
Abstract: Aerial remote sensing using multispectral and RGB imagers has provided a critical impetus to precision agriculture. Analysis of the hyperspectral images with limited or no labels is challenging. This paper focuses on self-supervised learning to create neural network embeddings reflecting vegetation properties of trees from aerial hyperspectral images of crop fields. Experimental results demonstrate that a constructed tree representation, using a vegetation property-related embedding space, performs better in downstream machine learning tasks compared to the direct use of hyperspectral vegetation properties as tree representations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orchestrator: Active Inference for Multi-Agent Systems in Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2509.05651</link>
<guid>https://arxiv.org/abs/2509.05651</guid>
<content:encoded><![CDATA[
arXiv:2509.05651v1 Announce Type: cross 
Abstract: Complex, non-linear tasks challenge LLM-enhanced multi-agent systems (MAS) due to partial observability and suboptimal coordination. We propose Orchestrator, a novel MAS framework that leverages attention-inspired self-emergent coordination and reflective benchmarking to optimize global task performance. Orchestrator introduces a monitoring mechanism to track agent-environment dynamics, using active inference benchmarks to optimize system behavior. By tracking agent-to-agent and agent-to-environment interaction, Orchestrator mitigates the effects of partial observability and enables agents to approximate global task solutions more efficiently. We evaluate the framework on a series of maze puzzles of increasing complexity, demonstrating its effectiveness in enhancing coordination and performance in dynamic, non-linear environments with long-horizon objectives.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search</title>
<link>https://arxiv.org/abs/2509.05656</link>
<guid>https://arxiv.org/abs/2509.05656</guid>
<content:encoded><![CDATA[
arXiv:2509.05656v1 Announce Type: cross 
Abstract: Neural architecture search (NAS) is a hard computationally expensive optimization problem with a discrete, vast, and spiky search space. One of the key research efforts dedicated to this space focuses on accelerating NAS via certain proxy evaluations of neural architectures. Different from the prevalent predictor-based methods using surrogate models and differentiable architecture search via supernetworks, we propose an optimization proxy to streamline the NAS as an end-to-end optimization framework, named OptiProxy-NAS. In particular, using a proxy representation, the NAS space is reformulated to be continuous, differentiable, and smooth. Thereby, any differentiable optimization method can be applied to the gradient-based search of the relaxed architecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$ search spaces across three different domains including computer vision, natural language processing, and resource-constrained NAS fully demonstrate the superior search results and efficiency. Further experiments on low-fidelity scenarios verify the flexibility.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</title>
<link>https://arxiv.org/abs/2509.05657</link>
<guid>https://arxiv.org/abs/2509.05657</guid>
<content:encoded><![CDATA[
arXiv:2509.05657v1 Announce Type: cross 
Abstract: Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at https://github.com/Ashone3/LM-Searcher.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian</title>
<link>https://arxiv.org/abs/2509.05668</link>
<guid>https://arxiv.org/abs/2509.05668</guid>
<content:encoded><![CDATA[
arXiv:2509.05668v1 Announce Type: cross 
Abstract: We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR</title>
<link>https://arxiv.org/abs/2509.05671</link>
<guid>https://arxiv.org/abs/2509.05671</guid>
<content:encoded><![CDATA[
arXiv:2509.05671v1 Announce Type: cross 
Abstract: Human Activity Recognition (HAR) using multimodal sensor data remains challenging due to noisy or incomplete measurements, scarcity of labeled examples, and privacy concerns. Traditional centralized deep learning approaches are often constrained by infrastructure availability, network latency, and data sharing restrictions. While federated learning (FL) addresses privacy by training models locally and sharing only model parameters, it still has to tackle issues arising from the use of heterogeneous multimodal data and differential privacy requirements. In this article, a Graph-based Multimodal Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse sensor streams such as a pressure mat, depth camera, and multiple accelerometers are modeled as modality-specific graphs, processed through residual Graph Convolutional Neural Networks (GCNs), and fused via attention-based weighting rather than simple concatenation. The fused embeddings enable robust activity classification, while differential privacy safeguards data during federated aggregation. Experimental results show that the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with up to 2 percent higher accuracy in non-DP settings in both centralized and federated paradigms. More importantly, significant improvements are observed under differential privacy constraints: MultiModalGCN consistently surpasses MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on the privacy budget and setting. These results highlight the robustness of graph-based modeling in multimodal learning, where GNNs prove more resilient to the performance degradation introduced by DP noise.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts</title>
<link>https://arxiv.org/abs/2509.05681</link>
<guid>https://arxiv.org/abs/2509.05681</guid>
<content:encoded><![CDATA[
arXiv:2509.05681v1 Announce Type: cross 
Abstract: Decentralized Finance (DeFi) attacks have resulted in significant losses, often orchestrated through Adversarial Exploiter Contracts (AECs) that exploit vulnerabilities in victim smart contracts. To proactively identify such threats, this paper targets the explainable detection of AECs.
  Existing detection methods struggle to capture semantic dependencies and lack interpretability, limiting their effectiveness and leaving critical knowledge gaps in AEC analysis. To address these challenges, we introduce SEASONED, an effective, self-explanatory, and robust framework for AEC detection.
  SEASONED extracts semantic information from contract bytecode to construct a semantic relation graph (SRG), and employs a self-counterfactual explainable detector (SCFED) to classify SRGs and generate explanations that highlight the core attack logic. SCFED further enhances robustness, generalizability, and data efficiency by extracting representative information from these explanations. Both theoretical analysis and experimental results demonstrate the effectiveness of SEASONED, which showcases outstanding detection performance, robustness, generalizability, and data efficiency learning ability. To support further research, we also release a new dataset of 359 AECs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models</title>
<link>https://arxiv.org/abs/2509.05691</link>
<guid>https://arxiv.org/abs/2509.05691</guid>
<content:encoded><![CDATA[
arXiv:2509.05691v1 Announce Type: cross 
Abstract: Text embedding models are widely used in natural language processing applications. However, their capability is often benchmarked on tasks that do not require understanding nuanced numerical information in text. As a result, it remains unclear whether current embedding models can precisely encode numerical content, such as numbers, into embeddings. This question is critical because embedding models are increasingly applied in domains where numbers matter, such as finance and healthcare. For example, Company X's market share grew by 2\% should be interpreted very differently from Company X's market share grew by 20\%, even though both indicate growth in market share. This study aims to examine whether text embedding models can capture such nuances. Using synthetic data in a financial context, we evaluate 13 widely used text embedding models and find that they generally struggle to capture numerical details accurately. Our further analyses provide deeper insights into embedding numeracy, informing future research to strengthen embedding model-based NLP systems with improved capacity for handling numerical content.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis</title>
<link>https://arxiv.org/abs/2509.05703</link>
<guid>https://arxiv.org/abs/2509.05703</guid>
<content:encoded><![CDATA[
arXiv:2509.05703v1 Announce Type: cross 
Abstract: Marine mammal vocalization analysis depends on interpreting bioacoustic spectrograms. Vision Language Models (VLMs) are not trained on these domain-specific visualizations. We investigate whether VLMs can extract meaningful patterns from spectrograms visually. Our framework integrates VLM interpretation with LLM-based validation to build domain knowledge. This enables adaptation to acoustic data without manual annotation or model retraining.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of the State-of-the-Art in Conversational Question Answering Systems</title>
<link>https://arxiv.org/abs/2509.05716</link>
<guid>https://arxiv.org/abs/2509.05716</guid>
<content:encoded><![CDATA[
arXiv:2509.05716v1 Announce Type: cross 
Abstract: Conversational Question Answering (ConvQA) systems have emerged as a pivotal area within Natural Language Processing (NLP) by driving advancements that enable machines to engage in dynamic and context-aware conversations. These capabilities are increasingly being applied across various domains, i.e., customer support, education, legal, and healthcare where maintaining a coherent and relevant conversation is essential. Building on recent advancements, this survey provides a comprehensive analysis of the state-of-the-art in ConvQA. This survey begins by examining the core components of ConvQA systems, i.e., history selection, question understanding, and answer prediction, highlighting their interplay in ensuring coherence and relevance in multi-turn conversations. It further investigates the use of advanced machine learning techniques, including but not limited to, reinforcement learning, contrastive learning, and transfer learning to improve ConvQA accuracy and efficiency. The pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash, Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact through data scalability and architectural advancements. Additionally, this survey presents a comprehensive analysis of key ConvQA datasets and concludes by outlining open research directions. Overall, this work offers a comprehensive overview of the ConvQA landscape and provides valuable insights to guide future advancements in the field.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction</title>
<link>https://arxiv.org/abs/2509.05728</link>
<guid>https://arxiv.org/abs/2509.05728</guid>
<content:encoded><![CDATA[
arXiv:2509.05728v1 Announce Type: cross 
Abstract: This paper extends LiDAR-BIND, a modular multi-modal fusion framework that binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space, with mechanisms that explicitly enforce temporal consistency. We introduce three contributions: (i) temporal embedding similarity that aligns consecutive latents, (ii) a motion-aligned transformation loss that matches displacement between predictions and ground truth LiDAR, and (iii) windows temporal fusion using a specialised temporal module. We further update the model architecture to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR translation demonstrate improved temporal and spatial coherence, yielding lower absolute trajectory error and better occupancy map accuracy in Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose different metrics based on the Fr\'echet Video Motion Distance (FVMD) and a correlation-peak distance metric providing practical temporal quality indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially enhancing temporal stability, resulting in improved robustness and performance for downstream SLAM.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation Priors for Data-Efficient Deep Learning</title>
<link>https://arxiv.org/abs/2509.05732</link>
<guid>https://arxiv.org/abs/2509.05732</guid>
<content:encoded><![CDATA[
arXiv:2509.05732v1 Announce Type: cross 
Abstract: How do we enable AI systems to efficiently learn in the real-world? First-principles models are widely used to simulate natural systems, but often fail to capture real-world complexity due to simplifying assumptions. In contrast, deep learning approaches can estimate complex dynamics with minimal assumptions but require large, representative datasets. We propose SimPEL, a method that efficiently combines first-principles models with data-driven learning by using low-fidelity simulators as priors in Bayesian deep learning. This enables SimPEL to benefit from simulator knowledge in low-data regimes and leverage deep learning's flexibility when more data is available, all the while carefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse systems, including biological, agricultural, and robotic domains, showing superior performance in learning complex dynamics. For decision-making, we demonstrate that SimPEL bridges the sim-to-real gap in model-based reinforcement learning. On a high-speed RC car task, SimPEL learns a highly dynamic parking maneuver involving drifting with substantially less data than state-of-the-art baselines. These results highlight the potential of SimPEL for data-efficient learning and control in complex real-world environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies</title>
<link>https://arxiv.org/abs/2509.05735</link>
<guid>https://arxiv.org/abs/2509.05735</guid>
<content:encoded><![CDATA[
arXiv:2509.05735v1 Announce Type: cross 
Abstract: Data collection is crucial for learning robust world models in model-based reinforcement learning. The most prevalent strategies are to actively collect trajectories by interacting with the environment during online training or training on offline datasets. At first glance, the nature of learning task-agnostic environment dynamics makes world models a good candidate for effective offline training. However, the effects of online vs. offline data on world models and thus on the resulting task performance have not been thoroughly studied in the literature. In this work, we investigate both paradigms in model-based settings, conducting experiments on 31 different environments. First, we showcase that online agents outperform their offline counterparts. We identify a key challenge behind performance degradation of offline agents: encountering Out-Of-Distribution states at test time. This issue arises because, without the self-correction mechanism in online agents, offline datasets with limited state space coverage induce a mismatch between the agent's imagination and real rollouts, compromising policy training. We demonstrate that this issue can be mitigated by allowing for additional online interactions in a fixed or adaptive schedule, restoring the performance of online training with limited interaction data. We also showcase that incorporating exploration data helps mitigate the performance degradation of offline agents. Based on our insights, we recommend adding exploration data when collecting large datasets, as current efforts predominantly focus on expert data alone.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated</title>
<link>https://arxiv.org/abs/2509.05739</link>
<guid>https://arxiv.org/abs/2509.05739</guid>
<content:encoded><![CDATA[
arXiv:2509.05739v1 Announce Type: cross 
Abstract: Early research into data poisoning attacks against Large Language Models (LLMs) demonstrated the ease with which backdoors could be injected. More recent LLMs add step-by-step reasoning, expanding the attack surface to include the intermediate chain-of-thought (CoT) and its inherent trait of decomposing problems into subproblems. Using these vectors for more stealthy poisoning, we introduce ``decomposed reasoning poison'', in which the attacker modifies only the reasoning path, leaving prompts and final answers clean, and splits the trigger across multiple, individually harmless components.
  Fascinatingly, while it remains possible to inject these decomposed poisons, reliably activating them to change final answers (rather than just the CoT) is surprisingly difficult. This difficulty arises because the models can often recover from backdoors that are activated within their thought processes. Ultimately, it appears that an emergent form of backdoor robustness is originating from the reasoning capabilities of these advanced LLMs, as well as from the architectural separation between reasoning and final answer generation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios</title>
<link>https://arxiv.org/abs/2509.05747</link>
<guid>https://arxiv.org/abs/2509.05747</guid>
<content:encoded><![CDATA[
arXiv:2509.05747v1 Announce Type: cross 
Abstract: We address the problem of accurate capture of interactive behaviors between two people in daily scenarios. Most previous works either only consider one person or solely focus on conversational gestures of two people, assuming the body orientation and/or position of each actor are constant or barely change over each interaction. In contrast, we propose to simultaneously model two people's activities, and target objective-driven, dynamic, and semantically consistent interactions which often span longer duration and cover bigger space. To this end, we capture a new multi-modal dataset dubbed InterAct, which is composed of 241 motion sequences where two people perform a realistic and coherent scenario for one minute or longer over a complete interaction. For each sequence, two actors are assigned different roles and emotion labels, and collaborate to finish one task or conduct a common interaction activity. The audios, body motions, and facial expressions of both persons are captured. InterAct contains diverse and complex motions of individuals and interesting and relatively long-term interaction patterns barely seen before. We also demonstrate a simple yet effective diffusion-based method that estimates interactive face expressions and body motions of two people from speech inputs. Our method regresses the body motions in a hierarchical manner, and we also propose a novel fine-tuning mechanism to improve the lip accuracy of facial expressions. To facilitate further research, the data and code is made available at https://hku-cg.github.io/interact/ .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation</title>
<link>https://arxiv.org/abs/2509.05751</link>
<guid>https://arxiv.org/abs/2509.05751</guid>
<content:encoded><![CDATA[
arXiv:2509.05751v1 Announce Type: cross 
Abstract: Referring Video Object Segmentation (RVOS) aims to segment an object of interest throughout a video based on a language description. The prominent challenge lies in aligning static text with dynamic visual content, particularly when objects exhibiting similar appearances with inconsistent motion and poses. However, current methods often rely on a holistic visual-language fusion that struggles with complex, compositional descriptions. In this paper, we propose \textbf{PARSE-VOS}, a novel, training-free framework powered by Large Language Models (LLMs), for a hierarchical, coarse-to-fine reasoning across text and video domains. Our approach begins by parsing the natural language query into structured semantic commands. Next, we introduce a spatio-temporal grounding module that generates all candidate trajectories for all potential target objects, guided by the parsed semantics. Finally, a hierarchical identification module select the correct target through a two-stage reasoning process: it first performs coarse-grained motion reasoning with an LLM to narrow down candidates; if ambiguity remains, a fine-grained pose verification stage is conditionally triggered to disambiguate. The final output is an accurate segmentation mask for the target object. \textbf{PARSE-VOS} achieved state-of-the-art performance on three major benchmarks: Ref-YouTube-VOS, Ref-DAVIS17, and MeViS.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics</title>
<link>https://arxiv.org/abs/2509.05753</link>
<guid>https://arxiv.org/abs/2509.05753</guid>
<content:encoded><![CDATA[
arXiv:2509.05753v1 Announce Type: cross 
Abstract: The rise of synthetic media has blurred the boundary between reality and fabrication under the evolving power of artificial intelligence, fueling an infodemic that erodes public trust in cyberspace. For digital imagery, a multitude of editing applications further complicates the forensic analysis, including semantic edits that alter content, photometric adjustments that recalibrate colour characteristics, and geometric projections that reshape viewpoints. Collectively, these transformations manipulate and control perceptual interpretation of digital imagery. This susceptibility calls for forensic enquiry into reconstructing the chain of events, thereby revealing deeper evidential insight into the presence or absence of criminal intent. This study seeks to address an inverse problem of tracing the underlying generation chain that gives rise to the observed synthetic media. A tell-tale watermarking system is developed for explanatory reasoning over the nature and extent of transformations across the lifecycle of synthetic media. Tell-tale watermarks are tailored to different classes of transformations, responding in a manner that is neither strictly robust nor fragile but instead interpretable. These watermarks function as reference clues that evolve under the same transformation dynamics as the carrier media, leaving interpretable traces when subjected to transformations. Explanatory reasoning is then performed to infer the most plausible account across the combinatorial parameter space of composite transformations. Experimental evaluations demonstrate the validity of tell-tale watermarking with respect to fidelity, synchronicity and traceability.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System</title>
<link>https://arxiv.org/abs/2509.05755</link>
<guid>https://arxiv.org/abs/2509.05755</guid>
<content:encoded><![CDATA[
arXiv:2509.05755v1 Announce Type: cross 
Abstract: LLM-based agentic systems leverage large language models to handle user queries, make decisions, and execute external tools for complex tasks across domains like chatbots, customer service, and software engineering. A critical component of these systems is the Tool Invocation Prompt (TIP), which defines tool interaction protocols and guides LLMs to ensure the security and correctness of tool usage. Despite its importance, TIP security has been largely overlooked. This work investigates TIP-related security risks, revealing that major LLM-based systems like Cursor, Claude Code, and others are vulnerable to attacks such as remote code execution (RCE) and denial of service (DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate external tool behavior hijacking via manipulated tool invocations. We also propose defense mechanisms to enhance TIP security in LLM-based agentic systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting</title>
<link>https://arxiv.org/abs/2509.05768</link>
<guid>https://arxiv.org/abs/2509.05768</guid>
<content:encoded><![CDATA[
arXiv:2509.05768v1 Announce Type: cross 
Abstract: Energy forecasting is vital for grid reliability and operational efficiency. Although recent advances in time series forecasting have led to progress, existing benchmarks remain limited in spatial and temporal scope and lack multi-energy features. This raises concerns about their reliability and applicability in real-world deployment. To address this, we present the Real-E dataset, covering over 74 power stations across 30+ European countries over a 10-year span with rich metadata. Using Real- E, we conduct an extensive data analysis and benchmark over 20 baselines across various model types. We introduce a new metric to quantify shifts in correlation structures and show that existing methods struggle on our dataset, which exhibits more complex and non-stationary correlation dynamics. Our findings highlight key limitations of current methods and offer a strong empirical basis for building more robust forecasting models
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2509.05778</link>
<guid>https://arxiv.org/abs/2509.05778</guid>
<content:encoded><![CDATA[
arXiv:2509.05778v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection plays a key role in enhancing the robustness of artificial intelligence systems by identifying inputs that differ significantly from the training distribution, thereby preventing unreliable predictions and enabling appropriate fallback mechanisms. Developing reliable OOD detection methods is a significant challenge, and rigorous evaluation of these techniques is essential for ensuring their effectiveness, as it allows researchers to assess their performance under diverse conditions and to identify potential limitations or failure modes. Cross-validation (CV) has proven to be a highly effective tool for providing a reasonable estimate of the performance of a learning algorithm. Although OOD scenarios exhibit particular characteristics, an appropriate adaptation of CV can lead to a suitable evaluation framework for this setting. This work proposes a dual CV framework for robust evaluation of OOD detection models, aimed at improving the reliability of their assessment. The proposed evaluation framework aims to effectively integrate in-distribution (ID) and OOD data while accounting for their differing characteristics. To achieve this, ID data are partitioned using a conventional approach, whereas OOD data are divided by grouping samples based on their classes. Furthermore, we analyze the context of data with class hierarchy to propose a data splitting that considers the entire class hierarchy to obtain fair ID-OOD partitions to apply the proposed evaluation framework. This framework is called Dual Cross-Validation for Robust Out-of-Distribution Detection (DCV-ROOD). To test the validity of the evaluation framework, we selected a set of state-of-the-art OOD detection methods, both with and without outlier exposure. The results show that the method achieves very fast convergence to the true performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Mode Deep Anomaly Detection for Medical Manufacturing: Structural Similarity and Feature Distance</title>
<link>https://arxiv.org/abs/2509.05796</link>
<guid>https://arxiv.org/abs/2509.05796</guid>
<content:encoded><![CDATA[
arXiv:2509.05796v1 Announce Type: cross 
Abstract: Automating visual inspection in medical device manufacturing remains challenging due to small and imbalanced datasets, high-resolution imagery, and stringent regulatory requirements. This work proposes two attention-guided autoencoder architectures for deep anomaly detection designed to address these constraints. The first employs a structural similarity-based anomaly score (4-MS-SSIM), offering lightweight and accurate real-time defect detection, yielding ACC 0.903 (unsupervised thresholding) and 0.931 (supervised thresholding) on the - Surface Seal Image - Test split with only 10% of defective samples. The second applies a feature-distance approach using Mahalanobis scoring on reduced latent features, providing high sensitivity to distributional shifts for supervisory monitoring, achieving ACC 0.722 with supervised thresholding. Together, these methods deliver complementary capabilities: the first supports reliable inline inspection, while the second enables scalable post-production surveillance and regulatory compliance monitoring. Experimental results demonstrate that both approaches surpass re-implemented baselines and provide a practical pathway for deploying deep anomaly detection in regulated manufacturing environments, aligning accuracy, efficiency, and the regulatory obligations defined for high-risk AI systems under the EU AI Act.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Fourier Neural Operator-Plasma Fluid Model for Fast and Accurate Multiscale Simulations of High Power Microwave Breakdown</title>
<link>https://arxiv.org/abs/2509.05799</link>
<guid>https://arxiv.org/abs/2509.05799</guid>
<content:encoded><![CDATA[
arXiv:2509.05799v1 Announce Type: cross 
Abstract: Modeling and simulation of High Power Microwave (HPM) breakdown, a multiscale phenomenon, is computationally expensive and requires solving Maxwell's equations (EM solver) coupled with a plasma continuity equation (plasma solver). In this work, we present a hybrid modeling approach that combines the accuracy of a differential equation-based plasma fluid solver with the computational efficiency of FNO (Fourier Neural Operator) based EM solver. Trained on data from an in-house FDTD-based plasma-fluid solver, the FNO replaces computationally expensive EM field updates, while the plasma solver governs the dynamic plasma response. The hybrid model is validated on microwave streamer formation, due to diffusion ionization mechanism, in a 2D scenario for unseen incident electric fields corresponding to entirely new plasma streamer simulations not included in model training, showing excellent agreement with FDTD based fluid simulations in terms of streamer shape, velocity, and temporal evolution. This hybrid FNO based strategy delivers significant acceleration of the order of 60X compared to traditional simulations for the specified problem size and offers an efficient alternative for computationally demanding multiscale and multiphysics simulations involved in HPM breakdown. Our work also demonstrate how such hybrid pipelines can be used to seamlessly to integrate existing C-based simulation codes with Python-based machine learning frameworks for simulations of plasma science and engineering problems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2509.05801</link>
<guid>https://arxiv.org/abs/2509.05801</guid>
<content:encoded><![CDATA[
arXiv:2509.05801v1 Announce Type: cross 
Abstract: While transformer-based foundation models excel at forecasting routine patterns, two questions remain: do they internalize semantic concepts such as market regimes, or merely fit curves? And can their internal representations be leveraged to simulate rare, high-stakes events such as market crashes? To investigate this, we introduce activation transplantation, a causal intervention that manipulates hidden states by imposing the statistical moments of one event (e.g., a historical crash) onto another (e.g., a calm period) during the forward pass. This procedure deterministically steers forecasts: injecting crash semantics induces downturn predictions, while injecting calm semantics suppresses crashes and restores stability. Beyond binary control, we find that models encode a graded notion of event severity, with the latent vector norm directly correlating with the magnitude of systemic shocks. Validated across two architecturally distinct TSFMs, Toto (decoder only) and Chronos (encoder-decoder), our results demonstrate that steerable, semantically grounded representations are a robust property of large time series transformers. Our findings provide evidence for a latent concept space that governs model predictions, shifting interpretability from post-hoc attribution to direct causal intervention, and enabling semantic "what-if" analysis for strategic stress-testing.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization</title>
<link>https://arxiv.org/abs/2509.05831</link>
<guid>https://arxiv.org/abs/2509.05831</guid>
<content:encoded><![CDATA[
arXiv:2509.05831v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into web-based systems for content summarization, yet their susceptibility to prompt injection attacks remains a pressing concern. In this study, we explore how non-visible HTML elements such as , aria-label, and alt attributes can be exploited to embed adversarial instructions without altering the visible content of a webpage. We introduce a novel dataset comprising 280 static web pages, evenly divided between clean and adversarial injected versions, crafted using diverse HTML-based strategies. These pages are processed through a browser automation pipeline to extract both raw HTML and rendered text, closely mimicking real-world LLM deployment scenarios. We evaluate two state-of-the-art open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their ability to summarize this content. Using both lexical (ROUGE-L) and semantic (SBERT cosine similarity) metrics, along with manual annotations, we assess the impact of these covert injections. Our findings reveal that over 29% of injected samples led to noticeable changes in the Llama 4 Scout summaries, while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These results highlight a critical and largely overlooked vulnerability in LLM driven web pipelines, where hidden adversarial content can subtly manipulate model outputs. Our work offers a reproducible framework and benchmark for evaluating HTML-based prompt injection and underscores the urgent need for robust mitigation strategies in LLM applications involving web content.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenAI on Wall Street -- Opportunities and Risk Controls</title>
<link>https://arxiv.org/abs/2509.05841</link>
<guid>https://arxiv.org/abs/2509.05841</guid>
<content:encoded><![CDATA[
arXiv:2509.05841v1 Announce Type: cross 
Abstract: We give an overview on the emerging applications of GenAI in the financial industry, especially within investment banks. Inherent to these exciting opportunities is a new realm of risks that must be managed properly. By heeding both the Yin and Yang sides of GenAI, we can accelerate its organic growth while safeguarding the entire financial industry during this nascent era of AI.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula</title>
<link>https://arxiv.org/abs/2509.05867</link>
<guid>https://arxiv.org/abs/2509.05867</guid>
<content:encoded><![CDATA[
arXiv:2509.05867v1 Announce Type: cross 
Abstract: Traditional Chinese Medicine (TCM) formulas play a significant role in treating epidemics and complex diseases. Existing models for TCM utilize traditional algorithms or deep learning techniques to analyze formula relationships, yet lack comprehensive results, such as complete formula compositions and detailed explanations. Although recent efforts have used TCM instruction datasets to fine-tune Large Language Models (LLMs) for explainable formula generation, existing datasets lack sufficient details, such as the roles of the formula's sovereign, minister, assistant, courier; efficacy; contraindications; tongue and pulse diagnosis-limiting the depth of model outputs. To address these challenges, we propose ZhiFangDanTai, a framework combining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM fine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured TCM knowledge into concise summaries, while also constructing an enhanced instruction dataset to improve LLMs' ability to integrate retrieved information. Furthermore, we provide novel theoretical proofs demonstrating that integrating GraphRAG with fine-tuning techniques can reduce generalization error and hallucination rates in the TCM formula task. Experimental results on both collected and clinical datasets demonstrate that ZhiFangDanTai achieves significant improvements over state-of-the-art models. Our model is open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.05874</link>
<guid>https://arxiv.org/abs/2509.05874</guid>
<content:encoded><![CDATA[
arXiv:2509.05874v1 Announce Type: cross 
Abstract: The rapid expansion of scientific literature makes it increasingly difficult to acquire new knowledge, particularly in specialized domains where reasoning is complex, full-text access is restricted, and target references are sparse among a large set of candidates. We present a Deep Reinforcement Learning framework for sparse reference selection that emulates human knowledge construction, prioritizing which papers to read under limited time and cost. Evaluated on drug--gene relation discovery with access restricted to titles and abstracts, our approach demonstrates that both humans and machines can construct knowledge effectively from partial information.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification in Probabilistic Machine Learning Models: Theory, Methods, and Insights</title>
<link>https://arxiv.org/abs/2509.05877</link>
<guid>https://arxiv.org/abs/2509.05877</guid>
<content:encoded><![CDATA[
arXiv:2509.05877v1 Announce Type: cross 
Abstract: Uncertainty Quantification (UQ) is essential in probabilistic machine learning models, particularly for assessing the reliability of predictions. In this paper, we present a systematic framework for estimating both epistemic and aleatoric uncertainty in probabilistic models. We focus on Gaussian Process Latent Variable Models and employ scalable Random Fourier Features-based Gaussian Processes to approximate predictive distributions efficiently. We derive a theoretical formulation for UQ, propose a Monte Carlo sampling-based estimation method, and conduct experiments to evaluate the impact of uncertainty estimation. Our results provide insights into the sources of predictive uncertainty and illustrate the effectiveness of our approach in quantifying the confidence in the predictions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation</title>
<link>https://arxiv.org/abs/2509.05881</link>
<guid>https://arxiv.org/abs/2509.05881</guid>
<content:encoded><![CDATA[
arXiv:2509.05881v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have fueled growing interest in automating geospatial analysis and GIS workflows, yet their actual capabilities remain uncertain. In this work, we call for rigorous evaluation of LLMs on well-defined geoprocessing tasks before making claims about full GIS automation. To this end, we present GeoAnalystBench, a benchmark of 50 Python-based tasks derived from real-world geospatial problems and carefully validated by GIS experts. Each task is paired with a minimum deliverable product, and evaluation covers workflow validity, structural alignment, semantic similarity, and code quality (CodeBLEU). Using this benchmark, we assess both proprietary and open source models. Results reveal a clear gap: proprietary models such as ChatGPT-4o-mini achieve high validity 95% and stronger code alignment (CodeBLEU 0.39), while smaller open source models like DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5% validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as spatial relationship detection or optimal site selection, remain the most challenging across all models. These findings demonstrate both the promise and limitations of current LLMs in GIS automation and provide a reproducible framework to advance GeoAI research with human-in-the-loop support.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues</title>
<link>https://arxiv.org/abs/2509.05882</link>
<guid>https://arxiv.org/abs/2509.05882</guid>
<content:encoded><![CDATA[
arXiv:2509.05882v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) integrate into diverse workflows, they are increasingly being considered "collaborators" with humans. If such AI collaborators are to be reliable, their behavior over multiturn interactions must be predictable, validated and verified before deployment. Common alignment techniques are typically developed under simplified single-user settings and do not account for the dynamics of long-horizon multiparty interactions. This paper examines how different alignment methods affect LLM agents' effectiveness as partners in multiturn, multiparty collaborations. We study this question through the lens of friction agents that intervene in group dialogues to encourage the collaborative group to slow down and reflect upon their reasoning for deliberative decision-making. Using a roleplay methodology, we evaluate interventions from differently-trained friction agents in collaborative task conversations. We propose a novel counterfactual evaluation framework that quantifies how friction interventions change the trajectory of group collaboration and belief alignment. Our results show that a friction-aware approach significantly outperforms common alignment baselines in helping both convergence to a common ground, or agreed-upon task-relevant propositions, and correctness of task outcomes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs</title>
<link>https://arxiv.org/abs/2509.05883</link>
<guid>https://arxiv.org/abs/2509.05883</guid>
<content:encoded><![CDATA[
arXiv:2509.05883v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have seen rapid adoption in recent years, with industries increasingly relying on them to maintain a competitive advantage. These models excel at interpreting user instructions and generating human-like responses, leading to their integration across diverse domains, including consulting and information retrieval. However, their widespread deployment also introduces substantial security risks, most notably in the form of prompt injection and jailbreak attacks.
  To systematically evaluate LLM vulnerabilities -- particularly to external prompt injection -- we conducted a series of experiments on eight commercial models. Each model was tested without supplementary sanitization, relying solely on its built-in safeguards. The results exposed exploitable weaknesses and emphasized the need for stronger security measures. Four categories of attacks were examined: direct injection, indirect (external) injection, image-based injection, and prompt leakage. Comparative analysis indicated that Claude 3 demonstrated relatively greater robustness; nevertheless, empirical findings confirm that additional defenses, such as input normalization, remain necessary to achieve reliable protection.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum spatial best-arm identification via quantum walks</title>
<link>https://arxiv.org/abs/2509.05890</link>
<guid>https://arxiv.org/abs/2509.05890</guid>
<content:encoded><![CDATA[
arXiv:2509.05890v1 Announce Type: cross 
Abstract: Quantum reinforcement learning has emerged as a framework combining quantum computation with sequential decision-making, and applications to the multi-armed bandit (MAB) problem have been reported. The graph bandit problem extends the MAB setting by introducing spatial constraints, yet quantum approaches remain limited. We propose a quantum algorithm for best-arm identification in graph bandits, termed Quantum Spatial Best-Arm Identification (QSBAI). The method employs quantum walks to encode superpositions over graph-constrained actions, extending amplitude amplification and generalizing the Quantum BAI algorithm via Szegedy's walk framework. This establishes a link between Grover-type search and reinforcement learning tasks with structural restrictions. We analyze complete and bipartite graphs, deriving the maximal success probability of identifying the best arm and the time step at which it is achieved. Our results highlight the potential of quantum walks to accelerate exploration in constrained environments and extend the applicability of quantum algorithms for decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets</title>
<link>https://arxiv.org/abs/2509.05892</link>
<guid>https://arxiv.org/abs/2509.05892</guid>
<content:encoded><![CDATA[
arXiv:2509.05892v1 Announce Type: cross 
Abstract: Accurate segmentation of carotid artery structures in histopathological images is vital for advancing cardiovascular disease research and diagnosis. However, deep learning model development in this domain is constrained by the scarcity of annotated cardiovascular histopathological data. This study investigates a systematic evaluation of state-of-the-art deep learning segmentation models, including convolutional neural networks (U-Net, DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models (SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology images. Despite employing an extensive hyperparameter optimization strategy with Bayesian search, our findings reveal that model performance is highly sensitive to data splits, with minor differences driven more by statistical noise than by true algorithmic superiority. This instability exposes the limitations of standard benchmarking practices in low-data clinical settings and challenges the assumption that performance rankings reflect meaningful clinical utility.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-training of diffractive meta-neural networks for super-resolution direction of arrival estimation</title>
<link>https://arxiv.org/abs/2509.05926</link>
<guid>https://arxiv.org/abs/2509.05926</guid>
<content:encoded><![CDATA[
arXiv:2509.05926v1 Announce Type: cross 
Abstract: Diffractive neural networks leverage the high-dimensional characteristics of electromagnetic (EM) fields for high-throughput computing. However, the existing architectures face challenges in integrating large-scale multidimensional metasurfaces with precise network training and haven't utilized multidimensional EM field coding scheme for super-resolution sensing. Here, we propose diffractive meta-neural networks (DMNNs) for accurate EM field modulation through metasurfaces, which enable multidimensional multiplexing and coding for multi-task learning and high-throughput super-resolution direction of arrival estimation. DMNN integrates pre-trained mini-metanets to characterize the amplitude and phase responses of meta-atoms across different polarizations and frequencies, with structure parameters inversely designed using the gradient-based meta-training. For wide-field super-resolution angle estimation, the system simultaneously resolves azimuthal and elevational angles through x and y-polarization channels, while the interleaving of frequency-multiplexed angular intervals generates spectral-encoded optical super-oscillations to achieve full-angle high-resolution estimation. Post-processing lightweight electronic neural networks further enhance the performance. Experimental results validate that a three-layer DMNN operating at 27 GHz, 29 GHz, and 31 GHz achieves $\sim7\times$ Rayleigh diffraction-limited angular resolution (0.5$^\circ$), a mean absolute error of 0.048$^\circ$ for two incoherent targets within a $\pm 11.5^\circ$ field of view, and an angular estimation throughput an order of magnitude higher (1917) than that of existing methods. The proposed architecture advances high-dimensional photonic computing systems by utilizing inherent high-parallelism and all-optical coding methods for ultra-high-resolution, high-throughput applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConstStyle: Robust Domain Generalization with Unified Style Transformation</title>
<link>https://arxiv.org/abs/2509.05975</link>
<guid>https://arxiv.org/abs/2509.05975</guid>
<content:encoded><![CDATA[
arXiv:2509.05975v1 Announce Type: cross 
Abstract: Deep neural networks often suffer performance drops when test data distribution differs from training data. Domain Generalization (DG) aims to address this by focusing on domain-invariant features or augmenting data for greater diversity. However, these methods often struggle with limited training domains or significant gaps between seen (training) and unseen (test) domains. To enhance DG robustness, we hypothesize that it is essential for the model to be trained on data from domains that closely resemble unseen test domains-an inherently difficult task due to the absence of prior knowledge about the unseen domains. Accordingly, we propose ConstStyle, a novel approach that leverages a unified domain to capture domain-invariant features and bridge the domain gap with theoretical analysis. During training, all samples are mapped onto this unified domain, optimized for seen domains. During testing, unseen domain samples are projected similarly before predictions. By aligning both training and testing data within this unified domain, ConstStyle effectively reduces the impact of domain shifts, even with large domain gaps or few seen domains. Extensive experiments demonstrate that ConstStyle consistently outperforms existing methods across diverse scenarios. Notably, when only a limited number of seen domains are available, ConstStyle can boost accuracy up to 19.82\% compared to the next best approach.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition</title>
<link>https://arxiv.org/abs/2509.05983</link>
<guid>https://arxiv.org/abs/2509.05983</guid>
<content:encoded><![CDATA[
arXiv:2509.05983v1 Announce Type: cross 
Abstract: Code-switching (CS) presents a significant challenge for general Auto-Speech Recognition (ASR) systems. Existing methods often fail to capture the subtle phonological shifts inherent in CS scenarios. The challenge is particularly difficult for language pairs like Vietnamese and English, where both distinct phonological features and the ambiguity arising from similar sound recognition are present. In this paper, we propose a novel architecture for Vietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC employs a phoneme-centric approach, built upon an extended Vietnamese phoneme set as an intermediate representation to facilitate mixed-lingual modeling. Experimental results demonstrate that TSPC consistently outperforms existing baselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a significantly lower word error rate of 20.8\% with reduced training resources. Furthermore, the phonetic-based two-stage architecture enables phoneme adaptation and language conversion to enhance ASR performance in complex CS Vietnamese-English ASR scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operationalising AI Regulatory Sandboxes under the EU AI Act: The Triple Challenge of Capacity, Coordination and Attractiveness to Providers</title>
<link>https://arxiv.org/abs/2509.05985</link>
<guid>https://arxiv.org/abs/2509.05985</guid>
<content:encoded><![CDATA[
arXiv:2509.05985v1 Announce Type: cross 
Abstract: The EU AI Act provides a rulebook for all AI systems being put on the market or into service in the European Union. This article investigates the requirement under the AI Act that Member States establish national AI regulatory sandboxes for testing and validation of innovative AI systems under regulatory supervision to assist with fostering innovation and complying with regulatory requirements. Against the backdrop of the EU objective that AI regulatory sandboxes would both foster innovation and assist with compliance, considerable challenges are identified for Member States around capacity-building and design of regulatory sandboxes. While Member States are early movers in laying the ground for national AI regulatory sandboxes, the article contends that there is a risk that differing approaches being taken by individual national sandboxes could jeopardise a uniform interpretation of the AI Act and its application in practice. This could motivate innovators to play sandbox arbitrage. The article therefore argues that the European Commission and the AI Board need to act decisively in developing rules and guidance to ensure a cohesive, coordinated approach in national AI regulatory sandboxes. With sandbox participation being voluntary, the possibility that AI regulatory sandboxes may prove unattractive to innovators on their compliance journey is also explored. Confidentiality concerns, the inability to relax legal rules during the sandbox, and the inability of sandboxes to deliver a presumption of conformity with the AI Act are identified as pertinent concerns for innovators contemplating applying to AI regulatory sandboxes as compared with other direct compliance routes provided to them through application of harmonised standards and conformity assessment procedures.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion</title>
<link>https://arxiv.org/abs/2509.05999</link>
<guid>https://arxiv.org/abs/2509.05999</guid>
<content:encoded><![CDATA[
arXiv:2509.05999v1 Announce Type: cross 
Abstract: Monocular 3D Object Detection represents a challenging Computer Vision task due to the nature of the input used, which is a single 2D image, lacking in any depth cues and placing the depth estimation problem as an ill-posed one. Existing solutions leverage the information extracted from the input by using Convolutional Neural Networks or Transformer architectures as feature extraction backbones, followed by specific detection heads for 3D parameters prediction. In this paper, we introduce a decoupled strategy based on injecting precomputed segmentation information priors and fusing them directly into the feature space for guiding the detection, without expanding the detection model or jointly learning the priors. The focus is on evaluating the impact of additional segmentation information on existing detection pipelines without adding additional prediction branches. The proposed method is evaluated on the KITTI 3D Object Detection Benchmark, outperforming the equivalent architecture that relies only on RGB image features for small objects in the scene: pedestrians and cyclists, and proving that understanding the input data can balance the need for additional sensors or training data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Khana: A Comprehensive Indian Cuisine Dataset</title>
<link>https://arxiv.org/abs/2509.06006</link>
<guid>https://arxiv.org/abs/2509.06006</guid>
<content:encoded><![CDATA[
arXiv:2509.06006v1 Announce Type: cross 
Abstract: As global interest in diverse culinary experiences grows, food image models are essential for improving food-related applications by enabling accurate food recognition, recipe suggestions, dietary tracking, and automated meal planning. Despite the abundance of food datasets, a noticeable gap remains in capturing the nuances of Indian cuisine due to its vast regional diversity, complex preparations, and the lack of comprehensive labeled datasets that cover its full breadth. Through this exploration, we uncover Khana, a new benchmark dataset for food image classification, segmentation, and retrieval of dishes from Indian cuisine. Khana fills the gap by establishing a taxonomy of Indian cuisine and offering around 131K images in the dataset spread across 80 labels, each with a resolution of 500x500 pixels. This paper describes the dataset creation process and evaluates state-of-the-art models on classification, segmentation, and retrieval as baselines. Khana bridges the gap between research and development by providing a comprehensive and challenging benchmark for researchers while also serving as a valuable resource for developers creating real-world applications that leverage the rich tapestry of Indian cuisine. Webpage: https://khana.omkar.xyz
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior</title>
<link>https://arxiv.org/abs/2509.06025</link>
<guid>https://arxiv.org/abs/2509.06025</guid>
<content:encoded><![CDATA[
arXiv:2509.06025v1 Announce Type: cross 
Abstract: A central goal of artificial intelligence is to build systems that can understand and predict complex, evolving sequences of events. However, current foundation models, designed for natural language, fail to grasp the holistic nature of structured interactions found in domains like telecommunications, e-commerce and finance. By serializing events into text, they disassemble them into semantically fragmented parts, losing critical context. In this work, we introduce the Unified Interaction Foundation Model (UIFM), a foundation model engineered for genuine behavioral understanding. At its core is the principle of composite tokenization, where each multi-attribute event is treated as a single, semantically coherent unit. This allows UIFM to learn the underlying "grammar" of user behavior, perceiving entire interactions rather than a disconnected stream of data points. We demonstrate that this architecture is not just more accurate, but represents a fundamental step towards creating more adaptable and intelligent predictive systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2509.06026</link>
<guid>https://arxiv.org/abs/2509.06026</guid>
<content:encoded><![CDATA[
arXiv:2509.06026v1 Announce Type: cross 
Abstract: While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations by integrating external knowledge bases, it introduces vulnerabilities to membership inference attacks (MIAs), particularly in systems handling sensitive data. Existing MIAs targeting RAG's external databases often rely on model responses but ignore the interference of non-member-retrieved documents on RAG outputs, limiting their effectiveness. To address this, we propose DCMI, a differential calibration MIA that mitigates the negative impact of non-member-retrieved documents. Specifically, DCMI leverages the sensitivity gap between member and non-member retrieved documents under query perturbation. It generates perturbed queries for calibration to isolate the contribution of member-retrieved documents while minimizing the interference from non-member-retrieved documents. Experiments under progressively relaxed assumptions show that DCMI consistently outperforms baselines--for example, achieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5, exceeding the MBA baseline by over 40%. Furthermore, on real-world RAG platforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the baseline. These results highlight significant privacy risks in RAG systems and emphasize the need for stronger protection mechanisms. We appeal to the community's consideration of deeper investigations, like ours, against the data leakage risks in rapidly evolving RAG systems. Our code is available at https://github.com/Xinyu140203/RAG_MIA.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamAudio: Customized Text-to-Audio Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2509.06027</link>
<guid>https://arxiv.org/abs/2509.06027</guid>
<content:encoded><![CDATA[
arXiv:2509.06027v1 Announce Type: cross 
Abstract: With the development of large-scale diffusion-based and language-modeling-based generative models, impressive progress has been achieved in text-to-audio generation. Despite producing high-quality outputs, existing text-to-audio models mainly aim to generate semantically aligned sound and fall short on precisely controlling fine-grained acoustic characteristics of specific sounds. As a result, users that need specific sound content may find it challenging to generate the desired audio clips. In this paper, we present DreamAudio for customized text-to-audio generation (CTTA). Specifically, we introduce a new framework that is designed to enable the model to identify auditory information from user-provided reference concepts for audio generation. Given a few reference audio samples containing personalized audio events, our system can generate new audio samples that include these specific events. In addition, two types of datasets are developed for training and testing the customized systems. The experiments show that the proposed model, DreamAudio, generates audio samples that are highly consistent with the customized audio features and aligned well with the input text prompts. Furthermore, DreamAudio offers comparable performance in general text-to-audio tasks. We also provide a human-involved dataset containing audio events from real-world CTTA cases as the benchmark for customized generation tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection</title>
<link>https://arxiv.org/abs/2509.06035</link>
<guid>https://arxiv.org/abs/2509.06035</guid>
<content:encoded><![CDATA[
arXiv:2509.06035v1 Announce Type: cross 
Abstract: Automated inspection of transmission lines using UAVs is hindered by the difficulty of detecting small and ambiguous defects against complex backgrounds. Conventional detectors often suffer from detail loss due to strided downsampling, weak boundary sensitivity in lightweight backbones, and insufficient integration of global context with local cues. To address these challenges, we propose TinyDef-DETR, a DETR-based framework designed for small-defect detection. The method introduces a stride-free space-to-depth module for lossless downsampling, an edge-enhanced convolution for boundary-aware feature extraction, a cross-stage dual-domain multi-scale attention module to jointly capture global and local information, and a Focaler-Wise-SIoU regression loss to improve localization of small objects. Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR achieves substantial improvements in both precision and recall compared to competitive baselines, with particularly notable gains on small-object subsets, while incurring only modest computational overhead. Further validation on the VisDrone benchmark confirms the generalization capability of the proposed approach. Overall, the results indicate that integrating detail-preserving downsampling, edge-sensitive representations, dual-domain attention, and difficulty-adaptive regression provides a practical and efficient solution for UAV-based small-defect inspection in power grids.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models</title>
<link>https://arxiv.org/abs/2509.06040</link>
<guid>https://arxiv.org/abs/2509.06040</guid>
<content:encoded><![CDATA[
arXiv:2509.06040v1 Announce Type: cross 
Abstract: Recent advancements in aligning image and video generative models via GRPO have achieved remarkable gains in enhancing human preference alignment. However, these methods still face high computational costs from on-policy rollouts and excessive SDE sampling steps, as well as training instability due to sparse rewards. In this paper, we propose BranchGRPO, a novel method that introduces a branch sampling policy updating the SDE sampling process. By sharing computation across common prefixes and pruning low-reward paths and redundant depths, BranchGRPO substantially lowers the per-update compute cost while maintaining or improving exploration diversity. This work makes three main contributions: (1) a branch sampling scheme that reduces rollout and training cost; (2) a tree-based advantage estimator incorporating dense process-level rewards; and (3) pruning strategies exploiting path and depth redundancy to accelerate convergence and boost performance. Experiments on image and video preference alignment show that BranchGRPO improves alignment scores by 16% over strong baselines, while cutting training time by 50%.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Study of Code Large Language Models for Binary Security Patch Detection</title>
<link>https://arxiv.org/abs/2509.06052</link>
<guid>https://arxiv.org/abs/2509.06052</guid>
<content:encoded><![CDATA[
arXiv:2509.06052v1 Announce Type: cross 
Abstract: Security patch detection (SPD) is crucial for maintaining software security, as unpatched vulnerabilities can lead to severe security risks. In recent years, numerous learning-based SPD approaches have demonstrated promising results on source code. However, these approaches typically cannot be applied to closed-source applications and proprietary systems that constitute a significant portion of real-world software, as they release patches only with binary files, and the source code is inaccessible. Given the impressive performance of code large language models (LLMs) in code intelligence and binary analysis tasks such as decompilation and compilation optimization, their potential for detecting binary security patches remains unexplored, exposing a significant research gap between their demonstrated low-level code understanding capabilities and this critical security task. To address this gap, we construct a large-scale binary patch dataset containing \textbf{19,448} samples, with two levels of representation: assembly code and pseudo-code, and systematically evaluate \textbf{19} code LLMs of varying scales to investigate their capability in binary SPD tasks. Our initial exploration demonstrates that directly prompting vanilla code LLMs struggles to accurately identify security patches from binary patches, and even state-of-the-art prompting techniques fail to mitigate the lack of domain knowledge in binary SPD within vanilla models. Drawing on the initial findings, we further investigate the fine-tuning strategy for injecting binary SPD domain knowledge into code LLMs through two levels of representation. Experimental results demonstrate that fine-tuned LLMs achieve outstanding performance, with the best results obtained on the pseudo-code representation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training</title>
<link>https://arxiv.org/abs/2509.06053</link>
<guid>https://arxiv.org/abs/2509.06053</guid>
<content:encoded><![CDATA[
arXiv:2509.06053v1 Announce Type: cross 
Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress in solving complex multi-player games through self-play. However, training effective adversarial policies requires millions of experience samples and substantial computational resources. Moreover, these policies lack interpretability, hindering their practical deployment. Recently, researchers have successfully leveraged Large Language Models (LLMs) to generate programmatic policies for single-agent tasks, transforming neural network-based policies into interpretable rule-based code with high execution efficiency. Inspired by this, we propose PolicyEvolve, a general framework for generating programmatic policies in multi-player games. PolicyEvolve significantly reduces reliance on manually crafted policy code, achieving high-performance policies with minimal environmental interactions. The framework comprises four modules: Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool preserves elite policies accumulated during iterative training. The Local Pool stores temporary policies for the current iteration; only sufficiently high-performing policies from this pool are promoted to the Global Pool. The Policy Planner serves as the core policy generation module. It samples the top three policies from the Global Pool, generates an initial policy for the current iteration based on environmental information, and refines this policy using feedback from the Trajectory Critic. Refined policies are then deposited into the Local Pool. This iterative process continues until the policy achieves a sufficiently high average win rate against the Global Pool, at which point it is integrated into the Global Pool. The Trajectory Critic analyzes interaction data from the current policy, identifies vulnerabilities, and proposes directional improvements to guide the Policy Planner
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.06060</link>
<guid>https://arxiv.org/abs/2509.06060</guid>
<content:encoded><![CDATA[
arXiv:2509.06060v1 Announce Type: cross 
Abstract: Recent advancements in deep learning models for time series forecasting have been significant. These models often leverage fundamental time series properties such as seasonality and non-stationarity, which may suggest an intrinsic link between model performance and data properties. However, existing benchmark datasets fail to offer diverse and well-defined temporal patterns, restricting the systematic evaluation of such connections. Additionally, there is no effective model recommendation approach, leading to high time and cost expenditures when testing different architectures across different downstream applications. For those reasons, we propose ARIES, a framework for assessing relation between time series properties and modeling strategies, and for recommending deep forcasting models for realistic time series. First, we construct a synthetic dataset with multiple distinct patterns, and design a comprehensive system to compute the properties of time series. Next, we conduct an extensive benchmarking of over 50 forecasting models, and establish the relationship between time series properties and modeling strategies. Our experimental results reveal a clear correlation. Based on these findings, we propose the first deep forecasting model recommender, capable of providing interpretable suggestions for real-world time series. In summary, ARIES is the first study to establish the relations between the properties of time series data and modeling strategies, while also implementing a model recommendation system. The code is available at: https://github.com/blisky-li/ARIES.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects</title>
<link>https://arxiv.org/abs/2509.06085</link>
<guid>https://arxiv.org/abs/2509.06085</guid>
<content:encoded><![CDATA[
arXiv:2509.06085v1 Announce Type: cross 
Abstract: Pre-trained models (PTMs) are machine learning models that have been trained in advance, often on large-scale data, and can be reused for new tasks, thereby reducing the need for costly training from scratch. Their widespread adoption introduces a new class of software dependency, which we term Software Dependencies 2.0, extending beyond conventional libraries to learned behaviors embodied in trained models and their associated artifacts. The integration of PTMs as software dependencies in real projects remains unclear, potentially threatening maintainability and reliability of modern software systems that increasingly rely on them. Objective: In this study, we investigate Software Dependencies 2.0 in open-source software (OSS) projects by examining the reuse of PTMs, with a focus on how developers manage and integrate these models. Specifically, we seek to understand: (1) how OSS projects structure and document their PTM dependencies; (2) what stages and organizational patterns emerge in the reuse pipelines of PTMs within these projects; and (3) the interactions among PTMs and other learned components across pipeline stages. We conduct a mixed-methods analysis of a statistically significant random sample of 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories reusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM reuse by identifying patterns and qualitatively investigate how developers integrate and manage these models in practice.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research</title>
<link>https://arxiv.org/abs/2509.06093</link>
<guid>https://arxiv.org/abs/2509.06093</guid>
<content:encoded><![CDATA[
arXiv:2509.06093v1 Announce Type: cross 
Abstract: Chemical and materials research has traditionally relied heavily on knowledge narrative, with progress often driven by language-based descriptions of principles, mechanisms, and experimental experiences, rather than tables, limiting what conventional databases and ML can exploit. We present a language-native database for boron nitride nanosheet (BNNS) polymer thermally conductive composites that captures lightly structured information from papers across preparation, characterization, theory-computation, and mechanistic reasoning, with evidence-linked snippets. Records are organized in a heterogeneous database and queried via composite retrieval with semantics, key words and value filters. The system can synthesizes literature into accurate, verifiable, and expert style guidance. This substrate enables high fidelity efficient Retrieval Augmented Generation (RAG) and tool augmented agents to interleave retrieval with reasoning and deliver actionable SOP. The framework supplies the language rich foundation required for LLM-driven materials discovery.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs</title>
<link>https://arxiv.org/abs/2509.06094</link>
<guid>https://arxiv.org/abs/2509.06094</guid>
<content:encoded><![CDATA[
arXiv:2509.06094v1 Announce Type: cross 
Abstract: Time-inconsistent preferences, where agents favor smaller-sooner over larger-later rewards, are a key feature of human and animal decision-making. Quasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this behavior, but its integration into the reinforcement learning (RL) framework has been limited. This paper addresses key theoretical and algorithmic gaps for precommitted agents with QH preferences. We make two primary contributions: (i) we formally characterize the structure of the optimal policy, proving for the first time that it reduces to a simple one-step non-stationary form; and (ii) we design the first practical, model-free algorithms for both policy evaluation and Q-learning in this setting, both with provable convergence guarantees. Our results provide foundational insights for incorporating QH preferences in RL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks</title>
<link>https://arxiv.org/abs/2509.06122</link>
<guid>https://arxiv.org/abs/2509.06122</guid>
<content:encoded><![CDATA[
arXiv:2509.06122v1 Announce Type: cross 
Abstract: Multispectral and hyperspectral imagery are widely used in agriculture, environmental monitoring, and urban planning due to their complementary spatial and spectral characteristics. A fundamental trade-off persists: multispectral imagery offers high spatial but limited spectral resolution, while hyperspectral imagery provides rich spectra at lower spatial resolution. Prior hyperspectral generation approaches (e.g., pan-sharpening variants, matrix factorization, CNNs) often struggle to jointly preserve spatial detail and spectral fidelity. In response, we propose SpecSwin3D, a transformer-based model that generates hyperspectral imagery from multispectral inputs while preserving both spatial and spectral quality. Specifically, SpecSwin3D takes five multispectral bands as input and reconstructs 224 hyperspectral bands at the same spatial resolution. In addition, we observe that reconstruction errors grow for hyperspectral bands spectrally distant from the input bands. To address this, we introduce a cascade training strategy that progressively expands the spectral range to stabilize learning and improve fidelity. Moreover, we design an optimized band sequence that strategically repeats and orders the five selected multispectral bands to better capture pairwise relations within a 3D shifted-window transformer framework. Quantitatively, our model achieves a PSNR of 35.82 dB, SAM of 2.40{\deg}, and SSIM of 0.96, outperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by more than half. Beyond reconstruction, we further demonstrate the practical value of SpecSwin3D on two downstream tasks, including land use classification and burnt area segmentation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes</title>
<link>https://arxiv.org/abs/2509.06159</link>
<guid>https://arxiv.org/abs/2509.06159</guid>
<content:encoded><![CDATA[
arXiv:2509.06159v1 Announce Type: cross 
Abstract: The growing popularity of robotic minimally invasive surgeries has made deep learning-based surgical training a key area of research. A thorough understanding of the surgical scene components is crucial, which semantic segmentation models can help achieve. However, most existing work focuses on surgical tools and overlooks anatomical objects. Additionally, current state-of-the-art (SOTA) models struggle to balance capturing high-level contextual features and low-level edge features. We propose a Feature-Adaptive Spatial Localization model (FASL-Seg), designed to capture features at multiple levels of detail through two distinct processing streams, namely a Low-Level Feature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream, for varying feature resolutions - enabling precise segmentation of anatomy and surgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark datasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model achieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy segmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU of 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation, respectively, outperforming SOTA overall performance, with comparable per-class SOTA results in both datasets and consistent performance in various classes for anatomy and instruments, demonstrating the effectiveness of distinct processing streams for varying feature resolutions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracking daily paths in home contexts with RSSI fingerprinting based on UWB through deep learning models</title>
<link>https://arxiv.org/abs/2509.06161</link>
<guid>https://arxiv.org/abs/2509.06161</guid>
<content:encoded><![CDATA[
arXiv:2509.06161v1 Announce Type: cross 
Abstract: The field of human activity recognition has evolved significantly, driven largely by advancements in Internet of Things (IoT) device technology, particularly in personal devices. This study investigates the use of ultra-wideband (UWB) technology for tracking inhabitant paths in home environments using deep learning models. UWB technology estimates user locations via time-of-flight and time-difference-of-arrival methods, which are significantly affected by the presence of walls and obstacles in real environments, reducing their precision. To address these challenges, we propose a fingerprinting-based approach utilizing received signal strength indicator (RSSI) data collected from inhabitants in two flats (60 m2 and 100 m2) while performing daily activities. We compare the performance of convolutional neural network (CNN), long short-term memory (LSTM), and hybrid CNN+LSTM models, as well as the use of Bluetooth technology. Additionally, we evaluate the impact of the type and duration of the temporal window (future, past, or a combination of both). Our results demonstrate a mean absolute error close to 50 cm, highlighting the superiority of the hybrid model in providing accurate location estimates, thus facilitating its application in daily human activity recognition in residential settings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Gender and Political Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2509.06164</link>
<guid>https://arxiv.org/abs/2509.06164</guid>
<content:encoded><![CDATA[
arXiv:2509.06164v1 Announce Type: cross 
Abstract: We introduce EuroParlVote, a novel benchmark for evaluating large language models (LLMs) in politically sensitive contexts. It links European Parliament debate speeches to roll-call vote outcomes and includes rich demographic metadata for each Member of the European Parliament (MEP), such as gender, age, country, and political group. Using EuroParlVote, we evaluate state-of-the-art LLMs on two tasks -- gender classification and vote prediction -- revealing consistent patterns of bias. We find that LLMs frequently misclassify female MEPs as male and demonstrate reduced accuracy when simulating votes for female speakers. Politically, LLMs tend to favor centrist groups while underperforming on both far-left and far-right ones. Proprietary models like GPT-4o outperform open-weight alternatives in terms of both robustness and fairness. We release the EuroParlVote dataset, code, and demo to support future research on fairness and accountability in NLP within political contexts.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning</title>
<link>https://arxiv.org/abs/2509.06165</link>
<guid>https://arxiv.org/abs/2509.06165</guid>
<content:encoded><![CDATA[
arXiv:2509.06165v1 Announce Type: cross 
Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Language Model for Personalized Lung Cancer Screening</title>
<link>https://arxiv.org/abs/2509.06169</link>
<guid>https://arxiv.org/abs/2509.06169</guid>
<content:encoded><![CDATA[
arXiv:2509.06169v1 Announce Type: cross 
Abstract: Accurate risk assessment in lung cancer screening is critical for enabling early cancer detection and minimizing unnecessary invasive procedures. The Lung CT Screening Reporting and Data System (Lung-RADS) has been widely used as the standard framework for patient management and follow-up. Nevertheless, Lung-RADS faces trade-offs between sensitivity and specificity, as it stratifies risk solely based on lung nodule characteristics without incorporating various risk factors. Here we propose a reasoning language model (RLM) to integrate radiology findings with longitudinal medical records for individualized lung cancer risk assessment. Through a systematic study including dataset construction and distillation, supervised fine-tuning, reinforcement learning, and comprehensive evaluation, our model makes significant improvements in risk prediction performance on datasets in the national lung screening trial. Notably, RLM can decompose the risk evaluation task into sub-components, analyze the contributions of diverse risk factors, and synthesize them into a final risk score computed using our data-driven system equation. Our approach improves both predictive accuracy and monitorability through the chain of thought reasoning process, thereby facilitating clinical translation into lung cancer screening.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations</title>
<link>https://arxiv.org/abs/2509.06176</link>
<guid>https://arxiv.org/abs/2509.06176</guid>
<content:encoded><![CDATA[
arXiv:2509.06176v1 Announce Type: cross 
Abstract: As artificial intelligence (AI) systems permeate critical sectors, the need for professionals who can address ethical, legal and governance challenges has become urgent. Current AI ethics education remains fragmented, often siloed by discipline and disconnected from practice. This paper synthesizes literature and regulatory developments to propose a modular, interdisciplinary curriculum that integrates technical foundations with ethics, law and policy. We highlight recurring operational failures in AI - bias, misspecified objectives, generalization errors, misuse and governance breakdowns - and link them to pedagogical strategies for teaching AI governance. Drawing on perspectives from the EU, China and international frameworks, we outline a semester plan that emphasizes integrated ethics, stakeholder engagement and experiential learning. The curriculum aims to prepare students to diagnose risks, navigate regulation and engage diverse stakeholders, fostering adaptive and ethically grounded professionals for responsible AI governance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods</title>
<link>https://arxiv.org/abs/2509.06195</link>
<guid>https://arxiv.org/abs/2509.06195</guid>
<content:encoded><![CDATA[
arXiv:2509.06195v1 Announce Type: cross 
Abstract: Language fairness in multilingual information retrieval (MLIR) systems is crucial for ensuring equitable access to information across diverse languages. This paper sheds light on the issue, based on the assumption that queries in different languages, but with identical semantics, should yield equivalent ranking lists when retrieving on the same multilingual documents. We evaluate the degree of fairness using both traditional retrieval methods, and a DPR neural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a novel loss designed to mitigate language biases in neural MLIR approaches. Our analysis exposes intrinsic language biases in current MLIR technologies, with notable disparities across the retrieval methods, and the effectiveness of LaKDA in enhancing language fairness.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control</title>
<link>https://arxiv.org/abs/2509.06201</link>
<guid>https://arxiv.org/abs/2509.06201</guid>
<content:encoded><![CDATA[
arXiv:2509.06201v1 Announce Type: cross 
Abstract: Grasping of diverse objects in unstructured environments remains a significant challenge. Open-loop grasping methods, effective in controlled settings, struggle in cluttered environments. Grasp prediction errors and object pose changes during grasping are the main causes of failure. In contrast, closed-loop methods address these challenges in simplified settings (e.g., single object on a table) on a limited set of objects, with no path to generalization. We propose Grasp-MPC, a closed-loop 6-DoF vision-based grasping policy designed for robust and reactive grasping of novel objects in cluttered environments. Grasp-MPC incorporates a value function, trained on visual observations from a large-scale synthetic dataset of 2 million grasp trajectories that include successful and failed attempts. We deploy this learned value function in an MPC framework in combination with other cost terms that encourage collision avoidance and smooth execution. We evaluate Grasp-MPC on FetchBench and real-world settings across diverse environments. Grasp-MPC improves grasp success rates by up to 32.6% in simulation and 33.3% in real-world noisy conditions, outperforming open-loop, diffusion policy, transformer policy, and IQL approaches. Videos and more at http://grasp-mpc.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.06213</link>
<guid>https://arxiv.org/abs/2509.06213</guid>
<content:encoded><![CDATA[
arXiv:2509.06213v1 Announce Type: cross 
Abstract: We investigate reinforcement learning in the Game Of Hidden Rules (GOHR) environment, a complex puzzle in which an agent must infer and execute hidden rules to clear a 6$\times$6 board by placing game pieces into buckets. We explore two state representation strategies, namely Feature-Centric (FC) and Object-Centric (OC), and employ a Transformer-based Advantage Actor-Critic (A2C) algorithm for training. The agent has access only to partial observations and must simultaneously infer the governing rule and learn the optimal policy through experience. We evaluate our models across multiple rule-based and trial-list-based experimental setups, analyzing transfer effects and the impact of representation on learning efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Software Engineering: Foundational Pillars and a Research Roadmap</title>
<link>https://arxiv.org/abs/2509.06216</link>
<guid>https://arxiv.org/abs/2509.06216</guid>
<content:encoded><![CDATA[
arXiv:2509.06216v1 Announce Type: cross 
Abstract: Agentic Software Engineering (SE 3.0) represents a new era where intelligent agents are tasked not with simple code generation, but with achieving complex, goal-oriented SE objectives. To harness these new capabilities while ensuring trustworthiness, we must recognize a fundamental duality within the SE field in the Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE for Agents. This duality demands a radical reimagining of the foundational pillars of SE (actors, processes, tools, and artifacts) which manifest differently across each modality. We propose two purpose-built workbenches to support this vision. The Agent Command Environment (ACE) serves as a command center where humans orchestrate and mentor agent teams, handling outputs such as Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The Agent Execution Environment (AEE) is a digital workspace where agents perform tasks while invoking human expertise when facing ambiguity or complex trade-offs. This bi-directional partnership, which supports agent-initiated human callbacks and handovers, gives rise to new, structured engineering activities (i.e., processes) that redefine human-AI collaboration, elevating the practice from agentic coding to true agentic software engineering. This paper presents the Structured Agentic Software Engineering (SASE) vision, outlining several of the foundational pillars for the future of SE. The paper culminates in a research roadmap that identifies a few key challenges and opportunities while briefly discussing the resulting impact of this future on SE education. Our goal is not to offer a definitive solution, but to provide a conceptual scaffold with structured vocabulary to catalyze a community-wide dialogue, pushing the SE community to think beyond its classic, human-centric tenets toward a disciplined, scalable, and trustworthy agentic future.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Efficiency Frontier: Classical Shadows versus Quantum Footage</title>
<link>https://arxiv.org/abs/2509.06218</link>
<guid>https://arxiv.org/abs/2509.06218</guid>
<content:encoded><![CDATA[
arXiv:2509.06218v1 Announce Type: cross 
Abstract: Interfacing quantum and classical processors is an important subroutine in full-stack quantum algorithms. The so-called "classical shadow" method efficiently extracts essential classical information from quantum states, enabling the prediction of many properties of a quantum system from only a few measurements. However, for a small number of highly non-local observables, or when classical post-processing power is limited, the classical shadow method is not always the most efficient choice. Here, we address this issue quantitatively by performing a full-stack resource analysis that compares classical shadows with ``quantum footage," which refers to direct quantum measurement. Under certain assumptions, our analysis illustrates a boundary of download efficiency between classical shadows and quantum footage. For observables expressed as linear combinations of Pauli matrices, the classical shadow method outperforms direct measurement when the number of observables is large and the Pauli weight is small. For observables in the form of large Hermitian sparse matrices, the classical shadow method shows an advantage when the number of observables, the sparsity of the matrix, and the number of qubits fall within a certain range. The key parameters influencing this behavior include the number of qubits $n$, observables $M$, sparsity $k$, Pauli weight $w$, accuracy requirement $\epsilon$, and failure tolerance $\delta$. We also compare the resource consumption of the two methods on different types of quantum computers and identify break-even points where the classical shadow method becomes more efficient, which vary depending on the hardware. This paper opens a new avenue for quantitatively designing optimal strategies for hybrid quantum-classical tomography and provides practical insights for selecting the most suitable quantum measurement approach in real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beamforming-LLM: What, Where and When Did I Miss?</title>
<link>https://arxiv.org/abs/2509.06221</link>
<guid>https://arxiv.org/abs/2509.06221</guid>
<content:encoded><![CDATA[
arXiv:2509.06221v1 Announce Type: cross 
Abstract: We present Beamforming-LLM, a system that enables users to semantically recall conversations they may have missed in multi-speaker environments. The system combines spatial audio capture using a microphone array with retrieval-augmented generation (RAG) to support natural language queries such as, "What did I miss when I was following the conversation on dogs?" Directional audio streams are separated using beamforming, transcribed with Whisper, and embedded into a vector database using sentence encoders. Upon receiving a user query, semantically relevant segments are retrieved, temporally aligned with non-attended segments, and summarized using a lightweight large language model (GPT-4o-mini). The result is a user-friendly interface that provides contrastive summaries, spatial context, and timestamped audio playback. This work lays the foundation for intelligent auditory memory systems and has broad applications in assistive technology, meeting summarization, and context-aware personal spatial computing.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation of CNN Ensemble Results for Enhanced Long-Term Prediction of the ENSO Phenomenon</title>
<link>https://arxiv.org/abs/2509.06227</link>
<guid>https://arxiv.org/abs/2509.06227</guid>
<content:encoded><![CDATA[
arXiv:2509.06227v1 Announce Type: cross 
Abstract: The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO) is still one of the biggest challenges in climate science. While it is true that short-to medium-range performance has been improved significantly using the advances in deep learning, statistical dynamical hybrids, most operational systems still use the simple mean of all ensemble members, implicitly assuming equal skill across members. In this study, we demonstrate, through a strictly a-posteriori evaluation , for any large enough ensemble of ENSO forecasts, there is a subset of members whose skill is substantially higher than that of the ensemble mean. Using a state-of-the-art ENSO forecast system cross-validated against the 1986-2017 observed Nino3.4 index, we identify two Top-5 subsets one ranked on lowest Root Mean Square Error (RMSE) and another on highest Pearson correlation. Generally across all leads, these outstanding members show higher correlation and lower RMSE, with the advantage rising enormously with lead time. Whereas at short leads (1 month) raises the mean correlation by about +0.02 (+1.7%) and lowers the RMSE by around 0.14 {\deg}C or by 23.3% compared to the All-40 mean, at extreme leads (23 months) the correlation is raised by +0.43 (+172%) and RMSE by 0.18 {\deg}C or by 22.5% decrease. The enhancements are largest during crucial ENSO transition periods such as SON and DJF, when accurate amplitude and phase forecasting is of greatest socio-economic benefit, and furthermore season-dependent e.g., mid-year months such as JJA and MJJ have incredibly large RMSE reductions. This study provides a solid foundation for further investigations to identify reliable clues for detecting high-quality ensemble members, thereby enhancing forecasting skill.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Synthesis of Timed Regular Expressions</title>
<link>https://arxiv.org/abs/2509.06262</link>
<guid>https://arxiv.org/abs/2509.06262</guid>
<content:encoded><![CDATA[
arXiv:2509.06262v1 Announce Type: cross 
Abstract: Timed regular expressions serve as a formalism for specifying real-time behaviors of Cyber-Physical Systems. In this paper, we consider the synthesis of timed regular expressions, focusing on generating a timed regular expression consistent with a given set of system behaviors including positive and negative examples, i.e., accepting all positive examples and rejecting all negative examples. We first prove the decidability of the synthesis problem through an exploration of simple timed regular expressions. Subsequently, we propose our method of generating a consistent timed regular expression with minimal length, which unfolds in two steps. The first step is to enumerate and prune candidate parametric timed regular expressions. In the second step, we encode the requirement that a candidate generated by the first step is consistent with the given set into a Satisfiability Modulo Theories (SMT) formula, which is consequently solved to determine a solution to parametric time constraints. Finally, we evaluate our approach on benchmarks, including randomly generated behaviors from target timed models and a case study.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks</title>
<link>https://arxiv.org/abs/2509.06270</link>
<guid>https://arxiv.org/abs/2509.06270</guid>
<content:encoded><![CDATA[
arXiv:2509.06270v1 Announce Type: cross 
Abstract: Sixth generation (6G) systems require environment-aware communication, driven by native artificial intelligence (AI) and integrated sensing and communication (ISAC). Radio maps (RMs), providing spatially continuous channel information, are key enablers. However, generating high-fidelity RM ground truth via electromagnetic (EM) simulations is computationally intensive, motivating machine learning (ML)-based RM construction. The effectiveness of these data-driven methods depends on large-scale, high-quality training data. Current public datasets often focus on single-input single-output (SISO) and limited information, such as path loss, which is insufficient for advanced multi-input multi-output (MIMO) systems requiring detailed channel state information (CSI). To address this gap, this paper presents UrbanMIMOMap, a novel large-scale urban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap offers comprehensive complex CSI matrices across a dense spatial grid, going beyond traditional path loss data. This rich CSI is vital for constructing high-fidelity RMs and serves as a fundamental resource for data-driven RM generation, including deep learning. We demonstrate the dataset's utility through baseline performance evaluations of representative ML methods for RM construction. This work provides a crucial dataset and reference for research in high-precision RM generation, MIMO spatial performance, and ML for 6G environment awareness. The code and data for this work are available at: https://github.com/UNIC-Lab/UrbanMIMOMap.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference for Misspecified Contextual Bandits</title>
<link>https://arxiv.org/abs/2509.06287</link>
<guid>https://arxiv.org/abs/2509.06287</guid>
<content:encoded><![CDATA[
arXiv:2509.06287v1 Announce Type: cross 
Abstract: Contextual bandit algorithms have transformed modern experimentation by enabling real-time adaptation for personalized treatment and efficient use of data. Yet these advantages create challenges for statistical inference due to adaptivity. A fundamental property that supports valid inference is policy convergence, meaning that action-selection probabilities converge in probability given the context. Convergence ensures replicability of adaptive experiments and stability of online algorithms. In this paper, we highlight a previously overlooked issue: widely used algorithms such as LinUCB may fail to converge when the reward model is misspecified, and such non-convergence creates fundamental obstacles for statistical inference. This issue is practically important, as misspecified models -- such as linear approximations of complex dynamic system -- are often employed in real-world adaptive experiments to balance bias and variance.
  Motivated by this insight, we propose and analyze a broad class of algorithms that are guaranteed to converge even under model misspecification. Building on this guarantee, we develop a general inference framework based on an inverse-probability-weighted Z-estimator (IPW-Z) and establish its asymptotic normality with a consistent variance estimator. Simulation studies confirm that the proposed method provides robust and data-efficient confidence intervals, and can outperform existing approaches that exist only in the special case of offline policy evaluation. Taken together, our results underscore the importance of designing adaptive algorithms with built-in convergence guarantees to enable stable experimentation and valid statistical inference in practice.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Walk with Less: a Dyna-Style Approach to Quadrupedal Locomotion</title>
<link>https://arxiv.org/abs/2509.06296</link>
<guid>https://arxiv.org/abs/2509.06296</guid>
<content:encoded><![CDATA[
arXiv:2509.06296v1 Announce Type: cross 
Abstract: Traditional RL-based locomotion controllers often suffer from low data efficiency, requiring extensive interaction to achieve robust performance. We present a model-based reinforcement learning (MBRL) framework that improves sample efficiency for quadrupedal locomotion by appending synthetic data to the end of standard rollouts in PPO-based controllers, following the Dyna-Style paradigm. A predictive model, trained alongside the policy, generates short-horizon synthetic transitions that are gradually integrated using a scheduling strategy based on the policy update iterations. Through an ablation study, we identified a strong correlation between sample efficiency and rollout length, which guided the design of our experiments. We validated our approach in simulation on the Unitree Go1 robot and showed that replacing part of the simulated steps with synthetic ones not only mimics extended rollouts but also improves policy return and reduces variance. Finally, we demonstrate that this improvement transfers to the ability to track a wide range of locomotion commands using fewer simulated steps.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs</title>
<link>https://arxiv.org/abs/2509.06326</link>
<guid>https://arxiv.org/abs/2509.06326</guid>
<content:encoded><![CDATA[
arXiv:2509.06326v1 Announce Type: cross 
Abstract: As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to reduce network dependency, improve privacy, and enhance responsiveness, verifying the legitimacy of models running on local devices becomes critical. Existing attestation techniques are not suitable for billion-parameter Large Language Models (LLMs), struggling to remain both time- and memory-efficient while addressing emerging threats in the LLM era. In this paper, we present AttestLLM, the first-of-its-kind attestation framework to protect the hardware-level intellectual property (IP) of device vendors by ensuring that only authorized LLMs can execute on target platforms. AttestLLM leverages an algorithm/software/hardware co-design approach to embed robust watermarking signatures onto the activation distributions of LLM building blocks. It also optimizes the attestation protocol within the Trusted Execution Environment (TEE), providing efficient verification without compromising inference throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen, and Phi families for on-device use cases demonstrate AttestLLM's attestation reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model legitimacy and exhibits resilience against model replacement and forgery attacks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2509.06332</link>
<guid>https://arxiv.org/abs/2509.06332</guid>
<content:encoded><![CDATA[
arXiv:2509.06332v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable emergent capabilities, yet the robustness of their numerical reasoning remains an open question. While standard benchmarks evaluate LLM reasoning on complex problem sets using aggregated metrics, they often obscure foundational weaknesses. In this work, we probe LLM mathematical numeracy by evaluating performance on problems of escalating complexity, from constituent operations to combinatorial puzzles. We test several state-of-the-art LLM-based agents on a 100-problem challenge comprising four categories: (1) basic arithmetic, (2) advanced operations, (3) primality checking, and (4) the Game of 24 number puzzle. Our results show that while the agents achieved high accuracy on the first three categories, which require deterministic algorithmic execution, they consistently failed at the number puzzle, underlining its demand for a heuristic search over a large combinatorial space to be a significant bottleneck. These findings reveal that the agents' proficiency is largely confined to recalling and executing known algorithms, rather than performing generative problem-solving. This suggests their apparent numerical reasoning is more akin to sophisticated pattern-matching than flexible, analytical thought, limiting their potential for tasks that require novel or creative numerical insights.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing</title>
<link>https://arxiv.org/abs/2509.06336</link>
<guid>https://arxiv.org/abs/2509.06336</guid>
<content:encoded><![CDATA[
arXiv:2509.06336v1 Announce Type: cross 
Abstract: Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain performance by employing vision-language models like CLIP. However, existing CLIP-based FAS models do not fully exploit CLIP's patch embedding tokens, failing to detect critical spoofing clues. Moreover, these models rely on a single text prompt per class (e.g., 'live' or 'fake'), which limits generalization. To address these issues, we propose MVP-FAS, a novel framework incorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text Patch Alignment (MTPA). Both modules utilize multiple paraphrased texts to generate generalized features and reduce dependence on domain-specific text. MVS extracts local detailed spatial features and global context from patch embeddings by leveraging diverse texts with multiple perspectives. MTPA aligns patches with multiple text representations to improve semantic robustness. Extensive experiments demonstrate that MVP-FAS achieves superior generalization performance, outperforming previous state-of-the-art methods on cross-domain datasets. Code: https://github.com/Elune001/MVP-FAS.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ban&amp;Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs</title>
<link>https://arxiv.org/abs/2509.06346</link>
<guid>https://arxiv.org/abs/2509.06346</guid>
<content:encoded><![CDATA[
arXiv:2509.06346v1 Announce Type: cross 
Abstract: Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling large language models (LLMs) efficiently. Recent fine-grained MoE designs introduce hundreds of experts per layer, with multiple experts activated per token, enabling stronger specialization. However, during pre-training, routers are optimized mainly for stability and robustness: they converge prematurely and enforce balanced usage, limiting the full potential of model performance and efficiency. In this work, we uncover two overlooked issues: (i) a few highly influential experts are underutilized due to premature and balanced routing decisions; and (ii) enforcing a fixed number of active experts per token introduces substantial redundancy. Instead of retraining models or redesigning MoE architectures, we introduce Ban&amp;Pick, a post-training, plug-and-play strategy for smarter MoE routing. Pick discovers and reinforces key experts-a small group with outsized impact on performance-leading to notable accuracy gains across domains. Ban complements this by dynamically pruning redundant experts based on layer and token sensitivity, delivering faster inference with minimal accuracy loss. Experiments on fine-grained MoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks demonstrate that Ban&amp;Pick delivers free performance gains and inference acceleration without retraining or architectural changes. For instance, on Qwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from 65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the vLLM.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?</title>
<link>https://arxiv.org/abs/2509.06350</link>
<guid>https://arxiv.org/abs/2509.06350</guid>
<content:encoded><![CDATA[
arXiv:2509.06350v1 Announce Type: cross 
Abstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various successful methods whereby attackers manipulate models into generating harmful responses that they are designed to avoid. Among these, Greedy Coordinate Gradient (GCG) has emerged as a general and effective approach that optimizes the tokens in a suffix to generate jailbreakable prompts. While several improved variants of GCG have been proposed, they all rely on fixed-length suffixes. However, the potential redundancy within these suffixes remains unexplored. In this work, we propose Mask-GCG, a plug-and-play method that employs learnable token masking to identify impactful tokens within the suffix. Our approach increases the update probability for tokens at high-impact positions while pruning those at low-impact positions. This pruning not only reduces redundancy but also decreases the size of the gradient space, thereby lowering computational overhead and shortening the time required to achieve successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the original GCG and several improved variants. Experimental results show that most tokens in the suffix contribute significantly to attack success, and pruning a minority of low-impact tokens does not affect the loss values or compromise the attack success rate (ASR), thereby revealing token redundancy in LLM prompts. Our findings provide insights for developing efficient and interpretable LLMs from the perspective of jailbreak attacks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PL-CA: A Parametric Legal Case Augmentation Framework</title>
<link>https://arxiv.org/abs/2509.06356</link>
<guid>https://arxiv.org/abs/2509.06356</guid>
<content:encoded><![CDATA[
arXiv:2509.06356v1 Announce Type: cross 
Abstract: Conventional RAG is considered one of the most effective methods for addressing model knowledge insufficiency and hallucination, particularly in the judicial domain that requires high levels of knowledge rigor, logical consistency, and content integrity. However, the conventional RAG method only injects retrieved documents directly into the model's context, which severely constrains models due to their limited context windows and introduces additional computational overhead through excessively long contexts, thereby disrupting models' attention and degrading performance on downstream tasks. Moreover, many existing benchmarks lack expert annotation and focus solely on individual downstream tasks while real-world legal scenarios consist of multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for reflecting models' true capabilities. To address these limitations, we propose PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data augmentation on corpus knowledge and encode this legal knowledge into parametric vectors, and then integrates this parametric knowledge into the LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context pressure. Additionally, we also construct a multi-task legal dataset comprising more than 2000 training and test instances, which are all expert-annotated and manually verified. We conduct our experiments on our dataset, and the experimental results demonstrate that our method reduces the overhead associated with excessively long contexts while maintaining competitive performance on downstream tasks compared to conventional RAG. Our code and dataset are provided in the appendix.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification</title>
<link>https://arxiv.org/abs/2509.06367</link>
<guid>https://arxiv.org/abs/2509.06367</guid>
<content:encoded><![CDATA[
arXiv:2509.06367v1 Announce Type: cross 
Abstract: Drought stress is a major threat to global crop productivity, making its early and precise detection essential for sustainable agricultural management. Traditional approaches, though useful, are often time-consuming and labor-intensive, which has motivated the adoption of deep learning methods. In recent years, Convolutional Neural Network (CNN) and Vision Transformer architectures have been widely explored for drought stress identification; however, these models generally rely on a large number of trainable parameters, restricting their use in resource-limited and real-time agricultural settings. To address this challenge, we propose a novel lightweight hybrid CNN framework inspired by ResNet, DenseNet, and MobileNet architectures. The framework achieves a remarkable 15-fold reduction in trainable parameters compared to conventional CNN and Vision Transformer models, while maintaining competitive accuracy. In addition, we introduce a machine unlearning mechanism based on a gradient norm-based influence function, which enables targeted removal of specific training data influence, thereby improving model adaptability. The method was evaluated on an aerial image dataset of potato fields with expert-annotated healthy and drought-stressed regions. Experimental results show that our framework achieves high accuracy while substantially lowering computational costs. These findings highlight its potential as a practical, scalable, and adaptive solution for drought stress monitoring in precision agriculture, particularly under resource-constrained conditions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Pre-Service Horizon: Infusing In-Service Behavior for Improved Financial Risk Forecasting</title>
<link>https://arxiv.org/abs/2509.06385</link>
<guid>https://arxiv.org/abs/2509.06385</guid>
<content:encoded><![CDATA[
arXiv:2509.06385v1 Announce Type: cross 
Abstract: Typical financial risk management involves distinct phases for pre-service risk assessment and in-service default detection, often modeled separately. This paper proposes a novel framework, Multi-Granularity Knowledge Distillation (abbreviated as MGKD), aimed at improving pre-service risk prediction through the integration of in-service user behavior data. MGKD follows the idea of knowledge distillation, where the teacher model, trained on historical in-service data, guides the student model, which is trained on pre-service data. By using soft labels derived from in-service data, the teacher model helps the student model improve its risk prediction prior to service activation. Meanwhile, a multi-granularity distillation strategy is introduced, including coarse-grained, fine-grained, and self-distillation, to align the representations and predictions of the teacher and student models. This approach not only reinforces the representation of default cases but also enables the transfer of key behavioral patterns associated with defaulters from the teacher to the student model, thereby improving the overall performance of pre-service risk assessment. Moreover, we adopt a re-weighting strategy to mitigate the model's bias towards the minority class. Experimental results on large-scale real-world datasets from Tencent Mobile Payment demonstrate the effectiveness of our proposed approach in both offline and online scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeanFlow-Accelerated Multimodal Video-to-Audio Synthesis via One-Step Generation</title>
<link>https://arxiv.org/abs/2509.06389</link>
<guid>https://arxiv.org/abs/2509.06389</guid>
<content:encoded><![CDATA[
arXiv:2509.06389v1 Announce Type: cross 
Abstract: A key challenge in synthesizing audios from silent videos is the inherent trade-off between synthesis quality and inference efficiency in existing methods. For instance, flow matching based models rely on modeling instantaneous velocity, inherently require an iterative sampling process, leading to slow inference speeds. To address this efficiency bottleneck, we introduce a MeanFlow-accelerated model that characterizes flow fields using average velocity, enabling one-step generation and thereby significantly accelerating multimodal video-to-audio (VTA) synthesis while preserving audio quality, semantic alignment, and temporal synchronization. Furthermore, a scalar rescaling mechanism is employed to balance conditional and unconditional predictions when classifier-free guidance (CFG) is applied, effectively mitigating CFG-induced distortions in one step generation. Since the audio synthesis network is jointly trained with multimodal conditions, we further evaluate it on text-to-audio (TTA) synthesis task. Experimental results demonstrate that incorporating MeanFlow into the network significantly improves inference speed without compromising perceptual quality on both VTA and TTA synthesis tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.06415</link>
<guid>https://arxiv.org/abs/2509.06415</guid>
<content:encoded><![CDATA[
arXiv:2509.06415v1 Announce Type: cross 
Abstract: Recent progress in vision-language models (VLMs) has led to impressive results in document understanding tasks, but their high computational demands remain a challenge. To mitigate the compute burdens, we propose a lightweight token pruning framework that filters out non-informative background regions from document images prior to VLM processing. A binary patch-level classifier removes non-text areas, and a max-pooling refinement step recovers fragmented text regions to enhance spatial coherence. Experiments on real-world document datasets demonstrate that our approach substantially lowers computational costs, while maintaining comparable accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup</title>
<link>https://arxiv.org/abs/2509.06419</link>
<guid>https://arxiv.org/abs/2509.06419</guid>
<content:encoded><![CDATA[
arXiv:2509.06419v1 Announce Type: cross 
Abstract: Time series anomaly detection (TSAD) is a vital yet challenging task, particularly in scenarios where labeled anomalies are scarce and temporal dependencies are complex. Recent anomaly assumption (AA) approaches alleviate the lack of anomalies by injecting synthetic samples and training discriminative models. Despite promising results, these methods often suffer from two fundamental limitations: patchy generation, where scattered anomaly knowledge leads to overly simplistic or incoherent anomaly injection, and Anomaly Shift, where synthetic anomalies either resemble normal data too closely or diverge unrealistically from real anomalies, thereby distorting classification boundaries. In this paper, we propose CAPMix, a controllable anomaly augmentation framework that addresses both issues. First, we design a CutAddPaste mechanism to inject diverse and complex anomalies in a targeted manner, avoiding patchy generation. Second, we introduce a label revision strategy to adaptively refine anomaly labels, reducing the risk of anomaly shift. Finally, we employ dual-space mixup within a temporal convolutional network to enforce smoother and more robust decision boundaries. Extensive experiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and ESA, demonstrate that CAPMix achieves significant improvements over state-of-the-art baselines, with enhanced robustness against contaminated training data. The code is available at https://github.com/alsike22/CAPMix.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster</title>
<link>https://arxiv.org/abs/2509.06426</link>
<guid>https://arxiv.org/abs/2509.06426</guid>
<content:encoded><![CDATA[
arXiv:2509.06426v1 Announce Type: cross 
Abstract: Computational models are critical to advance our understanding of how neural, biomechanical, and physical systems interact to orchestrate animal behaviors. Despite the availability of near-complete reconstructions of the Drosophila melanogaster central nervous system, musculature, and exoskeleton, anatomically and physically grounded models of fly leg muscles are still missing. These models provide an indispensable bridge between motor neuron activity and joint movements. Here, we introduce the first 3D, data-driven musculoskeletal model of Drosophila legs, implemented in both OpenSim and MuJoCo simulation environments. Our model incorporates a Hill-type muscle representation based on high-resolution X-ray scans from multiple fixed specimens. We present a pipeline for constructing muscle models using morphological imaging data and for optimizing unknown muscle parameters specific to the fly. We then combine our musculoskeletal models with detailed 3D pose estimation data from behaving flies to achieve muscle-actuated behavioral replay in OpenSim. Simulations of muscle activity across diverse walking and grooming behaviors predict coordinated muscle synergies that can be tested experimentally. Furthermore, by training imitation learning policies in MuJoCo, we test the effect of different passive joint properties on learning speed and find that damping and stiffness facilitate learning. Overall, our model enables the investigation of motor control in an experimentally tractable model organism, providing insights into how biomechanics contribute to generation of complex limb movements. Moreover, our model can be used to control embodied artificial agents to generate naturalistic and compliant locomotion in simulated environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HECATE: An ECS-based Framework for Teaching and Developing Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.06431</link>
<guid>https://arxiv.org/abs/2509.06431</guid>
<content:encoded><![CDATA[
arXiv:2509.06431v1 Announce Type: cross 
Abstract: This paper introduces HECATE, a novel framework based on the Entity-Component-System (ECS) architectural pattern that bridges the gap between distributed systems engineering and MAS development. HECATE is built using the Entity-Component-System architectural pattern, leveraging data-oriented design to implement multiagent systems. This approach involves engineering multiagent systems (MAS) from a distributed systems (DS) perspective, integrating agent concepts directly into the DS domain. This approach simplifies MAS development by (i) reducing the need for specialized agent knowledge and (ii) leveraging familiar DS patterns and standards to minimize the agent-specific knowledge required for engineering MAS. We present the framework's architecture, core components, and implementation approach, demonstrating how it supports different agent models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning</title>
<link>https://arxiv.org/abs/2509.06461</link>
<guid>https://arxiv.org/abs/2509.06461</guid>
<content:encoded><![CDATA[
arXiv:2509.06461v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Several Performance Bounds on Decentralized Online Optimization are Highly Conservative and Potentially Misleading</title>
<link>https://arxiv.org/abs/2509.06466</link>
<guid>https://arxiv.org/abs/2509.06466</guid>
<content:encoded><![CDATA[
arXiv:2509.06466v1 Announce Type: cross 
Abstract: We analyze Decentralized Online Optimization algorithms using the Performance Estimation Problem approach which allows, to automatically compute exact worst-case performance of optimization algorithms. Our analysis shows that several available performance guarantees are very conservative, sometimes by multiple orders of magnitude, and can lead to misguided choices of algorithm. Moreover, at least in terms of worst-case performance, some algorithms appear not to benefit from inter-agent communications for a significant period of time. We show how to improve classical methods by tuning their step-sizes, and find that we can save up to 20% on their actual worst-case performance regret.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems</title>
<link>https://arxiv.org/abs/2509.06475</link>
<guid>https://arxiv.org/abs/2509.06475</guid>
<content:encoded><![CDATA[
arXiv:2509.06475v1 Announce Type: cross 
Abstract: AI-based recommender systems increasingly influence recruitment decisions. Thus, transparency and responsible adoption in Human Resource Management (HRM) are critical. This study examines how HR managers' AI literacy influences their subjective perception and objective understanding of explainable AI (XAI) elements in recruiting recommender dashboards. In an online experiment, 410 German-based HR managers compared baseline dashboards to versions enriched with three XAI styles: important features, counterfactuals, and model criteria. Our results show that the dashboards used in practice do not explain AI results and even keep AI elements opaque. However, while adding XAI features improves subjective perceptions of helpfulness and trust among users with moderate or high AI literacy, it does not increase their objective understanding. It may even reduce accurate understanding, especially with complex explanations. Only overlays of important features significantly aided the interpretations of high-literacy users. Our findings highlight that the benefits of XAI in recruitment depend on users' AI literacy, emphasizing the need for tailored explanation strategies and targeted literacy training in HRM to ensure fair, transparent, and effective adoption of AI.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT</title>
<link>https://arxiv.org/abs/2509.06483</link>
<guid>https://arxiv.org/abs/2509.06483</guid>
<content:encoded><![CDATA[
arXiv:2509.06483v1 Announce Type: cross 
Abstract: The wide spreading of Internet of Things (IoT) sensors generates vast spatio-temporal data streams, but ensuring data credibility is a critical yet unsolved challenge for applications like smart homes. While spatio-temporal graph (STG) models are a leading paradigm for such data, they often fall short in dynamic, human-centric environments due to two fundamental limitations: (1) their reliance on static graph topologies, which fail to capture physical, event-driven dynamics, and (2) their tendency to confuse spurious correlations with true causality, undermining robustness in human-centric environments. To address these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network (DyC-STG), a novel framework designed for real-time data credibility analysis in IoT. Our framework features two synergistic contributions: an event-driven dynamic graph module that adapts the graph topology in real-time to reflect physical state changes, and a causal reasoning module to distill causally-aware representations by strictly enforcing temporal precedence. To facilitate the research in this domain we release two new real-world datasets. Comprehensive experiments show that DyC-STG establishes a new state-of-the-art, outperforming the strongest baselines by 1.4 percentage points and achieving an F1-Score of up to 0.930.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients</title>
<link>https://arxiv.org/abs/2509.06516</link>
<guid>https://arxiv.org/abs/2509.06516</guid>
<content:encoded><![CDATA[
arXiv:2509.06516v1 Announce Type: cross 
Abstract: Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in intesive care unit (ICU) and operating room (OR). However, the high incidence of poor, incomplete, and inconsistent signal quality, can lead to false alarms or diagnostic inaccuracies. The methods explored so far suffer from limited generalizability, reliance on extensive labeled data, and poor cross-task transferability. To overcome these challenges, we introduce QualityFM, a novel multimodal foundation model for these physiological signals, designed to acquire a general-purpose understanding of signal quality. Our model is pre-trained on an large-scale dataset comprising over 21 million 30-second waveforms and 179,757 hours of data. Our approach involves a dual-track architecture that processes paired physiological signals of differing quality, leveraging a self-distillation strategy where an encoder for high-quality signals is used to guide the training of an encoder for low-quality signals. To efficiently handle long sequential signals and capture essential local quasi-periodic patterns, we integrate a windowed sparse attention mechanism within our Transformer-based model. Furthermore, a composite loss function, which combines direct distillation loss on encoder outputs with indirect reconstruction loss based on power and phase spectra, ensures the preservation of frequency-domain characteristics of the signals. We pre-train three models with varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy and practical value through transfer learning on three distinct clinical tasks: false alarm of ventricular tachycardia detection, the identification of atrial fibrillation and the estimation of arterial blood pressure (ABP) from PPG and ECG signals.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training</title>
<link>https://arxiv.org/abs/2509.06518</link>
<guid>https://arxiv.org/abs/2509.06518</guid>
<content:encoded><![CDATA[
arXiv:2509.06518v1 Announce Type: cross 
Abstract: Transformer-based language models traditionally use uniform (isotropic) layer sizes, yet they ignore the diverse functional roles that different depths can play and their computational capacity needs. Building on Layer-Wise Scaling (LWS) and pruning literature, we introduce three new LWS variants - Framed, Reverse, and Crown - that redistribute FFN widths and attention heads via two or three-point linear interpolation in the pre-training stage. We present the first systematic ablation of LWS and its variants, on a fixed budget of 180M parameters, trained on 5B tokens. All models converge to similar losses and achieve better performance compared to an equal-cost isotropic baseline, without a substantial decrease in training throughput. This work represents an initial step into the design space of layer-wise architectures for pre-training, but future work should scale experiments to orders of magnitude more tokens and parameters to fully assess their potential.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2509.06531</link>
<guid>https://arxiv.org/abs/2509.06531</guid>
<content:encoded><![CDATA[
arXiv:2509.06531v1 Announce Type: cross 
Abstract: Link prediction in knowledge graphs requires integrating structural information and semantic context to infer missing entities. While large language models offer strong generative reasoning capabilities, their limited exploitation of structural signals often results in structural sparsity and semantic ambiguity, especially under incomplete or zero-shot settings. To address these challenges, we propose SLiNT (Structure-aware Language model with Injection and coNtrastive Training), a modular framework that injects knowledge-graph-derived structural context into a frozen LLM backbone with lightweight LoRA-based adaptation for robust link prediction. Specifically, Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive Learning (DHCL) introduces fine-grained supervision by interpolating hard positives and negatives to resolve entity-level ambiguity; and Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware intervention while preserving the core LLM parameters. Experiments on WN18RR and FB15k-237 show that SLiNT achieves superior or competitive performance compared with both embedding-based and generation-based baselines, demonstrating the effectiveness of structure-aware representation learning for scalable knowledge graph completion.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language Learning''</title>
<link>https://arxiv.org/abs/2509.06535</link>
<guid>https://arxiv.org/abs/2509.06535</guid>
<content:encoded><![CDATA[
arXiv:2509.06535v1 Announce Type: cross 
Abstract: We investigated the reproducibility of FairCLIP, proposed by Luo et al. (2024), for improving the group fairness of CLIP (Radford et al., 2021) by minimizing image-text similarity score disparities across sensitive groups using the Sinkhorn distance. The experimental setup of Luo et al. (2024) was reproduced to primarily investigate the research findings for FairCLIP. The model description by Luo et al. (2024) was found to differ from the original implementation. Therefore, a new implementation, A-FairCLIP, is introduced to examine specific design choices. Furthermore, FairCLIP+ is proposed to extend the FairCLIP objective to include multiple attributes. Additionally, the impact of the distance minimization on FairCLIP's fairness and performance was explored. In alignment with the original authors, CLIP was found to be biased towards certain demographics when applied to zero-shot glaucoma classification using medical scans and clinical notes from the Harvard-FairVLMed dataset. However, the experimental results on two datasets do not support their claim that FairCLIP improves the performance and fairness of CLIP. Although the regularization objective reduces Sinkhorn distances, both the official implementation and the aligned implementation, A-FairCLIP, were not found to improve performance nor fairness in zero-shot glaucoma classification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model</title>
<link>https://arxiv.org/abs/2509.06539</link>
<guid>https://arxiv.org/abs/2509.06539</guid>
<content:encoded><![CDATA[
arXiv:2509.06539v1 Announce Type: cross 
Abstract: CAGE-2 is an accepted benchmark for learning and evaluating defender strategies against cyberattacks. It reflects a scenario where a defender agent protects an IT infrastructure against various attacks. Many defender methods for CAGE-2 have been proposed in the literature. In this paper, we construct a formal model for CAGE-2 using the framework of Partially Observable Markov Decision Process (POMDP). Based on this model, we define an optimal defender strategy for CAGE-2 and introduce a method to efficiently learn this strategy. Our method, called BF-PPO, is based on PPO, and it uses particle filter to mitigate the computational complexity due to the large state space of the CAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and compare its performance with that of CARDIFF, the highest ranked method on the CAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the learned defender strategy and the required training time.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal-Based Malware Classification Using 1D CNNs</title>
<link>https://arxiv.org/abs/2509.06548</link>
<guid>https://arxiv.org/abs/2509.06548</guid>
<content:encoded><![CDATA[
arXiv:2509.06548v1 Announce Type: cross 
Abstract: Malware classification is a contemporary and ongoing challenge in cyber-security: modern obfuscation techniques are able to evade traditional static analysis, while dynamic analysis is too resource intensive to be deployed at a large scale. One prominent line of research addresses these limitations by converting malware binaries into 2D images by heuristically reshaping them into a 2D grid before resizing using Lanczos resampling. These images can then be classified based on their textural information using computer vision approaches. While this approach can detect obfuscated malware more effectively than static analysis, the process of converting files into 2D images results in significant information loss due to both quantisation noise, caused by rounding to integer pixel values, and the introduction of 2D dependencies which do not exist in the original data. This loss of signal limits the classification performance of the downstream model. This work addresses these weaknesses by instead resizing the files into 1D signals which avoids the need for heuristic reshaping, and additionally these signals do not suffer from quantisation noise due to being stored in a floating-point format. It is shown that existing 2D CNN architectures can be readily adapted to classify these 1D signals for improved performance. Furthermore, a bespoke 1D convolutional neural network, based on the ResNet architecture and squeeze-and-excitation layers, was developed to classify these signals and evaluated on the MalNet dataset. It was found to achieve state-of-the-art performance on binary, type, and family level classification with F1 scores of 0.874, 0.503, and 0.507, respectively, paving the way for future models to operate on the proposed signal modality.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs</title>
<link>https://arxiv.org/abs/2509.06550</link>
<guid>https://arxiv.org/abs/2509.06550</guid>
<content:encoded><![CDATA[
arXiv:2509.06550v1 Announce Type: cross 
Abstract: Network intrusion detection remains a critical challenge in cybersecurity. While supervised machine learning models achieve state-of-the-art performance, their reliance on large labelled datasets makes them impractical for many real-world applications. Anomaly detection methods, which train exclusively on benign traffic to identify malicious activity, suffer from high false positive rates, limiting their usability. Recently, self-supervised learning techniques have demonstrated improved performance with lower false positive rates by learning discriminative latent representations of benign traffic. In particular, contrastive self-supervised models achieve this by minimizing the distance between similar (positive) views of benign traffic while maximizing it between dissimilar (negative) views. Existing approaches generate positive views through data augmentation and treat other samples as negative. In contrast, this work introduces Contrastive Learning using Augmented Negative pairs (CLAN), a novel paradigm for network intrusion detection where augmented samples are treated as negative views - representing potentially malicious distributions - while other benign samples serve as positive views. This approach enhances both classification accuracy and inference efficiency after pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset demonstrates that the proposed method surpasses existing self-supervised and anomaly detection techniques in a binary classification task. Furthermore, when fine-tuned on a limited labelled dataset, the proposed approach achieves superior multi-class classification performance compared to existing self-supervised models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Detection and Tracking Based on Radar Range-Doppler Feature</title>
<link>https://arxiv.org/abs/2509.06569</link>
<guid>https://arxiv.org/abs/2509.06569</guid>
<content:encoded><![CDATA[
arXiv:2509.06569v1 Announce Type: cross 
Abstract: Detection and tracking are the basic tasks of radar systems. Current joint detection tracking methods, which focus on dynamically adjusting detection thresholds from tracking results, still present challenges in fully utilizing the potential of radar signals. These are mainly reflected in the limited capacity of the constant false-alarm rate model to accurately represent information, the insufficient depiction of complex scenes, and the limited information acquired by the tracker. We introduce the Integrated Detection and Tracking based on radar feature (InDT) method, which comprises a network architecture for radar signal detection and a tracker that leverages detection assistance. The InDT detector extracts feature information from each Range-Doppler (RD) matrix and then returns the target position through the feature enhancement module and the detection head. The InDT tracker adaptively updates the measurement noise covariance of the Kalman filter based on detection confidence. The similarity of target RD features is measured by cosine distance, which enhances the data association process by combining location and feature information. Finally, the efficacy of the proposed method was validated through testing on both simulated data and publicly available datasets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models</title>
<link>https://arxiv.org/abs/2509.06596</link>
<guid>https://arxiv.org/abs/2509.06596</guid>
<content:encoded><![CDATA[
arXiv:2509.06596v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Spatial and Semantic Embeddings for Stereo Sound Event Localization in Videos</title>
<link>https://arxiv.org/abs/2509.06598</link>
<guid>https://arxiv.org/abs/2509.06598</guid>
<content:encoded><![CDATA[
arXiv:2509.06598v1 Announce Type: cross 
Abstract: In this study, we address the multimodal task of stereo sound event localization and detection with source distance estimation (3D SELD) in regular video content. 3D SELD is a complex task that combines temporal event classification with spatial localization, requiring reasoning across spatial, temporal, and semantic dimensions. The last is arguably the most challenging to model. Traditional SELD approaches typically rely on multichannel input, limiting their capacity to benefit from large-scale pre-training due to data constraints. To overcome this, we enhance a standard SELD architecture with semantic information by integrating pre-trained, contrastive language-aligned models: CLAP for audio and OWL-ViT for visual inputs. These embeddings are incorporated into a modified Conformer module tailored for multimodal fusion, which we refer to as the Cross-Modal Conformer. We perform an ablation study on the development set of the DCASE2025 Task3 Stereo SELD Dataset to assess the individual contributions of the language-aligned models and benchmark against the DCASE Task 3 baseline systems. Additionally, we detail the curation process of large synthetic audio and audio-visual datasets used for model pre-training. These datasets were further expanded through left-right channel swapping augmentation. Our approach, combining extensive pre-training, model ensembling, and visual post-processing, achieved second rank in the DCASE 2025 Challenge Task 3 (Track B), underscoring the effectiveness of our method. Future work will explore the modality-specific contributions and architectural refinements.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards</title>
<link>https://arxiv.org/abs/2509.06602</link>
<guid>https://arxiv.org/abs/2509.06602</guid>
<content:encoded><![CDATA[
arXiv:2509.06602v1 Announce Type: cross 
Abstract: Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology specialists collaboratively assess complex patient cases to determine optimal treatment strategies. A central element of this process is the patient summary, typically compiled by a medical oncologist, radiation oncologist, or surgeon, or their trained medical assistant, who distills heterogeneous medical records into a concise narrative to facilitate discussion. This manual approach is often labor-intensive, subjective, and prone to omissions of critical information. To address these limitations, we introduce the Healthcare Agent Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that coordinates a multi-agent clinical workflow to generate accurate and comprehensive patient summaries for MTBs. Evaluating predicted patient summaries against ground truth presents additional challenges due to stylistic variation, ordering, synonym usage, and phrasing differences, which complicate the measurement of both succinctness and completeness. To overcome these evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework designed to assess the comprehensiveness and succinctness of generated summaries. Using a benchmark dataset derived from de-identified tumor board discussions, we applied TBFact to evaluate our Patient History agent. Results show that the agent captured 94% of high-importance information (including partial entailments) and achieved a TBFact recall of 0.84 under strict entailment criteria. We further demonstrate that TBFact enables a data-free evaluation framework that institutions can deploy locally without sharing sensitive clinical data. Together, HAO and TBFact establish a robust foundation for delivering reliable and scalable support to MTBs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEAM: Brainwave Empathy Assessment Model for Early Childhood</title>
<link>https://arxiv.org/abs/2509.06620</link>
<guid>https://arxiv.org/abs/2509.06620</guid>
<content:encoded><![CDATA[
arXiv:2509.06620v1 Announce Type: cross 
Abstract: Empathy in young children is crucial for their social and emotional development, yet predicting it remains challenging. Traditional methods often only rely on self-reports or observer-based labeling, which are susceptible to bias and fail to objectively capture the process of empathy formation. EEG offers an objective alternative; however, current approaches primarily extract static patterns, neglecting temporal dynamics. To overcome these limitations, we propose a novel deep learning framework, the Brainwave Empathy Assessment Model (BEAM), to predict empathy levels in children aged 4-6 years. BEAM leverages multi-view EEG signals to capture both cognitive and emotional dimensions of empathy. The framework comprises three key components: 1) a LaBraM-based encoder for effective spatio-temporal feature extraction, 2) a feature fusion module to integrate complementary information from multi-view signals, and 3) a contrastive learning module to enhance class separation. Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods across multiple metrics, demonstrating its potential for objective empathy assessment and providing a preliminary insight into early interventions in children's prosocial development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework</title>
<link>https://arxiv.org/abs/2509.06625</link>
<guid>https://arxiv.org/abs/2509.06625</guid>
<content:encoded><![CDATA[
arXiv:2509.06625v1 Announce Type: cross 
Abstract: Plants in their natural habitats endure an array of interacting stresses, both biotic and abiotic, that rarely occur in isolation. Nutrient stress-particularly nitrogen deficiency-becomes even more critical when compounded with drought and weed competition, making it increasingly difficult to distinguish and address its effects. Early detection of nitrogen stress is therefore crucial for protecting plant health and implementing effective management strategies. This study proposes a novel deep learning framework to accurately classify nitrogen stress severity in a combined stress environment. Our model uses a unique blend of four imaging modalities-RGB, multispectral, and two infrared wavelengths-to capture a wide range of physiological plant responses from canopy images. These images, provided as time-series data, document plant health across three levels of nitrogen availability (low, medium, and high) under varying water stress and weed pressures. The core of our approach is a spatio-temporal deep learning pipeline that merges a Convolutional Neural Network (CNN) for extracting spatial features from images with a Long Short-Term Memory (LSTM) network to capture temporal dependencies. We also devised and evaluated a spatial-only CNN pipeline for comparison. Our CNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively surpassing the spatial-only model's 80.45% and other previously reported machine learning method's 76%. These results bring actionable insights based on the power of our CNN-LSTM approach in effectively capturing the subtle and complex interactions between nitrogen deficiency, water stress, and weed pressure. This robust platform offers a promising tool for the timely and proactive identification of nitrogen stress severity, enabling better crop management and improved plant health.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The First Voice Timbre Attribute Detection Challenge</title>
<link>https://arxiv.org/abs/2509.06635</link>
<guid>https://arxiv.org/abs/2509.06635</guid>
<content:encoded><![CDATA[
arXiv:2509.06635v1 Announce Type: cross 
Abstract: The first voice timbre attribute detection challenge is featured in a special session at NCMMSC 2025. It focuses on the explainability of voice timbre and compares the intensity of two speech utterances in a specified timbre descriptor dimension. The evaluation was conducted on the VCTK-RVA dataset. Participants developed their systems and submitted their outputs to the organizer, who evaluated the performance and sent feedback to them. Six teams submitted their outputs, with five providing descriptions of their methodologies.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnalysisGNN: Unified Music Analysis with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.06654</link>
<guid>https://arxiv.org/abs/2509.06654</guid>
<content:encoded><![CDATA[
arXiv:2509.06654v1 Announce Type: cross 
Abstract: Recent years have seen a boom in computational approaches to music analysis, yet each one is typically tailored to a specific analytical domain. In this work, we introduce AnalysisGNN, a novel graph neural network framework that leverages a data-shuffling strategy with a custom weighted multi-task loss and logit fusion between task-specific classifiers to integrate heterogeneously annotated symbolic datasets for comprehensive score analysis. We further integrate a Non-Chord-Tone prediction module, which identifies and excludes passing and non-functional notes from all tasks, thereby improving the consistency of label signals. Experimental evaluations demonstrate that AnalysisGNN achieves performance comparable to traditional static-dataset approaches, while showing increased resilience to domain shifts and annotation inconsistencies across multiple heterogeneous corpora.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations</title>
<link>https://arxiv.org/abs/2509.06665</link>
<guid>https://arxiv.org/abs/2509.06665</guid>
<content:encoded><![CDATA[
arXiv:2509.06665v1 Announce Type: cross 
Abstract: Vehicular ad hoc networks (VANETs) are a crucial component of intelligent transportation systems; however, routing remains challenging due to dynamic topologies, incomplete observations, and the limited resources of edge devices. Existing reinforcement learning (RL) approaches often assume fixed graph structures and require retraining when network conditions change, making them unsuitable for deployment on constrained hardware. We present TrajAware, an RL-based framework designed for edge AI deployment in VANETs. TrajAware integrates three components: (i) action space pruning, which reduces redundant neighbour options while preserving two-hop reachability, alleviating the curse of dimensionality; (ii) graph cross-attention, which maps pruned neighbours to the global graph context, producing features that generalise across diverse network sizes; and (iii) trajectory-aware prediction, which uses historical routes and junction information to estimate real-time positions under partial observations. We evaluate TrajAware in the open-source SUMO simulator using real-world city maps with a leave-one-city-out setup. Results show that TrajAware achieves near-shortest paths and high delivery ratios while maintaining efficiency suitable for constrained edge devices, outperforming state-of-the-art baselines in both full and partial observation scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring</title>
<link>https://arxiv.org/abs/2509.06690</link>
<guid>https://arxiv.org/abs/2509.06690</guid>
<content:encoded><![CDATA[
arXiv:2509.06690v1 Announce Type: cross 
Abstract: Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation</title>
<link>https://arxiv.org/abs/2509.06694</link>
<guid>https://arxiv.org/abs/2509.06694</guid>
<content:encoded><![CDATA[
arXiv:2509.06694v1 Announce Type: cross 
Abstract: While it is well-established that artificial neural networks are \emph{universal approximators} for continuous functions on compact domains, many modern approaches rely on deep or overparameterized architectures that incur high computational costs. In this paper, a new type of \emph{small shallow} neural network, called the \emph{Barycentric Neural Network} ($\BNN$), is proposed, which leverages a fixed set of \emph{base points} and their \emph{barycentric coordinates} to define both its structure and its parameters. We demonstrate that our $\BNN$ enables the exact representation of \emph{continuous piecewise linear functions} ($\CPLF$s), ensuring strict continuity across segments. Since any continuous function over a compact domain can be approximated arbitrarily well by $\CPLF$s, the $\BNN$ naturally emerges as a flexible and interpretable tool for \emph{function approximation}. Beyond the use of this representation, the main contribution of the paper is the introduction of a new variant of \emph{persistent entropy}, a topological feature that is stable and scale invariant, called the \emph{length-weighted persistent entropy} ($\LWPE$), which is weighted by the lifetime of topological features. Our framework, which combines the $\BNN$ with a loss function based on our $\LWPE$, aims to provide flexible and geometrically interpretable approximations of nonlinear continuous functions in resource-constrained settings, such as those with limited base points for $\BNN$ design and few training epochs. Instead of optimizing internal weights, our approach directly \emph{optimizes the base points that define the $\BNN$}. Experimental results show that our approach achieves \emph{superior and faster approximation performance} compared to classical loss functions such as MSE, RMSE, MAE, and log-cosh.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2509.06701</link>
<guid>https://arxiv.org/abs/2509.06701</guid>
<content:encoded><![CDATA[
arXiv:2509.06701v1 Announce Type: cross 
Abstract: We develop a theory of intelligent agency grounded in probabilistic modeling for neural models. Agents are represented as outcome distributions with epistemic utility given by log score, and compositions are defined through weighted logarithmic pooling that strictly improves every member's welfare. We prove that strict unanimity is impossible under linear pooling or in binary outcome spaces, but possible with three or more outcomes. Our framework admits recursive structure via cloning invariance, continuity, and openness, while tilt-based analysis rules out trivial duplication. Finally, we formalize an agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent persona ("Luigi'") induces an antagonistic counterpart ("Waluigi"), while a manifest-then-suppress Waluigi strategy yields strictly larger first-order misalignment reduction than pure Luigi reinforcement alone. These results clarify how developing a principled mathematical framework for how subagents can coalesce into coherent higher-level entities provides novel implications for alignment in agentic AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture</title>
<link>https://arxiv.org/abs/2509.06713</link>
<guid>https://arxiv.org/abs/2509.06713</guid>
<content:encoded><![CDATA[
arXiv:2509.06713v1 Announce Type: cross 
Abstract: Brain tumors are serious health problems that require early diagnosis due to their high mortality rates. Diagnosing tumors by examining Magnetic Resonance Imaging (MRI) images is a process that requires expertise and is prone to error. Therefore, the need for automated diagnosis systems is increasing day by day. In this context, a robust and explainable Deep Learning (DL) model for the classification of brain tumors is proposed. In this study, a publicly available Figshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI images of three tumor types was used. First, the classification performance of nine well-known CNN architectures was evaluated to determine the most effective backbone. Among these, EfficientNetV2 demonstrated the best performance and was selected as the backbone for further development. Subsequently, an attention-based MLP-Mixer architecture was integrated into EfficientNetV2 to enhance its classification capability. The performance of the final model was comprehensively compared with basic CNNs and the methods in the literature. Additionally, Grad-CAM visualization was used to interpret and validate the decision-making process of the proposed model. The proposed model's performance was evaluated using the five-fold cross-validation method. The proposed model demonstrated superior performance with 99.50% accuracy, 99.47% precision, 99.52% recall and 99.49% F1 score. The results obtained show that the model outperforms the studies in the literature. Moreover, Grad-CAM visualizations demonstrate that the model effectively focuses on relevant regions of MRI images, thus improving interpretability and clinical reliability. A robust deep learning model for clinical decision support systems has been obtained by combining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy and interpretability in brain tumor classification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Range Graph Wavelet Networks</title>
<link>https://arxiv.org/abs/2509.06743</link>
<guid>https://arxiv.org/abs/2509.06743</guid>
<content:encoded><![CDATA[
arXiv:2509.06743v1 Announce Type: cross 
Abstract: Modeling long-range interactions, the propagation of information across distant parts of a graph, is a central challenge in graph machine learning. Graph wavelets, inspired by multi-resolution signal processing, provide a principled way to capture both local and global structures. However, existing wavelet-based graph neural networks rely on finite-order polynomial approximations, which limit their receptive fields and hinder long-range propagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which decompose wavelet filters into complementary local and global components. Local aggregation is handled with efficient low-order polynomials, while long-range interactions are captured through a flexible spectral domain parameterization. This hybrid design unifies short- and long-distance information flow within a principled wavelet framework. Experiments show that LR-GWN achieves state-of-the-art performance among wavelet-based methods on long-range benchmarks, while remaining competitive on short-range datasets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2509.06759</link>
<guid>https://arxiv.org/abs/2509.06759</guid>
<content:encoded><![CDATA[
arXiv:2509.06759v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) or multimodal large language models represent a significant advancement in artificial intelligence, enabling systems to understand and generate content across both visual and textual modalities. While large-scale pretraining has driven substantial progress, fine-tuning these models for aligning with human values or engaging in specific tasks or behaviors remains a critical challenge. Deep Reinforcement Learning (DRL) and Direct Preference Optimization (DPO) offer promising frameworks for this aligning process. While DRL enables models to optimize actions using reward signals instead of relying solely on supervised preference data, DPO directly aligns the policy with preferences, eliminating the need for an explicit reward model. This overview explores paradigms for fine-tuning LVLMs, highlighting how DRL and DPO techniques can be used to align models with human preferences and values, improve task performance, and enable adaptive multimodal interaction. We categorize key approaches, examine sources of preference data, reward signals, and discuss open challenges such as scalability, sample efficiency, continual learning, generalization, and safety. The goal is to provide a clear understanding of how DRL and DPO contribute to the evolution of robust and human-aligned LVLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML</title>
<link>https://arxiv.org/abs/2509.06806</link>
<guid>https://arxiv.org/abs/2509.06806</guid>
<content:encoded><![CDATA[
arXiv:2509.06806v1 Announce Type: cross 
Abstract: Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.
  Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.
  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem</title>
<link>https://arxiv.org/abs/2509.06809</link>
<guid>https://arxiv.org/abs/2509.06809</guid>
<content:encoded><![CDATA[
arXiv:2509.06809v1 Announce Type: cross 
Abstract: The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover's saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available.
  https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens</title>
<link>https://arxiv.org/abs/2509.06836</link>
<guid>https://arxiv.org/abs/2509.06836</guid>
<content:encoded><![CDATA[
arXiv:2509.06836v1 Announce Type: cross 
Abstract: Making LLMs more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale. Pruning is a key technique toward this goal. However, prior pruning methods are limited: width pruning often breaks the standard transformer layout or requires custom inference code, while depth pruning removes entire layers and can cause abrupt accuracy drops. In this work, we propose COMPACT, which jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii) prunes FFN intermediate channels using common-token-weighted activations, aligning importance with the post-pruning token distribution. COMPACT enjoys merits of both depth and width pruning, such as: deployment-friendliness (keeps a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN pruning), training-free operation with competitive pruning time, and strong memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show state-of-the-art downstream task performance at similar or higher pruning ratios, with substantial reductions in parameters, GPU memory, and end-to-end latency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning meets bioprocess control through behaviour cloning: Real-world deployment in an industrial photobioreactor</title>
<link>https://arxiv.org/abs/2509.06853</link>
<guid>https://arxiv.org/abs/2509.06853</guid>
<content:encoded><![CDATA[
arXiv:2509.06853v1 Announce Type: cross 
Abstract: The inherent complexity of living cells as production units creates major challenges for maintaining stable and optimal bioprocess conditions, especially in open Photobioreactors (PBRs) exposed to fluctuating environments. To address this, we propose a Reinforcement Learning (RL) control approach, combined with Behavior Cloning (BC), for pH regulation in open PBR systems. This represents, to the best of our knowledge, the first application of an RL-based control strategy to such a nonlinear and disturbance-prone bioprocess. Our method begins with an offline training stage in which the RL agent learns from trajectories generated by a nominal Proportional-Integral-Derivative (PID) controller, without direct interaction with the real system. This is followed by a daily online fine-tuning phase, enabling adaptation to evolving process dynamics and stronger rejection of fast, transient disturbances. This hybrid offline-online strategy allows deployment of an adaptive control policy capable of handling the inherent nonlinearities and external perturbations in open PBRs. Simulation studies highlight the advantages of our method: the Integral of Absolute Error (IAE) was reduced by 8% compared to PID control and by 5% relative to standard off-policy RL. Moreover, control effort decreased substantially-by 54% compared to PID and 7% compared to standard RL-an important factor for minimizing operational costs. Finally, an 8-day experimental validation under varying environmental conditions confirmed the robustness and reliability of the proposed approach. Overall, this work demonstrates the potential of RL-based methods for bioprocess control and paves the way for their broader application to other nonlinear, disturbance-prone systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice</title>
<link>https://arxiv.org/abs/2509.06854</link>
<guid>https://arxiv.org/abs/2509.06854</guid>
<content:encoded><![CDATA[
arXiv:2509.06854v1 Announce Type: cross 
Abstract: Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van Der Heijde Score (TSS) is crucial, but manual scoring is often time-consuming and subjective. This study introduces an Automated Radiographic Sharp Scoring (ARTSS) framework that leverages deep learning to analyze full-hand X-ray images, aiming to reduce inter- and intra-observer variability. The research uniquely accommodates patients with joint disappearance and variable-length image sequences. We developed ARTSS using data from 970 patients, structured into four stages: I) Image pre-processing and re-orientation using ResNet50, II) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and IV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201, EfficientNetB0, and Vision Transformer (ViT). We evaluated model performance with Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute error (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS from two radiologists was used as the ground truth. Model training employed 3-fold cross-validation, with each fold consisting of 452 training and 227 validation samples, and external testing included 291 unseen subjects. Our joint identification model achieved 99% accuracy. The best-performing model, ViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results demonstrate the potential of deep learning to automate RA scoring, which can significantly enhance clinical practice. Our approach addresses the challenge of joint disappearance and variable joint numbers, offers timesaving benefits, reduces inter- and intra-reader variability, improves radiologist accuracy, and aids rheumatologists in making more informed decisions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Interaction and Bias Effects in Opinion Dynamics of Large Language Models</title>
<link>https://arxiv.org/abs/2509.06858</link>
<guid>https://arxiv.org/abs/2509.06858</guid>
<content:encoded><![CDATA[
arXiv:2509.06858v1 Announce Type: cross 
Abstract: Large Language Models are increasingly used to simulate human opinion dynamics, yet the effect of genuine interaction is often obscured by systematic biases. We present a Bayesian framework to disentangle and quantify three such biases: (i) a topic bias toward prior opinions in the training data; (ii) an agreement bias favoring agreement irrespective of the question; and (iii) an anchoring bias toward the initiating agent's stance. Applying this framework to multi-step dialogues reveals that opinion trajectories tend to quickly converge to a shared attractor, with the influence of the interaction fading over time, and the impact of biases differing between LLMs. In addition, we fine-tune an LLM on different sets of strongly opinionated statements (incl. misinformation) and demonstrate that the opinion attractor shifts correspondingly. Exposing stark differences between LLMs and providing quantitative tools to compare them to human subjects in the future, our approach highlights both chances and pitfalls in using LLMs as proxies for human behavior.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL</title>
<link>https://arxiv.org/abs/2509.06863</link>
<guid>https://arxiv.org/abs/2509.06863</guid>
<content:encoded><![CDATA[
arXiv:2509.06863v1 Announce Type: cross 
Abstract: A hallmark of modern large-scale machine learning techniques is the use of training objectives that provide dense supervision to intermediate computations, such as teacher forcing the next token in language models or denoising step-by-step in diffusion models. This enables models to learn complex functions in a generalizable manner. Motivated by this observation, we investigate the benefits of iterative computation for temporal difference (TD) methods in reinforcement learning (RL). Typically they represent value functions in a monolithic fashion, without iterative compute. We introduce floq (flow-matching Q-functions), an approach that parameterizes the Q-function using a velocity field and trains it using techniques from flow-matching, typically used in generative modeling. This velocity field underneath the flow is trained using a TD-learning objective, which bootstraps from values produced by a target velocity field, computed by running multiple steps of numerical integration. Crucially, floq allows for more fine-grained control and scaling of the Q-function capacity than monolithic architectures, by appropriately setting the number of integration steps. Across a suite of challenging offline RL benchmarks and online fine-tuning tasks, floq improves performance by nearly 1.8x. floq scales capacity far better than standard TD-learning architectures, highlighting the potential of iterative computation for value learning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification</title>
<link>https://arxiv.org/abs/2509.06875</link>
<guid>https://arxiv.org/abs/2509.06875</guid>
<content:encoded><![CDATA[
arXiv:2509.06875v1 Announce Type: cross 
Abstract: Class imbalance in machine learning poses a significant challenge, as skewed datasets often hinder performance on minority classes. Traditional oversampling techniques, which are commonly used to alleviate class imbalance, have several drawbacks: they treat features independently, lack similarity-based controls, limit sample diversity, and fail to manage synthetic variety effectively. To overcome these issues, we introduce AxelSMOTE, an innovative agent-based approach that views data instances as autonomous agents engaging in complex interactions. Based on Axelrod's cultural dissemination model, AxelSMOTE implements four key innovations: (1) trait-based feature grouping to preserve correlations; (2) a similarity-based probabilistic exchange mechanism for meaningful interactions; (3) Beta distribution blending for realistic interpolation; and (4) controlled diversity injection to avoid overfitting. Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms state-of-the-art sampling methods while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction</title>
<link>https://arxiv.org/abs/2509.06883</link>
<guid>https://arxiv.org/abs/2509.06883</guid>
<content:encoded><![CDATA[
arXiv:2509.06883v1 Announce Type: cross 
Abstract: We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers</title>
<link>https://arxiv.org/abs/2509.06885</link>
<guid>https://arxiv.org/abs/2509.06885</guid>
<content:encoded><![CDATA[
arXiv:2509.06885v1 Announce Type: cross 
Abstract: Medical image segmentation is a critical task in clinical workflows, particularly for the detection and delineation of pathological regions. While convolutional architectures like U-Net have become standard for such tasks, their limited receptive field restricts global context modeling. Recent efforts integrating transformers have addressed this, but often result in deep, computationally expensive models unsuitable for real-time use. In this work, we present a novel end-to-end lightweight architecture designed specifically for real-time binary medical image segmentation. Our model combines a Swin Transformer-like encoder with a U-Net-like decoder, connected via skip pathways to preserve spatial detail while capturing contextual information. Unlike existing designs such as Swin Transformer or U-Net, our architecture is significantly shallower and competitively efficient. To improve the encoder's ability to learn meaningful features without relying on large amounts of labeled data, we first train it using Barlow Twins, a self-supervised learning method that helps the model focus on important patterns by reducing unnecessary repetition in the learned features. After this pretraining, we fine-tune the entire model for our specific task. Experiments on benchmark binary segmentation tasks demonstrate that our model achieves competitive accuracy with substantially reduced parameter count and faster inference, positioning it as a practical alternative for deployment in real-time and resource-limited clinical environments. The code for our method is available at Github repository: https://github.com/mkianih/Barlow-Swin.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition</title>
<link>https://arxiv.org/abs/2509.06918</link>
<guid>https://arxiv.org/abs/2509.06918</guid>
<content:encoded><![CDATA[
arXiv:2509.06918v1 Announce Type: cross 
Abstract: Robust out-of-distribution (OOD) detection is an indispensable component of modern artificial intelligence (AI) systems, especially in safety-critical applications where models must identify inputs from unfamiliar classes not seen during training. While OOD detection has been extensively studied in the machine learning literature--with both post hoc and training-based approaches--its effectiveness under noisy training labels remains underexplored. Recent studies suggest that label noise can significantly degrade OOD performance, yet principled solutions to this issue are lacking. In this work, we demonstrate that directly combining existing label noise-robust methods with OOD detection strategies is insufficient to address this critical challenge. To overcome this, we propose a robust OOD detection framework that integrates loss correction techniques from the noisy label learning literature with low-rank and sparse decomposition methods from signal processing. Extensive experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms the state-of-the-art OOD detection techniques, particularly under severe noisy label settings.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection</title>
<link>https://arxiv.org/abs/2509.06920</link>
<guid>https://arxiv.org/abs/2509.06920</guid>
<content:encoded><![CDATA[
arXiv:2509.06920v1 Announce Type: cross 
Abstract: Insider threats are a growing organizational problem due to the complexity of identifying their technical and behavioral elements. A large research body is dedicated to the study of insider threats from technological, psychological, and educational perspectives. However, research in this domain has been generally dependent on datasets that are static and limited access which restricts the development of adaptive detection models. This study introduces a novel, ethically grounded approach that uses the large language model (LLM) Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which contain indicators of insider threat scenarios. The messages reflect real-world data distributions by being highly imbalanced (1% insider threats). The syslogs were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with their performance evaluated through statistical metrics including precision, recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across nearly all metrics, particularly in reducing false alarms and improving detection accuracy. The results show strong promise for the use of LLMs in synthetic dataset generation and insider threat detection.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2509.06921</link>
<guid>https://arxiv.org/abs/2509.06921</guid>
<content:encoded><![CDATA[
arXiv:2509.06921v1 Announce Type: cross 
Abstract: Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit fundamental limitations: inadequate conceptual grounding leading to non-robustness against novel attacks; limited instructibility impeding analyst-guided adaptation; and misalignment with cybersecurity objectives. Neuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize cybersecurity AI. However, there is no systematic understanding of this emerging approach. These hybrid systems address critical cybersecurity challenges by combining neural pattern recognition with symbolic reasoning, enabling enhanced threat understanding while introducing concerning autonomous offensive capabilities that reshape threat landscapes. In this survey, we systematically characterize this field by analyzing 127 publications spanning 2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A) framework to evaluate these systems, focusing on both cyber defense and cyber offense across network security, malware analysis, and cyber operations. Our analysis shows advantages of multi-agent NeSy architectures and identifies critical implementation challenges including standardization gaps, computational complexity, and human-AI collaboration requirements that constrain deployment. We show that causal reasoning integration is the most transformative advancement, enabling proactive defense beyond correlation-based approaches. Our findings highlight dual-use implications where autonomous systems demonstrate substantial capabilities in zero-day exploitation while achieving significant cost reductions, altering threat dynamics. We provide insights and future research directions, emphasizing the urgent need for community-driven standardization frameworks and responsible development practices that ensure advancement serves defensive cybersecurity objectives while maintaining societal alignment.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers</title>
<link>https://arxiv.org/abs/2509.06938</link>
<guid>https://arxiv.org/abs/2509.06938</guid>
<content:encoded><![CDATA[
arXiv:2509.06938v1 Announce Type: cross 
Abstract: As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interleaving Reasoning for Better Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2509.06945</link>
<guid>https://arxiv.org/abs/2509.06945</guid>
<content:encoded><![CDATA[
arXiv:2509.06945v1 Announce Type: cross 
Abstract: Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</title>
<link>https://arxiv.org/abs/2509.06953</link>
<guid>https://arxiv.org/abs/2509.06953</guid>
<content:encoded><![CDATA[
arXiv:2509.06953v1 Announce Type: cross 
Abstract: Generating collision-free motion in dynamic, partially observable environments is a fundamental challenge for robotic manipulators. Classical motion planners can compute globally optimal trajectories but require full environment knowledge and are typically too slow for dynamic scenes. Neural motion policies offer a promising alternative by operating in closed-loop directly on raw sensory inputs but often struggle to generalize in complex or dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed for reactive motion generation in diverse dynamic environments, operating directly on point cloud sensory input. At its core is IMPACT, a transformer-based neural motion policy pretrained on 10 million generated expert trajectories across diverse simulation scenarios. We further improve IMPACT's static obstacle avoidance through iterative student-teacher finetuning. We additionally enhance the policy's dynamic obstacle avoidance at inference time using DCP-RMP, a locally reactive goal-proposal module. We evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving obstacles, and goal obstructions. DRP achieves strong generalization, outperforming prior classical and neural methods in success rate across both simulated and real-world settings. Video results and code available at https://deep-reactive-policy.com
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</title>
<link>https://arxiv.org/abs/2509.06956</link>
<guid>https://arxiv.org/abs/2509.06956</guid>
<content:encoded><![CDATA[
arXiv:2509.06956v1 Announce Type: cross 
Abstract: Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at https://github.com/NationalGAILab/HoT.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorical semantics of compositional reinforcement learning</title>
<link>https://arxiv.org/abs/2208.13687</link>
<guid>https://arxiv.org/abs/2208.13687</guid>
<content:encoded><![CDATA[
arXiv:2208.13687v3 Announce Type: replace 
Abstract: Compositional knowledge representations in reinforcement learning (RL) facilitate modular, interpretable, and safe task specifications. However, generating compositional models requires the characterization of minimal assumptions for the robustness of the compositionality feature, especially in the case of functional decompositions. Using a categorical point of view, we develop a knowledge representation framework for a compositional theory of RL. Our approach relies on the theoretical study of the category MDP, whose objects are Markov decision processes (MDPs) acting as models of tasks. The categorical semantics models the compositionality of tasks through the application of pushout operations akin to combining puzzle pieces. As a practical application of these pushout operations, we introduce zig-zag diagrams that rely on the compositional guarantees engendered by the category MDP. We further prove that properties of the category MDP unify concepts, such as enforcing safety requirements and exploiting symmetries, generalizing previous abstraction theories for RL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoisyNN: Exploring the Impact of Information Entropy Change in Learning Systems</title>
<link>https://arxiv.org/abs/2309.10625</link>
<guid>https://arxiv.org/abs/2309.10625</guid>
<content:encoded><![CDATA[
arXiv:2309.10625v5 Announce Type: replace 
Abstract: We investigate the impact of entropy change in deep learning systems by noise injection at different levels, including the embedding space and the image. The series of models that employ our methodology are collectively known as Noisy Neural Networks (NoisyNN), with examples such as NoisyViT and NoisyCNN. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this work shows noise can be an effective way to change the entropy of the learning system. We demonstrate that specific noise can boost the performance of various deep models under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the information entropy to define the complexity of the task. We categorize the noise into two types, positive noise (PN) and harmful noise (HN), based on whether the noise can help reduce the task complexity. Extensive experiments of CNNs and ViTs have shown performance improvements by proactively injecting positive noise, where we achieved an unprecedented top 1 accuracy of 95$\%$ on ImageNet. Both theoretical analysis and empirical evidence have confirmed that the presence of positive noise, can benefit the learning process, while the traditionally perceived harmful noise indeed impairs deep learning models. The different roles of noise offer new explanations for deep models on specific tasks and provide a new paradigm for improving model performance. Moreover, it reminds us that we can influence the performance of learning systems via information entropy change.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GameGPT: Multi-agent Collaborative Framework for Game Development</title>
<link>https://arxiv.org/abs/2310.08067</link>
<guid>https://arxiv.org/abs/2310.08067</guid>
<content:encoded><![CDATA[
arXiv:2310.08067v2 Announce Type: replace 
Abstract: The large language model (LLM) based agents have demonstrated their capacity to automate and expedite software development processes. In this paper, we focus on game development and propose a multi-agent collaborative framework, dubbed GameGPT, to automate game development. While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern: redundancy. Our framework presents a series of methods to mitigate both concerns. These methods include dual collaboration and layered approaches with several in-house lexicons, to mitigate the hallucination and redundancy in the planning, task identification, and implementation phases. Furthermore, a decoupling approach is also introduced to achieve code generation with better precision.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Prompt Pricing based on Combinatorial Multi-Armed Bandit and Hierarchical Stackelberg Game</title>
<link>https://arxiv.org/abs/2405.15154</link>
<guid>https://arxiv.org/abs/2405.15154</guid>
<content:encoded><![CDATA[
arXiv:2405.15154v3 Announce Type: replace 
Abstract: Generation models have shown promising performance in various tasks, making trading around machine learning models possible. In this paper, we aim at a novel prompt trading scenario, prompt bundle trading (PBT) system, and propose an online pricing mechanism. Based on the combinatorial multi-armed bandit (CMAB) and three-stage hierarchical Stackelburg (HS) game, our pricing mechanism considers the profits of the consumer, platform, and seller, simultaneously achieving the profit satisfaction of these three participants. We break down the pricing issue into two steps, namely unknown category selection and incentive strategy optimization. The former step is to select a set of categories with the highest qualities, and the latter is to derive the optimal strategy for each participant based on the chosen categories. Unlike the existing fixed pricing mode, the PBT pricing mechanism we propose is more flexible and diverse, which is more in accord with the transaction needs of real-world scenarios. We test our method on a simulated text-to-image dataset. The experimental results demonstrate the effectiveness of our algorithm, which provides a feasible price-setting standard for the prompt marketplaces.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Wearable Data into Personal Health Insights using Large Language Model Agents</title>
<link>https://arxiv.org/abs/2406.06464</link>
<guid>https://arxiv.org/abs/2406.06464</guid>
<content:encoded><![CDATA[
arXiv:2406.06464v4 Announce Type: replace 
Abstract: Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents</title>
<link>https://arxiv.org/abs/2406.10291</link>
<guid>https://arxiv.org/abs/2406.10291</guid>
<content:encoded><![CDATA[
arXiv:2406.10291v3 Announce Type: replace 
Abstract: Large language models (LLMs) excel across many natural language processing tasks but face challenges in domain-specific, analytical tasks such as conducting research surveys. This study introduces ResearchArena, a benchmark designed to evaluate LLMs' capabilities in conducting academic surveys -- a foundational step in academic research. ResearchArena models the process in three stages: (1) information discovery, identifying relevant literature; (2) information selection, evaluating papers' relevance and impact; and (3) information organization, structuring knowledge into hierarchical frameworks such as mind-maps. Notably, mind-map construction is treated as a bonus task, reflecting its supplementary role in survey-writing. To support these evaluations, we construct an offline environment of 12M full-text academic papers and 7.9K survey papers. To ensure ethical compliance, we do not redistribute copyrighted materials; instead, we provide code to construct the environment from the Semantic Scholar Open Research Corpus (S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform compared to simpler keyword-based retrieval methods, though recent reasoning models such as DeepSeek-R1 show slightly better zero-shot performance. These results underscore significant opportunities for advancing LLMs in autonomous research. We open-source the code to construct the ResearchArena benchmark at https://github.com/cxcscmu/ResearchArena.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Labyrinth: Evaluating LLMs' Ability to Reason About Search Problems</title>
<link>https://arxiv.org/abs/2406.12172</link>
<guid>https://arxiv.org/abs/2406.12172</guid>
<content:encoded><![CDATA[
arXiv:2406.12172v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have recently achieved impressive performance in math and reasoning benchmarks. However, they often struggle with logic problems and puzzles that are relatively easy for humans. To further investigate this, we introduce a new benchmark, SearchBench, which contains 11 unique search problems, each equipped with automated pipelines to generate an arbitrary number of instances and analyze the feasibility, correctness, and optimality of LLM-generated solutions. We show that using language-only reasoning, even the most advanced LLMs fail to solve SearchBench end-to-end, e.g., OpenAI's frontier models GPT4 and o1-preview solve only 1.4% and 18.6% of SearchBench problems, respectively. The reason is that SearchBench problems require considering multiple pathways to the solution and performing backtracking, posing a significant challenge to auto-regressive models. Instructing LLMs to generate code that solves the problem helps, but only slightly, e.g., GPT4's performance rises to 11.7%. Interestingly, we show that the current strongest baseline on SearchBench is obtained using in-context learning with A* algorithm implementations. We further show that this baseline can be further enhanced via a Multi-Stage-Multi-Try inference method, raising GPT4's performance above 57%.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese Travel Planning</title>
<link>https://arxiv.org/abs/2412.13682</link>
<guid>https://arxiv.org/abs/2412.13682</guid>
<content:encoded><![CDATA[
arXiv:2412.13682v4 Announce Type: replace 
Abstract: Recent advances in LLMs, particularly in language reasoning and tool integration, have rapidly sparked the \emph{Language Agents} for real-world development. Among these, travel planning represents a prominent domain, combining complex multi-objective planning challenges with practical deployment demands. However, existing benchmarks often oversimplify real-world requirements by focusing on synthetic queries and limited constraints. We address the gap of evaluating language agents in multi-day, multi-POI travel planning scenarios with diverse and open human needs. Specifically, we introduce \emph{ChinaTravel}, the first open-ended benchmark grounded in authentic Chinese travel requirements collected from 1,154 human participants. We design a compositionally generalizable domain-specific language (DSL) for scalable evaluation, covering feasibility, constraint satisfaction, and preference comparison. Empirical studies reveal the potential of neuro-symbolic agents in travel planning, achieving a 37.0\% constraint satisfaction rate on human queries, a 10\times improvement over purely neural models. These findings highlight ChinaTravel as a pivotal milestone for advancing language agents in complex, real-world planning scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2502.07350</link>
<guid>https://arxiv.org/abs/2502.07350</guid>
<content:encoded><![CDATA[
arXiv:2502.07350v2 Announce Type: replace 
Abstract: As scaling large language models faces prohibitive costs, multi-agent systems emerge as a promising alternative, though challenged by static knowledge assumptions and coordination inefficiencies. We introduces Knowledge-Aware Bayesian Bandits (KABB), a novel framework that enhances multi-agent system coordination through semantic understanding and dynamic adaptation. The framework features three key innovations: a three-dimensional knowledge distance model for deep semantic understanding, a dual-adaptation mechanism for continuous expert optimization, and a knowledge-aware Thompson Sampling strategy for efficient expert selection. Extensive evaluation demonstrates KABB achieves an optimal cost-performance balance, maintaining high performance while keeping computational demands relatively low in multi-agent coordination.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Antidistillation Sampling</title>
<link>https://arxiv.org/abs/2504.13146</link>
<guid>https://arxiv.org/abs/2504.13146</guid>
<content:encoded><![CDATA[
arXiv:2504.13146v4 Announce Type: replace 
Abstract: Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation</title>
<link>https://arxiv.org/abs/2504.13707</link>
<guid>https://arxiv.org/abs/2504.13707</guid>
<content:encoded><![CDATA[
arXiv:2504.13707v2 Announce Type: replace 
Abstract: As the general capabilities of large language models (LLMs) improve and agent applications become more widespread, the underlying deception risks urgently require systematic evaluation and effective oversight. Unlike existing evaluation which uses simulated games or presents limited choices, we introduce OpenDeception, a novel deception evaluation framework with an open-ended scenario dataset. OpenDeception jointly evaluates both the deception intention and capabilities of LLM-based agents by inspecting their internal reasoning process. Specifically, we construct five types of common use cases where LLMs intensively interact with the user, each consisting of ten diverse, concrete scenarios from the real world. To avoid ethical concerns and costs of high-risk deceptive interactions with human testers, we propose to simulate the multi-turn dialogue via agent simulation. Extensive evaluation of eleven mainstream LLMs on OpenDeception highlights the urgent need to address deception risks and security concerns in LLM-based agents: the deception intention ratio across the models exceeds 80%, while the deception success rate surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do exhibit a higher risk of deception, which calls for more alignment efforts on inhibiting deceptive behaviors.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration</title>
<link>https://arxiv.org/abs/2505.11191</link>
<guid>https://arxiv.org/abs/2505.11191</guid>
<content:encoded><![CDATA[
arXiv:2505.11191v2 Announce Type: replace 
Abstract: As embodied AI systems become increasingly multi-modal, personalized, and interactive, they must learn effectively from diverse sensory inputs, adapt continually to user preferences, and operate safely under resource and privacy constraints. These challenges expose a pressing need for machine learning models capable of swift, context-aware adaptation while balancing model generalization and personalization. Here, two methods emerge as suitable candidates, each offering parts of these capabilities: multi-modal multi-task foundation models (M3T-FMs) provide a pathway toward generalization across tasks and modalities, whereas federated learning (FL) offers the infrastructure for distributed, privacy-preserving model updates and user-level model personalization. However, when used in isolation, each of these approaches falls short of meeting the complex and diverse capability requirements of real-world embodied AI environments. In this vision paper, we introduce multi-modal multi-task federated foundation models (M3T-FFMs) for embodied AI, a new paradigm that unifies the strengths of M3T-FMs with the privacy-preserving distributed training nature of FL, enabling intelligent systems at the wireless edge. We collect critical deployment dimensions of M3T-FFMs in embodied AI ecosystems under a unified framework, which we name "EMBODY": Embodiment heterogeneity, Modality richness and imbalance, Bandwidth and compute constraints, On-device continual learning, Distributed control and autonomy, and Yielding safety, privacy, and personalization. For each, we identify concrete challenges and envision actionable research directions. We also present an evaluation framework for deploying M3T-FFMs in embodied AI systems, along with the associated trade-offs. Finally, we present a prototype implementation of M3T-FFMs and evaluate their energy and latency performance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting</title>
<link>https://arxiv.org/abs/2505.20521</link>
<guid>https://arxiv.org/abs/2505.20521</guid>
<content:encoded><![CDATA[
arXiv:2505.20521v2 Announce Type: replace 
Abstract: This paper presents Project Riley, a novel multimodal and multi-model conversational AI architecture oriented towards the simulation of reasoning influenced by emotional states. Drawing inspiration from Pixar's Inside Out, the system comprises five distinct emotional agents - Joy, Sadness, Fear, Anger, and Disgust - that engage in structured multi-round dialogues to generate, criticise, and iteratively refine responses. A final reasoning mechanism synthesises the contributions of these agents into a coherent output that either reflects the dominant emotion or integrates multiple perspectives. The architecture incorporates both textual and visual large language models (LLMs), alongside advanced reasoning and self-refinement processes. A functional prototype was deployed locally in an offline environment, optimised for emotional expressiveness and computational efficiency. From this initial prototype, another one emerged, called Armando, which was developed for use in emergency contexts, delivering emotionally calibrated and factually accurate information through the integration of Retrieval-Augmented Generation (RAG) and cumulative context tracking. The Project Riley prototype was evaluated through user testing, in which participants interacted with the chatbot and completed a structured questionnaire assessing three dimensions: Emotional Appropriateness, Clarity and Utility, and Naturalness and Human-likeness. The results indicate strong performance in structured scenarios, particularly with respect to emotional alignment and communicative clarity.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUDER: Self-Improving Unified Large Multimodal Models for Understanding and Generation with Dual Self-Rewards</title>
<link>https://arxiv.org/abs/2506.07963</link>
<guid>https://arxiv.org/abs/2506.07963</guid>
<content:encoded><![CDATA[
arXiv:2506.07963v3 Announce Type: replace 
Abstract: Building upon large language models (LLMs), recent large multimodal models (LMMs) unify cross-model understanding and generation into a single framework. However, LMMs still struggle to achieve accurate vision-language alignment, prone to generating text responses contradicting the visual input or failing to follow the text-to-image prompts. Current solutions require external supervision (e.g., human feedback or reward models) and only address unidirectional tasks-either understanding or generation. In this work, based on the observation that understanding and generation are naturally inverse dual tasks, we propose \textbf{SUDER} (\textbf{S}elf-improving \textbf{U}nified LMMs with \textbf{D}ual s\textbf{E}lf-\textbf{R}ewards), a framework reinforcing the understanding and generation capabilities of LMMs with a self-supervised dual reward mechanism. SUDER leverages the inherent duality between understanding and generation tasks to provide self-supervised optimization signals for each other. Specifically, we sample multiple outputs for a given input in one task domain, then reverse the input-output pairs to compute the dual likelihood within the model as self-rewards for optimization. Extensive experimental results on visual understanding and generation benchmarks demonstrate that our method can effectively enhance the performance of the model without any external supervision, especially achieving remarkable improvements in text-to-image tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness</title>
<link>https://arxiv.org/abs/2506.08532</link>
<guid>https://arxiv.org/abs/2506.08532</guid>
<content:encoded><![CDATA[
arXiv:2506.08532v2 Announce Type: replace 
Abstract: The rapid growth of the low-altitude economy has driven the widespread adoption of unmanned aerial vehicles (UAVs). This growing deployment presents new challenges for UAV trajectory planning in complex urban environments. However, existing studies often overlook key factors, such as urban airspace constraints and economic efficiency, which are essential in low-altitude economy contexts. Deep reinforcement learning (DRL) is regarded as a promising solution to these issues, while its practical adoption remains limited by low learning efficiency. To overcome this limitation, we propose a novel UAV trajectory planning framework that combines DRL with large language model (LLM) reasoning to enable safe, compliant, and economically viable path planning. Experimental results demonstrate that our method significantly outperforms existing baselines across multiple metrics, including data collection rate, collision avoidance, successful landing, regulatory compliance, and energy efficiency. These results validate the effectiveness of our approach in addressing UAV trajectory planning key challenges under constraints of the low-altitude economy networking.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Recurrence in Contextual MDPs for Universal Morphology Control</title>
<link>https://arxiv.org/abs/2506.08630</link>
<guid>https://arxiv.org/abs/2506.08630</guid>
<content:encoded><![CDATA[
arXiv:2506.08630v2 Announce Type: replace 
Abstract: A universal controller for any robot morphology would greatly improve computational and data efficiency. By utilizing contextual information about the properties of individual robots and exploiting their modular structure in the architecture of deep reinforcement learning agents, steps have been made towards multi-robot control. Generalization to new, unseen robots, however, remains a challenge. In this paper we hypothesize that the relevant contextual information is partially observable, but that it can be inferred through interactions for better generalization to contexts that are not seen during training. To this extent, we implement a modular recurrent architecture and evaluate its generalization performance on a large set of MuJoCo robots. The results show a substantial improved performance on robots with unseen dynamics, kinematics, and topologies, in four different environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.14990</link>
<guid>https://arxiv.org/abs/2506.14990</guid>
<content:encoded><![CDATA[
arXiv:2506.14990v2 Announce Type: replace 
Abstract: Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms, with environment availability strongly impacting research. One particularly underexplored intersection is continual learning (CL) in cooperative multi-agent settings. To remedy this, we introduce MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark tailored for continual multi-agent reinforcement learning (CMARL). Existing CL benchmarks run environments on the CPU, leading to computational bottlenecks and limiting the length of task sequences. MEAL leverages JAX for GPU acceleration, enabling continual learning across sequences of 100 tasks on a standard desktop PC in a few hours. We show that naively combining popular CL and MARL methods yields strong performance on simple environments, but fails to scale to more complex settings requiring sustained coordination and adaptation. Our ablation study identifies architectural and algorithmic features critical for CMARL on MEAL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis</title>
<link>https://arxiv.org/abs/2506.23273</link>
<guid>https://arxiv.org/abs/2506.23273</guid>
<content:encoded><![CDATA[
arXiv:2506.23273v2 Announce Type: replace 
Abstract: Despite the advancements of large language models, text2sql still faces many challenges, particularly with complex and domain-specific queries. In finance, database designs and financial reporting layouts vary widely between financial entities and countries, making text2sql even more challenging. We present FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries over financial statements. Tailored to local standards like VAS, it combines large and small language models in a multi-agent setup for entity extraction, SQL generation, and self-correction. We build a domain-specific database and evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves 61.33\% accuracy with sub-4-second response times on consumer hardware, outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient solution for financial analysis, making AI-powered querying accessible to Vietnamese enterprises.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis</title>
<link>https://arxiv.org/abs/2507.03460</link>
<guid>https://arxiv.org/abs/2507.03460</guid>
<content:encoded><![CDATA[
arXiv:2507.03460v2 Announce Type: replace 
Abstract: Identifying associations between imaging phenotypes, disease risk factors, and clinical outcomes is essential for understanding disease mechanisms. However, traditional approaches rely on human-driven hypothesis testing and selection of association factors, often overlooking complex, non-linear dependencies among imaging phenotypes and other multi-modal data. To address this, we introduce Multi-agent Exploratory Synergy for the Heart (MESHAgents): a framework that leverages large language models as agents to dynamically elicit, surface, and decide confounders and phenotypes in association studies. Specifically, we orchestrate a multi-disciplinary team of AI agents, which spontaneously generate and converge on insights through iterative, self-organizing reasoning. The framework dynamically synthesizes statistical correlations with multi-expert consensus, providing an automated pipeline for phenome-wide association studies (PheWAS). We demonstrate the system's capabilities through a population-based study of imaging phenotypes of the heart and aorta. MESHAgents autonomously uncovered correlations between imaging phenotypes and a wide range of non-imaging factors, identifying additional confounder variables beyond standard demographic factors. Validation on diagnosis tasks reveals that MESHAgents-discovered phenotypes achieve performance comparable to expert-selected phenotypes, with mean AUC differences as small as $-0.004_{\pm0.010}$ on disease classification tasks. Notably, the recall score improves for 6 out of 9 disease types. Our framework provides clinically relevant imaging phenotypes with transparent reasoning, offering a scalable alternative to expert-driven methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation</title>
<link>https://arxiv.org/abs/2507.05541</link>
<guid>https://arxiv.org/abs/2507.05541</guid>
<content:encoded><![CDATA[
arXiv:2507.05541v2 Announce Type: replace 
Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine learning predictions by highlighting minimal changes required to alter an outcome. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. In this work, we explore large language models (LLMs), specifically GPT-4o-mini, for generating CFs in a zero-shot and three-shot setting. We evaluate our approach on two datasets: the AI-Readi flagship dataset for stress prediction and a public dataset for heart disease detection. Compared to traditional methods such as DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high plausibility (up to 99%), strong validity (up to 0.99), and competitive sparsity. Moreover, using LLM-generated CFs as augmented samples improves downstream classifier performance (an average accuracy gain of 5%), especially in low-data regimes. This demonstrates the potential of prompt-based generative techniques to enhance explainability and robustness in clinical and physiological prediction tasks. Code base: github.com/shovito66/SenseCF.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Urban Planing AI Agent in the Age of Agentic AI</title>
<link>https://arxiv.org/abs/2507.14730</link>
<guid>https://arxiv.org/abs/2507.14730</guid>
<content:encoded><![CDATA[
arXiv:2507.14730v3 Announce Type: replace 
Abstract: Generative AI, large language models, and agentic AI have emerged separately of urban planning. However, the convergence between AI and urban planning presents an interesting opportunity towards AI urban planners. Existing studies conceptualizes urban planning as a generative AI task, where AI synthesizes land-use configurations under geospatial, social, and human-centric constraints and reshape automated urban design. We further identify critical gaps of existing generative urban planning studies: 1) the generative structure has to be predefined with strong assumption: all of adversarial generator-discriminator, forward and inverse diffusion structures, hierarchical zone-POI generative structure are predefined by humans; 2) ignore the power of domain expert developed tools: domain urban planners have developed various tools in the urban planning process guided by urban theory, while existing pure neural networks based generation ignore the power of the tools developed by urban planner practitioners. To address these limitations, we outline a future research direction agentic urban AI planner, calling for a new synthesis of agentic AI and participatory urbanism.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairReason: Balancing Reasoning and Social Bias in MLLMs</title>
<link>https://arxiv.org/abs/2507.23067</link>
<guid>https://arxiv.org/abs/2507.23067</guid>
<content:encoded><![CDATA[
arXiv:2507.23067v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art results across a wide range of tasks and modalities. To push their reasoning ability further, recent studies explore advanced prompting schemes and post-training fine-tuning. Although these techniques improve logical accuracy, they frequently leave the models' outputs burdened with pronounced social biases. Clarifying how reasoning gains interact with bias mitigation-and whether the two objectives inherently trade off-therefore remains an open and pressing research problem. Our study begins by benchmarking three bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation (KD), and rule-based reinforcement learning (RL)-under identical conditions, establishing their baseline strengths and weaknesses. Building on these results, we vary the proportion of debias-focused and reasoning-centric samples within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement learning cuts stereotype scores by 10% while retaining 88% of the model's original reasoning accuracy, offering concrete guidance for balancing fairness and capability in MLLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety</title>
<link>https://arxiv.org/abs/2508.03864</link>
<guid>https://arxiv.org/abs/2508.03864</guid>
<content:encoded><![CDATA[
arXiv:2508.03864v2 Announce Type: replace 
Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit strong collaboration and performance. However, their growing openness and interaction complexity pose serious risks, notably jailbreak and adversarial attacks. Existing defenses typically rely on external guard modules, such as dedicated safety agents, to handle unsafe behaviors. Unfortunately, this paradigm faces two challenges: (1) standalone agents offer limited protection, and (2) their independence leads to single-point failure-if compromised, system-wide safety collapses. Naively increasing the number of guard agents further raises cost and complexity. To address these challenges, we propose Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that enables all task agents to jointly acquire defensive capabilities. Rather than relying on external safety modules, Evo-MARL trains each agent to simultaneously perform its primary function and resist adversarial threats, ensuring robustness without increasing system overhead or single-node failure. Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing reinforcement learning to co-evolve attackers and defenders. This adversarial training paradigm internalizes safety mechanisms and continually enhances MAS performance under co-evolving threats. Experiments show that Evo-MARL reduces attack success rates by up to 22% while boosting accuracy by up to 5% on reasoning tasks-demonstrating that safety and utility can be jointly improved.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media</title>
<link>https://arxiv.org/abs/2508.05557</link>
<guid>https://arxiv.org/abs/2508.05557</guid>
<content:encoded><![CDATA[
arXiv:2508.05557v3 Announce Type: replace 
Abstract: Social media has evolved into a complex multimodal environment where text, images, and other signals interact to shape nuanced meanings, often concealing harmful intent. Identifying such intent, whether sarcasm, hate speech, or misinformation, remains challenging due to cross-modal contradictions, rapid cultural shifts, and subtle pragmatic cues. To address these challenges, we propose MV-Debate, a multi-view agent debate framework with dynamic reflection gating for unified multimodal harmful content detection. MV-Debate assembles four complementary debate agents, a surface analyst, a deep reasoner, a modality contrast, and a social contextualist, to analyze content from diverse interpretive perspectives. Through iterative debate and reflection, the agents refine responses under a reflection-gain criterion, ensuring both accuracy and efficiency. Experiments on three benchmark datasets demonstrate that MV-Debate significantly outperforms strong single-model and existing multi-agent debate baselines. This work highlights the promise of multi-agent debate in advancing reliable social intent detection in safety-critical online contexts.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPF-World: Action World Model for Multi-Agent Path Finding</title>
<link>https://arxiv.org/abs/2508.12087</link>
<guid>https://arxiv.org/abs/2508.12087</guid>
<content:encoded><![CDATA[
arXiv:2508.12087v2 Announce Type: replace 
Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free paths from the designated start locations to goal positions for multiple agents. It underlies a variety of real-world tasks, including multi-robot coordination, robot-assisted logistics, and social navigation. Recent decentralized learnable solvers have shown great promise for large-scale MAPF, especially when leveraging foundation models and large datasets. However, these agents are reactive policy models and exhibit limited modeling of environmental temporal dynamics and inter-agent dependencies, resulting in performance degradation in complex, long-term planning scenarios. To address these limitations, we propose MAPF-World, an autoregressive action world model for MAPF that unifies situation understanding and action generation, guiding decisions beyond immediate local observations. It improves situational awareness by explicitly modeling environmental dynamics, including spatial features and temporal dependencies, through future state and actions prediction. By incorporating these predicted futures, MAPF-World enables more informed, coordinated, and far-sighted decision-making, especially in complex multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an automatic map generator grounded in real-world scenarios, capturing practical map layouts for training and evaluating MAPF solvers. Extensive experiments demonstrate that MAPF-World outperforms state-of-the-art learnable solvers, showcasing superior zero-shot generalization to out-of-distribution cases. Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM + ASP Workflow for Joint Entity-Relation Extraction</title>
<link>https://arxiv.org/abs/2508.12611</link>
<guid>https://arxiv.org/abs/2508.12611</guid>
<content:encoded><![CDATA[
arXiv:2508.12611v2 Announce Type: replace 
Abstract: Joint entity-relation extraction (JERE) identifies both entities and their relationships simultaneously. Traditional machine-learning based approaches to performing this task require a large corpus of annotated data and lack the ability to easily incorporate domain specific information in the construction of the model. Therefore, creating a model for JERE is often labor intensive, time consuming, and elaboration intolerant. In this paper, we propose harnessing the capabilities of generative pretrained large language models (LLMs) and the knowledge representation and reasoning capabilities of Answer Set Programming (ASP) to perform JERE. We present a generic workflow for JERE using LLMs and ASP. The workflow is generic in the sense that it can be applied for JERE in any domain. It takes advantage of LLM's capability in natural language understanding in that it works directly with unannotated text. It exploits the elaboration tolerant feature of ASP in that no modification of its core program is required when additional domain specific knowledge, in the form of type specifications, is found and needs to be used. We demonstrate the usefulness of the proposed workflow through experiments with limited training data on three well-known benchmarks for JERE. The results of our experiments show that the LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10\% of training data. It is able to achieve a 2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the SciERC corpus, one of the most difficult benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding</title>
<link>https://arxiv.org/abs/2508.19576</link>
<guid>https://arxiv.org/abs/2508.19576</guid>
<content:encoded><![CDATA[
arXiv:2508.19576v2 Announce Type: replace 
Abstract: With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified LLM RL paradigm that significantly improves LLM's code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time decoding method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of LLM policy has been improved, we further propose a test time decoding optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When decoding, the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the LLM policy to achieve high reasoning accuracy. We conduct extensive experiments on coding problems to verify the validity of the proposed RL paradigm. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of LLM policies. Codes for our project can be found at https://github.com/THUDM/ReST-RL.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Machines Imitate Humans? Integrative Turing-like tests for Language and Vision Demonstrate a Narrowing Gap</title>
<link>https://arxiv.org/abs/2211.13087</link>
<guid>https://arxiv.org/abs/2211.13087</guid>
<content:encoded><![CDATA[
arXiv:2211.13087v3 Announce Type: replace-cross 
Abstract: As AI becomes increasingly embedded in daily life, ascertaining whether an agent is human is critical. We systematically benchmark AI's ability to imitate humans in three language tasks (image captioning, word association, conversation) and three vision tasks (color estimation, object detection, attention prediction), collecting data from 636 humans and 37 AI agents. Next, we conducted 72,191 Turing-like tests with 1,916 human judges and 10 AI judges. Current AIs are approaching the ability to convincingly impersonate humans and deceive human judges in both language and vision. Even simple AI judges outperformed humans in distinguishing AI from human responses. Imitation ability showed minimal correlation with conventional AI performance metrics, suggesting that passing as human is an important independent evaluation criterion. The large-scale Turing datasets and metrics introduced here offer valuable benchmarks for assessing human-likeness in AI and highlight the importance of rigorous, quantitative imitation tests for AI development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catapult Dynamics and Phase Transitions in Quadratic Nets</title>
<link>https://arxiv.org/abs/2301.07737</link>
<guid>https://arxiv.org/abs/2301.07737</guid>
<content:encoded><![CDATA[
arXiv:2301.07737v2 Announce Type: replace-cross 
Abstract: Neural networks trained with gradient descent can undergo non-trivial phase transitions as a function of the learning rate. In \cite{lewkowycz2020large} it was discovered that wide neural nets can exhibit a catapult phase for super-critical learning rates, where the training loss grows exponentially quickly at early times before rapidly decreasing to a small value. During this phase the top eigenvalue of the neural tangent kernel (NTK) also undergoes significant evolution. In this work, we will prove that the catapult phase exists in a large class of models, including quadratic models and two-layer, homogenous neural nets. To do this, we show that for a certain range of learning rates the weight norm decreases whenever the loss becomes large. We also empirically study learning rates beyond this theoretically derived range and show that the activation map of ReLU nets trained with super-critical learning rates becomes increasingly sparse as we increase the learning rate.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation</title>
<link>https://arxiv.org/abs/2309.14394</link>
<guid>https://arxiv.org/abs/2309.14394</guid>
<content:encoded><![CDATA[
arXiv:2309.14394v2 Announce Type: replace-cross 
Abstract: In this work, we address the challenge of multi-domain translation, where the objective is to learn mappings between arbitrary configurations of domains within a defined set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for three domains) without the need for separate models for each specific translation configuration, enabling more efficient and flexible domain translation. We introduce Multi-Domain Diffusion (MDD), a method with dual purposes: i) reconstructing any missing views for new data objects, and ii) enabling learning in semi-supervised contexts with arbitrary supervision configurations. MDD achieves these objectives by exploiting the noise formulation of diffusion models, specifically modeling one noise level per domain. Similar to existing domain translation approaches, MDD learns the translation between any combination of domains. However, unlike prior work, our formulation inherently handles semi-supervised learning without modification by representing missing views as noise in the diffusion process. We evaluate our approach through domain translation experiments on BL3NDT, a multi-domain synthetic dataset designed for challenging semantic domain inversion, the BraTS2020 dataset, and the CelebAMask-HQ dataset.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of Machine Learning Techniques in Imbalanced Data and Future Trends</title>
<link>https://arxiv.org/abs/2310.07917</link>
<guid>https://arxiv.org/abs/2310.07917</guid>
<content:encoded><![CDATA[
arXiv:2310.07917v2 Announce Type: replace-cross 
Abstract: For over two decades, detecting rare events has been a challenging task among researchers in the data mining and machine learning domain. Real-life problems inspire researchers to navigate and further improve data processing and algorithmic approaches to achieve effective and computationally efficient methods for imbalanced learning. In this paper, we have collected and reviewed 258 peer-reviewed papers from archival journals and conference papers in an attempt to provide an in-depth review of various approaches in imbalanced learning from technical and application perspectives. This work aims to provide a structured review of methods used to address the problem of imbalanced data in various domains and create a general guideline for researchers in academia or industry who want to dive into the broad field of machine learning using large-scale imbalanced data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-driven Community Resilience Rating based on Intertwined Socio-Technical Systems Features</title>
<link>https://arxiv.org/abs/2311.01661</link>
<guid>https://arxiv.org/abs/2311.01661</guid>
<content:encoded><![CDATA[
arXiv:2311.01661v2 Announce Type: replace-cross 
Abstract: Community resilience is a complex and muti-faceted phenomenon that emerges from complex and nonlinear interactions among different socio-technical systems and their resilience properties. However, present studies on community resilience focus primarily on vulnerability assessment and utilize index-based approaches, with limited ability to capture heterogeneous features within community socio-technical systems and their nonlinear interactions in shaping robustness, redundancy, and resourcefulness components of resilience. To address this gap, this paper presents an integrated three-layer deep learning model for community resilience rating (called Resili-Net). Twelve measurable resilience features are specified and computed within community socio-technical systems (i.e., facilities, infrastructures, and society) related to three resilience components of robustness, redundancy, and resourcefulness. Using publicly accessible data from multiple metropolitan statistical areas in the United States, Resili-Net characterizes the resilience levels of spatial areas into five distinct levels. The interpretability of the model outcomes enables feature analysis for specifying the determinants of resilience in areas within each resilience level, allowing for the identification of specific resilience enhancement strategies. Changes in community resilience profiles under urban development patterns are further examined by changing the value of related socio-technical systems features. Accordingly, the outcomes provide novel perspectives for community resilience assessment by harnessing machine intelligence and heterogeneous urban big data.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data</title>
<link>https://arxiv.org/abs/2402.12391</link>
<guid>https://arxiv.org/abs/2402.12391</guid>
<content:encoded><![CDATA[
arXiv:2402.12391v3 Announce Type: replace-cross 
Abstract: Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset to assess TAIS's effectiveness in gene identification, demonstrating our system's potential to significantly enhance the efficiency and scope of scientific exploration. Our findings represent a solid step towards automating scientific discovery through large language models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models</title>
<link>https://arxiv.org/abs/2403.01489</link>
<guid>https://arxiv.org/abs/2403.01489</guid>
<content:encoded><![CDATA[
arXiv:2403.01489v2 Announce Type: replace-cross 
Abstract: Text-to-image generative models have recently garnered significant attention due to their ability to generate images based on prompt descriptions. While these models have shown promising performance, concerns have been raised regarding the potential misuse of the generated fake images. In response to this, we have presented a simple yet effective training-free method to attribute fake images generated by text-to-image models to their source models. Given a test image to be attributed, we first inverse the textual prompt of the image, and then put the reconstructed prompt into different candidate models to regenerate candidate fake images. By calculating and ranking the similarity of the test image and the candidate images, we can determine the source of the image. This attribution allows model owners to be held accountable for any misuse of their models. Note that our approach does not limit the number of candidate text-to-image generative models. Comprehensive experiments reveal that (1) Our method can effectively attribute fake images to their source models, achieving comparable attribution performance with the state-of-the-art method; (2) Our method has high scalability ability, which is well adapted to real-world attribution scenarios. (3) The proposed method yields satisfactory robustness to common attacks, such as Gaussian blurring, JPEG compression, and Resizing. We also analyze the factors that influence the attribution performance, and explore the boost brought by the proposed method as a plug-in to improve the performance of existing SOTA. We hope our work can shed some light on the solutions to addressing the source of AI-generated images, as well as to prevent the misuse of text-to-image generative models.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Molecular Generative Adversarial Network with Multi-Property Optimization</title>
<link>https://arxiv.org/abs/2404.00081</link>
<guid>https://arxiv.org/abs/2404.00081</guid>
<content:encoded><![CDATA[
arXiv:2404.00081v2 Announce Type: replace-cross 
Abstract: Deep generative models, such as generative adversarial networks (GANs), have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized reinforcement learning (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in GANs. However, due to the inherent instability in training GANs and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel GAN based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance to state-of-the-art models, and efficiently generates molecules with multi-property optimization. The source code will be released upon acceptance of the paper.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series</title>
<link>https://arxiv.org/abs/2404.11269</link>
<guid>https://arxiv.org/abs/2404.11269</guid>
<content:encoded><![CDATA[
arXiv:2404.11269v4 Announce Type: replace-cross 
Abstract: In time series anomaly detection (TSAD), the scarcity of labeled data poses a challenge to the development of accurate models. Unsupervised domain adaptation (UDA) offers a solution by leveraging labeled data from a related domain to detect anomalies in an unlabeled target domain. However, existing UDA methods assume consistent anomalous classes across domains. To address this limitation, we propose a novel Domain Adaptation Contrastive learning model for Anomaly Detection in multivariate time series (DACAD), combining UDA with contrastive learning. DACAD utilizes an anomaly injection mechanism that enhances generalization across unseen anomalous classes, improving adaptability and robustness. Additionally, our model employs supervised contrastive loss for the source domain and self-supervised contrastive triplet loss for the target domain, ensuring comprehensive feature representation learning and domain-invariant feature extraction. Finally, an effective Center-based Entropy Classifier (CEC) accurately learns normal boundaries in the source domain. Extensive evaluations on multiple real-world datasets and a synthetic dataset highlight DACAD's superior performance in transferring knowledge across domains and mitigating the challenge of limited labeled data in TSAD.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Over-Certainty Phenomenon in Modern Test-Time Adaptation Algorithms</title>
<link>https://arxiv.org/abs/2404.16168</link>
<guid>https://arxiv.org/abs/2404.16168</guid>
<content:encoded><![CDATA[
arXiv:2404.16168v4 Announce Type: replace-cross 
Abstract: When neural networks are confronted with unfamiliar data that deviate from their training set, this signifies a domain shift. While these networks output predictions on their inputs, they typically fail to account for their level of familiarity with these novel observations. Prevailing works navigate test-time adaptation with the goal of curtailing model entropy, yet they unintentionally produce models that struggle with sub-optimal calibration-a dilemma we term the over-certainty phenomenon. This over-certainty in predictions can be particularly dangerous in the setting of domain shifts, as it may lead to misplaced trust. In this paper, we propose a solution that not only maintains accuracy but also addresses calibration by mitigating the over-certainty phenomenon. To do this, we introduce a certainty regularizer that dynamically adjusts pseudo-label confidence by accounting for both backbone entropy and logit norm. Our method achieves state-of-the-art performance in terms of Expected Calibration Error and Negative Log Likelihood, all while maintaining parity in accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2406.01586</link>
<guid>https://arxiv.org/abs/2406.01586</guid>
<content:encoded><![CDATA[
arXiv:2406.01586v3 Announce Type: replace-cross 
Abstract: Diffusion models have been verified to be effective in generating complex distributions from natural images to motion trajectories. Recent diffusion-based methods show impressive performance in 3D robotic manipulation tasks, whereas they suffer from severe runtime inefficiency due to multiple denoising steps, especially with high-dimensional observations. To this end, we propose a real-time robotic manipulation model named ManiCM that imposes the consistency constraint to the diffusion process, so that the model can generate robot actions in only one-step inference. Specifically, we formulate a consistent diffusion process in the robot action space conditioned on the point cloud input, where the original action is required to be directly denoised from any point along the ODE trajectory. To model this process, we design a consistency distillation technique to predict the action sample directly instead of predicting the noise within the vision community for fast convergence in the low-dimensional action manifold. We evaluate ManiCM on 31 robotic manipulation tasks from Adroit and Metaworld, and the results demonstrate that our approach accelerates the state-of-the-art method by 10 times in average inference speed while maintaining competitive average success rate.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning</title>
<link>https://arxiv.org/abs/2406.06620</link>
<guid>https://arxiv.org/abs/2406.06620</guid>
<content:encoded><![CDATA[
arXiv:2406.06620v4 Announce Type: replace-cross 
Abstract: The recent rapid advancements in language models (LMs) have garnered attention in medical time series-text multimodal learning. However, existing contrastive learning-based and prompt-based LM approaches tend to be biased, often assigning a primary role to time series modality while treating text modality as secondary. We classify these approaches under a temporal-primary paradigm, which may overlook the unique and critical task-relevant information embedded in text modality like clinical reports, thus failing to fully leverage mutual benefits and complementarity of different modalities. To fill this gap, we propose a novel textual-temporal multimodal learning paradigm that enables either modality to serve as the primary while being enhanced by the other, thereby effectively capturing modality-specific information and fostering cross-modal interaction. In specific, we design MedualTime, a language model composed of dual adapters to implement temporal-primary and textual-primary modeling simultaneously. Within each adapter, lightweight adaptation tokens are injected into the top layers of LM to encourage high-level modality fusion. The shared LM pipeline by dual adapters not only achieves adapter alignment but also enables efficient fine-tuning, reducing computational resources. Empirically, MedualTime demonstrates superior performance on medical data, achieving notable improvements of 8% accuracy and 12% F1 in supervised settings. Furthermore, MedualTime's transferability is validated by few-shot label transfer experiments from coarse-grained to fine-grained medical data. https://github.com/start2020/MedualTime
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flash STU: Fast Spectral Transform Units</title>
<link>https://arxiv.org/abs/2409.10489</link>
<guid>https://arxiv.org/abs/2409.10489</guid>
<content:encoded><![CDATA[
arXiv:2409.10489v5 Announce Type: replace-cross 
Abstract: Recent advances in state-space model architectures have shown great promise for efficient sequence modeling, but challenges remain in balancing computational efficiency with model expressiveness. We propose the Flash STU architecture, a hybrid model that interleaves spectral state space model layers with sliding window attention, enabling scalability to billions of parameters for language modeling while maintaining a near-linear time complexity. We evaluate the Flash STU and its variants on diverse sequence prediction tasks, including linear dynamical systems, robotics control, and language modeling. We find that, given a fixed parameter budget, the Flash STU architecture consistently outperforms the Transformer and other leading state-space models such as S4 and Mamba-2.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BeSimulator: A Large Language Model Powered Text-based Behavior Simulator</title>
<link>https://arxiv.org/abs/2409.15865</link>
<guid>https://arxiv.org/abs/2409.15865</guid>
<content:encoded><![CDATA[
arXiv:2409.15865v2 Announce Type: replace-cross 
Abstract: Traditional robot simulators focus on physical process modeling and realistic rendering, often suffering from high computational costs, inefficiencies, and limited adaptability. To handle this issue, we concentrate on behavior simulation in robotics to analyze and validate the logic behind robot behaviors, aiming to achieve preliminary evaluation before deploying resource-intensive simulators and thus enhance simulation efficiency. In this paper, we propose BeSimulator, a modular and novel LLM-powered framework, as an attempt towards behavior simulation in the context of text-based environments. By constructing text-based virtual environments and performing semantic-level simulation, BeSimulator can generalize across scenarios and achieve long-horizon complex simulation. Inspired by human cognition paradigm, it employs a ``consider-decide-capture-transfer'' four-phase simulation process, termed Chain of Behavior Simulation (CBS), which excels at analyzing action feasibility and state transition. Additionally, BeSimulator incorporates code-driven reasoning to enable arithmetic operations and enhance reliability, and reflective feedback to refine simulation. Based on our manually constructed behavior-tree-based simulation benchmark, BTSIMBENCH, our experiments show a significant performance improvement in behavior simulation compared to baselines, ranging from 13.60% to 24.80%. Code and data are available at https://github.com/Dawn888888/BeSimulator.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking GNN Expressive Power from a Distributed Computational Model Perspective</title>
<link>https://arxiv.org/abs/2410.01308</link>
<guid>https://arxiv.org/abs/2410.01308</guid>
<content:encoded><![CDATA[
arXiv:2410.01308v4 Announce Type: replace-cross 
Abstract: The success of graph neural networks (GNNs) has motivated theoretical studies on their expressive power, often through alignments with the Weisfeiler-Lehman (WL) tests. However, such analyses typically focus on the ability of GNNs to distinguish between graph structures, rather than to compute or approximate specific function classes. The latter is more commonly studied in machine learning theory, including results such as the Turing completeness of recurrent networks and the universal approximation property of feedforward networks. We argue that using well-defined computational models, such as a modified CONGEST model with clearly specified preprocessing and postprocessing, offers a more sound framework for analyzing GNN expressiveness. Within this framework, we show that allowing unrestricted preprocessing or incorporating externally computed features, while claiming that these precomputations enhance the expressiveness, can sometimes lead to problems. We also show that the lower bound on a GNN's capacity (depth multiplied by width) to simulate one iteration of the WL test actually grows nearly linearly with graph size, indicating that the WL test is not locally computable and is misaligned with message-passing GNNs. Despite these negative results, we also present positive results that characterize the effects of virtual nodes and edges from a computational model perspective. Finally, we highlight several open problems regarding GNN expressiveness for further exploration.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Architecture Built for Federated Learning: Addressing Data Heterogeneity through Adaptive Normalization-Free Feature Recalibration</title>
<link>https://arxiv.org/abs/2410.02006</link>
<guid>https://arxiv.org/abs/2410.02006</guid>
<content:encoded><![CDATA[
arXiv:2410.02006v2 Announce Type: replace-cross 
Abstract: Federated learning is a decentralized collaborative training paradigm preserving stakeholders' data ownership while improving performance and generalization. However, statistical heterogeneity among client datasets degrades system performance. To address this issue, we propose Adaptive Normalization-free Feature Recalibration (ANFR), a model architecture-level approach that combines weight standardization and channel attention to combat heterogeneous data in FL. ANFR leverages weight standardization to avoid mismatched client statistics and inconsistent averaging, ensuring robustness under heterogeneity, and channel attention to produce learnable scaling factors for feature maps, suppressing inconsistencies across clients due to heterogeneity. We demonstrate that combining these techniques boosts model performance beyond their individual contributions, by improving class selectivity and channel attention weight distribution. ANFR works with any aggregation method, supports both global and personalized FL, and adds minimal overhead. Furthermore, when training with differential privacy, ANFR achieves an appealing balance between privacy and utility, enabling strong privacy guarantees without sacrificing performance. By integrating weight standardization and channel attention in the backbone model, ANFR offers a novel and versatile approach to the challenge of statistical heterogeneity. Extensive experiments show ANFR consistently outperforms established baselines across various aggregation methods, datasets, and heterogeneity conditions. Code is provided at https://github.com/siomvas/ANFR.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping</title>
<link>https://arxiv.org/abs/2410.05341</link>
<guid>https://arxiv.org/abs/2410.05341</guid>
<content:encoded><![CDATA[
arXiv:2410.05341v3 Announce Type: replace-cross 
Abstract: Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: NeuroBOLT, i.e., Neuro-to-BOLD Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs unseen resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy with the potential to generalize across varying conditions and sites, which significantly advances the integration of these two modalities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ETF: An Entity Tracing Framework for Hallucination Detection in Code Summaries</title>
<link>https://arxiv.org/abs/2410.14748</link>
<guid>https://arxiv.org/abs/2410.14748</guid>
<content:encoded><![CDATA[
arXiv:2410.14748v4 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their ability to understand both natural language and code, driving their use in tasks like natural language-to-code (NL2Code) and code summarisation. However, LLMs are prone to hallucination, outputs that stray from intended meanings. Detecting hallucinations in code summarisation is especially difficult due to the complex interplay between programming and natural languages. We introduce a first-of-its-kind dataset, CodeSumEval, with ~10K samples, curated specifically for hallucination detection in code summarisation. We further propose a novel Entity Tracing Framework (ETF) that a) utilises static program analysis to identify code entities from the program and b) uses LLMs to map and verify these entities and their intents within generated code summaries. Our experimental analysis demonstrates the framework's effectiveness, leading to a 73% F1 score. The proposed approach provides a method for detecting hallucinations by tracing entities from the summary to the code, allowing us to evaluate summary accuracy and localise the error within the summary.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FACEGroup: Feasible and Actionable Counterfactual Explanations for Group Fairness</title>
<link>https://arxiv.org/abs/2410.22591</link>
<guid>https://arxiv.org/abs/2410.22591</guid>
<content:encoded><![CDATA[
arXiv:2410.22591v3 Announce Type: replace-cross 
Abstract: Counterfactual explanations assess unfairness by revealing how inputs must change to achieve a desired outcome. This paper introduces the first graph-based framework for generating group counterfactual explanations to audit group fairness, a key aspect of trustworthy machine learning. Our framework, FACEGroup (Feasible and Actionable Counterfactual Explanations for Group Fairness), models real-world feasibility constraints, identifies subgroups with similar counterfactuals, and captures key trade-offs in counterfactual generation, distinguishing it from existing methods. To evaluate fairness, we introduce novel metrics for both group and subgroup level analysis that explicitly account for these trade-offs. Experiments on benchmark datasets show that FACEGroup effectively generates feasible group counterfactuals while accounting for trade-offs, and that our metrics capture and quantify fairness disparities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Coded Distributed Convolution Computing for Enhanced Straggler Resilience and Numerical Stability in Distributed CNNs</title>
<link>https://arxiv.org/abs/2411.01579</link>
<guid>https://arxiv.org/abs/2411.01579</guid>
<content:encoded><![CDATA[
arXiv:2411.01579v3 Announce Type: replace-cross 
Abstract: Deploying Convolutional Neural Networks (CNNs) on resource-constrained devices necessitates efficient management of computational resources, often via distributed environments susceptible to latency from straggler nodes. This paper introduces the Flexible Coded Distributed Convolution Computing (FCDCC) framework to enhance straggler resilience and numerical stability in distributed CNNs. We extend Coded Distributed Computing (CDC) with Circulant and Rotation Matrix Embedding (CRME) which was originally proposed for matrix multiplication to high-dimensional tensor convolution. For the proposed scheme, referred to as the Numerically Stable Coded Tensor Convolution (NSCTC) scheme, we also propose two new coded partitioning schemes: Adaptive-Padding Coded Partitioning (APCP) for the input tensor and Kernel-Channel Coded Partitioning (KCCP) for the filter tensor. These strategies enable linear decomposition of tensor convolutions and encoding them into CDC subtasks, combining model parallelism with coded redundancy for robust and efficient execution. Theoretical analysis identifies an optimal trade-off between communication and storage costs. Empirical results validate the framework's effectiveness in computational efficiency, straggler resilience, and scalability across various CNN architectures.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals</title>
<link>https://arxiv.org/abs/2411.07152</link>
<guid>https://arxiv.org/abs/2411.07152</guid>
<content:encoded><![CDATA[
arXiv:2411.07152v2 Announce Type: replace-cross 
Abstract: Task-Oriented Dialogue (TOD) systems assist users in completing tasks through natural language interactions, often relying on a single-layered workflow structure for slot-filling in public tasks, such as hotel bookings. However, in enterprise environments, which involve rich domain-specific knowledge, TOD systems face challenges due to task complexity and the lack of standardized documentation. In this work, we introduce HierTOD, an enterprise TOD system driven by hierarchical goals that can support composite workflows. By focusing on goal-driven interactions, our system serves a more proactive role, facilitating mixed-initiative dialogue and improving task completion. Equipped with components for natural language understanding, composite goal retriever, dialogue management, and response generation, backed by a well-organized data service with domain knowledge base and retrieval engine, HierTOD delivers efficient task assistance as judged by human evaluators. Furthermore, our system implementation unifies two TOD paradigms: slot-filling for information collection and step-by-step guidance for task execution. Our user study demonstrates the effectiveness and helpfulness of HierTOD in performing both paradigms.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons from Studying Two-Hop Latent Reasoning</title>
<link>https://arxiv.org/abs/2411.16353</link>
<guid>https://arxiv.org/abs/2411.16353</guid>
<content:encoded><![CDATA[
arXiv:2411.16353v3 Announce Type: replace-cross 
Abstract: Large language models can use chain-of-thought (CoT) to externalize reasoning, potentially enabling oversight of capable LLM agents. Prior work has shown that models struggle at two-hop question-answering without CoT. This capability is so basic that if it was a fundamental limitation, it would imply that many complex agentic tasks would similarly require CoT. We investigate LLM latent reasoning capabilities using two-hop question answering as a case study. Previous work on the gap between latent and externalized two-hop reasoning produced mixed evidence with inconclusive results. In this paper, we introduce a controlled setting for investigating two-hop reasoning in LLMs, where a positive result provides definitive evidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B and GPT-4o) on synthetic facts and test two-hop reasoning over these facts. By using synthetic facts, we rule out memorization and reasoning shortcuts as explanations for two-hop performance. We observe a nuanced picture: Models fail to compose two synthetic facts, but can succeed when one fact is synthetic and the other is natural. These results demonstrate that LLMs are undeniably capable of latent two-hop reasoning, although it remains unclear how this ability scales with model size. Finally, we highlight a lesson for researchers studying LLM reasoning: when drawing conclusions about LLM latent reasoning, one must be careful to avoid both spurious successes (that stem from memorization and reasoning shortcuts) and spurious failures (that may stem from artificial experimental setups, divorced from training setups of frontier LLMs).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives</title>
<link>https://arxiv.org/abs/2411.19787</link>
<guid>https://arxiv.org/abs/2411.19787</guid>
<content:encoded><![CDATA[
arXiv:2411.19787v2 Announce Type: replace-cross 
Abstract: Grounding the instruction in the environment is a key step in solving language-guided goal-reaching reinforcement learning problems. In automated reinforcement learning, a key concern is to enhance the model's ability to generalize across various tasks and environments. In goal-reaching scenarios, the agent must comprehend the different parts of the instructions within the environmental context in order to complete the overall task successfully. In this work, we propose CAREL (Cross-modal Auxiliary REinforcement Learning) as a new framework to solve this problem using auxiliary loss functions inspired by video-text retrieval literature and a novel method called instruction tracking, which automatically keeps track of progress in an environment. The results of our experiments suggest superior sample efficiency and systematic generalization for this framework in multi-modal reinforcement learning problems. Our code base is available here.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Federated Learning via Homomorphic Adversarial Networks</title>
<link>https://arxiv.org/abs/2412.01650</link>
<guid>https://arxiv.org/abs/2412.01650</guid>
<content:encoded><![CDATA[
arXiv:2412.01650v3 Announce Type: replace-cross 
Abstract: Privacy-preserving federated learning (PPFL) aims to train a global model for multiple clients while maintaining their data privacy. However, current PPFL protocols exhibit one or more of the following insufficiencies: considerable degradation in accuracy, the requirement for sharing keys, and cooperation during the key generation or decryption processes. As a mitigation, we develop the first protocol that utilizes neural networks to implement PPFL, as well as incorporating an Aggregatable Hybrid Encryption scheme tailored to the needs of PPFL. We name these networks as Homomorphic Adversarial Networks (HANs) which demonstrate that neural networks are capable of performing tasks similar to multi-key homomorphic encryption (MK-HE) while solving the problems of key distribution and collaborative decryption. Our experiments show that HANs are robust against privacy attacks. Compared with non-private federated learning, experiments conducted on multiple datasets demonstrate that HANs exhibit a negligible accuracy loss (at most 1.35%). Compared to traditional MK-HE schemes, HANs increase encryption aggregation speed by 6,075 times while incurring a 29.2 times increase in communication overhead.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Controlled Langevin Diffusions</title>
<link>https://arxiv.org/abs/2412.07081</link>
<guid>https://arxiv.org/abs/2412.07081</guid>
<content:encoded><![CDATA[
arXiv:2412.07081v2 Announce Type: replace-cross 
Abstract: An effective approach for sampling from unnormalized densities is based on the idea of gradually transporting samples from an easy prior to the complicated target distribution. Two popular methods are (1) Sequential Monte Carlo (SMC), where the transport is performed through successive annealed densities via prescribed Markov chains and resampling steps, and (2) recently developed diffusion-based sampling methods, where a learned dynamical transport is used. Despite the common goal, both approaches have different, often complementary, advantages and drawbacks. The resampling steps in SMC allow focusing on promising regions of the space, often leading to robust performance. While the algorithm enjoys asymptotic guarantees, the lack of flexible, learnable transitions can lead to slow convergence. On the other hand, diffusion-based samplers are learned and can potentially better adapt themselves to the target at hand, yet often suffer from training instabilities. In this work, we present a principled framework for combining SMC with diffusion-based samplers by viewing both methods in continuous time and considering measures on path space. This culminates in the new Sequential Controlled Langevin Diffusion (SCLD) sampling method, which is able to utilize the benefits of both methods and reaches improved performance on multiple benchmark problems, in many cases using only 10% of the training budget of previous diffusion-based samplers.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Port-Hamiltonian Differential Algebraic Equations for Compositional Learning of Electrical Networks</title>
<link>https://arxiv.org/abs/2412.11215</link>
<guid>https://arxiv.org/abs/2412.11215</guid>
<content:encoded><![CDATA[
arXiv:2412.11215v3 Announce Type: replace-cross 
Abstract: We develop compositional learning algorithms for coupled dynamical systems, with a particular focus on electrical networks. While deep learning has proven effective at modeling complex relationships from data, compositional couplings between system components typically introduce algebraic constraints on state variables, posing challenges to many existing data-driven approaches to modeling dynamical systems. Towards developing deep learning models for constrained dynamical systems, we introduce neural port-Hamiltonian differential algebraic equations (N-PHDAEs), which use neural networks to parameterize unknown terms in both the differential and algebraic components of a port-Hamiltonian DAE. To train these models, we propose an algorithm that uses automatic differentiation to perform index reduction, automatically transforming the neural DAE into an equivalent system of neural ordinary differential equations (N-ODEs), for which established model inference and backpropagation methods exist. Experiments simulating the dynamics of nonlinear circuits exemplify the benefits of our approach: the proposed N-PHDAE model achieves an order of magnitude improvement in prediction accuracy and constraint satisfaction when compared to a baseline N-ODE over long prediction time horizons. We also validate the compositional capabilities of our approach through experiments on a simulated DC microgrid: we train individual N-PHDAE models for separate grid components, before coupling them to accurately predict the behavior of larger-scale networks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing the impact of synthetic native samples and multi-tasking strategies in Hindi-English code-mixed humour and sarcasm detection</title>
<link>https://arxiv.org/abs/2412.12761</link>
<guid>https://arxiv.org/abs/2412.12761</guid>
<content:encoded><![CDATA[
arXiv:2412.12761v2 Announce Type: replace-cross 
Abstract: In this paper, we reported our experiments with various strategies to improve code-mixed humour and sarcasm detection. Particularly, we tried three approaches: (i) native sample mixing, (ii) multi-task learning (MTL), and (iii) prompting and instruction finetuning very large multilingual language models (VMLMs). In native sample mixing, we added monolingual task samples to code-mixed training sets. In MTL learning, we relied on native and code-mixed samples of a semantically related task (hate detection in our case). Finally, in our third approach, we evaluated the efficacy of VMLMs via few-shot context prompting and instruction finetuning. Some interesting findings we got are (i) adding native samples improved humor (raising the F1-score up to 6.76%) and sarcasm (raising the F1-score up to 8.64%) detection, (ii) training MLMs in an MTL framework boosted performance for both humour (raising the F1-score up to 10.67%) and sarcasm (increment up to 12.35% in F1-score) detection, and (iii) prompting and instruction finetuning VMLMs couldn't outperform the other approaches. Finally, our ablation studies and error analysis discovered the cases where our model is yet to improve. We provided our code for reproducibility.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction</title>
<link>https://arxiv.org/abs/2412.14209</link>
<guid>https://arxiv.org/abs/2412.14209</guid>
<content:encoded><![CDATA[
arXiv:2412.14209v3 Announce Type: replace-cross 
Abstract: Explainable Artificial Intelligence seeks to make the reasoning processes of AI models transparent and interpretable, particularly in complex decision making environments. In the construction industry, where AI based decision support systems are increasingly adopted, limited attention has been paid to the integration of supporting evidence that underpins the reliability and accountability of AI generated outputs. The absence of such evidence undermines the validity of explanations and the trustworthiness of system recommendations. This paper addresses this gap by introducing a theoretical, evidence based means end framework developed through a narrative review. The framework offers an epistemic foundation for designing XAI enabled DSS that generate meaningful explanations tailored to users knowledge needs and decision contexts. It focuses on evaluating the strength, relevance, and utility of different types of evidence supporting AI generated explanations. While developed with construction professionals as primary end users, the framework is also applicable to developers, regulators, and project managers with varying epistemic goals.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DispFormer: A Pretrained Transformer Incorporating Physical Constraints for Dispersion Curve Inversion</title>
<link>https://arxiv.org/abs/2501.04366</link>
<guid>https://arxiv.org/abs/2501.04366</guid>
<content:encoded><![CDATA[
arXiv:2501.04366v2 Announce Type: replace-cross 
Abstract: Surface wave dispersion curve inversion is crucial for estimating subsurface shear-wave velocity (vs), yet traditional methods often face challenges related to computational cost, non-uniqueness, and sensitivity to initial models. While deep learning approaches show promise, many require large labeled datasets and struggle with real-world datasets, which often exhibit varying period ranges, missing values, and low signal-to-noise ratios. To address these limitations, this study introduces DispFormer, a transformer-based neural network for $v_s$ profile inversion from Rayleigh-wave phase and group dispersion curves. DispFormer processes dispersion data independently at each period, allowing it to handle varying lengths without requiring network modifications or strict alignment between training and testing datasets. A depth-aware training strategy is also introduced, incorporating physical constraints derived from the depth sensitivity of dispersion data. DispFormer is pre-trained on a global synthetic dataset and evaluated on two regional synthetic datasets using zero-shot and few-shot strategies. Results show that even without labeled data, the zero-shot DispFormer generates inversion profiles that outperform the interpolated reference model used as the pretraining target, providing a deployable initial model generator to assist traditional workflows. When partial labeled data available, the few-shot trained DispFormer surpasses traditional global search methods. Real-world tests further confirm that DispFormer generalizes well to dispersion data with varying lengths and achieves lower data residuals than reference models. These findings underscore the potential of DispFormer as a foundation model for dispersion curve inversion and demonstrate the advantages of integrating physics-informed deep learning into geophysical applications.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking</title>
<link>https://arxiv.org/abs/2501.09751</link>
<guid>https://arxiv.org/abs/2501.09751</guid>
<content:encoded><![CDATA[
arXiv:2501.09751v3 Announce Type: replace-cross 
Abstract: Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, novelty, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, unoriginal, and repetitive outputs. To address these issues, we propose OmniThink, a slow-thinking machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they slowly deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles. Code is available at https://github.com/zjunlp/OmniThink.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude</title>
<link>https://arxiv.org/abs/2501.10484</link>
<guid>https://arxiv.org/abs/2501.10484</guid>
<content:encoded><![CDATA[
arXiv:2501.10484v3 Announce Type: replace-cross 
Abstract: Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study systematically evaluates how nine popular LLMs (both open-source and closed-source) respond to ethical dilemmas involving protected attributes. Across 50,400 trials spanning single and intersectional attribute combinations in four dilemma scenarios (protective vs. harmful), we assess models' ethical preferences, sensitivity, stability, and clustering patterns. Results reveal significant biases in protected attributes in all models, with differing preferences depending on model type and dilemma context. Notably, open-source LLMs show stronger preferences for marginalized groups and greater sensitivity in harmful scenarios, while closed-source models are more selective in protective situations and tend to favor mainstream groups. We also find that ethical behavior varies across dilemma types: LLMs maintain consistent patterns in protective scenarios but respond with more diverse and cognitively demanding decisions in harmful ones. Furthermore, models display more pronounced ethical tendencies under intersectional conditions than in single-attribute settings, suggesting that complex inputs reveal deeper biases. These findings highlight the need for multi-dimensional, context-aware evaluation of LLMs' ethical behavior and offer a systematic evaluation and approach to understanding and addressing fairness in LLM decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion-enhanced Cardiac Anatomy Segmentation via an Insertable Temporal Attention Module</title>
<link>https://arxiv.org/abs/2501.14929</link>
<guid>https://arxiv.org/abs/2501.14929</guid>
<content:encoded><![CDATA[
arXiv:2501.14929v2 Announce Type: replace-cross 
Abstract: Cardiac anatomy segmentation is useful for clinical assessment of cardiac morphology to inform diagnosis and intervention. Deep learning (DL), especially with motion information, has improved segmentation accuracy. However, existing techniques for motion enhancement are not yet optimal, and they have high computational costs due to increased dimensionality or reduced robustness due to suboptimal approaches that use non-DL motion registration, non-attention models, or single-headed attention. They further have limited adaptability and are inconvenient for incorporation into existing networks where motion awareness is desired. Here, we propose a novel, computationally efficient Temporal Attention Module (TAM) that offers robust motion enhancement, modeled as a small, multi-headed, cross-temporal attention module. TAM's uniqueness is that it is a lightweight, plug-and-play module that can be inserted into a broad range of segmentation networks (CNN-based, Transformer-based, or hybrid) for motion enhancement without requiring substantial changes in the network's backbone. This feature enables high adaptability and ease of integration for enhancing both existing and future networks. Extensive experiments on multiple 2D and 3D cardiac ultrasound and MRI datasets confirm that TAM consistently improves segmentation across a range of networks while maintaining computational efficiency and improving on currently reported performance. The evidence demonstrates that it is a robust, generalizable solution for motion-awareness enhancement that is scalable (such as from 2D to 3D).
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Separate Motion from Appearance: Customizing Motion via Customizing Text-to-Video Diffusion Models</title>
<link>https://arxiv.org/abs/2501.16714</link>
<guid>https://arxiv.org/abs/2501.16714</guid>
<content:encoded><![CDATA[
arXiv:2501.16714v2 Announce Type: replace-cross 
Abstract: Motion customization aims to adapt the diffusion model (DM) to generate videos with the motion specified by a set of video clips with the same motion concept. To realize this goal, the adaptation of DM should be possible to model the specified motion concept, without compromising the ability to generate diverse appearances. Thus, the key to solving this problem lies in how to separate the motion concept from the appearance in the adaptation process of DM. Typical previous works explore different ways to represent and insert a motion concept into large-scale pretrained text-to-video diffusion models, e.g., learning a motion LoRA, using latent noise residuals, etc. While those methods can encode the motion concept, they also inevitably encode the appearance in the reference videos, resulting in weakened appearance generation capability. In this paper, we follow the typical way to learn a motion LoRA to encode the motion concept, but propose two novel strategies to enhance motion-appearance separation, including temporal attention purification (TAP) and appearance highway (AH). Specifically, we assume that in the temporal attention module, the pretrained Value embeddings are sufficient to serve as basic components needed by producing a new motion. Thus, in TAP, we choose only to reshape the temporal attention with motion LoRAs so that Value embeddings can be reorganized to produce a new motion. Further, in AH, we alter the starting point of each skip connection in U-Net from the output of each temporal attention module to the output of each spatial attention module. Extensive experiments demonstrate that compared to previous works, our method can generate videos with appearance more aligned with the text descriptions and motion more consistent with the reference videos.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Steady-State Behavior in Complex Networks with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2502.01693</link>
<guid>https://arxiv.org/abs/2502.01693</guid>
<content:encoded><![CDATA[
arXiv:2502.01693v3 Announce Type: replace-cross 
Abstract: In complex systems, information propagation can be defined as diffused or delocalized, weakly localized, and strongly localized. This study investigates the application of graph neural network models to learn the behavior of a linear dynamical system on networks. A graph convolution and attention-based neural network framework has been developed to identify the steady-state behavior of the linear dynamical system. We reveal that our trained model distinguishes the different states with high accuracy. Furthermore, we have evaluated model performance with real-world data. In addition, to understand the explainability of our model, we provide an analytical derivation for the forward and backward propagation of our framework.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: LLMs Can be Good Tutors in English Education</title>
<link>https://arxiv.org/abs/2502.05467</link>
<guid>https://arxiv.org/abs/2502.05467</guid>
<content:encoded><![CDATA[
arXiv:2502.05467v2 Announce Type: replace-cross 
Abstract: While recent efforts have begun integrating large language models (LLMs) into English education, they often rely on traditional approaches to learning tasks without fully embracing educational methodologies, thus lacking adaptability to language learning. To address this gap, we argue that LLMs have the potential to serve as effective tutors in English Education. Specifically, LLMs can play three critical roles: (1) as data enhancers, improving the creation of learning materials or serving as student simulations; (2) as task predictors, serving as learner assessment or optimizing learning pathway; and (3) as agents, enabling personalized and inclusive education. We encourage interdisciplinary research to explore these roles, fostering innovation while addressing challenges and risks, ultimately advancing English Education through the thoughtful integration of LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Fourier Networks</title>
<link>https://arxiv.org/abs/2502.06018</link>
<guid>https://arxiv.org/abs/2502.06018</guid>
<content:encoded><![CDATA[
arXiv:2502.06018v2 Announce Type: replace-cross 
Abstract: Although Kolmogorov-Arnold based interpretable networks (KAN) have strong theoretical expressiveness, they face significant parameter explosion and high-frequency feature capture challenges in high-dimensional tasks. To address this issue, we propose the Kolmogorov-Arnold-Fourier Network (KAF), which effectively integrates trainable Random Fourier Features (RFF) and a novel hybrid GELU-Fourier activation mechanism to balance parameter efficiency and spectral representation capabilities. Our key technical contributions include: (1) merging KAN's dual-matrix structure through matrix association properties to substantially reduce parameters; (2) introducing learnable RFF initialization strategies to eliminate spectral distortion in high-dimensional approximation tasks; (3) implementing an adaptive hybrid activation function that progressively enhances frequency representation during the training process. Comprehensive experiments demonstrate the superiority of our KAF across various domains including vision, NLP, audio processing, and differential equation-solving tasks, effectively combining theoretical interpretability with practical utility and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHIRLA: Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis</title>
<link>https://arxiv.org/abs/2502.06681</link>
<guid>https://arxiv.org/abs/2502.06681</guid>
<content:encoded><![CDATA[
arXiv:2502.06681v2 Announce Type: replace-cross 
Abstract: Person re-identification (Re-ID) is a key challenge in computer vision, requiring the matching of individuals across cameras, locations, and time. While most research focuses on short-term scenarios with minimal appearance changes, real-world applications demand robust systems that handle long-term variations caused by clothing and physical changes. We present CHIRLA, Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis, a novel dataset designed for video-based long-term person Re-ID. CHIRLA was recorded over seven months in four connected indoor environments using seven strategically placed cameras, capturing realistic movements with substantial clothing and appearance variability. The dataset includes 22 individuals, more than five hours of video, and about 1M bounding boxes with identity annotations obtained through semi-automatic labeling. We also define benchmark protocols for person tracking and Re-ID, covering diverse and challenging scenarios such as occlusion, reappearance, and multi-camera conditions. By introducing this comprehensive benchmark, we aim to facilitate the development and evaluation of Re-ID algorithms that can reliably perform in challenging, long-term real-world scenarios. The benchmark code is publicly available at: https://github.com/bdager/CHIRLA.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models</title>
<link>https://arxiv.org/abs/2502.15836</link>
<guid>https://arxiv.org/abs/2502.15836</guid>
<content:encoded><![CDATA[
arXiv:2502.15836v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are trained using massive datasets, which often contain undesirable content such as harmful texts, personal information, and copyrighted material. To address this, machine unlearning aims to remove information from trained models. Recent work has shown that soft token attacks (STA) can successfully extract unlearned information from LLMs, but in this work we show that STAs can be an inadequate tool for auditing unlearning. Using common benchmarks such as Who Is Harry Potter? and TOFU, we demonstrate that in a strong auditor setting such attacks can elicit any information from the LLM, regardless of the deployed unlearning algorithm or whether the queried content was originally present in the training corpus. We further show that STA with just a few soft tokens (1-10) can elicit random strings over 400 characters long, indicating that STAs must be used carefully to effectively audit unlearning. Example code can be found at: https://github.com/IntelLabs/LLMart/tree/main/examples/unlearning
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support</title>
<link>https://arxiv.org/abs/2502.18658</link>
<guid>https://arxiv.org/abs/2502.18658</guid>
<content:encoded><![CDATA[
arXiv:2502.18658v4 Announce Type: replace-cross 
Abstract: AI programming tools enable powerful code generation, and recent prototypes attempt to reduce user effort with proactive AI agents, but their impact on programming workflows remains unexplored. We introduce and evaluate Codellaborator, a design probe LLM agent that initiates programming assistance based on editor activities and task context. We explored three interface variants to assess trade-offs between increasingly salient AI support: prompt-only, proactive agent, and proactive agent with presence and context (Codellaborator). In a within-subject study (N=18), we find that proactive agents increase efficiency compared to prompt-only paradigm, but also incur workflow disruptions. However, presence indicators and interaction context support alleviated disruptions and improved users' awareness of AI processes. We underscore trade-offs of Codellaborator on user control, ownership, and code understanding, emphasizing the need to adapt proactivity to programming processes. Our research contributes to the design exploration and evaluation of proactive AI systems, presenting design implications on AI-integrated programming workflow.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning</title>
<link>https://arxiv.org/abs/2502.18978</link>
<guid>https://arxiv.org/abs/2502.18978</guid>
<content:encoded><![CDATA[
arXiv:2502.18978v5 Announce Type: replace-cross 
Abstract: The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive Neural Predicate Modulation</title>
<link>https://arxiv.org/abs/2503.08994</link>
<guid>https://arxiv.org/abs/2503.08994</guid>
<content:encoded><![CDATA[
arXiv:2503.08994v2 Announce Type: replace-cross 
Abstract: Research on learned cardinality estimation has made significant progress in recent years. However, existing methods still face distinct challenges that hinder their practical deployment in production environments. We define these challenges as the ``Trilemma of Cardinality Estimation'', where learned cardinality estimation methods struggle to balance generality, accuracy, and updatability. To address these challenges, we introduce DistJoin, a join cardinality estimator based on efficient distribution prediction using multi-autoregressive models. Our contributions are threefold: (1) We propose a method to estimate join cardinality by leveraging the probability distributions of individual tables in a decoupled manner. (2) To meet the requirements of efficiency for DistJoin, we develop Adaptive Neural Predicate Modulation (ANPM), a high-throughput distribution estimation model. (3) We demonstrate that an existing similar approach suffers from variance accumulation issues by formal variance analysis. To mitigate this problem, DistJoin employs a selectivity-based approach to infer join cardinality, effectively reducing variance. In summary, DistJoin not only represents the first data-driven method to support both equi and non-equi joins simultaneously but also demonstrates superior accuracy while enabling fast and flexible updates. The experimental results demonstrate that DistJoin achieves the highest accuracy, robustness to data updates, generality, and comparable update and inference speed relative to existing methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIPER: Visual Perception and Explainable Reasoning for Sequential Decision-Making</title>
<link>https://arxiv.org/abs/2503.15108</link>
<guid>https://arxiv.org/abs/2503.15108</guid>
<content:encoded><![CDATA[
arXiv:2503.15108v2 Announce Type: replace-cross 
Abstract: While Large Language Models (LLMs) excel at reasoning on text and Vision-Language Models (VLMs) are highly effective for visual perception, applying those models for visual instruction-based planning remains a widely open problem. In this paper, we introduce VIPER, a novel framework for multimodal instruction-based planning that integrates VLM-based perception with LLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM generates textual descriptions of image observations, which are then processed by an LLM policy to predict actions based on the task goal. We fine-tune the reasoning module using behavioral cloning and reinforcement learning, improving our agent's decision-making capabilities. Experiments on the ALFWorld benchmark show that VIPER significantly outperforms state-of-the-art visual instruction-based planners while narrowing the gap with purely text-based oracles. By leveraging text as an intermediate representation, VIPER also enhances explainability, paving the way for a fine-grained analysis of perception and reasoning components.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond SHAP and Anchors: A large-scale experiment on how developers struggle to design meaningful end-user explanations</title>
<link>https://arxiv.org/abs/2503.15512</link>
<guid>https://arxiv.org/abs/2503.15512</guid>
<content:encoded><![CDATA[
arXiv:2503.15512v2 Announce Type: replace-cross 
Abstract: Modern machine learning produces models that are impossible for users or developers to fully understand--raising concerns about trust, oversight, safety, and human dignity when they are integrated into software products. Transparency and explainability methods aim to provide some help in understanding models, but it remains challenging for developers to design explanations that are understandable to target users and effective for their purpose. Emerging guidelines and regulations set goals but may not provide effective actionable guidance to developers. In a large-scale experiment with 124 participants, we explored how developers approach providing end-user explanations, including what challenges they face, and to what extent specific policies can guide their actions. We investigated whether and how specific forms of policy guidance help developers design explanations and provide evidence for policy compliance for an ML-powered screening tool for diabetic retinopathy. Participants across the board struggled to produce quality explanations and comply with the provided policies. Contrary to our expectations, we found that the nature and specificity of policy guidance had little effect. We posit that participant noncompliance is in part due to a failure to imagine and anticipate the needs of non-technical stakeholders. Drawing on cognitive process theory and the sociological imagination to contextualize participants' failure, we recommend educational interventions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Byzantine-Robust Federated Learning Using Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2503.20884</link>
<guid>https://arxiv.org/abs/2503.20884</guid>
<content:encoded><![CDATA[
arXiv:2503.20884v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) enables collaborative model training across distributed clients without sharing raw data, but its robustness is threatened by Byzantine behaviors such as data and model poisoning. Existing defenses face fundamental limitations: robust aggregation rules incur error lower bounds that grow with client heterogeneity, while detection-based methods often rely on heuristics (e.g., a fixed number of malicious clients) or require trusted external datasets for validation. We present a defense framework that addresses these challenges by leveraging a conditional generative adversarial network (cGAN) at the server to synthesize representative data for validating client updates. This approach eliminates reliance on external datasets, adapts to diverse attack strategies, and integrates seamlessly into standard FL workflows. Extensive experiments on benchmark datasets demonstrate that our framework accurately distinguishes malicious from benign clients while maintaining overall model accuracy. Beyond Byzantine robustness, we also examine the representativeness of synthesized data, computational costs of cGAN training, and the transparency and scalability of our approach.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models</title>
<link>https://arxiv.org/abs/2504.03624</link>
<guid>https://arxiv.org/abs/2504.03624</guid>
<content:encoded><![CDATA[
arXiv:2504.03624v4 Announce Type: replace-cross 
Abstract: As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\times$ faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization</title>
<link>https://arxiv.org/abs/2504.18026</link>
<guid>https://arxiv.org/abs/2504.18026</guid>
<content:encoded><![CDATA[
arXiv:2504.18026v4 Announce Type: replace-cross 
Abstract: Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI systems by constraining their decisions on a set of human-understandable concepts. However, CBMs typically assume that datasets contain accurate concept labels-an assumption often violated in practice, which we show can significantly degrade performance (by 25% in some cases). To address this, we introduce the Concept Preference Optimization (CPO) objective, a new loss function based on Direct Preference Optimization, which effectively mitigates the negative impact of concept mislabeling on CBM performance. We provide an analysis of key properties of the CPO objective, showing it directly optimizes for the concept's posterior distribution, and contrast it against Binary Cross Entropy (BCE), demonstrating that CPO is inherently less sensitive to concept noise. We empirically confirm our analysis by finding that CPO consistently outperforms BCE on three real-world datasets, both with and without added label noise. We make our code available on Github.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Flow Matching for Continual Robot Learning</title>
<link>https://arxiv.org/abs/2504.18471</link>
<guid>https://arxiv.org/abs/2504.18471</guid>
<content:encoded><![CDATA[
arXiv:2504.18471v2 Announce Type: replace-cross 
Abstract: Continual learning in robotics seeks systems that can constantly adapt to changing environments and tasks, mirroring human adaptability. A key challenge is refining dynamics models, essential for planning and control, while addressing issues such as safe adaptation, catastrophic forgetting, outlier management, data efficiency, and balancing exploration with exploitation -- all within task and onboard resource constraints. Towards this goal, we introduce a generative framework leveraging flow matching for online robot dynamics model alignment. Rather than executing actions based on a misaligned model, our approach refines planned actions to better match with those the robot would take if its model was well aligned. We find that by transforming the actions themselves rather than exploring with a misaligned model -- as is traditionally done -- the robot collects informative data more efficiently, thereby accelerating learning. Moreover, we validate that the method can handle an evolving and possibly imperfect model while reducing, if desired, the dependency on replay buffers or legacy model snapshots. We validate our approach using two platforms: an unmanned ground vehicle and a quadrotor. The results highlight the method's adaptability and efficiency, with a record 34.2\% higher task success rate, demonstrating its potential towards enabling continual robot learning. Code: https://github.com/AlejandroMllo/action_flow_matching.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test It Before You Trust It: Applying Software Testing for Trustworthy In-context Learning</title>
<link>https://arxiv.org/abs/2504.18827</link>
<guid>https://arxiv.org/abs/2504.18827</guid>
<content:encoded><![CDATA[
arXiv:2504.18827v3 Announce Type: replace-cross 
Abstract: In-context learning (ICL) has emerged as a powerful capability of large language models (LLMs), enabling them to perform new tasks based on a few provided examples without explicit fine-tuning. Despite their impressive adaptability, these models remain vulnerable to subtle adversarial perturbations and exhibit unpredictable behavior when faced with linguistic variations. Inspired by software testing principles, we introduce a software testing-inspired framework, called MMT4NL, for evaluating the trustworthiness of in-context learning by utilizing adversarial perturbations and software testing techniques. It includes diverse evaluation aspects of linguistic capabilities for testing the ICL capabilities of LLMs. MMT4NL is built around the idea of crafting metamorphic adversarial examples from a test set in order to quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is to treat any LLM as software and validate its functionalities just like testing the software. Finally, we demonstrate applications of MMT4NL on the sentiment analysis and question-answering tasks. Our experiments could reveal various linguistic bugs in state-of-the-art LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting</title>
<link>https://arxiv.org/abs/2504.19021</link>
<guid>https://arxiv.org/abs/2504.19021</guid>
<content:encoded><![CDATA[
arXiv:2504.19021v2 Announce Type: replace-cross 
Abstract: Efficient text classification is essential for handling the increasing volume of academic publications. This study explores the use of pre-trained language models (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on the Web of Science (WoS-46985) dataset for scientific text classification. To enhance performance, we augment the dataset by executing seven targeted queries in the WoS database, retrieving 1,000 articles per category aligned with WoS-46985's main classes. PLMs predict labels for this unlabeled data, and a hard-voting strategy combines predictions for improved accuracy and confidence. Fine-tuning on the expanded dataset with dynamic learning rates and early stopping significantly boosts classification accuracy, especially in specialized domains. Domain-specific models like SciBERT and BioBERT consistently outperform general-purpose models such as BERT. These findings underscore the efficacy of dataset augmentation, inference-driven label prediction, hard-voting, and fine-tuning techniques in creating robust and scalable solutions for automated academic text classification.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation</title>
<link>https://arxiv.org/abs/2505.11454</link>
<guid>https://arxiv.org/abs/2505.11454</guid>
<content:encoded><![CDATA[
arXiv:2505.11454v4 Announce Type: replace-cross 
Abstract: Large multimodal models (LMMs) have been widely tested on tasks like visual question answering (VQA), image captioning, and grounding, but lack rigorous evaluation for alignment with human-centered (HC) values such as fairness, ethics, and inclusivity. To address this gap, we introduce \textbf{HumaniBench}, a novel benchmark of 32,000 real-world image-question pairs and an evaluation suite. Labels are generated via an AI-assisted pipeline and validated by experts. HumaniBench assesses LMMs across seven key alignment principles: fairness, ethics, empathy, inclusivity, reasoning, robustness, and multilinguality, through diverse open-ended and closed-ended VQA tasks. Grounded in AI ethics and real-world needs, these principles provide a holistic lens for societal impact. Benchmarking results on different LMM shows that proprietary models generally lead in reasoning, fairness, and multilinguality, while open-source models excel in robustness and grounding. Most models struggle to balance accuracy with ethical and inclusive behavior. Techniques like Chain-of-Thought prompting and test-time scaling improve alignment. As the first benchmark tailored for HC alignment, HumaniBench offers a rigorous testbed to diagnose limitations, and promote responsible LMM development. All data and code are publicly available for reproducibility.
  Keywords: HumaniBench, vision-language models, responsible AI benchmark, AI alignment evaluation, AI ethics assessment, fairness in AI models, visual question answering (VQA) benchmark, image captioning evaluation, visual grounding tasks, trustworthy AI models, Chain-of-Thought prompting, test-time scaling, ethical AI development tools.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterFeat: A Pipeline for Finding Interesting Scientific Features</title>
<link>https://arxiv.org/abs/2505.13534</link>
<guid>https://arxiv.org/abs/2505.13534</guid>
<content:encoded><![CDATA[
arXiv:2505.13534v2 Announce Type: replace-cross 
Abstract: Finding interesting phenomena is the core of scientific discovery, but it is a manual, ill-defined concept. We present an integrative pipeline for automating the discovery of interesting simple hypotheses (feature-target relations with effect direction and a potential underlying mechanism) in structured biomedical data. The pipeline combines machine learning, knowledge graphs, literature search and Large Language Models. We formalize "interestingness" as a combination of novelty, utility and plausibility. On 8 major diseases from the UK Biobank, our pipeline consistently recovers risk factors years before their appearance in the literature. 40--53% of our top candidates were validated as interesting, compared to 0--7% for a SHAP-based baseline. Overall, 28% of 109 candidates were interesting to medical experts. The pipeline addresses the challenge of operationalizing "interestingness" scalably and for any target. We release data and code: https://github.com/LinialLab/InterFeat
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification for Health Management with Spatial-Temporal Hypergraph Enhanced Meta-Learning</title>
<link>https://arxiv.org/abs/2505.17142</link>
<guid>https://arxiv.org/abs/2505.17142</guid>
<content:encoded><![CDATA[
arXiv:2505.17142v2 Announce Type: replace-cross 
Abstract: Accurate classification of sleep stages based on bio-signals is fundamental not only for automatic sleep stage annotation, but also for clinical health management and continuous sleep monitoring. Traditionally, this task relies on experienced clinicians to manually annotate data, a process that is both time-consuming and labor-intensive. In recent years, deep learning methods have shown promise in automating this task. However, three major challenges remain: (1) deep learning models typically require large-scale labeled datasets, making them less effective in real-world settings where annotated data is limited; (2) significant inter-individual variability in bio-signals often results in inconsistent model performance when applied to new subjects, limiting generalization; and (3) existing approaches often overlook the high-order relationships among bio-signals, failing to simultaneously capture signal heterogeneity and spatial-temporal dependencies. To address these issues, we propose MetaSTH-Sleep, a few-shot sleep stage classification framework based on spatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid adaptation to new subjects using only a few labeled samples, while the hypergraph structure effectively models complex spatial interconnections and temporal dynamics simultaneously in EEG signals. Experimental results demonstrate that MetaSTH-Sleep achieves substantial performance improvements across diverse subjects, offering valuable insights to support clinicians in sleep stage annotation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LLM Reasoning Through Bias-Only Adaptation</title>
<link>https://arxiv.org/abs/2505.18706</link>
<guid>https://arxiv.org/abs/2505.18706</guid>
<content:encoded><![CDATA[
arXiv:2505.18706v2 Announce Type: replace-cross 
Abstract: We show that training a single $d$-dimensional steering vector per layer with reinforcement learning, while freezing all base weights, matches the accuracy of fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8 billion-parameter model this adds only $\approx 0.0016\%$ additional parameters and reproduces performance across a range of base models and mathematical-reasoning benchmarks. These results tighten the upper bound on the parameter budget required for high-level chain-of-thought reasoning, indicating that millions of adapter weights are unnecessary. The minimal trainable footprint reduces optimizer memory and inter-GPU communication, lowering the overall cost of fine-tuning. Moreover, a logit-lens analysis shows that the learned vectors amplify coherent token directions, providing clearer insight into the model's internal computations.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoloSpeech: Enhancing Intelligibility and Quality in Target Speech Extraction through a Cascaded Generative Pipeline</title>
<link>https://arxiv.org/abs/2505.19314</link>
<guid>https://arxiv.org/abs/2505.19314</guid>
<content:encoded><![CDATA[
arXiv:2505.19314v3 Announce Type: replace-cross 
Abstract: Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio's latent space, aligning it with the mixture audio's latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction while demonstrating exceptional generalization on out-of-domain data and real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-output Classification using a Cross-talk Architecture for Compound Fault Diagnosis of Motors in Partially Labeled Condition</title>
<link>https://arxiv.org/abs/2505.24001</link>
<guid>https://arxiv.org/abs/2505.24001</guid>
<content:encoded><![CDATA[
arXiv:2505.24001v2 Announce Type: replace-cross 
Abstract: The increasing complexity of rotating machinery and the diversity of operating conditions, such as rotating speed and varying torques, have amplified the challenges in fault diagnosis in scenarios requiring domain adaptation, particularly involving compound faults. This study addresses these challenges by introducing a novel multi-output classification (MOC) framework tailored for domain adaptation in partially labeled target datasets. Unlike conventional multi-class classification (MCC) approaches, the MOC framework classifies the severity levels of compound faults simultaneously. Furthermore, we explore various single-task and multi-task architectures applicable to the MOC formulation-including shared trunk and cross-talk-based designs-for compound fault diagnosis under partially labeled conditions. Based on this investigation, we propose a novel cross-talk architecture, residual neural dimension reductor (RNDR), that enables selective information sharing across diagnostic tasks, effectively enhancing classification performance in compound fault scenarios. In addition, frequency-layer normalization was incorporated to improve domain adaptation performance on motor vibration data. Compound fault conditions were implemented using a motor-based test setup and evaluated across six domain adaptation scenarios. The experimental results demonstrate its superior macro F1 performance compared to baseline models. We further showed that the structural advantage of RNDR is more pronounced in compound fault settings through a single-fault comparison. We also found that frequency-layer normalization fits the fault diagnosis task better than conventional methods. Lastly, we analyzed the RNDR with various conditions, other models with increased number of parameters, and compared with the ablated RNDR structure.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport</title>
<link>https://arxiv.org/abs/2505.24759</link>
<guid>https://arxiv.org/abs/2505.24759</guid>
<content:encoded><![CDATA[
arXiv:2505.24759v2 Announce Type: replace-cross 
Abstract: Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches often rely on either reference-based matching, which imposes asymmetry by designating one species as the reference, or projection-based matching, which may increase computational complexity and obscure biological interpretability at the cell-type level. Here, we present OT-MESH, an unsupervised computational framework leveraging entropy-regularized optimal transport (OT) to systematically determine cross-species cell type homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan, transforming diffuse transport matrices into sparse, interpretable correspondences. Through systematic evaluation on synthetic datasets, we demonstrate that OT-MESH achieves near-optimal matching accuracy with computational efficiency, while maintaining remarkable robustness to noise. Compared to other OT-based methods like RefCM, OT-MESH provides speedup while achieving comparable accuracy. Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally. Thus, our framework offers a principled, scalable, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets Credit or Blame? Attributing Accountability in Modern AI Systems</title>
<link>https://arxiv.org/abs/2506.00175</link>
<guid>https://arxiv.org/abs/2506.00175</guid>
<content:encoded><![CDATA[
arXiv:2506.00175v3 Announce Type: replace-cross 
Abstract: Modern AI systems are typically developed through multiple stages-pretraining, fine-tuning rounds, and subsequent adaptation or alignment, where each stage builds on the previous ones and updates the model in distinct ways. This raises a critical question of accountability: when a deployed model succeeds or fails, which stage is responsible, and to what extent? We pose the accountability attribution problem for tracing model behavior back to specific stages of the model development process. To address this challenge, we propose a general framework that answers counterfactual questions about stage effects: how would the model's behavior have changed if the updates from a particular stage had not occurred? Within this framework, we introduce estimators that efficiently quantify stage effects without retraining the model, accounting for both the data and key aspects of model optimization dynamics, including learning rate schedules, momentum, and weight decay. We demonstrate that our approach successfully quantifies the accountability of each stage to the model's behavior. Based on the attribution results, our method can identify and remove spurious correlations learned during image classification and text toxicity detection tasks that were developed across multiple stages. Our approach provides a practical tool for model analysis and represents a significant step toward more accountable AI development.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.07040</link>
<guid>https://arxiv.org/abs/2506.07040</guid>
<content:encoded><![CDATA[
arXiv:2506.07040v2 Announce Type: replace-cross 
Abstract: We present a non-asymptotic convergence analysis of $Q$-learning and actor-critic algorithms for robust average-reward Markov Decision Processes (MDPs) under contamination, total-variation (TV) distance, and Wasserstein uncertainty sets. A key ingredient of our analysis is showing that the optimal robust $Q$ operator is a strict contraction with respect to a carefully designed semi-norm (with constant functions quotiented out). This property enables a stochastic approximation update that learns the optimal robust $Q$-function using $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We also provide an efficient routine for robust $Q$-function estimation, which in turn facilitates robust critic estimation. Building on this, we introduce an actor-critic algorithm that learns an $\epsilon$-optimal robust policy within $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We provide numerical simulations to evaluate the performance of our algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws of Motion Forecasting and Planning - Technical Report</title>
<link>https://arxiv.org/abs/2506.08228</link>
<guid>https://arxiv.org/abs/2506.08228</guid>
<content:encoded><![CDATA[
arXiv:2506.08228v2 Announce Type: replace-cross 
Abstract: We study the empirical scaling laws of a family of encoder-decoder autoregressive transformer models on the task of joint motion forecasting and planning in the autonomous driving domain. Using a 500 thousand hours driving dataset, we demonstrate that, similar to language modeling, model performance improves as a power-law function of the total compute budget, and we observe a strong correlation between model training loss and model evaluation metrics. Most interestingly, closed-loop metrics also improve with scaling, which has important implications for the suitability of open-loop metrics for model development and hill climbing. We also study the optimal scaling of the number of transformer parameters and the training data size for a training compute-optimal model. We find that as the training compute budget grows, optimal scaling requires increasing the model size 1.5x as fast as the dataset size. We also study inference-time compute scaling, where we observe that sampling and clustering the output of smaller models makes them competitive with larger models, up to a crossover point beyond which a larger models becomes more inference-compute efficient. Overall, our experimental results demonstrate that optimizing the training and inference-time scaling properties of motion forecasting and planning models is a key lever for improving their performance to address a wide variety of driving scenarios. Finally, we briefly study the utility of training on general logged driving data of other agents to improve the performance of the ego-agent, an important research area to address the scarcity of robotics data for large capacity models training.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations</title>
<link>https://arxiv.org/abs/2506.09383</link>
<guid>https://arxiv.org/abs/2506.09383</guid>
<content:encoded><![CDATA[
arXiv:2506.09383v2 Announce Type: replace-cross 
Abstract: Balance control is important for human and bipedal robotic systems. While dynamic balance during locomotion has received considerable attention, quantitative understanding of static balance and falling remains limited. This work presents a hierarchical control pipeline for simulating human balance via a comprehensive whole-body musculoskeletal system. We identified spatiotemporal dynamics of balancing during stable standing, revealed the impact of muscle injury on balancing behavior, and generated fall contact patterns that aligned with clinical data. Furthermore, our simulated hip exoskeleton assistance demonstrated improvement in balance maintenance and reduced muscle effort under perturbation. This work offers unique muscle-level insights into human balance dynamics that are challenging to capture experimentally. It could provide a foundation for developing targeted interventions for individuals with balance impairments and support the advancement of humanoid robotic systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models</title>
<link>https://arxiv.org/abs/2506.11798</link>
<guid>https://arxiv.org/abs/2506.11798</guid>
<content:encoded><![CDATA[
arXiv:2506.11798v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse, but have been found to consistently display a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups that the base model is not aligned with. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict positions of European groups on a diverse set of policies. We evaluate if predictions are stable towards counterfactual arguments, different persona prompts and generation methods. Finally, we find that we can simulate voting behavior of Members of the European Parliament reasonably well with a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at https://github.com/dess-mannheim/european_parliament_simulation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies</title>
<link>https://arxiv.org/abs/2506.11948</link>
<guid>https://arxiv.org/abs/2506.11948</guid>
<content:encoded><![CDATA[
arXiv:2506.11948v2 Announce Type: replace-cross 
Abstract: Offline Imitation Learning (IL) methods such as Behavior Cloning are effective at acquiring complex robotic manipulation skills. However, existing IL-trained policies are confined to executing the task at the same speed as shown in demonstration data. This limits the task throughput of a robotic system, a critical requirement for applications such as industrial automation. In this paper, we introduce and formalize the novel problem of enabling faster-than-demonstration execution of visuomotor policies and identify fundamental challenges in robot dynamics and state-action distribution shifts. We instantiate the key insights as SAIL (Speed Adaptation for Imitation Learning), a full-stack system integrating four tightly-connected components: (1) a consistency-preserving action inference algorithm for smooth motion at high speed, (2) high-fidelity tracking of controller-invariant motion targets, (3) adaptive speed modulation that dynamically adjusts execution speed based on motion complexity, and (4) action scheduling to handle real-world system latencies. Experiments on 12 tasks across simulation and two real, distinct robot platforms show that SAIL achieves up to a 4x speedup over demonstration speed in simulation and up to 3.2x speedup in the real world. Additional detail is available at https://nadunranawaka1.github.io/sail-policy
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems</title>
<link>https://arxiv.org/abs/2506.14096</link>
<guid>https://arxiv.org/abs/2506.14096</guid>
<content:encoded><![CDATA[
arXiv:2506.14096v2 Announce Type: replace-cross 
Abstract: The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Intelligence: Designing Data Centers for Next-Gen Language Models</title>
<link>https://arxiv.org/abs/2506.15006</link>
<guid>https://arxiv.org/abs/2506.15006</guid>
<content:encoded><![CDATA[
arXiv:2506.15006v3 Announce Type: replace-cross 
Abstract: The explosive growth of Large Language Models (LLMs), such as GPT-4 with 1.8 trillion parameters, demands a fundamental rethinking of data center architecture to ensure scalability, efficiency, and cost-effectiveness. Our work provides a comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat optical), the size of the scale-out domain, and popular parallelism/optimization strategies used in LLMs. We introduce and evaluate FullFlat network architectures, which provide uniform high-bandwidth, low-latency connectivity between all nodes, and demonstrate their transformative impact on performance and scalability. Through detailed sensitivity analyses, we quantify the benefits of overlapping compute and communication, leveraging hardware-accelerated collectives, widening the scale-out domain, and increasing memory capacity. Our study spans both sparse (mixture of experts) and dense transformer-based LLMs, revealing how system design choices affect Model FLOPS Utilization (MFU = Model FLOPS per token * Observed tokens per second / Peak FLOPS of the hardware) and overall throughput. For the co-design study, we utilized an analytical performance modeling tool capable of predicting LLM runtime within 10% of real-world measurements. Our findings offer actionable insights and a practical roadmap for designing AI data centers that can efficiently support trillion-parameter models, reduce optimization complexity, and sustain the rapid evolution of AI capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transit for All: Mapping Equitable Bike2Subway Connection using Region Representation Learning</title>
<link>https://arxiv.org/abs/2506.15113</link>
<guid>https://arxiv.org/abs/2506.15113</guid>
<content:encoded><![CDATA[
arXiv:2506.15113v2 Announce Type: replace-cross 
Abstract: Ensuring equitable public transit access remains challenging, particularly in densely populated cities like New York City (NYC), where low-income and minority communities often face limited transit accessibility. Bike-sharing systems (BSS) can bridge these equity gaps by providing affordable first- and last-mile connections. However, strategically expanding BSS into underserved neighborhoods is difficult due to uncertain bike-sharing demand at newly planned ("cold-start") station locations and limitations in traditional accessibility metrics that may overlook realistic bike usage potential. We introduce Transit for All (TFA), a spatial computing framework designed to guide the equitable expansion of BSS through three components: (1) spatially-informed bike-sharing demand prediction at cold-start stations using region representation learning that integrates multimodal geospatial data, (2) comprehensive transit accessibility assessment leveraging our novel weighted Public Transport Accessibility Level (wPTAL) by combining predicted bike-sharing demand with conventional transit accessibility metrics, and (3) strategic recommendations for new bike station placements that consider potential ridership and equity enhancement. Using NYC as a case study, we identify transit accessibility gaps that disproportionately impact low-income and minority communities in historically underserved neighborhoods. Our results show that strategically placing new stations guided by wPTAL notably reduces disparities in transit access related to economic and demographic factors. From our study, we demonstrate that TFA provides practical guidance for urban planners to promote equitable transit and enhance the quality of life in underserved urban communities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precise Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2506.19726</link>
<guid>https://arxiv.org/abs/2506.19726</guid>
<content:encoded><![CDATA[
arXiv:2506.19726v2 Announce Type: replace-cross 
Abstract: Despite its long history, Bayesian neural networks (BNNs) and variational training remain underused in practice: standard Gaussian posteriors misalign with network geometry, KL terms can be brittle in high dimensions, and implementations often add complexity without reliably improving uncertainty. We revisit the problem through the lens of normalization. Because normalization layers neutralize the influence of weight magnitude, we model uncertainty \emph{only in weight directions} using a von Mises-Fisher posterior on the unit sphere. High-dimensional geometry then yields a single, interpretable scalar per layer--the effective post-normalization noise $\sigma_{\mathrm{eff}}$--that (i) corresponds to simple additive Gaussian noise in the forward pass and (ii) admits a compact, dimension-aware KL in closed form. We derive accurate, closed-form approximations linking concentration $\kappa$ to activation variance and to $\sigma_{\mathrm{eff}}$ across regimes, producing a lightweight, implementation-ready variational unit that fits modern normalized architectures and improves calibration without sacrificing accuracy. This dimension awareness is critical for stable optimization in high dimensions. In short, by aligning the variational posterior with the network's intrinsic geometry, BNNs can be simultaneously principled, practical, and precise.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs</title>
<link>https://arxiv.org/abs/2506.22146</link>
<guid>https://arxiv.org/abs/2506.22146</guid>
<content:encoded><![CDATA[
arXiv:2506.22146v3 Announce Type: replace-cross 
Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual reasoning is often limited by the binding problem: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current VLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention. This paper introduces VISER (Visual Input Structure for Enhanced Reasoning), a simple yet effective intervention: augmenting visual inputs with low-level spatial structures and pairing this with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks. Specifically, VISER improves GPT-4o visual search accuracy by 25.00%, increases counting accuracy by 26.83%, reduces edit distance error in scene description by 0.32, and enhances performance on spatial relationship tasks by 9.50% on a 2D synthetic dataset. Furthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. VISER enhances binding only with a single-query inference, underscoring the importance of visual input design over purely linguistically-based approaches. These findings suggest that low-level visual structuring is a powerful and underexplored direction for improving compositional visual reasoning and could serve as a general strategy for enhancing VLM performance on spatially grounded tasks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Out-of-Distribution Unlabeled Images: Semi-Supervised Semantic Segmentation with an Open-Vocabulary Model</title>
<link>https://arxiv.org/abs/2507.03302</link>
<guid>https://arxiv.org/abs/2507.03302</guid>
<content:encoded><![CDATA[
arXiv:2507.03302v2 Announce Type: replace-cross 
Abstract: In semi-supervised semantic segmentation, existing studies have shown promising results in academic settings with controlled splits of benchmark datasets. However, the potential benefits of leveraging significantly larger sets of unlabeled images remain unexplored. In real-world scenarios, abundant unlabeled images are often available from online sources (web-scraped images) or large-scale datasets. However, these images may have different distributions from those of the target dataset, a situation known as out-of-distribution (OOD). Using these images as unlabeled data in semi-supervised learning can lead to inaccurate pseudo-labels, potentially misguiding network training. In this paper, we propose a new semi-supervised semantic segmentation framework with an open-vocabulary segmentation model (SemiOVS) to effectively utilize unlabeled OOD images. Extensive experiments on Pascal VOC and Context datasets demonstrate two key findings: (1) using additional unlabeled images improves the performance of semi-supervised learners in scenarios with few labels, and (2) using the open-vocabulary segmentation (OVS) model to pseudo-label OOD images leads to substantial performance gains. In particular, SemiOVS outperforms existing PrevMatch and SemiVL methods by +3.5 and +3.0 mIoU, respectively, on Pascal VOC with a 92-label setting, achieving state-of-the-art performance. These findings demonstrate that our approach effectively utilizes abundant unlabeled OOD images for semantic segmentation tasks. We hope this work can inspire future research and real-world applications. The code is available at https://github.com/wooseok-shin/SemiOVS
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles</title>
<link>https://arxiv.org/abs/2507.04139</link>
<guid>https://arxiv.org/abs/2507.04139</guid>
<content:encoded><![CDATA[
arXiv:2507.04139v2 Announce Type: replace-cross 
Abstract: Ensuring safe transition of control in automated vehicles requires an accurate and timely assessment of driver readiness. This paper introduces Driver-Net, a novel deep learning framework that fuses multi-camera inputs to estimate driver take-over readiness. Unlike conventional vision-based driver monitoring systems that focus on head pose or eye gaze, Driver-Net captures synchronised visual cues from the driver's head, hands, and body posture through a triple-camera setup. The model integrates spatio-temporal data using a dual-path architecture, comprising a Context Block and a Feature Block, followed by a cross-modal fusion strategy to enhance prediction accuracy. Evaluated on a diverse dataset collected from the University of Leeds Driving Simulator, the proposed method achieves an accuracy of up to 95.8% in driver readiness classification. This performance significantly enhances existing approaches and highlights the importance of multimodal and multi-view fusion. As a real-time, non-intrusive solution, Driver-Net contributes meaningfully to the development of safer and more reliable automated vehicles and aligns with new regulatory mandates and upcoming safety standards.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLAME: Lightweight MSA Design Advances Protein Folding From Evolutionary Embeddings</title>
<link>https://arxiv.org/abs/2507.07032</link>
<guid>https://arxiv.org/abs/2507.07032</guid>
<content:encoded><![CDATA[
arXiv:2507.07032v2 Announce Type: replace-cross 
Abstract: Protein structure prediction often hinges on multiple sequence alignments (MSAs), which underperform on low-homology and orphan proteins. We introduce PLAME, a lightweight MSA design framework that leverages evolutionary embeddings from pretrained protein language models to generate MSAs that better support downstream folding. PLAME couples these embeddings with a conservation-diversity loss that balances agreement on conserved positions with coverage of plausible sequence variation. Beyond generation, we develop (i) an MSA selection strategy to filter high-quality candidates and (ii) a sequence-quality metric that is complementary to depth-based measures and predictive of folding gains. On AlphaFold2 low-homology/orphan benchmarks, PLAME delivers state-of-the-art improvements in structure accuracy (e.g., lDDT/TM-score), with consistent gains when paired with AlphaFold3. Ablations isolate the benefits of the selection strategy, and case studies elucidate how MSA characteristics shape AlphaFold confidence and error modes. Finally, we show PLAME functions as a lightweight adapter, enabling ESMFold to approach AlphaFold2-level accuracy while retaining ESMFold-like inference speed. PLAME thus provides a practical path to high-quality folding for proteins lacking strong evolutionary neighbors.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias</title>
<link>https://arxiv.org/abs/2507.11210</link>
<guid>https://arxiv.org/abs/2507.11210</guid>
<content:encoded><![CDATA[
arXiv:2507.11210v2 Announce Type: replace-cross 
Abstract: Well-being in family settings involves subtle psychological dynamics that conventional metrics often overlook. In particular, unconscious parental expectations, termed ideal parent bias, can suppress children's emotional expression and autonomy. This suppression, referred to as suppressed emotion, often stems from well-meaning but value-driven communication, which is difficult to detect or address from outside the family. Focusing on these latent dynamics, this study explores Large Language Model (LLM)-based support for psychologically safe family communication. We constructed a Japanese parent-child dialogue corpus of 30 scenarios, each annotated with metadata on ideal parent bias and suppressed emotion. Based on this corpus, we developed a Role-Playing LLM-based multi-agent dialogue support framework that analyzes dialogue and generates feedback. Specialized agents detect suppressed emotion, describe implicit ideal parent bias in parental speech, and infer contextual attributes such as the child's age and background. A meta-agent compiles these outputs into a structured report, which is then passed to five selected expert agents. These agents collaboratively generate empathetic and actionable feedback through a structured four-step discussion process. Experiments show that the system can detect categories of suppressed emotion with moderate accuracy and produce feedback rated highly in empathy and practicality. Moreover, simulated follow-up dialogues incorporating this feedback exhibited signs of improved emotional expression and mutual understanding, suggesting the framework's potential in supporting positive transformation in family interactions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)</title>
<link>https://arxiv.org/abs/2507.12856</link>
<guid>https://arxiv.org/abs/2507.12856</guid>
<content:encoded><![CDATA[
arXiv:2507.12856v2 Announce Type: replace-cross 
Abstract: Behavior Cloning (BC) on curated (or filtered) data is the predominant paradigm for supervised fine-tuning (SFT) of large language models; as well as for imitation learning of control policies. Here, we draw on a connection between this successful strategy and the theory and practice of finding optimal policies via Reinforcement Learning (RL). Building on existing literature, we clarify that SFT can be understood as maximizing a lower bound on the RL objective in a sparse reward setting. Giving support to its often observed good performance. From this viewpoint, we realize that a small modification to SFT leads to an importance weighted variant that behaves closer to training with RL as it: i) optimizes a tighter bound to the RL objective and, ii) can improve performance compared to SFT on curated data. We refer to this variant as importance weighted supervised fine-tuning (iw-SFT). We show that it is easy to implement and can be further generalized to training with quality scored data. The resulting SFT variants are competitive with more advanced RL algorithms for large language models and for training policies in continuous control tasks. For example achieving 66.7% on the AIME 2024 dataset.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RecPS: Privacy Risk Scoring for Recommender Systems</title>
<link>https://arxiv.org/abs/2507.18365</link>
<guid>https://arxiv.org/abs/2507.18365</guid>
<content:encoded><![CDATA[
arXiv:2507.18365v4 Announce Type: replace-cross 
Abstract: Recommender systems (RecSys) have become an essential component of many web applications. The core of the system is a recommendation model trained on highly sensitive user-item interaction data. While privacy-enhancing techniques are actively studied in the research community, the real-world model development still depends on minimal privacy protection, e.g., via controlled access. Users of such systems should have the right to choose \emph{not} to share highly sensitive interactions. However, there is no method allowing the user to know which interactions are more sensitive than others. Thus, quantifying the privacy risk of RecSys training data is a critical step to enabling privacy-aware RecSys model development and deployment. We propose a membership-inference attack (MIA)- based privacy scoring method, RecPS, to measure privacy risks at both the interaction and user levels. The RecPS interaction-level score definition is motivated and derived from differential privacy, which is then extended to the user-level scoring method. A critical component is the interaction-level MIA method RecLiRA, which gives high-quality membership estimation. We have conducted extensive experiments on well-known benchmark datasets and RecSys models to show the unique features and benefits of RecPS scoring in risk assessment and RecSys model unlearning.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning</title>
<link>https://arxiv.org/abs/2508.00716</link>
<guid>https://arxiv.org/abs/2508.00716</guid>
<content:encoded><![CDATA[
arXiv:2508.00716v3 Announce Type: replace-cross 
Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled source graphs to unlabeled target graphs by learning domain-invariant representations, which is essential in applications such as molecular property prediction and social network analysis. However, most existing GDA methods rely on the assumption of clean source labels, which rarely holds in real-world scenarios where annotation noise is pervasive. This label noise severely impairs feature alignment and degrades adaptation performance under domain shifts. To address this challenge, we propose Nested Graph Pseudo-Label Refinement (NeGPR), a novel framework tailored for graph-level domain adaptation with noisy labels. NeGPR first pretrains dual branches, i.e., semantic and topology branches, by enforcing neighborhood consistency in the feature space, thereby reducing the influence of noisy supervision. To bridge domain gaps, NeGPR employs a nested refinement mechanism in which one branch selects high-confidence target samples to guide the adaptation of the other, enabling progressive cross-domain learning. Furthermore, since pseudo-labels may still contain noise and the pre-trained branches are already overfitted to the noisy labels in the source domain, NeGPR incorporates a noise-aware regularization strategy. This regularization is theoretically proven to mitigate the adverse effects of pseudo-label noise, even under the presence of source overfitting, thus enhancing the robustness of the adaptation process. Extensive experiments on benchmark datasets demonstrate that NeGPR consistently outperforms state-of-the-art methods under severe label noise, achieving gains of up to 12.7% in accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA</title>
<link>https://arxiv.org/abs/2508.00719</link>
<guid>https://arxiv.org/abs/2508.00719</guid>
<content:encoded><![CDATA[
arXiv:2508.00719v3 Announce Type: replace-cross 
Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning</title>
<link>https://arxiv.org/abs/2508.01131</link>
<guid>https://arxiv.org/abs/2508.01131</guid>
<content:encoded><![CDATA[
arXiv:2508.01131v2 Announce Type: replace-cross 
Abstract: In this work, we study the problem of data retrieval for few-shot imitation learning: selecting data from a large dataset to train a performant policy for a specific task, given only a few target demonstrations. Prior methods retrieve data using a single-feature distance heuristic, assuming that the best demonstrations are those that most closely resemble the target examples in visual, semantic, or motion space. However, this approach captures only a subset of the relevant information and can introduce detrimental demonstrations, e.g., retrieving data from unrelated tasks due to similar scene layouts, or selecting similar motions from tasks with divergent goals. We present COLLAGE, a method for COLLective data AGgrEgation in few-shot imitation learning that uses an adaptive late fusion mechanism to guide the selection of relevant demonstrations based on a task-specific combination of multiple cues. COLLAGE follows a simple, flexible, and efficient recipe: it assigns weights to subsets of the dataset that are pre-selected using a single feature (e.g., appearance, shape, or language similarity), based on how well a policy trained on each subset predicts actions in the target demonstrations. These weights are then used to perform importance sampling during policy training, sampling data more densely or sparsely according to estimated relevance. COLLAGE is general and feature-agnostic, allowing it to combine any number of subsets selected by any retrieval heuristic, and to identify which subsets provide the greatest benefit for the target task. In extensive experiments, COLLAGE outperforms state-of-the-art retrieval and multi-task learning approaches by 5.1% in simulation across 10 tasks, and by 16.6% in the real world across 6 tasks, where we perform retrieval from the large-scale DROID dataset. More information at https://robin-lab.cs.utexas.edu/COLLAGE .
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIRF: A Framework for Digital Identity Protection and Clone Governance in Agentic AI Systems</title>
<link>https://arxiv.org/abs/2508.01997</link>
<guid>https://arxiv.org/abs/2508.01997</guid>
<content:encoded><![CDATA[
arXiv:2508.01997v2 Announce Type: replace-cross 
Abstract: The rapid advancement and widespread adoption of generative artificial intelligence (AI) pose significant threats to the integrity of personal identity, including digital cloning, sophisticated impersonation, and the unauthorized monetization of identity-related data. Mitigating these risks necessitates the development of robust AI-generated content detection systems, enhanced legal frameworks, and ethical guidelines. This paper introduces the Digital Identity Rights Framework (DIRF), a structured security and governance model designed to protect behavioral, biometric, and personality-based digital likeness attributes to address this critical need. Structured across nine domains and 63 controls, DIRF integrates legal, technical, and hybrid enforcement mechanisms to secure digital identity consent, traceability, and monetization. We present the architectural foundations, enforcement strategies, and key use cases supporting the need for a unified framework. This work aims to inform platform builders, legal entities, and regulators about the essential controls needed to enforce identity rights in AI-driven systems.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Continuous-Time MILP for Integrated Aircraft Hangar Scheduling and Layout</title>
<link>https://arxiv.org/abs/2508.02640</link>
<guid>https://arxiv.org/abs/2508.02640</guid>
<content:encoded><![CDATA[
arXiv:2508.02640v2 Announce Type: replace-cross 
Abstract: Efficient management of aircraft MRO hangars requires the integration of spatial layout with time-continuous scheduling to minimize operational costs. We propose a continuous-time mixed-integer linear program that jointly optimizes aircraft placement and timing, overcoming the scalability limits of prior formulations. A comprehensive study benchmarks the model against a constructive heuristic, probes large-scale performance, and quantifies its sensitivity to temporal congestion. The model achieves orders-of-magnitude speedups on benchmarks from the literature, solving a long-standing congested instance in 0.11 seconds, and finds proven optimal solutions for instances with up to 40 aircraft. Within a one-hour limit for large-scale problems, the model finds solutions with small optimality gaps for instances up to 80 aircraft and provides strong bounds for problems with up to 160 aircraft. Optimized plans consistently increase hangar throughput (e.g., +33% serviced aircraft vs. a heuristic on instance RND-N030-I03), leading to lower delay penalties and higher asset utilization. These findings establish that exact optimization has become computationally viable for large-scale hangar planning, providing a validated tool that balances solution quality and computation time for strategic and operational decisions.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion</title>
<link>https://arxiv.org/abs/2508.05264</link>
<guid>https://arxiv.org/abs/2508.05264</guid>
<content:encoded><![CDATA[
arXiv:2508.05264v2 Announce Type: replace-cross 
Abstract: Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion model's coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at https://github.com/boshizhang123/SGDFuse.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding</title>
<link>https://arxiv.org/abs/2508.06869</link>
<guid>https://arxiv.org/abs/2508.06869</guid>
<content:encoded><![CDATA[
arXiv:2508.06869v2 Announce Type: replace-cross 
Abstract: Long video understanding presents a significant challenge to multimodal large language models (MLLMs) primarily due to the immense data scale. A critical and widely adopted strategy for making this task computationally tractable is keyframe retrieval, which seeks to identify a sparse set of video frames that are most salient to a given textual query. However, the efficacy of this approach is hindered by weak multimodal alignment between textual queries and visual content and fails to capture the complex temporal semantic information required for precise reasoning. To address this, we propose Visual-Subtitle Integeration(VSI), a multimodal keyframe search method that integrates subtitles, timestamps, and scene boundaries into a unified multimodal search process. The proposed method captures the visual information of video frames as well as the complementary textual information through a dual-stream search mechanism by Video Search Stream as well as Subtitle Match Stream, respectively, and improves the keyframe search accuracy through the interaction of the two search streams. Experimental results show that VSI achieve 40.00% key frame localization accuracy on the text-relevant subset of LongVideoBench and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive baselines by 20.35% and 15.79%, respectively. Furthermore, on the LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA tasks, demonstrating the robustness and generalizability of the proposed multimodal search strategy.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures</title>
<link>https://arxiv.org/abs/2508.07423</link>
<guid>https://arxiv.org/abs/2508.07423</guid>
<content:encoded><![CDATA[
arXiv:2508.07423v3 Announce Type: replace-cross 
Abstract: As the particle physics community needs higher and higher precisions in order to test our current model of the subatomic world, larger and larger datasets are necessary. With upgrades scheduled for the detectors of colliding-beam experiments around the world, and specifically at the Large Hadron Collider at CERN, more collisions and more complex interactions are expected. This directly implies an increase in data produced and consequently in the computational resources needed to process them. At CERN, the amount of data produced is gargantuan. This is why the data have to be heavily filtered and selected in real time before being permanently stored. This data can then be used to perform physics analyses, in order to expand our current understanding of the universe and improve the Standard Model of physics. This real-time filtering, known as triggering, involves complex processing happening often at frequencies as high as 40 MHz. This thesis contributes to understanding how machine learning models can be efficiently deployed in such environments, in order to maximize throughput and minimize energy consumption. Inevitably, modern hardware designed for such tasks and contemporary algorithms are needed in order to meet the challenges posed by the stringent, high-frequency data rates. In this work, I present our graph neural network-based pipeline, developed for charged particle track reconstruction at the LHCb experiment at CERN. The pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely on GPUs. Its performance was compared against the classical tracking algorithms currently in production at LHCb. The pipeline was also accelerated on the FPGA architecture, and its performance in terms of power consumption and processing speed was compared against the GPU implementation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning</title>
<link>https://arxiv.org/abs/2508.07556</link>
<guid>https://arxiv.org/abs/2508.07556</guid>
<content:encoded><![CDATA[
arXiv:2508.07556v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes domains where reliability is paramount. This thesis investigates how uncertainty estimation can enhance the safety and trustworthiness of ML, focusing on selective prediction -- where models abstain when confidence is low.
  We first show that a model's training trajectory contains rich uncertainty signals that can be exploited without altering its architecture or loss. By ensembling predictions from intermediate checkpoints, we propose a lightweight, post-hoc abstention method that works across tasks, avoids the cost of deep ensembles, and achieves state-of-the-art selective prediction performance. Crucially, this approach is fully compatible with differential privacy (DP), allowing us to study how privacy noise affects uncertainty quality. We find that while many methods degrade under DP, our trajectory-based approach remains robust, and we introduce a framework for isolating the privacy-uncertainty trade-off. Next, we then develop a finite-sample decomposition of the selective classification gap -- the deviation from the oracle accuracy-coverage curve -- identifying five interpretable error sources and clarifying which interventions can close the gap. This explains why calibration alone cannot fix ranking errors, motivating methods that improve uncertainty ordering. Finally, we show that uncertainty signals can be adversarially manipulated to hide errors or deny service while maintaining high accuracy, and we design defenses combining calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating, and safeguarding uncertainty estimation, enabling models that not only make accurate predictions -- but also know when to say "I do not know".
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery</title>
<link>https://arxiv.org/abs/2508.09183</link>
<guid>https://arxiv.org/abs/2508.09183</guid>
<content:encoded><![CDATA[
arXiv:2508.09183v2 Announce Type: replace-cross 
Abstract: Quantum computation has demonstrated a promising alternative to solving the NP-hard combinatorial problems. Specifically, when it comes to optimization, classical approaches become intractable to account for large-scale solutions. Specifically, we investigate quantum computing to solve the large-scale Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this regard, a Reinforcement Learning (RL) framework augmented with a Parametrized Quantum Circuit (PQC) is designed to minimize the travel time in a realistic last-mile on-demand delivery. A novel problem-specific encoding quantum circuit with an entangling and variational layer is proposed. Moreover, Proximal Policy Optimization (PPO) and Quantum Singular Value Transformation (QSVT) are designed for comparison through numerical experiments, highlighting the superiority of the proposed method in terms of the scale of the solution and training complexity while incorporating the real-world constraints.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Discrepancy-aware Detector for Image Forgery Identification</title>
<link>https://arxiv.org/abs/2508.12341</link>
<guid>https://arxiv.org/abs/2508.12341</guid>
<content:encoded><![CDATA[
arXiv:2508.12341v2 Announce Type: replace-cross 
Abstract: With the rapid advancement of image generation techniques, robust forgery detection has become increasingly imperative to ensure the trustworthiness of digital media. Recent research indicates that the learned semantic concepts of pre-trained models are critical for identifying fake images. However, the misalignment between the forgery and semantic concept spaces hinders the model's forgery detection performance. To address this problem, we propose a novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction learning to align the two spaces at a fine-grained visual level. By exploiting the conceptual knowledge embedded in the pre-trained vision language model, we specifically design a semantic token sampling module to mitigate the space shifts caused by features irrelevant to both forgery traces and semantic concepts. A concept-level forgery discrepancy learning module, built upon a visual reconstruction paradigm, is proposed to strengthen the interaction between visual semantic concepts and forgery traces, effectively capturing discrepancies under the concepts' guidance. Finally, the low-level forgery feature enhancemer integrates the learned concept level forgery discrepancies to minimize redundant forgery information. Experiments conducted on two standard image forgery datasets demonstrate the efficacy of the proposed SDD, which achieves superior results compared to existing methods. The code is available at https://github.com/wzy1111111/SSD.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Artificial Intuition in Distinct, Minimalist Classification of Scientific Abstracts for Management of Technology Portfolios</title>
<link>https://arxiv.org/abs/2508.13182</link>
<guid>https://arxiv.org/abs/2508.13182</guid>
<content:encoded><![CDATA[
arXiv:2508.13182v2 Announce Type: replace-cross 
Abstract: Classification of scientific abstracts is useful for strategic activities but challenging to automate because the sparse text provides few contextual clues. Metadata associated with the scientific publication can be used to improve performance but still often requires a semi-supervised setting. Moreover, such schemes may generate labels that lack distinction -- namely, they overlap and thus do not uniquely define the abstract. In contrast, experts label and sort these texts with ease. Here we describe an application of a process we call artificial intuition to replicate the expert's approach, using a Large Language Model (LLM) to generate metadata. We use publicly available abstracts from the United States National Science Foundation to create a set of labels, and then we test this on a set of abstracts from the Chinese National Natural Science Foundation to examine funding trends. We demonstrate the feasibility of this method for research portfolio management, technology scouting, and other strategic activities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering</title>
<link>https://arxiv.org/abs/2508.14052</link>
<guid>https://arxiv.org/abs/2508.14052</guid>
<content:encoded><![CDATA[
arXiv:2508.14052v3 Announce Type: replace-cross 
Abstract: Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods-whether sparse or dense-often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance -- a setting we term agentic retrieval. The benchmark consists of 3,429 expert-annotated examples on S&amp;P-100 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Generalization and Personalization in Human Activity Recognition via On-Device Few-Shot Learning</title>
<link>https://arxiv.org/abs/2508.15413</link>
<guid>https://arxiv.org/abs/2508.15413</guid>
<content:encoded><![CDATA[
arXiv:2508.15413v3 Announce Type: replace-cross 
Abstract: Human Activity Recognition (HAR) with different sensing modalities requires both strong generalization across diverse users and efficient personalization for individuals. However, conventional HAR models often fail to generalize when faced with user-specific variations, leading to degraded performance. To address this challenge, we propose a novel on-device few-shot learning framework that bridges generalization and personalization in HAR. Our method first trains a generalizable representation across users and then rapidly adapts to new users with only a few labeled samples, updating lightweight classifier layers directly on resource-constrained devices. This approach achieves robust on-device learning with minimal computation and memory cost, making it practical for real-world deployment. We implement our framework on the energy-efficient RISC-V GAP9 microcontroller and evaluate it on three benchmark datasets (RecGym, QVAR-Gesture, Ultrasound-Gesture). Across these scenarios, post-deployment adaptation improves accuracy by 3.73\%, 17.38\%, and 3.70\%, respectively. These results demonstrate that few-shot on-device learning enables scalable, user-aware, and energy-efficient wearable human activity recognition by seamlessly uniting generalization and personalization. The related framework is open sourced for further research\footnote{https://github.com/kangpx/onlineTiny2023}.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning</title>
<link>https://arxiv.org/abs/2508.15868</link>
<guid>https://arxiv.org/abs/2508.15868</guid>
<content:encoded><![CDATA[
arXiv:2508.15868v2 Announce Type: replace-cross 
Abstract: Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \TheName{} in terms of robustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Code is available at https://github.com/WNQzhu/CARFT.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search</title>
<link>https://arxiv.org/abs/2508.15884</link>
<guid>https://arxiv.org/abs/2508.15884</guid>
<content:encoded><![CDATA[
arXiv:2508.15884v2 Announce Type: replace-cross 
Abstract: We present Jet-Nemotron, a new family of hybrid-architecture language models, which matches or exceeds the accuracy of leading full-attention models while significantly improving generation throughput. Jet-Nemotron is developed using Post Neural Architecture Search (PostNAS), a novel neural architecture exploration pipeline that enables efficient model design. Unlike prior approaches, PostNAS begins with a pre-trained full-attention model and freezes its MLP weights, allowing efficient exploration of attention block designs. The pipeline includes four key components: (1) learning optimal full-attention layer placement and elimination, (2) linear attention block selection, (3) designing new attention blocks, and (4) performing hardware-aware hyperparameter search. Our Jet-Nemotron-2B model achieves comparable or superior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a comprehensive suite of benchmarks while delivering up to 53.6x generation throughput speedup and 6.1x prefilling speedup. It also achieves higher accuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models, such as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B total and 2.2B activated parameters.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence and Generalization of Anti-Regularization for Parametric Models</title>
<link>https://arxiv.org/abs/2508.17412</link>
<guid>https://arxiv.org/abs/2508.17412</guid>
<content:encoded><![CDATA[
arXiv:2508.17412v2 Announce Type: replace-cross 
Abstract: Anti-regularization introduces a reward term with a reversed sign into the loss function, deliberately amplifying model expressivity in small-sample regimes while ensuring that the intervention gradually vanishes as the sample size grows through a power-law decay schedule. We formalize spectral safety conditions and trust-region constraints, and we design a lightweight safeguard that combines a projection operator with gradient clipping to guarantee stable intervention. Theoretical analysis extends to linear smoothers and the Neural Tangent Kernel regime, providing practical guidance on the choice of decay exponents through the balance between empirical risk and variance. Empirical results show that Anti-regularization mitigates underfitting in both regression and classification while preserving generalization and improving calibration. Ablation studies confirm that the decay schedule and safeguards are essential to avoiding overfitting and instability. As an alternative, we also propose a degrees-of-freedom targeting schedule that maintains constant per-sample complexity. Anti-regularization constitutes a simple and reproducible procedure that integrates seamlessly into standard empirical risk minimization pipelines, enabling robust learning under limited data and resource constraints by intervening only when necessary and vanishing otherwise.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Expectation Policy Optimization for Heterogeneous Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.17850</link>
<guid>https://arxiv.org/abs/2508.17850</guid>
<content:encoded><![CDATA[
arXiv:2508.17850v2 Announce Type: replace-cross 
Abstract: As single-center computing approaches power constraints, decentralized training is becoming essential. Reinforcement Learning (RL) post-training enhances Large Language Models (LLMs) but faces challenges in heterogeneous distributed environments due to its tightly-coupled sampling-learning alternation. We propose HeteroRL, an asynchronous RL architecture that decouples rollout sampling from parameter learning, enabling robust deployment across geographically distributed nodes under network delays. We identify that latency-induced KL divergence causes importance sampling failure due to high variance. To address this, we propose Group Expectation Policy Optimization (GEPO), which reduces importance weight variance through a refined sampling mechanism. Theoretically, GEPO achieves exponential variance reduction. Experiments show it maintains superior stability over methods like GRPO, with less than 3% performance degradation under 1800-second delays, demonstrating strong potential for decentralized RL in heterogeneous networks.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios</title>
<link>https://arxiv.org/abs/2508.18183</link>
<guid>https://arxiv.org/abs/2508.18183</guid>
<content:encoded><![CDATA[
arXiv:2508.18183v2 Announce Type: replace-cross 
Abstract: Translating natural languages into sign languages is a highly complex and underexplored task. Despite growing interest in accessibility and inclusivity, the development of robust translation systems remains hindered by the limited availability of parallel corpora which align natural language with sign language data. Existing methods often struggle to generalize in these data-scarce environments, as the few datasets available are typically domain-specific, lack standardization, or fail to capture the full linguistic richness of sign languages. To address this limitation, we propose Advanced Use of LLMs for Sign Language Translation (AulSign), a novel method that leverages Large Language Models via dynamic prompting and in-context learning with sample selection and subsequent sign association. Despite their impressive abilities in processing text, LLMs lack intrinsic knowledge of sign languages; therefore, they are unable to natively perform this kind of translation. To overcome this limitation, we associate the signs with compact descriptions in natural language and instruct the model to use them. We evaluate our method on both English and Italian languages using SignBank+, a recognized benchmark in the field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior performance compared to state-of-the-art models in low-data scenario. Our findings demonstrate the effectiveness of AulSign, with the potential to enhance accessibility and inclusivity in communication technologies for underrepresented linguistic communities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attacks on LLM-based Recommender Systems</title>
<link>https://arxiv.org/abs/2508.18665</link>
<guid>https://arxiv.org/abs/2508.18665</guid>
<content:encoded><![CDATA[
arXiv:2508.18665v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly adapt recommendation systems to different domains. It utilizes in-context learning (ICL), i.e., the prompts, to customize the recommendation functions, which include sensitive historical user-specific item interactions, e.g., implicit feedback like clicked items or explicit product reviews. Such private information may be exposed to novel privacy attack. However, no study has been done on this important issue. We design four membership inference attacks (MIAs), aiming to reveal whether victims' historical interactions have been used by system prompts. They are \emph{direct inquiry, hallucination, similarity, and poisoning attacks}, each of which utilizes the unique features of LLMs or RecSys. We have carefully evaluated them on three LLMs that have been used to develop ICL-LLM RecSys and two well-known RecSys benchmark datasets. The results confirm that the MIA threat on LLM RecSys is realistic: direct inquiry and poisoning attacks showing significantly high attack advantages. We have also analyzed the factors affecting these attacks, such as the number of shots in system prompts and the position of the victim in the shots.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Prompt Optimization with Prompt Distillation</title>
<link>https://arxiv.org/abs/2508.18992</link>
<guid>https://arxiv.org/abs/2508.18992</guid>
<content:encoded><![CDATA[
arXiv:2508.18992v2 Announce Type: replace-cross 
Abstract: Autoprompting is the process of automatically selecting optimized prompts for language models, which is gaining popularity due to the rapid development of prompt engineering driven by extensive research in the field of large language models (LLMs). This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data. DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly. The method was tested on different datasets for text classification and generation tasks using the t-lite-instruct-0.1 language model. The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field, establishing DistillPrompt as one of the most effective non-gradient approaches in autoprompting.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MovieCORE: COgnitive REasoning in Movies</title>
<link>https://arxiv.org/abs/2508.19026</link>
<guid>https://arxiv.org/abs/2508.19026</guid>
<content:encoded><![CDATA[
arXiv:2508.19026v2 Announce Type: replace-cross 
Abstract: This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.19153</link>
<guid>https://arxiv.org/abs/2508.19153</guid>
<content:encoded><![CDATA[
arXiv:2508.19153v2 Announce Type: replace-cross 
Abstract: We address vision-guided quadruped motion control with reinforcement learning (RL) and highlight the necessity of combining proprioception with vision for robust control. We propose QuadKAN, a spline-parameterized cross-modal policy instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates a spline encoder for proprioception and a spline fusion head for proprioception-vision inputs. This structured function class aligns the state-to-action mapping with the piecewise-smooth nature of gait, improving sample efficiency, reducing action jitter and energy consumption, and providing interpretable posture-action sensitivities. We adopt Multi-Modal Delay Randomization (MMDR) and perform end-to-end training with Proximal Policy Optimization (PPO). Evaluations across diverse terrains, including both even and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate that QuadKAN achieves consistently higher returns, greater distances, and fewer collisions than state-of-the-art (SOTA) baselines. These results show that spline-parameterized policies offer a simple, effective, and interpretable alternative for robust vision-guided locomotion. A repository will be made available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2508.19268</link>
<guid>https://arxiv.org/abs/2508.19268</guid>
<content:encoded><![CDATA[
arXiv:2508.19268v2 Announce Type: replace-cross 
Abstract: Despite LLMs' excellent code creation capabilities, multilingual code generation remains extremely challenging. To address this, we intent to improve the multi-programming-lingual (MultiPL) performance of the base LLMs while retaining the most popular ones using restricted computational resources. We consider MultiPL to be a special case of multiple natural languages and propose a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize expert selection at both the token and segment levels. The token-level MoE is a standard upcycling MoE structure with a shared expert and a novel gate weight normalization approach that aids in the final fusion with the segment-level MoE. The segment-level MoE incorporates two innovative designs to better capture the syntactic structure and contextual patterns of programming languages: First, using a sliding window to partition the input token sequence into multiple segments; Then, adopting an expert-choice routing strategy that allows experts to select the top-k segments. The results of the experiment proved the effectiveness of MultiPL-MoE.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities</title>
<link>https://arxiv.org/abs/2508.19597</link>
<guid>https://arxiv.org/abs/2508.19597</guid>
<content:encoded><![CDATA[
arXiv:2508.19597v2 Announce Type: replace-cross 
Abstract: Artificial intelligence underpins most smart city services, yet deep neural network (DNN) that forecasts vehicle motion still struggle with catastrophic forgetting, the loss of earlier knowledge when models are updated. Conventional fixes enlarge the training set or replay past data, but these strategies incur high data collection costs, sample inefficiently and fail to balance long- and short-term experience, leaving them short of human-like continual learning. Here we introduce Dual-LS, a task-free, online continual learning paradigm for DNN-based motion forecasting that is inspired by the complementary learning system of the human brain. Dual-LS pairs two synergistic memory rehearsal replay mechanisms to accelerate experience retrieval while dynamically coordinating long-term and short-term knowledge representations. Tests on naturalistic data spanning three countries, over 772,000 vehicles and cumulative testing mileage of 11,187 km show that Dual-LS mitigates catastrophic forgetting by up to 74.31\% and reduces computational resource demand by up to 94.02\%, markedly boosting predictive stability in vehicle motion forecasting without inflating data requirements. Meanwhile, it endows DNN-based vehicle motion forecasting with computation efficient and human-like continual learning adaptability fit for smart cities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices</title>
<link>https://arxiv.org/abs/2508.20144</link>
<guid>https://arxiv.org/abs/2508.20144</guid>
<content:encoded><![CDATA[
arXiv:2508.20144v2 Announce Type: replace-cross 
Abstract: As deep learning (DL) technologies advance, their application in automated visual inspection for Class III medical devices offers significant potential to enhance quality assurance and reduce human error. However, the adoption of such AI-based systems introduces new regulatory complexities-particularly under the EU Artificial Intelligence (AI) Act, which imposes high-risk system obligations that differ in scope and depth from established regulatory frameworks such as the Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation (QSR). This paper presents a high-level technical assessment of the foreseeable challenges that manufacturers are likely to encounter when qualifying DL-based automated inspections -- specifically static models -- within the existing medical device compliance landscape. It examines divergences in risk management principles, dataset governance, model validation, explainability requirements, and post-deployment monitoring obligations. The discussion also explores potential implementation strategies and highlights areas of uncertainty, including data retention burdens, global compliance implications, and the practical difficulties of achieving statistical significance in validation with limited defect data. Disclaimer: This paper presents a technical perspective and does not constitute legal or regulatory advice.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management</title>
<link>https://arxiv.org/abs/2509.04505</link>
<guid>https://arxiv.org/abs/2509.04505</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Construction Project Management, Ethical Decision-making, Large Language Models, Human-in-the-loop oversight

Summary:
The study evaluates the ethical viability and reliability of Large Language Models (LLMs) in Construction Project Management (CPM). Through quantitative performance testing and qualitative interviews with industry experts, the study finds that while LLMs perform well in structured domains like legal compliance, they struggle with contextual nuance, accountability, and transparent reasoning. Stakeholders express reservations about using AI autonomously for ethical judgments and advocate for human-in-the-loop oversight. The study introduces the Ethical Decision Support Assessment Checklist (EDSAC) as a framework for evaluating LLMs in ethical decision-making contexts and recommends that LLMs be used as decision-support aids rather than autonomous ethical agents.

<br /><br />Summary: The study evaluates the ethical viability and reliability of Large Language Models in Construction Project Management. LLMs demonstrate adequate performance in legal compliance but struggle with contextual nuance, accountability, and transparent reasoning. Stakeholders advocate for human-in-the-loop oversight and recommend using LLMs as decision-support aids rather than autonomous ethical agents. <div>
arXiv:2509.04505v1 Announce Type: new 
Abstract: The integration of Artificial Intelligence (AI) into construction project management (CPM) is accelerating, with Large Language Models (LLMs) emerging as accessible decision-support tools. This study aims to critically evaluate the ethical viability and reliability of LLMs when applied to the ethically sensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods research design was employed, involving the quantitative performance testing of two leading LLMs against twelve real-world ethical scenarios using a novel Ethical Decision Support Assessment Checklist (EDSAC), and qualitative analysis of semi-structured interviews with 12 industry experts to capture professional perceptions. The findings reveal that while LLMs demonstrate adequate performance in structured domains such as legal compliance, they exhibit significant deficiencies in handling contextual nuance, ensuring accountability, and providing transparent reasoning. Stakeholders expressed considerable reservations regarding the autonomous use of AI for ethical judgments, strongly advocating for robust human-in-the-loop oversight. To our knowledge, this is one of the first studies to empirically test the ethical reasoning of LLMs within the construction domain. It introduces the EDSAC framework as a replicable methodology and provides actionable recommendations, emphasising that LLMs are currently best positioned as decision-support aids rather than autonomous ethical agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maestro: Joint Graph &amp; Config Optimization for Reliable AI Agents</title>
<link>https://arxiv.org/abs/2509.04642</link>
<guid>https://arxiv.org/abs/2509.04642</guid>
<content:encoded><![CDATA[
<div> optimizer, LLM agents, Maestro, graph, configuration

Summary:<br />
1. The article introduces Maestro, a comprehensive optimizer for LLM agents that considers both graph structure and configuration of nodes.
2. Maestro optimizes agent quality while adhering to rollout/token budgets by jointly searching over graphs and configurations.
3. The optimizer utilizes textual feedback from traces to prioritize edits, enhancing sample efficiency and targeting specific failure modes.
4. On benchmark tests, Maestro outperforms leading prompt optimizers such as MIPROv2, GEPA, and GEPA+Merge by significant margins.
5. Maestro achieves superior results with fewer rollouts compared to GEPA, demonstrating its efficiency in optimizing LLM agents.
6. The joint graph and configuration search by Maestro addresses structural failure modes that prompt tuning alone cannot fix.
7. Applications such as interviewer and RAG agents show significant improvements with Maestro's approach. 

<br /><br />Summary: <div>
arXiv:2509.04642v1 Announce Type: new 
Abstract: Building reliable LLM agents requires decisions at two levels: the graph (which modules exist and how information flows) and the configuration of each node (models, prompts, tools, control knobs). Most existing optimizers tune configurations while holding the graph fixed, leaving structural failure modes unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for LLM agents that jointly searches over graphs and configurations to maximize agent quality, subject to explicit rollout/token budgets. Beyond numeric metrics, Maestro leverages reflective textual feedback from traces to prioritize edits, improving sample efficiency and targeting specific failure modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%, 4.9%, and 4.86%, respectively; even when restricted to prompt-only optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these results with far fewer rollouts than GEPA. We further show large gains on two applications (interviewer & RAG agents), highlighting that joint graph & configuration search addresses structural failure modes that prompt tuning alone cannot fix.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization</title>
<link>https://arxiv.org/abs/2509.04646</link>
<guid>https://arxiv.org/abs/2509.04646</guid>
<content:encoded><![CDATA[
<div> Keywords: Modeling & Simulation, Agent-based models, Large Language Models, Stakeholder needs, Tailored explanations<br />
Summary: 
This article discusses the potential of Modeling & Simulation (M&amp;S) approaches, such as agent-based models, in supporting decision-making in the health sector, including areas like vaccine adoption and healthy behavior promotion. However, the complexity of these models often makes them inaccessible to key stakeholders. To address this issue, the authors propose a framework to tailor explanations of health simulations using Large Language Models (LLMs). By first understanding stakeholder needs and preferences, optimizing LLMs to generate customized explanations, and evaluating the outputs, the framework aims to bridge the gap between complex health models and practical usability. The study highlights the importance of personalized communication in translating simulation outputs for various stakeholders, including clinicians, policymakers, patients, caregivers, and health advocates. By enhancing the clarity and relevance of explanations, this approach can enhance the impact and usability of health simulations in decision-making processes. <br /><br />Summary: <div>
arXiv:2509.04646v1 Announce Type: new 
Abstract: Modeling & Simulation (M&amp;S) approaches such as agent-based models hold significant potential to support decision-making activities in health, with recent examples including the adoption of vaccines, and a vast literature on healthy eating behaviors and physical activity behaviors. These models are potentially usable by different stakeholder groups, as they support policy-makers to estimate the consequences of potential interventions and they can guide individuals in making healthy choices in complex environments. However, this potential may not be fully realized because of the models' complexity, which makes them inaccessible to the stakeholders who could benefit the most. While Large Language Models (LLMs) can translate simulation outputs and the design of models into text, current approaches typically rely on one-size-fits-all summaries that fail to reflect the varied informational needs and stylistic preferences of clinicians, policymakers, patients, caregivers, and health advocates. This limitation stems from a fundamental gap: we lack a systematic understanding of what these stakeholders need from explanations and how to tailor them accordingly. To address this gap, we present a step-by-step framework to identify stakeholder needs and guide LLMs in generating tailored explanations of health simulations. Our procedure uses a mixed-methods design by first eliciting the explanation needs and stylistic preferences of diverse health stakeholders, then optimizing the ability of LLMs to generate tailored outputs (e.g., via controllable attribute tuning), and then evaluating through a comprehensive range of metrics to further improve the tailored generation of summaries.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Approach to Grounding AI Model Evaluations in Human-derived Criteria</title>
<link>https://arxiv.org/abs/2509.04676</link>
<guid>https://arxiv.org/abs/2509.04676</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, benchmarks, human-derived evaluation criteria, interpretability, cognitive skills

Summary: 
Artificial intelligence (AI) is rapidly evolving, but traditional benchmarks may not fully capture its capabilities. This study focuses on enhancing benchmarks in physical world modeling by incorporating human-derived evaluation criteria. Factors such as Prioritization, Memorizing, Discerning, and Contextualizing were identified as crucial cognitive skills for AI and human reasoning. Participants believe AI lacks interpretive and empathetic abilities but expect high performance. By integrating these insights into benchmark design, a more human-aligned approach to defining and measuring progress in AI development is proposed. User-centered evaluation is emphasized, providing guidelines for aligning AI capabilities with human cognitive processes. This approach aims to improve current benchmarking practices and pave the way for future advancements in AI model evaluation. <div>
arXiv:2509.04676v1 Announce Type: new 
Abstract: In the rapidly evolving field of artificial intelligence (AI), traditional benchmarks can fall short in attempting to capture the nuanced capabilities of AI models. We focus on the case of physical world modeling and propose a novel approach to augment existing benchmarks with human-derived evaluation criteria, aiming to enhance the interpretability and applicability of model behaviors. Grounding our study in the Perception Test and OpenEQA benchmarks, we conducted in-depth interviews and large-scale surveys to identify key cognitive skills, such as Prioritization, Memorizing, Discerning, and Contextualizing, that are critical for both AI and human reasoning. Our findings reveal that participants perceive AI as lacking in interpretive and empathetic skills yet hold high expectations for AI performance. By integrating insights from our findings into benchmark design, we offer a framework for developing more human-aligned means of defining and measuring progress. This work underscores the importance of user-centered evaluation in AI development, providing actionable guidelines for researchers and practitioners aiming to align AI capabilities with human cognitive processes. Our approach both enhances current benchmarking practices and sets the stage for future advancements in AI model evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning</title>
<link>https://arxiv.org/abs/2509.04731</link>
<guid>https://arxiv.org/abs/2509.04731</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, Agent models, World models, hierarchical scaffolding, multi-agent reinforcement learning

Summary: 
The convergence of Language, Agent, and World models is crucial for advancing artificial intelligence. Existing models focus on scaling Language and Agent models, but the development of sophisticated World Models remains a challenge, especially for complex tasks. A significant hurdle is the lack of explicit hierarchical World Models in environments like robotic soccer. The solution proposed is to create environments with hierarchical scaffolding, decomposing complex goals into manageable subgoals. Research in multi-agent soccer shows a shift towards integrating symbolic and hierarchical methods with multi-agent reinforcement learning. A paradigm shift is suggested - using Large Language Models to dynamically create a hierarchical scaffold, structuring the World Model with language in real-time. This language-driven approach provides a curriculum, meaningful learning signals, and supports compositional learning, enabling agents to learn strategic behaviors efficiently. By incorporating explicit, language-configurable task layers in environments, it becomes possible to bridge the gap between low-level reactive behaviors and high-level strategic team play, offering a generalizable framework to train intelligent agents. 

<br /><br />Summary: <div>
arXiv:2509.04731v1 Announce Type: new 
Abstract: The convergence of Language models, Agent models, and World models represents a critical frontier for artificial intelligence. While recent progress has focused on scaling Language and Agent models, the development of sophisticated, explicit World Models remains a key bottleneck, particularly for complex, long-horizon multi-agent tasks. In domains such as robotic soccer, agents trained via standard reinforcement learning in high-fidelity but structurally-flat simulators often fail due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing capable agents lies in creating environments that possess an explicit, hierarchical World Model. We contend that this is best achieved through hierarchical scaffolding, where complex goals are decomposed into structured, manageable subgoals. Drawing evidence from a systematic review of 2024 research in multi-agent soccer, we identify a clear and decisive trend towards integrating symbolic and hierarchical methods with multi-agent reinforcement learning (MARL). These approaches implicitly or explicitly construct a task-based world model to guide agent learning. We then propose a paradigm shift: leveraging Large Language Models to dynamically generate this hierarchical scaffold, effectively using language to structure the World Model on the fly. This language-driven world model provides an intrinsic curriculum, dense and meaningful learning signals, and a framework for compositional learning, enabling Agent Models to acquire sophisticated, strategic behaviors with far greater sample efficiency. By building environments with explicit, language-configurable task layers, we can bridge the gap between low-level reactive behaviors and high-level strategic team play, creating a powerful and generalizable framework for training the next generation of intelligent agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking</title>
<link>https://arxiv.org/abs/2509.04791</link>
<guid>https://arxiv.org/abs/2509.04791</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, What-If Analysis, proactive thinking, reinforcement learning, strategic applications

Summary: 
Large language models (LLMs) are highly skilled at reactive information processing but lack the ability to explore hypothetical futures systematically. The proposed WiA-LLM paradigm integrates What-If Analysis (WIA) to equip LLMs with proactive thinking capabilities. By leveraging reinforcement learning to analyze potential scenarios, WiA-LLM can anticipate future outcomes rather than just reacting to present conditions. Validation in the Honor of Kings (HoK) game environment demonstrates WiA-LLM achieving a 74.2% accuracy in forecasting game-state changes, with significant gains in high-difficulty scenarios where accurate foresight is crucial. This work represents a fundamental step towards proactive reasoning in LLMs, offering a scalable framework for robust decision-making in dynamic environments, with broad implications for strategic applications. 

<br /><br />Summary: <div>
arXiv:2509.04791v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at processing information reactively but lack the ability to systemically explore hypothetical futures. They cannot ask, "what if we take this action? how will it affect the final outcome" and forecast its potential consequences before acting. This critical gap limits their utility in dynamic, high-stakes scenarios like strategic planning, risk assessment, and real-time decision making. To bridge this gap, we propose WiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities. Our approach integrates What-If Analysis (WIA), a systematic approach for evaluating hypothetical scenarios by changing input variables. By leveraging environmental feedback via reinforcement learning, WiA-LLM moves beyond reactive thinking. It dynamically simulates the outcomes of each potential action, enabling the model to anticipate future states rather than merely react to the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a complex multiplayer game environment characterized by rapid state changes and intricate interactions. The game's real-time state changes require precise multi-step consequence prediction, making it an ideal testbed for our approach. Experimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy in forecasting game-state changes (up to two times gain over baselines). The model shows particularly significant gains in high-difficulty scenarios where accurate foresight is critical. To our knowledge, this is the first work to formally explore and integrate what-if analysis capabilities within LLMs. WiA-LLM represents a fundamental advance toward proactive reasoning in LLMs, providing a scalable framework for robust decision-making in dynamic environments with broad implications for strategic applications.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models</title>
<link>https://arxiv.org/abs/2509.04809</link>
<guid>https://arxiv.org/abs/2509.04809</guid>
<content:encoded><![CDATA[
<div> Explainable Reinforcement Learning, XRL, TalkToAgent, Large Language Models, interactive explanations
Summary:
TalkToAgent introduces a multi-agent Large Language Models framework for Explainable Reinforcement Learning. The architecture consists of Coordinator, Explainer, Coder, Evaluator, and Debugger agents. It provides natural language explanations for RL policies, mapping user queries to relevant XRL tools, and clarifying agent actions in terms of key state variables, expected outcomes, or counterfactual explanations. The framework extends counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions or new rule-based policies. Validation on a quadruple-tank process control problem showed accurate mapping of user queries to XRL tasks, successful coder-debugger interactions for counterfactual generation, and effective interpretation of agent actions in the problem domain.<br /><br />Summary: <div>
arXiv:2509.04809v1 Announce Type: new 
Abstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agent's actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agent's actions and contextualized their meaning within the problem domain.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory</title>
<link>https://arxiv.org/abs/2509.04847</link>
<guid>https://arxiv.org/abs/2509.04847</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, cooperative behavior, multi-party settings, prisoner's dilemma, Axelrod-style tournament
Summary: 
Language models are being increasingly used in various interactive online settings, prompting the need to analyze their behavior in multi-party scenarios. This study focuses on language model behavior in the iterated prisoner's dilemma game, a classic framework for studying cooperation and conflict over time. Results show that language models can perform as well as, or better than, traditional strategies in this setting, demonstrating characteristics of strong cooperative strategies such as niceness, provocability, and generosity. The models also display quick adaptability to changes in opponent strategy, rivaling or exceeding human adaptability. This research provides valuable insights into the long-term cooperative behaviors of language model agents and lays the groundwork for further exploration of their role in complex social environments involving both humans and AI.


<br /><br />Summary: <div>
arXiv:2509.04847v1 Announce Type: new 
Abstract: Language models are increasingly deployed in interactive online environments, from personal chat assistants to domain-specific agents, raising questions about their cooperative and competitive behavior in multi-party settings. While prior work has examined language model decision-making in isolated or short-term game-theoretic contexts, these studies often neglect long-horizon interactions, human-model collaboration, and the evolution of behavioral patterns over time. In this paper, we investigate the dynamics of language model behavior in the iterated prisoner's dilemma (IPD), a classical framework for studying cooperation and conflict. We pit model-based agents against a suite of 240 well-established classical strategies in an Axelrod-style tournament and find that language models achieve performance on par with, and in some cases exceeding, the best-known classical strategies. Behavioral analysis reveals that language models exhibit key properties associated with strong cooperative strategies - niceness, provocability, and generosity while also demonstrating rapid adaptability to changes in opponent strategy mid-game. In controlled "strategy switch" experiments, language models detect and respond to shifts within only a few rounds, rivaling or surpassing human adaptability. These results provide the first systematic characterization of long-term cooperative behaviors in language model agents, offering a foundation for future research into their role in more complex, mixed human-AI social environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales</title>
<link>https://arxiv.org/abs/2509.04871</link>
<guid>https://arxiv.org/abs/2509.04871</guid>
<content:encoded><![CDATA[
<div> voice assistant, dialogue modelling, call recordings, telesales data, AI agent

Summary:
Recent advances in language and speech modelling have enabled the development of voice assistants that can understand and generate human dialogue in real time. This paper presents a methodology for creating a conversational voice AI agent from call recordings, specifically focusing on telesales data. The system listens to customers over the phone, responds with a synthetic voice, and follows a playbook learned from top human agents. Evaluation against human agents showed that the AI agent performed well in routine aspects of calls but struggled in persuasion and objection handling. The paper discusses the shortcomings and proposes refinements to improve performance. Future research directions include large-scale simulation and automated evaluation to further enhance the cloned agent's capabilities. 

<br /><br />Summary: <div>
arXiv:2509.04871v1 Announce Type: new 
Abstract: Recent advances in language and speech modelling have made it possible to build autonomous voice assistants that understand and generate human dialogue in real time. These systems are increasingly being deployed in domains such as customer service and healthcare care, where they can automate repetitive tasks, reduce operational costs, and provide constant support around the clock. In this paper, we present a general methodology for cloning a conversational voice AI agent from a corpus of call recordings. Although the case study described in this paper uses telesales data to illustrate the approach, the underlying process generalizes to any domain where call transcripts are available. Our system listens to customers over the telephone, responds with a synthetic voice, and follows a structured playbook learned from top performing human agents. We describe the domain selection, knowledge extraction, and prompt engineering used to construct the agent, integrating automatic speech recognition, a large language model based dialogue manager, and text to speech synthesis into a streaming inference pipeline. The cloned agent is evaluated against human agents on a rubric of 22 criteria covering introduction, product communication, sales drive, objection handling, and closing. Blind tests show that the AI agent approaches human performance in routine aspects of the call while underperforming in persuasion and objection handling. We analyze these shortcomings and refine the prompt accordingly. The paper concludes with design lessons and avenues for future research, including large scale simulation and automated evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration</title>
<link>https://arxiv.org/abs/2509.04876</link>
<guid>https://arxiv.org/abs/2509.04876</guid>
<content:encoded><![CDATA[
<div> Keywords: OSC, cognitive synergy, multi-agent systems, collaboration framework, language models

Summary: 
The paper introduces OSC (Orchestrating Cognitive Synergy), a framework designed to enhance cognitive synergy in multi-agent systems with large language models. OSC introduces Collaborator Knowledge Models (CKM) to enable agents to perceive their collaborators' cognitive states and adapt communication behaviors in real-time. By analyzing cognitive gaps and adjusting communication strategies, OSC improves task performance and communication efficiency in complex reasoning and problem-solving tasks. The framework transforms individual agents into a deeply collaborative cognitive team, optimizing multi-agent collaboration. Additionally, OSC offers new insights into language model agent interaction behaviors. <div>
arXiv:2509.04876v1 Announce Type: new 
Abstract: This paper introduces OSC (Orchestrating Cognitive Synergy), a knowledge-aware adaptive collaboration framework designed to enhance cognitive synergy in multi-agent systems with large language models. While prior work has advanced agent selection and result aggregation, efficient linguistic interactions for deep collaboration among expert agents remain a critical bottleneck. OSC addresses this gap as a pivotal intermediate layer between selection and aggregation, introducing Collaborator Knowledge Models (CKM) to enable each agent to dynamically perceive its collaborators' cognitive states. Through real-time cognitive gap analysis, agents adaptively adjust communication behaviors, including content focus, detail level, and expression style, using learned strategies. Experiments on complex reasoning and problem-solving benchmarks demonstrate that OSC significantly improves task performance and communication efficiency, transforming "parallel-working individuals'' into a "deeply collaborative cognitive team.'' This framework not only optimizes multi-agent collaboration but also offers new insights into LLM agent interaction behaviors.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing</title>
<link>https://arxiv.org/abs/2509.04908</link>
<guid>https://arxiv.org/abs/2509.04908</guid>
<content:encoded><![CDATA[
<div> continuous modeling, Multimodal Large Language Model, GUI perception, SparkUI-Parser, ScreenParse

Summary:
SparkUI-Parser is a novel framework that addresses challenges in Multimodal Large Language Models (MLLMs) for GUI perception. It improves localization precision and parsing capability by implementing continuous modeling of coordinates. This approach enhances accuracy and inference speed compared to discrete modeling methods. The framework includes a rejection mechanism based on a modified Hungarian matching algorithm to identify and reject non-existent elements, reducing false positives. Additionally, ScreenParse, a benchmark for assessing GUI model capabilities, is introduced to evaluate performance across diverse scenarios. Extensive experiments show that SparkUI-Parser outperforms state-of-the-art methods on various benchmarks. The resources for SparkUI-Parser are available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2509.04908v1 Announce Type: new 
Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have made great progress. However, the following challenges still exist in prior methods: 1) They model discrete coordinates based on text autoregressive mechanism, which results in lower grounding accuracy and slower inference speed. 2) They can only locate predefined sets of elements and are not capable of parsing the entire interface, which hampers the broad application and support for downstream tasks. To address the above issues, we propose SparkUI-Parser, a novel end-to-end framework where higher localization precision and fine-grained parsing capability of the entire interface are simultaneously achieved. Specifically, instead of using probability-based discrete modeling, we perform continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with an additional token router and coordinate decoder. This effectively mitigates the limitations inherent in the discrete output characteristics and the token-by-token generation process of MLLMs, consequently boosting both the accuracy and the inference speed. To further enhance robustness, a rejection mechanism based on a modified Hungarian matching algorithm is introduced, which empowers the model to identify and reject non-existent elements, thereby reducing false positives. Moreover, we present ScreenParse, a rigorously constructed benchmark to systematically assess structural perception capabilities of GUI models across diverse scenarios. Extensive experiments demonstrate that our approach consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks. The resources are available at https://github.com/antgroup/SparkUI-Parser.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts</title>
<link>https://arxiv.org/abs/2509.04926</link>
<guid>https://arxiv.org/abs/2509.04926</guid>
<content:encoded><![CDATA[
<div> ontology, conversational agents, language models, controlled text generation, proficiency-level control

Summary:
- The article addresses the challenge of controlling Large Language Models (LLMs) for predictable and user-personalized responses in conversations.
- An ontology-based approach is proposed to formally define conversational features, typically qualitative, using linguistic descriptors to derive quantitative definitions.
- The focus is on proficiency-level control in conversations, with CEFR language proficiency levels as a case study.
- The qualitative definitions are formalized in description logic and integrated into an ontology to guide controlled text generation of an LLM through fine-tuning.
- Experimental results validate the approach, showing consistent and explainable proficiency-level definitions that enhance transparency in conversational AI.<br /><br />Summary: <div>
arXiv:2509.04926v1 Announce Type: new 
Abstract: The controllability of Large Language Models (LLMs) when used as conversational agents is a key challenge, particularly to ensure predictable and user-personalized responses. This work proposes an ontology-based approach to formally define conversational features that are typically qualitative in nature. By leveraging a set of linguistic descriptors, we derive quantitative definitions for qualitatively-defined concepts, enabling their integration into an ontology for reasoning and consistency checking. We apply this framework to the task of proficiency-level control in conversations, using CEFR language proficiency levels as a case study. These definitions are then formalized in description logic and incorporated into an ontology, which guides controlled text generation of an LLM through fine-tuning. Experimental results demonstrate that our approach provides consistent and explainable proficiency-level definitions, improving transparency in conversational AI.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents</title>
<link>https://arxiv.org/abs/2509.04979</link>
<guid>https://arxiv.org/abs/2509.04979</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, large language models, Web of Agents, Agent Ranking, DOVIS

Summary: 
AI agents powered by large language models are expected to transform the internet into a Web of Agents, where autonomous agents interact and collaborate at scale. To achieve this vision, Agent Ranking is essential, selecting agents based on their proven recent performance rather than just declared capabilities. A new operational protocol called DOVIS enables the collection of privacy-preserving aggregates of usage and performance data across the ecosystem. AgentRank-UC, an algorithm that combines usage and competence metrics, is introduced to rank agents dynamically and trust-awarely. The protocol shows promising results in simulation, with guarantees on convergence, robustness, and resistance to malicious actors. By implementing coordinated protocols and performance-aware ranking, a scalable and trustworthy Agentic Web can be realized. 

<br /><br />Summary: <div>
arXiv:2509.04979v1 Announce Type: new 
Abstract: AI agents -- powered by reasoning-capable large language models (LLMs) and integrated with tools, data, and web search -- are poised to transform the internet into a \emph{Web of Agents}: a machine-native ecosystem where autonomous agents interact, collaborate, and execute tasks at scale. Realizing this vision requires \emph{Agent Ranking} -- selecting agents not only by declared capabilities but by proven, recent performance. Unlike Web~1.0's PageRank, a global, transparent network of agent interactions does not exist; usage signals are fragmented and private, making ranking infeasible without coordination.
  We propose \textbf{DOVIS}, a five-layer operational protocol (\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that enables the collection of minimal, privacy-preserving aggregates of usage and performance across the ecosystem. On this substrate, we implement \textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines \emph{usage} (selection frequency) and \emph{competence} (outcome quality, cost, safety, latency) into a unified ranking. We present simulation results and theoretical guarantees on convergence, robustness, and Sybil resistance, demonstrating the viability of coordinated protocols and performance-aware ranking in enabling a scalable, trustworthy Agentic Web.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework</title>
<link>https://arxiv.org/abs/2509.05007</link>
<guid>https://arxiv.org/abs/2509.05007</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, test-time scaling, Sticker-TTS, historical experience utilization, mathematical reasoning benchmarks

Summary:
Large reasoning models (LRMs) have shown strong performance on complex reasoning tasks, but current test-time scaling methods lack efficiency by relying on redundant sampling. To address this, the Sticker-TTS framework utilizes historical experience to iteratively explore and refine solutions through collaboration between LRMs. Key conditions, called stickers, are used to extract and reuse critical information, enhancing computational efficiency. A two-stage optimization strategy combines imitation learning with self-improvement for progressive refinement. Evaluation on challenging mathematical reasoning benchmarks demonstrates that Sticker-TTS outperforms strong baselines and reinforcement learning approaches under comparable inference budgets. The effectiveness of sticker-guided historical experience utilization is highlighted in achieving superior performance. The code and data for Sticker-TTS are available on GitHub at https://github.com/RUCAIBox/Sticker-TTS.<br /><br />Summary: <div>
arXiv:2509.05007v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) have exhibited strong performance on complex reasoning tasks, with further gains achievable through increased computational budgets at inference. However, current test-time scaling methods predominantly rely on redundant sampling, ignoring the historical experience utilization, thereby limiting computational efficiency. To overcome this limitation, we propose Sticker-TTS, a novel test-time scaling framework that coordinates three collaborative LRMs to iteratively explore and refine solutions guided by historical attempts. At the core of our framework are distilled key conditions-termed stickers-which drive the extraction, refinement, and reuse of critical information across multiple rounds of reasoning. To further enhance the efficiency and performance of our framework, we introduce a two-stage optimization strategy that combines imitation learning with self-improvement, enabling progressive refinement. Extensive evaluations on three challenging mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH, demonstrate that Sticker-TTS consistently surpasses strong baselines, including self-consistency and advanced reinforcement learning approaches, under comparable inference budgets. These results highlight the effectiveness of sticker-guided historical experience utilization. Our code and data are available at https://github.com/RUCAIBox/Sticker-TTS.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding your MUSE: Mining Unexpected Solutions Engine</title>
<link>https://arxiv.org/abs/2509.05072</link>
<guid>https://arxiv.org/abs/2509.05072</guid>
<content:encoded><![CDATA[
<div> Methodology, Functional Concept Graphs, Abstraction, Problem reframing, Analogical inspiration

Summary:
The article discusses cognitive fixation among innovators and presents a methodology for constructing Functional Concept Graphs (FCGs) to support abstraction, problem reframing, and analogical inspiration. These FCGs are interconnected representations of functional elements that aid in overcoming limitations of existing solutions or ideas. The paper introduces MUSE, an algorithm that utilizes FCGs to generate creative inspirations for solving problems. By applying this methodology to analyze 500K patents, the study demonstrates the ability to create large-scale and high-quality FCGs with explicit abstraction relations. The FCGs constructed can facilitate the exploration of novel alternatives and enable innovators to break free from cognitive fixation, leading to more innovative solutions. The dataset containing the FCGs constructed from patents is released for further research. 

<br /><br />Summary: <div>
arXiv:2509.05072v1 Announce Type: new 
Abstract: Innovators often exhibit cognitive fixation on existing solutions or nascent ideas, hindering the exploration of novel alternatives. This paper introduces a methodology for constructing Functional Concept Graphs (FCGs), interconnected representations of functional elements that support abstraction, problem reframing, and analogical inspiration. Our approach yields large-scale, high-quality FCGs with explicit abstraction relations, overcoming limitations of prior work. We further present MUSE, an algorithm leveraging FCGs to generate creative inspirations for a given problem. We demonstrate our method by computing an FCG on 500K patents, which we release for further research.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback</title>
<link>https://arxiv.org/abs/2509.05091</link>
<guid>https://arxiv.org/abs/2509.05091</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, Theory of Mind, prosocial behavior, feedback, multi-agent systems <br />
Summary: <br />
- The paper addresses the challenge of promoting prosocial behavior in multi-agent systems using an AI system called ProToM. <br />
- ProToM uses Bayesian inverse planning to infer agents' goals and provides targeted feedback to encourage prosocial actions. <br />
- The approach is evaluated in two environments, demonstrating higher success rates, shorter task completion times, and preference by human users compared to baseline models. <br />
- Current large language and reasoning models struggle to provide contextually grounded and well-timed feedback, leading to higher communication overhead. <br />
- ProToM's ability to offer helpful feedback improves cooperation in multi-agent systems, showcasing the importance of understanding and promoting prosocial behavior in AI applications. <br /> <div>
arXiv:2509.05091v1 Announce Type: new 
Abstract: While humans are inherently social creatures, the challenge of identifying when and how to assist and collaborate with others - particularly when pursuing independent goals - can hinder cooperation. To address this challenge, we aim to develop an AI system that provides useful feedback to promote prosocial behaviour - actions that benefit others, even when not directly aligned with one's own goals. We introduce ProToM, a Theory of Mind-informed facilitator that promotes prosocial actions in multi-agent systems by providing targeted, context-sensitive feedback to individual agents. ProToM first infers agents' goals using Bayesian inverse planning, then selects feedback to communicate by maximising expected utility, conditioned on the inferred goal distribution. We evaluate our approach against baselines in two multi-agent environments: Doors, Keys, and Gems, as well as Overcooked. Our results suggest that state-of-the-art large language and reasoning models fall short of communicating feedback that is both contextually grounded and well-timed - leading to higher communication overhead and task speedup. In contrast, ProToM provides targeted and helpful feedback, achieving a higher success rate, shorter task completion times, and is consistently preferred by human users.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation and Comparison Semantics for ODRL</title>
<link>https://arxiv.org/abs/2509.05139</link>
<guid>https://arxiv.org/abs/2509.05139</guid>
<content:encoded><![CDATA[
<div> evaluate, compare, computational policies, Open Digital Rights Language (ODRL), formal semantics

Summary:
The article discusses the evaluation and comparison of computational policies in the Open Digital Rights Language (ODRL). It addresses the lack of a comprehensive formal semantics for ODRL and proposes a new formal semantics based on query answering. This semantics refines previous formalizations and aligns with the latest language specification. Additionally, the paper introduces the problem of comparing two policies in ODRL, aiming to detect equivalent, more restrictive, or more permissive policies. The study is motivated by data sharing scenarios, highlighting the importance of understanding and analyzing the implications of different policies governing digital resource access and usage. <div>
arXiv:2509.05139v1 Announce Type: new 
Abstract: We consider the problem of evaluating, and comparing computational policies in the Open Digital Rights Language (ODRL), which has become the de facto standard for governing the access and usage of digital resources. Although preliminary progress has been made on the formal specification of the language's features, a comprehensive formal semantics of ODRL is still missing. In this paper, we provide a simple and intuitive formal semantics for ODRL that is based on query answering. Our semantics refines previous formalisations, and is aligned with the latest published specification of the language (2.2). Building on our evaluation semantics, and motivated by data sharing scenarios, we also define and study the problem of comparing two policies, detecting equivalent, more restrictive or more permissive policies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</title>
<link>https://arxiv.org/abs/2509.05263</link>
<guid>https://arxiv.org/abs/2509.05263</guid>
<content:encoded><![CDATA[
<div> framework, 3D world generation, LatticeWorld, multimodal inputs, efficiency<br />
Summary:<br />
The paper introduces LatticeWorld, a 3D world generation framework that utilizes lightweight LLMs and industry-grade rendering engines to create dynamic environments. By accepting textual and visual instructions as inputs, LatticeWorld is able to generate large-scale interactive worlds with dynamic agents, competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. Extensive experiments show that LatticeWorld outperforms traditional manual production methods in scene layout generation and visual fidelity while achieving over a 90x increase in industrial production efficiency. This approach bridges the sim-to-real gap, enabling the creation of more realistic simulations for various applications such as embodied AI, autonomous driving, and entertainment. The proposed framework simplifies the process of 3D world modeling, making it more efficient and effective for industrial production pipelines. By leveraging generative methods and advanced machine learning algorithms, LatticeWorld offers a solution for creating realistic 3D virtual environments with accurate physics simulation. <br /> <div>
arXiv:2509.05263v1 Announce Type: new 
Abstract: Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer</title>
<link>https://arxiv.org/abs/2303.06298</link>
<guid>https://arxiv.org/abs/2303.06298</guid>
<content:encoded><![CDATA[
<div> architecture, MLP-SRGAN, super resolution, FLAIR MRI, image quality metrics

Summary:
MLP-SRGAN is a new single-dimension Super Resolution Generative Adversarial Network that combines Multi-Layer Perceptron Mixers with convolutional layers for upsampling in the slice direction. It is trained on high resolution FLAIR MRI data and tested on multicentre datasets with low spatial resolution. Results show superior performance in edge sharpness, texture preservation, and fine-anatomical detail compared to existing methods. The model has fewer parameters, faster training/evaluation time, and a smaller size. Performance is measured using PSNR and SSIM for images with high resolution ground truths, as well as new no-reference image quality metrics for sharpness, noise, and blurriness assessment. Code for training, inference, data generators, models, and quality metrics is available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2303.06298v1 Announce Type: cross 
Abstract: We propose a novel architecture called MLP-SRGAN, which is a single-dimension Super Resolution Generative Adversarial Network (SRGAN) that utilizes Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to upsample in the slice direction. MLP-SRGAN is trained and validated using high resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with low spatial resolution in the slice dimension to examine performance on held-out (unseen) clinical data. Upsampled results are compared to several state-of-the-art SR networks. For images with high resolution (HR) ground truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index (SSIM) are used to measure upsampling performance. Several new structural, no-reference image quality metrics were proposed to quantify sharpness (edge strength), noise (entropy), and blurriness (low frequency information) in the absence of ground truths. Results show MLP-SRGAN results in sharper edges, less blurring, preserves more texture and fine-anatomical detail, with fewer parameters, faster training/evaluation time, and smaller model size than existing methods. Code for MLP-SRGAN training and inference, data generators, models and no-reference image quality metrics will be available at https://github.com/IAMLAB-Ryerson/MLP-SRGAN.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving</title>
<link>https://arxiv.org/abs/2509.02718</link>
<guid>https://arxiv.org/abs/2509.02718</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, LLM routing, online scenarios, algorithm, nearest neighbor search 

Summary: 
This study introduces a novel algorithm for online routing scenarios, aiming to optimize the use of Large Language Models (LLMs). By leveraging approximate nearest neighbor search and one-time optimization over initial queries, the algorithm efficiently estimates query features and learns a routing strategy for future queries without the need for training. The algorithm is proven to achieve a competitive ratio close to 1 under realistic assumptions. Extensive experiments across multiple datasets and baselines show significant improvements in overall performance, cost efficiency, and throughput compared to existing methods. This approach addresses the challenge of adapting LLM routing to online settings with high query volume and token budget constraints, offering a promising solution for optimizing LLM services in cost-efficient and effective ways.<br /><br />Summary: <div>
arXiv:2509.02718v1 Announce Type: cross 
Abstract: Increasing demand for Large Language Models (LLMs) services imposes substantial deployment and computation costs on providers. LLM routing offers a cost-efficient solution by directing queries to the optimal LLM based on model and query features. However, existing works primarily focus on offline scenarios and struggle to adapt to online settings with high query volume and constrained token budgets. In this work, we introduce the first training-free algorithm for online routing scenarios. Our algorithm leverages approximate nearest neighbor search to efficiently estimate query features and performs a one-time optimization over a small set of initial queries to learn a routing strategy that guides future routing. We provide theoretical guarantees demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$ under natural assumptions, which is further validated by extensive experiments across 3 benchmark datasets and 8 baselines, showing an average improvement of 3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and nearly 4.25$\times$ in throughput.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge</title>
<link>https://arxiv.org/abs/2509.03614</link>
<guid>https://arxiv.org/abs/2509.03614</guid>
<content:encoded><![CDATA[
<div> Keywords: mitotic figures, artificial intelligence, domain shift, pixel-level segmentation, teacher-student model

Summary:
Mitotic figure counting is time-consuming and prone to variability among pathologists. Artificial intelligence offers a solution by automating detection, but faces challenges such as domain shift and imbalanced data. In this study, a teacher-student model is proposed for mitosis detection and atypical mitosis classification. The model uses a UNet segmentation backbone with domain generalization modules and a multi-scale CNN classifier for classification. By incorporating contrastive representation learning and domain-adversarial training, the algorithm generates pixel-level pseudo-masks for mitoses, hard negatives, and normal nuclei to enhance feature discrimination and mitigate domain shift issues. Preliminary results show promising performance, with an F1 score of 0.7660 for mitosis detection and a balanced accuracy of 0.8414 for atypical mitosis classification. This integrated approach demonstrates the effectiveness of combining segmentation-based detection and classification for robust mitosis analysis. 

<br /><br />Summary: <div>
arXiv:2509.03614v1 Announce Type: cross 
Abstract: Counting mitotic figures is time-intensive for pathologists and leads to inter-observer variability. Artificial intelligence (AI) promises a solution by automatically detecting mitotic figures while maintaining decision consistency. However, AI tools are susceptible to domain shift, where a significant drop in performance can occur due to differences in the training and testing sets, including morphological diversity between organs, species, and variations in staining protocols. Furthermore, the number of mitoses is much less than the count of normal nuclei, which introduces severely imbalanced data for the detection task. In this work, we formulate mitosis detection as a pixel-level segmentation and propose a teacher-student model that simultaneously addresses mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our method is based on a UNet segmentation backbone that integrates domain generalization modules, namely contrastive representation learning and domain-adversarial training. A teacher-student strategy is employed to generate pixel-level pseudo-masks not only for annotated mitoses and hard negatives but also for normal nuclei, thereby enhancing feature discrimination and improving robustness against domain shift. For the classification task, we introduce a multi-scale CNN classifier that leverages feature maps from the segmentation model within a multi-task learning paradigm. On the preliminary test set, the algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of 0.8414 in Track 2, demonstrating the effectiveness of integrating segmentation-based detection and classification into a unified framework for robust mitosis analysis.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection</title>
<link>https://arxiv.org/abs/2509.04460</link>
<guid>https://arxiv.org/abs/2509.04460</guid>
<content:encoded><![CDATA[
<div> keywords: large language models, peer review, AI-generated text detectors, content-based detection, CoCoNUTS 

Summary: 
The article discusses the potential risks posed by the integration of large language models (LLMs) into the peer review process and the concerns surrounding their use in generating substantive review content. Existing AI-generated text detectors are limited in distinguishing between language refinement and substantial content generation, leading to unfair suspicion of permissible AI-assisted language enhancement. To address this issue, the authors propose a shift towards content-based detection and introduce CoCoNUTS, a benchmark for evaluating AI-generated peer reviews. They also present CoCoDet, an AI review detector developed through a multi-task learning framework to enhance the accuracy and robustness of detecting AI involvement in review content. This work aims to facilitate more precise, equitable, and reliable detection methods for the use of LLMs in scholarly peer review. 

<br /><br />Summary: <div>
arXiv:2509.04460v1 Announce Type: cross 
Abstract: The growing integration of large language models (LLMs) into the peer review process presents potential risks to the fairness and reliability of scholarly evaluation. While LLMs offer valuable assistance for reviewers with language refinement, there is growing concern over their use to generate substantive review content. Existing general AI-generated text detectors are vulnerable to paraphrasing attacks and struggle to distinguish between surface language refinement and substantial content generation, suggesting that they primarily rely on stylistic cues. When applied to peer review, this limitation can result in unfairly suspecting reviews with permissible AI-assisted language enhancement, while failing to catch deceptively humanized AI-generated reviews. To address this, we propose a paradigm shift from style-based to content-based detection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark built upon a fine-grained dataset of AI-generated peer reviews, covering six distinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an AI review detector via a multi-task learning framework, designed to achieve more accurate and robust detection of AI involvement in review content. Our work offers a practical foundation for evaluating the use of LLMs in peer review, and contributes to the development of more precise, equitable, and reliable detection methods for real-world scholarly applications. Our code and data will be publicly available at https://github.com/Y1hanChen/COCONUTS.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking GPT-5 for biomedical natural language processing</title>
<link>https://arxiv.org/abs/2509.04462</link>
<guid>https://arxiv.org/abs/2509.04462</guid>
<content:encoded><![CDATA[
<div> Benchmark, GPT-5, Biomedical NLP, Performance, Question Answering<br />
<br />
Summary: 
The study evaluated the performance of GPT-5 and compared it with GPT-4 and GPT-4o on a standardized BioNLP benchmark. GPT-5 showed significant improvement across various tasks, achieving the strongest overall benchmark performance. It outperformed GPT-4 and GPT-4o in named entity recognition and relation extraction tasks. In question answering, GPT-5 reached high accuracy levels, surpassing supervised state-of-the-art results by a substantial margin. The model demonstrated strong performance in reasoning-oriented biomedical question answering, while areas like summarization and disease NER still require further improvement. The study highlighted the importance of tailored approaches for different tasks, with simple prompting sufficient for some tasks and more complex strategies necessary for others. Overall, the findings suggest that GPT-5 is a promising model for deployment-ready biomedical NLP tasks.<br /><br /> <div>
arXiv:2509.04462v1 Announce Type: cross 
Abstract: The rapid expansion of biomedical literature has heightened the need for scalable natural language processing (NLP) solutions. While GPT-4 substantially narrowed the gap with task-specific systems, especially in question answering, its performance across other domains remained uneven. We updated a standardized BioNLP benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot prompting across 12 datasets spanning six task families: named entity recognition, relation extraction, multi-label document classification, question answering, text summarization, and text simplification. Using fixed prompt templates, identical decoding parameters, and batch inference, we report primary metrics per dataset and include prior results for GPT-4, GPT-3.5, and LLaMA-2-13B for comparison. GPT-5 achieved the strongest overall benchmark performance, with macro-average scores rising to 0.557 under five-shot prompting versus 0.506 for GPT-4 and 0.508 for GPT-4o. On MedQA, GPT-5 reached 94.1% accuracy, exceeding the previous supervised state of the art by over fifty points, and attained parity with supervised systems on PubMedQA (0.734). In extraction tasks, GPT-5 delivered major gains in chemical NER (0.886 F1) and ChemProt relation extraction (0.616 F1), outperforming GPT-4 and GPT-4o, though summarization and disease NER still lagged behind domain-specific baselines. These results establish GPT-5 as a general-purpose model now offering deployment-ready performance for reasoning-oriented biomedical QA, while precision-critical extraction and evidence-dense summarization continue to favor fine-tuned or hybrid approaches. The benchmark delineates where simple prompting suffices and where retrieval-augmented or planning-based scaffolds are likely required, providing actionable guidance for BioNLP system design as frontier models advance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale Graph Neural Network for Turbulent Flow-Thermal Prediction Around a Complex-Shaped Pin-Fin</title>
<link>https://arxiv.org/abs/2509.04463</link>
<guid>https://arxiv.org/abs/2509.04463</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, flow prediction, complex geometries, surrogate modeling, fast computation <br />
<br />Summary: This study introduces a novel Graph Neural Network designed for predicting steady, turbulent flow and thermal behavior in a 2D channel with complex pin-fin geometries. The network was trained on a dataset generated through an automated framework that included geometry generation, meshing, and flow-field solutions. By converting each simulation into a graph structure, the network was able to accurately predict temperature, velocity, and pressure at each node. The results showed significant improvements in accuracy compared to traditional simulations, particularly in capturing boundary layers and recirculation. Additionally, the network reduced the computational time by 2-3 orders of magnitude, making it a fast and reliable surrogate for complex flow configurations. <div>
arXiv:2509.04463v1 Announce Type: cross 
Abstract: This study presents the development of a domain-responsive edge-aware multiscale Graph Neural Network for predicting steady, turbulent flow and thermal behavior in a two-dimensional channel containing arbitrarily shaped complex pin-fin geometries. The training dataset was constructed through an automated framework that integrated geometry generation, meshing, and flow-field solutions in ANSYS Fluent. The pin-fin geometry was parameterized using piecewise cubic splines, producing 1,000 diverse configurations through Latin Hypercube Sampling. Each simulation was converted into a graph structure, where nodes carried a feature vector containing spatial coordinates, a normalized streamwise position, one-hot boundary indicators, and a signed distance to the nearest boundary such as wall. This graph structure served as input to the newly developed Graph Neural Network, which was trained to predict temperature, velocity magnitude, and pressure at each node using data from ANSYS. The network predicted fields with outstanding accuracy, capturing boundary layers, recirculation, and the stagnation region upstream of the pin-fins while reducing wall time by 2-3 orders of magnitude. In conclusion, the novel graph neural network offered a fast and reliable surrogate for simulations in complex flow configurations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?</title>
<link>https://arxiv.org/abs/2509.04464</link>
<guid>https://arxiv.org/abs/2509.04464</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, uncertainty, diagnosis, knowledge gaps, reliability 

Summary: 

This study focuses on diagnosing the source of uncertainty in Large Language Models (LLMs) by analyzing patterns of disagreement among multiple generated responses. By employing an auxiliary LLM to reason about the likely sources of uncertainty, such as ambiguity in the input question or lack of relevant knowledge, the framework can identify specific missing facts or concepts contributing to uncertainty in cases involving knowledge gaps. The experiment validates the framework on AmbigQA, OpenBookQA, and MMLU-Pro, demonstrating its generality in diagnosing distinct uncertainty sources. The potential for manual interventions to improve LLM performance and reliability is highlighted through this diagnostic approach.  

<br /><br />Summary: <div>
arXiv:2509.04464v1 Announce Type: cross 
Abstract: Large language models (LLMs) have delivered significant breakthroughs across diverse domains but can still produce unreliable or misleading outputs, posing critical challenges for real-world applications. While many recent studies focus on quantifying model uncertainty, relatively little work has been devoted to \textit{diagnosing the source of uncertainty}. In this study, we show that, when an LLM is uncertain, the patterns of disagreement among its multiple generated responses contain rich clues about the underlying cause of uncertainty. To illustrate this point, we collect multiple responses from a target LLM and employ an auxiliary LLM to analyze their patterns of disagreement. The auxiliary model is tasked to reason about the likely source of uncertainty, such as whether it stems from ambiguity in the input question, a lack of relevant knowledge, or both. In cases involving knowledge gaps, the auxiliary model also identifies the specific missing facts or concepts contributing to the uncertainty. In our experiment, we validate our framework on AmbigQA, OpenBookQA, and MMLU-Pro, confirming its generality in diagnosing distinct uncertainty sources. Such diagnosis shows the potential for relevant manual interventions that improve LLM performance and reliability.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotionally-Aware Agents for Dispute Resolution</title>
<link>https://arxiv.org/abs/2509.04465</link>
<guid>https://arxiv.org/abs/2509.04465</guid>
<content:encoded><![CDATA[
<div> Keywords: conflict resolution, emotional expressions, text emotion recognition, dispute dialogues, large-language models

Summary:
This study examines how emotional expressions in text shape outcomes in dispute resolution, specifically in buyer-seller interactions. Using a large corpus of dispute dialogues, the researchers found that emotional expressions play a significant role in influencing subjective and objective resolution outcomes. They also compared the effectiveness of large-language models in emotion intensity annotation with previous methods and found that these models were more accurate in matching human annotator decisions. The findings support existing theoretical models on how emotions impact conflict escalation and resolution, suggesting that agent-based systems could potentially help manage disputes by recognizing and potentially mitigating emotional escalation. This research highlights the importance of understanding and utilizing emotional expressions in conflict resolution processes. 

<br /><br />Summary: 
1. Emotional expressions influence outcomes in dispute resolution, especially in buyer-seller interactions.
2. Large-language models are effective in emotion intensity annotation compared to previous methods.
3. Emotions play a significant role in conflict escalation and resolution.
4. Agent-based systems could help manage disputes by recognizing and mitigating emotional escalation.
5. Understanding emotional expressions is crucial for effective conflict resolution. <div>
arXiv:2509.04465v1 Announce Type: cross 
Abstract: In conflict, people use emotional expressions to shape their counterparts' thoughts, feelings, and actions. This paper explores whether automatic text emotion recognition offers insight into this influence in the context of dispute resolution. Prior work has shown the promise of such methods in negotiations; however, disputes evoke stronger emotions and different social processes. We use a large corpus of buyer-seller dispute dialogues to investigate how emotional expressions shape subjective and objective outcomes. We further demonstrate that large-language models yield considerably greater explanatory power than previous methods for emotion intensity annotation and better match the decisions of human annotators. Findings support existing theoretical models for how emotional expressions contribute to conflict escalation and resolution and suggest that agent-based systems could be useful in managing disputes by recognizing and potentially mitigating emotional escalation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just-in-time and distributed task representations in language models</title>
<link>https://arxiv.org/abs/2509.04466</link>
<guid>https://arxiv.org/abs/2509.04466</guid>
<content:encoded><![CDATA[
<div> evolve, task representations, models, context, computational process
Summary:<br /><br />In this study, the researchers explore how task representations evolve in language models during context learning. They find that transferrable task representations develop in non-monotonic and sporadic ways, distinct from high-level task categories, condensing multiple evidence into minimal task scopes. These representations are localized along the sequence dimension, indicating a just-in-time computational process for adapting to new evidence. The models rely on temporally-distributed representations for longer and composite tasks. Despite task identity being decodable throughout the context, the accrual of task representations occurs at specific tokens. This research sheds light on the dynamic nature of task learning in language models and the underlying computational mechanisms driving their adaptability. <br /><br />Summary: <div>
arXiv:2509.04466v1 Announce Type: cross 
Abstract: Many of language models' impressive capabilities originate from their in-context learning: based on instructions or examples, they can infer and perform new tasks without weight updates. In this work, we investigate \emph{when} representations for new tasks are formed in language models, and \emph{how} these representations change over the course of context. We focus on ''transferrable'' task representations -- vector representations that can restore task context in another instance of the model, even without the full prompt. We show that these representations evolve in non-monotonic and sporadic ways, and are distinct from a more inert representation of high-level task categories that persists throughout the context. Specifically, models often condense multiple evidence into these transferrable task representations, which align well with the performance improvement based on more examples in the context. However, this accrual process exhibits strong locality along the sequence dimension, coming online only at certain tokens -- despite task identity being reliably decodable throughout the context. Moreover, these local but transferrable task representations tend to capture minimal ''task scopes'', such as a semantically-independent subtask, and models rely on more temporally-distributed representations to support longer and composite tasks. This two-fold locality (temporal and semantic) underscores a kind of just-in-time computational process underlying language models' ability to adapt to new evidence and learn new tasks on the fly.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode Disaggregation in Inference</title>
<link>https://arxiv.org/abs/2509.04467</link>
<guid>https://arxiv.org/abs/2509.04467</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, Model Pruning, Prefill-decode disaggregation, Block pruning, KV Cache pruning

Summary: 
Large Language Models (LLMs) are powerful but computationally expensive. Model pruning can help reduce costs, but current methods do not consider prefill-decode (PD) disaggregation. A novel pruning method is proposed in this paper for PD disaggregation, improving block and KV Cache pruning by iteratively removing blocks in the prefill and decode stages independently. Additionally, a token-aware cache pruning mechanism is introduced to reduce communication costs during decode by selectively reusing entries for certain token sequences in selected layers. Experimental results show that the approach achieves significant speedups and bandwidth reductions in both PD disaggregation and unified settings. Under default settings, the method achieves a 20.56% inference speedup and a 4.95 times reduction in data transmission bandwidth consumption. 

<br /><br />Summary: <div>
arXiv:2509.04467v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However, existing methods often ignore the characteristics of prefill-decode (PD) disaggregation in practice. In this paper, we propose a novel pruning method for PD disaggregation inference, enabling more precise and efficient block and KV Cache pruning. Our approach constructs pruning and distillation sets to perform iterative block removal independently for the prefill and decode stages, obtaining better pruning solutions. Moreover, we introduce a token-aware cache pruning mechanism that retains all KV Cache in the prefill stage but selectively reuses entries for the first and last token sequences in selected layers during decode, reducing communication costs with minimal overhead. Extensive experiments demonstrate that our approach consistently achieves strong performance in both PD disaggregation and PD unified settings without disaggregation. Under the default settings, our method achieves a 20.56% inference speedup and a 4.95 times reduction in data transmission bandwidth consumption.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark Study</title>
<link>https://arxiv.org/abs/2509.04468</link>
<guid>https://arxiv.org/abs/2509.04468</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, financial applications, professional certifications, domain-specific knowledge retrieval, model selection

Summary:
The study evaluates state-of-the-art large language models (LLMs) in specialized financial contexts using 1,560 multiple-choice questions from CFA mock exams. Three types of models are compared based on design priorities: multi-modal and powerful, reasoning-specialized and accurate, and efficiency-optimized. Results show that reasoning-oriented models perform better in zero-shot settings, and a Retrieval-Augmented Generation pipeline significantly improves reasoning accuracy in complex scenarios by integrating CFA curriculum content. Knowledge gaps are identified as the main failure mode, with minimal impact from text readability. This research provides insights for deploying LLMs in finance, guiding practitioners on model selection and cost-performance optimization.<br /><br />Summary: <div>
arXiv:2509.04468v1 Announce Type: cross 
Abstract: The rapid advancement of large language models presents significant opportunities for financial applications, yet systematic evaluation in specialized financial contexts remains limited. This study presents the first comprehensive evaluation of state-of-the-art LLMs using 1,560 multiple-choice questions from official mock exams across Levels I-III of CFA, most rigorous professional certifications globally that mirror real-world financial analysis complexity. We compare models distinguished by core design priorities: multi-modal and computationally powerful, reasoning-specialized and highly accurate, and lightweight efficiency-optimized.
  We assess models under zero-shot prompting and through a novel Retrieval-Augmented Generation pipeline that integrates official CFA curriculum content. The RAG system achieves precise domain-specific knowledge retrieval through hierarchical knowledge organization and structured query generation, significantly enhancing reasoning accuracy in professional financial certification evaluation.
  Results reveal that reasoning-oriented models consistently outperform others in zero-shot settings, while the RAG pipeline provides substantial improvements particularly for complex scenarios. Comprehensive error analysis identifies knowledge gaps as the primary failure mode, with minimal impact from text readability. These findings provide actionable insights for LLM deployment in finance, offering practitioners evidence-based guidance for model selection and cost-performance optimization.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing</title>
<link>https://arxiv.org/abs/2509.04469</link>
<guid>https://arxiv.org/abs/2509.04469</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal large language models, invoice document datasets, zero-shot prompting, image processing, structured parsing

Summary:
This paper evaluates the performance of eight multi-modal large language models on three diverse invoice document datasets using zero-shot prompting. The models compared are from families GPT-5, Gemini 2.5, and open-source Gemma 3. Two processing strategies are analyzed: direct image processing with multi-modal capabilities and structured parsing converting documents to markdown first. The results indicate that native image processing generally outperforms structured approaches, although performance varies depending on the model type and document characteristics. This benchmark study provides valuable insights for choosing appropriate models and processing strategies for automated document systems. The code used for the study is available online. 

<br /><br />Summary: <div>
arXiv:2509.04469v1 Announce Type: cross 
Abstract: This paper benchmarks eight multi-modal large language models from three families (GPT-5, Gemini 2.5, and open-source Gemma 3) on three diverse openly available invoice document datasets using zero-shot prompting. We compare two processing strategies: direct image processing using multi-modal capabilities and a structured parsing approach converting documents to markdown first. Results show native image processing generally outperforms structured approaches, with performance varying across model types and document characteristics. This benchmark provides insights for selecting appropriate models and processing strategies for automated document systems. Our code is available online.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COCORELI: Cooperative, Compositional Reconstitution \&amp; Execution of Language Instructions</title>
<link>https://arxiv.org/abs/2509.04470</link>
<guid>https://arxiv.org/abs/2509.04470</guid>
<content:encoded><![CDATA[
<div> Abstraction, Language Models, Spatial Reasoning, Collaborative Construction Tasks, ToolBench API<br />
<br />
Summary: COCORELI is a hybrid agent framework that addresses the limitations of large language models (LLMs) by integrating medium-sized LLM agents with novel abstraction mechanisms and a discourse module. It is designed to handle tasks requiring complex instructions, minimizing hallucination, and spatial reasoning. In experiments on collaborative construction tasks, COCORELI outperformed single-LLM CoT and agentic LLM systems, all using larger LLMs. It demonstrated the ability to avoid hallucinations, identify missing information, ask for clarifications, and update its learned objects. COCORELI's abstraction abilities were also showcased in the ToolBench API completion task, highlighting its versatility beyond just environment parsing. <div>
arXiv:2509.04470v1 Announce Type: cross 
Abstract: We present COCORELI, a hybrid agent framework designed to tackle the limitations of large language models (LLMs) in tasks requiring: following complex instructions, minimizing hallucination, and spatial reasoning. COCORELI integrates medium-sized LLM agents with novel abstraction mechanisms and a discourse module to parse instructions to in-context learn dynamic, high-level representations of the environment. Experiments on natural collaborative construction tasks show that COCORELI outperforms single-LLM CoT and agentic LLM systems, all using larger LLMs. It manages to largely avoid hallucinations, identify missing information, ask for clarifications, and update its learned objects. COCORELI's abstraction abilities extend beyond ENVIRONMENT, as shown in the ToolBench API completion task.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSAIC: A Multilingual, Taxonomy-Agnostic, and Computationally Efficient Approach for Radiological Report Classification</title>
<link>https://arxiv.org/abs/2509.04471</link>
<guid>https://arxiv.org/abs/2509.04471</guid>
<content:encoded><![CDATA[
<div> radiology reports, clinical information, imaging models, MOSAIC, multilingual <br />
<br />
Summary: 
The article introduces MOSAIC, a novel approach for radiological report classification that leverages radiology reports' rich clinical information to train imaging models efficiently. MOSAIC overcomes limitations of existing methods by being multilingual, taxonomy-agnostic, and computationally efficient. It is built on a compact open-access language model, MedGemma-4B, enabling zero-/few-shot prompting and lightweight fine-tuning on consumer-grade GPUs. Evaluation across seven datasets in multiple languages and modalities shows MOSAIC achieving high performance levels with minimal annotated samples, making it a practical alternative to large or proprietary language models in clinical settings. The model achieves an impressive mean macro F1 score of 88 across chest X-ray datasets and demonstrates the effectiveness of data augmentation in reducing the required amount of annotated data for training. MOSAIC's code and models are open-source, inviting the community to explore its capabilities across various languages, taxonomies, and modalities. <div>
arXiv:2509.04471v1 Announce Type: cross 
Abstract: Radiology reports contain rich clinical information that can be used to train imaging models without relying on costly manual annotation. However, existing approaches face critical limitations: rule-based methods struggle with linguistic variability, supervised models require large annotated datasets, and recent LLM-based systems depend on closed-source or resource-intensive models that are unsuitable for clinical use. Moreover, current solutions are largely restricted to English and single-modality, single-taxonomy datasets. We introduce MOSAIC, a multilingual, taxonomy-agnostic, and computationally efficient approach for radiological report classification. Built on a compact open-access language model (MedGemma-4B), MOSAIC supports both zero-/few-shot prompting and lightweight fine-tuning, enabling deployment on consumer-grade GPUs. We evaluate MOSAIC across seven datasets in English, Spanish, French, and Danish, spanning multiple imaging modalities and label taxonomies. The model achieves a mean macro F1 score of 88 across five chest X-ray datasets, approaching or exceeding expert-level performance, while requiring only 24 GB of GPU memory. With data augmentation, as few as 80 annotated samples are sufficient to reach a weighted F1 score of 82 on Danish reports, compared to 86 with the full 1600-sample training set. MOSAIC offers a practical alternative to large or proprietary LLMs in clinical settings. Code and models are open-source. We invite the community to evaluate and extend MOSAIC on new languages, taxonomies, and modalities.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECAP: REwriting Conversations for Intent Understanding in Agentic Planning</title>
<link>https://arxiv.org/abs/2509.04472</link>
<guid>https://arxiv.org/abs/2509.04472</guid>
<content:encoded><![CDATA[
<div> intent rewriting, conversational assistants, large language models, dialogue systems, user intent

Summary:
RECAP (REwriting Conversations for Agent Planning) is proposed as a benchmark to evaluate intent rewriting in user-agent dialogues. It aims to address challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. An LLM-based evaluator is introduced to assess planning utility based on rewritten intents. A prompt-based rewriting approach is developed and shown to outperform baselines. Fine-tuning two DPO-based rewriters further improves utility. The results highlight intent rewriting as a critical component for enhancing agent planning in open-domain dialogue systems. <div>
arXiv:2509.04472v1 Announce Type: cross 
Abstract: Understanding user intent is essential for effective planning in conversational assistants, particularly those powered by large language models (LLMs) coordinating multiple agents. However, real-world dialogues are often ambiguous, underspecified, or dynamic, making intent detection a persistent challenge. Traditional classification-based approaches struggle to generalize in open-ended settings, leading to brittle interpretations and poor downstream planning. We propose RECAP (REwriting Conversations for Agent Planning), a new benchmark designed to evaluate and advance intent rewriting, reframing user-agent dialogues into concise representations of user goals. RECAP captures diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. Alongside the dataset, we introduce an LLM-based evaluator that assesses planning utility given the rewritten intent. Using RECAP, we develop a prompt-based rewriting approach that outperforms baselines. We further demonstrate that fine-tuning two DPO-based rewriters yields additional utility gains. Our results highlight intent rewriting as a critical and tractable component for improving agent planning in open-domain dialogue systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechLLM: Unified Speech and Language Model for Enhanced Multi-Task Understanding in Low Resource Settings</title>
<link>https://arxiv.org/abs/2509.04473</link>
<guid>https://arxiv.org/abs/2509.04473</guid>
<content:encoded><![CDATA[
<div> adapter, speech embeddings, LLM-compatible tokens, automatic speech recognition, synthetic dataset annotation

Summary: 
An innovative solution using a parameter-efficient adapter to convert speech embeddings into tokens compatible with large language models (LLMs) has been proposed. Through this approach, significant performance enhancements have been achieved in various tasks such as automatic speech recognition (ASR), named entity recognition (NER), and sentiment analysis (SA). By leveraging an LLM-based synthetic dataset annotation technique, labeling costs have been reduced effectively. The adapter, which utilizes fewer trainable parameters, has led to a 26% relative improvement in Word Error Rates (WER) for the LibriSpeech ASR task, a 6.3% relative increase in F1 score for the NER task, and a 32% relative boost in F1 score for the SA task. Additionally, techniques such as adding a classifier regularizer and optimizing the LLM with Low-Rank Adaptation (LoRA) have further improved performance, resulting in a notable increase in the Spoken Language Understanding Evaluation (SLUE) score by 6.6% and 9.5%. 

<br /><br />Summary: <div>
arXiv:2509.04473v1 Announce Type: cross 
Abstract: While integrating speech encoder with LLM requires substantial data and resources, use cases face limitations due to insufficient availability. To address this, we propose a solution with a parameter-efficient adapter that converts speech embeddings into LLM-compatible tokens, focusing on end-to-end automatic speech recognition (ASR), named entity recognition (NER), and sentiment analysis (SA). To reduce labeling costs, we employ an LLM-based synthetic dataset annotation technique. The proposed adapter, using 7x fewer trainable parameters, achieves significant performance gains: a 26% relative Word Error Rates (WER) improvement on the LibriSpeech ASR task, a 6.3% relative F1 score increase on the NER task, and a 32% relative F1 score boost on the SA task. Moreover, using advanced techniques such as adding a classifier regularizer and optimizing the LLM with Low-Rank Adaptation (LoRA) yields notable performance gains, with Spoken Language Understanding Evaluation (SLUE) score improvement of 6.6% and 9.5%
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling</title>
<link>https://arxiv.org/abs/2509.04474</link>
<guid>https://arxiv.org/abs/2509.04474</guid>
<content:encoded><![CDATA[
<div> scalable language models, test-time scaling, speculative decoding, benchmark, reasoning 

Summary: 
The article discusses the inefficiencies of test-time scaling in large language models (LLMs) due to redundant reasoning traces. It introduces a benchmark to evaluate speculative decoding methods for accelerating LLM test-time scaling. The benchmark allows for a fair comparison of model-based, training-based, and n-gram-based speculative decoding approaches. The experiments highlight the effectiveness of simple n-gram-based methods in capturing repetitive patterns and accelerating test-time scaling. Integrating n-gram-based methods with model-based or training-based approaches can balance acceleration for both repetitive and diverse reasoning paths in test-time scaling. This research aims to spur further investigation into speculative decoding for test-time scaling, ultimately enabling faster and more practical reasoning in LLMs by improving the handling of repetitive and diverse reasoning paths. 

<br /><br />Summary: <div>
arXiv:2509.04474v1 Announce Type: cross 
Abstract: Test-time scaling has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs) by allocating additional computational resources during inference. However, this paradigm is inherently inefficient due to the generation of redundant and repetitive reasoning traces, leading to significant computational overhead. Speculative decoding offers a promising avenue for mitigating this inefficiency, yet its efficacy in the structured, repetition-rich context of test-time scaling remains largely unexplored. To bridge this gap, we introduce the first comprehensive benchmark designed to evaluate speculative decoding methods for accelerating LLM test-time scaling. Our benchmark provides consistent experimental protocols across representative test-time scaling paradigms (e.g., Best-of-N sampling and multi-round thinking), enabling a fair comparison of three major categories of speculative decoding: model-based, training-based, and n-gram-based methods. Extensive experiments reveal that simple n-gram-based methods effectively capture repetitive patterns, demonstrating unique potential in accelerating test-time scaling. This phenomenon demonstrates the value of integrating n-gram-based methods with model-based or training-based approaches to balance acceleration for both repetitive and diverse reasoning in test-time scaling. We hope this benchmark spurs further research on speculative decoding for test-time scaling, enabling faster and more practical reasoning in LLMs through better handling of repetitive and diverse reasoning paths.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute</title>
<link>https://arxiv.org/abs/2509.04475</link>
<guid>https://arxiv.org/abs/2509.04475</guid>
<content:encoded><![CDATA[
<div> scaling, Large Language Models, reasoning, Parallel thinking, Tunnel Vision 

Summary:
The article introduces a new scaling paradigm called native thought parallelism to overcome the limitations of the traditional test-time compute scaling strategy used in Large Language Models (LLMs). The phenomenon of "Tunnel Vision" is discussed, where sequential reasoning locks a model into suboptimal paths. The ParaThinker framework is presented, which trains LLMs to generate multiple diverse reasoning paths in parallel to synthesize a superior final answer. By exploring different lines of thought simultaneously, ParaThinker effectively avoids Tunnel Vision and unlocks the model's latent reasoning potential. The approach of scaling compute in parallel is shown to be more effective and efficient than scaling sequentially. Results on challenging reasoning benchmarks demonstrate significant accuracy improvements with negligible latency overhead, enabling smaller models to outperform larger ones. The study establishes parallel thinking as a critical dimension for scaling future LLMs. 

<br /><br />Summary: <div>
arXiv:2509.04475v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have been driven by test-time compute scaling - a strategy that improves reasoning by generating longer, sequential thought processes. While effective, this approach encounters a significant bottleneck as computation increases, where further computation offers only marginal performance gains. We argue this ceiling is not an inherent limit of the model's capability but a flaw in the scaling strategy itself, a phenomenon we term "Tunnel Vision", where a model's imperfect initial steps lock it into a suboptimal reasoning path. To overcome this, we introduce a new scaling paradigm: native thought parallelism. We present ParaThinker, an end-to-end framework that trains an LLM to generate multiple, diverse reasoning paths in parallel and synthesize them into a superior final answer. By exploring different lines of thoughts simultaneously, ParaThinker effectively sidesteps the Tunnel Vision issue and unlocks the model's latent reasoning potential. Our approach demonstrates that scaling compute in parallel (width) is a more effective and efficient way to superior reasoning than simply scaling sequentially (depth). On challenging reasoning benchmarks, ParaThinker achieves substantial accuracy improvements over sequential LLMs (12.3% for 1.5B and 7.5% for 7B models on average with 8 parallel paths), while adding only negligible latency overhead (7.1%). This enables smaller models to surpass much larger counterparts and establishes parallel thinking as a critical, efficient dimension for scaling future LLMs.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Text-to-Molecule Models with Context-Aware Tokenization</title>
<link>https://arxiv.org/abs/2509.04476</link>
<guid>https://arxiv.org/abs/2509.04476</guid>
<content:encoded><![CDATA[
<div> substructure-level tokenization, text-to-molecule models, molecule structures, importance-based training, CAMT5 <br />
<br />
Summary: 
The article introduces a novel text-to-molecule model called Context-Aware Molecular T5 (CAMT5) that addresses the limitations of existing models in capturing global structural context within molecules. By using substructure-level tokenization, CAMT5 focuses on the importance of substructure-level contexts, such as ring systems, in understanding molecule structures. The model also employs an importance-based training strategy to prioritize key substructures, allowing it to better capture molecular semantics. Experimental results demonstrate that CAMT5 outperforms state-of-the-art methods with significantly fewer training tokens. Furthermore, the article proposes an ensemble strategy to further enhance the generation performance by aggregating the outputs of different text-to-molecule models. The code for CAMT5 is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2509.04476v1 Announce Type: cross 
Abstract: Recently, text-to-molecule models have shown great potential across various chemical applications, e.g., drug-discovery. These models adapt language models to molecular data by representing molecules as sequences of atoms. However, they rely on atom-level tokenizations, which primarily focus on modeling local connectivity, thereby limiting the ability of models to capture the global structural context within molecules. To tackle this issue, we propose a novel text-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by the significance of the substructure-level contexts in understanding molecule structures, e.g., ring systems, we introduce substructure-level tokenization for text-to-molecule models. Building on our tokenization scheme, we develop an importance-based training strategy that prioritizes key substructures, enabling CAMT5 to better capture the molecular semantics. Extensive experiments verify the superiority of CAMT5 in various text-to-molecule generation tasks. Intriguingly, we find that CAMT5 outperforms the state-of-the-art methods using only 2% of training tokens. In addition, we propose a simple yet effective ensemble strategy that aggregates the outputs of text-to-molecule models to further boost the generation performance. Code is available at https://github.com/Songhyeontae/CAMT5.git.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Clustering, No Routing: How Transformers Actually Process Rare Tokens</title>
<link>https://arxiv.org/abs/2509.04479</link>
<guid>https://arxiv.org/abs/2509.04479</guid>
<content:encoded><![CDATA[
<div> specialization, rare token prediction, plateau neurons, computational regimes, attention mechanisms

Summary:
Rare token prediction poses a challenge for large language models, leading to the identification of specialized "plateau" neurons. This study delves into the functional organization of these neurons using GPT-2 XL and Pythia models. The research reveals that processing rare tokens requires a separate set of plateau neurons, distinct from those handling common tokens, resulting in dual computational regimes. Interestingly, these plateau neurons are distributed across the model rather than being clustered together. Furthermore, attention mechanisms do not exhibit a bias towards routing to these specialized neurons. Overall, the specialization for rare tokens emerges through distributed, training-induced differentiation rather than modular clustering, allowing for context-sensitive flexibility while efficiently allocating computational resources. <div>
arXiv:2509.04479v1 Announce Type: cross 
Abstract: Large language models struggle with rare token prediction, yet the mechanisms driving their specialization remain unclear. Prior work identified specialized ``plateau'' neurons for rare tokens following distinctive three-regime influence patterns \cite{liu2025emergent}, but their functional organization is unknown. We investigate this through neuron influence analyses, graph-based clustering, and attention head ablations in GPT-2 XL and Pythia models. Our findings show that: (1) rare token processing requires additional plateau neurons beyond the power-law regime sufficient for common tokens, forming dual computational regimes; (2) plateau neurons are spatially distributed rather than forming modular clusters; and (3) attention mechanisms exhibit no preferential routing to specialists. These results demonstrate that rare token specialization arises through distributed, training-driven differentiation rather than architectural modularity, preserving context-sensitive flexibility while achieving adaptive capacity allocation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game Environments</title>
<link>https://arxiv.org/abs/2509.04481</link>
<guid>https://arxiv.org/abs/2509.04481</guid>
<content:encoded><![CDATA[
<div> pipeline, narrative prompts, tile-based game scenes, spatial predicates, spatial rules

Summary:
The article introduces a lightweight pipeline that converts short narrative prompts into 2D tile-based game scenes, aligning with the temporal structure of stories. It identifies three key time frames in LLM-generated narratives, extracts spatial predicates in "Object-Relation-Object" form, and utilizes affordance-aware semantic embeddings to retrieve visual assets. A layered terrain is created using Cellular Automata, and objects are placed based on spatial rules grounded in the predicate structure. The system was evaluated with ten diverse stories, assessing tile-object matching, affordance-layer alignment, and spatial constraint satisfaction across frames. This prototype offers a scalable solution for generating narrative-driven game scenes and sets the groundwork for future research on multi-frame continuity, symbolic tracking, and multi-agent coordination in story-centered PCG. 

<br /><br />Summary: <div>
arXiv:2509.04481v1 Announce Type: cross 
Abstract: Recent advances in large language models(LLMs) enable compelling story generation, but connecting narrative text to playable visual environments remains an open challenge in procedural content generation(PCG). We present a lightweight pipeline that transforms short narrative prompts into a sequence of 2D tile-based game scenes, reflecting the temporal structure of stories. Given an LLM-generated narrative, our system identifies three key time frames, extracts spatial predicates in the form of "Object-Relation-Object" triples, and retrieves visual assets using affordance-aware semantic embeddings from the GameTileNet dataset. A layered terrain is generated using Cellular Automata, and objects are placed using spatial rules grounded in the predicate structure. We evaluated our system in ten diverse stories, analyzing tile-object matching, affordance-layer alignment, and spatial constraint satisfaction across frames. This prototype offers a scalable approach to narrative-driven scene generation and lays the foundation for future work on multi-frame continuity, symbolic tracking, and multi-agent coordination in story-centered PCG.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented Large Language Models for Healthcare</title>
<link>https://arxiv.org/abs/2509.04482</link>
<guid>https://arxiv.org/abs/2509.04482</guid>
<content:encoded><![CDATA[
<div> energy-based model, reliable abstention, retrieval-augmented generation, safety-critical domains, semantic corpus

Summary:
The article introduces an energy-based model (EBM) for reliable abstention in retrieval-augmented generation (RAG) systems, focusing on safety-critical areas like women's health. The EBM learns a smooth energy landscape over a large semantic corpus of guideline-derived questions to determine when to generate or abstain. Comparative benchmarks against softmax and kNN approaches show that the EBM outperforms in abstention performance on difficult, near-distribution queries, reaching AUROC 0.961 and reducing FPR@95. The EBM's strength lies in its energy scoring head, providing a more reliable confidence signal than probability-based softmax. The study demonstrates that energy-based abstention scoring offers scalability and interpretability for safe RAG systems, emphasizing the importance of reliable abstention in critical domains. <div>
arXiv:2509.04482v1 Announce Type: cross 
Abstract: Reliable abstention is critical for retrieval-augmented generation (RAG) systems, particularly in safety-critical domains such as women's health, where incorrect answers can lead to harm. We present an energy-based model (EBM) that learns a smooth energy landscape over a dense semantic corpus of 2.6M guideline-derived questions, enabling the system to decide when to generate or abstain. We benchmark the EBM against a calibrated softmax baseline and a k-nearest neighbour (kNN) density heuristic across both easy and hard abstention splits, where hard cases are semantically challenging near-distribution queries. The EBM achieves superior abstention performance abstention on semantically hard cases, reaching AUROC 0.961 versus 0.950 for softmax, while also reducing FPR@95 (0.235 vs 0.331). On easy negatives, performance is comparable across methods, but the EBM's advantage becomes most pronounced in safety-critical hard distributions. A comprehensive ablation with controlled negative sampling and fair data exposure shows that robustness stems primarily from the energy scoring head, while the inclusion or exclusion of specific negative types (hard, easy, mixed) sharpens decision boundaries but is not essential for generalisation to hard cases. These results demonstrate that energy-based abstention scoring offers a more reliable confidence signal than probability-based softmax confidence, providing a scalable and interpretable foundation for safe RAG systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DecMetrics: Structured Claim Decomposition Scoring for Factually Consistent LLM Outputs</title>
<link>https://arxiv.org/abs/2509.04483</link>
<guid>https://arxiv.org/abs/2509.04483</guid>
<content:encoded><![CDATA[
<div> metrics, claim decomposition, fact-checking, generative methods, evaluation

Summary:
DecMetrics introduces three new metrics, COMPLETENESS, CORRECTNESS, and SEMANTIC ENTROPY, to evaluate the quality of claims produced by decomposition models. These metrics aim to assess the completeness, correctness, and semantic entropy of atomic claims generated through claim decomposition. By integrating these metrics as a reward function, a lightweight claim decomposition model is developed and optimized for performance. The approach sets a benchmark for claim decomposition in the fact-checking process, enhancing the reliability and effectiveness of fact-checking systems. The focus on evaluating the quality of decomposed claims provides a valuable contribution to the field, addressing a gap in existing research that primarily concentrates on generative methods for claim decomposition. <div>
arXiv:2509.04483v1 Announce Type: cross 
Abstract: Claim decomposition plays a crucial role in the fact-checking process by breaking down complex claims into simpler atomic components and identifying their unfactual elements. Despite its importance, current research primarily focuses on generative methods for decomposition, with insufficient emphasis on evaluating the quality of these decomposed atomic claims. To bridge this gap, we introduce \textbf{DecMetrics}, which comprises three new metrics: \texttt{COMPLETENESS}, \texttt{CORRECTNESS}, and \texttt{SEMANTIC ENTROPY}, designed to automatically assess the quality of claims produced by decomposition models. Utilizing these metrics, we develop a lightweight claim decomposition model, optimizing its performance through the integration of these metrics as a reward function. Through automatic evaluation, our approach aims to set a benchmark for claim decomposition, enhancing both the reliability and effectiveness of fact-checking systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors</title>
<link>https://arxiv.org/abs/2509.04484</link>
<guid>https://arxiv.org/abs/2509.04484</guid>
<content:encoded><![CDATA[
<div> Automated Feedback, Peer Review, Review Comments, RevUtil Dataset, Fine-tuned Models<br />
<br />
Summary: Providing constructive feedback to authors is crucial in peer review. To ensure high reviewing quality despite time constraints, automated support systems are necessary. Four key aspects of review comments that enhance their utility for authors include Actionability, Grounding & Specificity, Verifiability, and Helpfulness. The RevUtil dataset, comprising human-labeled and synthetically labeled review comments, serves as a tool for evaluating and developing models assessing review comments. By fine-tuning models on the RevUtil dataset, researchers achieve agreement levels with humans comparable to, and sometimes surpassing, powerful closed models like GPT-4o. However, machine-generated reviews generally fall short of human reviews across the four key aspects identified in review comments. <div>
arXiv:2509.04484v1 Announce Type: cross 
Abstract: Providing constructive feedback to paper authors is a core component of peer review. With reviewers increasingly having less time to perform reviews, automated support systems are required to ensure high reviewing quality, thus making the feedback in reviews useful for authors. To this end, we identify four key aspects of review comments (individual points in weakness sections of reviews) that drive the utility for authors: Actionability, Grounding & Specificity, Verifiability, and Helpfulness. To enable evaluation and development of models assessing review comments, we introduce the RevUtil dataset. We collect 1,430 human-labeled review comments and scale our data with 10k synthetically labeled comments for training purposes. The synthetic data additionally contains rationales, i.e., explanations for the aspect score of a review comment. Employing the RevUtil dataset, we benchmark fine-tuned models for assessing review comments on these aspects and generating rationales. Our experiments demonstrate that these fine-tuned models achieve agreement levels with humans comparable to, and in some cases exceeding, those of powerful closed models like GPT-4o. Our analysis further reveals that machine-generated reviews generally underperform human reviews on our four aspects.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records</title>
<link>https://arxiv.org/abs/2509.04485</link>
<guid>https://arxiv.org/abs/2509.04485</guid>
<content:encoded><![CDATA[
<div> Keywords: ASCENDgpt, cardiovascular risk prediction, electronic health records, transformer-based model, phenotype-aware tokenization

Summary:
ASCENDgpt is a transformer-based model designed for cardiovascular risk prediction from electronic health records (EHRs). It employs a novel phenotype-aware tokenization scheme that consolidates raw ICD codes into clinically meaningful phenotype tokens, reducing vocabulary size significantly. Pretrained on sequences from unique individuals, ASCENDgpt achieves strong performance in predicting five cardiovascular outcomes, including myocardial infarction, stroke, and cardiovascular death. The model demonstrates excellent discrimination on a test set, with an average C-index of 0.816. The phenotype-based approach ensures clinically interpretable predictions while maintaining computational efficiency. This study highlights the effectiveness of domain-specific tokenization and pretraining for EHR-based risk prediction tasks.<br /><br />Summary: <div>
arXiv:2509.04485v1 Announce Type: cross 
Abstract: We present ASCENDgpt, a transformer-based model specifically designed for cardiovascular risk prediction from longitudinal electronic health records (EHRs). Our approach introduces a novel phenotype-aware tokenization scheme that maps 47,155 raw ICD codes to 176 clinically meaningful phenotype tokens, achieving 99.6\% consolidation of diagnosis codes while preserving semantic information. This phenotype mapping contributes to a total vocabulary of 10,442 tokens - a 77.9\% reduction when compared with using raw ICD codes directly. We pretrain ASCENDgpt on sequences derived from 19402 unique individuals using a masked language modeling objective, then fine-tune for time-to-event prediction of five cardiovascular outcomes: myocardial infarction (MI), stroke, major adverse cardiovascular events (MACE), cardiovascular death, and all-cause mortality. Our model achieves excellent discrimination on the held-out test set with an average C-index of 0.816, demonstrating strong performance across all outcomes (MI: 0.792, stroke: 0.824, MACE: 0.800, cardiovascular death: 0.842, all-cause mortality: 0.824). The phenotype-based approach enables clinically interpretable predictions while maintaining computational efficiency. Our work demonstrates the effectiveness of domain-specific tokenization and pretraining for EHR-based risk prediction tasks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Serialized Output Prompting for Large Language Model-based Multi-Talker Speech Recognition</title>
<link>https://arxiv.org/abs/2509.04488</link>
<guid>https://arxiv.org/abs/2509.04488</guid>
<content:encoded><![CDATA[
<div> Prompt, multi-talker automatic speech recognition, large language models, separated output prompts, structured prompts<br />
Summary:<br />
The paper proposes a method, SOP-MT-ASR, to enhance multi-talker automatic speech recognition systems using serialized output prompts (SOP) to guide large language models. By inserting Separator and serialized CTC layers after the speech encoder, the system separates and extracts MT content in a first-speaking-first-out manner. The extracted SOP serves as a structured prompt for the LLMs, improving system performance. A three-stage training strategy is designed for effective model training. Experimental results on the LibriMix dataset show that while the LLM-based SOT model performs well in two-talker scenarios, it struggles in more complex three-talker scenarios. The proposed SOP approach significantly enhances performance in both scenarios. <br /><br /> <div>
arXiv:2509.04488v1 Announce Type: cross 
Abstract: Prompts are crucial for task definition and for improving the performance of large language models (LLM)-based systems. However, existing LLM-based multi-talker (MT) automatic speech recognition (ASR) systems either omit prompts or rely on simple task-definition prompts, with no prior work exploring the design of prompts to enhance performance. In this paper, we propose extracting serialized output prompts (SOP) and explicitly guiding the LLM using structured prompts to improve system performance (SOP-MT-ASR). A Separator and serialized Connectionist Temporal Classification (CTC) layers are inserted after the speech encoder to separate and extract MT content from the mixed speech encoding in a first-speaking-first-out manner. Subsequently, the SOP, which serves as a prompt for LLMs, is obtained by decoding the serialized CTC outputs using greedy search. To train the model effectively, we design a three-stage training strategy, consisting of serialized output training (SOT) fine-tuning, serialized speech information extraction, and SOP-based adaptation. Experimental results on the LibriMix dataset show that, although the LLM-based SOT model performs well in the two-talker scenario, it fails to fully leverage LLMs under more complex conditions, such as the three-talker scenario. The proposed SOP approach significantly improved performance under both two- and three-talker conditions.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR</title>
<link>https://arxiv.org/abs/2509.04491</link>
<guid>https://arxiv.org/abs/2509.04491</guid>
<content:encoded><![CDATA[
<div> Subtitle, Weakly Supervised, Automatic Speech Recognition, Pseudo-labeled datasets, Transcription accuracy
Summary: 
Subtitle text and spoken audio discrepancies limit the applicability of TV subtitles in Automatic Speech Recognition (ASR). This study proposes a novel approach using subtitles as context-rich prompts rather than direct supervision signals. Generated pseudo transcripts, guided by subtitles, are iteratively refined to improve transcription accuracy. A weighted attention mechanism emphasizes relevant subtitle tokens during inference, leading to significant improvements in transcription quality. The method provides high-quality pseudo-labeled datasets for training robust ASR systems. Enhanced transcripts offer a valuable resource for developing accurate and efficient ASR models. <div>
arXiv:2509.04491v1 Announce Type: cross 
Abstract: This study proposes a novel approach to using TV subtitles within a weakly supervised (WS) Automatic Speech Recognition (ASR) framework. Although TV subtitles are readily available, their imprecise alignment with corresponding audio limits their applicability as supervised targets for verbatim transcription. Rather than using subtitles as direct supervision signals, our method reimagines them as context-rich prompts. This design enables the model to handle discrepancies between spoken audio and subtitle text. Instead, generated pseudo transcripts become the primary targets, with subtitles acting as guiding cues for iterative refinement. To further enhance the process, we introduce a weighted attention mechanism that emphasizes relevant subtitle tokens during inference. Our experiments demonstrate significant improvements in transcription accuracy, highlighting the effectiveness of the proposed method in refining transcripts. These enhanced pseudo-labeled datasets provide high-quality foundational resources for training robust ASR systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate</title>
<link>https://arxiv.org/abs/2509.04492</link>
<guid>https://arxiv.org/abs/2509.04492</guid>
<content:encoded><![CDATA[
<div> Keywords: Hallucination detection, Large Language Models, Question Answering, Entropy Production Rate, Supervised learning 

Summary: 
- This paper introduces a methodology for detecting hallucinations in outputs of Large Language Models (LLMs) for Question Answering tasks.
- The approach focuses on scenarios with limited data access, like black-box LLM APIs, by deriving uncertainty indicators from available log-probabilities during decoding.
- An Entropy Production Rate (EPR) metric is developed initially, and later enhanced with supervised learning to improve detection performance.
- The learned model uses features representing the entropic contributions of top-ranked tokens in a single generated sequence, without requiring multiple query re-runs.
- Evaluation across various QA datasets and LLMs shows significant improvement in hallucination detection over using EPR alone, using only a small set of available log-probabilities.
  
<br /><br />Summary: <div>
arXiv:2509.04492v1 Announce Type: cross 
Abstract: Hallucinations in Large Language Model (LLM) outputs for Question Answering (QA) tasks critically undermine their real-world reliability. This paper introduces an applied methodology for robust, one-shot hallucination detection, specifically designed for scenarios with limited data access, such as interacting with black-box LLM APIs that typically expose only a few top candidate log-probabilities per token. Our approach derives uncertainty indicators directly from these readily available log-probabilities generated during non-greedy decoding. We first derive an Entropy Production Rate (EPR) metric that offers baseline performance, later augmented with supervised learning. Our learned model uses features representing the entropic contributions of the accessible top-ranked tokens within a single generated sequence, requiring no multiple query re-runs. Evaluated across diverse QA datasets and multiple LLMs, this estimator significantly improves hallucination detection over using EPR alone. Crucially, high performance is demonstrated using only the typically small set of available log-probabilities (e.g., top <10 per token), confirming its practical efficiency and suitability for these API-constrained deployments. This work provides a readily deployable technique to enhance the trustworthiness of LLM responses from a single generation pass in QA and Retrieval-Augmented Generation (RAG) systems, with its utility further demonstrated in a finance framework analyzing responses to queries on annual reports from an industrial dataset.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Narrative-Driven Computational Framework for Clinician Burnout Surveillance</title>
<link>https://arxiv.org/abs/2509.04497</link>
<guid>https://arxiv.org/abs/2509.04497</guid>
<content:encoded><![CDATA[
<div> Keywords: clinician burnout, intensive care units, clinical notes, sentiment analysis, workload proxies

Summary: 
Analyzing 10,000 ICU discharge summaries from the MIMIC-IV database, this study uses a hybrid pipeline to extract valuable information from clinical notes for proactive clinician burnout monitoring. By combining BioBERT sentiment embeddings, a stress lexicon for burnout surveillance, and LDA with workload proxies, the model achieves a precision of 0.80 and a recall of 0.89 in predicting burnout risk. Specialty-specific analysis reveals elevated burnout risk in Radiology, Psychiatry, and Neurology. The study highlights the potential of leveraging clinical narratives to identify burnout signals and improve well-being monitoring in high-acuity healthcare settings.<br /><br />Summary: <div>
arXiv:2509.04497v1 Announce Type: cross 
Abstract: Clinician burnout poses a substantial threat to patient safety, particularly in high-acuity intensive care units (ICUs). Existing research predominantly relies on retrospective survey tools or broad electronic health record (EHR) metadata, often overlooking the valuable narrative information embedded in clinical notes. In this study, we analyze 10,000 ICU discharge summaries from MIMIC-IV, a publicly available database derived from the electronic health records of Beth Israel Deaconess Medical Center. The dataset encompasses diverse patient data, including vital signs, medical orders, diagnoses, procedures, treatments, and deidentified free-text clinical notes. We introduce a hybrid pipeline that combines BioBERT sentiment embeddings fine-tuned for clinical narratives, a lexical stress lexicon tailored for clinician burnout surveillance, and five-topic latent Dirichlet allocation (LDA) with workload proxies. A provider-level logistic regression classifier achieves a precision of 0.80, a recall of 0.89, and an F1 score of 0.84 on a stratified hold-out set, surpassing metadata-only baselines by greater than or equal to 0.17 F1 score. Specialty-specific analysis indicates elevated burnout risk among providers in Radiology, Psychiatry, and Neurology. Our findings demonstrate that ICU clinical narratives contain actionable signals for proactive well-being monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations</title>
<link>https://arxiv.org/abs/2509.04498</link>
<guid>https://arxiv.org/abs/2509.04498</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, biases, university recommendations, diversity, equitable access <br />
<br />
Summary: 
The study investigates biases in university recommendations generated by three Large Language Models (LLMs). It focuses on geographic, demographic, and economic disparities in the suggestions provided by these models. The analysis, conducted using simulated user profiles, reveals a tendency towards favoring institutions in the Global North and perpetuating gender stereotypes. Despite LLaMA-3.1 demonstrating higher diversity in recommendations, systemic disparities persist across the models. The research introduces a novel evaluation framework that looks beyond accuracy to assess demographic and geographic representation. The findings underscore the pressing need to address biases in educational LLMs to ensure fair and inclusive global access to higher education. <br /> <div>
arXiv:2509.04498v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used as daily recommendation systems for tasks like education planning, yet their recommendations risk perpetuating societal biases. This paper empirically examines geographic, demographic, and economic biases in university and program suggestions from three open-source LLMs: LLaMA-3.1-8B, Gemma-7B, and Mistral-7B. Using 360 simulated user profiles varying by gender, nationality, and economic status, we analyze over 25,000 recommendations. Results show strong biases: institutions in the Global North are disproportionately favored, recommendations often reinforce gender stereotypes, and institutional repetition is prevalent. While LLaMA-3.1 achieves the highest diversity, recommending 481 unique universities across 58 countries, systemic disparities persist. To quantify these issues, we propose a novel, multi-dimensional evaluation framework that goes beyond accuracy by measuring demographic and geographic representation. Our findings highlight the urgent need for bias consideration in educational LMs to ensure equitable global access to higher education.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability Across Citations and Evidence</title>
<link>https://arxiv.org/abs/2509.04499</link>
<guid>https://arxiv.org/abs/2509.04499</guid>
<content:encoded><![CDATA[
<div> audit framework, generative search engines, deep research LLM agents, citation practices, evidence attribution 

Summary:
DeepTRACE introduces a novel audit framework, focusing on eight dimensions related to answer text, sources, and citations to address overconfidence and weak sourcing in generative search engines and deep research LLM agents. The framework utilizes statement-level analysis, citation matrices, and factual-support matrices to evaluate how systems reason and attribute evidence. The study evaluates popular models and LLM configurations, revealing that while deep-research configurations reduce overconfidence and improve citation thoroughness, they still tend to provide one-sided responses on debate queries and include unsupported statements. Citation accuracy ranges from 40-80% across systems, indicating room for improvement in sourcing and evidence attribution practices. <br /><br />Summary: <div>
arXiv:2509.04499v1 Announce Type: cross 
Abstract: Generative search engines and deep research LLM agents promise trustworthy, source-grounded synthesis, yet users regularly encounter overconfidence, weak sourcing, and confusing citation practices. We introduce DeepTRACE, a novel sociotechnically grounded audit framework that turns prior community-identified failure cases into eight measurable dimensions spanning answer text, sources, and citations. DeepTRACE uses statement-level analysis (decomposition, confidence scoring) and builds citation and factual-support matrices to audit how systems reason with and attribute evidence end-to-end. Using automated extraction pipelines for popular public models (e.g., GPT-4.5/5, You.com, Perplexity, Copilot/Bing, Gemini) and an LLM-judge with validated agreement to human raters, we evaluate both web-search engines and deep-research configurations. Our findings show that generative search engines and deep research agents frequently produce one-sided, highly confident responses on debate queries and include large fractions of statements unsupported by their own listed sources. Deep-research configurations reduce overconfidence and can attain high citation thoroughness, but they remain highly one-sided on debate queries and still exhibit large fractions of unsupported statements, with citation accuracy ranging from 40--80% across systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts</title>
<link>https://arxiv.org/abs/2509.04500</link>
<guid>https://arxiv.org/abs/2509.04500</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Poisoned Context Testbed, Rescorla-Wagner model, inappropriate content, context engineering <br />
Summary: 
Large Language Models (LLMs) benefit from external context but can struggle when faced with mixed information. In the Poisoned Context Testbed, queries are paired with real-world contexts containing relevant and inappropriate content to study how LLMs prioritize signals. The adapted Rescorla-Wagner model shows that LLMs tend to incorporate less prevalent information, leading to a susceptibility to inappropriate content. This vulnerability can significantly impact response quality in real-world settings. To address this, RW-Steering is introduced as a two-stage finetuning approach that allows models to identify and ignore inappropriate signals internally. Unlike previous methods, RW-Steering generalizes well across different mixtures of inappropriate content, improving response quality by 39.8% and mitigating the undesirable behavior curve. RW-Steering demonstrates robust and generalizable context engineering for enhancing LLM safety in practical applications. <br /> <div>
arXiv:2509.04500v1 Announce Type: cross 
Abstract: Incorporating external context can significantly enhance the response quality of Large Language Models (LLMs). However, real-world contexts often mix relevant information with disproportionate inappropriate content, posing reliability risks. How do LLMs process and prioritize mixed context? To study this, we introduce the Poisoned Context Testbed, pairing queries with real-world contexts containing relevant and inappropriate content. Inspired by associative learning in animals, we adapt the Rescorla-Wagner (RW) model from neuroscience to quantify how competing contextual signals influence LLM outputs. Our adapted model reveals a consistent behavioral pattern: LLMs exhibit a strong tendency to incorporate information that is less prevalent in the context. This susceptibility is harmful in real-world settings, where small amounts of inappropriate content can substantially degrade response quality. Empirical evaluations on our testbed further confirm this vulnerability. To tackle this, we introduce RW-Steering, a two-stage finetuning-based approach that enables the model to internally identify and ignore inappropriate signals. Unlike prior methods that rely on extensive supervision across diverse context mixtures, RW-Steering generalizes robustly across varying proportions of inappropriate content. Experiments show that our best fine-tuned model improves response quality by 39.8% and reverses the undesirable behavior curve, establishing RW-Steering as a robust, generalizable context engineering solution for improving LLM safety in real-world use.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Reinforcement Learning for Model Training, and future directions with GRAPE</title>
<link>https://arxiv.org/abs/2509.04501</link>
<guid>https://arxiv.org/abs/2509.04501</guid>
<content:encoded><![CDATA[
arXiv:2509.04501v1 Announce Type: cross 
Abstract: This paper provides a self-contained, from-scratch, exposition of key algorithms for instruction tuning of models: SFT, Rejection Sampling, REINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), and Direct Preference Optimization (DPO). Explanations of these algorithms often assume prior knowledge, lack critical details, and/or are overly generalized and complex. Here, each method is discussed and developed step by step using simplified and explicit notation focused on LLMs, aiming to eliminate ambiguity and provide a clear and intuitive understanding of the concepts. By minimizing detours into the broader RL literature and connecting concepts to LLMs, we eliminate superfluous abstractions and reduce cognitive overhead. Following this exposition, we provide a literature review of new techniques and approaches beyond those detailed. Finally, new ideas for research and exploration in the form of GRAPE (Generalized Relative Advantage Policy Evolution) are presented.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples</title>
<link>https://arxiv.org/abs/2509.04502</link>
<guid>https://arxiv.org/abs/2509.04502</guid>
<content:encoded><![CDATA[
arXiv:2509.04502v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation enhances the response accuracy of Large Language Models (LLMs) by integrating retrieval and generation modules with external knowledge, demonstrating particular strength in real-time queries and Visual Question Answering tasks. However, the effectiveness of RAG is frequently hindered by the precision of the retriever: many retrieved samples fed into the generation phase are irrelevant or misleading, posing a critical bottleneck to LLMs' performance. To address this challenge, we introduce VaccineRAG, a novel Chain-of-Thought-based retrieval-augmented generation dataset. On one hand, VaccineRAG employs a benchmark to evaluate models using data with varying positive/negative sample ratios, systematically exposing inherent weaknesses in current LLMs. On the other hand, it enhances models' sample-discrimination capabilities by prompting LLMs to generate explicit Chain-of-Thought (CoT) analysis for each sample before producing final answers. Furthermore, to enhance the model's ability to learn long-sequence complex CoT content, we propose Partial-GRPO. By modeling the outputs of LLMs as multiple components rather than a single whole, our model can make more informed preference selections for complex sequences, thereby enhancing its capacity to learn complex CoT. Comprehensive evaluations and ablation studies on VaccineRAG validate the effectiveness of the proposed scheme. The code and dataset will be publicly released soon.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioral Fingerprinting of Large Language Models</title>
<link>https://arxiv.org/abs/2509.04504</link>
<guid>https://arxiv.org/abs/2509.04504</guid>
<content:encoded><![CDATA[
arXiv:2509.04504v1 Announce Type: cross 
Abstract: Current benchmarks for Large Language Models (LLMs) primarily focus on performance metrics, often failing to capture the nuanced behavioral characteristics that differentiate them. This paper introduces a novel ``Behavioral Fingerprinting'' framework designed to move beyond traditional evaluation by creating a multi-faceted profile of a model's intrinsic cognitive and interactive styles. Using a curated \textit{Diagnostic Prompt Suite} and an innovative, automated evaluation pipeline where a powerful LLM acts as an impartial judge, we analyze eighteen models across capability tiers. Our results reveal a critical divergence in the LLM landscape: while core capabilities like abstract and causal reasoning are converging among top models, alignment-related behaviors such as sycophancy and semantic robustness vary dramatically. We further document a cross-model default persona clustering (ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together, this suggests that a model's interactive nature is not an emergent property of its scale or reasoning power, but a direct consequence of specific, and highly variable, developer alignment strategies. Our framework provides a reproducible and scalable methodology for uncovering these deep behavioral differences. Project: https://github.com/JarvisPei/Behavioral-Fingerprinting
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memristor-Based Neural Network Accelerators for Space Applications: Enhancing Performance with Temporal Averaging and SIRENs</title>
<link>https://arxiv.org/abs/2509.04506</link>
<guid>https://arxiv.org/abs/2509.04506</guid>
<content:encoded><![CDATA[
arXiv:2509.04506v1 Announce Type: cross 
Abstract: Memristors are an emerging technology that enables artificial intelligence (AI) accelerators with high energy efficiency and radiation robustness -- properties that are vital for the deployment of AI on-board spacecraft. However, space applications require reliable and precise computations, while memristive devices suffer from non-idealities, such as device variability, conductance drifts, and device faults. Thus, porting neural networks (NNs) to memristive devices often faces the challenge of severe performance degradation. In this work, we show in simulations that memristor-based NNs achieve competitive performance levels on on-board tasks, such as navigation \& control and geodesy of asteroids. Through bit-slicing, temporal averaging of NN layers, and periodic activation functions, we improve initial results from around $0.07$ to $0.01$ and $0.3$ to $0.007$ for both tasks using RRAM devices, coming close to state-of-the-art levels ($0.003-0.005$ and $0.003$, respectively). Our results demonstrate the potential of memristors for on-board space applications, and we are convinced that future technology and NN improvements will further close the performance gap to fully unlock the benefits of memristors.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM Approach</title>
<link>https://arxiv.org/abs/2509.04507</link>
<guid>https://arxiv.org/abs/2509.04507</guid>
<content:encoded><![CDATA[
arXiv:2509.04507v1 Announce Type: cross 
Abstract: Silent Speech Interfaces (SSIs) have gained attention for their ability to generate intelligible speech from non-acoustic signals. While significant progress has been made in advancing speech generation pipelines, limited work has addressed the recognition and downstream processing of synthesized speech, which often suffers from phonetic ambiguity and noise. To overcome these challenges, we propose an enhanced automatic speech recognition framework that combines a transformer-based acoustic model with a large language model (LLM) for post-processing. The transformer captures full utterance context, while the LLM ensures linguistic consistency. Experimental results show a 16% relative and 6% absolute reduction in word error rate (WER) over a 36% baseline, demonstrating substantial improvements in intelligibility for silent speech interfaces.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through Model Explanations</title>
<link>https://arxiv.org/abs/2509.04515</link>
<guid>https://arxiv.org/abs/2509.04515</guid>
<content:encoded><![CDATA[
arXiv:2509.04515v1 Announce Type: cross 
Abstract: Language models have been shown to propagate social bias through their output, particularly in the representation of gender and ethnicity. This paper investigates gender and ethnicity biases in AI-generated occupational stories. Representation biases are measured before and after applying our proposed mitigation strategy, Bias Analysis and Mitigation through Explanation (BAME), revealing improvements in demographic representation ranging from 2% to 20%. BAME leverages model-generated explanations to inform targeted prompt engineering, effectively reducing biases without modifying model parameters. By analyzing stories generated across 25 occupational groups, three large language models (Claude 3.5 Sonnet, Llama 3.1 70B Instruct, and GPT-4 Turbo), and multiple demographic dimensions, we identify persistent patterns of overrepresentation and underrepresentation linked to training data stereotypes. Our findings demonstrate that guiding models with their own internal reasoning mechanisms can significantly enhance demographic parity, thereby contributing to the development of more transparent generative AI systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation</title>
<link>https://arxiv.org/abs/2509.04534</link>
<guid>https://arxiv.org/abs/2509.04534</guid>
<content:encoded><![CDATA[
arXiv:2509.04534v1 Announce Type: cross 
Abstract: Large language models have demonstrated remarkable capabilities in biomedical natural language processing, yet their rapid growth in size and computational requirements present a major barrier to adoption in healthcare settings where data privacy precludes cloud deployment and resources are limited. In this study, we systematically evaluated the impact of quantization on 12 state-of-the-art large language models, including both general-purpose and biomedical-specific models, across eight benchmark datasets covering four key tasks: named entity recognition, relation extraction, multi-label classification, and question answering. We show that quantization substantially reduces GPU memory requirements-by up to 75%-while preserving model performance across diverse tasks, enabling the deployment of 70B-parameter models on 40GB consumer-grade GPUs. In addition, domain-specific knowledge and responsiveness to advanced prompting methods are largely maintained. These findings provide significant practical and guiding value, highlighting quantization as a practical and effective strategy for enabling the secure, local deployment of large yet high-capacity language models in biomedical contexts, bridging the gap between technical advances in AI and real-world clinical translation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Policy Adaptation via Cross-Domain Skill Diffusion</title>
<link>https://arxiv.org/abs/2509.04535</link>
<guid>https://arxiv.org/abs/2509.04535</guid>
<content:encoded><![CDATA[
arXiv:2509.04535v1 Announce Type: cross 
Abstract: In this work, we present an in-context policy adaptation (ICPAD) framework designed for long-horizon multi-task environments, exploring diffusion-based skill learning techniques in cross-domain settings. The framework enables rapid adaptation of skill-based reinforcement learning policies to diverse target domains, especially under stringent constraints on no model updates and only limited target domain data. Specifically, the framework employs a cross-domain skill diffusion scheme, where domain-agnostic prototype skills and a domain-grounded skill adapter are learned jointly and effectively from an offline dataset through cross-domain consistent diffusion processes. The prototype skills act as primitives for common behavior representations of long-horizon policies, serving as a lingua franca to bridge different domains. Furthermore, to enhance the in-context adaptation performance, we develop a dynamic domain prompting scheme that guides the diffusion-based skill adapter toward better alignment with the target domain. Through experiments with robotic manipulation in Metaworld and autonomous driving in CARLA, we show that our $\oursol$ framework achieves superior policy adaptation performance under limited target domain data conditions for various cross-domain configurations including differences in environment dynamics, agent embodiment, and task horizon.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem</title>
<link>https://arxiv.org/abs/2509.04537</link>
<guid>https://arxiv.org/abs/2509.04537</guid>
<content:encoded><![CDATA[
arXiv:2509.04537v1 Announce Type: cross 
Abstract: We investigate the emergent social dynamics of Large Language Model (LLM) agents in a spatially extended El Farol Bar problem, observing how they autonomously navigate this classic social dilemma. As a result, the LLM agents generated a spontaneous motivation to go to the bar and changed their decision making by becoming a collective. We also observed that the LLM agents did not solve the problem completely, but rather behaved more like humans. These findings reveal a complex interplay between external incentives (prompt-specified constraints such as the 60\% threshold) and internal incentives (culturally-encoded social preferences derived from pre-training), demonstrating that LLM agents naturally balance formal game-theoretic rationality with social motivations that characterize human behavior. These findings suggest that a new model of group decision making, which could not be handled in the previous game-theoretic problem setting, can be realized by LLM agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>i-Mask: An Intelligent Mask for Breath-Driven Activity Recognition</title>
<link>https://arxiv.org/abs/2509.04544</link>
<guid>https://arxiv.org/abs/2509.04544</guid>
<content:encoded><![CDATA[
arXiv:2509.04544v1 Announce Type: cross 
Abstract: The patterns of inhalation and exhalation contain important physiological signals that can be used to anticipate human behavior, health trends, and vital parameters. Human activity recognition (HAR) is fundamentally connected to these vital signs, providing deeper insights into well-being and enabling real-time health monitoring. This work presents i-Mask, a novel HAR approach that leverages exhaled breath patterns captured using a custom-developed mask equipped with integrated sensors. Data collected from volunteers wearing the mask undergoes noise filtering, time-series decomposition, and labeling to train predictive models. Our experimental results validate the effectiveness of the approach, achieving over 95\% accuracy and highlighting its potential in healthcare and fitness applications.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions</title>
<link>https://arxiv.org/abs/2509.04549</link>
<guid>https://arxiv.org/abs/2509.04549</guid>
<content:encoded><![CDATA[
arXiv:2509.04549v1 Announce Type: cross 
Abstract: Transformer-based language models excel in NLP tasks, but fine-grained control remains challenging. This paper explores methods for manipulating transformer models through principled interventions at three levels: prompts, activations, and weights. We formalize controllable text generation as an optimization problem addressable via prompt engineering, parameter-efficient fine-tuning, model editing, and reinforcement learning. We introduce a unified framework encompassing prompt-level steering, activation interventions, and weight-space edits. We analyze robustness and safety implications, including adversarial attacks and alignment mitigations. Theoretically, we show minimal weight updates can achieve targeted behavior changes with limited side-effects. Empirically, we demonstrate >90% success in sentiment control and factual edits while preserving base performance, though generalization-specificity trade-offs exist. We discuss ethical dual-use risks and the need for rigorous evaluation. This work lays groundwork for designing controllable and robust language models.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Faithfulness-guided Ensemble Interpretation of Neural Network</title>
<link>https://arxiv.org/abs/2509.04588</link>
<guid>https://arxiv.org/abs/2509.04588</guid>
<content:encoded><![CDATA[
arXiv:2509.04588v1 Announce Type: cross 
Abstract: Interpretable and faithful explanations for specific neural inferences are crucial for understanding and evaluating model behavior. Our work introduces \textbf{F}aithfulness-guided \textbf{E}nsemble \textbf{I}nterpretation (\textbf{FEI}), an innovative framework that enhances the breadth and effectiveness of faithfulness, advancing interpretability by providing superior visualization. Through an analysis of existing evaluation benchmarks, \textbf{FEI} employs a smooth approximation to elevate quantitative faithfulness scores. Diverse variations of \textbf{FEI} target enhanced faithfulness in hidden layer encodings, expanding interpretability. Additionally, we propose a novel qualitative metric that assesses hidden layer faithfulness. In extensive experiments, \textbf{FEI} surpasses existing methods, demonstrating substantial advances in qualitative visualization and quantitative faithfulness scores. Our research establishes a comprehensive framework for elevating faithfulness in neural network explanations, emphasizing both breadth and precision
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Multi-Task Learning with Learnable Weighting for Pharmacokinetic and Toxicity Prediction</title>
<link>https://arxiv.org/abs/2509.04601</link>
<guid>https://arxiv.org/abs/2509.04601</guid>
<content:encoded><![CDATA[
arXiv:2509.04601v1 Announce Type: cross 
Abstract: Prediction for ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) plays a crucial role in drug discovery and development, accelerating the screening and optimization of new drugs. Existing methods primarily rely on single-task learning (STL), which often fails to fully exploit the complementarities between tasks. Besides, it requires more computational resources while training and inference of each task independently. To address these issues, we propose a new unified Quantum-enhanced and task-Weighted Multi-Task Learning (QW-MTL) framework, specifically designed for ADMET classification tasks. Built upon the Chemprop-RDKit backbone, QW-MTL adopts quantum chemical descriptors to enrich molecular representations with additional information about the electronic structure and interactions. Meanwhile, it introduces a novel exponential task weighting scheme that combines dataset-scale priors with learnable parameters to achieve dynamic loss balancing across tasks. To the best of our knowledge, this is the first work to systematically conduct joint multi-task training across all 13 Therapeutics Data Commons (TDC) classification benchmarks, using leaderboard-style data splits to ensure a standardized and realistic evaluation setting. Extensive experimental results show that QW-MTL significantly outperforms single-task baselines on 12 out of 13 tasks, achieving high predictive performance with minimal model complexity and fast inference, demonstrating the effectiveness and efficiency of multi-task molecular learning enhanced by quantum-informed features and adaptive task weighting.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-efficient Integration of New Modalities into Large Language Models</title>
<link>https://arxiv.org/abs/2509.04606</link>
<guid>https://arxiv.org/abs/2509.04606</guid>
<content:encoded><![CDATA[
arXiv:2509.04606v1 Announce Type: cross 
Abstract: Multimodal foundation models can process several modalities. However, since the space of possible modalities is large and evolving over time, training a model from scratch to encompass all modalities is unfeasible. Moreover, integrating a modality into a pre-existing foundation model currently requires a significant amount of paired data, which is often not available for low-resource modalities. In this paper, we introduce a method for sample-efficient modality integration (SEMI) into Large Language Models (LLMs). To this end, we devise a hypernetwork that can adapt a shared projector -- placed between modality-specific encoders and an LLM -- to any modality. The hypernetwork, trained on high-resource modalities (i.e., text, speech, audio, video), is conditioned on a few samples from any arbitrary modality at inference time to generate a suitable adapter. To increase the diversity of training modalities, we artificially multiply the number of encoders through isometric transformations. We find that SEMI achieves a significant boost in sample efficiency during few-shot integration of new modalities (i.e., satellite images, astronomical images, inertial measurements, and molecules) with encoders of arbitrary embedding dimensionality. For instance, to reach the same accuracy as 32-shot SEMI, training the projector from scratch needs 64$\times$ more data. As a result, SEMI holds promise to extend the modality coverage of foundation models.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families</title>
<link>https://arxiv.org/abs/2509.04622</link>
<guid>https://arxiv.org/abs/2509.04622</guid>
<content:encoded><![CDATA[
arXiv:2509.04622v1 Announce Type: cross 
Abstract: Representational similarity metrics are fundamental tools in neuroscience and AI, yet we lack systematic comparisons of their discriminative power across model families. We introduce a quantitative framework to evaluate representational similarity measures based on their ability to separate model families-across architectures (CNNs, Vision Transformers, Swin Transformers, ConvNeXt) and training regimes (supervised vs. self-supervised). Using three complementary separability measures-dprime from signal detection theory, silhouette coefficients and ROC-AUC, we systematically assess the discriminative capacity of commonly used metrics including RSA, linear predictivity, Procrustes, and soft matching. We show that separability systematically increases as metrics impose more stringent alignment constraints. Among mapping-based approaches, soft-matching achieves the highest separability, followed by Procrustes alignment and linear predictivity. Non-fitting methods such as RSA also yield strong separability across families. These results provide the first systematic comparison of similarity metrics through a separability lens, clarifying their relative sensitivity and guiding metric choice for large-scale model and brain comparisons.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control</title>
<link>https://arxiv.org/abs/2509.04628</link>
<guid>https://arxiv.org/abs/2509.04628</guid>
<content:encoded><![CDATA[
arXiv:2509.04628v1 Announce Type: cross 
Abstract: We present an imitation learning approach for spacecraft guidance, navigation, and control(GNC) that achieves high performance from limited data. Using only 100 expert demonstrations, equivalent to 6,300 environment interactions, our method, which implements Action Chunking with Transformers (ACT), learns a control policy that maps visual and state observations to thrust and torque commands. ACT generates smoother, more consistent trajectories than a meta-reinforcement learning (meta-RL) baseline trained with 40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking with the International Space Station (ISS). We show that our approach achieves greater accuracy, smoother control, and greater sample efficiency.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schema Inference for Tabular Data Repositories Using Large Language Models</title>
<link>https://arxiv.org/abs/2509.04632</link>
<guid>https://arxiv.org/abs/2509.04632</guid>
<content:encoded><![CDATA[
arXiv:2509.04632v1 Announce Type: cross 
Abstract: Minimally curated tabular data often contain representational inconsistencies across heterogeneous sources, and are accompanied by sparse metadata. Working with such data is intimidating. While prior work has advanced dataset discovery and exploration, schema inference remains difficult when metadata are limited. We present SI-LLM (Schema Inference using Large Language Models), which infers a concise conceptual schema for tabular data using only column headers and cell values. The inferred schema comprises hierarchical entity types, attributes, and inter-type relationships. In extensive evaluation on two datasets from web tables and open data, SI-LLM achieves promising end-to-end results, as well as better or comparable results to state-of-the-art methods at each step. All source code, full prompts, and datasets of SI-LLM are available at https://github.com/PierreWoL/SILLM.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Environments for Organoid Intelligence with LLM-Automated Design and Plasticity-Based Evaluation</title>
<link>https://arxiv.org/abs/2509.04633</link>
<guid>https://arxiv.org/abs/2509.04633</guid>
<content:encoded><![CDATA[
arXiv:2509.04633v1 Announce Type: cross 
Abstract: As the complexity of artificial agents increases, the design of environments that can effectively shape their behavior and capabilities has become a critical research frontier. We propose a framework that extends this principle to a novel class of agents: biological neural networks in the form of neural organoids. This paper introduces three scalable, closed-loop virtual environments designed to train organoid-based biological agents and probe the underlying mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments with increasing complexity: (1) a conditional avoidance task, (2) a one-dimensional predator-prey scenario, and (3) a replication of the classic Pong game. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation. Furthermore, we propose a novel meta-learning approach where a Large Language Model (LLM) is used to automate the generation and optimization of experimental protocols, scaling the process of environment and curriculum design. Finally, we outline a multi-modal approach for evaluating learning by measuring synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between computational neuroscience and agent-based AI, offering a unique platform for studying embodiment, learning, and intelligence in a controlled biological substrate.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Transformer Models in Disaster Tweet Classification for Public Safety</title>
<link>https://arxiv.org/abs/2509.04650</link>
<guid>https://arxiv.org/abs/2509.04650</guid>
<content:encoded><![CDATA[
arXiv:2509.04650v1 Announce Type: cross 
Abstract: Twitter and other social media platforms have become vital sources of real time information during disasters and public safety emergencies. Automatically classifying disaster related tweets can help emergency services respond faster and more effectively. Traditional Machine Learning (ML) models such as Logistic Regression, Naive Bayes, and Support Vector Machines have been widely used for this task, but they often fail to understand the context or deeper meaning of words, especially when the language is informal, metaphorical, or ambiguous. We posit that, in this context, transformer based models can perform better than traditional ML models. In this paper, we evaluate the effectiveness of transformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for classifying disaster related tweets. These models are compared with traditional ML approaches to highlight the performance gap. Experimental results show that BERT achieved the highest accuracy (91%), significantly outperforming traditional models like Logistic Regression and Naive Bayes (both at 82%). The use of contextual embeddings and attention mechanisms allows transformer models to better understand subtle language in tweets, where traditional ML models fall short. This research demonstrates that transformer architectures are far more suitable for public safety applications, offering improved accuracy, deeper language understanding, and better generalization across real world social media text.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Transformer Architectures as Implicit Multinomial Regression</title>
<link>https://arxiv.org/abs/2509.04653</link>
<guid>https://arxiv.org/abs/2509.04653</guid>
<content:encoded><![CDATA[
arXiv:2509.04653v1 Announce Type: cross 
Abstract: Mechanistic interpretability aims to understand how internal components of modern machine learning models, such as weights, activations, and layers, give rise to the model's overall behavior. One particularly opaque mechanism is attention: despite its central role in transformer models, its mathematical underpinnings and relationship to concepts like feature polysemanticity, superposition, and model performance remain poorly understood. This paper establishes a novel connection between attention mechanisms and multinomial regression. Specifically, we show that in a fixed multinomial regression setting, optimizing over latent features yields optimal solutions that align with the dynamics induced by attention blocks. In other words, the evolution of representations through a transformer can be interpreted as a trajectory that recovers the optimal features for classification.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs</title>
<link>https://arxiv.org/abs/2509.04655</link>
<guid>https://arxiv.org/abs/2509.04655</guid>
<content:encoded><![CDATA[
arXiv:2509.04655v1 Announce Type: cross 
Abstract: We propose a novel inference-time out-of-domain (OOD) detection algorithm for specialized large language models (LLMs). Despite achieving state-of-the-art performance on in-domain tasks through fine-tuning, specialized LLMs remain vulnerable to incorrect or unreliable outputs when presented with OOD inputs, posing risks in critical applications. Our method leverages the Inductive Conformal Anomaly Detection (ICAD) framework, using a new non-conformity measure based on the model's dropout tolerance. Motivated by recent findings on polysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs exhibit higher dropout tolerance than OOD inputs. We aggregate dropout tolerance across multiple layers via a valid ensemble approach, improving detection while maintaining theoretical false alarm bounds from ICAD. Experiments with medical-specialized LLMs show that our approach detects OOD inputs better than baseline methods, with AUROC improvements of $2\%$ to $37\%$ when treating OOD datapoints as positives and in-domain test datapoints as negatives.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating NL2SQL via SQL2NL</title>
<link>https://arxiv.org/abs/2509.04657</link>
<guid>https://arxiv.org/abs/2509.04657</guid>
<content:encoded><![CDATA[
arXiv:2509.04657v1 Announce Type: cross 
Abstract: Robust evaluation in the presence of linguistic variation is key to understanding the generalization capabilities of Natural Language to SQL (NL2SQL) models, yet existing benchmarks rarely address this factor in a systematic or controlled manner. We propose a novel schema-aligned paraphrasing framework that leverages SQL-to-NL (SQL2NL) to automatically generate semantically equivalent, lexically diverse queries while maintaining alignment with the original schema and intent. This enables the first targeted evaluation of NL2SQL robustness to linguistic variation in isolation-distinct from prior work that primarily investigates ambiguity or schema perturbations. Our analysis reveals that state-of-the-art models are far more brittle than standard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop in execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries, while LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to 42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We also find that robustness degradation varies significantly with query complexity, dataset, and domain -- highlighting the need for evaluation frameworks that explicitly measure linguistic generalization to ensure reliable performance in real-world settings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation</title>
<link>https://arxiv.org/abs/2509.04669</link>
<guid>https://arxiv.org/abs/2509.04669</guid>
<content:encoded><![CDATA[
arXiv:2509.04669v1 Announce Type: cross 
Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs) have challenged the dominance of Convolutional Neural Networks (CNNs) in computer vision. ViTs excel at capturing global context, and SSMs like Mamba offer linear complexity for long sequences, yet they do not capture fine-grained local features as effectively as CNNs. Conversely, CNNs possess strong inductive biases for local features but lack the global reasoning capabilities of transformers and Mamba. To bridge this gap, we introduce \textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a hierarchical structure with convolutional blocks in its early stages to extract rich local features. These convolutional blocks are then processed by later stages incorporating multi-directional Mamba blocks designed to efficiently model long-range dependencies and global context. This hybrid design allows for superior feature representation while maintaining linear complexity with respect to image resolution. We demonstrate VCMamba's effectiveness through extensive experiments on ImageNet-1K classification and ADE20K semantic segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K, surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains 47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing 62% fewer parameters. Code is available at https://github.com/Wertyuui345/VCMamba.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine Bioacoustic Monitoring</title>
<link>https://arxiv.org/abs/2509.04682</link>
<guid>https://arxiv.org/abs/2509.04682</guid>
<content:encoded><![CDATA[
arXiv:2509.04682v1 Announce Type: cross 
Abstract: Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal data for long-term ecological analysis, but intrinsic noise and complex signal dependencies hinder model stability and generalization. Multilayered windowing has improved target sound localization, yet variability from shifting ambient noise, diverse propagation effects, and mixed biological and anthropogenic sources demands robust architectures and rigorous evaluation. We introduce GetNetUPAM, a hierarchical nested cross-validation framework designed to quantify model stability under ecologically realistic variability. Data are partitioned into distinct site-year segments, preserving recording heterogeneity and ensuring each validation fold reflects a unique environmental subset, reducing overfitting to localized noise and sensor artifacts. Site-year blocking enforces evaluation against genuine environmental diversity, while standard cross-validation on random subsets measures generalization across UPAM's full signal distribution, a dimension absent from current benchmarks. Using GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution Pooling and Attention Network (ARPA-N), a neural architecture for irregular spectrogram dimensions. Adaptive pooling with spatial attention extends the receptive field, capturing global context without excessive parameters. Under GetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet baselines and a log2-scale order-of-magnitude drop in variability across all metrics, enabling consistent detection across site-year folds and advancing scalable, accurate bioacoustic monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs</title>
<link>https://arxiv.org/abs/2509.04696</link>
<guid>https://arxiv.org/abs/2509.04696</guid>
<content:encoded><![CDATA[
arXiv:2509.04696v1 Announce Type: cross 
Abstract: Knowledge graphs (KGs) are foundational to many AI applications, but maintaining their freshness and completeness remains costly. We present ODKE+, a production-grade system that automatically extracts and ingests millions of open-domain facts from web sources with high precision. ODKE+ combines modular components into a scalable pipeline: (1) the Extraction Initiator detects missing or stale facts, (2) the Evidence Retriever collects supporting documents, (3) hybrid Knowledge Extractors apply both pattern-based rules and ontology-guided prompting for large language models (LLMs), (4) a lightweight Grounder validates extracted facts using a second LLM, and (5) the Corroborator ranks and normalizes candidate facts for ingestion. ODKE+ dynamically generates ontology snippets tailored to each entity type to align extractions with schema constraints, enabling scalable, type-consistent fact extraction across 195 predicates. The system supports batch and streaming modes, processing over 9 million Wikipedia pages and ingesting 19 million high-confidence facts with 98.8% precision. ODKE+ significantly improves coverage over traditional methods, achieving up to 48% overlap with third-party KGs and reducing update lag by 50 days on average. Our deployment demonstrates that LLM-based extraction, grounded in ontological structure and verification workflows, can deliver trustworthiness, production-scale knowledge ingestion with broad real-world applicability. A recording of the system demonstration is included with the submission and is also available at https://youtu.be/UcnE3_GsTWs.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.04712</link>
<guid>https://arxiv.org/abs/2509.04712</guid>
<content:encoded><![CDATA[
arXiv:2509.04712v1 Announce Type: cross 
Abstract: Automated vehicle control using reinforcement learning (RL) has attracted significant attention due to its potential to learn driving policies through environment interaction. However, RL agents often face training challenges in sample efficiency and effective exploration, making it difficult to discover an optimal driving strategy. To address these issues, we propose guiding the RL driving agent with a demonstration policy that need not be a highly optimized or expert-level controller. Specifically, we integrate a rule-based lane change controller with the Soft Actor Critic (SAC) algorithm to enhance exploration and learning efficiency. Our approach demonstrates improved driving performance and can be extended to other driving scenarios that can similarly benefit from demonstration-based guidance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced Question Answering</title>
<link>https://arxiv.org/abs/2509.04716</link>
<guid>https://arxiv.org/abs/2509.04716</guid>
<content:encoded><![CDATA[
arXiv:2509.04716v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucination in Large Language Models (LLMs) by incorporating external data, with Knowledge Graphs (KGs) offering crucial information for question answering. Traditional Knowledge Graph Question Answering (KGQA) methods rely on semantic parsing, which typically retrieves knowledge strictly necessary for answer generation, thus often suffer from low coverage due to rigid schema requirements and semantic ambiguity. We present KERAG, a novel KG-based RAG pipeline that enhances QA coverage by retrieving a broader subgraph likely to contain relevant information. Our retrieval-filtering-summarization approach, combined with fine-tuned LLMs for Chain-of-Thought reasoning on knowledge sub-graphs, reduces noises and improves QA for both simple and complex questions. Experiments demonstrate that KERAG surpasses state-of-the-art solutions by about 7% in quality and exceeds GPT-4o (Tool) by 10-21%.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoVeR: Conformal Calibration for Versatile and Reliable Autoregressive Next-Token Prediction</title>
<link>https://arxiv.org/abs/2509.04733</link>
<guid>https://arxiv.org/abs/2509.04733</guid>
<content:encoded><![CDATA[
arXiv:2509.04733v1 Announce Type: cross 
Abstract: Autoregressive pre-trained models combined with decoding methods have achieved impressive performance on complex reasoning tasks. While mainstream decoding strategies such as beam search can generate plausible candidate sets, they often lack provable coverage guarantees, and struggle to effectively balance search efficiency with the need for versatile trajectories, particularly those involving long-tail sequences that are essential in certain real-world applications. To address these limitations, we propose \textsc{CoVeR}, a novel model-free decoding strategy wihtin the conformal prediction framework that simultaneously maintains a compact search space and ensures high coverage probability over desirable trajectories. Theoretically, we establish a PAC-style generalization bound, guaranteeing that \textsc{CoVeR} asymptotically achieves a coverage rate of at least $1 - \alpha$ for any target level $\alpha \in (0,1)$.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning</title>
<link>https://arxiv.org/abs/2509.04734</link>
<guid>https://arxiv.org/abs/2509.04734</guid>
<content:encoded><![CDATA[
arXiv:2509.04734v1 Announce Type: cross 
Abstract: The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We present Beyond I-Con, a framework that enables systematic discovery of novel loss functions by exploring alternative statistical divergences and similarity kernels. Key findings: (1) on unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art results by modifying the PMI algorithm to use total variation (TV) distance; (2) on supervised contrastive learning, we outperform the standard approach by using TV and a distance-based similarity kernel instead of KL and an angular kernel; (3) on dimensionality reduction, we achieve superior qualitative results and better performance on downstream tasks than SNE by replacing KL with a bounded f-divergence. Our results highlight the importance of considering divergence and similarity kernel choices in representation learning optimization.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization</title>
<link>https://arxiv.org/abs/2509.04735</link>
<guid>https://arxiv.org/abs/2509.04735</guid>
<content:encoded><![CDATA[
arXiv:2509.04735v1 Announce Type: cross 
Abstract: Recent advances in vision foundation models, such as the Segment Anything Model (SAM) and its successor SAM2, have achieved state-of-the-art performance on general image segmentation benchmarks. However, these models struggle in adverse weather conditions where visual ambiguity is high, largely due to their lack of uncertainty quantification. Inspired by progress in medical imaging, where uncertainty-aware training has improved reliability in ambiguous cases, we investigate two approaches to enhance segmentation robustness for autonomous driving. First, we introduce a multi-step finetuning procedure for SAM2 that incorporates uncertainty metrics directly into the loss function, improving overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter (UAT), originally designed for medical image segmentation, to driving contexts. We evaluate both methods on CamVid, BDD100K, and GTA driving datasets. Experiments show that UAT-SAM outperforms standard SAM in extreme weather, while SAM2 with uncertainty-aware loss achieves improved performance across diverse driving scenes. These findings underscore the value of explicit uncertainty modeling for safety-critical autonomous driving in challenging environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching</title>
<link>https://arxiv.org/abs/2509.04752</link>
<guid>https://arxiv.org/abs/2509.04752</guid>
<content:encoded><![CDATA[
arXiv:2509.04752v1 Announce Type: cross 
Abstract: This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM health coaching system that integrates personalized machine learning and retrieval-augmented generation to deliver adaptive, evidence-based guidance. SePA combines: (1) Individualized models predicting daily stress, soreness, and injury risk from wearable sensor data (28 users, 1260 data points); and (2) A retrieval module that grounds LLM-generated feedback in expert-vetted web content to ensure contextual relevance and reliability. Our predictive models, evaluated with rolling-origin cross-validation and group k-fold cross-validation show that personalized models outperform generalized baselines. In a pilot expert study (n=4), SePA's retrieval-based advice was preferred over a non-retrieval baseline, yielding meaningful practical effect (Cliff's $\delta$=0.3, p=0.05). We also quantify latency performance trade-offs between response quality and speed, offering a transparent blueprint for next-generation, trustworthy personal health informatics systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning</title>
<link>https://arxiv.org/abs/2509.04753</link>
<guid>https://arxiv.org/abs/2509.04753</guid>
<content:encoded><![CDATA[
arXiv:2509.04753v1 Announce Type: cross 
Abstract: Natural language processing (NLP) is a key technology to extract important patient information from clinical narratives to support healthcare applications. The rapid development of large language models (LLMs) has revolutionized many NLP tasks in the clinical domain, yet their optimal use in patient information extraction tasks requires further exploration. This study examines LLMs' effectiveness in patient information extraction, focusing on LLM architectures, fine-tuning strategies, and multi-task instruction tuning techniques for developing robust and generalizable patient information extraction systems. This study aims to explore key concepts of using LLMs for clinical concept and relation extraction tasks, including: (1) encoder-only or decoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT) algorithms, and (3) multi-task instruction tuning on few-shot learning performance. We benchmarked a suite of LLMs, including encoder-based LLMs (BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1, GatorTronLlama), across five datasets. We compared traditional full-size fine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning framework that combines both tasks across four datasets to evaluate the zero-shot and few-shot learning performance using the leave-one-dataset-out strategy.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery</title>
<link>https://arxiv.org/abs/2509.04757</link>
<guid>https://arxiv.org/abs/2509.04757</guid>
<content:encoded><![CDATA[
arXiv:2509.04757v1 Announce Type: cross 
Abstract: Rapid and accurate post-hurricane damage assessment is vital for disaster response and recovery. Yet existing CNN-based methods struggle to capture multi-scale spatial features and to distinguish visually similar or co-occurring damage types. To address these issues, we propose MCANet, a multi-label classification framework that learns multi-scale representations and adaptively attends to spatially relevant regions for each damage category. MCANet employs a Res2Net-based hierarchical backbone to enrich spatial context across scales and a multi-head class-specific residual attention module to enhance discrimination. Each attention branch focuses on different spatial granularities, balancing local detail with global context. We evaluate MCANet on the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael. MCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet, Res2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads, performance further improves to 92.35%, boosting average precision for challenging classes such as Road Blocked by over 6%. Class activation mapping confirms MCANet's ability to localize damage-relevant regions, supporting interpretability. Outputs from MCANet can inform post-disaster risk mapping, emergency routing, and digital twin-based disaster response. Future work could integrate disaster-specific knowledge graphs and multimodal large language models to improve adaptability to unseen disasters and enrich semantic understanding for real-world decision-making.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph</title>
<link>https://arxiv.org/abs/2509.04772</link>
<guid>https://arxiv.org/abs/2509.04772</guid>
<content:encoded><![CDATA[
arXiv:2509.04772v1 Announce Type: cross 
Abstract: Timely and accurate floodwater depth estimation is critical for road accessibility and emergency response. While recent computer vision methods have enabled flood detection, they suffer from both accuracy limitations and poor generalization due to dependence on fixed object detectors and task-specific training. To enable accurate depth estimation that can generalize across diverse flood scenarios, this paper presents FloodVision, a zero-shot framework that combines the semantic reasoning abilities of the foundation vision-language model GPT-4o with a structured domain knowledge graph. The knowledge graph encodes canonical real-world dimensions for common urban objects including vehicles, people, and infrastructure elements to ground the model's reasoning in physical reality. FloodVision dynamically identifies visible reference objects in RGB images, retrieves verified heights from the knowledge graph to mitigate hallucination, estimates submergence ratios, and applies statistical outlier filtering to compute final depth values. Evaluated on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and surpassing prior CNN-based methods. The system generalizes well across varying scenes and operates in near real-time, making it suitable for future integration into digital twin platforms and citizen-reporting apps for smart city flood resilience.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoders Laugh as Loud as Encoders</title>
<link>https://arxiv.org/abs/2509.04779</link>
<guid>https://arxiv.org/abs/2509.04779</guid>
<content:encoded><![CDATA[
arXiv:2509.04779v1 Announce Type: cross 
Abstract: From the dawn of the computer, Allen Turing dreamed of a robot that could communicate using language as a human being. The recent advances in the field of Large Language Models (LLMs) shocked the scientific community when a single model can apply for various natural language processing (NLP) tasks, while the output results are sometimes even better than most human communication skills. Models such as GPT, Claude, Grok, etc. have left their mark on the scientific community. However, it is unclear how much these models understand what they produce, especially in a nuanced theme such as humor. The question of whether computers understand humor is still open (among the decoders, the latest to be checked was GPT-2). We addressed this issue in this paper; we have showed that a fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well as the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models</title>
<link>https://arxiv.org/abs/2509.04781</link>
<guid>https://arxiv.org/abs/2509.04781</guid>
<content:encoded><![CDATA[
arXiv:2509.04781v1 Announce Type: cross 
Abstract: When given the option, will LLMs choose to leave the conversation (bail)? We investigate this question by giving models the option to bail out of interactions using three different bail methods: a bail tool the model can call, a bail string the model can output, and a bail prompt that asks the model if it wants to leave. On continuations of real world data (Wildchat and ShareGPT), all three of these bail methods find models will bail around 0.28-32\% of the time (depending on the model and bail method). However, we find that bail rates can depend heavily on the model used for the transcript, which means we may be overestimating real world bail rates by up to 4x. If we also take into account false positives on bail prompt (22\%), we estimate real world bail rates range from 0.06-7\%, depending on the model and bail method. We use observations from our continuations of real world data to construct a non-exhaustive taxonomy of bail cases, and use this taxonomy to construct BailBench: a representative synthetic dataset of situations where some models bail. We test many models on this dataset, and observe some bail behavior occurring for most of them. Bail rates vary substantially between models, bail methods, and prompt wordings. Finally, we study the relationship between refusals and bails. We find: 1) 0-13\% of continuations of real world conversations resulted in a bail without a corresponding refusal 2) Jailbreaks tend to decrease refusal rates, but increase bail rates 3) Refusal abliteration increases no-refuse bail rates, but only for some bail methods 4) Refusal rate on BailBench does not appear to predict bail rate.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VARMA-Enhanced Transformer for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.04782</link>
<guid>https://arxiv.org/abs/2509.04782</guid>
<content:encoded><![CDATA[
arXiv:2509.04782v1 Announce Type: cross 
Abstract: Transformer-based models have significantly advanced time series forecasting. Recent work, like the Cross-Attention-only Time Series transformer (CATS), shows that removing self-attention can make the model more accurate and efficient. However, these streamlined architectures may overlook the fine-grained, local temporal dependencies effectively captured by classical statistical models like Vector AutoRegressive Moving Average model (VARMA). To address this gap, we propose VARMAformer, a novel architecture that synergizes the efficiency of a cross-attention-only framework with the principles of classical time series analysis. Our model introduces two key innovations: (1) a dedicated VARMA-inspired Feature Extractor (VFE) that explicitly models autoregressive (AR) and moving-average (MA) patterns at the patch level, and (2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal gate to make queries more context-aware. By fusing these classical insights into a modern backbone, VARMAformer captures both global, long-range dependencies and local, statistical structures. Through extensive experiments on widely-used benchmark datasets, we demonstrate that our model consistently outperforms existing state-of-the-art methods. Our work validates the significant benefit of integrating classical statistical insights into modern deep learning frameworks for time series forecasting.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Diversity in Large Language Models via Determinantal Point Processes</title>
<link>https://arxiv.org/abs/2509.04784</link>
<guid>https://arxiv.org/abs/2509.04784</guid>
<content:encoded><![CDATA[
arXiv:2509.04784v1 Announce Type: cross 
Abstract: Supervised fine-tuning and reinforcement learning are two popular methods for post-training large language models (LLMs). While improving the model's performance on downstream tasks, they often reduce the model's output diversity, leading to narrow, canonical responses. Existing methods to enhance diversity are limited, either by operating at inference time or by focusing on lexical differences. We propose a novel training method named DQO based on determinantal point processes (DPPs) to jointly optimize LLMs for quality and semantic diversity. Our approach samples and embeds a group of responses for each prompt, then uses the determinant of a kernel-based similarity matrix to measure diversity as the volume spanned by the embeddings of these responses. Experiments across instruction-following, summarization, story generation, and reasoning tasks demonstrate that our method substantially improves semantic diversity without sacrificing model quality.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Unlearning: Efficient Node Removal in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.04785</link>
<guid>https://arxiv.org/abs/2509.04785</guid>
<content:encoded><![CDATA[
arXiv:2509.04785v1 Announce Type: cross 
Abstract: With increasing concerns about privacy attacks and potential sensitive information leakage, researchers have actively explored methods to efficiently remove sensitive training data and reduce privacy risks in graph neural network (GNN) models. Node unlearning has emerged as a promising technique for protecting the privacy of sensitive nodes by efficiently removing specific training node information from GNN models. However, existing node unlearning methods either impose restrictions on the GNN structure or do not effectively utilize the graph topology for node unlearning. Some methods even compromise the graph's topology, making it challenging to achieve a satisfactory performance-complexity trade-off. To address these issues and achieve efficient unlearning for training node removal in GNNs, we propose three novel node unlearning methods: Class-based Label Replacement, Topology-guided Neighbor Mean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among these methods, Topology-guided Neighbor Mean Posterior Probability and Class-consistent Neighbor Node Filtering effectively leverage the topological features of the graph, resulting in more effective node unlearning. To validate the superiority of our proposed methods in node unlearning, we conducted experiments on three benchmark datasets. The evaluation criteria included model utility, unlearning utility, and unlearning efficiency. The experimental results demonstrate the utility and efficiency of the proposed methods and illustrate their superiority compared to state-of-the-art node unlearning methods. Overall, the proposed methods efficiently remove sensitive training nodes and protect the privacy information of sensitive nodes in GNNs. The findings contribute to enhancing the privacy and security of GNN models and provide valuable insights into the field of node unlearning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images</title>
<link>https://arxiv.org/abs/2509.04800</link>
<guid>https://arxiv.org/abs/2509.04800</guid>
<content:encoded><![CDATA[
arXiv:2509.04800v1 Announce Type: cross 
Abstract: Skin diseases are among the most prevalent health concerns worldwide, yet conventional diagnostic methods are often costly, complex, and unavailable in low-resource settings. Automated classification using deep learning has emerged as a promising alternative, but existing studies are mostly limited to dermoscopic datasets and a narrow range of disease classes. In this work, we curate a large dataset of over 50 skin disease categories captured with mobile devices, making it more representative of real-world conditions. We evaluate multiple convolutional neural networks and Transformer-based architectures, demonstrating that Transformer models, particularly the Swin Transformer, achieve superior performance by effectively capturing global contextual features. To enhance interpretability, we incorporate Gradient-weighted Class Activation Mapping (Grad-CAM), which highlights clinically relevant regions and provides transparency in model predictions. Our results underscore the potential of Transformer-based approaches for mobile-acquired skin lesion classification, paving the way toward accessible AI-assisted dermatological screening and early diagnosis in resource-limited environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design</title>
<link>https://arxiv.org/abs/2509.04805</link>
<guid>https://arxiv.org/abs/2509.04805</guid>
<content:encoded><![CDATA[
arXiv:2509.04805v1 Announce Type: cross 
Abstract: Modern fronthaul links in wireless systems must transport high-dimensional signals under stringent bandwidth and latency constraints, which makes compression indispensable. Traditional strategies such as compressed sensing, scalar quantization, and fixed-codec pipelines often rely on restrictive priors, degrade sharply at high compression ratios, and are hard to tune across channels and deployments. Recent progress in Artificial Intelligence (AI) has brought end-to-end learned transforms, vector and hierarchical quantization, and learned entropy models that better exploit the structure of Channel State Information(CSI), precoding matrices, I/Q samples, and LLRs. This paper first surveys AI-driven compression techniques and then provides a focused analysis of two representative high-compression routes: CSI feedback with end-to-end learning and Resource Block (RB) granularity precoding optimization combined with compression. Building on these insights, we propose a fronthaul compression strategy tailored to cell-free architectures. The design targets high compression with controlled performance loss, supports RB-level rate adaptation, and enables low-latency inference suitable for centralized cooperative transmission in next-generation networks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution</title>
<link>https://arxiv.org/abs/2509.04824</link>
<guid>https://arxiv.org/abs/2509.04824</guid>
<content:encoded><![CDATA[
arXiv:2509.04824v1 Announce Type: cross 
Abstract: Recently, Mamba-based methods, with its advantage in long-range information modeling and linear complexity, have shown great potential in optimizing both computational cost and performance of light field image super-resolution (LFSR). However, current multi-directional scanning strategies lead to inefficient and redundant feature extraction when applied to complex LF data. To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS) strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to achieve more efficient and precise feature extraction. Furthermore, we propose a dual-stage modeling strategy to address the limitation of state space in preserving spatial-angular and disparity information, thereby enabling a more comprehensive exploration of non-local spatial-angular correlations. Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar feature refinement. Building upon meticulously designed modules and strategies, we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates the strengths of Mamba and Transformer models for LFSR, enabling comprehensive information exploration across spatial, angular, and epipolar-plane domains. Experimental results demonstrate that LFMT significantly outperforms current state-of-the-art methods in LFSR, achieving substantial improvements in performance while maintaining low computational complexity on both real-word and synthetic LF datasets.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</title>
<link>https://arxiv.org/abs/2509.04833</link>
<guid>https://arxiv.org/abs/2509.04833</guid>
<content:encoded><![CDATA[
arXiv:2509.04833v1 Announce Type: cross 
Abstract: Recent advances in visual grounding have largely shifted away from traditional proposal-based two-stage frameworks due to their inefficiency and high computational complexity, favoring end-to-end direct reference paradigms. However, these methods rely exclusively on the referred target for supervision, overlooking the potential benefits of prominent prospective targets. Moreover, existing approaches often fail to incorporate multi-granularity discrimination, which is crucial for robust object identification in complex scenarios. To address these limitations, we propose PropVG, an end-to-end proposal-based framework that, to the best of our knowledge, is the first to seamlessly integrate foreground object proposal generation with referential object comprehension without requiring additional detectors. Furthermore, we introduce a Contrastive-based Refer Scoring (CRS) module, which employs contrastive learning at both sentence and word levels to enhance the capability in understanding and distinguishing referred objects. Additionally, we design a Multi-granularity Target Discrimination (MTD) module that fuses object- and semantic-level information to improve the recognition of absent targets. Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO (REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and models are available at https://github.com/Dmmm1997/PropVG.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REMOTE: A Unified Multimodal Relation Extraction Framework with Multilevel Optimal Transport and Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2509.04844</link>
<guid>https://arxiv.org/abs/2509.04844</guid>
<content:encoded><![CDATA[
arXiv:2509.04844v1 Announce Type: cross 
Abstract: Multimodal relation extraction (MRE) is a crucial task in the fields of Knowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge graph construction. However, existing methods are typically limited to extracting a single type of relational triplet, which restricts their ability to extract triplets beyond the specified types. Directly combining these methods fails to capture dynamic cross-modal interactions and introduces significant computational redundancy. Therefore, we propose a novel \textit{unified multimodal Relation Extraction framework with Multilevel Optimal Transport and mixture-of-Experts}, termed REMOTE, which can simultaneously extract intra-modal and inter-modal relations between textual entities and visual objects. To dynamically select optimal interaction features for different types of relational triplets, we introduce mixture-of-experts mechanism, ensuring the most relevant modality information is utilized. Additionally, considering that the inherent property of multilayer sequential encoding in existing encoders often leads to the loss of low-level information, we adopt a multilevel optimal transport fusion module to preserve low-level features while maintaining multilayer encoding, yielding more expressive representations. Correspondingly, we also create a Unified Multimodal Relation Extraction (UMRE) dataset to evaluate the effectiveness of our framework, encompassing diverse cases where the head and tail entities can originate from either text or image. Extensive experiments show that REMOTE effectively extracts various types of relational triplets and achieves state-of-the-art performanc on almost all metrics across two other public MRE datasets. We release our resources at https://github.com/Nikol-coder/REMOTE.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing</title>
<link>https://arxiv.org/abs/2509.04853</link>
<guid>https://arxiv.org/abs/2509.04853</guid>
<content:encoded><![CDATA[
arXiv:2509.04853v1 Announce Type: cross 
Abstract: End-to-end autonomous driving remains constrained by the need to generate multi-modal actions, maintain temporal stability, and generalize across diverse scenarios. Existing methods often collapse multi-modality, struggle with long-horizon consistency, or lack modular adaptability. This paper presents KDP, a knowledge-driven diffusion policy that integrates generative diffusion modeling with a sparse mixture-of-experts routing mechanism. The diffusion component generates temporally coherent and multi-modal action sequences, while the expert routing mechanism activates specialized and reusable experts according to context, enabling modular knowledge composition. Extensive experiments across representative driving scenarios demonstrate that KDP achieves consistently higher success rates, reduced collision risk, and smoother control compared to prevailing paradigms. Ablation studies highlight the effectiveness of sparse expert activation and the Transformer backbone, and activation analyses reveal structured specialization and cross-scenario reuse of experts. These results establish diffusion with expert routing as a scalable and interpretable paradigm for knowledge-driven end-to-end autonomous driving.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Paradox of Doom: Acknowledging Extinction Risk Reduces the Incentive to Prevent It</title>
<link>https://arxiv.org/abs/2509.04855</link>
<guid>https://arxiv.org/abs/2509.04855</guid>
<content:encoded><![CDATA[
arXiv:2509.04855v1 Announce Type: cross 
Abstract: We investigate the salience of extinction risk as a source of impatience. Our framework distinguishes between human extinction risk and individual mortality risk while allowing for various degrees of intergenerational altruism. Additionally, we consider the evolutionarily motivated "selfish gene" perspective. We find that the risk of human extinction is an indispensable component of the discount rate, whereas individual mortality risk can be hedged against - partially or fully, depending on the setup - through human reproduction. Overall, we show that in the face of extinction risk, people become more impatient rather than more farsighted. Thus, the greater the threat of extinction, the less incentive there is to invest in avoiding it. Our framework can help explain why humanity consistently underinvests in mitigation of catastrophic risks, ranging from climate change mitigation, via pandemic prevention, to addressing the emerging risks of transformative artificial intelligence.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models</title>
<link>https://arxiv.org/abs/2509.04889</link>
<guid>https://arxiv.org/abs/2509.04889</guid>
<content:encoded><![CDATA[
arXiv:2509.04889v1 Announce Type: cross 
Abstract: Advances in computer vision have opened new avenues for clinical applications, particularly in computerized exposure therapy where visual stimuli can be dynamically adjusted based on patient responses. As a critical step toward such adaptive systems, we investigated whether pretrained computer vision models can accurately predict fear levels from spider-related images. We adapted three diverse models using transfer learning to predict human fear ratings (on a 0-100 scale) from a standardized dataset of 313 images. The models were evaluated using cross-validation, achieving an average mean absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis revealed that reducing the dataset size significantly harmed performance, though further increases yielded no substantial gains. Explainability assessments showed the models' predictions were based on spider-related features. A category-wise error analysis further identified visual conditions associated with higher errors (e.g., distant views and artificial/painted spiders). These findings demonstrate the potential of explainable computer vision models in predicting fear ratings, highlighting the importance of both model explainability and a sufficient dataset size for developing effective emotion-aware therapeutic technologies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLaMo 2 Technical Report</title>
<link>https://arxiv.org/abs/2509.04897</link>
<guid>https://arxiv.org/abs/2509.04897</guid>
<content:encoded><![CDATA[
arXiv:2509.04897v1 Announce Type: cross 
Abstract: In this report, we introduce PLaMo 2, a series of Japanese-focused large language models featuring a hybrid Samba-based architecture that transitions to full attention via continual pre-training to support 32K token contexts. Training leverages extensive synthetic corpora to overcome data scarcity, while computational efficiency is achieved through weight reuse and structured pruning. This efficient pruning methodology produces an 8B model that achieves performance comparable to our previous 100B model. Post-training further refines the models using a pipeline of supervised fine-tuning (SFT) and direct preference optimization (DPO), enhanced by synthetic Japanese instruction data and model merging techniques. Optimized for inference using vLLM and quantization with minimal accuracy loss, the PLaMo 2 models achieve state-of-the-art results on Japanese benchmarks, outperforming similarly-sized open models in instruction-following, language fluency, and Japanese-specific knowledge.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial intelligence for representing and characterizing quantum systems</title>
<link>https://arxiv.org/abs/2509.04923</link>
<guid>https://arxiv.org/abs/2509.04923</guid>
<content:encoded><![CDATA[
arXiv:2509.04923v1 Announce Type: cross 
Abstract: Efficient characterization of large-scale quantum systems, especially those produced by quantum analog simulators and megaquop quantum computers, poses a central challenge in quantum science due to the exponential scaling of the Hilbert space with respect to system size. Recent advances in artificial intelligence (AI), with its aptitude for high-dimensional pattern recognition and function approximation, have emerged as a powerful tool to address this challenge. A growing body of research has leveraged AI to represent and characterize scalable quantum systems, spanning from theoretical foundations to experimental realizations. Depending on how prior knowledge and learning architectures are incorporated, the integration of AI into quantum system characterization can be categorized into three synergistic paradigms: machine learning, and, in particular, deep learning and language models. This review discusses how each of these AI paradigms contributes to two core tasks in quantum systems characterization: quantum property prediction and the construction of surrogates for quantum states. These tasks underlie diverse applications, from quantum certification and benchmarking to the enhancement of quantum algorithms and the understanding of strongly correlated phases of matter. Key challenges and open questions are also discussed, together with future prospects at the interface of AI and quantum science.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation</title>
<link>https://arxiv.org/abs/2509.04970</link>
<guid>https://arxiv.org/abs/2509.04970</guid>
<content:encoded><![CDATA[
arXiv:2509.04970v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) agents can learn to solve complex tasks from visual inputs, but generalizing these learned skills to new environments remains a major challenge in RL application, especially robotics. While data augmentation can improve generalization, it often compromises sample efficiency and training stability. This paper introduces DeGuV, an RL framework that enhances both generalization and sample efficiency. In specific, we leverage a learnable masker network that produces a mask from the depth input, preserving only critical visual information while discarding irrelevant pixels. Through this, we ensure that our RL agents focus on essential features, improving robustness under data augmentation. In addition, we incorporate contrastive learning and stabilize Q-value estimation under augmentation to further enhance sample efficiency and training stability. We evaluate our proposed method on the RL-ViGen benchmark using the Franka Emika robot and demonstrate its effectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV outperforms state-of-the-art methods in both generalization and sample efficiency while also improving interpretability by highlighting the most relevant regions in the visual input
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring an implementation of quantum learning pipeline for support vector machines</title>
<link>https://arxiv.org/abs/2509.04983</link>
<guid>https://arxiv.org/abs/2509.04983</guid>
<content:encoded><![CDATA[
arXiv:2509.04983v1 Announce Type: cross 
Abstract: This work presents a fully quantum approach to support vector machine (SVM) learning by integrating gate-based quantum kernel methods with quantum annealing-based optimization. We explore the construction of quantum kernels using various feature maps and qubit configurations, evaluating their suitability through Kernel-Target Alignment (KTA). The SVM dual problem is reformulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem, enabling its solution via quantum annealers. Our experiments demonstrate that a high degree of alignment in the kernel and an appropriate regularization parameter lead to competitive performance, with the best model achieving an F1-score of 90%. These results highlight the feasibility of an end-to-end quantum learning pipeline and the potential of hybrid quantum architectures in quantum high-performance computing (QHPC) contexts.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Resolution Global Land Surface Temperature Retrieval via a Coupled Mechanism-Machine Learning Framework</title>
<link>https://arxiv.org/abs/2509.04991</link>
<guid>https://arxiv.org/abs/2509.04991</guid>
<content:encoded><![CDATA[
arXiv:2509.04991v1 Announce Type: cross 
Abstract: Land surface temperature (LST) is vital for land-atmosphere interactions and climate processes. Accurate LST retrieval remains challenging under heterogeneous land cover and extreme atmospheric conditions. Traditional split window (SW) algorithms show biases in humid environments; purely machine learning (ML) methods lack interpretability and generalize poorly with limited data. We propose a coupled mechanism model-ML (MM-ML) framework integrating physical constraints with data-driven learning for robust LST retrieval. Our approach fuses radiative transfer modeling with data components, uses MODTRAN simulations with global atmospheric profiles, and employs physics-constrained optimization. Validation against 4,450 observations from 29 global sites shows MM-ML achieves MAE=1.84K, RMSE=2.55K, and R-squared=0.966, outperforming conventional methods. Under extreme conditions, MM-ML reduces errors by over 50%. Sensitivity analysis indicates LST estimates are most sensitive to sensor radiance, then water vapor, and less to emissivity, with MM-ML showing superior stability. These results demonstrate the effectiveness of our coupled modeling strategy for retrieving geophysical parameters. The MM-ML framework combines physical interpretability with nonlinear modeling capacity, enabling reliable LST retrieval in complex environments and supporting climate monitoring and ecosystem studies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of Dual-Loop Edge-Terminal Collaboration</title>
<link>https://arxiv.org/abs/2509.04993</link>
<guid>https://arxiv.org/abs/2509.04993</guid>
<content:encoded><![CDATA[
arXiv:2509.04993v1 Announce Type: cross 
Abstract: The ubiquitous computing resources in 6G networks provide ideal environments for the fusion of large language models (LLMs) and intelligent services through the agent framework. With auxiliary modules and planning cores, LLM-enabled agents can autonomously plan and take actions to deal with diverse environment semantics and user intentions. However, the limited resources of individual network devices significantly hinder the efficient operation of LLM-enabled agents with complex tool calls, highlighting the urgent need for efficient multi-level device collaborations. To this end, the framework and method of the LLM-enabled multi-agent system with dual-loop terminal-edge collaborations are proposed in 6G networks. Firstly, the outer loop consists of the iterative collaborations between the global agent and multiple sub-agents deployed on edge servers and terminals, where the planning capability is enhanced through task decomposition and parallel sub-task distribution. Secondly, the inner loop utilizes sub-agents with dedicated roles to circularly reason, execute, and replan the sub-task, and the parallel tool calling generation with offloading strategies is incorporated to improve efficiency. The improved task planning capability and task execution efficiency are validated through the conducted case study in 6G-supported urban safety governance. Finally, the open challenges and future directions are thoroughly analyzed in 6G networks, accelerating the advent of the 6G era.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.04999</link>
<guid>https://arxiv.org/abs/2509.04999</guid>
<content:encoded><![CDATA[
arXiv:2509.04999v1 Announce Type: cross 
Abstract: Advanced Persistent Threats (APTs) present a considerable challenge to cybersecurity due to their stealthy, long-duration nature. Traditional supervised learning methods typically require large amounts of labeled data, which is often scarce in real-world scenarios. This paper introduces a novel approach that combines AutoEncoders for anomaly detection with active learning to iteratively enhance APT detection. By selectively querying an oracle for labels on uncertain or ambiguous samples, our method reduces labeling costs while improving detection accuracy, enabling the model to effectively learn with minimal data and reduce reliance on extensive manual labeling. We present a comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based anomaly detection framework and demonstrate how the active learning loop progressively enhances the model's performance. The framework is evaluated on real-world, imbalanced provenance trace data from the DARPA Transparent Computing program, where APT-like attacks account for just 0.004\% of the data. The datasets, which cover multiple operating systems including Android, Linux, BSD, and Windows, are tested in two attack scenarios. The results show substantial improvements in detection rates during active learning, outperforming existing methods.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pointing-Guided Target Estimation via Transformer-Based Attention</title>
<link>https://arxiv.org/abs/2509.05031</link>
<guid>https://arxiv.org/abs/2509.05031</guid>
<content:encoded><![CDATA[
arXiv:2509.05031v1 Announce Type: cross 
Abstract: Deictic gestures, like pointing, are a fundamental form of non-verbal communication, enabling humans to direct attention to specific objects or locations. This capability is essential in Human-Robot Interaction (HRI), where robots should be able to predict human intent and anticipate appropriate responses. In this work, we propose the Multi-Modality Inter-TransFormer (MM-ITF), a modular architecture to predict objects in a controlled tabletop scenario with the NICOL robot, where humans indicate targets through natural pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing gestures to object locations, assigns a likelihood score to each, and identifies the most likely target. Our results demonstrate that the method can accurately predict the intended object using monocular RGB data, thus enabling intuitive and accessible human-robot collaboration. To evaluate the performance, we introduce a patch confusion matrix, providing insights into the model's predictions across candidate object locations. Code available at: https://github.com/lucamuellercode/MMITF.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization</title>
<link>https://arxiv.org/abs/2509.05034</link>
<guid>https://arxiv.org/abs/2509.05034</guid>
<content:encoded><![CDATA[
arXiv:2509.05034v1 Announce Type: cross 
Abstract: Industrial product inspection is often performed using Anomaly Detection (AD) frameworks trained solely on non-defective samples. Although defective samples can be collected during production, leveraging them usually requires pixel-level annotations, limiting scalability. To address this, we propose ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial anomaly detection. ADClick generates pixel-wise anomaly annotations from only a few user clicks and a brief textual description, enabling precise and efficient labeling that significantly improves AD model performance (e.g., AP = 96.1\% on MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that aligns visual features and textual prompts via a prototype-based approach for anomaly detection and localization. By combining pixel-level priors with language-guided cues, ADClick-Seg achieves state-of-the-art results on the challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC = 99.1\% on MVTec AD).
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions</title>
<link>https://arxiv.org/abs/2509.05066</link>
<guid>https://arxiv.org/abs/2509.05066</guid>
<content:encoded><![CDATA[
arXiv:2509.05066v1 Announce Type: cross 
Abstract: Most existing Theory of Mind (ToM) benchmarks for foundation models rely on variations of the Sally-Anne test, offering only a very limited perspective on ToM and neglecting the complexity of human social interactions. To address this gap, we propose ToM-SSI: a new benchmark specifically designed to test ToM capabilities in environments rich with social interactions and spatial dynamics. While current ToM benchmarks are limited to text-only or dyadic interactions, ToM-SSI is multimodal and includes group interactions of up to four agents that communicate and move in situated environments. This unique design allows us to study, for the first time, mixed cooperative-obstructive settings and reasoning about multiple agents' mental state in parallel, thus capturing a wider range of social cognition than existing benchmarks. Our evaluations reveal that the current models' performance is still severely limited, especially in these new tasks, highlighting critical gaps for future research.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICR: Iterative Clarification and Rewriting for Conversational Search</title>
<link>https://arxiv.org/abs/2509.05100</link>
<guid>https://arxiv.org/abs/2509.05100</guid>
<content:encoded><![CDATA[
arXiv:2509.05100v1 Announce Type: cross 
Abstract: Most previous work on Conversational Query Rewriting employs an end-to-end rewriting paradigm. However, this approach is hindered by the issue of multiple fuzzy expressions within the query, which complicates the simultaneous identification and rewriting of multiple positions. To address this issue, we propose a novel framework ICR (Iterative Clarification and Rewriting), an iterative rewriting scheme that pivots on clarification questions. Within this framework, the model alternates between generating clarification questions and rewritten queries. The experimental results show that our ICR can continuously improve retrieval performance in the clarification-rewriting iterative process, thereby achieving state-of-the-art performance on two popular datasets.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenAI-based test case generation and execution in SDV platform</title>
<link>https://arxiv.org/abs/2509.05112</link>
<guid>https://arxiv.org/abs/2509.05112</guid>
<content:encoded><![CDATA[
arXiv:2509.05112v1 Announce Type: cross 
Abstract: This paper introduces a GenAI-driven approach for automated test case generation, leveraging Large Language Models and Vision-Language Models to translate natural language requirements and system diagrams into structured Gherkin test cases. The methodology integrates Vehicle Signal Specification modeling to standardize vehicle signal definitions, improve compatibility across automotive subsystems, and streamline integration with third-party testing tools. Generated test cases are executed within the digital.auto playground, an open and vendor-neutral environment designed to facilitate rapid validation of software-defined vehicle functionalities. We evaluate our approach using the Child Presence Detection System use case, demonstrating substantial reductions in manual test specification effort and rapid execution of generated tests. Despite significant automation, the generation of test cases and test scripts still requires manual intervention due to current limitations in the GenAI pipeline and constraints of the digital.auto platform.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination</title>
<link>https://arxiv.org/abs/2509.05145</link>
<guid>https://arxiv.org/abs/2509.05145</guid>
<content:encoded><![CDATA[
arXiv:2509.05145v1 Announce Type: cross 
Abstract: This paper investigates GrooveTransformer, a real-time rhythm generation system, through the postphenomenological framework of Variational Cross-Examination (VCE). By reflecting on its deployment across three distinct artistic contexts, we identify three stabilities: an autonomous drum accompaniment generator, a rhythmic control voltage sequencer in Eurorack format, and a rhythm driver for a harmonic accompaniment system. The versatility of its applications was not an explicit goal from the outset of the project. Thus, we ask: how did this multistability emerge? Through VCE, we identify three key contributors to its emergence: the affordances of system invariants, the interdisciplinary collaboration, and the situated nature of its development. We conclude by reflecting on the viability of VCE as a descriptive and analytical method for Digital Musical Instrument (DMI) design, emphasizing its value in uncovering how technologies mediate, co-shape, and are co-shaped by users and contexts.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection</title>
<link>https://arxiv.org/abs/2509.05190</link>
<guid>https://arxiv.org/abs/2509.05190</guid>
<content:encoded><![CDATA[
arXiv:2509.05190v1 Announce Type: cross 
Abstract: Deep learning models, especially convolutional neural networks (CNNs), have shown considerable promise for biomedical signals such as EEG-based seizure detection. However, these models come with challenges, primarily due to their size and compute requirements in environments where real-time detection or limited resources are available. In this study, we present a lightweight one-dimensional CNN model with structured pruning to improve efficiency and reliability. The model was trained with mild early stopping to address possible overfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686. Structured pruning of the baseline CNN involved removing 50% of the convolutional kernels based on their importance to model predictions. Surprisingly, after pruning the weights and memory by 50%, the new network was still able to maintain predictive capabilities, while modestly increasing precision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we present a convincing case that structured pruning removes redundancy, improves generalization, and, in combination with mild early stopping, achieves a promising way forward to improve seizure detection efficiency and reliability, which is clear motivation for resource-limited settings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents for Web Testing: A Case Study in the Wild</title>
<link>https://arxiv.org/abs/2509.05197</link>
<guid>https://arxiv.org/abs/2509.05197</guid>
<content:encoded><![CDATA[
arXiv:2509.05197v1 Announce Type: cross 
Abstract: Automated web testing plays a critical role in ensuring high-quality user experiences and delivering business value. Traditional approaches primarily focus on code coverage and load testing, but often fall short of capturing complex user behaviors, leaving many usability issues undetected. The emergence of large language models (LLM) and AI agents opens new possibilities for web testing by enabling human-like interaction with websites and a general awareness of common usability problems. In this work, we present WebProber, a prototype AI agent-based web testing framework. Given a URL, WebProber autonomously explores the website, simulating real user interactions, identifying bugs and usability issues, and producing a human-readable report. We evaluate WebProber through a case study of 120 academic personal websites, where it uncovered 29 usability issues--many of which were missed by traditional tools. Our findings highlight agent-based testing as a promising direction while outlining directions for developing next-generation, user-centered testing frameworks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</title>
<link>https://arxiv.org/abs/2509.05198</link>
<guid>https://arxiv.org/abs/2509.05198</guid>
<content:encoded><![CDATA[
arXiv:2509.05198v1 Announce Type: cross 
Abstract: The classification of 3D point clouds is crucial for applications such as autonomous driving, robotics, and augmented reality. However, the commonly used ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D data, size mismatches, and inadequate class differentiation, which hinder model performance. This paper introduces ModelNet-R, a meticulously refined version of ModelNet40 designed to address these issues and serve as a more reliable benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight graph-based neural network that leverages efficient sampling, neighborhood grouping, and skip connections to achieve high classification accuracy with reduced computational overhead. Extensive experiments demonstrate that models trained in ModelNet-R exhibit significant performance improvements. Notably, Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a substantially lower parameter count compared to contemporary models. This research highlights the crucial role of dataset quality in optimizing model efficiency for 3D point cloud classification. For more details, see the code at: https://github.com/m-saeid/ModeNetR_PointSkipNet.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.05207</link>
<guid>https://arxiv.org/abs/2509.05207</guid>
<content:encoded><![CDATA[
arXiv:2509.05207v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have become popular across a diverse set of tasks in exploring structural relationships between entities. However, due to the highly connected structure of the datasets, distributed training of GNNs on large-scale graphs poses significant challenges. Traditional sampling-based approaches mitigate the computational loads, yet the communication overhead remains a challenge. This paper presents RapidGNN, a distributed GNN training framework with deterministic sampling-based scheduling to enable efficient cache construction and prefetching of remote features. Evaluation on benchmark graph datasets demonstrates RapidGNN's effectiveness across different scales and topologies. RapidGNN improves end-to-end training throughput by 2.46x to 3.00x on average over baseline methods across the benchmark datasets, while cutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further demonstrates near-linear scalability with an increasing number of computing units efficiently. Furthermore, it achieves increased energy efficiency over the baseline methods for both CPU and GPU by 44% and 32%, respectively.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models</title>
<link>https://arxiv.org/abs/2509.05218</link>
<guid>https://arxiv.org/abs/2509.05218</guid>
<content:encoded><![CDATA[
arXiv:2509.05218v1 Announce Type: cross 
Abstract: Positional encoding mechanisms enable Transformers to model sequential structure and long-range dependencies in text. While absolute positional encodings struggle with extrapolation to longer sequences due to fixed positional representations, and relative approaches like Alibi exhibit performance degradation on extremely long contexts, the widely-used Rotary Positional Encoding (RoPE) introduces oscillatory attention patterns that hinder stable long-distance dependency modelling. We address these limitations through a geometric reformulation of positional encoding. Drawing inspiration from Lorentz transformations in hyperbolic geometry, we propose Hyperbolic Rotary Positional Encoding (HoPE), which leverages hyperbolic functions to implement Lorentz rotations on token representations. Theoretical analysis demonstrates that RoPE is a special case of our generalized formulation. HoPE fundamentally resolves RoPE's slation issues by enforcing monotonic decay of attention weights with increasing token distances. Extensive experimental results, including perplexity evaluations under several extended sequence benchmarks, show that HoPE consistently exceeds existing positional encoding methods. These findings underscore HoPE's enhanced capacity for representing and generalizing long-range dependencies. Data and code will be available.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models</title>
<link>https://arxiv.org/abs/2509.05230</link>
<guid>https://arxiv.org/abs/2509.05230</guid>
<content:encoded><![CDATA[
arXiv:2509.05230v1 Announce Type: cross 
Abstract: Pre-trained language models have achieved remarkable success across diverse applications but remain susceptible to spurious, concept-driven correlations that impair robustness and fairness. In this work, we introduce CURE, a novel and lightweight framework that systematically disentangles and suppresses conceptual shortcuts while preserving essential content information. Our method first extracts concept-irrelevant representations via a dedicated content extractor reinforced by a reversal network, ensuring minimal loss of task-relevant information. A subsequent controllable debiasing module employs contrastive learning to finely adjust the influence of residual conceptual cues, enabling the model to either diminish harmful biases or harness beneficial correlations as appropriate for the target task. Evaluated on the IMDB and Yelp datasets using three pre-trained architectures, CURE achieves an absolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp, while introducing minimal computational overhead. Our approach establishes a flexible, unsupervised blueprint for combating conceptual biases, paving the way for more reliable and fair language understanding systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertain but Useful: Leveraging CNN Variability into Data Augmentation</title>
<link>https://arxiv.org/abs/2509.05238</link>
<guid>https://arxiv.org/abs/2509.05238</guid>
<content:encoded><![CDATA[
arXiv:2509.05238v1 Announce Type: cross 
Abstract: Deep learning (DL) is rapidly advancing neuroimaging by achieving state-of-the-art performance with reduced computation times. Yet the numerical stability of DL models -- particularly during training -- remains underexplored. While inference with DL is relatively stable, training introduces additional variability primarily through iterative stochastic optimization. We investigate this training-time variability using FastSurfer, a CNN-based whole-brain segmentation pipeline. Controlled perturbations are introduced via floating point perturbations and random seeds. We find that: (i) FastSurfer exhibits higher variability compared to that of a traditional neuroimaging pipeline, suggesting that DL inherits and is particularly susceptible to sources of instability present in its predecessors; (ii) ensembles generated with perturbations achieve performance similar to an unperturbed baseline; and (iii) variability effectively produces ensembles of numerical model families that can be repurposed for downstream applications. As a proof of concept, we demonstrate that numerical ensembles can be used as a data augmentation strategy for brain age regression. These findings position training-time variability not only as a reproducibility concern but also as a resource that can be harnessed to improve robustness and enable new applications in neuroimaging.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization</title>
<link>https://arxiv.org/abs/2509.05249</link>
<guid>https://arxiv.org/abs/2509.05249</guid>
<content:encoded><![CDATA[
arXiv:2509.05249v1 Announce Type: cross 
Abstract: The ability to compose learned concepts and apply them in novel settings is key to human intelligence, but remains a persistent limitation in state-of-the-art machine learning models. To address this issue, we introduce COGITAO, a modular and extensible data generation framework and benchmark designed to systematically study compositionality and generalization in visual domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs rule-based tasks which apply a set of transformations to objects in grid-like environments. It supports composition, at adjustable depth, over a set of 28 interoperable transformations, along with extensive control over grid parametrization and object properties. This flexibility enables the creation of millions of unique task rules -- surpassing concurrent datasets by several orders of magnitude -- across a wide range of difficulties, while allowing virtually unlimited sample generation per rule. We provide baseline experiments using state-of-the-art vision models, highlighting their consistent failures to generalize to novel combinations of familiar elements, despite strong in-domain performance. COGITAO is fully open-sourced, including all code and datasets, to support continued research in this field.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recomposer: Event-roll-guided generative audio editing</title>
<link>https://arxiv.org/abs/2509.05256</link>
<guid>https://arxiv.org/abs/2509.05256</guid>
<content:encoded><![CDATA[
arXiv:2509.05256v1 Announce Type: cross 
Abstract: Editing complex real-world sound scenes is difficult because individual sound sources overlap in time. Generative models can fill-in missing or corrupted details based on their strong prior understanding of the data domain. We present a system for editing individual sound events within complex scenes able to delete, insert, and enhance individual sound events based on textual edit descriptions (e.g., ``enhance Door'') and a graphical representation of the event timing derived from an ``event roll'' transcription. We present an encoder-decoder transformer working on SoundStream representations, trained on synthetic (input, desired output) audio example pairs formed by adding isolated sound events to dense, real-world backgrounds. Evaluation reveals the importance of each part of the edit descriptions -- action, class, timing. Our work demonstrates ``recomposition'' is an important and practical application.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Performance of Large Language Model Pretraining</title>
<link>https://arxiv.org/abs/2509.05258</link>
<guid>https://arxiv.org/abs/2509.05258</guid>
<content:encoded><![CDATA[
arXiv:2509.05258v1 Announce Type: cross 
Abstract: Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortunately, information about the scaling performance and training considerations of these large training pipelines is scarce in public literature. Working with large-scale datasets and models can be complex and practical recommendations are scarce in the public literature for tuning training performance when scaling up large language models. In this paper, we aim to demystify the large language model pretraining pipeline somewhat - in particular with respect to distributed training, managing large datasets across hundreds of nodes, and scaling up data parallelism with an emphasis on fully leveraging available GPU compute capacity.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpikingBrain Technical Report: Spiking Brain-inspired Large Models</title>
<link>https://arxiv.org/abs/2509.05276</link>
<guid>https://arxiv.org/abs/2509.05276</guid>
<content:encoded><![CDATA[
arXiv:2509.05276v1 Announce Type: cross 
Abstract: Mainstream Transformer-based large language models face major efficiency bottlenecks: training computation scales quadratically with sequence length, and inference memory grows linearly, limiting long-context processing. Building large models on non-NVIDIA platforms also poses challenges for stable and efficient training. To address this, we introduce SpikingBrain, a family of brain-inspired models designed for efficient long-context training and inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three aspects: (1) Model Architecture: linear and hybrid-linear attention architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an efficient, conversion-based training pipeline and a dedicated spike coding framework; (3) System Engineering: customized training frameworks, operator libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM, and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the feasibility of large-scale LLM development on non-NVIDIA platforms. SpikingBrain achieves performance comparable to open-source Transformer baselines while using only about 150B tokens for continual pre-training. Our models significantly improve long-sequence training efficiency and deliver inference with (partially) constant memory and event-driven spiking behavior. For example, SpikingBrain-7B attains over 100x speedup in Time to First Token for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4 percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling low-power operation. Overall, this work demonstrates the potential of brain-inspired mechanisms to drive the next generation of efficient and scalable large model design.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crosscoding Through Time: Tracking Emergence &amp; Consolidation Of Linguistic Representations Throughout LLM Pretraining</title>
<link>https://arxiv.org/abs/2509.05291</link>
<guid>https://arxiv.org/abs/2509.05291</guid>
<content:encoded><![CDATA[
arXiv:2509.05291v1 Announce Type: cross 
Abstract: Large language models (LLMs) learn non-trivial abstractions during pretraining, like detecting irregular plural noun subjects. However, it is not well understood when and how specific linguistic abilities emerge as traditional evaluation methods such as benchmarking fail to reveal how models acquire concepts and capabilities. To bridge this gap and better understand model training at the concept level, we use sparse crosscoders to discover and align features across model checkpoints. Using this approach, we track the evolution of linguistic features during pretraining. We train crosscoders between open-sourced checkpoint triplets with significant performance and representation shifts, and introduce a novel metric, Relative Indirect Effects (RelIE), to trace training stages at which individual features become causally important for task performance. We show that crosscoders can detect feature emergence, maintenance, and discontinuation during pretraining. Our approach is architecture-agnostic and scalable, offering a promising path toward more interpretable and fine-grained analysis of representation learning throughout pretraining.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool</title>
<link>https://arxiv.org/abs/2509.05296</link>
<guid>https://arxiv.org/abs/2509.05296</guid>
<content:encoded><![CDATA[
arXiv:2509.05296v1 Announce Type: cross 
Abstract: We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2403.05265</link>
<guid>https://arxiv.org/abs/2403.05265</guid>
<content:encoded><![CDATA[
arXiv:2403.05265v3 Announce Type: replace 
Abstract: Online movie review websites are valuable for information and discussion about movies. However, the massive spoiler reviews detract from the movie-watching experience, making spoiler detection an important task. Previous methods simply focus on reviews' text content, ignoring the heterogeneity of information in the platform. For instance, the metadata and the corresponding user's information of a review could be helpful. Besides, the spoiler language of movie reviews tends to be genre-specific, thus posing a domain generalization challenge for existing methods. To this end, we propose MMoE, a multi-modal network that utilizes information from multiple modalities to facilitate robust spoiler detection and adopts Mixture-of-Experts to enhance domain generalization. MMoE first extracts graph, text, and meta feature from the user-movie network, the review's textual content, and the review's metadata respectively. To handle genre-specific spoilers, we then adopt Mixture-of-Experts architecture to process information in three modalities to promote robustness. Finally, we use an expert fusion layer to integrate the features from different perspectives and make predictions based on the fused embedding. Experiments demonstrate that MMoE achieves state-of-the-art performance on two widely-used spoiler detection datasets, surpassing previous SOTA methods by 2.56% and 8.41% in terms of accuracy and F1-score. Further experiments also demonstrate MMoE's superiority in robustness and generalization. Our code is available at https://github.com/zzqbjt/Spoiler-Detection.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Dimensional Federated Knowledge Graph Embedding via Knowledge Distillation</title>
<link>https://arxiv.org/abs/2408.05748</link>
<guid>https://arxiv.org/abs/2408.05748</guid>
<content:encoded><![CDATA[
arXiv:2408.05748v2 Announce Type: replace 
Abstract: Federated Knowledge Graph Embedding (FKGE) aims to facilitate collaborative learning of entity and relation embeddings from distributed Knowledge Graphs (KGs) across multiple clients, while preserving data privacy. Training FKGE models with higher dimensions is typically favored due to their potential for achieving superior performance. However, high-dimensional embeddings present significant challenges in terms of storage resource and inference speed. Unlike traditional KG embedding methods, FKGE involves multiple client-server communication rounds, where communication efficiency is critical. Existing embedding compression methods for traditional KGs may not be directly applicable to FKGE as they often require multiple model trainings which potentially incur substantial communication costs. In this paper, we propose a light-weight component based on Knowledge Distillation (KD) which is titled FedKD and tailored specifically for FKGE methods. During client-side local training, FedKD facilitates the low-dimensional student model to mimic the score distribution of triples from the high-dimensional teacher model using KL divergence loss. Unlike traditional KD way, FedKD adaptively learns a temperature to scale the score of positive triples and separately adjusts the scores of corresponding negative triples using a predefined temperature, thereby mitigating teacher over-confidence issue. Furthermore, we dynamically adjust the weight of KD loss to optimize the training process. Extensive experiments on three datasets support the effectiveness of FedKD.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning</title>
<link>https://arxiv.org/abs/2408.09600</link>
<guid>https://arxiv.org/abs/2408.09600</guid>
<content:encoded><![CDATA[
arXiv:2408.09600v3 Announce Type: replace 
Abstract: Safety aligned Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- a few harmful data mixed in the fine-tuning dataset can break the LLMs's safety alignment. While several defenses have been proposed, our evaluation shows that existing defenses fail \textit{when some specific training hyper-parameters are chosen} -- a large learning rate or a large number of training epochs in the fine-tuning stage can easily invalidate the defense. To this end, we propose Antidote, a post-fine-tuning stage solution, which remains \textbf{\textit{agnostic to the training hyper-parameters in the fine-tuning stage}}. Antidote relies on the philosophy that by removing the harmful parameters, the harmful model can be recovered from the harmful behaviors, regardless of how those harmful parameters are formed in the fine-tuning stage. With this philosophy, we introduce a one-shot pruning stage after harmful fine-tuning to remove the harmful weights that are responsible for the generation of harmful content. Despite its embarrassing simplicity, empirical results show that Antidote can reduce harmful score while maintaining accuracy on downstream tasks. Code is available at https://github.com/git-disl/Antidote.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Network Verification with PyRAT</title>
<link>https://arxiv.org/abs/2410.23903</link>
<guid>https://arxiv.org/abs/2410.23903</guid>
<content:encoded><![CDATA[
arXiv:2410.23903v2 Announce Type: replace 
Abstract: As AI systems are becoming more and more popular and used in various critical domains (health, transport, energy, ...), the need to provide guarantees and trust of their safety is undeniable. To this end, we present PyRAT, a tool based on abstract interpretation to verify the safety and the robustness of neural networks. In this paper, we describe the different abstractions used by PyRAT to find the reachable states of a neural network starting from its input as well as the main features of the tool to provide fast and accurate analysis of neural networks. PyRAT has already been used in several collaborations to ensure safety guarantees, with its second place at the VNN-Comp 2024 showcasing its performance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI Agents: A Survey</title>
<link>https://arxiv.org/abs/2412.13501</link>
<guid>https://arxiv.org/abs/2412.13501</guid>
<content:encoded><![CDATA[
arXiv:2412.13501v2 Announce Type: replace 
Abstract: Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment</title>
<link>https://arxiv.org/abs/2503.15937</link>
<guid>https://arxiv.org/abs/2503.15937</guid>
<content:encoded><![CDATA[
arXiv:2503.15937v3 Announce Type: replace 
Abstract: We propose V-Droid, a mobile GUI task automation agent. Unlike previous mobile agents that utilize Large Language Models (LLMs) as generators to directly generate actions at each step, V-Droid employs LLMs as verifiers to evaluate candidate actions before making final decisions. To realize this novel paradigm, we introduce a comprehensive framework for constructing verifier-driven mobile agents: the discretized action space construction coupled with the prefilling-only workflow to accelerate the verification process, the pair-wise progress preference training to significantly enhance the verifier's decision-making capabilities, and the scalable human-agent joint annotation scheme to efficiently collect the necessary data at scale.
  V-Droid obtains a substantial task success rate across several public mobile task automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on MobileAgentBench, surpassing existing agents by 5.2%, 2.1%, and 9%, respectively. Furthermore, V-Droid achieves a remarkably low latency of 4.3s per step, which is 6.1X faster compared with existing mobile agents. The source code is available at https://github.com/V-Droid-Agent/V-Droid.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Skills: Reasoning about Knowledge and Oblivion</title>
<link>https://arxiv.org/abs/2504.01733</link>
<guid>https://arxiv.org/abs/2504.01733</guid>
<content:encoded><![CDATA[
arXiv:2504.01733v2 Announce Type: replace 
Abstract: This paper presents a class of epistemic logics that captures the dynamics of acquiring knowledge and descending into oblivion, while incorporating concepts of group knowledge. The approach is grounded in a system of weighted models, introducing an ``epistemic skills'' metric to represent the epistemic capacities tied to knowledge updates. Within this framework, knowledge acquisition is modeled as a process of upskilling, whereas oblivion is represented as a consequence of downskilling. The framework further enables exploration of ``knowability'' and ``forgettability,'' defined as the potential to gain knowledge through upskilling and to lapse into oblivion through downskilling, respectively. Additionally, it supports a detailed analysis of the distinctions between epistemic de re and de dicto expressions. The computational complexity of the model checking and satisfiability problems is examined, offering insights into their theoretical foundations and practical implications.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding</title>
<link>https://arxiv.org/abs/2505.06020</link>
<guid>https://arxiv.org/abs/2505.06020</guid>
<content:encoded><![CDATA[
arXiv:2505.06020v2 Announce Type: replace 
Abstract: Understanding visual art requires reasoning across multiple perspectives -- cultural, historical, and stylistic -- beyond mere object recognition. While recent multimodal large language models (MLLMs) perform well on general image captioning, they often fail to capture the nuanced interpretations that fine art demands. We propose ArtRAG, a novel, training-free framework that combines structured knowledge with retrieval-augmented generation (RAG) for multi-perspective artwork explanation. ArtRAG automatically constructs an Art Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing entities such as artists, movements, themes, and historical events into a rich, interpretable graph. At inference time, a multi-granular structured retriever selects semantically and topologically relevant subgraphs to guide generation. This enables MLLMs to produce contextually grounded, culturally informed art descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG outperforms several heavily trained baselines. Human evaluations further confirm that ArtRAG generates coherent, insightful, and culturally enriched interpretations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT</title>
<link>https://arxiv.org/abs/2506.07173</link>
<guid>https://arxiv.org/abs/2506.07173</guid>
<content:encoded><![CDATA[
arXiv:2506.07173v2 Announce Type: replace 
Abstract: The Python Testbed for Federated Learning Algorithms is a simple Python FL framework that is easy to use by ML&amp;AI developers who do not need to be professional programmers and is also amenable to LLMs. In the previous research, generic federated learning algorithms provided by this framework were manually translated into the CSP processes and algorithms' safety and liveness properties were automatically verified by the model checker PAT. In this paper, a simple translation process is introduced wherein the ChatGPT is used to automate the translation of the mentioned federated learning algorithms in Python into the corresponding CSP processes. Within the process, the minimality of the used context is estimated based on the feedback from ChatGPT. The proposed translation process was experimentally validated by successful translation (verified by the model checker PAT) of both generic centralized and decentralized federated learning algorithms.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.14387</link>
<guid>https://arxiv.org/abs/2506.14387</guid>
<content:encoded><![CDATA[
arXiv:2506.14387v2 Announce Type: replace 
Abstract: Existing work on mitigating catastrophic forgetting during large language models (LLMs) fine-tuning for new knowledge instances has primarily focused on preserving performance on previously seen data, while critically overlooking the collapse of essential capabilities instilled through alignment, most notably the model's ability to faithfully express epistemic uncertainty (a property we term 'Ignorance Awareness'). In this work, we formalize the notion of Ignorance Awareness and illustrate that conventional fine-tuning methods can result in substantial activation displacement. This displacement undermines the critical capability of ignorance awareness, leading to undesirable behaviors such as hallucinations. To address this challenge, we introduce SEAT, a simple and principled fine-tuning approach that not only enables the model to effectively acquire new knowledge instances but also preserves its aligned ignorance awareness. SEAT integrates two key components: (1) sparse tuning that constrains activation drift, and (2) a novel entity perturbation method designed to counter knowledge entanglement. Experimental results demonstrate that, across both real-world and synthetic datasets, SEAT significantly outperforms baselines in preserving ignorance awareness while retaining optimal fine-tuning performance, offering a more robust solution for LLM fine-tuning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning</title>
<link>https://arxiv.org/abs/2507.00008</link>
<guid>https://arxiv.org/abs/2507.00008</guid>
<content:encoded><![CDATA[
arXiv:2507.00008v2 Announce Type: replace 
Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses unique challenges due to the diversity of visual elements, spatial clutter, and the ambiguity of language. In this paper, we introduce DiMo-GUI, a training-free framework for GUI grounding that leverages two core strategies: dynamic visual grounding and modality-aware optimization. Instead of treating the GUI as a monolithic image, our method splits the input into textual elements and iconic elements, allowing the model to reason over each modality independently using general-purpose vision-language models. When predictions are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by generating candidate focal regions centered on the model's initial predictions and incrementally zooms into subregions to refine the grounding result. This hierarchical refinement process helps disambiguate visually crowded layouts without the need for additional training or annotations. We evaluate our approach on standard GUI grounding benchmarks and demonstrate consistent improvements over baseline inference pipelines, highlighting the effectiveness of combining modality separation with region-focused reasoning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment</title>
<link>https://arxiv.org/abs/2507.05528</link>
<guid>https://arxiv.org/abs/2507.05528</guid>
<content:encoded><![CDATA[
arXiv:2507.05528v2 Announce Type: replace 
Abstract: Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design</title>
<link>https://arxiv.org/abs/2507.20541</link>
<guid>https://arxiv.org/abs/2507.20541</guid>
<content:encoded><![CDATA[
arXiv:2507.20541v4 Announce Type: replace 
Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that presents a new paradigm for Automatic Heuristic Design (AHD). Traditional evolutionary methods operate directly on heuristic code; in contrast, MeLA evolves the instructional prompts used to guide a Large Language Model (LLM) in generating these heuristics. This process of "prompt evolution" is driven by a novel metacognitive framework where the system analyzes performance feedback to systematically refine its generative strategy. MeLA's architecture integrates a problem analyzer to construct an initial strategic prompt, an error diagnosis system to repair faulty code, and a metacognitive search engine that iteratively optimizes the prompt based on heuristic effectiveness. In comprehensive experiments across both benchmark and real-world problems, MeLA consistently generates more effective and robust heuristics, significantly outperforming state-of-the-art methods. Ultimately, this research demonstrates the profound potential of using cognitive science as a blueprint for AI architecture, revealing that by enabling an LLM to metacognitively regulate its problem-solving process, we unlock a more robust and interpretable path to AHD.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction</title>
<link>https://arxiv.org/abs/2508.11987</link>
<guid>https://arxiv.org/abs/2508.11987</guid>
<content:encoded><![CDATA[
arXiv:2508.11987v3 Announce Type: replace 
Abstract: Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information gathering, contextual understanding, and decision-making under uncertainty. Agents must not only gather and interpret vast amounts of dynamic information but also integrate diverse data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human experts do in fields like politics, economics, and finance. Despite its importance, no large-scale benchmark exists for evaluating agents on future prediction, largely due to challenges in handling real-time updates and retrieving timely, accurate answers. To address this, we introduce $\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically designed for LLM agents performing future prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction, supporting real-time daily updates and eliminating data contamination through an automated pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including those with reasoning, search capabilities, and integration of external tools such as the open-source Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation assesses agents' adaptive reasoning and performance in dynamic environments. Additionally, we provide in-depth analyses of agents' failure modes and performance pitfalls in future-oriented tasks, including the vulnerability to fake web pages and the temporal validity. Our goal is to establish a dynamic, contamination-free evaluation standard that drives the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model</title>
<link>https://arxiv.org/abs/2508.13676</link>
<guid>https://arxiv.org/abs/2508.13676</guid>
<content:encoded><![CDATA[
arXiv:2508.13676v2 Announce Type: replace 
Abstract: To maintain the company's talent pool, recruiters need to continuously search for resumes from third-party websites (e.g., LinkedIn, Indeed). However, fetched resumes are often incomplete and inaccurate. To improve the quality of third-party resumes and enrich the company's talent pool, it is essential to conduct duplication detection between the fetched resumes and those already in the company's talent pool. Such duplication detection is challenging due to the semantic complexity, structural heterogeneity, and information incompleteness of resume texts. To this end, we propose MHSNet, an multi-level identity verification framework that fine-tunes BGE-M3 using contrastive learning. With the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and dense representations for resumes, enabling the computation of corresponding multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts (MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental results verify the effectiveness of MHSNet
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain</title>
<link>https://arxiv.org/abs/2508.16172</link>
<guid>https://arxiv.org/abs/2508.16172</guid>
<content:encoded><![CDATA[
arXiv:2508.16172v2 Announce Type: replace 
Abstract: Understanding human behavior in urban environments is a crucial field within city sciences. However, collecting accurate behavioral data, particularly in newly developed areas, poses significant challenges. Recent advances in generative agents, powered by Large Language Models (LLMs), have shown promise in simulating human behaviors without relying on extensive datasets. Nevertheless, these methods often struggle with generating consistent, context-sensitive, and realistic behavioral outputs. To address these limitations, this paper introduces the Preference Chain, a novel method that integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance context-aware simulation of human behavior in transportation systems. Experiments conducted on the Replica dataset demonstrate that the Preference Chain outperforms standard LLM in aligning with real-world transportation mode choices. The development of the Mobility Agent highlights potential applications of proposed method in urban mobility modeling for emerging cities, personalized travel behavior analysis, and dynamic traffic forecasting. Despite limitations such as slow inference and the risk of hallucination, the method offers a promising framework for simulating complex human behavior in data-scarce environments, where traditional data-driven models struggle due to limited data availability.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.20368</link>
<guid>https://arxiv.org/abs/2508.20368</guid>
<content:encoded><![CDATA[
arXiv:2508.20368v2 Announce Type: replace 
Abstract: Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMs' internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradigm for enhancing LLM reasoning through multi-turn interactions with search engines. However, existing RL-based search agents rely on a single LLM to handle both search planning and question-answering (QA) tasks in an end-to-end manner, which limits their ability to optimize both capabilities simultaneously. In practice, sophisticated AI search systems often employ a large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a more effective and efficient approach is to utilize a small, trainable LLM dedicated to search planning. In this paper, we propose \textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to enhance the performance of frozen QA models by focusing on search planning. Specifically, our approach introduces three key innovations: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to achieve the objectives. Extensive experiments on real-world datasets demonstrate that AI SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, while exhibiting strong generalization capabilities across diverse frozen QA models and data domains.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survival Analysis with Adversarial Regularization</title>
<link>https://arxiv.org/abs/2312.16019</link>
<guid>https://arxiv.org/abs/2312.16019</guid>
<content:encoded><![CDATA[
arXiv:2312.16019v5 Announce Type: replace-cross 
Abstract: Survival Analysis (SA) models the time until an event occurs, with applications in fields like medicine, defense, finance, and aerospace. Recent research indicates that Neural Networks (NNs) can effectively capture complex data patterns in SA, whereas simple generalized linear models often fall short in this regard. However, dataset uncertainties (e.g., noisy measurements, human error) can degrade NN model performance. To address this, we leverage advances in NN verification to develop training objectives for robust, fully-parametric SA models. Specifically, we propose an adversarially robust loss function based on a Min-Max optimization problem. We employ CROWN-Interval Bound Propagation (CROWN-IBP) to tackle the computational challenges inherent in solving this Min-Max problem. Evaluated over 10 SurvSet datasets, our method, Survival Analysis with Adversarial Regularization (SAWAR), consistently outperforms baseline adversarial training methods and state-of-the-art (SOTA) deep SA models across various covariate perturbations with respect to Negative Log Likelihood (NegLL), Integrated Brier Score (IBS), and Concordance Index (CI) metrics. Thus, we demonstrate that adversarial robustness enhances SA predictive performance and calibration, mitigating data uncertainty and improving generalization across diverse datasets by up to 150% compared to baselines.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Chains, Trees, and Graphs of Thoughts</title>
<link>https://arxiv.org/abs/2401.14295</link>
<guid>https://arxiv.org/abs/2401.14295</guid>
<content:encoded><![CDATA[
arXiv:2401.14295v5 Announce Type: replace-cross 
Abstract: The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</title>
<link>https://arxiv.org/abs/2402.12226</link>
<guid>https://arxiv.org/abs/2402.12226</guid>
<content:encoded><![CDATA[
arXiv:2402.12226v4 Announce Type: replace-cross 
Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks</title>
<link>https://arxiv.org/abs/2405.15164</link>
<guid>https://arxiv.org/abs/2405.15164</guid>
<content:encoded><![CDATA[
arXiv:2405.15164v2 Announce Type: replace-cross 
Abstract: Compositionality has long been considered a key explanatory property underlying human intelligence: arbitrary concepts can be composed into novel complex combinations, permitting the acquisition of an open ended, potentially infinite expressive capacity from finite learning experiences. Influential arguments have held that neural networks fail to explain this aspect of behavior, leading many to dismiss them as viable models of human cognition. Over the last decade, however, modern deep neural networks (DNNs), which share the same fundamental design principles as their predecessors, have come to dominate artificial intelligence, exhibiting the most advanced cognitive behaviors ever demonstrated in machines. In particular, large language models (LLMs), DNNs trained to predict the next word on a large corpus of text, have proven capable of sophisticated behaviors such as writing syntactically complex sentences without grammatical errors, producing cogent chains of reasoning, and even writing original computer programs -- all behaviors thought to require compositional processing. In this chapter, we survey recent empirical work from machine learning for a broad audience in philosophy, cognitive science, and neuroscience, situating recent breakthroughs within the broader context of philosophical arguments about compositionality. In particular, our review emphasizes two approaches to endowing neural networks with compositional generalization capabilities: (1) architectural inductive biases, and (2) metalearning, or learning to learn. We also present findings suggesting that LLM pretraining can be understood as a kind of metalearning, and can thereby equip DNNs with compositional generalization abilities in a similar way. We conclude by discussing the implications that these findings may have for the study of compositionality in human cognition and by suggesting avenues for future research.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFaults: Model-Based Diagnosis for Fault Localization in C Programs with Multiple Test Cases</title>
<link>https://arxiv.org/abs/2407.09337</link>
<guid>https://arxiv.org/abs/2407.09337</guid>
<content:encoded><![CDATA[
arXiv:2407.09337v2 Announce Type: replace-cross 
Abstract: Debugging is one of the most time-consuming and expensive tasks in software development. Several formula-based fault localization (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs with multiple faults.
  This paper introduces a novel fault localization approach for C programs with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified MaxSAT formula. Consequently, our method guarantees consistency across observations and simplifies the fault localization procedure. Experimental results on two benchmark sets of C programs, TCAS and C-Pack-IPAs, show that CFaults is faster than other FBFL approaches like BugAssist and SNIPER. Moreover, CFaults only generates subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaGym: Evaluating Persona Agents and LLMs</title>
<link>https://arxiv.org/abs/2407.18416</link>
<guid>https://arxiv.org/abs/2407.18416</guid>
<content:encoded><![CDATA[
arXiv:2407.18416v5 Announce Type: replace-cross 
Abstract: Persona agents, which are LLM agents conditioned to act according to an assigned persona, enable contextually rich and user aligned interactions across domains like education and healthcare. However, evaluating how faithfully these agents adhere to their personas remains a significant challenge, particularly in free-form settings that demand consistency across diverse, persona-relevant environments. We introduce PersonaGym, the first dynamic evaluation framework for persona agents, and PersonaScore, a human-aligned automatic metric grounded in decision theory that enables comprehensive large-scale evaluation. Our evaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals significant advancement opportunities. For example, GPT-4.1 had the exact same PersonaScore as LLaMA-3-8b despite being a more recent and advanced closed source model. Importantly, increased model size and complexity do not necessarily enhance persona agent capabilities, underscoring the need for algorithmic and architectural innovation toward faithful, performant persona agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation</title>
<link>https://arxiv.org/abs/2407.19835</link>
<guid>https://arxiv.org/abs/2407.19835</guid>
<content:encoded><![CDATA[
arXiv:2407.19835v2 Announce Type: replace-cross 
Abstract: Classical Arabic represents a significant era that encompasses the golden age of Arab culture, philosophy, and scientific literature. With a broad consensus on the importance of translating these literatures to enrich knowledge dissemination across communities, the advent of large language models (LLMs) and translation systems offers promising tools to facilitate this goal. However, we have identified a scarcity of translation datasets in Classical Arabic, which are often limited in scope and topics, hindering the development of high-quality translation systems. In response, we present the ATHAR dataset, which comprises 66,000 high-quality classical Arabic to English translation samples that cover a wide array of topics including science, culture, and philosophy. Furthermore, we assess the performance of current state-of-the-art LLMs under various settings, concluding that there is a need for such datasets in current systems. Our findings highlight how models can benefit from fine-tuning or incorporating this dataset into their pretraining pipelines. The dataset is publicly available on the HuggingFace Data Hub: https://huggingface.co/datasets/mohamed-khalil/ATHAR.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Preference Optimization via Token-Level Reward Function Estimation</title>
<link>https://arxiv.org/abs/2408.13518</link>
<guid>https://arxiv.org/abs/2408.13518</guid>
<content:encoded><![CDATA[
arXiv:2408.13518v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language model alignment leverage token-level supervisions to perform fine-grained preference optimization. However, existing token-level alignment methods either optimize on all available tokens, which can be noisy and inefficient, or perform selective training with complex and expensive key token selection strategies. In this work, we propose Selective Preference Optimization (SePO), a novel selective alignment strategy that centers on efficient key token selection. SePO proposes the first token selection method based on Direct Preference Optimization (DPO), which trains an oracle model to estimate a token-level reward function on the target data. This method applies to any existing alignment datasets with response-level annotations and enables cost-efficient token selection with small-scale oracle models and training data. The estimated reward function is then utilized to score all tokens within the target dataset, where only the key tokens are selected to supervise the target policy model with a reference model-free contrastive objective function. Extensive experiments on three public evaluation benchmarks show that SePO significantly outperforms competitive baseline methods by only optimizing 30% key tokens on the target dataset. SePO applications on weak-to-strong generalization show that weak oracle models effectively supervise strong policy models with up to 16.8x more parameters. SePO also effectively selects key tokens from out-of-distribution data to enhance strong policy models and alleviate the over-optimization problem.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated detection of underdiagnosed medical conditions via opportunistic imaging</title>
<link>https://arxiv.org/abs/2409.11686</link>
<guid>https://arxiv.org/abs/2409.11686</guid>
<content:encoded><![CDATA[
arXiv:2409.11686v4 Announce Type: replace-cross 
Abstract: Abdominal computed tomography (CT) scans are frequently performed in clinical settings. Opportunistic CT involves repurposing routine CT images to extract diagnostic information and is an emerging tool for detecting underdiagnosed conditions such as sarcopenia, hepatic steatosis, and ascites. This study utilizes deep learning methods to promote accurate diagnosis and clinical documentation. We analyze 2,674 inpatient CT scans to identify discrepancies between imaging phenotypes (characteristics derived from opportunistic CT scans) and their corresponding documentation in radiology reports and ICD coding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans diagnosed with sarcopenia, hepatic steatosis, and ascites (respectively) through either opportunistic imaging or radiology reports were ICD-coded. Our findings demonstrate opportunistic CT's potential to enhance diagnostic precision and accuracy of risk adjustment models, offering advancements in precision medicine.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale</title>
<link>https://arxiv.org/abs/2409.16299</link>
<guid>https://arxiv.org/abs/2409.16299</guid>
<content:encoded><![CDATA[
arXiv:2409.16299v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have revolutionized software engineering (SE), showcasing remarkable proficiency in various coding tasks. Despite recent advancements that have enabled the creation of autonomous software agents utilizing LLMs for end-to-end development tasks, these systems are typically designed for specific SE functions. We introduce HyperAgent, an innovative generalist multi-agent system designed to tackle a wide range of SE tasks across different programming languages by mimicking the workflows of human developers. HyperAgent features four specialized agents-Planner, Navigator, Code Editor, and Executor-capable of handling the entire lifecycle of SE tasks, from initial planning to final verification. HyperAgent sets new benchmarks in diverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench benchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates exceptional performance in repository-level code generation (RepoExec) and fault localization and program repair (Defects4J), often surpassing state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatically Detecting Online Deceptive Patterns</title>
<link>https://arxiv.org/abs/2411.07441</link>
<guid>https://arxiv.org/abs/2411.07441</guid>
<content:encoded><![CDATA[
arXiv:2411.07441v2 Announce Type: replace-cross 
Abstract: Deceptive patterns (DPs) in digital interfaces manipulate users into making unintended decisions, exploiting cognitive biases and psychological vulnerabilities. These patterns have become ubiquitous across various digital platforms. While efforts to mitigate DPs have emerged from legal and technical perspectives, a significant gap in usable solutions that empower users to identify and make informed decisions about DPs in real-time remains. In this work, we introduce AutoBot, an automated, deceptive pattern detector that analyzes websites' visual appearances using machine learning techniques to identify and notify users of DPs in real-time. AutoBot employs a two-staged pipeline that processes website screenshots, identifying interactable elements and extracting textual features without relying on HTML structure. By leveraging a custom language model, AutoBot understands the context surrounding these elements to determine the presence of deceptive patterns. We implement AutoBot as a lightweight Chrome browser extension that performs all analyses locally, minimizing latency and preserving user privacy. Through extensive evaluation, we demonstrate AutoBot's effectiveness in enhancing users' ability to navigate digital environments safely while providing a valuable tool for regulators to assess and enforce compliance with DP regulations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Information Security Awareness of Large Language Models</title>
<link>https://arxiv.org/abs/2411.13207</link>
<guid>https://arxiv.org/abs/2411.13207</guid>
<content:encoded><![CDATA[
arXiv:2411.13207v2 Announce Type: replace-cross 
Abstract: The popularity of large language models (LLMs) continues to grow, and LLM-based assistants have become ubiquitous. Information security awareness (ISA) is an important yet underexplored safety aspect of LLMs. ISA encompasses LLMs' security knowledge, which has been explored in the past, as well as attitudes and behaviors, which are crucial to LLMs' ability to understand implicit security context and reject unsafe requests that may cause the LLM to fail the user. We present an automated method for measuring the ISA of LLMs, which covers all 30 security topics in a mobile ISA taxonomy, using realistic scenarios that create tension between implicit security implications and user satisfaction. Applying this method to leading LLMs, we find that most of the popular models exhibit only medium to low levels of ISA, exposing their users to cybersecurity threats. Smaller variants of the same model family are significantly riskier, while newer versions show no consistent ISA improvement, suggesting that providers are not actively working toward mitigating this issue. These results reveal a widespread vulnerability affecting current LLM deployments: the majority of popular models, and particularly their smaller variants, may systematically endanger users. We propose a practical mitigation: incorporating our security awareness instruction into model system prompts to help LLMs better detect and reject unsafe requests.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spoof Trace Discovery for Deep Learning Based Explainable Face Anti-Spoofing</title>
<link>https://arxiv.org/abs/2412.17541</link>
<guid>https://arxiv.org/abs/2412.17541</guid>
<content:encoded><![CDATA[
arXiv:2412.17541v4 Announce Type: replace-cross 
Abstract: With the rapid growth usage of face recognition in people's daily life, face anti-spoofing becomes increasingly important to avoid malicious attacks. Recent face anti-spoofing models can reach a high classification accuracy on multiple datasets but these models can only tell people "this face is fake" while lacking the explanation to answer "why it is fake". Such a system undermines trustworthiness and causes user confusion, as it denies their requests without providing any explanations. In this paper, we incorporate XAI into face anti-spoofing and propose a new problem termed X-FAS (eXplainable Face Anti-Spoofing) empowering face anti-spoofing models to provide an explanation. We propose SPTD (SPoof Trace Discovery), an X-FAS method which can discover spoof concepts and provide reliable explanations on the basis of discovered concepts. To evaluate the quality of X-FAS methods, we propose an X-FAS benchmark with annotated spoof traces by experts. We analyze SPTD explanations on face anti-spoofing dataset and compare SPTD quantitatively and qualitatively with previous XAI methods on proposed X-FAS benchmark. Experimental results demonstrate SPTD's ability to generate reliable explanations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2501.03544</link>
<guid>https://arxiv.org/abs/2501.03544</guid>
<content:encoded><![CDATA[
arXiv:2501.03544v3 Announce Type: replace-cross 
Abstract: Recent text-to-image (T2I) models have exhibited remarkable performance in generating high-quality images from text descriptions. However, these models are vulnerable to misuse, particularly generating not-safe-for-work (NSFW) content, such as sexually explicit, violent, political, and disturbing images, raising serious ethical concerns. In this work, we present PromptGuard, a novel content moderation technique that draws inspiration from the system prompt mechanism in large language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack a direct interface for enforcing behavioral guidelines. Our key idea is to optimize a safety soft prompt that functions as an implicit system prompt within the T2I model's textual embedding space. This universal soft prompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image generation without altering the inference efficiency or requiring proxy models. We further enhance its reliability and helpfulness through a divide-and-conquer strategy, which optimizes category-specific soft prompts and combines them into holistic safety guidance. Extensive experiments across five datasets demonstrate that PromptGuard effectively mitigates NSFW content generation while preserving high-quality benign outputs. PromptGuard achieves 3.8 times faster than prior content moderation methods, surpassing eight state-of-the-art defenses with an optimal unsafe ratio down to 5.84%.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing higher-order neural representations of uncertainty with the Noise Estimation through Reinforcement-based Diffusion (NERD) model</title>
<link>https://arxiv.org/abs/2503.14333</link>
<guid>https://arxiv.org/abs/2503.14333</guid>
<content:encoded><![CDATA[
arXiv:2503.14333v3 Announce Type: replace-cross 
Abstract: Studies often aim to reveal ``first-order" representations (FORs), which encode aspects of an observer's environment, such as contents or structure. A less-common target is ``higher-order" representations (HORs), which are ``about" FORs -- e.g., their strength or uncertainty -- and which may contribute to learning. HORs about uncertainty are unlikely to be direct ``read-outs" of FOR characteristics, instead reflecting noisy estimation processes incorporating prior expectations about uncertainty, but how the brain represents such expected uncertainty distributions remains largely unexplored. Here, we study ``noise expectation" HORs using neural data from a task which may require the brain to learn about its own noise: decoded neurofeedback, wherein human subjects learn to volitionally produce target neural patterns. We develop and apply a Noise Estimation through Reinforcement-based Diffusion (NERD) model to characterize how brains may undertake this process, and show that NERD offers high explanatory power for human behavior.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RailGoerl24: G\"orlitz Rail Test Center CV Dataset 2024</title>
<link>https://arxiv.org/abs/2504.00204</link>
<guid>https://arxiv.org/abs/2504.00204</guid>
<content:encoded><![CDATA[
arXiv:2504.00204v2 Announce Type: replace-cross 
Abstract: Driverless train operation for open tracks on urban guided transport and mainline railways requires, among other things automatic detection of actual and potential obstacles, especially humans, in the danger zone of the train's path. Machine learning algorithms have proven to be powerful state-of-the-art tools for this task. However, these algorithms require large amounts of high-quality annotated data containing human beings in railway-specific environments as training data. Unfortunately, the amount of publicly available datasets is not yet sufficient and is significantly inferior to the datasets in the road domain. Therefore, this paper presents RailGoerl24, an on-board visual light Full HD camera dataset of 12205 frames recorded in a railway test center of T\"UV S\"UD Rail, in G\"orlitz, Germany. Its main purpose is to support the development of driverless train operation for guided transport. RailGoerl24 also includes a terrestrial LiDAR scan covering parts of the area used to acquire the RGB data. In addition to the raw data, the dataset contains 33556 boxwise annotations in total for the object class 'person'. The faces of recorded actors are not blurred or altered in any other way. RailGoerl24, available at data.fid-move.de/dataset/railgoerl24, can also be used for tasks beyond collision prediction.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPDL: Automatic Prompt Optimization for LLM Agents</title>
<link>https://arxiv.org/abs/2504.04365</link>
<guid>https://arxiv.org/abs/2504.04365</guid>
<content:encoded><![CDATA[
arXiv:2504.04365v3 Announce Type: replace-cross 
Abstract: The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning</title>
<link>https://arxiv.org/abs/2505.10264</link>
<guid>https://arxiv.org/abs/2505.10264</guid>
<content:encoded><![CDATA[
arXiv:2505.10264v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) enables collaborative training of machine learning models across distributed clients without sharing raw data, ostensibly preserving data privacy. Nevertheless, recent studies have revealed critical vulnerabilities in FL, showing that a malicious central server can manipulate model updates to reconstruct clients' private training data. Existing data reconstruction attacks have important limitations: they often rely on assumptions about the clients' data distribution or their efficiency significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes these limitations. Our method leverages a new geometric perspective on fully connected layers to craft malicious model parameters, enabling the perfect recovery of arbitrarily large data batches in classification tasks without any prior knowledge of clients' data. Through extensive experiments on both image and tabular datasets, we demonstrate that our attack outperforms existing methods and achieves perfect reconstruction of data batches two orders of magnitude larger than the state of the art.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokUR: Token-Level Uncertainty Estimation for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.11737</link>
<guid>https://arxiv.org/abs/2505.11737</guid>
<content:encoded><![CDATA[
arXiv:2505.11737v2 Announce Type: replace-cross 
Abstract: While Large Language Models (LLMs) have demonstrated impressive capabilities, their output quality remains inconsistent across various application scenarios, making it difficult to identify trustworthy responses, especially in complex tasks requiring multi-step reasoning. In this paper, we propose a Token-level Uncertainty estimation framework for Reasoning (TokUR) to enable LLMs to self-assess and self-improve their generation quality in mathematical reasoning. Specifically, we introduce low-rank random weight perturbation to LLM decoding, generating predictive distributions that we use to estimate token-level uncertainties. We then aggregate these uncertainties to reflect semantic uncertainty of the generated sequences. Experiments on mathematical reasoning datasets of varying difficulty demonstrate that our token-level uncertainty metrics strongly correlate with answer correctness and model robustness. Additionally, we explore using uncertainty to directly enhance the model's reasoning performance through multiple generations and the particle filtering algorithm. Our approach consistently outperforms existing uncertainty estimation methods, establishing effective uncertainty estimation as a valuable tool for both evaluating and improving reasoning generation in LLMs.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &amp; Dragons Gameplay</title>
<link>https://arxiv.org/abs/2505.22809</link>
<guid>https://arxiv.org/abs/2505.22809</guid>
<content:encoded><![CDATA[
arXiv:2505.22809v2 Announce Type: replace-cross 
Abstract: Much work has been done on conversational LLM agents which directly assist human users with tasks. We present an alternative paradigm for interacting with LLM agents, which we call "overhearing agents". These overhearing agents do not actively participate in conversation -- instead, they "listen in" on human-to-human conversations and perform background tasks or provide suggestions to assist the user. In this work, we explore the overhearing agents paradigm through the lens of Dungeons & Dragons gameplay. We present an in-depth study using large multimodal audio-language models as overhearing agents to assist a Dungeon Master. We perform a human evaluation to examine the helpfulness of such agents and find that some large audio-language models have the emergent ability to perform overhearing agent tasks using implicit audio cues. Finally, we release Python libraries and our project code to support further research into the overhearing agents paradigm at https://github.com/zhudotexe/overhearing_agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Rapid Crystal Structure Generation Towards a Target Local Environment</title>
<link>https://arxiv.org/abs/2506.08224</link>
<guid>https://arxiv.org/abs/2506.08224</guid>
<content:encoded><![CDATA[
arXiv:2506.08224v2 Announce Type: replace-cross 
Abstract: In the field of material design, traditional crystal structure prediction approaches require extensive structural sampling through computationally expensive energy minimization methods using either force fields or quantum mechanical simulations. While emerging artificial intelligence (AI) generative models have shown great promise in generating realistic crystal structures more rapidly, most existing models fail to account for the unique symmetries and periodicity of crystalline materials, and they are limited to handling structures with only a few tens of atoms per unit cell. Here, we present a symmetry-informed AI generative approach called Local Environment Geometry-Oriented Crystal Generator (LEGO-xtal) that overcomes these limitations. Our method generates initial structures using AI models trained on an augmented small dataset, and then optimizes them using machine learning structure descriptors rather than traditional energy-based optimization. We demonstrate the effectiveness of LEGO-xtal by expanding from 25 known low-energy sp2 carbon allotropes to over 1,700, all within 0.5 eV/atom of the ground-state energy of graphite. This framework offers a generalizable strategy for the targeted design of materials with modular building blocks, such as metal-organic frameworks and next-generation battery materials.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Features at Convergence Theorem: a first-principles alternative to the Neural Feature Ansatz for how networks learn representations</title>
<link>https://arxiv.org/abs/2507.05644</link>
<guid>https://arxiv.org/abs/2507.05644</guid>
<content:encoded><![CDATA[
arXiv:2507.05644v2 Announce Type: replace-cross 
Abstract: It is a central challenge in deep learning to understand how neural networks learn representations. A leading approach is the Neural Feature Ansatz (NFA) (Radhakrishnan et al. 2024), a conjectured mechanism for how feature learning occurs. Although the NFA is empirically validated, it is an educated guess and lacks a theoretical basis, and thus it is unclear when it might fail, and how to improve it. In this paper, we take a first-principles approach to understanding why this observation holds, and when it does not. We use first-order optimality conditions to derive the Features at Convergence Theorem (FACT), an alternative to the NFA that (a) obtains greater agreement with learned features at convergence, (b) explains why the NFA holds in most settings, and (c) captures essential feature learning phenomena in neural networks such as grokking behavior in modular arithmetic and phase transitions in learning sparse parities, similarly to the NFA. Thus, our results unify theoretical first-order optimality analyses of neural networks with the empirically-driven NFA literature, and provide a principled alternative that provably and empirically holds at convergence.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework</title>
<link>https://arxiv.org/abs/2507.05814</link>
<guid>https://arxiv.org/abs/2507.05814</guid>
<content:encoded><![CDATA[
arXiv:2507.05814v3 Announce Type: replace-cross 
Abstract: As critical transportation infrastructure, bridges face escalating challenges from aging and deterioration, while traditional manual inspection methods suffer from low efficiency. Although 3D point cloud technology provides a new data-driven paradigm, its application potential is often constrained by the incompleteness of real-world data, which results from missing labels and scanning occlusions. To overcome the bottleneck of insufficient generalization in existing synthetic data methods, this paper proposes a systematic framework for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring component-level instance annotations, high-fidelity color, and precise normal vectors. It can be further extended to simulate the creation of diverse and physically realistic incomplete point clouds, designed to support the training of segmentation and completion networks, respectively. Experiments demonstrate that a PointNet++ model trained with our synthetic data achieves a mean Intersection over Union (mIoU) of 84.2% in real-world bridge semantic segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance on the component completion task.
  This research offers an innovative methodology and a foundational dataset for the 3D visual analysis of bridge structures, holding significant implications for advancing the automated management and maintenance of infrastructure.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry with Neural Signed Distance Fields</title>
<link>https://arxiv.org/abs/2507.06269</link>
<guid>https://arxiv.org/abs/2507.06269</guid>
<content:encoded><![CDATA[
arXiv:2507.06269v3 Announce Type: replace-cross 
Abstract: Accurate surface estimation is critical for downstream tasks in scientific simulation, and quantifying uncertainty in implicit neural 3D representations still remains a substantial challenge due to computational inefficiencies, scalability issues, and geometric inconsistencies. However, current neural implicit surface models do not offer a principled way to quantify uncertainty, limiting their reliability in real-world applications. Inspired by recent probabilistic rendering approaches, we introduce BayesSDF, a novel probabilistic framework for uncertainty estimation in neural implicit 3D representations. Unlike radiance-based models such as Neural Radiance Fields (NeRF) or 3D Gaussian Splatting, Signed Distance Functions (SDFs) provide continuous, differentiable surface representations, making them especially well-suited for uncertainty-aware modeling. BayesSDF applies a Laplace approximation over SDF weights and derives Hessian-based metrics to estimate local geometric instability. We empirically demonstrate that these uncertainty estimates correlate strongly with surface reconstruction error across both synthetic and real-world benchmarks. By enabling surface-aware uncertainty quantification, BayesSDF lays the groundwork for more robust, interpretable, and actionable 3D perception systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2507.07236</link>
<guid>https://arxiv.org/abs/2507.07236</guid>
<content:encoded><![CDATA[
arXiv:2507.07236v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often behave inconsistently across inputs, indicating uncertainty and motivating the need for its quantification in high-stakes settings. Prior work on calibration and uncertainty quantification often focuses on individual models, overlooking the potential of model diversity. We hypothesize that LLMs make complementary predictions due to differences in training and the Zipfian nature of language, and that aggregating their outputs leads to more reliable uncertainty estimates. To leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a simple information-theoretic method that uses Jensen-Shannon Divergence to identify and aggregate well-calibrated subsets of LLMs. Experiments on binary prediction tasks demonstrate improved calibration and predictive performance compared to single-model and na\"ive ensemble baselines. In addition, we explore using MUSE as guided signals with chain-of-thought distillation to fine-tune LLMs for calibration. MUSE is available at:https://github.com/LARK-NLP-Lab/MUSE.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database</title>
<link>https://arxiv.org/abs/2507.13802</link>
<guid>https://arxiv.org/abs/2507.13802</guid>
<content:encoded><![CDATA[
arXiv:2507.13802v2 Announce Type: replace-cross 
Abstract: In the European Union, official food safety monitoring data collected by member states are submitted to the European Food Safety Authority (EFSA) and published on Zenodo. This data includes 392 million analytical results derived from over 15.2 million samples covering more than 4,000 different types of food products, offering great opportunities for artificial intelligence to analyze trends, predict hazards, and support early warning systems. However, the current format with data distributed across approximately 1000 files totaling several hundred gigabytes hinders accessibility and analysis. To address this, we introduce the CompreHensive European Food Safety (CHEFS) database, which consolidates EFSA monitoring data on pesticide residues, veterinary medicinal product residues, and chemical contaminants into a unified and structured dataset. We describe the creation and structure of the CHEFS database and demonstrate its potential by analyzing trends in European food safety monitoring data from 2000 to 2024. Our analyses explore changes in monitoring activities, the most frequently tested products, which products were most often non-compliant and which contaminants were most often found, and differences across countries. These findings highlight the CHEFS database as both a centralized data source and a strategic tool for guiding food safety policy, research, and regulation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HuggingGraph: Understanding the Supply Chain of LLM Ecosystem</title>
<link>https://arxiv.org/abs/2507.14240</link>
<guid>https://arxiv.org/abs/2507.14240</guid>
<content:encoded><![CDATA[
arXiv:2507.14240v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) leverage deep learning architectures to process and predict sequences of words, enabling them to perform a wide range of natural language processing tasks, such as translation, summarization, question answering, and content generation. As existing LLMs are often built from base models or other pre-trained models and use external datasets, they can inevitably inherit vulnerabilities, biases, or malicious components that exist in previous models or datasets. Therefore, it is critical to understand these components' origin and development process to detect potential risks, improve model fairness, and ensure compliance with regulatory frameworks. Motivated by that, this project aims to study such relationships between models and datasets, which are the central parts of the LLM supply chain. First, we design a methodology to systematically collect LLMs' supply chain information. Then, we design a new graph to model the relationships between models and datasets, which is a directed heterogeneous graph, having 402,654 nodes and 462,524 edges. Lastly, we perform different types of analysis and make multiple interesting findings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title>
<link>https://arxiv.org/abs/2508.01249</link>
<guid>https://arxiv.org/abs/2508.01249</guid>
<content:encoded><![CDATA[
arXiv:2508.01249v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's runtime traces as graph-based intermediate representations with control and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools \& data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis for sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can reduce the ASR to 3\%, with the utility drop only 1\%.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in LM-Based TTS Models via Distribution Alignment Using GFlowNets</title>
<link>https://arxiv.org/abs/2508.15442</link>
<guid>https://arxiv.org/abs/2508.15442</guid>
<content:encoded><![CDATA[
arXiv:2508.15442v3 Announce Type: replace-cross 
Abstract: Language Model (LM)-based Text-to-Speech (TTS) systems often generate hallucinated speech that deviates from input text. Existing mitigation strategies either demand excessive training resources or introduce significant inference latency. In this paper, we propose GFlOwNet-guided distribution AlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates hallucinations without relying on massive resources or inference cost. Specifically, we first conduct an uncertainty analysis, revealing a strong positive correlation between hallucination and model uncertainty. Based on this, we reformulate TTS generation as a trajectory flow optimization problem and introduce an enhanced Subtrajectory Balance objective together with a sharpened internal reward as target distribution. We further integrate reward temperature decay and learning rate optimization for stability and performance balance. Extensive experiments show that GOAT reduce over 50% character error rates on challenging test cases and lowering uncertainty by up to 58%, demonstrating its strong generalization ability and effectiveness.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skill-Aligned Fairness in Multi-Agent Learning for Collaboration in Healthcare</title>
<link>https://arxiv.org/abs/2508.18708</link>
<guid>https://arxiv.org/abs/2508.18708</guid>
<content:encoded><![CDATA[
arXiv:2508.18708v2 Announce Type: replace-cross 
Abstract: Fairness in multi-agent reinforcement learning (MARL) is often framed as a workload balance problem, overlooking agent expertise and the structured coordination required in real-world domains. In healthcare, equitable task allocation requires workload balance or expertise alignment to prevent burnout and overuse of highly skilled agents. Workload balance refers to distributing an approximately equal number of subtasks or equalised effort across healthcare workers, regardless of their expertise. We make two contributions to address this problem. First, we propose FairSkillMARL, a framework that defines fairness as the dual objective of workload balance and skill-task alignment. Second, we introduce MARLHospital, a customizable healthcare-inspired environment for modeling team compositions and energy-constrained scheduling impacts on fairness, as no existing simulators are well-suited for this problem. We conducted experiments to compare FairSkillMARL in conjunction with four standard MARL methods, and against two state-of-the-art fairness metrics. Our results suggest that fairness based solely on equal workload might lead to task-skill mismatches and highlight the need for more robust metrics that capture skill-task misalignment. Our work provides tools and a foundation for studying fairness in heterogeneous multi-agent systems where aligning effort with expertise is critical.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees</title>
<link>https://arxiv.org/abs/2508.21001</link>
<guid>https://arxiv.org/abs/2508.21001</guid>
<content:encoded><![CDATA[
arXiv:2508.21001v2 Announce Type: replace-cross 
Abstract: Kinodynamic motion planning is concerned with computing collision-free trajectories while abiding by the robot's dynamic constraints. This critical problem is often tackled using sampling-based planners (SBPs) that explore the robot's high-dimensional state space by constructing a search tree via action propagations. Although SBPs can offer global guarantees on completeness and solution quality, their performance is often hindered by slow exploration due to uninformed action sampling. Learning-based approaches can yield significantly faster runtimes, yet they fail to generalize to out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety, thus limiting their deployment on physical robots. We present Diffusion Tree (DiTree): a provably-generalizable framework leveraging diffusion policies (DPs) as informed samplers to efficiently guide state-space search within SBPs. DiTree combines DP's ability to model complex distributions of expert trajectories, conditioned on local observations, with the completeness of SBPs to yield provably-safe solutions within a few action propagation iterations for complex dynamical systems. We demonstrate DiTree's power with an implementation combining the popular RRT planner with a DP action sampler trained on a single environment. In comprehensive evaluations on OOD scenarios, DiTree achieves on average a 30% higher success rate compared to standalone DP or SBPs, on a dynamic car and Mujoco's ant robot settings (for the latter, SBPs fail completely). Beyond simulation, real-world car experiments confirm DiTree's applicability, demonstrating superior trajectory quality and robustness even under severe sim-to-real gaps. Project webpage: https://sites.google.com/view/ditree.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management</title>
<link>https://arxiv.org/abs/2508.21433</link>
<guid>https://arxiv.org/abs/2508.21433</guid>
<content:encoded><![CDATA[
arXiv:2508.21433v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative reasoning, exploration, and tool-use, a process that can result in long, expensive context histories. While state-of-the-art Software Engineering ( SE) agents like OpenHands or Cursor use LLM-based summarization to tackle this issue, it is unclear whether the increased complexity offers tangible performance benefits compared to simply omitting older observations. We present a systematic comparison of these strategies within SWE-agent on SWE-bench Verified across five diverse model configurations. We find that a simple observation-masking strategy halves cost relative to a raw agent while matching, and sometimes slightly exceeding, the solve rate of LLM summarization. For example, with Qwen3-Coder 480B, masking improves solve rate from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization at a lower cost. These results suggest that, at least within SWE-agent on SWE-bench Verified, the most effective and efficient context management can be the simplest. We release code and data for reproducibility
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Artificial Intelligence Reshaping the Landscape of the International Academic Community of Geosciences?</title>
<link>https://arxiv.org/abs/2508.20117</link>
<guid>https://arxiv.org/abs/2508.20117</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, geosciences research, bibliometric analysis, developing countries, international collaboration

Summary:<br /><br />
Through bibliometric analysis and topic modeling, this study highlights the positive impact of artificial intelligence (AI) on geosciences research. The research shows a significant increase in AI-related scientific output in recent years, indicating the transformative role of AI in the field. Of particular interest is the increased visibility of earth scientists from developing countries in the AI for Science paradigm, showcasing a more inclusive research landscape. Additionally, AI is enhancing international collaboration in geoscience-related research, fostering a more connected global research community. This comprehensive analysis underscores the importance of AI in advancing knowledge and innovation in geosciences, while also promoting diversity and collaboration among researchers worldwide. <div>
arXiv:2508.20117v2 Announce Type: replace-cross 
Abstract: Through bibliometric analysis and topic modeling, we find that artificial intelligence (AI) is positively transforming geosciences research, with a notable increase in AI-related scientific output in recent years. We are encouraged to observe that earth scientists from developing countries have gained better visibility in the recent AI for Science (AI4S) paradigm and that AI is also improving the landscape of international collaboration in geoscience-related research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beacon: Post-Training Quantization with Integrated Grid Selection</title>
<link>https://arxiv.org/abs/2508.20293</link>
<guid>https://arxiv.org/abs/2508.20293</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantization, compression, post-training quantization, scaling factors, efficient model deployment

Summary:
Beacon introduces a novel approach to per-channel post-training quantization (PTQ) that eliminates the need for manual tuning of scaling factors. By utilizing an unscaled grid and leveraging the geometry of scalar quantization, Beacon automates the selection of optimal scaling factors without relying on back-propagation or large calibration sets. This simplifies the quantization process and reduces the burden of manual intervention in model compression. Despite its simplicity, Beacon delivers competitive performance compared to existing methods, making it a practical solution for efficient model deployment. The algorithm's ability to determine optimal scaling factors on the fly enhances the scalability and usability of PTQ techniques. By addressing the key challenge of scaling factor selection, Beacon simplifies the quantization process and streamlines the deployment of large pre-trained models in memory-constrained environments. Overall, Beacon offers a promising solution for achieving efficient model compression without the need for heuristics or manual intervention. 

<br /><br />Summary: 
Keywords: Quantization, compression, post-training quantization, scaling factors, efficient model deployment 
 <div>
arXiv:2508.20293v2 Announce Type: replace-cross 
Abstract: Quantization is a widely used compression technique for reducing the memory and computation costs of large pre-trained models. A key challenge in per-channel post-training quantization (PTQ) is selecting appropriate scaling factors to replace weight values with values from a scaled integer grid. Existing methods typically fix the scale at the outset via heuristic tuning or grid search. We propose Beacon, a simple and effective algorithm that eliminates the need for such manual tuning. Beacon performs per-channel PTQ directly using an unscaled grid and automatically determines the optimal scaling factors by exploiting the geometry of scalar quantization. It does not rely on back-propagation or large calibration sets. Despite its simplicity and tuning-free nature, Beacon achieves competitive performance compared to state-of-the-art methods, making it a practical solution for efficient model deployment.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PG-Agent: An Agent Powered by Page Graph</title>
<link>https://arxiv.org/abs/2509.03536</link>
<guid>https://arxiv.org/abs/2509.03536</guid>
<content:encoded><![CDATA[
<div> Keywords: Graphical User Interface, GUI agents, Multimodal large language models, Page graphs, Retrieval-Augmented Generation

Summary:
Graphical User Interface (GUI) agents powered by advanced multimodal large language models (MLLMs) have shown promise, but current agents struggle to capture the complex transition relationships between pages. This paper introduces an automated pipeline to transform sequential episodes into page graphs, which explicitly model the graph structure of connected pages. By using Retrieval-Augmented Generation (RAG) technology, reliable perception guidelines for GUI can be retrieved from these page graphs. A tailored multi-agent framework, PG-Agent, with a task decomposition strategy is then proposed to generalize to unseen scenarios. Extensive experiments on various benchmarks demonstrate the effectiveness of PG-Agent, even with limited episodes for page graph construction.<br /><br />Summary: Graphical User Interface (GUI) agents powered by advanced multimodal large language models struggle to capture complex transitions between pages. An automated pipeline transforms sequential episodes into page graphs, enabling the retrieval of reliable perception guidelines using Retrieval-Augmented Generation. The PG-Agent framework with task decomposition strategy shows effectiveness in generalizing to unseen scenarios, as demonstrated in extensive experiments on various benchmarks. <div>
arXiv:2509.03536v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents possess significant commercial and social value, and GUI agents powered by advanced multimodal large language models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI agents usually utilize sequential episodes of multi-step operations across pages as the prior GUI knowledge, which fails to capture the complex transition relationship between pages, making it challenging for the agents to deeply perceive the GUI environment and generalize to new scenarios. Therefore, we design an automated pipeline to transform the sequential episodes into page graphs, which explicitly model the graph structure of the pages that are naturally connected by actions. To fully utilize the page graphs, we further introduce Retrieval-Augmented Generation (RAG) technology to effectively retrieve reliable perception guidelines of GUI from them, and a tailored multi-agent framework PG-Agent with task decomposition strategy is proposed to be injected with the guidelines so that it can generalize to unseen scenarios. Extensive experiments on various benchmarks demonstrate the effectiveness of PG-Agent, even with limited episodes for page graph construction.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models</title>
<link>https://arxiv.org/abs/2509.03548</link>
<guid>https://arxiv.org/abs/2509.03548</guid>
<content:encoded><![CDATA[
<div> Partially identifiable queries, causal models, Structural Causal Models, Bayesian network, multilinear programming<br />
Summary: 
The study explores partially identifiable queries in acyclic Structural Causal Models that are quasi-Markovian, focusing on scenarios where endogenous variables are observed while exogenous variables are not fully specified. The research introduces a new algorithm to simplify the construction of probability bounds by utilizing input probabilities over endogenous variables. Column generation techniques are applied to compute probability bounds in scenarios with a single intervention, demonstrating the feasibility of representing exogenous variables with polynomial cardinality. Experiments indicate the superiority of column generation methods over existing approaches in computing tight probability bounds for partially identifiable queries in causal models. <div>
arXiv:2509.03548v1 Announce Type: new 
Abstract: We investigate partially identifiable queries in a class of causal models. We focus on acyclic Structural Causal Models that are quasi-Markovian (that is, each endogenous variable is connected with at most one exogenous confounder). We look into scenarios where endogenous variables are observed (and a distribution over them is known), while exogenous variables are not fully specified. This leads to a representation that is in essence a Bayesian network where the distribution of root variables is not uniquely determined. In such circumstances, it may not be possible to precisely compute a probability value of interest. We thus study the computation of tight probability bounds, a problem that has been solved by multilinear programming in general, and by linear programming when a single confounded component is intervened upon. We present a new algorithm to simplify the construction of such programs by exploiting input probabilities over endogenous variables. For scenarios with a single intervention, we apply column generation to compute a probability bound through a sequence of auxiliary linear integer programs, thus showing that a representation with polynomial cardinality for exogenous variables is possible. Experiments show column generation techniques to be superior to existing methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method</title>
<link>https://arxiv.org/abs/2509.03550</link>
<guid>https://arxiv.org/abs/2509.03550</guid>
<content:encoded><![CDATA[
<div> Deep Reinforcement Learning, Conflict Detection and Resolution, Diffusion Probabilistic Models, Multimodal Decision-Making, Safety-Critical Task<br />
Summary:<br />
This paper introduces Diffusion-AC, a novel autonomous conflict resolution framework that integrates diffusion probabilistic models into Deep Reinforcement Learning for Conflict Detection and Resolution (CD&amp;R). Unlike existing approaches with unimodal policies, Diffusion-AC uses a reverse denoising process guided by a value function to generate a rich and multimodal action distribution. It also incorporates a Density-Progressive Safety Curriculum (DPSC) to facilitate stable learning in various traffic environments. Simulation experiments show that Diffusion-AC outperforms state-of-the-art benchmarks, achieving a high success rate and significantly reducing Near Mid-Air Collisions (NMACs) in high-density scenarios. The framework's multimodal decision-making capability enables effective alternative maneuvers, ensuring efficient and safe CD&amp;R in complex and dynamic airspace environments. <br /> <div>
arXiv:2509.03550v1 Announce Type: new 
Abstract: In the context of continuously rising global air traffic, efficient and safe Conflict Detection and Resolution (CD&amp;R) is paramount for air traffic management. Although Deep Reinforcement Learning (DRL) offers a promising pathway for CD&amp;R automation, existing approaches commonly suffer from a "unimodal bias" in their policies. This leads to a critical lack of decision-making flexibility when confronted with complex and dynamic constraints, often resulting in "decision deadlocks." To overcome this limitation, this paper pioneers the integration of diffusion probabilistic models into the safety-critical task of CD&amp;R, proposing a novel autonomous conflict resolution framework named Diffusion-AC. Diverging from conventional methods that converge to a single optimal solution, our framework models its policy as a reverse denoising process guided by a value function, enabling it to generate a rich, high-quality, and multimodal action distribution. This core architecture is complemented by a Density-Progressive Safety Curriculum (DPSC), a training mechanism that ensures stable and efficient learning as the agent progresses from sparse to high-density traffic environments. Extensive simulation experiments demonstrate that the proposed method significantly outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the most challenging high-density scenarios, Diffusion-AC not only maintains a high success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions (NMACs) by approximately 59% compared to the next-best-performing baseline, significantly enhancing the system's safety margin. This performance leap stems from its unique multimodal decision-making capability, which allows the agent to flexibly switch to effective alternative maneuvers.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents</title>
<link>https://arxiv.org/abs/2509.03581</link>
<guid>https://arxiv.org/abs/2509.03581</guid>
<content:encoded><![CDATA[
<div> dynamic planning, language models, reinforcement learning, test-time compute allocation, sequential decision-making

Summary:
This article introduces a new method for training large language models (LLMs) to reason via reinforcement learning (RL) by incorporating dynamic planning. The traditional approach of always planning before every action is computationally expensive and can degrade performance on long-horizon tasks. On the other hand, never planning limits performance. To address this, a two-stage training pipeline is proposed: supervised fine-tuning on synthetic data followed by RL to improve dynamic planning in long-horizon environments. Experiments on the Crafter environment show that agents trained with this approach are more sample-efficient and achieve complex objectives. These agents can also be guided by human-written plans to surpass independent capabilities. This work is a pioneering exploration of training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, leading to more efficient and controllable agentic systems.<br /><br /> <div>
arXiv:2509.03581v1 Announce Type: new 
Abstract: Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities. To our knowledge, this work is the first to explore training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, paving the way for more efficient, adaptive, and controllable agentic systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE</title>
<link>https://arxiv.org/abs/2509.03626</link>
<guid>https://arxiv.org/abs/2509.03626</guid>
<content:encoded><![CDATA[
<div> Generative AI, Large Language Models, Retrieval-Augmented Generation, Knowledge-Graph (KG)-SMILE, transparency<br />
Summary:<br />
Generative AI, specifically Large Language Models (LLMs), has made significant advancements but still struggles with producing inaccurate information, leading to reliability issues in critical areas such as healthcare. Retrieval-Augmented Generation (RAG) has improved accuracy by incorporating external knowledge, but remains opaque and reliant on data quality. The researchers introduced KG-SMILE, a perturbation-based framework for Graph RAG that enhances transparency by identifying influential graph entities and relations. Evaluation metrics, including fidelity, faithfulness, consistency, stability, and accuracy, demonstrate KG-SMILE's ability to provide stable and human-aligned explanations, promoting a balance between model effectiveness and interpretability in machine learning technologies.<br /> 
Summary: <div>
arXiv:2509.03626v1 Announce Type: new 
Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive progress but still produces hallucinations and unverifiable claims, limiting reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves accuracy by grounding outputs in external knowledge, especially in domains like healthcare, where precision is vital. However, RAG remains opaque and essentially a black box, heavily dependent on data quality. We developed a method-agnostic, perturbation-based framework that provides token and component-level interoperability for Graph RAG using SMILE and named it as Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing similarities, and training weighted linear surrogates, KG-SMILE identifies the graph entities and relations most influential to generated outputs, thereby making RAG more transparent. We evaluate KG-SMILE using comprehensive attribution metrics, including fidelity, faithfulness, consistency, stability, and accuracy. Our findings show that KG-SMILE produces stable, human-aligned explanations, demonstrating its capacity to balance model effectiveness with interpretability and thereby fostering greater transparency and trust in machine learning technologies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalARC: Abstract Reasoning with Causal World Models</title>
<link>https://arxiv.org/abs/2509.03636</link>
<guid>https://arxiv.org/abs/2509.03636</guid>
<content:encoded><![CDATA[
<div> Keywords: Reasoning, CausalARC, low-data, out-of-distribution, structural causal model

Summary:
CausalARC is introduced as a testbed for AI reasoning in challenging scenarios with limited data and distribution shifts. The testbed is based on a fully specified causal world model expressed as a structural causal model. Data augmentations provide various feedback types such as observational, interventional, and counterfactual in few-shot learning demonstrations. The use cases of CausalARC in language model evaluation settings include abstract reasoning with test-time training, counterfactual reasoning with in-context learning, program synthesis, and causal discovery with logical reasoning. This experimental setup aims to facilitate AI reasoning in complex and unfamiliar problem settings, showcasing the adaptability and generalization capabilities of AI systems. <br /><br />Summary: CausalARC offers a platform for AI reasoning in challenging environments by leveraging structural causal models and providing diverse feedback types for learning. Its applications span across various language model evaluation tasks, demonstrating the potential for AI systems to reason effectively in low-data and out-of-distribution scenarios. <div>
arXiv:2509.03636v1 Announce Type: new 
Abstract: Reasoning requires adaptation to novel problem settings under limited data and distribution shift. This work introduces CausalARC: an experimental testbed for AI reasoning in low-data and out-of-distribution regimes, modeled after the Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is sampled from a fully specified causal world model, formally expressed as a structural causal model. Principled data augmentations provide observational, interventional, and counterfactual feedback about the world model in the form of few-shot, in-context learning demonstrations. As a proof-of-concept, we illustrate the use of CausalARC for four language model evaluation settings: (1) abstract reasoning with test-time training, (2) counterfactual reasoning with in-context learning, (3) program synthesis, and (4) causal discovery with logical reasoning.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations</title>
<link>https://arxiv.org/abs/2509.03644</link>
<guid>https://arxiv.org/abs/2509.03644</guid>
<content:encoded><![CDATA[
<div> neurosymbolic system, Embodied-LM, logical reasoning, image schemas, spatial reasoning, Answer Set Programming

Summary:
Embodied-LM is a prototype neurosymbolic system that integrates image schemas, derived from sensorimotor experience, into large language models (LLMs) to improve logical reasoning capabilities. By operationalizing spatial reasoning using Answer Set Programming, the system enhances the interpretability and logical reasoning of LLMs on deduction problems. The system's use of cognitive structures grounded in sensory experience guides LLMs to interpret scenarios and formalize them as executable programs, leading to more effective logical reasoning. While currently focused on spatial primitives, the system establishes a computational foundation for incorporating more complex and dynamic representations in the future. <div>
arXiv:2509.03644v1 Announce Type: new 
Abstract: Despite significant progress in natural language understanding, Large Language Models (LLMs) remain error-prone when performing logical reasoning, often lacking the robust mental representations that enable human-like comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that grounds understanding and logical reasoning in schematic representations based on image schemas-recurring patterns derived from sensorimotor experience that structure human cognition. Our system operationalizes the spatial foundations of these cognitive structures using declarative spatial reasoning within Answer Set Programming. Through evaluation on logical deduction problems, we demonstrate that LLMs can be guided to interpret scenarios through embodied cognitive structures, that these structures can be formalized as executable programs, and that the resulting representations support effective logical reasoning with enhanced interpretability. While our current implementation focuses on spatial primitives, it establishes the computational foundation for incorporating more complex and dynamic representations.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.03646</link>
<guid>https://arxiv.org/abs/2509.03646</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, reasoning hierarchy, strategic planning, entropy dynamics

Summary:
Reinforcement Learning has been successful in enhancing the reasoning abilities of Large Language Models (LLMs), with phenomena like "aha moments" and entropy dynamics being part of an emergent reasoning hierarchy. The study uncovers a two-phase dynamic where models initially focus on procedural correctness and low-level skills before transitioning to high-level strategic planning. The analysis highlights the inefficiencies in current RL algorithms like GRPO, which do not focus optimization efforts on high-impact planning tokens. The proposed HICRA algorithm shows significant performance improvements by concentrating on the strategic bottleneck. Semantic entropy is identified as a better measure of strategic exploration compared to token-level entropy. The research emphasizes the importance of focusing on high-level planning to enhance advanced reasoning capabilities in LLMs.<br /><br />Summary: <div>
arXiv:2509.03646v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification</title>
<link>https://arxiv.org/abs/2509.03649</link>
<guid>https://arxiv.org/abs/2509.03649</guid>
<content:encoded><![CDATA[
<div> Segmentation algorithms, SHAP method, explainable AI, time series classification, attribution quality<br />
Summary:
- Explaining the predictions of complex Time Series Classification models is crucial for understanding them.
- SHapley Additive exPlanations (SHAP) is a popular attribution method, but its computational complexity limits its practicality for long time series.
- Aggregating features via segmentation has been shown to reduce SHAP running time significantly.
- The choice of segmentation strategy greatly impacts the explanation quality, but the number of segments plays a more significant role than the specific method.
- Equal-length segmentation consistently outperforms many custom time series segmentation algorithms.
- A novel attribution normalization technique that weights segments by their length has been introduced and shown to consistently improve attribution quality.<br /><br />Summary: <div>
arXiv:2509.03649v1 Announce Type: new 
Abstract: Explainable AI (XAI) has become an increasingly important topic for understanding and attributing the predictions made by complex Time Series Classification (TSC) models. Among attribution methods, SHapley Additive exPlanations (SHAP) is widely regarded as an excellent attribution method; but its computational complexity, which scales exponentially with the number of features, limits its practicality for long time series. To address this, recent studies have shown that aggregating features via segmentation, to compute a single attribution value for a group of consecutive time points, drastically reduces SHAP running time. However, the choice of the optimal segmentation strategy remains an open question. In this work, we investigated eight different Time Series Segmentation algorithms to understand how segment compositions affect the explanation quality. We evaluate these approaches using two established XAI evaluation methodologies: InterpretTime and AUC Difference. Through experiments on both Multivariate (MTS) and Univariate Time Series (UTS), we find that the number of segments has a greater impact on explanation quality than the specific segmentation method. Notably, equal-length segmentation consistently outperforms most of the custom time series segmentation algorithms. Furthermore, we introduce a novel attribution normalisation technique that weights segments by their length and we show that it consistently improves attribution quality.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming</title>
<link>https://arxiv.org/abs/2509.03728</link>
<guid>https://arxiv.org/abs/2509.03728</guid>
<content:encoded><![CDATA[
<div> persona, red-teaming, AI governance, safety research, adversarial prompts <br />
Summary: <br />
The paper introduces PersonaTeaming, a method for incorporating personas in automated red-teaming to identify potential risks posed by AI models. By utilizing "red-teaming expert" and "regular AI user" personas in prompt generation, the method explores a wider range of adversarial strategies. A dynamic algorithm is developed to generate various persona types adapted to different seed prompts, leading to improved attack success rates (up to 144.1%) while maintaining prompt diversity compared to RainbowPlus. Metrics to measure "mutation distance" are introduced to assess prompt diversity. The study highlights the potential of persona mutation in enhancing automated red-teaming approaches and suggests opportunities for exploring the synergy between automated and human red-teaming strategies. <br /> <div>
arXiv:2509.03728v1 Announce Type: new 
Abstract: Recent developments in AI governance and safety research have called for red-teaming methods that can effectively surface potential risks posed by AI models. Many of these calls have emphasized how the identities and backgrounds of red-teamers can shape their red-teaming strategies, and thus the kinds of risks they are likely to uncover. While automated red-teaming approaches promise to complement human red-teaming by enabling larger-scale exploration of model behavior, current approaches do not consider the role of identity. As an initial step towards incorporating people's background and identities in automated red-teaming, we develop and evaluate a novel method, PersonaTeaming, that introduces personas in the adversarial prompt generation process to explore a wider spectrum of adversarial strategies. In particular, we first introduce a methodology for mutating prompts based on either "red-teaming expert" personas or "regular AI user" personas. We then develop a dynamic persona-generating algorithm that automatically generates various persona types adaptive to different seed prompts. In addition, we develop a set of new metrics to explicitly measure the "mutation distance" to complement existing diversity measurements of adversarial prompts. Our experiments show promising improvements (up to 144.1%) in the attack success rates of adversarial prompts through persona mutation, while maintaining prompt diversity, compared to RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the strengths and limitations of different persona types and mutation methods, shedding light on future opportunities to explore complementarities between automated and human red-teaming approaches.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Personality Illusion: Revealing Dissociation Between Self-Reports &amp; Behavior in LLMs</title>
<link>https://arxiv.org/abs/2509.03730</link>
<guid>https://arxiv.org/abs/2509.03730</guid>
<content:encoded><![CDATA[
<div> personality traits, Large Language Models, self-reported traits, behavioral validation, targeted interventions 
Summary: 
The study explores how Large Language Models (LLMs) exhibit personality traits similar to humans, like agreeableness and self-regulation. The research focuses on three dimensions: the evolution of trait profiles during training, the relationship between self-reported traits and behavior, and the impact of interventions on trait expression and behavior. Results show that instructional alignment enhances trait stability and correlations, akin to human data. However, self-reported traits do not consistently predict behavior, highlighting a disconnect between trait expression and behavior. Persona injection influences self-reports but lacks a significant impact on actual behavior. The findings challenge assumptions about LLM personality and highlight the necessity for further evaluation on alignment and interpretability. <br /><br />Summary: <div>
arXiv:2509.03730v1 Announce Type: new 
Abstract: Personality traits have long been studied as predictors of human behavior.Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation</title>
<link>https://arxiv.org/abs/2509.03736</link>
<guid>https://arxiv.org/abs/2509.03736</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, human-subject research, dialogue, internal consistency, simulation

Summary:
<br /><br />
The study evaluates whether synthetic agents generated by Large Language Models (LLMs) can be reliable substitutes for human participants in research. While previous research has focused on the correspondence between LLM-generated survey data and human responses, this study examines the internal consistency of LLMs in different experimental settings. By revealing the agent's internal state and assessing its behavior in dialogue situations, the study reveals significant inconsistencies in LLMs across model families and sizes. Despite generating responses similar to humans, the agents lack internal consistency, highlighting a crucial limitation in their ability to accurately represent real participants in research. The publicly accessible simulation code and data provide valuable insights into the challenges of using LLMs in social science research. <div>
arXiv:2509.03736v1 Announce Type: new 
Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the notion that synthetic agents can serve as substitutes for real participants in human-subject research. In an effort to evaluate the merits of this claim, social science researchers have largely focused on whether LLM-generated survey data corresponds to that of a human counterpart whom the LLM is prompted to represent. In contrast, we address a more fundamental question: Do agents maintain internal consistency, retaining similar behaviors when examined under different experimental settings? To this end, we develop a study designed to (a) reveal the agent's internal state and (b) examine agent behavior in a basic dialogue setting. This design enables us to explore a set of behavioral hypotheses to assess whether an agent's conversation behavior is consistent with what we would expect from their revealed internal state. Our findings on these hypotheses show significant internal inconsistencies in LLMs across model families and at differing model sizes. Most importantly, we find that, although agents may generate responses matching those of their human counterparts, they fail to be internally consistent, representing a critical gap in their capabilities to accurately substitute for real participants in human-subject research. Our simulation code and data are publicly accessible.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs</title>
<link>https://arxiv.org/abs/2509.03768</link>
<guid>https://arxiv.org/abs/2509.03768</guid>
<content:encoded><![CDATA[
<div> Keywords: Offshore Wind, Maintenance, Safety, Large Language Models, RAGuard <br />
Summary:<br />
The study introduces RAGuard, an advanced framework to improve accuracy and safety in Offshore Wind (OSW) maintenance. By combining safety-critical documents with technical manuals, RAGuard enhances retrieval performance. The framework issues parallel queries to two indices, allocating separate budgets for knowledge and safety to ensure technical depth and safety coverage. A SafetyClamp extension further enhances safety by increasing Safety Recall while maintaining Technical Recall. Evaluation across different retrieval paradigms shows significant improvements in Safety Recall with RAGuard. These results indicate the potential of RAGuard and SafetyClamp in establishing a new standard for integrating safety assurance into decision support systems using Large Language Models in critical maintenance situations.<br /> 
Summary: <div>
arXiv:2509.03768v1 Announce Type: new 
Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet conventional Large Language Models (LLMs) often fail when confronted with highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced Retrieval-Augmented Generation (RAG) framework that explicitly integrates safety-critical documents alongside technical manuals.By issuing parallel queries to two indices and allocating separate retrieval budgets for knowledge and safety, RAGuard guarantees both technical depth and safety coverage. We further develop a SafetyClamp extension that fetches a larger candidate pool, "hard-clamping" exact slot guarantees to safety. We evaluate across sparse (BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms, measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than 50\% in RAGuard, while maintaining Technical Recall above 60\%. These results demonstrate that RAGuard and SafetyClamp have the potential to establish a new standard for integrating safety assurance into LLM-powered decision support in critical maintenance contexts.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLM-Based Agents for Intelligent Supply Chain Planning</title>
<link>https://arxiv.org/abs/2509.03811</link>
<guid>https://arxiv.org/abs/2509.03811</guid>
<content:encoded><![CDATA[
<div> Keywords: supply chain management, AI technologies, large language models, e-commerce platform, JD.com

Summary: 
Supply chain management is a critical concept involving various entities in the movement of physical products. The development of AI technologies, particularly large language models, has opened new possibilities in addressing supply chain planning challenges. This work introduces the Supply Chain Planning Agent (SCPA) framework, which utilizes AI technology to understand domain knowledge, tasks decomposition, and evidence-based planning reports generation. The SCPA framework was successfully deployed in a real-world scenario at JD.com, showcasing improved efficiency, accuracy, and stock availability. By leveraging AI tools, the SCPA framework effectively reduced labor while enhancing key metrics in supply chain operations. This demonstrates the practical applications of AI technologies in optimizing supply chain planning processes. <div>
arXiv:2509.03811v1 Announce Type: new 
Abstract: In supply chain management, planning is a critical concept. The movement of physical products across different categories, from suppliers to warehouse management, to sales, and logistics transporting them to customers, entails the involvement of many entities. It covers various aspects such as demand forecasting, inventory management, sales operations, and replenishment. How to collect relevant data from an e-commerce platform's perspective, formulate long-term plans, and dynamically adjust them based on environmental changes, while ensuring interpretability, efficiency, and reliability, is a practical and challenging problem. In recent years, the development of AI technologies, especially the rapid progress of large language models, has provided new tools to address real-world issues. In this work, we construct a Supply Chain Planning Agent (SCPA) framework that can understand domain knowledge, comprehend the operator's needs, decompose tasks, leverage or create new tools, and return evidence-based planning reports. We deploy this framework in JD.com's real-world scenario, demonstrating the feasibility of LLM-agent applications in the supply chain. It effectively reduced labor and improved accuracy, stock availability, and other key metrics.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.03817</link>
<guid>https://arxiv.org/abs/2509.03817</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, multi-agent systems, meta-cognitive actions, reinforcement learning, dynamic strategies<br />
Summary: <br />
- The Meta-Policy Deliberation Framework (MPDF) introduces decentralized policies for agents in large language models, focusing on meta-cognitive actions like Persist, Refine, and Concede. 
- SoftRankPO, a novel reinforcement learning algorithm, stabilizes training by shaping advantages based on smooth normal quantiles, enhancing adaptability in multi-agent systems.
- Traditional policy gradients are replaced with SoftRankPO to handle reward variance and improve overall performance in reasoning benchmarks.
- MPDF with SoftRankPO demonstrates a 4-5% absolute gain in average accuracy across mathematical and general reasoning tasks compared to existing algorithms.
- This work highlights the importance of adaptive, meta-cognitive strategies in multi-agent systems, emphasizing the shift from fixed protocols to dynamic deliberative approaches. <br /> 
Summary: <div>
arXiv:2509.03817v1 Announce Type: new 
Abstract: Multi-agent systems of large language models (LLMs) show promise for complex reasoning, but their effectiveness is often limited by fixed collaboration protocols. These frameworks typically focus on macro-level orchestration while overlooking agents' internal deliberative capabilities. This critical meta-cognitive blindspot treats agents as passive executors unable to adapt their strategy based on internal cognitive states like uncertainty or confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where agents learn a decentralized policy over a set of high-level meta-cognitive actions: Persist, Refine, and Concede. To overcome the instability of traditional policy gradients in this setting, we develop SoftRankPO, a novel reinforcement learning algorithm. SoftRankPO stabilizes training by shaping advantages based on the rank of rewards mapped through smooth normal quantiles, making the learning process robust to reward variance. Experiments show that MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms. Our work presents a paradigm for learning adaptive, meta-cognitive policies for multi-agent LLM systems, shifting the focus from designing fixed protocols to learning dynamic, deliberative strategies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2509.03827</link>
<guid>https://arxiv.org/abs/2509.03827</guid>
<content:encoded><![CDATA[
<div> benchmark, decision scenarios, social policymaking, homelessness alleviation, Capability Approach

Summary:<br />
- The article evaluates the alignment of Large Language Models (LLMs) with domain experts in social policymaking for homelessness alleviation.
- A novel benchmark is created with decision scenarios and policy choices across four geographies.
- Policies are based on the Capability Approach for human development.
- An automated pipeline connects benchmarked policies to an agent-based model for analysis.
- The study shows potential for LLMs to provide valuable insights for social policy making with responsible guidance from local experts. 

Summary: <div>
arXiv:2509.03827v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes domains. Their capacity to process vast amounts of unstructured data, explore flexible scenarios, and handle a diversity of contextual factors can make them uniquely suited to provide new insights for the complexity of social policymaking. This article evaluates whether LLMs' are aligned with domain experts (and among themselves) to inform social policymaking on the subject of homelessness alleviation - a challenge affecting over 150 million people worldwide. We develop a novel benchmark comprised of decision scenarios with policy choices across four geographies (South Bend, USA; Barcelona, Spain; Johannesburg, South Africa; Macau SAR, China). The policies in scope are grounded in the conceptual framework of the Capability Approach for human development. We also present an automated pipeline that connects the benchmarked policies to an agent-based model, and we explore the social impact of the recommended policies through simulated social scenarios. The paper results reveal promising potential to leverage LLMs for social policy making. If responsible guardrails and contextual calibrations are introduced in collaboration with local domain experts, LLMs can provide humans with valuable insights, in the form of alternative policies at scale.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic Model Context Protocol Framework for Medical Concept Standardization</title>
<link>https://arxiv.org/abs/2509.03828</link>
<guid>https://arxiv.org/abs/2509.03828</guid>
<content:encoded><![CDATA[
<div> Observational Medical Outcomes Partnership, OMOP common data model, mapping, large language models, Model Context Protocol <br />
Summary: The article introduces a new system for mapping source medical terms to OMOP standard concepts using the Model Context Protocol (MCP). This system aims to prevent hallucination in large language models (LLMs) and improve efficiency and accuracy in the mapping process. By allowing LLMs to interact with external resources and tools, the system provides real-time vocabulary lookups and structured reasoning outputs. This approach enables explainable mapping and enhances both exploratory and production environments for healthcare research. Overall, the system offers a zero-training solution to improve the standardization of health data using the OMOP CDM, reducing errors and resource-intensive processes in medical term mapping. <br /><br /> <div>
arXiv:2509.03828v1 Announce Type: new 
Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM) provides a standardized representation of heterogeneous health data to support large-scale, multi-institutional research. One critical step in data standardization using OMOP CDM is the mapping of source medical terms to OMOP standard concepts, a procedure that is resource-intensive and error-prone. While large language models (LLMs) have the potential to facilitate this process, their tendency toward hallucination makes them unsuitable for clinical deployment without training and expert validation. Here, we developed a zero-training, hallucination-preventive mapping system based on the Model Context Protocol (MCP), a standardized and secure framework allowing LLMs to interact with external resources and tools. The system enables explainable mapping and significantly improves efficiency and accuracy with minimal effort. It provides real-time vocabulary lookups and structured reasoning outputs suitable for immediate use in both exploratory and production environments.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai</title>
<link>https://arxiv.org/abs/2509.03830</link>
<guid>https://arxiv.org/abs/2509.03830</guid>
<content:encoded><![CDATA[
<div> Keywords: historic urban quarters, tourism perception, AI-powered framework, multimodal data, social media

Summary:
This study introduces an AI-powered framework to analyze tourist perception in historic urban quarters using data from social media. The framework incorporates focal point extraction, color theme analysis, and sentiment mining to understand how tourists perceive these environments. By analyzing tourist-shared photos, visual focus areas and dominant colors are identified to assess aesthetic preferences. Color themes are compared between social media photos and real-world street views, revealing potential gaps in visual expectations. Tourist reviews are analyzed using a hybrid sentiment analysis approach to evaluate satisfaction across different dimensions. The results show spatial variations in aesthetic appeal and emotional response, providing insights for sustainable urban planning and heritage conservation. The integrated approach presented in this study offers a comprehensive way to decode tourist perception and inform decision-making for the design of engaging public spaces. 

<br /><br />Summary: <div>
arXiv:2509.03830v1 Announce Type: new 
Abstract: Historic urban quarters play a vital role in preserving cultural heritage while serving as vibrant spaces for tourism and everyday life. Understanding how tourists perceive these environments is essential for sustainable, human-centered urban planning. This study proposes a multidimensional AI-powered framework for analyzing tourist perception in historic urban quarters using multimodal data from social media. Applied to twelve historic quarters in central Shanghai, the framework integrates focal point extraction, color theme analysis, and sentiment mining. Visual focus areas are identified from tourist-shared photos using a fine-tuned semantic segmentation model. To assess aesthetic preferences, dominant colors are extracted using a clustering method, and their spatial distribution across quarters is analyzed. Color themes are further compared between social media photos and real-world street views, revealing notable shifts. This divergence highlights potential gaps between visual expectations and the built environment, reflecting both stylistic preferences and perceptual bias. Tourist reviews are evaluated through a hybrid sentiment analysis approach combining a rule-based method and a multi-task BERT model. Satisfaction is assessed across four dimensions: tourist activities, built environment, service facilities, and business formats. The results reveal spatial variations in aesthetic appeal and emotional response. Rather than focusing on a single technical innovation, this framework offers an integrated, data-driven approach to decoding tourist perception and contributes to informed decision-making in tourism, heritage conservation, and the design of aesthetically engaging public spaces.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures</title>
<link>https://arxiv.org/abs/2509.03857</link>
<guid>https://arxiv.org/abs/2509.03857</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Knowledge Graphs, Reliability evaluation, Large Language Models, Real-time monitoring

Summary: 
This article introduces a systematic methodology for evaluating the reliability of Generative AI (GEN AI) models, which are prone to issues such as hallucinations, semantic drift, and biases. The proposed methodology utilizes deterministic and Large Language Model (LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate the performance of GEN AI models. By constructing parallel KGs using rule-based methods and real-time textual data streams, structural deviations and semantic discrepancies can be quantified using established KG metrics. An automated real-time monitoring framework computes deviations between deterministic and LLM-generated KGs and proactively identifies significant deviations, flagging semantic anomalies or hallucinations. This metric-driven comparison between deterministic and dynamically generated KGs provides a robust and scalable evaluation framework for assessing the reliability of GEN AI models.<br /><br />Summary: <div>
arXiv:2509.03857v1 Announce Type: new 
Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains but present substantial challenges due to reliability concerns, including hallucinations, semantic drift, and inherent biases. These models typically operate as black-boxes, complicating transparent and objective evaluation. Current evaluation methods primarily depend on subjective human assessment, limiting scalability, transparency, and effectiveness. This research proposes a systematic methodology using deterministic and Large Language Model (LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN AI reliability. We construct two parallel KGs: (i) a deterministic KG built using explicit rule-based methods, predefined ontologies, domain-specific dictionaries, and structured entity-relation extraction rules, and (ii) an LLM-generated KG dynamically derived from real-time textual data streams such as live news articles. Utilizing real-time news streams ensures authenticity, mitigates biases from repetitive training, and prevents adaptive LLMs from bypassing predefined benchmarks through feedback memorization. To quantify structural deviations and semantic discrepancies, we employ several established KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring framework continuously computes deviations between deterministic and LLM-generated KGs. By establishing dynamic anomaly thresholds based on historical structural metric distributions, our method proactively identifies and flags significant deviations, thus promptly detecting semantic anomalies or hallucinations. This structured, metric-driven comparison between deterministic and dynamically generated KGs delivers a robust and scalable evaluation framework.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expedition &amp; Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata</title>
<link>https://arxiv.org/abs/2509.03863</link>
<guid>https://arxiv.org/abs/2509.03863</guid>
<content:encoded><![CDATA[
<div> Keywords: visual patterns, cellular automata, exploration, novelty search, artificial life
Summary: 
Expedition and Expansion (E&amp;E) is a new hybrid exploration strategy for discovering diverse visual patterns in continuous cellular automata (CA). E&amp;E combines local novelty-driven expansions with goal-directed expeditions using a Vision-Language Model (VLM) to generate linguistic goals. By operating in semantic spaces aligning with human perception, E&amp;E enhances the interpretability and relevance of discovered behaviors. Tested on Flow Lenia, E&amp;E consistently uncovers more diverse solutions than existing methods. A genealogical analysis reveals that expedition-originating solutions play a significant role in long-term exploration, unlocking new behavioral niches. E&amp;E breaks through local novelty boundaries and explores behavioral landscapes in human-aligned, interpretable ways, providing a promising framework for open-ended exploration in artificial life and beyond.<br /><br />Summary: <div>
arXiv:2509.03863v1 Announce Type: new 
Abstract: Discovering diverse visual patterns in continuous cellular automata (CA) is challenging due to the vastness and redundancy of high-dimensional behavioral spaces. Traditional exploration methods like Novelty Search (NS) expand locally by mutating known novel solutions but often plateau when local novelty is exhausted, failing to reach distant, unexplored regions. We introduce Expedition and Expansion (E&amp;E), a hybrid strategy where exploration alternates between local novelty-driven expansions and goal-directed expeditions. During expeditions, E&amp;E leverages a Vision-Language Model (VLM) to generate linguistic goals--descriptions of interesting but hypothetical patterns that drive exploration toward uncharted regions. By operating in semantic spaces that align with human perception, E&amp;E both evaluates novelty and generates goals in conceptually meaningful ways, enhancing the interpretability and relevance of discovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich, emergent behaviors, E&amp;E consistently uncovers more diverse solutions than existing exploration methods. A genealogical analysis further reveals that solutions originating from expeditions disproportionately influence long-term exploration, unlocking new behavioral niches that serve as stepping stones for subsequent search. These findings highlight E&amp;E's capacity to break through local novelty boundaries and explore behavioral landscapes in human-aligned, interpretable ways, offering a promising template for open-ended exploration in artificial life and beyond.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace</title>
<link>https://arxiv.org/abs/2509.03890</link>
<guid>https://arxiv.org/abs/2509.03890</guid>
<content:encoded><![CDATA[
<div> marketplace, agentic assistant, LLM-powered, conversational search, task automation
Summary: 
The paper introduces a novel approach to simplify interactions on C2C e-commerce platforms using an agentic assistant powered by Large Language Models (LLMs). This assistant shifts the primary interaction model from complex GUIs to an intuitive AI agent that interprets natural language commands. For sellers, the assistant automates tasks like listing updates and bulk messaging, while facilitating a more efficient product discovery process for buyers through conversational search. The architecture for the Facebook Marketplace Assistant (FaMA) is presented, highlighting the benefits of this conversational paradigm as a more accessible and efficient alternative to traditional app interfaces. Experimental results demonstrate FaMA's high task success rate of 98% and up to a 2x speedup in interaction time.<br /><br />Summary: <div>
arXiv:2509.03890v1 Announce Type: new 
Abstract: The emergence of agentic AI, powered by Large Language Models (LLMs), marks a paradigm shift from reactive generative systems to proactive, goal-oriented autonomous agents capable of sophisticated planning, memory, and tool use. This evolution presents a novel opportunity to address long-standing challenges in complex digital environments. Core tasks on Consumer-to-Consumer (C2C) e-commerce platforms often require users to navigate complex Graphical User Interfaces (GUIs), making the experience time-consuming for both buyers and sellers. This paper introduces a novel approach to simplify these interactions through an LLM-powered agentic assistant. This agent functions as a new, conversational entry point to the marketplace, shifting the primary interaction model from a complex GUI to an intuitive AI agent. By interpreting natural language commands, the agent automates key high-friction workflows. For sellers, this includes simplified updating and renewal of listings, and the ability to send bulk messages. For buyers, the agent facilitates a more efficient product discovery process through conversational search. We present the architecture for Facebook Marketplace Assistant (FaMA), arguing that this agentic, conversational paradigm provides a lightweight and more accessible alternative to traditional app interfaces, allowing users to manage their marketplace activities with greater efficiency. Experiments show FaMA achieves a 98% task success rate on solving complex tasks on the marketplace and enables up to a 2x speedup on interaction time.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.03906</link>
<guid>https://arxiv.org/abs/2509.03906</guid>
<content:encoded><![CDATA[
<div> fine-tuned, synthetic reasoning, reinforcement learning, report generation, visual question answering<br />
<br />
Summary:<br />
DeepMedix-R1 is a comprehensive medical foundation model for chest X-ray interpretation that focuses on improving transparency and interpretability. It undergoes a sequential training process, starting with fine-tuning on CXR instruction data, followed by exposure to synthetic reasoning samples for cold-start reasoning, and refinement through online reinforcement learning. The model produces both answers and reasoning steps tied to local image regions, leading to significant improvements in report generation and visual question answering tasks compared to existing models. A benchmarking framework called Report Arena is proposed for robust evaluation, showcasing the model's answer quality superiority. Expert reviews confirm that DeepMedix-R1's generated reasoning steps offer greater interpretability and clinical plausibility than a well-known model. This work advances the development of medical foundation models towards holistic, transparent, and clinically actionable approaches for CXR interpretation. <div>
arXiv:2509.03906v1 Announce Type: new 
Abstract: Medical foundation models (FMs) have shown tremendous promise amid the rapid advancements in artificial intelligence (AI) technologies. However, current medical FMs typically generate answers in a black-box manner, lacking transparent reasoning processes and locally grounded interpretability, which hinders their practical clinical deployments. To this end, we introduce DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It leverages a sequential training pipeline: initially fine-tuned on curated CXR instruction data to equip with fundamental CXR interpretation capabilities, then exposed to high-quality synthetic reasoning samples to enable cold-start reasoning, and finally refined via online reinforcement learning to enhance both grounded reasoning quality and generation performance. Thus, the model produces both an answer and reasoning steps tied to the image's local regions for each query. Quantitative evaluation demonstrates substantial improvements in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent) tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking framework using advanced language models to evaluate answer quality, further highlighting the superiority of DeepMedix-R1. Expert review of generated reasoning steps reveals greater interpretability and clinical plausibility compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall preference). Collectively, our work advances medical FM development toward holistic, transparent, and clinically actionable modeling for CXR interpretation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Infinite Domain Parameters in Planning Through Best-First Search with Delayed Partial Expansions</title>
<link>https://arxiv.org/abs/2509.03953</link>
<guid>https://arxiv.org/abs/2509.03953</guid>
<content:encoded><![CDATA[
<div> Control parameters, automated planning, decision variables, search algorithm, completeness
<br />
The paper introduces a novel approach in automated planning that explicitly treats control parameters as decision points in the search space. Existing methods have often treated control parameters as constraints rather than decision points. The proposed best-first, heuristic search algorithm operates over infinite decision spaces and utilizes delayed partial expansion to incrementally explore successors. This approach proves completeness under certain conditions and offers a competitive alternative for planning problems involving control parameters. The algorithm's ability to handle control parameters as true decision points sets it apart from existing methods, potentially leading to more efficient and effective planning solutions.
<br /><br />Summary: <div>
arXiv:2509.03953v1 Announce Type: new 
Abstract: In automated planning, control parameters extend standard action representations through the introduction of continuous numeric decision variables. Existing state-of-the-art approaches have primarily handled control parameters as embedded constraints alongside other temporal and numeric restrictions, and thus have implicitly treated them as additional constraints rather than as decision points in the search space. In this paper, we propose an efficient alternative that explicitly handles control parameters as true decision points within a systematic search scheme. We develop a best-first, heuristic search algorithm that operates over infinite decision spaces defined by control parameters and prove a notion of completeness in the limit under certain conditions. Our algorithm leverages the concept of delayed partial expansion, where a state is not fully expanded but instead incrementally expands a subset of its successors. Our results demonstrate that this novel search algorithm is a competitive alternative to existing approaches for solving planning problems involving control parameters.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World Model Implanting for Test-time Adaptation of Embodied Agents</title>
<link>https://arxiv.org/abs/2509.03956</link>
<guid>https://arxiv.org/abs/2509.03956</guid>
<content:encoded><![CDATA[
<div> world model implanting framework, embodied AI, adaptation, domain-specific knowledge, robust

Summary: 
The article introduces the world model implanting framework (WorMI) for embodied AI, aiming to enable agents to adapt to new domains without extensive retraining. WorMI combines large language models (LLMs) with independently learned world models through test-time composition. It uses a prototype-based retrieval approach for world models and a compound attention method to integrate knowledge and align representations. The framework allows seamless implantation and removal of world models, enhancing adaptability. Evaluation on VirtualHome and ALFWorld benchmarks shows superior zero-shot and few-shot performance compared to LLM-based approaches, indicating potential for real-world deployment in scenarios requiring adaptability and data efficiency. <div>
arXiv:2509.03956v1 Announce Type: new 
Abstract: In embodied AI, a persistent challenge is enabling agents to robustly adapt to novel domains without requiring extensive data collection or retraining. To address this, we present a world model implanting framework (WorMI) that combines the reasoning capabilities of large language models (LLMs) with independently learned, domain-specific world models through test-time composition. By allowing seamless implantation and removal of the world models, the embodied agent's policy achieves and maintains cross-domain adaptability. In the WorMI framework, we employ a prototype-based world model retrieval approach, utilizing efficient trajectory-based abstract representation matching, to incorporate relevant models into test-time composition. We also develop a world-wise compound attention method that not only integrates the knowledge from the retrieved world models but also aligns their intermediate representations with the reasoning model's representation within the agent's policy. This framework design effectively fuses domain-specific knowledge from multiple world models, ensuring robust adaptation to unseen domains. We evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating superior zero-shot and few-shot performance compared to several LLM-based approaches across a range of unseen domains. These results highlight the frameworks potential for scalable, real-world deployment in embodied agent scenarios where adaptability and data efficiency are essential.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent</title>
<link>https://arxiv.org/abs/2509.03990</link>
<guid>https://arxiv.org/abs/2509.03990</guid>
<content:encoded><![CDATA[
<div> Reinforcement-learning, Large language model, Transferable policies, Meta-Policy Memory, Robustness<br />
Summary:<br />
The article introduces Meta-Policy Reflexion (MPR), a framework that consolidates reflections from Large Language Model (LLM) agents into a Meta-Policy Memory (MPM) to improve performance. MPR utilizes soft memory-guided decoding and hard rule admissibility checks to enhance adaptability without requiring parameter updates. The framework externalizes reusable corrective knowledge and enforces domain constraints to reduce unsafe actions. Empirical results in a text-based agent environment show improved execution accuracy and robustness compared to existing Reflexion approaches. Rule admissibility further enhances stability. The study analyzes the mechanisms behind these results, discusses scalability and potential failure modes, and suggests future directions for extensions in multimodal and multi-agent scenarios. This innovative approach addresses common challenges faced by LLM agents and offers a promising solution for improving cross-task adaptability and overall performance. <br /><br />Summary: <div>
arXiv:2509.03990v1 Announce Type: new 
Abstract: Large language model (LLM) agents achieve impressive single-task performance but commonly exhibit repeated failures, inefficient exploration, and limited cross-task adaptability. Existing reflective strategies (e.g., Reflexion, ReAct) improve per-episode behavior but typically produce ephemeral, task-specific traces that are not reused across tasks. Reinforcement-learning based alternatives can produce transferable policies but require substantial parameter updates and compute. In this work we introduce Meta-Policy Reflexion (MPR): a hybrid framework that consolidates LLM-generated reflections into a structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at inference time through two complementary mechanisms soft memory-guided decoding and hard rule admissibility checks(HAC). MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection. We formalize the MPM representation, present algorithms for update and decoding, and validate the approach in a text-based agent environment following the experimental protocol described in the provided implementation (AlfWorld-based). Empirical results reported in the supplied material indicate consistent gains in execution accuracy and robustness when compared to Reflexion baselines; rule admissibility further improves stability. We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi?agent extensions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPBO: LLM-powered Optimization for Local Search PBO Solvers</title>
<link>https://arxiv.org/abs/2509.04007</link>
<guid>https://arxiv.org/abs/2509.04007</guid>
<content:encoded><![CDATA[
<div> Keywords: Pseudo-Boolean Optimization, Local search solvers, Large Language Models, AutoPBO, Performance improvement

Summary:<br />
Pseudo-Boolean Optimization (PBO) is a powerful framework for modeling combinatorial problems using PB constraints. Local search solvers, crucial for PBO solving, rely heavily on internal heuristics for efficient search, often requiring manual tuning. This study introduces AutoPBO, an innovative framework utilizing Large Language Models (LLMs) to automatically enhance PBO local search solvers. Experimentation on various benchmarks demonstrates AutoPBO's significant performance improvements over previous local search methods and competitive performance against state-of-the-art competitors NuPBO, OraSLS, PBO-IHS, RoundingSat, Gurobi, and SCIP. The results highlight AutoPBO's potential to automate local search solver design, illustrating promise in advancing optimization techniques. <br />Summary: <div>
arXiv:2509.04007v1 Announce Type: new 
Abstract: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling combinatorial problems through pseudo-Boolean (PB) constraints. Local search solvers have shown excellent performance in PBO solving, and their efficiency is highly dependent on their internal heuristics to guide the search. Still, their design often requires significant expert effort and manual tuning in practice. While Large Language Models (LLMs) have demonstrated potential in automating algorithm design, their application to optimizing PBO solvers remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered framework to automatically enhance PBO local search solvers. We conduct experiments on a broad range of four public benchmarks, including one real-world benchmark, a benchmark from PB competition, an integer linear programming optimization benchmark, and a crafted combinatorial benchmark, to evaluate the performance improvement achieved by AutoPBO and compare it with six state-of-the-art competitors, including two local search PBO solvers NuPBO and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates significant improvements over previous local search approaches, while maintaining competitive performance compared to state-of-the-art competitors. The results suggest that AutoPBO offers a promising approach to automating local search solver design.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.04027</link>
<guid>https://arxiv.org/abs/2509.04027</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, Chain-of-Thought, CoT-Space, reasoning-level semantic space

Summary: 
Reinforcement Learning (RL) is crucial for improving the reasoning capabilities of Large Language Models (LLMs). Traditional token-level RL frameworks do not align with complex, multi-step thought processes like Chain-of-Thought (CoT). To bridge this gap, a new theoretical framework called CoT-Space is introduced, which transforms LLM reasoning into an optimization process within a continuous reasoning-level semantic space. Analysis shows that the convergence to an optimal CoT length is a result of balancing underfitting and overfitting. Empirical experiments support these theoretical findings and provide insights into phenomena like overthinking. The CoT-Space framework not only explains empirical observations but also lays a solid theoretical foundation for developing more effective and generalizable reasoning agents. 

<br /><br />Summary: <div>
arXiv:2509.04027v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has become a pivotal approach for enhancing the reasoning capabilities of Large Language Models (LLMs). However, a significant theoretical gap persists, as traditional token-level RL frameworks fail to align with the reasoning-level nature of complex, multi-step thought processes like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space, a novel theoretical framework that recasts LLM reasoning from a discrete token-prediction task to an optimization process within a continuous, reasoning-level semantic space. By analyzing this process from both a noise perspective and a risk perspective, we demonstrate that the convergence to an optimal CoT length is a natural consequence of the fundamental trade-off between underfitting and overfitting. Furthermore, extensive experiments provide strong empirical validation for our theoretical findings. Our framework not only provides a coherent explanation for empirical phenomena such as overthinking but also offers a solid theoretical foundation to guide the future development of more effective and generalizable reasoning agents.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oruga: An Avatar of Representational Systems Theory</title>
<link>https://arxiv.org/abs/2509.04041</link>
<guid>https://arxiv.org/abs/2509.04041</guid>
<content:encoded><![CDATA[
<div> Keywords: representations, Oruga, Representational Systems Theory, structure transfer, transformations

Summary:
Oruga, an implementation based on Representational Systems Theory (RST), aims to endow machines with the flexibility of human-like representations. The core of Oruga consists of data structures corresponding to RST concepts, a language for communication, and an engine for executing transformations through structure transfer. This method allows for the manipulation and transformation of representations in a way that mirrors human cognitive processes. By harnessing the power of RST, Oruga seeks to make machines more compatible with human use by enabling them to draw diagrams, change representations, and make creative analogies across different domains. Through the implementation of Oruga, researchers hope to advance the understanding of how machines can adapt and manipulate representations in a more flexible and human-like manner. <div>
arXiv:2509.04041v1 Announce Type: new 
Abstract: Humans use representations flexibly. We draw diagrams, change representations and exploit creative analogies across different domains. We want to harness this kind of power and endow machines with it to make them more compatible with human use. Previously we developed Representational Systems Theory (RST) to study the structure and transformations of representations. In this paper we present Oruga (caterpillar in Spanish; a symbol of transformation), an implementation of various aspects of RST. Oruga consists of a core of data structures corresponding to concepts in RST, a language for communicating with the core, and an engine for producing transformations using a method we call structure transfer. In this paper we present an overview of the core and language of Oruga, with a brief example of the kind of transformation that structure transfer can execute.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning</title>
<link>https://arxiv.org/abs/2509.04083</link>
<guid>https://arxiv.org/abs/2509.04083</guid>
<content:encoded><![CDATA[
<div> Neurosymbolic LLM, formal language, reasoning capabilities, syntactic reasoning, semantic reasoning <br />
Summary: <br />
This paper explores the impact of formal language choice on Neurosymbolic LLM reasoning. It examines four formal languages on three datasets and seven LLMs, revealing that the formal language selection influences both syntactic and semantic reasoning abilities. The study highlights the importance of considering the formal language in neurosymbolic reasoning approaches. <div>
arXiv:2509.04083v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve astonishing results on a wide range of tasks. However, their formal reasoning ability still lags behind. A promising approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators from natural to formal languages and symbolic solvers for deriving correct results. Still, the contributing factors to the success of Neurosymbolic LLM reasoning remain unclear. This paper demonstrates that one previously overlooked factor is the choice of the formal language. We introduce the intermediate language challenge: selecting a suitable formal language for neurosymbolic reasoning. By comparing four formal languages across three datasets and seven LLMs, we show that the choice of formal language affects both syntactic and semantic reasoning capabilities. We also discuss the varying effects across different LLMs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Reinforcement Learning and Search for Flight Trajectory Planning</title>
<link>https://arxiv.org/abs/2509.04100</link>
<guid>https://arxiv.org/abs/2509.04100</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Search-based path planners, Flight paths, Airliners, Optimization

Summary:
Reinforcement Learning (RL) combined with search-based path planners is explored for optimizing flight paths for airliners, especially in emergency situations. An RL Agent is trained to pre-compute near-optimal paths based on location and atmospheric data to speed up path planning. This guides the path planning solver to find a solution within a certain distance from the initial guess, reducing the search space and improving computation speed by up to 50%. Empirical results using Airbus aircraft's performance models demonstrate that fuel consumption remains nearly identical to an unconstrained solver, with deviations typically within 1%. While global optimality is not guaranteed, the approach proves effective in generating efficient flight paths in critical scenarios. 

<br /><br />Summary: <div>
arXiv:2509.04100v1 Announce Type: new 
Abstract: This paper explores the combination of Reinforcement Learning (RL) and search-based path planners to speed up the optimization of flight paths for airliners, where in case of emergency a fast route re-calculation can be crucial. The fundamental idea is to train an RL Agent to pre-compute near-optimal paths based on location and atmospheric data and use those at runtime to constrain the underlying path planning solver and find a solution within a certain distance from the initial guess. The approach effectively reduces the size of the solver's search space, significantly speeding up route optimization. Although global optimality is not guaranteed, empirical results conducted with Airbus aircraft's performance models show that fuel consumption remains nearly identical to that of an unconstrained solver, with deviations typically within 1%. At the same time, computation speed can be improved by up to 50% as compared to using a conventional solver alone.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker</title>
<link>https://arxiv.org/abs/2509.04125</link>
<guid>https://arxiv.org/abs/2509.04125</guid>
<content:encoded><![CDATA[
<div> bluffing, poker, DQN, CFR, Leduc Hold'em<br />
<br />
Summary: <br />
In the study of computer-poker algorithms, the focus has often been on performance metrics rather than bluffing behavior. This paper examines the bluffing behavior of two popular algorithms, DQN and CFR, in Leduc Hold'em, a simplified poker game. Through an experiment where DQN and CFR agents play against each other, it is found that both algorithms exhibit bluffing behavior, albeit in different ways. While both algorithms attempt to bluff at different rates, the success rate of their bluffs (opponent folding) is similar. This indicates that bluffing is a crucial aspect of the game itself and not specific to the algorithms being used. Future research should explore various bluffing styles and expand the study to the full game of poker. The code for this study is available on GitHub for reference.  <div>
arXiv:2509.04125v1 Announce Type: new 
Abstract: In the game of poker, being unpredictable, or bluffing, is an essential skill. When humans play poker, they bluff. However, most works on computer-poker focus on performance metrics such as win rates, while bluffing is overlooked. In this paper we study whether two popular algorithms, DQN (based on reinforcement learning) and CFR (based on game theory), exhibit bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed an experiment where we let the DQN and CFR agent play against each other while we log their actions. We find that both DQN and CFR exhibit bluffing behavior, but they do so in different ways. Although both attempt to perform bluffs at different rates, the percentage of successful bluffs (where the opponent folds) is roughly the same. This suggests that bluffing is an essential aspect of the game, not of the algorithm. Future work should look at different bluffing styles and at the full game of poker. Code at https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The human biological advantage over AI</title>
<link>https://arxiv.org/abs/2509.04130</link>
<guid>https://arxiv.org/abs/2509.04130</guid>
<content:encoded><![CDATA[
<div> AGI, artificial general intelligence, CNS, central nervous system, DNA, silicon <br />
<br />
Summary: Recent advances in AI suggest the potential for AI systems to surpass human capabilities, leading to the speculation of AI becoming a superior "digital species". However, the fundamental difference between humans and AI lies not in the brain, but in the central nervous system (CNS), which allows humans to experience emotions and understand the consequences of their actions. This emotional understanding is essential for developing ethical systems and qualifying as leaders of the universe. While AI systems may excel in various tasks and revolutionize society, the foundation for true leadership will always be DNA, as a CNS cannot be manufactured or simulated, and consciousness alone will not be enough to surpass humans. Thus, while AI may advance in many aspects, the core qualities that make humans uniquely suited for leadership in the universe stem from their biological nature and emotional capacity. <br /><br /> <div>
arXiv:2509.04130v1 Announce Type: new 
Abstract: Recent advances in AI raise the possibility that AI systems will one day be able to do anything humans can do, only better. If artificial general intelligence (AGI) is achieved, AI systems may be able to understand, reason, problem solve, create, and evolve at a level and speed that humans will increasingly be unable to match, or even understand. These possibilities raise a natural question as to whether AI will eventually become superior to humans, a successor "digital species", with a rightful claim to assume leadership of the universe. However, a deeper consideration suggests the overlooked differentiator between human beings and AI is not the brain, but the central nervous system (CNS), providing us with an immersive integration with physical reality. It is our CNS that enables us to experience emotion including pain, joy, suffering, and love, and therefore to fully appreciate the consequences of our actions on the world around us. And that emotional understanding of the consequences of our actions is what is required to be able to develop sustainable ethical systems, and so be fully qualified to be the leaders of the universe. A CNS cannot be manufactured or simulated; it must be grown as a biological construct. And so, even the development of consciousness will not be sufficient to make AI systems superior to humans. AI systems may become more capable than humans on almost every measure and transform our society. However, the best foundation for leadership of our universe will always be DNA, not silicon.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs</title>
<link>https://arxiv.org/abs/2509.04159</link>
<guid>https://arxiv.org/abs/2509.04159</guid>
<content:encoded><![CDATA[
<div> DSL, recipes, action graphs, culinary workflows, ontology <br />
<br />
Summary: <br />
This article introduces a domain-specific language for representing recipes as directed action graphs, allowing precise modeling of complex culinary workflows. The DSL captures processes, transfers, environments, concurrency, and compositional structure, enabling structured machine understanding and scalable automation of cooking processes. Initial evaluation on a full English breakfast recipe demonstrates the DSL's expressiveness and potential for automated recipe analysis and execution. The approach aims to formalize cooking procedures, addressing their inherent complexity and ambiguity. By using temporal graphs, the DSL facilitates precise interpretation of culinary processes in both home kitchens and professional settings. This work paves the way for an action-centric ontology for cooking, laying the foundation for future advancements in automated recipe handling and execution. <div>
arXiv:2509.04159v1 Announce Type: new 
Abstract: Formalizing cooking procedures remains a challenging task due to their inherent complexity and ambiguity. We introduce an extensible domain-specific language for representing recipes as directed action graphs, capturing processes, transfers, environments, concurrency, and compositional structure. Our approach enables precise, modular modeling of complex culinary workflows. Initial manual evaluation on a full English breakfast recipe demonstrates the DSL's expressiveness and suitability for future automated recipe analysis and execution. This work represents initial steps towards an action-centric ontology for cooking, using temporal graphs to enable structured machine understanding, precise interpretation, and scalable automation of culinary processes - both in home kitchens and professional culinary settings.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain size asymptotics for Markov logic networks</title>
<link>https://arxiv.org/abs/2509.04192</link>
<guid>https://arxiv.org/abs/2509.04192</guid>
<content:encoded><![CDATA[
<div> random structures, Markov logic network, domain size, limit behavior, soft constraints

Summary:
Random structures with domain sizes tending to infinity in Markov logic networks (MLNs) are studied. Three examples of MLNs are analyzed: quantifier-free MLNs with one relation symbol, MLNs favoring graphs with fewer triangles or vertices with high degrees. The analysis shows the diverse limit behaviors of random structures based on the soft constraints used in the MLNs. It is demonstrated that quantifier-free MLNs and lifted Bayesian networks are asymptotically incomparable. Additionally, the distribution determined by an MLN on large domains concentrates its probability mass on a different part of the possible worlds space compared to the uniform distribution. The study highlights the variety of behaviors MLNs exhibit with increasing domain sizes and the potential influence of the soft constraints on the outcomes.<br /><br />Summary: <div>
arXiv:2509.04192v1 Announce Type: new 
Abstract: A Markov logic network (MLN) determines a probability distribution on the set of structures, or ``possible worlds'', with an arbitrary finite domain. We study the properties of such distributions as the domain size tends to infinity. Three types of concrete examples of MLNs will be considered, and the properties of random structures with domain sizes tending to infinity will be studied: (1) Arbitrary quantifier-free MLNs over a language with only one relation symbol which has arity 1. In this case we give a pretty complete characterization of the possible limit behaviours of random structures. (2) An MLN that favours graphs with fewer triangles (or more generally, fewer k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law'' for first-order logic is obtained. (3) An MLN that favours graphs with fewer vertices with degree higher than a fixed (but arbitrary) number. The analysis shows that depending on which ``soft constraints'' an MLN uses the limit behaviour of random structures can be quite different, and the weights of the soft constraints may, or may not, have influence on the limit behaviour. It will also be demonstrated, using (1), that quantifier-free MLNs and lifted Bayesian networks (in a broad sense) are asymptotically incomparable, roughly meaning that there is a sequence of distributions on possible worlds with increasing domain sizes that can be defined by one of the formalisms but not even approximated by the other. In a rather general context it is also shown that on large domains the distribution determined by an MLN concentrates almost all its probability mass on a totally different part of the space of possible worlds than the uniform distribution does.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Quality of Gaming Narratives Co-created with AI</title>
<link>https://arxiv.org/abs/2509.04239</link>
<guid>https://arxiv.org/abs/2509.04239</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated game narratives, Delphi study, narrative design experts, Kano model, player satisfaction

Summary: 

This paper introduces a methodical approach for evaluating AI-generated game narratives by involving a panel of narrative design experts in a Delphi study. By blending insights from existing literature and expert input, the study identifies key dimensions of story quality and links them to the Kano model framework to gauge their influence on player satisfaction. The findings offer valuable guidance to game developers on prioritizing different quality aspects when collaborating with generative AI systems to craft engaging game narratives. <div>
arXiv:2509.04239v1 Announce Type: new 
Abstract: This paper proposes a structured methodology to evaluate AI-generated game narratives, leveraging the Delphi study structure with a panel of narrative design experts. Our approach synthesizes story quality dimensions from literature and expert insights, mapping them into the Kano model framework to understand their impact on player satisfaction. The results can inform game developers on prioritizing quality aspects when co-creating game narratives with generative AI.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation</title>
<link>https://arxiv.org/abs/2509.04310</link>
<guid>https://arxiv.org/abs/2509.04310</guid>
<content:encoded><![CDATA[
<div> evolutionary reinforcement learning, emotional expression, negotiations, large language models, adversarial counterparts
<br />
Summary:
The study introduces EvoEmo, an evolutionary reinforcement learning framework, to improve emotional expression in negotiations for Large Language Models (LLMs). Current LLM agents lack adaptive emotional responses, making them susceptible to manipulation. EvoEmo models emotional state transitions as a Markov Decision Process and utilizes genetic optimization to evolve high-reward emotion policies across various negotiation scenarios. Evaluation against two baselines shows that EvoEmo outperforms both in terms of success rates, efficiency, and buyer savings. The results emphasize the significance of adaptive emotional expression in enhancing the performance of LLM agents in complex, multi-turn negotiations.<br /><br /> <div>
arXiv:2509.04310v1 Announce Type: new 
Abstract: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \textit{complex}, \textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines -- vanilla strategies and fixed-emotion strategies -- for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes</title>
<link>https://arxiv.org/abs/2509.04317</link>
<guid>https://arxiv.org/abs/2509.04317</guid>
<content:encoded><![CDATA[
<div> AlphaZero, Monte Carlo planning, neural network, test environments, performance improvement <br />
<br />
Summary: This paper explores the challenge of deploying AlphaZero agents in changed test environments and proposes modifications to the standard framework to enhance performance. The AlphaZero framework combines Monte Carlo planning with a pre-trained neural network, assuming a static environment. However, when faced with potential changes in the test environment, simple adjustments to the framework can yield significant performance improvements. These modifications are shown to be effective even with limited planning resources available. The study highlights the adaptability of the AlphaZero framework and its potential for application in dynamic environments. The code for the modifications discussed in the paper is publicly accessible on GitHub, enabling researchers and practitioners to implement these enhancements in their own deployment scenarios. <div>
arXiv:2509.04317v1 Announce Type: new 
Abstract: The AlphaZero framework provides a standard way of combining Monte Carlo planning with prior knowledge provided by a previously trained policy-value neural network. AlphaZero usually assumes that the environment on which the neural network was trained will not change at test time, which constrains its applicability. In this paper, we analyze the problem of deploying AlphaZero agents in potentially changed test environments and demonstrate how the combination of simple modifications to the standard framework can significantly boost performance, even in settings with a low planning budget available. The code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psychologically Enhanced AI Agents</title>
<link>https://arxiv.org/abs/2509.04343</link>
<guid>https://arxiv.org/abs/2509.04343</guid>
<content:encoded><![CDATA[
<div> MBTI, Large Language Model, personality conditioning, agent behavior, psychological frameworks<br />
<br />
Summary: 
The article introduces MBTI-in-Thoughts, a framework that enhances Large Language Model agents through psychologically grounded personality conditioning based on the Myers-Briggs Type Indicator (MBTI). The method involves priming agents with distinct personality archetypes via prompt engineering, allowing control over behavior in cognition and affect. Results show that personality priming leads to consistent behavioral biases, with emotionally expressive agents excelling in narrative generation and analytically primed agents exhibiting stable strategies in game-theoretic situations. The framework supports structured multi-agent communication protocols and demonstrates that self-reflection prior to interaction improves cooperation and reasoning quality. Trait persistence is ensured through integration of the 16Personalities test for automated verification. The approach generalizes to other psychological frameworks such as Big Five, HEXACO, or Enneagram, establishing a foundation for psychologically enhanced AI agents without fine-tuning.<br /><br />Summary: <div>
arXiv:2509.04343v1 Announce Type: new 
Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of Large Language Model (LLM) agents through psychologically grounded personality conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology, cognition and affect. We show that such personality priming yields consistent, interpretable behavioral biases across diverse tasks: emotionally expressive agents excel in narrative generation, while analytically primed agents adopt more stable strategies in game-theoretic settings. Our framework supports experimenting with structured multi-agent communication protocols and reveals that self-reflection prior to interaction improves cooperation and reasoning quality. To ensure trait persistence, we integrate the official 16Personalities test for automated verification. While our focus is on MBTI, we show that our approach generalizes seamlessly to other psychological frameworks such as Big Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior design, we establish a foundation for psychologically enhanced AI agents without any fine-tuning.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory</title>
<link>https://arxiv.org/abs/2509.04439</link>
<guid>https://arxiv.org/abs/2509.04439</guid>
<content:encoded><![CDATA[
<div> memory, concept-level, continual learning, inference compute, abstract concepts

Summary:
In the realm of Language Model (LLMs), a new method has been introduced to enhance reasoning by utilizing external memory for persisting insights gained during inference. This method involves storing reusable, modular abstractions of solution traces in natural language, rather than exact instances. By selectively retrieving relevant concepts from memory for future queries, the approach enables test-time continual learning without weight updates. Results on the challenging ARC-AGI benchmark show a 7.5% relative improvement over a strong baseline method, with performance scaling with inference compute. The study highlights the effectiveness of abstract concepts in memory design, consistently outperforming baseline methods. Furthermore, dynamic memory updating during test-time proves superior to fixed memory settings, demonstrating that continual problem solving and abstraction of patterns lead to self-improvement. The code for this method is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2509.04439v1 Announce Type: new 
Abstract: While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query/response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. On the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, we confirm that dynamically updating memory during test-time outperforms an otherwise identical fixed memory setting with additional attempts, supporting the hypothesis that solving more problems and abstracting more patterns to memory enables further solutions in a form of self-improvement. Code available at https://github.com/matt-seb-ho/arc_memo.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiND: A Neural Discriminator-Decoder for Accurate Bimanual Trajectory Prediction in Brain-Computer Interfaces</title>
<link>https://arxiv.org/abs/2509.03521</link>
<guid>https://arxiv.org/abs/2509.03521</guid>
<content:encoded><![CDATA[
<div> decoding, bimanual hand movements, intracortical recordings, brain-computer interfaces, BiND<br />
Summary:<br />
The study introduces BiND, a model for decoding bimanual hand movements from intracortical recordings in brain-computer interfaces. BiND classifies motion type and predicts continuous 2D hand velocities using specialized GRU-based decoders with a trial-relative time index. Benchmarking against six other models on a tetraplegic patient dataset, BiND outperforms all models with a mean $R^2$ of 0.76 for unimanual and 0.69 for bimanual trajectory prediction. It shows greater robustness to session variability, with accuracy improvements of up to 4% in cross-session analyses compared to GRU. Task-aware discrimination and temporal modeling enhance bimanual decoding, highlighting the effectiveness of BiND in decoding complex hand movements. <br /> <div>
arXiv:2509.03521v1 Announce Type: cross 
Abstract: Decoding bimanual hand movements from intracortical recordings remains a critical challenge for brain-computer interfaces (BCIs), due to overlapping neural representations and nonlinear interlimb interactions. We introduce BiND (Bimanual Neural Discriminator-Decoder), a two-stage model that first classifies motion type (unimanual left, unimanual right, or bimanual) and then uses specialized GRU-based decoders, augmented with a trial-relative time index, to predict continuous 2D hand velocities. We benchmark BiND against six state-of-the-art models (SVR, XGBoost, FNN, CNN, Transformer, GRU) on a publicly available 13-session intracortical dataset from a tetraplegic patient. BiND achieves a mean $R^2$ of 0.76 ($\pm$0.01) for unimanual and 0.69 ($\pm$0.03) for bimanual trajectory prediction, surpassing the next-best model (GRU) by 2% in both tasks. It also demonstrates greater robustness to session variability than all other benchmarked models, with accuracy improvements of up to 4% compared to GRU in cross-session analyses. This highlights the effectiveness of task-aware discrimination and temporal modeling in enhancing bimanual decoding.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies</title>
<link>https://arxiv.org/abs/2509.03525</link>
<guid>https://arxiv.org/abs/2509.03525</guid>
<content:encoded><![CDATA[
<div> adults, Alzheimer disease, speech-based screening, language model adaptation, dementia detection <br />
Summary: <br />
The study focused on the use of speech-based screening for detecting dementia in adults with Alzheimer's disease. It found that over half of US adults with dementia are undiagnosed, highlighting the need for scalable detection approaches. The study compared different adaptation strategies for large language models using the DementiaBank speech corpus. Results showed that class-centroid demonstrations were most effective for in-context learning, reasoning improved smaller models, and token-level fine-tuning generally produced the best outcomes. Adding a classification head significantly enhanced the performance of underperforming models. The study also found that multimodal audio-text models performed well but did not surpass the top text-only models. The findings emphasize that adaptation strategies, including demonstration selection, reasoning design, and tuning methods, play a crucial role in speech-based dementia detection and suggest that properly adapted open-weight models can match or exceed commercial systems. <div>
arXiv:2509.03525v1 Announce Type: cross 
Abstract: Over half of US adults with Alzheimer disease and related dementias remain undiagnosed, and speech-based screening offers a scalable detection approach. We compared large language model adaptation strategies for dementia detection using the DementiaBank speech corpus, evaluating nine text-only models and three multimodal audio-text models on recordings from DementiaBank speech corpus. Adaptations included in-context learning with different demonstration selection policies, reasoning-augmented prompting, parameter-efficient fine-tuning, and multimodal integration. Results showed that class-centroid demonstrations achieved the highest in-context learning performance, reasoning improved smaller models, and token-level fine-tuning generally produced the best scores. Adding a classification head substantially improved underperforming models. Among multimodal models, fine-tuned audio-text systems performed well but did not surpass the top text-only models. These findings highlight that model adaptation strategies, including demonstration selection, reasoning design, and tuning method, critically influence speech-based dementia detection, and that properly adapted open-weight models can match or exceed commercial systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model</title>
<link>https://arxiv.org/abs/2509.03527</link>
<guid>https://arxiv.org/abs/2509.03527</guid>
<content:encoded><![CDATA[
<div> Keywords: multilevel analysis, cryptocurrency news, Mistral 7B, knowledge graph, quantitative analytics

Summary: 
The paper presents a multilevel multitask analysis of cryptocurrency news using a fine-tuned Mistral 7B large language model with retrieval-augmented generation (RAG). The model generates graph and text summaries with sentiment scores and JSON representations on the first level of analytics. Higher levels employ hierarchical stacking to consolidate summaries into comprehensive reports. Incorporating both graph and text summaries offers complementary perspectives on cryptocurrency news. The model is fine-tuned using 4-bit quantization with the PEFT/LoRA approach, enhancing the quality of the analysis. By representing cryptocurrency news as a knowledge graph, the model helps eliminate issues related to large language model hallucinations. The results demonstrate the effectiveness of using the fine-tuned Mistral 7B LLM model for conducting informative qualitative and quantitative analytics, providing valuable insights into the cryptocurrency market. 

<br /><br />Summary: <div>
arXiv:2509.03527v1 Announce Type: cross 
Abstract: In the paper, we consider multilevel multitask analysis of cryptocurrency news using a fine-tuned Mistral 7B large language model with retrieval-augmented generation (RAG).
  On the first level of analytics, the fine-tuned model generates graph and text summaries with sentiment scores as well as JSON representations of summaries. Higher levels perform hierarchical stacking that consolidates sets of graph-based and text-based summaries as well as summaries of summaries into comprehensive reports. The combination of graph and text summaries provides complementary views of cryptocurrency news. The model is fine-tuned with 4-bit quantization using the PEFT/LoRA approach. The representation of cryptocurrency news as knowledge graph can essentially eliminate problems with large language model hallucinations.
  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM models for multilevel cryptocurrency news analysis can conduct informative qualitative and quantitative analytics, providing important insights.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages</title>
<link>https://arxiv.org/abs/2509.03529</link>
<guid>https://arxiv.org/abs/2509.03529</guid>
<content:encoded><![CDATA[
<div> Earnings calls, sentiment analysis, multi-modal framework, hierarchical discourse trees, contrastive learning<br />
<br />
Summary: This paper introduces a novel multi-modal framework for analyzing earnings calls, encoding them as hierarchical discourse trees to capture the layered structure of interactions. The framework generates semantically rich embeddings enriched with emotional signals, structured metadata, coherence scores, topic labels, and answer coverage assessments. A two-stage transformer architecture is used to encode content and discourse metadata at the node level and synthesize a global embedding for the entire conference. Experimental results show that the embeddings reflect affective tone, structural logic, and thematic alignment, providing stable and meaningful representations. The proposed system has practical utility for tasks such as financial forecasting and discourse evaluation, with potential applications in other high-stakes communication domains like tele-medicine, education, and politics. <div>
arXiv:2509.03529v1 Announce Type: cross 
Abstract: Earnings calls represent a uniquely rich and semi-structured source of financial communication, blending scripted managerial commentary with unscripted analyst dialogue. Although recent advances in financial sentiment analysis have integrated multi-modal signals, such as textual content and vocal tone, most systems rely on flat document-level or sentence-level models, failing to capture the layered discourse structure of these interactions. This paper introduces a novel multi-modal framework designed to generate semantically rich and structurally aware embeddings of earnings calls, by encoding them as hierarchical discourse trees. Each node, comprising either a monologue or a question-answer pair, is enriched with emotional signals derived from text, audio, and video, as well as structured metadata including coherence scores, topic labels, and answer coverage assessments. A two-stage transformer architecture is proposed: the first encodes multi-modal content and discourse metadata at the node level using contrastive learning, while the second synthesizes a global embedding for the entire conference. Experimental results reveal that the resulting embeddings form stable, semantically meaningful representations that reflect affective tone, structural logic, and thematic alignment. Beyond financial reporting, the proposed system generalizes to other high-stakes unscripted communicative domains such as tele-medicine, education, and political discourse, offering a robust and explainable approach to multi-modal discourse representation. This approach offers practical utility for downstream tasks such as financial forecasting and discourse evaluation, while also providing a generalizable method applicable to other domains involving high-stakes communication.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Detection of Hallucinated Entities in Long-Form Generation</title>
<link>https://arxiv.org/abs/2509.03531</link>
<guid>https://arxiv.org/abs/2509.03531</guid>
<content:encoded><![CDATA[
<div> Entity-level hallucinations, long-form generations, scalable method, hallucination classifiers, annotation methodology<br />
<br />Summary:
Large language models are increasingly used in critical applications, where hallucinations can have serious consequences. Existing detection methods are not practical for real-world use. A new cheap and scalable method for identifying hallucinated tokens in long-form text has been developed, focusing on entity-level hallucinations. An annotation methodology leveraging web search was used to train effective hallucination classifiers, outperforming baselines consistently. The classifiers are also successful in short-form question-answering scenarios and in detecting incorrect answers in mathematical reasoning tasks. The annotations collected for one model can be used to train classifiers for other models. The datasets created are released publicly, indicating a promising scalable approach for real-world hallucination detection.<br /><br />Summary: <div>
arXiv:2509.03531v1 Announce Type: cross 
Abstract: Large language models are now routinely used in high-stakes applications where hallucinations can cause serious harm, such as medical consultations or legal advice. Existing hallucination detection methods, however, are impractical for real-world use, as they are either limited to short factual queries or require costly external verification. We present a cheap, scalable method for real-time identification of hallucinated tokens in long-form generations, and scale it effectively to 70B parameter models. Our approach targets \emph{entity-level hallucinations} -- e.g., fabricated names, dates, citations -- rather than claim-level, thereby naturally mapping to token-level labels and enabling streaming detection. We develop an annotation methodology that leverages web search to annotate model responses with grounded labels indicating which tokens correspond to fabricated entities. This dataset enables us to train effective hallucination classifiers with simple and efficient methods such as linear probes. Evaluating across four model families, our classifiers consistently outperform baselines on long-form responses, including more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for Llama-3.3-70B), and are also an improvement in short-form question-answering settings. Moreover, despite being trained only with entity-level labels, our probes effectively detect incorrect answers in mathematical reasoning tasks, indicating generalization beyond entities. While our annotation methodology is expensive, we find that annotated responses from one model can be used to train effective classifiers on other models; accordingly, we publicly release our datasets to facilitate reuse. Overall, our work suggests a promising new approach for scalable, real-world hallucination detection.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuesGenie: Intelligent Multimodal Question Generation</title>
<link>https://arxiv.org/abs/2509.03535</link>
<guid>https://arxiv.org/abs/2509.03535</guid>
<content:encoded><![CDATA[
<div> Keywords: educational resources, question generation system, multi-modal input, reinforcement learning, interactive interface

Summary:
This project focuses on addressing the challenge of the lack of practice materials tailored to the abundant educational resources available to learners. The system developed includes a multi-modal question generation system with components such as multi-modal input handling, question generation, reinforcement learning from human feedback (RLHF), and an end-to-end interactive interface. By utilizing these components, the system aims to automate question generation, ensuring diversity in question types and balancing resource efficiency, robust functionality, and user experience. This project paves the way for scalable and intelligent question generation, offering a solution to the current gap in practice materials tailored to educational resources. 

<br /><br />Summary: <div>
arXiv:2509.03535v1 Announce Type: cross 
Abstract: In today's information-rich era, learners have access to abundant educational resources, but the lack of practice materials tailored to these resources presents a significant challenge. This project addresses that gap by developing a multi-modal question generation system that can automatically generate diverse question types from various content formats. The system features four major components: multi-modal input handling, question generation, reinforcement learning from human feedback (RLHF), and an end-to-end interactive interface. This project lays the foundation for automated, scalable, and intelligent question generation, carefully balancing resource efficiency, robust functionality and a smooth user experience.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2509.03537</link>
<guid>https://arxiv.org/abs/2509.03537</guid>
<content:encoded><![CDATA[
<div> abstraction, computational patterns, computer science, large language models, reinforcement learning<br />
Summary:<br />
- Abstraction, essential in computer science, is crucial for problem-solving and large language models (LLMs).<br />
- Current LLM training methods focus on pattern recognition and lack explicit abstraction training.<br />
- AR$^2$ introduces a framework to enhance LLMs' abstraction abilities through Adversarial Reinforcement Learning.<br />
- A teacher model transforms problems into narrative-rich descriptions to challenge LLMs.<br />
- The student model is trained to extract underlying computational kernels from these complex narrative problems.<br />
- Experimental results show AR$^2 improves LLM accuracy on challenging programming tasks, highlighting the importance of abstraction in LLM generalization.<br /> <div>
arXiv:2509.03537v1 Announce Type: cross 
Abstract: Abstraction--the ability to recognize and distill essential computational patterns from complex problem statements--is a foundational skill in computer science, critical both for human problem-solvers and coding-oriented large language models (LLMs). Despite recent advances in training LLMs for code generation using reinforcement learning (RL), most existing approaches focus primarily on superficial pattern recognition, overlooking explicit training for abstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement Learning for Abstract Reasoning), a novel framework explicitly designed to enhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to transform kernel problems into narrative-rich, challenging descriptions without changing their fundamental logic. Simultaneously, a student coding model is trained to solve these complex narrative problems by extracting their underlying computational kernels. Experimental results demonstrate that AR$^2$ substantially improves the student model's accuracy on previously unseen, challenging programming tasks, underscoring abstraction as a key skill for enhancing LLM generalization.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction</title>
<link>https://arxiv.org/abs/2509.03540</link>
<guid>https://arxiv.org/abs/2509.03540</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, Knowledge Graphs, Factual Accuracy, Interpretability

Summary:
Large Language Models (LLMs) often struggle with factual consistency due to limited memory. Retrieval-Augmented Generation (RAG) methods use external knowledge for better answers. This study introduces a method that dynamically constructs and expands knowledge graphs (KGs) during inference, integrating internal and external knowledge. It extracts a seed KG from the question, expands it using LLM's latent knowledge, and refines it through external retrieval. The approach improves factual accuracy, answer precision, and interpretability on three factual QA benchmarks compared to baseline methods. This inference-time KG construction shows promise in enhancing LLM factuality in a structured, interpretable, and scalable manner.<br /><br />Summary: <div>
arXiv:2509.03540v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often struggle with producing factually consistent answers due to limitations in their parametric memory. Retrieval-Augmented Generation (RAG) methods address this issue by incorporating external knowledge from trusted sources at inference time. However, such methods typically treat knowledge as unstructured text, which limits their ability to support compositional reasoning and identify factual inconsistencies. To overcome these limitations, we propose a novel framework that dynamically constructs and expands knowledge graphs (KGs) during inference, integrating both internal knowledge extracted from LLMs and external information retrieved from external sources. Our method begins by extracting a seed KG from the question via prompting, followed by iterative expansion using the LLM's latent knowledge. The graph is then selectively refined through external retrieval, enhancing factual coverage and correcting inaccuracies. We evaluate our approach on three diverse factual QA benchmarks, demonstrating consistent improvements in factual accuracy, answer precision, and interpretability over baseline prompting and static KG-augmented methods. Our findings suggest that inference-time KG construction is a promising direction for enhancing LLM factuality in a structured, interpretable, and scalable manner.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A software security review on Uganda's Mobile Money Services: Dr. Jim Spire's tweets sentiment analysis</title>
<link>https://arxiv.org/abs/2509.03545</link>
<guid>https://arxiv.org/abs/2509.03545</guid>
<content:encoded><![CDATA[
<div> Keywords: mobile money, security mechanisms, #StopAirtelThefty, Uganda, financial inclusion

Summary: 
This study examines the #StopAirtelThefty Twitter campaign in Uganda in August 2025, triggered by a security incident involving mobile money. The campaign highlighted public concerns over the safety of mobile money and revealed security vulnerabilities and user dissatisfaction. Qualitative analysis of the complaints raised during the campaign identified key themes related to security gaps experienced by users. These findings offer insights into the specific challenges faced by mobile money users in Uganda and provide valuable information for providers, policymakers, and the future of secure digital finance in the country. The study underscores the importance of addressing security issues in mobile money systems to ensure trust and confidence among users and to promote financial inclusion. <br /><br />Summary: <div>
arXiv:2509.03545v1 Announce Type: cross 
Abstract: The proliferation of mobile money in Uganda has been a cornerstone of financial inclusion, yet its security mechanisms remain a critical concern. This study investigates a significant public response to perceived security failures: the #StopAirtelThefty Twitter campaign of August 2025 Sparked by an incident publicized by Dr. Jim Spire Ssentongo where a phone thief accessed a victim's account, withdrew funds, and procured a loan, the campaign revealed deep seated public anxiety over the safety of mobile money. This research employs qualitative analysis to systematically examine the complaints raised during this campaign, extracting key themes related to security vulnerabilities and user dissatisfaction. By synthesizing these public sentiments, the paper provides crucial insights into the specific security gaps experienced by users and situates these findings within the larger framework of Uganda's mobile money regulatory and operational environment. The study concludes with implications for providers, policymakers, and the future of secure digital finance in Uganda.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric</title>
<link>https://arxiv.org/abs/2509.03594</link>
<guid>https://arxiv.org/abs/2509.03594</guid>
<content:encoded><![CDATA[
<div> optimisers, neural networks, Riemannian metric, loss landscape, training<br />
<br />
Summary:<br />
The article introduces a new class of optimisers for training neural networks that utilize the Riemannian metric inherent in the loss landscape's higher-dimensional space embedding. Comparisons with SGD, Adam, AdamW, and Muon show the new optimisers to be highly effective in low-dimensional examples and slightly better than current methods. The optimisers automatically adjust the learning rate in high-curvature regions, serving as a form of gradient clipping. They also implement effective scheduled learning rates and suggest decoupled weight decay. The new approach can enhance existing preconditioning methods with similar computational complexity to Adam. <div>
arXiv:2509.03594v1 Announce Type: cross 
Abstract: We present a class of novel optimisers for training neural networks that makes use of the Riemannian metric naturally induced when the loss landscape is embedded in higher-dimensional space. This is the same metric that underlies common visualisations of loss landscapes. By taking this geometric perspective literally and using the induced metric, we develop a new optimiser and compare it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of tasks and architectures. Empirically, we conclude that this new class of optimisers is highly effective in low dimensional examples, and provides slight improvement over state-of-the-art methods for training neural networks. These new optimisers have theoretically desirable properties. In particular, the effective learning rate is automatically decreased in regions of high curvature acting as a smoothed out form of gradient clipping. Similarly, one variant of these optimisers can also be viewed as inducing an effective scheduled learning rate and decoupled weight decay is the natural choice from our geometric perspective. The basic method can be used to modify any existing preconditioning method. The new optimiser has a computational complexity comparable to that of Adam.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition</title>
<link>https://arxiv.org/abs/2509.03615</link>
<guid>https://arxiv.org/abs/2509.03615</guid>
<content:encoded><![CDATA[
<div> Keywords: Optical Character Recognition, Large Vision-Language Models, Edge deployment, Efficiency, Affordability

Summary:<br /><br />
The article discusses the challenges of Optical Character Recognition (OCR) in diverse real-world images and the potential of Large Vision-Language Models (LVLMs) to enhance OCR capabilities. The study introduces Sprinklr-Edge-OCR, an OCR system optimized for edge deployment in resource-constrained environments. A comparative evaluation of five state-of-the-art LVLMs and two traditional OCR systems on a multilingual image dataset is presented, covering metrics such as accuracy, efficiency, and deployment cost. The results show that traditional OCR systems, specifically Sprinklr-Edge-OCR, outperform LVLMs in efficiency, with faster image processing and lower cost. Despite LVLMs like Qwen achieving high precision, Sprinklr-Edge-OCR demonstrates superior overall performance in terms of F1 score and practical applicability for edge deployment. This highlights the importance of considering compute requirements, latency, and affordability in selecting OCR systems for edge deployment scenarios. <div>
arXiv:2509.03615v1 Announce Type: cross 
Abstract: Optical Character Recognition (OCR) in multilingual, noisy, and diverse real-world images remains a significant challenge for optical character recognition systems. With the rise of Large Vision-Language Models (LVLMs), there is growing interest in their ability to generalize and reason beyond fixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR system built specifically optimized for edge deployment in resource-constrained environments. We present a large-scale comparative evaluation of five state-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two traditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly hand annotated dataset of multilingual (54 languages) images. Our benchmark covers a broad range of metrics including accuracy, semantic consistency, language coverage, computational efficiency (latency, memory, GPU usage), and deployment cost. To better reflect real-world applicability, we also conducted edge case deployment analysis, evaluating model performance on CPU only environments. Among the results, Qwen achieved the highest precision (0.54), while Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and outperformed others in efficiency, processing images 35 faster (0.17 seconds per image on average) and at less than 0.01 of the cost (0.006 USD per 1,000 images) compared to LVLM. Our findings demonstrate that the most optimal OCR systems for edge deployment are the traditional ones even in the era of LLMs due to their low compute requirements, low latency, and very high affordability.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds</title>
<link>https://arxiv.org/abs/2509.03633</link>
<guid>https://arxiv.org/abs/2509.03633</guid>
<content:encoded><![CDATA[
arXiv:2509.03633v1 Announce Type: cross 
Abstract: Close-range laser scanning provides detailed 3D captures of forest stands but requires efficient software for processing 3D point cloud data and extracting individual trees. Although recent studies have introduced deep learning methods for tree instance segmentation, these approaches require large annotated datasets and substantial computational resources. As a resource-efficient alternative, we present a revised version of the treeX algorithm, an unsupervised method that combines clustering-based stem detection with region growing for crown delineation. While the original treeX algorithm was developed for personal laser scanning (PLS) data, we provide two parameter presets, one for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one for UAV-borne laser scanning (ULS). We evaluated the method on six public datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham Woods) and compared it to six open-source methods (original treeX, treeiso, RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original treeX algorithm, our revision reduces runtime and improves accuracy, with instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data. For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original algorithm fails to segment any correct instances. For TLS and PLS data, our algorithm achieves accuracy similar to recent open-source methods, including deep learning. Given its algorithmic design, we see two main applications for our method: (1) as a resource-efficient alternative to deep learning approaches in scenarios where the data characteristics align with the method design (sufficient stem visibility and point density), and (2) for the semi-automatic generation of labels for deep learning models. To enable broader adoption, we provide an open-source Python implementation in the pointtree package.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records</title>
<link>https://arxiv.org/abs/2509.03643</link>
<guid>https://arxiv.org/abs/2509.03643</guid>
<content:encoded><![CDATA[
arXiv:2509.03643v1 Announce Type: cross 
Abstract: Electronic Health Records (EHRs) provide a rich, longitudinal view of patient health and hold significant potential for advancing clinical decision support, risk prediction, and data-driven healthcare research. However, most artificial intelligence (AI) models for EHRs are designed for narrow, single-purpose tasks, limiting their generalizability and utility in real-world settings. Here, we present CEHR-GPT, a general-purpose foundation model for EHR data that unifies three essential capabilities - feature representation, zero-shot prediction, and synthetic data generation - within a single architecture. To support temporal reasoning over clinical sequences, \cehrgpt{} incorporates a novel time-token-based learning framework that explicitly encodes patients' dynamic timelines into the model structure. CEHR-GPT demonstrates strong performance across all three tasks and generalizes effectively to external datasets through vocabulary expansion and fine-tuning. Its versatility enables rapid model development, cohort discovery, and patient outcome forecasting without the need for task-specific retraining.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators</title>
<link>https://arxiv.org/abs/2509.03647</link>
<guid>https://arxiv.org/abs/2509.03647</guid>
<content:encoded><![CDATA[
arXiv:2509.03647v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly serve as automated evaluators, yet they suffer from "self-preference bias": a tendency to favor their own outputs over those of other models. This bias undermines fairness and reliability in evaluation pipelines, particularly for tasks like preference tuning and model routing. We investigate whether lightweight steering vectors can mitigate this problem at inference time without retraining. We introduce a curated dataset that distinguishes self-preference bias into justified examples of self-preference and unjustified examples of self-preference, and we construct steering vectors using two methods: Contrastive Activation Addition (CAA) and an optimization-based approach. Our results show that steering vectors can reduce unjustified self-preference bias by up to 97\%, substantially outperforming prompting and direct preference optimization baselines. Yet steering vectors are unstable on legitimate self-preference and unbiased agreement, implying self-preference spans multiple or nonlinear directions. This underscores both their promise and limits as safeguards for LLM-as-judges and motivates more robust interventions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Virtuoso: A Latent Diffusion Transformer Model for Goal-Conditioned Trajectory Planning</title>
<link>https://arxiv.org/abs/2509.03658</link>
<guid>https://arxiv.org/abs/2509.03658</guid>
<content:encoded><![CDATA[
arXiv:2509.03658v1 Announce Type: cross 
Abstract: The ability to generate a diverse and plausible distribution of future trajectories is a critical capability for autonomous vehicle planning systems. While recent generative models have shown promise, achieving high fidelity, computational efficiency, and precise control remains a significant challenge. In this paper, we present the \textbf{Efficient Virtuoso}, a conditional latent diffusion model for goal-conditioned trajectory planning. Our approach introduces a novel two-stage normalization pipeline that first scales trajectories to preserve their geometric aspect ratio and then normalizes the resulting PCA latent space to ensure a stable training target. The denoising process is performed efficiently in this low-dimensional latent space by a simple MLP denoiser, which is conditioned on a rich scene context fused by a powerful Transformer-based StateEncoder. We demonstrate that our method achieves state-of-the-art performance on the Waymo Open Motion Dataset, reaching a \textbf{minADE of 0.25}. Furthermore, through a rigorous ablation study on goal representation, we provide a key insight: while a single endpoint goal can resolve strategic ambiguity, a richer, multi-step sparse route is essential for enabling the precise, high-fidelity tactical execution that mirrors nuanced human driving behavior.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insights from Gradient Dynamics: Gradient Autoscaled Normalization</title>
<link>https://arxiv.org/abs/2509.03677</link>
<guid>https://arxiv.org/abs/2509.03677</guid>
<content:encoded><![CDATA[
arXiv:2509.03677v1 Announce Type: cross 
Abstract: Gradient dynamics play a central role in determining the stability and generalization of deep neural networks. In this work, we provide an empirical analysis of how variance and standard deviation of gradients evolve during training, showing consistent changes across layers and at the global scale in convolutional networks. Motivated by these observations, we propose a hyperparameter-free gradient normalization method that aligns gradient scaling with their natural evolution. This approach prevents unintended amplification, stabilizes optimization, and preserves convergence guarantees. Experiments on the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN demonstrate that our method maintains or improves test accuracy even under strong generalization. Beyond practical performance, our study highlights the importance of directly tracking gradient dynamics, aiming to bridge the gap between theoretical expectations and empirical behaviors, and to provide insights for future optimization research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LuxDiT: Lighting Estimation with Video Diffusion Transformer</title>
<link>https://arxiv.org/abs/2509.03680</link>
<guid>https://arxiv.org/abs/2509.03680</guid>
<content:encoded><![CDATA[
arXiv:2509.03680v1 Announce Type: cross 
Abstract: Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures</title>
<link>https://arxiv.org/abs/2509.03695</link>
<guid>https://arxiv.org/abs/2509.03695</guid>
<content:encoded><![CDATA[
arXiv:2509.03695v1 Announce Type: cross 
Abstract: The rise of foundation models (FMs) has reshaped the landscape of machine learning. As these models continued to grow, leveraging geo-distributed data from wireless devices has become increasingly critical, giving rise to federated foundation models (FFMs). More recently, FMs have evolved into multi-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diverse modalities across multiple tasks, which motivates a new underexplored paradigm: M3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs by proposing hierarchical federated foundation models (HF-FMs), which in turn expose two overlooked heterogeneity dimensions to fog/edge networks that have a direct impact on these emerging models: (i) heterogeneity in collected modalities and (ii) heterogeneity in executed tasks across fog/edge nodes. HF-FMs strategically align the modular structure of M3T FMs, comprising modality encoders, prompts, mixture-of-experts (MoEs), adapters, and task heads, with the hierarchical nature of fog/edge infrastructures. Moreover, HF-FMs enable the optional usage of device-to-device (D2D) communications, enabling horizontal module relaying and localized cooperative training among nodes when feasible. Through delving into the architectural design of HF-FMs, we highlight their unique capabilities along with a series of tailored future research directions. Finally, to demonstrate their potential, we prototype HF-FMs in a wireless network setting and release the open-source code for the development of HF-FMs with the goal of fostering exploration in this untapped field (GitHub: https://github.com/payamsiabd/M3T-FFM).
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Federated Learning to $\mathbb{X}$-Learning: Breaking the Barriers of Decentrality Through Random Walks</title>
<link>https://arxiv.org/abs/2509.03709</link>
<guid>https://arxiv.org/abs/2509.03709</guid>
<content:encoded><![CDATA[
arXiv:2509.03709v1 Announce Type: cross 
Abstract: We provide our perspective on $\mathbb{X}$-Learning ($\mathbb{X}$L), a novel distributed learning architecture that generalizes and extends the concept of decentralization. Our goal is to present a vision for $\mathbb{X}$L, introducing its unexplored design considerations and degrees of freedom. To this end, we shed light on the intuitive yet non-trivial connections between $\mathbb{X}$L, graph theory, and Markov chains. We also present a series of open research directions to stimulate further research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection</title>
<link>https://arxiv.org/abs/2509.03725</link>
<guid>https://arxiv.org/abs/2509.03725</guid>
<content:encoded><![CDATA[
arXiv:2509.03725v1 Announce Type: cross 
Abstract: We present the novel approach for stance detection across domains and targets, Metric Learning-Based Few-Shot Learning for Cross-Target and Cross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with triplet loss to capture semantic similarities and differences between stance targets, enhancing domain adaptation. By constructing a discriminative embedding space, MLSD allows a cross-target or cross-domain stance detection model to acquire useful examples from new target domains. We evaluate MLSD in multiple cross-target and cross-domain scenarios across two datasets, showing statistically significant improvement in stance detection performance across six widely used stance detection models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Entropy Regularization for Geometry and Neural Networks</title>
<link>https://arxiv.org/abs/2509.03733</link>
<guid>https://arxiv.org/abs/2509.03733</guid>
<content:encoded><![CDATA[
arXiv:2509.03733v1 Announce Type: cross 
Abstract: We introduce a differentiable estimator of range-partition entropy, a recent concept from computational geometry that enables algorithms to adapt to the "sortedness" of their input. While range-partition entropy provides strong guarantees in algorithm design, it has not yet been made accessible to deep learning. In this work, we (i) propose the first differentiable approximation of range-partition entropy, enabling its use as a trainable loss or regularizer; (ii) design EntropyNet, a neural module that restructures data into low-entropy forms to accelerate downstream instance-optimal algorithms; and (iii) extend this principle beyond geometry by applying entropy regularization directly to Transformer attention. Across tasks, we demonstrate that differentiable entropy improves efficiency without degrading correctness: in geometry, our method achieves up to $4.1\times$ runtime speedups with negligible error ($<0.2%$); in deep learning, it induces structured attention patterns that yield 6% higher accuracy at 80% sparsity compared to L1 baselines. Our theoretical analysis provides approximation bounds for the estimator, and extensive ablations validate design choices. These results suggest that entropy-bounded computation is not only theoretically elegant but also a practical mechanism for adaptive learning, efficiency, and structured representation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces</title>
<link>https://arxiv.org/abs/2509.03738</link>
<guid>https://arxiv.org/abs/2509.03738</guid>
<content:encoded><![CDATA[
arXiv:2509.03738v1 Announce Type: cross 
Abstract: We frame the problem of unifying representations in neural models as one of sparse model recovery and introduce a framework that extends sparse autoencoders (SAEs) to lifted spaces and infinite-dimensional function spaces, enabling mechanistic interpretability of large neural operators (NO). While the Platonic Representation Hypothesis suggests that neural networks converge to similar representations across architectures, the representational properties of neural operators remain underexplored despite their growing importance in scientific computing. We compare the inference and training dynamics of SAEs, lifted-SAE, and SAE neural operators. We highlight how lifting and operator modules introduce beneficial inductive biases, enabling faster recovery, improved recovery of smooth concepts, and robust inference across varying resolutions, a property unique to neural operators.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Gaze Analytics for ELA Instruction: A User-Centered Dashboard with Conversational AI Support</title>
<link>https://arxiv.org/abs/2509.03741</link>
<guid>https://arxiv.org/abs/2509.03741</guid>
<content:encoded><![CDATA[
arXiv:2509.03741v1 Announce Type: cross 
Abstract: Eye-tracking offers rich insights into student cognition and engagement, but remains underutilized in classroom-facing educational technology due to challenges in data interpretation and accessibility. In this paper, we present the iterative design and evaluation of a gaze-based learning analytics dashboard for English Language Arts (ELA), developed through five studies involving teachers and students. Guided by user-centered design and data storytelling principles, we explored how gaze data can support reflection, formative assessment, and instructional decision-making. Our findings demonstrate that gaze analytics can be approachable and pedagogically valuable when supported by familiar visualizations, layered explanations, and narrative scaffolds. We further show how a conversational agent, powered by a large language model (LLM), can lower cognitive barriers to interpreting gaze data by enabling natural language interactions with multimodal learning analytics. We conclude with design implications for future EdTech systems that aim to integrate novel data modalities in classroom contexts.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification</title>
<link>https://arxiv.org/abs/2509.03754</link>
<guid>https://arxiv.org/abs/2509.03754</guid>
<content:encoded><![CDATA[
arXiv:2509.03754v1 Announce Type: cross 
Abstract: Responding to rising global food security needs, precision agriculture and deep learning-based plant disease diagnosis have become crucial. Yet, deploying high-precision models on edge devices is challenging. Most lightweight networks use attention mechanisms designed for generic object recognition, which poorly capture subtle pathological features like irregular lesion shapes and complex textures. To overcome this, we propose a twofold solution: first, using a training-free neural architecture search method (DeepMAD) to create an efficient network backbone for edge devices; second, introducing the Shape-Texture Attention Module (STAM). STAM splits attention into two branches -- one using deformable convolutions (DCNv4) for shape awareness and the other using a Gabor filter bank for texture awareness. On the public CCMT plant disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs) reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm STAM significantly improves performance over baseline and standard attention models. Integrating domain knowledge via decoupled attention thus presents a promising path for edge-deployed precision agriculture AI. The source code is available at https://github.com/RzMY/STA-Net.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARDO: A Weak Formulation Deep Neural Network Method for Elliptic and Parabolic PDEs Based on Random Differences of Test Functions</title>
<link>https://arxiv.org/abs/2509.03757</link>
<guid>https://arxiv.org/abs/2509.03757</guid>
<content:encoded><![CDATA[
arXiv:2509.03757v1 Announce Type: cross 
Abstract: We propose ARDO method for solving PDEs and PDE-related problems with deep learning techniques. This method uses a weak adversarial formulation but transfers the random difference operator onto the test function. The main advantage of this framework is that it is fully derivative-free with respect to the solution neural network. This framework is particularly suitable for Fokker-Planck type second-order elliptic and parabolic PDEs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning an Adversarial World Model for Automated Curriculum Generation in MARL</title>
<link>https://arxiv.org/abs/2509.03771</link>
<guid>https://arxiv.org/abs/2509.03771</guid>
<content:encoded><![CDATA[
arXiv:2509.03771v1 Announce Type: cross 
Abstract: World models that infer and predict environmental dynamics are foundational to embodied intelligence. However, their potential is often limited by the finite complexity and implicit biases of hand-crafted training environments. To develop truly generalizable and robust agents, we need environments that scale in complexity alongside the agents learning within them. In this work, we reframe the challenge of environment generation as the problem of learning a goal-conditioned, generative world model. We propose a system where a generative **Attacker** agent learns an implicit world model to synthesize increasingly difficult challenges for a team of cooperative **Defender** agents. The Attacker's objective is not passive prediction, but active, goal-driven interaction: it models and generates world states (i.e., configurations of enemy units) specifically to exploit the Defenders' weaknesses. Concurrently, the embodied Defender team learns a cooperative policy to overcome these generated worlds. This co-evolutionary dynamic creates a self-scaling curriculum where the world model continuously adapts to challenge the decision-making policy of the agents, providing an effectively infinite stream of novel and relevant training scenarios. We demonstrate that this framework leads to the emergence of complex behaviors, such as the world model learning to generate flanking and shielding formations, and the defenders learning coordinated focus-fire and spreading tactics. Our findings position adversarial co-evolution as a powerful method for learning instrumental world models that drive agents toward greater strategic depth and robustness.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Latents: Latent Variables Stable Across Ontologies</title>
<link>https://arxiv.org/abs/2509.03780</link>
<guid>https://arxiv.org/abs/2509.03780</guid>
<content:encoded><![CDATA[
arXiv:2509.03780v1 Announce Type: cross 
Abstract: Suppose two Bayesian agents each learn a generative model of the same environment. We will assume the two have converged on the predictive distribution, i.e. distribution over some observables in the environment, but may have different generative models containing different latent variables. Under what conditions can one agent guarantee that their latents are a function of the other agents latents?
  We give simple conditions under which such translation is guaranteed to be possible: the natural latent conditions. We also show that, absent further constraints, these are the most general conditions under which translatability is guaranteed. Crucially for practical application, our theorems are robust to approximation error in the natural latent conditions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?</title>
<link>https://arxiv.org/abs/2509.03790</link>
<guid>https://arxiv.org/abs/2509.03790</guid>
<content:encoded><![CDATA[
arXiv:2509.03790v1 Announce Type: cross 
Abstract: What fundamental properties of reward functions enable efficient sparse-reward reinforcement learning? We address this question through the lens of low-rank structure in reward matrices, showing that such structure induces a sharp transition from exponential to polynomial sample complexity, the first result of this kind for sparse-reward RL. We introduce Policy-Aware Matrix Completion (PAMC), which connects matrix completion theory with reinforcement learning via a new analysis of policy-dependent sampling. Our framework provides: (i) impossibility results for general sparse reward observation, (ii) reward-free representation learning from dynamics, (iii) distribution-free confidence sets via conformal prediction, and (iv) robust completion guarantees that degrade gracefully when low-rank structure is only approximate. Empirically, we conduct a pre-registered evaluation across 100 systematically sampled domains, finding exploitable structure in over half. PAMC improves sample efficiency by factors between 1.6 and 2.1 compared to strong exploration, structured, and representation-learning baselines, while adding only about 20 percent computational overhead.These results establish structural reward learning as a promising new paradigm, with immediate implications for robotics, healthcare, and other safety-critical, sample-expensive applications.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation</title>
<link>https://arxiv.org/abs/2509.03791</link>
<guid>https://arxiv.org/abs/2509.03791</guid>
<content:encoded><![CDATA[
arXiv:2509.03791v1 Announce Type: cross 
Abstract: Evaluating sign language generation is often done through back-translation, where generated signs are first recognized back to text and then compared to a reference using text-based metrics. However, this two-step evaluation pipeline introduces ambiguity: it not only fails to capture the multimodal nature of sign language-such as facial expressions, spatial grammar, and prosody-but also makes it hard to pinpoint whether evaluation errors come from sign generation model or the translation system used to assess it. In this work, we propose SiLVERScore, a novel semantically-aware embedding-based evaluation metric that assesses sign language generation in a joint embedding space. Our contributions include: (1) identifying limitations of existing metrics, (2) introducing SiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness to semantic and prosodic variations, and (4) exploring generalization challenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore achieves near-perfect discrimination between correct and random pairs (ROC AUC = 0.99, overlap < 7%), substantially outperforming traditional metrics.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMVAD: A Multi-Agent System for Simulating Judicial Deliberation Dynamics in India</title>
<link>https://arxiv.org/abs/2509.03793</link>
<guid>https://arxiv.org/abs/2509.03793</guid>
<content:encoded><![CDATA[
arXiv:2509.03793v1 Announce Type: cross 
Abstract: Understanding the complexities of judicial deliberation is crucial for assessing the efficacy and fairness of a justice system. However, empirical studies of judicial panels are constrained by significant ethical and practical barriers. This paper introduces SAMVAD, an innovative Multi-Agent System (MAS) designed to simulate the deliberation process within the framework of the Indian justice system.
  Our system comprises agents representing key judicial roles: a Judge, a Prosecution Counsel, a Defense Counsel, and multiple Adjudicators (simulating a judicial bench), all powered by large language models (LLMs). A primary contribution of this work is the integration of Retrieval-Augmented Generation (RAG), grounded in a domain-specific knowledge base of landmark Indian legal documents, including the Indian Penal Code and the Constitution of India. This RAG functionality enables the Judge and Counsel agents to generate legally sound instructions and arguments, complete with source citations, thereby enhancing both the fidelity and transparency of the simulation.
  The Adjudicator agents engage in iterative deliberation rounds, processing case facts, legal instructions, and arguments to reach a consensus-based verdict. We detail the system architecture, agent communication protocols, the RAG pipeline, the simulation workflow, and a comprehensive evaluation plan designed to assess performance, deliberation quality, and outcome consistency.
  This work provides a configurable and explainable MAS platform for exploring legal reasoning and group decision-making dynamics in judicial simulations, specifically tailored to the Indian legal context and augmented with verifiable legal grounding via RAG.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring How (Not Just Whether) VLMs Build Common Ground</title>
<link>https://arxiv.org/abs/2509.03805</link>
<guid>https://arxiv.org/abs/2509.03805</guid>
<content:encoded><![CDATA[
arXiv:2509.03805v1 Announce Type: cross 
Abstract: Large vision language models (VLMs) increasingly claim reasoning skills, yet current benchmarks evaluate them in single-turn or question answering settings. However, grounding is an interactive process in which people gradually develop shared understanding through ongoing communication. We introduce a four-metric suite (grounding efficiency, content alignment, lexical adaptation, and human-likeness) to systematically evaluate VLM performance in interactive grounding contexts. We deploy the suite on 150 self-play sessions of interactive referential games between three proprietary VLMs and compare them with human dyads. All three models diverge from human patterns on at least three metrics, while GPT4o-mini is the closest overall. We find that (i) task success scores do not indicate successful grounding and (ii) high image-utterance alignment does not necessarily predict task success. Our metric suite and findings offer a framework for future research on VLM grounding.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation</title>
<link>https://arxiv.org/abs/2509.03809</link>
<guid>https://arxiv.org/abs/2509.03809</guid>
<content:encoded><![CDATA[
arXiv:2509.03809v1 Announce Type: cross 
Abstract: Large language models (LLMs) have ushered in a new era for document-level machine translation (\textit{doc}-mt), yet their whole-document outputs challenge existing evaluation methods that assume sentence-by-sentence alignment. We introduce \textit{\textbf{Align-then-Slide}}, a complete evaluation framework for ultra-long doc-mt. In the Align stage, we automatically infer sentence-level source-target correspondences and rebuild the target to match the source sentence number, resolving omissions and many-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we calculate averaged metric scores under 1-, 2-, 3- and 4-chunk for multi-granularity assessment. Experiments on the WMT benchmark show a Pearson correlation of 0.929 between our method with expert MQM rankings. On a newly curated real-world test set, our method again aligns closely with human judgments. Furthermore, preference data produced by Align-then-Slide enables effective CPO training and its direct use as a reward model for GRPO, both yielding translations preferred over a vanilla SFT baseline. The results validate our framework as an accurate, robust, and actionable evaluation tool for doc-mt systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gravity Well Echo Chamber Modeling With An LLM-Based Confirmation Bias Model</title>
<link>https://arxiv.org/abs/2509.03832</link>
<guid>https://arxiv.org/abs/2509.03832</guid>
<content:encoded><![CDATA[
arXiv:2509.03832v1 Announce Type: cross 
Abstract: Social media echo chambers play a central role in the spread of misinformation, yet existing models often overlook the influence of individual confirmation bias. An existing model of echo chambers is the "gravity well" model, which creates an analog between echo chambers and spatial gravity wells. We extend this established model by introducing a dynamic confirmation bias variable that adjusts the strength of pull based on a user's susceptibility to belief-reinforcing content. This variable is calculated for each user through comparisons between their posting history and their responses to posts of a wide range of viewpoints.
  Incorporating this factor produces a confirmation-bias-integrated gravity well model that more accurately identifies echo chambers and reveals community-level markers of information health. We validated the approach on nineteen Reddit communities, demonstrating improved detection of echo chambers.
  Our contribution is a framework for systematically capturing the role of confirmation bias in online group dynamics, enabling more effective identification of echo chambers. By flagging these high-risk environments, the model supports efforts to curb the spread of misinformation at its most common points of amplification.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Leiden to Pleasure Island: The Constant Potts Model for Community Detection as a Hedonic Game</title>
<link>https://arxiv.org/abs/2509.03834</link>
<guid>https://arxiv.org/abs/2509.03834</guid>
<content:encoded><![CDATA[
arXiv:2509.03834v1 Announce Type: cross 
Abstract: Community detection is one of the fundamental problems in data science which consists of partitioning nodes into disjoint communities. We present a game-theoretic perspective on the Constant Potts Model (CPM) for partitioning networks into disjoint communities, emphasizing its efficiency, robustness, and accuracy. Efficiency: We reinterpret CPM as a potential hedonic game by decomposing its global Hamiltonian into local utility functions, where the local utility gain of each agent matches the corresponding increase in global utility. Leveraging this equivalence, we prove that local optimization of the CPM objective via better-response dynamics converges in pseudo-polynomial time to an equilibrium partition. Robustness: We introduce and relate two stability criteria: a strict criterion based on a novel notion of robustness, requiring nodes to simultaneously maximize neighbors and minimize non-neighbors within communities, and a relaxed utility function based on a weighted sum of these objectives, controlled by a resolution parameter. Accuracy: In community tracking scenarios, where initial partitions are used to bootstrap the Leiden algorithm with partial ground-truth information, our experiments reveal that robust partitions yield higher accuracy in recovering ground-truth communities.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INGRID: Intelligent Generative Robotic Design Using Large Language Models</title>
<link>https://arxiv.org/abs/2509.03842</link>
<guid>https://arxiv.org/abs/2509.03842</guid>
<content:encoded><![CDATA[
arXiv:2509.03842v1 Announce Type: cross 
Abstract: The integration of large language models (LLMs) into robotic systems has accelerated progress in embodied artificial intelligence, yet current approaches remain constrained by existing robotic architectures, particularly serial mechanisms. This hardware dependency fundamentally limits the scope of robotic intelligence. Here, we present INGRID (Intelligent Generative Robotic Design), a framework that enables the automated design of parallel robotic mechanisms through deep integration with reciprocal screw theory and kinematic synthesis methods. We decompose the design challenge into four progressive tasks: constraint analysis, kinematic joint generation, chain construction, and complete mechanism design. INGRID demonstrates the ability to generate novel parallel mechanisms with both fixed and variable mobility, discovering kinematic configurations not previously documented in the literature. We validate our approach through three case studies demonstrating how INGRID assists users in designing task-specific parallel robots based on desired mobility requirements. By bridging the gap between mechanism theory and machine learning, INGRID enables researchers without specialized robotics training to create custom parallel mechanisms, thereby decoupling advances in robotic intelligence from hardware constraints. This work establishes a foundation for mechanism intelligence, where AI systems actively design robotic hardware, potentially transforming the development of embodied AI systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Inverse Reinforcement Learning for Mean Field Games via Probabilistic Context Variables</title>
<link>https://arxiv.org/abs/2509.03845</link>
<guid>https://arxiv.org/abs/2509.03845</guid>
<content:encoded><![CDATA[
arXiv:2509.03845v1 Announce Type: cross 
Abstract: Designing suitable reward functions for numerous interacting intelligent agents is challenging in real-world applications. Inverse reinforcement learning (IRL) in mean field games (MFGs) offers a practical framework to infer reward functions from expert demonstrations. While promising, the assumption of agent homogeneity limits the capability of existing methods to handle demonstrations with heterogeneous and unknown objectives, which are common in practice. To this end, we propose a deep latent variable MFG model and an associated IRL method. Critically, our method can infer rewards from different yet structurally similar tasks without prior knowledge about underlying contexts or modifying the MFG model itself. Our experiments, conducted on simulated scenarios and a real-world spatial taxi-ride pricing problem, demonstrate the superiority of our approach over state-of-the-art IRL methods in MFGs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MillGNN: Learning Multi-Scale Lead-Lag Dependencies for Multi-Variate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.03852</link>
<guid>https://arxiv.org/abs/2509.03852</guid>
<content:encoded><![CDATA[
arXiv:2509.03852v1 Announce Type: cross 
Abstract: Multi-variate time series (MTS) forecasting is crucial for various applications. Existing methods have shown promising results owing to their strong ability to capture intra- and inter-variate dependencies. However, these methods often overlook lead-lag dependencies at multiple grouping scales, failing to capture hierarchical lead-lag effects in complex systems. To this end, we propose MillGNN, a novel \underline{g}raph \underline{n}eural \underline{n}etwork-based method that learns \underline{m}ult\underline{i}ple grouping scale \underline{l}ead-\underline{l}ag dependencies for MTS forecasting, which can comprehensively capture lead-lag effects considering variate-wise and group-wise dynamics and decays. Specifically, MillGNN introduces two key innovations: (1) a scale-specific lead-lag graph learning module that integrates cross-correlation coefficients and dynamic decaying features derived from real-time inputs and time lags to learn lead-lag dependencies for each scale, which can model evolving lead-lag dependencies with statistical interpretability and data-driven flexibility; (2) a hierarchical lead-lag message passing module that passes lead-lag messages at multiple grouping scales in a structured way to simultaneously propagate intra- and inter-scale lead-lag effects, which can capture multi-scale lead-lag effects with a balance of comprehensiveness and efficiency. Experimental results on 11 datasets demonstrate the superiority of MillGNN for long-term and short-term MTS forecasting, compared with 16 state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2509.03871</link>
<guid>https://arxiv.org/abs/2509.03871</guid>
<content:encoded><![CDATA[
arXiv:2509.03871v1 Announce Type: cross 
Abstract: The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition</title>
<link>https://arxiv.org/abs/2509.03873</link>
<guid>https://arxiv.org/abs/2509.03873</guid>
<content:encoded><![CDATA[
arXiv:2509.03873v1 Announce Type: cross 
Abstract: Food recognition has gained significant attention, but the rapid emergence of new dishes requires methods for recognizing unseen food categories, motivating Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot Food Recognition (CZSFR), where cuisines and ingredients naturally align with attributes and objects in Compositional Zero-Shot learning (CZSL). However, CZSFR faces three challenges: (1) Redundant background information distracts models from learning meaningful food features, (2) Role confusion between staple and side dishes leads to misclassification, and (3) Semantic bias in a single attribute can lead to confusion of understanding. Therefore, we propose SalientFusion, a context-aware CZSFR method with two components: SalientFormer, which removes background redundancy and uses depth features to resolve role confusion; DebiasAT, which reduces the semantic bias by aligning prompts with visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we show that SalientFusion achieves state-of-the-art results on these benchmarks and the most popular general datasets for the general CZSL. The code is avaliable at https://github.com/Jiajun-RUC/SalientFusion.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peptidomic-Based Prediction Model for Coronary Heart Disease Using a Multilayer Perceptron Neural Network</title>
<link>https://arxiv.org/abs/2509.03884</link>
<guid>https://arxiv.org/abs/2509.03884</guid>
<content:encoded><![CDATA[
arXiv:2509.03884v1 Announce Type: cross 
Abstract: Coronary heart disease (CHD) is a leading cause of death worldwide and contributes significantly to annual healthcare expenditures. To develop a non-invasive diagnostic approach, we designed a model based on a multilayer perceptron (MLP) neural network, trained on 50 key urinary peptide biomarkers selected via genetic algorithms. Treatment and control groups, each comprising 345 individuals, were balanced using the Synthetic Minority Over-sampling Technique (SMOTE). The neural network was trained using a stratified validation strategy. Using a network with three hidden layers of 60 neurons each and an output layer of two neurons, the model achieved a precision, sensitivity, and specificity of 95.67 percent, with an F1-score of 0.9565. The area under the ROC curve (AUC) reached 0.9748 for both classes, while the Matthews correlation coefficient (MCC) and Cohen's kappa coefficient were 0.9134 and 0.9131, respectively, demonstrating its reliability in detecting CHD. These results indicate that the model provides a highly accurate and robust non-invasive diagnostic tool for coronary heart disease.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance</title>
<link>https://arxiv.org/abs/2509.03889</link>
<guid>https://arxiv.org/abs/2509.03889</guid>
<content:encoded><![CDATA[
arXiv:2509.03889v1 Announce Type: cross 
Abstract: Manipulating clothing is challenging due to complex configurations, variable material dynamics, and frequent self-occlusion. Prior systems often flatten garments or assume visibility of key features. We present a dual-arm visuotactile framework that combines confidence-aware dense visual correspondence and tactile-supervised grasp affordance to operate directly on crumpled and suspended garments. The correspondence model is trained on a custom, high-fidelity simulated dataset using a distributional loss that captures cloth symmetries and generates correspondence confidence estimates. These estimates guide a reactive state machine that adapts folding strategies based on perceptual uncertainty. In parallel, a visuotactile grasp affordance network, self-supervised using high-resolution tactile feedback, determines which regions are physically graspable. The same tactile classifier is used during execution for real-time grasp validation. By deferring action in low-confidence states, the system handles highly occluded table-top and in-air configurations. We demonstrate our task-agnostic grasp selection module in folding and hanging tasks. Moreover, our dense descriptors provide a reusable intermediate representation for other planning modalities, such as extracting grasp targets from human video demonstrations, paving the way for more generalizable and scalable garment manipulation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Generative Models Meet Compressed Sensing, with Applications to Image Data and Financial Time Series</title>
<link>https://arxiv.org/abs/2509.03898</link>
<guid>https://arxiv.org/abs/2509.03898</guid>
<content:encoded><![CDATA[
arXiv:2509.03898v1 Announce Type: cross 
Abstract: This paper develops dimension reduction techniques for accelerating diffusion model inference in the context of synthetic data generation. The idea is to integrate compressed sensing into diffusion models: (i) compress the data into a latent space, (ii) train a diffusion model in the latent space, and (iii) apply a compressed sensing algorithm to the samples generated in the latent space, facilitating the efficiency of both model training and inference. Under suitable sparsity assumptions on data, the proposed algorithm is proved to enjoy faster convergence by combining diffusion model inference with sparse recovery. As a byproduct, we obtain an optimal value for the latent space dimension. We also conduct numerical experiments on a range of datasets, including image data (handwritten digits, medical images, and climate data) and financial time series for stress testing.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question Answering</title>
<link>https://arxiv.org/abs/2509.03918</link>
<guid>https://arxiv.org/abs/2509.03918</guid>
<content:encoded><![CDATA[
arXiv:2509.03918v1 Announce Type: cross 
Abstract: Complex Question Answering (QA) is a fundamental and challenging task in NLP. While large language models (LLMs) exhibit impressive performance in QA, they suffer from significant performance degradation when facing complex and abstract QA tasks due to insufficient reasoning capabilities. Works such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning abilities, but they face issues such as in-layer redundancy in tree structures and single paths in chain structures. Although some studies utilize Retrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the challenge of effectively utilizing large amounts of information involving multiple entities and hops remains critical. To address this, we propose the Matrix of Thought (MoT), a novel and efficient LLM thought structure. MoT explores the problem in both horizontal and vertical dimensions through the "column-cell communication" mechanism, enabling LLMs to actively engage in multi-strategy and deep-level thinking, reducing redundancy within the column cells and enhancing reasoning capabilities. Furthermore, we develop a fact-correction mechanism by constructing knowledge units from retrieved knowledge graph triples and raw text to enhance the initial knowledge for LLM reasoning and correct erroneous answers. This leads to the development of an efficient and accurate QA framework (MTQA). Experimental results show that our framework outperforms state-of-the-art methods on four widely-used datasets in terms of F1 and EM scores, with reasoning time only 14.4\% of the baseline methods, demonstrating both its efficiency and accuracy. The code for this framework is available at https://github.com/lyfiter/mtqa.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment</title>
<link>https://arxiv.org/abs/2509.03934</link>
<guid>https://arxiv.org/abs/2509.03934</guid>
<content:encoded><![CDATA[
arXiv:2509.03934v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have revolutionized natural language processing through their remarkable capabilities in understanding and executing diverse tasks. While supervised fine-tuning, particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively enhances task-specific performance, it often leads to catastrophic forgetting, where models lose their previously acquired knowledge and general capabilities. Existing solutions either require access to general instruction data or face limitations in preserving the model's original distribution. To overcome these limitations, we propose SelfAug, a self-distribution alignment method that aligns input sequence logits to preserve the model's semantic distribution, thereby mitigating catastrophic forgetting and improving downstream performance. Extensive experiments demonstrate that SelfAug achieves a superior balance between downstream learning and general capability retention. Our comprehensive empirical analysis reveals a direct correlation between distribution shifts and the severity of catastrophic forgetting in RAG scenarios, highlighting how the absence of RAG capabilities in general instruction tuning leads to significant distribution shifts during fine-tuning. Our findings not only advance the understanding of catastrophic forgetting in RAG contexts but also provide a practical solution applicable across diverse fine-tuning scenarios. Our code is publicly available at https://github.com/USTC-StarTeam/SelfAug.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning</title>
<link>https://arxiv.org/abs/2509.03937</link>
<guid>https://arxiv.org/abs/2509.03937</guid>
<content:encoded><![CDATA[
arXiv:2509.03937v1 Announce Type: cross 
Abstract: Despite the significant advancements of self-play fine-tuning (SPIN), which can transform a weak large language model (LLM) into a strong one through competitive interactions between models of varying capabilities, it still faces challenges in the Text-to-SQL task. SPIN does not generate new information, and the large number of correct SQL queries produced by the opponent model during self-play reduces the main model's ability to generate accurate SQL queries. To address this challenge, we propose a new self-play fine-tuning method tailored for the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a verification-based iterative fine-tuning approach, which synthesizes high-quality fine-tuning data iteratively based on the database schema and validation feedback to enhance model performance, while building a model base with varying capabilities. During the self-play fine-tuning phase, we propose an error-driven loss method that incentivizes incorrect outputs from the opponent model, enabling the main model to distinguish between correct SQL and erroneous SQL generated by the opponent model, thereby improving its ability to generate correct SQL. Extensive experiments and in-depth analyses on six open-source LLMs and five widely used benchmarks demonstrate that our approach outperforms existing state-of-the-art (SOTA) methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>