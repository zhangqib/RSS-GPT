<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management</title>
<link>https://arxiv.org/abs/2509.04505</link>
<guid>https://arxiv.org/abs/2509.04505</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Construction Project Management, Ethical Decision-making, Large Language Models, Human-in-the-loop oversight

Summary:
The study evaluates the ethical viability and reliability of Large Language Models (LLMs) in Construction Project Management (CPM). Through quantitative performance testing and qualitative interviews with industry experts, the study finds that while LLMs perform well in structured domains like legal compliance, they struggle with contextual nuance, accountability, and transparent reasoning. Stakeholders express reservations about using AI autonomously for ethical judgments and advocate for human-in-the-loop oversight. The study introduces the Ethical Decision Support Assessment Checklist (EDSAC) as a framework for evaluating LLMs in ethical decision-making contexts and recommends that LLMs be used as decision-support aids rather than autonomous ethical agents.

<br><br>Summary: The study evaluates the ethical viability and reliability of Large Language Models in Construction Project Management. LLMs demonstrate adequate performance in legal compliance but struggle with contextual nuance, accountability, and transparent reasoning. Stakeholders advocate for human-in-the-loop oversight and recommend using LLMs as decision-support aids rather than autonomous ethical agents. <div>
arXiv:2509.04505v1 Announce Type: new 
Abstract: The integration of Artificial Intelligence (AI) into construction project management (CPM) is accelerating, with Large Language Models (LLMs) emerging as accessible decision-support tools. This study aims to critically evaluate the ethical viability and reliability of LLMs when applied to the ethically sensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods research design was employed, involving the quantitative performance testing of two leading LLMs against twelve real-world ethical scenarios using a novel Ethical Decision Support Assessment Checklist (EDSAC), and qualitative analysis of semi-structured interviews with 12 industry experts to capture professional perceptions. The findings reveal that while LLMs demonstrate adequate performance in structured domains such as legal compliance, they exhibit significant deficiencies in handling contextual nuance, ensuring accountability, and providing transparent reasoning. Stakeholders expressed considerable reservations regarding the autonomous use of AI for ethical judgments, strongly advocating for robust human-in-the-loop oversight. To our knowledge, this is one of the first studies to empirically test the ethical reasoning of LLMs within the construction domain. It introduces the EDSAC framework as a replicable methodology and provides actionable recommendations, emphasising that LLMs are currently best positioned as decision-support aids rather than autonomous ethical agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maestro: Joint Graph &amp; Config Optimization for Reliable AI Agents</title>
<link>https://arxiv.org/abs/2509.04642</link>
<guid>https://arxiv.org/abs/2509.04642</guid>
<content:encoded><![CDATA[
<div> optimizer, LLM agents, Maestro, graph, configuration

Summary:<br>
1. The article introduces Maestro, a comprehensive optimizer for LLM agents that considers both graph structure and configuration of nodes.
2. Maestro optimizes agent quality while adhering to rollout/token budgets by jointly searching over graphs and configurations.
3. The optimizer utilizes textual feedback from traces to prioritize edits, enhancing sample efficiency and targeting specific failure modes.
4. On benchmark tests, Maestro outperforms leading prompt optimizers such as MIPROv2, GEPA, and GEPA+Merge by significant margins.
5. Maestro achieves superior results with fewer rollouts compared to GEPA, demonstrating its efficiency in optimizing LLM agents.
6. The joint graph and configuration search by Maestro addresses structural failure modes that prompt tuning alone cannot fix.
7. Applications such as interviewer and RAG agents show significant improvements with Maestro's approach. 

<br><br>Summary: <div>
arXiv:2509.04642v1 Announce Type: new 
Abstract: Building reliable LLM agents requires decisions at two levels: the graph (which modules exist and how information flows) and the configuration of each node (models, prompts, tools, control knobs). Most existing optimizers tune configurations while holding the graph fixed, leaving structural failure modes unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for LLM agents that jointly searches over graphs and configurations to maximize agent quality, subject to explicit rollout/token budgets. Beyond numeric metrics, Maestro leverages reflective textual feedback from traces to prioritize edits, improving sample efficiency and targeting specific failure modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%, 4.9%, and 4.86%, respectively; even when restricted to prompt-only optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these results with far fewer rollouts than GEPA. We further show large gains on two applications (interviewer & RAG agents), highlighting that joint graph & configuration search addresses structural failure modes that prompt tuning alone cannot fix.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization</title>
<link>https://arxiv.org/abs/2509.04646</link>
<guid>https://arxiv.org/abs/2509.04646</guid>
<content:encoded><![CDATA[
<div> Keywords: Modeling & Simulation, Agent-based models, Large Language Models, Stakeholder needs, Tailored explanations<br>
Summary: 
This article discusses the potential of Modeling & Simulation (M&S) approaches, such as agent-based models, in supporting decision-making in the health sector, including areas like vaccine adoption and healthy behavior promotion. However, the complexity of these models often makes them inaccessible to key stakeholders. To address this issue, the authors propose a framework to tailor explanations of health simulations using Large Language Models (LLMs). By first understanding stakeholder needs and preferences, optimizing LLMs to generate customized explanations, and evaluating the outputs, the framework aims to bridge the gap between complex health models and practical usability. The study highlights the importance of personalized communication in translating simulation outputs for various stakeholders, including clinicians, policymakers, patients, caregivers, and health advocates. By enhancing the clarity and relevance of explanations, this approach can enhance the impact and usability of health simulations in decision-making processes. <br><br>Summary: <div>
arXiv:2509.04646v1 Announce Type: new 
Abstract: Modeling & Simulation (M&amp;S) approaches such as agent-based models hold significant potential to support decision-making activities in health, with recent examples including the adoption of vaccines, and a vast literature on healthy eating behaviors and physical activity behaviors. These models are potentially usable by different stakeholder groups, as they support policy-makers to estimate the consequences of potential interventions and they can guide individuals in making healthy choices in complex environments. However, this potential may not be fully realized because of the models' complexity, which makes them inaccessible to the stakeholders who could benefit the most. While Large Language Models (LLMs) can translate simulation outputs and the design of models into text, current approaches typically rely on one-size-fits-all summaries that fail to reflect the varied informational needs and stylistic preferences of clinicians, policymakers, patients, caregivers, and health advocates. This limitation stems from a fundamental gap: we lack a systematic understanding of what these stakeholders need from explanations and how to tailor them accordingly. To address this gap, we present a step-by-step framework to identify stakeholder needs and guide LLMs in generating tailored explanations of health simulations. Our procedure uses a mixed-methods design by first eliciting the explanation needs and stylistic preferences of diverse health stakeholders, then optimizing the ability of LLMs to generate tailored outputs (e.g., via controllable attribute tuning), and then evaluating through a comprehensive range of metrics to further improve the tailored generation of summaries.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Approach to Grounding AI Model Evaluations in Human-derived Criteria</title>
<link>https://arxiv.org/abs/2509.04676</link>
<guid>https://arxiv.org/abs/2509.04676</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, benchmarks, human-derived evaluation criteria, interpretability, cognitive skills

Summary: 
Artificial intelligence (AI) is rapidly evolving, but traditional benchmarks may not fully capture its capabilities. This study focuses on enhancing benchmarks in physical world modeling by incorporating human-derived evaluation criteria. Factors such as Prioritization, Memorizing, Discerning, and Contextualizing were identified as crucial cognitive skills for AI and human reasoning. Participants believe AI lacks interpretive and empathetic abilities but expect high performance. By integrating these insights into benchmark design, a more human-aligned approach to defining and measuring progress in AI development is proposed. User-centered evaluation is emphasized, providing guidelines for aligning AI capabilities with human cognitive processes. This approach aims to improve current benchmarking practices and pave the way for future advancements in AI model evaluation. <div>
arXiv:2509.04676v1 Announce Type: new 
Abstract: In the rapidly evolving field of artificial intelligence (AI), traditional benchmarks can fall short in attempting to capture the nuanced capabilities of AI models. We focus on the case of physical world modeling and propose a novel approach to augment existing benchmarks with human-derived evaluation criteria, aiming to enhance the interpretability and applicability of model behaviors. Grounding our study in the Perception Test and OpenEQA benchmarks, we conducted in-depth interviews and large-scale surveys to identify key cognitive skills, such as Prioritization, Memorizing, Discerning, and Contextualizing, that are critical for both AI and human reasoning. Our findings reveal that participants perceive AI as lacking in interpretive and empathetic skills yet hold high expectations for AI performance. By integrating insights from our findings into benchmark design, we offer a framework for developing more human-aligned means of defining and measuring progress. This work underscores the importance of user-centered evaluation in AI development, providing actionable guidelines for researchers and practitioners aiming to align AI capabilities with human cognitive processes. Our approach both enhances current benchmarking practices and sets the stage for future advancements in AI model evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning</title>
<link>https://arxiv.org/abs/2509.04731</link>
<guid>https://arxiv.org/abs/2509.04731</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, Agent models, World models, hierarchical scaffolding, multi-agent reinforcement learning

Summary: 
The convergence of Language, Agent, and World models is crucial for advancing artificial intelligence. Existing models focus on scaling Language and Agent models, but the development of sophisticated World Models remains a challenge, especially for complex tasks. A significant hurdle is the lack of explicit hierarchical World Models in environments like robotic soccer. The solution proposed is to create environments with hierarchical scaffolding, decomposing complex goals into manageable subgoals. Research in multi-agent soccer shows a shift towards integrating symbolic and hierarchical methods with multi-agent reinforcement learning. A paradigm shift is suggested - using Large Language Models to dynamically create a hierarchical scaffold, structuring the World Model with language in real-time. This language-driven approach provides a curriculum, meaningful learning signals, and supports compositional learning, enabling agents to learn strategic behaviors efficiently. By incorporating explicit, language-configurable task layers in environments, it becomes possible to bridge the gap between low-level reactive behaviors and high-level strategic team play, offering a generalizable framework to train intelligent agents. 

<br><br>Summary: <div>
arXiv:2509.04731v1 Announce Type: new 
Abstract: The convergence of Language models, Agent models, and World models represents a critical frontier for artificial intelligence. While recent progress has focused on scaling Language and Agent models, the development of sophisticated, explicit World Models remains a key bottleneck, particularly for complex, long-horizon multi-agent tasks. In domains such as robotic soccer, agents trained via standard reinforcement learning in high-fidelity but structurally-flat simulators often fail due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing capable agents lies in creating environments that possess an explicit, hierarchical World Model. We contend that this is best achieved through hierarchical scaffolding, where complex goals are decomposed into structured, manageable subgoals. Drawing evidence from a systematic review of 2024 research in multi-agent soccer, we identify a clear and decisive trend towards integrating symbolic and hierarchical methods with multi-agent reinforcement learning (MARL). These approaches implicitly or explicitly construct a task-based world model to guide agent learning. We then propose a paradigm shift: leveraging Large Language Models to dynamically generate this hierarchical scaffold, effectively using language to structure the World Model on the fly. This language-driven world model provides an intrinsic curriculum, dense and meaningful learning signals, and a framework for compositional learning, enabling Agent Models to acquire sophisticated, strategic behaviors with far greater sample efficiency. By building environments with explicit, language-configurable task layers, we can bridge the gap between low-level reactive behaviors and high-level strategic team play, creating a powerful and generalizable framework for training the next generation of intelligent agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking</title>
<link>https://arxiv.org/abs/2509.04791</link>
<guid>https://arxiv.org/abs/2509.04791</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, What-If Analysis, proactive thinking, reinforcement learning, strategic applications

Summary: 
Large language models (LLMs) are highly skilled at reactive information processing but lack the ability to explore hypothetical futures systematically. The proposed WiA-LLM paradigm integrates What-If Analysis (WIA) to equip LLMs with proactive thinking capabilities. By leveraging reinforcement learning to analyze potential scenarios, WiA-LLM can anticipate future outcomes rather than just reacting to present conditions. Validation in the Honor of Kings (HoK) game environment demonstrates WiA-LLM achieving a 74.2% accuracy in forecasting game-state changes, with significant gains in high-difficulty scenarios where accurate foresight is crucial. This work represents a fundamental step towards proactive reasoning in LLMs, offering a scalable framework for robust decision-making in dynamic environments, with broad implications for strategic applications. 

<br><br>Summary: <div>
arXiv:2509.04791v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at processing information reactively but lack the ability to systemically explore hypothetical futures. They cannot ask, "what if we take this action? how will it affect the final outcome" and forecast its potential consequences before acting. This critical gap limits their utility in dynamic, high-stakes scenarios like strategic planning, risk assessment, and real-time decision making. To bridge this gap, we propose WiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities. Our approach integrates What-If Analysis (WIA), a systematic approach for evaluating hypothetical scenarios by changing input variables. By leveraging environmental feedback via reinforcement learning, WiA-LLM moves beyond reactive thinking. It dynamically simulates the outcomes of each potential action, enabling the model to anticipate future states rather than merely react to the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a complex multiplayer game environment characterized by rapid state changes and intricate interactions. The game's real-time state changes require precise multi-step consequence prediction, making it an ideal testbed for our approach. Experimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy in forecasting game-state changes (up to two times gain over baselines). The model shows particularly significant gains in high-difficulty scenarios where accurate foresight is critical. To our knowledge, this is the first work to formally explore and integrate what-if analysis capabilities within LLMs. WiA-LLM represents a fundamental advance toward proactive reasoning in LLMs, providing a scalable framework for robust decision-making in dynamic environments with broad implications for strategic applications.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models</title>
<link>https://arxiv.org/abs/2509.04809</link>
<guid>https://arxiv.org/abs/2509.04809</guid>
<content:encoded><![CDATA[
<div> Explainable Reinforcement Learning, XRL, TalkToAgent, Large Language Models, interactive explanations
Summary:
TalkToAgent introduces a multi-agent Large Language Models framework for Explainable Reinforcement Learning. The architecture consists of Coordinator, Explainer, Coder, Evaluator, and Debugger agents. It provides natural language explanations for RL policies, mapping user queries to relevant XRL tools, and clarifying agent actions in terms of key state variables, expected outcomes, or counterfactual explanations. The framework extends counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions or new rule-based policies. Validation on a quadruple-tank process control problem showed accurate mapping of user queries to XRL tasks, successful coder-debugger interactions for counterfactual generation, and effective interpretation of agent actions in the problem domain.<br><br>Summary: <div>
arXiv:2509.04809v1 Announce Type: new 
Abstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agent's actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agent's actions and contextualized their meaning within the problem domain.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory</title>
<link>https://arxiv.org/abs/2509.04847</link>
<guid>https://arxiv.org/abs/2509.04847</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, cooperative behavior, multi-party settings, prisoner's dilemma, Axelrod-style tournament
Summary: 
Language models are being increasingly used in various interactive online settings, prompting the need to analyze their behavior in multi-party scenarios. This study focuses on language model behavior in the iterated prisoner's dilemma game, a classic framework for studying cooperation and conflict over time. Results show that language models can perform as well as, or better than, traditional strategies in this setting, demonstrating characteristics of strong cooperative strategies such as niceness, provocability, and generosity. The models also display quick adaptability to changes in opponent strategy, rivaling or exceeding human adaptability. This research provides valuable insights into the long-term cooperative behaviors of language model agents and lays the groundwork for further exploration of their role in complex social environments involving both humans and AI.


<br><br>Summary: <div>
arXiv:2509.04847v1 Announce Type: new 
Abstract: Language models are increasingly deployed in interactive online environments, from personal chat assistants to domain-specific agents, raising questions about their cooperative and competitive behavior in multi-party settings. While prior work has examined language model decision-making in isolated or short-term game-theoretic contexts, these studies often neglect long-horizon interactions, human-model collaboration, and the evolution of behavioral patterns over time. In this paper, we investigate the dynamics of language model behavior in the iterated prisoner's dilemma (IPD), a classical framework for studying cooperation and conflict. We pit model-based agents against a suite of 240 well-established classical strategies in an Axelrod-style tournament and find that language models achieve performance on par with, and in some cases exceeding, the best-known classical strategies. Behavioral analysis reveals that language models exhibit key properties associated with strong cooperative strategies - niceness, provocability, and generosity while also demonstrating rapid adaptability to changes in opponent strategy mid-game. In controlled "strategy switch" experiments, language models detect and respond to shifts within only a few rounds, rivaling or surpassing human adaptability. These results provide the first systematic characterization of long-term cooperative behaviors in language model agents, offering a foundation for future research into their role in more complex, mixed human-AI social environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales</title>
<link>https://arxiv.org/abs/2509.04871</link>
<guid>https://arxiv.org/abs/2509.04871</guid>
<content:encoded><![CDATA[
<div> voice assistant, dialogue modelling, call recordings, telesales data, AI agent

Summary:
Recent advances in language and speech modelling have enabled the development of voice assistants that can understand and generate human dialogue in real time. This paper presents a methodology for creating a conversational voice AI agent from call recordings, specifically focusing on telesales data. The system listens to customers over the phone, responds with a synthetic voice, and follows a playbook learned from top human agents. Evaluation against human agents showed that the AI agent performed well in routine aspects of calls but struggled in persuasion and objection handling. The paper discusses the shortcomings and proposes refinements to improve performance. Future research directions include large-scale simulation and automated evaluation to further enhance the cloned agent's capabilities. 

<br><br>Summary: <div>
arXiv:2509.04871v1 Announce Type: new 
Abstract: Recent advances in language and speech modelling have made it possible to build autonomous voice assistants that understand and generate human dialogue in real time. These systems are increasingly being deployed in domains such as customer service and healthcare care, where they can automate repetitive tasks, reduce operational costs, and provide constant support around the clock. In this paper, we present a general methodology for cloning a conversational voice AI agent from a corpus of call recordings. Although the case study described in this paper uses telesales data to illustrate the approach, the underlying process generalizes to any domain where call transcripts are available. Our system listens to customers over the telephone, responds with a synthetic voice, and follows a structured playbook learned from top performing human agents. We describe the domain selection, knowledge extraction, and prompt engineering used to construct the agent, integrating automatic speech recognition, a large language model based dialogue manager, and text to speech synthesis into a streaming inference pipeline. The cloned agent is evaluated against human agents on a rubric of 22 criteria covering introduction, product communication, sales drive, objection handling, and closing. Blind tests show that the AI agent approaches human performance in routine aspects of the call while underperforming in persuasion and objection handling. We analyze these shortcomings and refine the prompt accordingly. The paper concludes with design lessons and avenues for future research, including large scale simulation and automated evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration</title>
<link>https://arxiv.org/abs/2509.04876</link>
<guid>https://arxiv.org/abs/2509.04876</guid>
<content:encoded><![CDATA[
<div> Keywords: OSC, cognitive synergy, multi-agent systems, collaboration framework, language models

Summary: 
The paper introduces OSC (Orchestrating Cognitive Synergy), a framework designed to enhance cognitive synergy in multi-agent systems with large language models. OSC introduces Collaborator Knowledge Models (CKM) to enable agents to perceive their collaborators' cognitive states and adapt communication behaviors in real-time. By analyzing cognitive gaps and adjusting communication strategies, OSC improves task performance and communication efficiency in complex reasoning and problem-solving tasks. The framework transforms individual agents into a deeply collaborative cognitive team, optimizing multi-agent collaboration. Additionally, OSC offers new insights into language model agent interaction behaviors. <div>
arXiv:2509.04876v1 Announce Type: new 
Abstract: This paper introduces OSC (Orchestrating Cognitive Synergy), a knowledge-aware adaptive collaboration framework designed to enhance cognitive synergy in multi-agent systems with large language models. While prior work has advanced agent selection and result aggregation, efficient linguistic interactions for deep collaboration among expert agents remain a critical bottleneck. OSC addresses this gap as a pivotal intermediate layer between selection and aggregation, introducing Collaborator Knowledge Models (CKM) to enable each agent to dynamically perceive its collaborators' cognitive states. Through real-time cognitive gap analysis, agents adaptively adjust communication behaviors, including content focus, detail level, and expression style, using learned strategies. Experiments on complex reasoning and problem-solving benchmarks demonstrate that OSC significantly improves task performance and communication efficiency, transforming "parallel-working individuals'' into a "deeply collaborative cognitive team.'' This framework not only optimizes multi-agent collaboration but also offers new insights into LLM agent interaction behaviors.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing</title>
<link>https://arxiv.org/abs/2509.04908</link>
<guid>https://arxiv.org/abs/2509.04908</guid>
<content:encoded><![CDATA[
<div> continuous modeling, Multimodal Large Language Model, GUI perception, SparkUI-Parser, ScreenParse

Summary:
SparkUI-Parser is a novel framework that addresses challenges in Multimodal Large Language Models (MLLMs) for GUI perception. It improves localization precision and parsing capability by implementing continuous modeling of coordinates. This approach enhances accuracy and inference speed compared to discrete modeling methods. The framework includes a rejection mechanism based on a modified Hungarian matching algorithm to identify and reject non-existent elements, reducing false positives. Additionally, ScreenParse, a benchmark for assessing GUI model capabilities, is introduced to evaluate performance across diverse scenarios. Extensive experiments show that SparkUI-Parser outperforms state-of-the-art methods on various benchmarks. The resources for SparkUI-Parser are available on GitHub for further exploration and implementation. <br><br>Summary: <div>
arXiv:2509.04908v1 Announce Type: new 
Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have made great progress. However, the following challenges still exist in prior methods: 1) They model discrete coordinates based on text autoregressive mechanism, which results in lower grounding accuracy and slower inference speed. 2) They can only locate predefined sets of elements and are not capable of parsing the entire interface, which hampers the broad application and support for downstream tasks. To address the above issues, we propose SparkUI-Parser, a novel end-to-end framework where higher localization precision and fine-grained parsing capability of the entire interface are simultaneously achieved. Specifically, instead of using probability-based discrete modeling, we perform continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with an additional token router and coordinate decoder. This effectively mitigates the limitations inherent in the discrete output characteristics and the token-by-token generation process of MLLMs, consequently boosting both the accuracy and the inference speed. To further enhance robustness, a rejection mechanism based on a modified Hungarian matching algorithm is introduced, which empowers the model to identify and reject non-existent elements, thereby reducing false positives. Moreover, we present ScreenParse, a rigorously constructed benchmark to systematically assess structural perception capabilities of GUI models across diverse scenarios. Extensive experiments demonstrate that our approach consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks. The resources are available at https://github.com/antgroup/SparkUI-Parser.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts</title>
<link>https://arxiv.org/abs/2509.04926</link>
<guid>https://arxiv.org/abs/2509.04926</guid>
<content:encoded><![CDATA[
<div> ontology, conversational agents, language models, controlled text generation, proficiency-level control

Summary:
- The article addresses the challenge of controlling Large Language Models (LLMs) for predictable and user-personalized responses in conversations.
- An ontology-based approach is proposed to formally define conversational features, typically qualitative, using linguistic descriptors to derive quantitative definitions.
- The focus is on proficiency-level control in conversations, with CEFR language proficiency levels as a case study.
- The qualitative definitions are formalized in description logic and integrated into an ontology to guide controlled text generation of an LLM through fine-tuning.
- Experimental results validate the approach, showing consistent and explainable proficiency-level definitions that enhance transparency in conversational AI.<br><br>Summary: <div>
arXiv:2509.04926v1 Announce Type: new 
Abstract: The controllability of Large Language Models (LLMs) when used as conversational agents is a key challenge, particularly to ensure predictable and user-personalized responses. This work proposes an ontology-based approach to formally define conversational features that are typically qualitative in nature. By leveraging a set of linguistic descriptors, we derive quantitative definitions for qualitatively-defined concepts, enabling their integration into an ontology for reasoning and consistency checking. We apply this framework to the task of proficiency-level control in conversations, using CEFR language proficiency levels as a case study. These definitions are then formalized in description logic and incorporated into an ontology, which guides controlled text generation of an LLM through fine-tuning. Experimental results demonstrate that our approach provides consistent and explainable proficiency-level definitions, improving transparency in conversational AI.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents</title>
<link>https://arxiv.org/abs/2509.04979</link>
<guid>https://arxiv.org/abs/2509.04979</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, large language models, Web of Agents, Agent Ranking, DOVIS

Summary: 
AI agents powered by large language models are expected to transform the internet into a Web of Agents, where autonomous agents interact and collaborate at scale. To achieve this vision, Agent Ranking is essential, selecting agents based on their proven recent performance rather than just declared capabilities. A new operational protocol called DOVIS enables the collection of privacy-preserving aggregates of usage and performance data across the ecosystem. AgentRank-UC, an algorithm that combines usage and competence metrics, is introduced to rank agents dynamically and trust-awarely. The protocol shows promising results in simulation, with guarantees on convergence, robustness, and resistance to malicious actors. By implementing coordinated protocols and performance-aware ranking, a scalable and trustworthy Agentic Web can be realized. 

<br><br>Summary: <div>
arXiv:2509.04979v1 Announce Type: new 
Abstract: AI agents -- powered by reasoning-capable large language models (LLMs) and integrated with tools, data, and web search -- are poised to transform the internet into a \emph{Web of Agents}: a machine-native ecosystem where autonomous agents interact, collaborate, and execute tasks at scale. Realizing this vision requires \emph{Agent Ranking} -- selecting agents not only by declared capabilities but by proven, recent performance. Unlike Web~1.0's PageRank, a global, transparent network of agent interactions does not exist; usage signals are fragmented and private, making ranking infeasible without coordination.
  We propose \textbf{DOVIS}, a five-layer operational protocol (\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that enables the collection of minimal, privacy-preserving aggregates of usage and performance across the ecosystem. On this substrate, we implement \textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines \emph{usage} (selection frequency) and \emph{competence} (outcome quality, cost, safety, latency) into a unified ranking. We present simulation results and theoretical guarantees on convergence, robustness, and Sybil resistance, demonstrating the viability of coordinated protocols and performance-aware ranking in enabling a scalable, trustworthy Agentic Web.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework</title>
<link>https://arxiv.org/abs/2509.05007</link>
<guid>https://arxiv.org/abs/2509.05007</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, test-time scaling, Sticker-TTS, historical experience utilization, mathematical reasoning benchmarks

Summary:
Large reasoning models (LRMs) have shown strong performance on complex reasoning tasks, but current test-time scaling methods lack efficiency by relying on redundant sampling. To address this, the Sticker-TTS framework utilizes historical experience to iteratively explore and refine solutions through collaboration between LRMs. Key conditions, called stickers, are used to extract and reuse critical information, enhancing computational efficiency. A two-stage optimization strategy combines imitation learning with self-improvement for progressive refinement. Evaluation on challenging mathematical reasoning benchmarks demonstrates that Sticker-TTS outperforms strong baselines and reinforcement learning approaches under comparable inference budgets. The effectiveness of sticker-guided historical experience utilization is highlighted in achieving superior performance. The code and data for Sticker-TTS are available on GitHub at https://github.com/RUCAIBox/Sticker-TTS.<br><br>Summary: <div>
arXiv:2509.05007v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) have exhibited strong performance on complex reasoning tasks, with further gains achievable through increased computational budgets at inference. However, current test-time scaling methods predominantly rely on redundant sampling, ignoring the historical experience utilization, thereby limiting computational efficiency. To overcome this limitation, we propose Sticker-TTS, a novel test-time scaling framework that coordinates three collaborative LRMs to iteratively explore and refine solutions guided by historical attempts. At the core of our framework are distilled key conditions-termed stickers-which drive the extraction, refinement, and reuse of critical information across multiple rounds of reasoning. To further enhance the efficiency and performance of our framework, we introduce a two-stage optimization strategy that combines imitation learning with self-improvement, enabling progressive refinement. Extensive evaluations on three challenging mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH, demonstrate that Sticker-TTS consistently surpasses strong baselines, including self-consistency and advanced reinforcement learning approaches, under comparable inference budgets. These results highlight the effectiveness of sticker-guided historical experience utilization. Our code and data are available at https://github.com/RUCAIBox/Sticker-TTS.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding your MUSE: Mining Unexpected Solutions Engine</title>
<link>https://arxiv.org/abs/2509.05072</link>
<guid>https://arxiv.org/abs/2509.05072</guid>
<content:encoded><![CDATA[
<div> Methodology, Functional Concept Graphs, Abstraction, Problem reframing, Analogical inspiration

Summary:
The article discusses cognitive fixation among innovators and presents a methodology for constructing Functional Concept Graphs (FCGs) to support abstraction, problem reframing, and analogical inspiration. These FCGs are interconnected representations of functional elements that aid in overcoming limitations of existing solutions or ideas. The paper introduces MUSE, an algorithm that utilizes FCGs to generate creative inspirations for solving problems. By applying this methodology to analyze 500K patents, the study demonstrates the ability to create large-scale and high-quality FCGs with explicit abstraction relations. The FCGs constructed can facilitate the exploration of novel alternatives and enable innovators to break free from cognitive fixation, leading to more innovative solutions. The dataset containing the FCGs constructed from patents is released for further research. 

<br><br>Summary: <div>
arXiv:2509.05072v1 Announce Type: new 
Abstract: Innovators often exhibit cognitive fixation on existing solutions or nascent ideas, hindering the exploration of novel alternatives. This paper introduces a methodology for constructing Functional Concept Graphs (FCGs), interconnected representations of functional elements that support abstraction, problem reframing, and analogical inspiration. Our approach yields large-scale, high-quality FCGs with explicit abstraction relations, overcoming limitations of prior work. We further present MUSE, an algorithm leveraging FCGs to generate creative inspirations for a given problem. We demonstrate our method by computing an FCG on 500K patents, which we release for further research.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback</title>
<link>https://arxiv.org/abs/2509.05091</link>
<guid>https://arxiv.org/abs/2509.05091</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, Theory of Mind, prosocial behavior, feedback, multi-agent systems <br>
Summary: <br>
- The paper addresses the challenge of promoting prosocial behavior in multi-agent systems using an AI system called ProToM. <br>
- ProToM uses Bayesian inverse planning to infer agents' goals and provides targeted feedback to encourage prosocial actions. <br>
- The approach is evaluated in two environments, demonstrating higher success rates, shorter task completion times, and preference by human users compared to baseline models. <br>
- Current large language and reasoning models struggle to provide contextually grounded and well-timed feedback, leading to higher communication overhead. <br>
- ProToM's ability to offer helpful feedback improves cooperation in multi-agent systems, showcasing the importance of understanding and promoting prosocial behavior in AI applications. <br> <div>
arXiv:2509.05091v1 Announce Type: new 
Abstract: While humans are inherently social creatures, the challenge of identifying when and how to assist and collaborate with others - particularly when pursuing independent goals - can hinder cooperation. To address this challenge, we aim to develop an AI system that provides useful feedback to promote prosocial behaviour - actions that benefit others, even when not directly aligned with one's own goals. We introduce ProToM, a Theory of Mind-informed facilitator that promotes prosocial actions in multi-agent systems by providing targeted, context-sensitive feedback to individual agents. ProToM first infers agents' goals using Bayesian inverse planning, then selects feedback to communicate by maximising expected utility, conditioned on the inferred goal distribution. We evaluate our approach against baselines in two multi-agent environments: Doors, Keys, and Gems, as well as Overcooked. Our results suggest that state-of-the-art large language and reasoning models fall short of communicating feedback that is both contextually grounded and well-timed - leading to higher communication overhead and task speedup. In contrast, ProToM provides targeted and helpful feedback, achieving a higher success rate, shorter task completion times, and is consistently preferred by human users.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation and Comparison Semantics for ODRL</title>
<link>https://arxiv.org/abs/2509.05139</link>
<guid>https://arxiv.org/abs/2509.05139</guid>
<content:encoded><![CDATA[
<div> evaluate, compare, computational policies, Open Digital Rights Language (ODRL), formal semantics

Summary:
The article discusses the evaluation and comparison of computational policies in the Open Digital Rights Language (ODRL). It addresses the lack of a comprehensive formal semantics for ODRL and proposes a new formal semantics based on query answering. This semantics refines previous formalizations and aligns with the latest language specification. Additionally, the paper introduces the problem of comparing two policies in ODRL, aiming to detect equivalent, more restrictive, or more permissive policies. The study is motivated by data sharing scenarios, highlighting the importance of understanding and analyzing the implications of different policies governing digital resource access and usage. <div>
arXiv:2509.05139v1 Announce Type: new 
Abstract: We consider the problem of evaluating, and comparing computational policies in the Open Digital Rights Language (ODRL), which has become the de facto standard for governing the access and usage of digital resources. Although preliminary progress has been made on the formal specification of the language's features, a comprehensive formal semantics of ODRL is still missing. In this paper, we provide a simple and intuitive formal semantics for ODRL that is based on query answering. Our semantics refines previous formalisations, and is aligned with the latest published specification of the language (2.2). Building on our evaluation semantics, and motivated by data sharing scenarios, we also define and study the problem of comparing two policies, detecting equivalent, more restrictive or more permissive policies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</title>
<link>https://arxiv.org/abs/2509.05263</link>
<guid>https://arxiv.org/abs/2509.05263</guid>
<content:encoded><![CDATA[
<div> framework, 3D world generation, LatticeWorld, multimodal inputs, efficiency<br>
Summary:<br>
The paper introduces LatticeWorld, a 3D world generation framework that utilizes lightweight LLMs and industry-grade rendering engines to create dynamic environments. By accepting textual and visual instructions as inputs, LatticeWorld is able to generate large-scale interactive worlds with dynamic agents, competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. Extensive experiments show that LatticeWorld outperforms traditional manual production methods in scene layout generation and visual fidelity while achieving over a 90x increase in industrial production efficiency. This approach bridges the sim-to-real gap, enabling the creation of more realistic simulations for various applications such as embodied AI, autonomous driving, and entertainment. The proposed framework simplifies the process of 3D world modeling, making it more efficient and effective for industrial production pipelines. By leveraging generative methods and advanced machine learning algorithms, LatticeWorld offers a solution for creating realistic 3D virtual environments with accurate physics simulation. <br> <div>
arXiv:2509.05263v1 Announce Type: new 
Abstract: Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer</title>
<link>https://arxiv.org/abs/2303.06298</link>
<guid>https://arxiv.org/abs/2303.06298</guid>
<content:encoded><![CDATA[
<div> architecture, MLP-SRGAN, super resolution, FLAIR MRI, image quality metrics

Summary:
MLP-SRGAN is a new single-dimension Super Resolution Generative Adversarial Network that combines Multi-Layer Perceptron Mixers with convolutional layers for upsampling in the slice direction. It is trained on high resolution FLAIR MRI data and tested on multicentre datasets with low spatial resolution. Results show superior performance in edge sharpness, texture preservation, and fine-anatomical detail compared to existing methods. The model has fewer parameters, faster training/evaluation time, and a smaller size. Performance is measured using PSNR and SSIM for images with high resolution ground truths, as well as new no-reference image quality metrics for sharpness, noise, and blurriness assessment. Code for training, inference, data generators, models, and quality metrics is available on GitHub for further exploration. <br><br>Summary: <div>
arXiv:2303.06298v1 Announce Type: cross 
Abstract: We propose a novel architecture called MLP-SRGAN, which is a single-dimension Super Resolution Generative Adversarial Network (SRGAN) that utilizes Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to upsample in the slice direction. MLP-SRGAN is trained and validated using high resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with low spatial resolution in the slice dimension to examine performance on held-out (unseen) clinical data. Upsampled results are compared to several state-of-the-art SR networks. For images with high resolution (HR) ground truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index (SSIM) are used to measure upsampling performance. Several new structural, no-reference image quality metrics were proposed to quantify sharpness (edge strength), noise (entropy), and blurriness (low frequency information) in the absence of ground truths. Results show MLP-SRGAN results in sharper edges, less blurring, preserves more texture and fine-anatomical detail, with fewer parameters, faster training/evaluation time, and smaller model size than existing methods. Code for MLP-SRGAN training and inference, data generators, models and no-reference image quality metrics will be available at https://github.com/IAMLAB-Ryerson/MLP-SRGAN.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving</title>
<link>https://arxiv.org/abs/2509.02718</link>
<guid>https://arxiv.org/abs/2509.02718</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, LLM routing, online scenarios, algorithm, nearest neighbor search 

Summary: 
This study introduces a novel algorithm for online routing scenarios, aiming to optimize the use of Large Language Models (LLMs). By leveraging approximate nearest neighbor search and one-time optimization over initial queries, the algorithm efficiently estimates query features and learns a routing strategy for future queries without the need for training. The algorithm is proven to achieve a competitive ratio close to 1 under realistic assumptions. Extensive experiments across multiple datasets and baselines show significant improvements in overall performance, cost efficiency, and throughput compared to existing methods. This approach addresses the challenge of adapting LLM routing to online settings with high query volume and token budget constraints, offering a promising solution for optimizing LLM services in cost-efficient and effective ways.<br><br>Summary: <div>
arXiv:2509.02718v1 Announce Type: cross 
Abstract: Increasing demand for Large Language Models (LLMs) services imposes substantial deployment and computation costs on providers. LLM routing offers a cost-efficient solution by directing queries to the optimal LLM based on model and query features. However, existing works primarily focus on offline scenarios and struggle to adapt to online settings with high query volume and constrained token budgets. In this work, we introduce the first training-free algorithm for online routing scenarios. Our algorithm leverages approximate nearest neighbor search to efficiently estimate query features and performs a one-time optimization over a small set of initial queries to learn a routing strategy that guides future routing. We provide theoretical guarantees demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$ under natural assumptions, which is further validated by extensive experiments across 3 benchmark datasets and 8 baselines, showing an average improvement of 3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and nearly 4.25$\times$ in throughput.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge</title>
<link>https://arxiv.org/abs/2509.03614</link>
<guid>https://arxiv.org/abs/2509.03614</guid>
<content:encoded><![CDATA[
<div> Keywords: mitotic figures, artificial intelligence, domain shift, pixel-level segmentation, teacher-student model

Summary:
Mitotic figure counting is time-consuming and prone to variability among pathologists. Artificial intelligence offers a solution by automating detection, but faces challenges such as domain shift and imbalanced data. In this study, a teacher-student model is proposed for mitosis detection and atypical mitosis classification. The model uses a UNet segmentation backbone with domain generalization modules and a multi-scale CNN classifier for classification. By incorporating contrastive representation learning and domain-adversarial training, the algorithm generates pixel-level pseudo-masks for mitoses, hard negatives, and normal nuclei to enhance feature discrimination and mitigate domain shift issues. Preliminary results show promising performance, with an F1 score of 0.7660 for mitosis detection and a balanced accuracy of 0.8414 for atypical mitosis classification. This integrated approach demonstrates the effectiveness of combining segmentation-based detection and classification for robust mitosis analysis. 

<br><br>Summary: <div>
arXiv:2509.03614v1 Announce Type: cross 
Abstract: Counting mitotic figures is time-intensive for pathologists and leads to inter-observer variability. Artificial intelligence (AI) promises a solution by automatically detecting mitotic figures while maintaining decision consistency. However, AI tools are susceptible to domain shift, where a significant drop in performance can occur due to differences in the training and testing sets, including morphological diversity between organs, species, and variations in staining protocols. Furthermore, the number of mitoses is much less than the count of normal nuclei, which introduces severely imbalanced data for the detection task. In this work, we formulate mitosis detection as a pixel-level segmentation and propose a teacher-student model that simultaneously addresses mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our method is based on a UNet segmentation backbone that integrates domain generalization modules, namely contrastive representation learning and domain-adversarial training. A teacher-student strategy is employed to generate pixel-level pseudo-masks not only for annotated mitoses and hard negatives but also for normal nuclei, thereby enhancing feature discrimination and improving robustness against domain shift. For the classification task, we introduce a multi-scale CNN classifier that leverages feature maps from the segmentation model within a multi-task learning paradigm. On the preliminary test set, the algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of 0.8414 in Track 2, demonstrating the effectiveness of integrating segmentation-based detection and classification into a unified framework for robust mitosis analysis.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection</title>
<link>https://arxiv.org/abs/2509.04460</link>
<guid>https://arxiv.org/abs/2509.04460</guid>
<content:encoded><![CDATA[
<div> keywords: large language models, peer review, AI-generated text detectors, content-based detection, CoCoNUTS 

Summary: 
The article discusses the potential risks posed by the integration of large language models (LLMs) into the peer review process and the concerns surrounding their use in generating substantive review content. Existing AI-generated text detectors are limited in distinguishing between language refinement and substantial content generation, leading to unfair suspicion of permissible AI-assisted language enhancement. To address this issue, the authors propose a shift towards content-based detection and introduce CoCoNUTS, a benchmark for evaluating AI-generated peer reviews. They also present CoCoDet, an AI review detector developed through a multi-task learning framework to enhance the accuracy and robustness of detecting AI involvement in review content. This work aims to facilitate more precise, equitable, and reliable detection methods for the use of LLMs in scholarly peer review. 

<br><br>Summary: <div>
arXiv:2509.04460v1 Announce Type: cross 
Abstract: The growing integration of large language models (LLMs) into the peer review process presents potential risks to the fairness and reliability of scholarly evaluation. While LLMs offer valuable assistance for reviewers with language refinement, there is growing concern over their use to generate substantive review content. Existing general AI-generated text detectors are vulnerable to paraphrasing attacks and struggle to distinguish between surface language refinement and substantial content generation, suggesting that they primarily rely on stylistic cues. When applied to peer review, this limitation can result in unfairly suspecting reviews with permissible AI-assisted language enhancement, while failing to catch deceptively humanized AI-generated reviews. To address this, we propose a paradigm shift from style-based to content-based detection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark built upon a fine-grained dataset of AI-generated peer reviews, covering six distinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an AI review detector via a multi-task learning framework, designed to achieve more accurate and robust detection of AI involvement in review content. Our work offers a practical foundation for evaluating the use of LLMs in peer review, and contributes to the development of more precise, equitable, and reliable detection methods for real-world scholarly applications. Our code and data will be publicly available at https://github.com/Y1hanChen/COCONUTS.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking GPT-5 for biomedical natural language processing</title>
<link>https://arxiv.org/abs/2509.04462</link>
<guid>https://arxiv.org/abs/2509.04462</guid>
<content:encoded><![CDATA[
<div> Benchmark, GPT-5, Biomedical NLP, Performance, Question Answering<br>
<br>
Summary: 
The study evaluated the performance of GPT-5 and compared it with GPT-4 and GPT-4o on a standardized BioNLP benchmark. GPT-5 showed significant improvement across various tasks, achieving the strongest overall benchmark performance. It outperformed GPT-4 and GPT-4o in named entity recognition and relation extraction tasks. In question answering, GPT-5 reached high accuracy levels, surpassing supervised state-of-the-art results by a substantial margin. The model demonstrated strong performance in reasoning-oriented biomedical question answering, while areas like summarization and disease NER still require further improvement. The study highlighted the importance of tailored approaches for different tasks, with simple prompting sufficient for some tasks and more complex strategies necessary for others. Overall, the findings suggest that GPT-5 is a promising model for deployment-ready biomedical NLP tasks.<br><br> <div>
arXiv:2509.04462v1 Announce Type: cross 
Abstract: The rapid expansion of biomedical literature has heightened the need for scalable natural language processing (NLP) solutions. While GPT-4 substantially narrowed the gap with task-specific systems, especially in question answering, its performance across other domains remained uneven. We updated a standardized BioNLP benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot prompting across 12 datasets spanning six task families: named entity recognition, relation extraction, multi-label document classification, question answering, text summarization, and text simplification. Using fixed prompt templates, identical decoding parameters, and batch inference, we report primary metrics per dataset and include prior results for GPT-4, GPT-3.5, and LLaMA-2-13B for comparison. GPT-5 achieved the strongest overall benchmark performance, with macro-average scores rising to 0.557 under five-shot prompting versus 0.506 for GPT-4 and 0.508 for GPT-4o. On MedQA, GPT-5 reached 94.1% accuracy, exceeding the previous supervised state of the art by over fifty points, and attained parity with supervised systems on PubMedQA (0.734). In extraction tasks, GPT-5 delivered major gains in chemical NER (0.886 F1) and ChemProt relation extraction (0.616 F1), outperforming GPT-4 and GPT-4o, though summarization and disease NER still lagged behind domain-specific baselines. These results establish GPT-5 as a general-purpose model now offering deployment-ready performance for reasoning-oriented biomedical QA, while precision-critical extraction and evidence-dense summarization continue to favor fine-tuned or hybrid approaches. The benchmark delineates where simple prompting suffices and where retrieval-augmented or planning-based scaffolds are likely required, providing actionable guidance for BioNLP system design as frontier models advance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale Graph Neural Network for Turbulent Flow-Thermal Prediction Around a Complex-Shaped Pin-Fin</title>
<link>https://arxiv.org/abs/2509.04463</link>
<guid>https://arxiv.org/abs/2509.04463</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, flow prediction, complex geometries, surrogate modeling, fast computation <br>
<br>Summary: This study introduces a novel Graph Neural Network designed for predicting steady, turbulent flow and thermal behavior in a 2D channel with complex pin-fin geometries. The network was trained on a dataset generated through an automated framework that included geometry generation, meshing, and flow-field solutions. By converting each simulation into a graph structure, the network was able to accurately predict temperature, velocity, and pressure at each node. The results showed significant improvements in accuracy compared to traditional simulations, particularly in capturing boundary layers and recirculation. Additionally, the network reduced the computational time by 2-3 orders of magnitude, making it a fast and reliable surrogate for complex flow configurations. <div>
arXiv:2509.04463v1 Announce Type: cross 
Abstract: This study presents the development of a domain-responsive edge-aware multiscale Graph Neural Network for predicting steady, turbulent flow and thermal behavior in a two-dimensional channel containing arbitrarily shaped complex pin-fin geometries. The training dataset was constructed through an automated framework that integrated geometry generation, meshing, and flow-field solutions in ANSYS Fluent. The pin-fin geometry was parameterized using piecewise cubic splines, producing 1,000 diverse configurations through Latin Hypercube Sampling. Each simulation was converted into a graph structure, where nodes carried a feature vector containing spatial coordinates, a normalized streamwise position, one-hot boundary indicators, and a signed distance to the nearest boundary such as wall. This graph structure served as input to the newly developed Graph Neural Network, which was trained to predict temperature, velocity magnitude, and pressure at each node using data from ANSYS. The network predicted fields with outstanding accuracy, capturing boundary layers, recirculation, and the stagnation region upstream of the pin-fins while reducing wall time by 2-3 orders of magnitude. In conclusion, the novel graph neural network offered a fast and reliable surrogate for simulations in complex flow configurations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?</title>
<link>https://arxiv.org/abs/2509.04464</link>
<guid>https://arxiv.org/abs/2509.04464</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, uncertainty, diagnosis, knowledge gaps, reliability 

Summary: 

This study focuses on diagnosing the source of uncertainty in Large Language Models (LLMs) by analyzing patterns of disagreement among multiple generated responses. By employing an auxiliary LLM to reason about the likely sources of uncertainty, such as ambiguity in the input question or lack of relevant knowledge, the framework can identify specific missing facts or concepts contributing to uncertainty in cases involving knowledge gaps. The experiment validates the framework on AmbigQA, OpenBookQA, and MMLU-Pro, demonstrating its generality in diagnosing distinct uncertainty sources. The potential for manual interventions to improve LLM performance and reliability is highlighted through this diagnostic approach.  

<br><br>Summary: <div>
arXiv:2509.04464v1 Announce Type: cross 
Abstract: Large language models (LLMs) have delivered significant breakthroughs across diverse domains but can still produce unreliable or misleading outputs, posing critical challenges for real-world applications. While many recent studies focus on quantifying model uncertainty, relatively little work has been devoted to \textit{diagnosing the source of uncertainty}. In this study, we show that, when an LLM is uncertain, the patterns of disagreement among its multiple generated responses contain rich clues about the underlying cause of uncertainty. To illustrate this point, we collect multiple responses from a target LLM and employ an auxiliary LLM to analyze their patterns of disagreement. The auxiliary model is tasked to reason about the likely source of uncertainty, such as whether it stems from ambiguity in the input question, a lack of relevant knowledge, or both. In cases involving knowledge gaps, the auxiliary model also identifies the specific missing facts or concepts contributing to the uncertainty. In our experiment, we validate our framework on AmbigQA, OpenBookQA, and MMLU-Pro, confirming its generality in diagnosing distinct uncertainty sources. Such diagnosis shows the potential for relevant manual interventions that improve LLM performance and reliability.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotionally-Aware Agents for Dispute Resolution</title>
<link>https://arxiv.org/abs/2509.04465</link>
<guid>https://arxiv.org/abs/2509.04465</guid>
<content:encoded><![CDATA[
<div> Keywords: conflict resolution, emotional expressions, text emotion recognition, dispute dialogues, large-language models

Summary:
This study examines how emotional expressions in text shape outcomes in dispute resolution, specifically in buyer-seller interactions. Using a large corpus of dispute dialogues, the researchers found that emotional expressions play a significant role in influencing subjective and objective resolution outcomes. They also compared the effectiveness of large-language models in emotion intensity annotation with previous methods and found that these models were more accurate in matching human annotator decisions. The findings support existing theoretical models on how emotions impact conflict escalation and resolution, suggesting that agent-based systems could potentially help manage disputes by recognizing and potentially mitigating emotional escalation. This research highlights the importance of understanding and utilizing emotional expressions in conflict resolution processes. 

<br><br>Summary: 
1. Emotional expressions influence outcomes in dispute resolution, especially in buyer-seller interactions.
2. Large-language models are effective in emotion intensity annotation compared to previous methods.
3. Emotions play a significant role in conflict escalation and resolution.
4. Agent-based systems could help manage disputes by recognizing and mitigating emotional escalation.
5. Understanding emotional expressions is crucial for effective conflict resolution. <div>
arXiv:2509.04465v1 Announce Type: cross 
Abstract: In conflict, people use emotional expressions to shape their counterparts' thoughts, feelings, and actions. This paper explores whether automatic text emotion recognition offers insight into this influence in the context of dispute resolution. Prior work has shown the promise of such methods in negotiations; however, disputes evoke stronger emotions and different social processes. We use a large corpus of buyer-seller dispute dialogues to investigate how emotional expressions shape subjective and objective outcomes. We further demonstrate that large-language models yield considerably greater explanatory power than previous methods for emotion intensity annotation and better match the decisions of human annotators. Findings support existing theoretical models for how emotional expressions contribute to conflict escalation and resolution and suggest that agent-based systems could be useful in managing disputes by recognizing and potentially mitigating emotional escalation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Just-in-time and distributed task representations in language models</title>
<link>https://arxiv.org/abs/2509.04466</link>
<guid>https://arxiv.org/abs/2509.04466</guid>
<content:encoded><![CDATA[
<div> evolve, task representations, models, context, computational process
Summary:<br><br>In this study, the researchers explore how task representations evolve in language models during context learning. They find that transferrable task representations develop in non-monotonic and sporadic ways, distinct from high-level task categories, condensing multiple evidence into minimal task scopes. These representations are localized along the sequence dimension, indicating a just-in-time computational process for adapting to new evidence. The models rely on temporally-distributed representations for longer and composite tasks. Despite task identity being decodable throughout the context, the accrual of task representations occurs at specific tokens. This research sheds light on the dynamic nature of task learning in language models and the underlying computational mechanisms driving their adaptability. <br><br>Summary: <div>
arXiv:2509.04466v1 Announce Type: cross 
Abstract: Many of language models' impressive capabilities originate from their in-context learning: based on instructions or examples, they can infer and perform new tasks without weight updates. In this work, we investigate \emph{when} representations for new tasks are formed in language models, and \emph{how} these representations change over the course of context. We focus on ''transferrable'' task representations -- vector representations that can restore task context in another instance of the model, even without the full prompt. We show that these representations evolve in non-monotonic and sporadic ways, and are distinct from a more inert representation of high-level task categories that persists throughout the context. Specifically, models often condense multiple evidence into these transferrable task representations, which align well with the performance improvement based on more examples in the context. However, this accrual process exhibits strong locality along the sequence dimension, coming online only at certain tokens -- despite task identity being reliably decodable throughout the context. Moreover, these local but transferrable task representations tend to capture minimal ''task scopes'', such as a semantically-independent subtask, and models rely on more temporally-distributed representations to support longer and composite tasks. This two-fold locality (temporal and semantic) underscores a kind of just-in-time computational process underlying language models' ability to adapt to new evidence and learn new tasks on the fly.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode Disaggregation in Inference</title>
<link>https://arxiv.org/abs/2509.04467</link>
<guid>https://arxiv.org/abs/2509.04467</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, Model Pruning, Prefill-decode disaggregation, Block pruning, KV Cache pruning

Summary: 
Large Language Models (LLMs) are powerful but computationally expensive. Model pruning can help reduce costs, but current methods do not consider prefill-decode (PD) disaggregation. A novel pruning method is proposed in this paper for PD disaggregation, improving block and KV Cache pruning by iteratively removing blocks in the prefill and decode stages independently. Additionally, a token-aware cache pruning mechanism is introduced to reduce communication costs during decode by selectively reusing entries for certain token sequences in selected layers. Experimental results show that the approach achieves significant speedups and bandwidth reductions in both PD disaggregation and unified settings. Under default settings, the method achieves a 20.56% inference speedup and a 4.95 times reduction in data transmission bandwidth consumption. 

<br><br>Summary: <div>
arXiv:2509.04467v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However, existing methods often ignore the characteristics of prefill-decode (PD) disaggregation in practice. In this paper, we propose a novel pruning method for PD disaggregation inference, enabling more precise and efficient block and KV Cache pruning. Our approach constructs pruning and distillation sets to perform iterative block removal independently for the prefill and decode stages, obtaining better pruning solutions. Moreover, we introduce a token-aware cache pruning mechanism that retains all KV Cache in the prefill stage but selectively reuses entries for the first and last token sequences in selected layers during decode, reducing communication costs with minimal overhead. Extensive experiments demonstrate that our approach consistently achieves strong performance in both PD disaggregation and PD unified settings without disaggregation. Under the default settings, our method achieves a 20.56% inference speedup and a 4.95 times reduction in data transmission bandwidth consumption.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark Study</title>
<link>https://arxiv.org/abs/2509.04468</link>
<guid>https://arxiv.org/abs/2509.04468</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, financial applications, professional certifications, domain-specific knowledge retrieval, model selection

Summary:
The study evaluates state-of-the-art large language models (LLMs) in specialized financial contexts using 1,560 multiple-choice questions from CFA mock exams. Three types of models are compared based on design priorities: multi-modal and powerful, reasoning-specialized and accurate, and efficiency-optimized. Results show that reasoning-oriented models perform better in zero-shot settings, and a Retrieval-Augmented Generation pipeline significantly improves reasoning accuracy in complex scenarios by integrating CFA curriculum content. Knowledge gaps are identified as the main failure mode, with minimal impact from text readability. This research provides insights for deploying LLMs in finance, guiding practitioners on model selection and cost-performance optimization.<br><br>Summary: <div>
arXiv:2509.04468v1 Announce Type: cross 
Abstract: The rapid advancement of large language models presents significant opportunities for financial applications, yet systematic evaluation in specialized financial contexts remains limited. This study presents the first comprehensive evaluation of state-of-the-art LLMs using 1,560 multiple-choice questions from official mock exams across Levels I-III of CFA, most rigorous professional certifications globally that mirror real-world financial analysis complexity. We compare models distinguished by core design priorities: multi-modal and computationally powerful, reasoning-specialized and highly accurate, and lightweight efficiency-optimized.
  We assess models under zero-shot prompting and through a novel Retrieval-Augmented Generation pipeline that integrates official CFA curriculum content. The RAG system achieves precise domain-specific knowledge retrieval through hierarchical knowledge organization and structured query generation, significantly enhancing reasoning accuracy in professional financial certification evaluation.
  Results reveal that reasoning-oriented models consistently outperform others in zero-shot settings, while the RAG pipeline provides substantial improvements particularly for complex scenarios. Comprehensive error analysis identifies knowledge gaps as the primary failure mode, with minimal impact from text readability. These findings provide actionable insights for LLM deployment in finance, offering practitioners evidence-based guidance for model selection and cost-performance optimization.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing</title>
<link>https://arxiv.org/abs/2509.04469</link>
<guid>https://arxiv.org/abs/2509.04469</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal large language models, invoice document datasets, zero-shot prompting, image processing, structured parsing

Summary:
This paper evaluates the performance of eight multi-modal large language models on three diverse invoice document datasets using zero-shot prompting. The models compared are from families GPT-5, Gemini 2.5, and open-source Gemma 3. Two processing strategies are analyzed: direct image processing with multi-modal capabilities and structured parsing converting documents to markdown first. The results indicate that native image processing generally outperforms structured approaches, although performance varies depending on the model type and document characteristics. This benchmark study provides valuable insights for choosing appropriate models and processing strategies for automated document systems. The code used for the study is available online. 

<br><br>Summary: <div>
arXiv:2509.04469v1 Announce Type: cross 
Abstract: This paper benchmarks eight multi-modal large language models from three families (GPT-5, Gemini 2.5, and open-source Gemma 3) on three diverse openly available invoice document datasets using zero-shot prompting. We compare two processing strategies: direct image processing using multi-modal capabilities and a structured parsing approach converting documents to markdown first. Results show native image processing generally outperforms structured approaches, with performance varying across model types and document characteristics. This benchmark provides insights for selecting appropriate models and processing strategies for automated document systems. Our code is available online.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COCORELI: Cooperative, Compositional Reconstitution \&amp; Execution of Language Instructions</title>
<link>https://arxiv.org/abs/2509.04470</link>
<guid>https://arxiv.org/abs/2509.04470</guid>
<content:encoded><![CDATA[
<div> Abstraction, Language Models, Spatial Reasoning, Collaborative Construction Tasks, ToolBench API<br>
<br>
Summary: COCORELI is a hybrid agent framework that addresses the limitations of large language models (LLMs) by integrating medium-sized LLM agents with novel abstraction mechanisms and a discourse module. It is designed to handle tasks requiring complex instructions, minimizing hallucination, and spatial reasoning. In experiments on collaborative construction tasks, COCORELI outperformed single-LLM CoT and agentic LLM systems, all using larger LLMs. It demonstrated the ability to avoid hallucinations, identify missing information, ask for clarifications, and update its learned objects. COCORELI's abstraction abilities were also showcased in the ToolBench API completion task, highlighting its versatility beyond just environment parsing. <div>
arXiv:2509.04470v1 Announce Type: cross 
Abstract: We present COCORELI, a hybrid agent framework designed to tackle the limitations of large language models (LLMs) in tasks requiring: following complex instructions, minimizing hallucination, and spatial reasoning. COCORELI integrates medium-sized LLM agents with novel abstraction mechanisms and a discourse module to parse instructions to in-context learn dynamic, high-level representations of the environment. Experiments on natural collaborative construction tasks show that COCORELI outperforms single-LLM CoT and agentic LLM systems, all using larger LLMs. It manages to largely avoid hallucinations, identify missing information, ask for clarifications, and update its learned objects. COCORELI's abstraction abilities extend beyond ENVIRONMENT, as shown in the ToolBench API completion task.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSAIC: A Multilingual, Taxonomy-Agnostic, and Computationally Efficient Approach for Radiological Report Classification</title>
<link>https://arxiv.org/abs/2509.04471</link>
<guid>https://arxiv.org/abs/2509.04471</guid>
<content:encoded><![CDATA[
<div> radiology reports, clinical information, imaging models, MOSAIC, multilingual <br>
<br>
Summary: 
The article introduces MOSAIC, a novel approach for radiological report classification that leverages radiology reports' rich clinical information to train imaging models efficiently. MOSAIC overcomes limitations of existing methods by being multilingual, taxonomy-agnostic, and computationally efficient. It is built on a compact open-access language model, MedGemma-4B, enabling zero-/few-shot prompting and lightweight fine-tuning on consumer-grade GPUs. Evaluation across seven datasets in multiple languages and modalities shows MOSAIC achieving high performance levels with minimal annotated samples, making it a practical alternative to large or proprietary language models in clinical settings. The model achieves an impressive mean macro F1 score of 88 across chest X-ray datasets and demonstrates the effectiveness of data augmentation in reducing the required amount of annotated data for training. MOSAIC's code and models are open-source, inviting the community to explore its capabilities across various languages, taxonomies, and modalities. <div>
arXiv:2509.04471v1 Announce Type: cross 
Abstract: Radiology reports contain rich clinical information that can be used to train imaging models without relying on costly manual annotation. However, existing approaches face critical limitations: rule-based methods struggle with linguistic variability, supervised models require large annotated datasets, and recent LLM-based systems depend on closed-source or resource-intensive models that are unsuitable for clinical use. Moreover, current solutions are largely restricted to English and single-modality, single-taxonomy datasets. We introduce MOSAIC, a multilingual, taxonomy-agnostic, and computationally efficient approach for radiological report classification. Built on a compact open-access language model (MedGemma-4B), MOSAIC supports both zero-/few-shot prompting and lightweight fine-tuning, enabling deployment on consumer-grade GPUs. We evaluate MOSAIC across seven datasets in English, Spanish, French, and Danish, spanning multiple imaging modalities and label taxonomies. The model achieves a mean macro F1 score of 88 across five chest X-ray datasets, approaching or exceeding expert-level performance, while requiring only 24 GB of GPU memory. With data augmentation, as few as 80 annotated samples are sufficient to reach a weighted F1 score of 82 on Danish reports, compared to 86 with the full 1600-sample training set. MOSAIC offers a practical alternative to large or proprietary LLMs in clinical settings. Code and models are open-source. We invite the community to evaluate and extend MOSAIC on new languages, taxonomies, and modalities.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECAP: REwriting Conversations for Intent Understanding in Agentic Planning</title>
<link>https://arxiv.org/abs/2509.04472</link>
<guid>https://arxiv.org/abs/2509.04472</guid>
<content:encoded><![CDATA[
<div> intent rewriting, conversational assistants, large language models, dialogue systems, user intent

Summary:
RECAP (REwriting Conversations for Agent Planning) is proposed as a benchmark to evaluate intent rewriting in user-agent dialogues. It aims to address challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. An LLM-based evaluator is introduced to assess planning utility based on rewritten intents. A prompt-based rewriting approach is developed and shown to outperform baselines. Fine-tuning two DPO-based rewriters further improves utility. The results highlight intent rewriting as a critical component for enhancing agent planning in open-domain dialogue systems. <div>
arXiv:2509.04472v1 Announce Type: cross 
Abstract: Understanding user intent is essential for effective planning in conversational assistants, particularly those powered by large language models (LLMs) coordinating multiple agents. However, real-world dialogues are often ambiguous, underspecified, or dynamic, making intent detection a persistent challenge. Traditional classification-based approaches struggle to generalize in open-ended settings, leading to brittle interpretations and poor downstream planning. We propose RECAP (REwriting Conversations for Agent Planning), a new benchmark designed to evaluate and advance intent rewriting, reframing user-agent dialogues into concise representations of user goals. RECAP captures diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. Alongside the dataset, we introduce an LLM-based evaluator that assesses planning utility given the rewritten intent. Using RECAP, we develop a prompt-based rewriting approach that outperforms baselines. We further demonstrate that fine-tuning two DPO-based rewriters yields additional utility gains. Our results highlight intent rewriting as a critical and tractable component for improving agent planning in open-domain dialogue systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechLLM: Unified Speech and Language Model for Enhanced Multi-Task Understanding in Low Resource Settings</title>
<link>https://arxiv.org/abs/2509.04473</link>
<guid>https://arxiv.org/abs/2509.04473</guid>
<content:encoded><![CDATA[
<div> adapter, speech embeddings, LLM-compatible tokens, automatic speech recognition, synthetic dataset annotation

Summary: 
An innovative solution using a parameter-efficient adapter to convert speech embeddings into tokens compatible with large language models (LLMs) has been proposed. Through this approach, significant performance enhancements have been achieved in various tasks such as automatic speech recognition (ASR), named entity recognition (NER), and sentiment analysis (SA). By leveraging an LLM-based synthetic dataset annotation technique, labeling costs have been reduced effectively. The adapter, which utilizes fewer trainable parameters, has led to a 26% relative improvement in Word Error Rates (WER) for the LibriSpeech ASR task, a 6.3% relative increase in F1 score for the NER task, and a 32% relative boost in F1 score for the SA task. Additionally, techniques such as adding a classifier regularizer and optimizing the LLM with Low-Rank Adaptation (LoRA) have further improved performance, resulting in a notable increase in the Spoken Language Understanding Evaluation (SLUE) score by 6.6% and 9.5%. 

<br><br>Summary: <div>
arXiv:2509.04473v1 Announce Type: cross 
Abstract: While integrating speech encoder with LLM requires substantial data and resources, use cases face limitations due to insufficient availability. To address this, we propose a solution with a parameter-efficient adapter that converts speech embeddings into LLM-compatible tokens, focusing on end-to-end automatic speech recognition (ASR), named entity recognition (NER), and sentiment analysis (SA). To reduce labeling costs, we employ an LLM-based synthetic dataset annotation technique. The proposed adapter, using 7x fewer trainable parameters, achieves significant performance gains: a 26% relative Word Error Rates (WER) improvement on the LibriSpeech ASR task, a 6.3% relative F1 score increase on the NER task, and a 32% relative F1 score boost on the SA task. Moreover, using advanced techniques such as adding a classifier regularizer and optimizing the LLM with Low-Rank Adaptation (LoRA) yields notable performance gains, with Spoken Language Understanding Evaluation (SLUE) score improvement of 6.6% and 9.5%
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling</title>
<link>https://arxiv.org/abs/2509.04474</link>
<guid>https://arxiv.org/abs/2509.04474</guid>
<content:encoded><![CDATA[
<div> scalable language models, test-time scaling, speculative decoding, benchmark, reasoning 

Summary: 
The article discusses the inefficiencies of test-time scaling in large language models (LLMs) due to redundant reasoning traces. It introduces a benchmark to evaluate speculative decoding methods for accelerating LLM test-time scaling. The benchmark allows for a fair comparison of model-based, training-based, and n-gram-based speculative decoding approaches. The experiments highlight the effectiveness of simple n-gram-based methods in capturing repetitive patterns and accelerating test-time scaling. Integrating n-gram-based methods with model-based or training-based approaches can balance acceleration for both repetitive and diverse reasoning paths in test-time scaling. This research aims to spur further investigation into speculative decoding for test-time scaling, ultimately enabling faster and more practical reasoning in LLMs by improving the handling of repetitive and diverse reasoning paths. 

<br><br>Summary: <div>
arXiv:2509.04474v1 Announce Type: cross 
Abstract: Test-time scaling has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs) by allocating additional computational resources during inference. However, this paradigm is inherently inefficient due to the generation of redundant and repetitive reasoning traces, leading to significant computational overhead. Speculative decoding offers a promising avenue for mitigating this inefficiency, yet its efficacy in the structured, repetition-rich context of test-time scaling remains largely unexplored. To bridge this gap, we introduce the first comprehensive benchmark designed to evaluate speculative decoding methods for accelerating LLM test-time scaling. Our benchmark provides consistent experimental protocols across representative test-time scaling paradigms (e.g., Best-of-N sampling and multi-round thinking), enabling a fair comparison of three major categories of speculative decoding: model-based, training-based, and n-gram-based methods. Extensive experiments reveal that simple n-gram-based methods effectively capture repetitive patterns, demonstrating unique potential in accelerating test-time scaling. This phenomenon demonstrates the value of integrating n-gram-based methods with model-based or training-based approaches to balance acceleration for both repetitive and diverse reasoning in test-time scaling. We hope this benchmark spurs further research on speculative decoding for test-time scaling, enabling faster and more practical reasoning in LLMs through better handling of repetitive and diverse reasoning paths.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute</title>
<link>https://arxiv.org/abs/2509.04475</link>
<guid>https://arxiv.org/abs/2509.04475</guid>
<content:encoded><![CDATA[
<div> scaling, Large Language Models, reasoning, Parallel thinking, Tunnel Vision 

Summary:
The article introduces a new scaling paradigm called native thought parallelism to overcome the limitations of the traditional test-time compute scaling strategy used in Large Language Models (LLMs). The phenomenon of "Tunnel Vision" is discussed, where sequential reasoning locks a model into suboptimal paths. The ParaThinker framework is presented, which trains LLMs to generate multiple diverse reasoning paths in parallel to synthesize a superior final answer. By exploring different lines of thought simultaneously, ParaThinker effectively avoids Tunnel Vision and unlocks the model's latent reasoning potential. The approach of scaling compute in parallel is shown to be more effective and efficient than scaling sequentially. Results on challenging reasoning benchmarks demonstrate significant accuracy improvements with negligible latency overhead, enabling smaller models to outperform larger ones. The study establishes parallel thinking as a critical dimension for scaling future LLMs. 

<br><br>Summary: <div>
arXiv:2509.04475v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have been driven by test-time compute scaling - a strategy that improves reasoning by generating longer, sequential thought processes. While effective, this approach encounters a significant bottleneck as computation increases, where further computation offers only marginal performance gains. We argue this ceiling is not an inherent limit of the model's capability but a flaw in the scaling strategy itself, a phenomenon we term "Tunnel Vision", where a model's imperfect initial steps lock it into a suboptimal reasoning path. To overcome this, we introduce a new scaling paradigm: native thought parallelism. We present ParaThinker, an end-to-end framework that trains an LLM to generate multiple, diverse reasoning paths in parallel and synthesize them into a superior final answer. By exploring different lines of thoughts simultaneously, ParaThinker effectively sidesteps the Tunnel Vision issue and unlocks the model's latent reasoning potential. Our approach demonstrates that scaling compute in parallel (width) is a more effective and efficient way to superior reasoning than simply scaling sequentially (depth). On challenging reasoning benchmarks, ParaThinker achieves substantial accuracy improvements over sequential LLMs (12.3% for 1.5B and 7.5% for 7B models on average with 8 parallel paths), while adding only negligible latency overhead (7.1%). This enables smaller models to surpass much larger counterparts and establishes parallel thinking as a critical, efficient dimension for scaling future LLMs.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Text-to-Molecule Models with Context-Aware Tokenization</title>
<link>https://arxiv.org/abs/2509.04476</link>
<guid>https://arxiv.org/abs/2509.04476</guid>
<content:encoded><![CDATA[
<div> substructure-level tokenization, text-to-molecule models, molecule structures, importance-based training, CAMT5 <br>
<br>
Summary: 
The article introduces a novel text-to-molecule model called Context-Aware Molecular T5 (CAMT5) that addresses the limitations of existing models in capturing global structural context within molecules. By using substructure-level tokenization, CAMT5 focuses on the importance of substructure-level contexts, such as ring systems, in understanding molecule structures. The model also employs an importance-based training strategy to prioritize key substructures, allowing it to better capture molecular semantics. Experimental results demonstrate that CAMT5 outperforms state-of-the-art methods with significantly fewer training tokens. Furthermore, the article proposes an ensemble strategy to further enhance the generation performance by aggregating the outputs of different text-to-molecule models. The code for CAMT5 is available on GitHub for further exploration and implementation. <br> <div>
arXiv:2509.04476v1 Announce Type: cross 
Abstract: Recently, text-to-molecule models have shown great potential across various chemical applications, e.g., drug-discovery. These models adapt language models to molecular data by representing molecules as sequences of atoms. However, they rely on atom-level tokenizations, which primarily focus on modeling local connectivity, thereby limiting the ability of models to capture the global structural context within molecules. To tackle this issue, we propose a novel text-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by the significance of the substructure-level contexts in understanding molecule structures, e.g., ring systems, we introduce substructure-level tokenization for text-to-molecule models. Building on our tokenization scheme, we develop an importance-based training strategy that prioritizes key substructures, enabling CAMT5 to better capture the molecular semantics. Extensive experiments verify the superiority of CAMT5 in various text-to-molecule generation tasks. Intriguingly, we find that CAMT5 outperforms the state-of-the-art methods using only 2% of training tokens. In addition, we propose a simple yet effective ensemble strategy that aggregates the outputs of text-to-molecule models to further boost the generation performance. Code is available at https://github.com/Songhyeontae/CAMT5.git.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Clustering, No Routing: How Transformers Actually Process Rare Tokens</title>
<link>https://arxiv.org/abs/2509.04479</link>
<guid>https://arxiv.org/abs/2509.04479</guid>
<content:encoded><![CDATA[
<div> specialization, rare token prediction, plateau neurons, computational regimes, attention mechanisms

Summary:
Rare token prediction poses a challenge for large language models, leading to the identification of specialized "plateau" neurons. This study delves into the functional organization of these neurons using GPT-2 XL and Pythia models. The research reveals that processing rare tokens requires a separate set of plateau neurons, distinct from those handling common tokens, resulting in dual computational regimes. Interestingly, these plateau neurons are distributed across the model rather than being clustered together. Furthermore, attention mechanisms do not exhibit a bias towards routing to these specialized neurons. Overall, the specialization for rare tokens emerges through distributed, training-induced differentiation rather than modular clustering, allowing for context-sensitive flexibility while efficiently allocating computational resources. <div>
arXiv:2509.04479v1 Announce Type: cross 
Abstract: Large language models struggle with rare token prediction, yet the mechanisms driving their specialization remain unclear. Prior work identified specialized ``plateau'' neurons for rare tokens following distinctive three-regime influence patterns \cite{liu2025emergent}, but their functional organization is unknown. We investigate this through neuron influence analyses, graph-based clustering, and attention head ablations in GPT-2 XL and Pythia models. Our findings show that: (1) rare token processing requires additional plateau neurons beyond the power-law regime sufficient for common tokens, forming dual computational regimes; (2) plateau neurons are spatially distributed rather than forming modular clusters; and (3) attention mechanisms exhibit no preferential routing to specialists. These results demonstrate that rare token specialization arises through distributed, training-driven differentiation rather than architectural modularity, preserving context-sensitive flexibility while achieving adaptive capacity allocation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game Environments</title>
<link>https://arxiv.org/abs/2509.04481</link>
<guid>https://arxiv.org/abs/2509.04481</guid>
<content:encoded><![CDATA[
<div> pipeline, narrative prompts, tile-based game scenes, spatial predicates, spatial rules

Summary:
The article introduces a lightweight pipeline that converts short narrative prompts into 2D tile-based game scenes, aligning with the temporal structure of stories. It identifies three key time frames in LLM-generated narratives, extracts spatial predicates in "Object-Relation-Object" form, and utilizes affordance-aware semantic embeddings to retrieve visual assets. A layered terrain is created using Cellular Automata, and objects are placed based on spatial rules grounded in the predicate structure. The system was evaluated with ten diverse stories, assessing tile-object matching, affordance-layer alignment, and spatial constraint satisfaction across frames. This prototype offers a scalable solution for generating narrative-driven game scenes and sets the groundwork for future research on multi-frame continuity, symbolic tracking, and multi-agent coordination in story-centered PCG. 

<br><br>Summary: <div>
arXiv:2509.04481v1 Announce Type: cross 
Abstract: Recent advances in large language models(LLMs) enable compelling story generation, but connecting narrative text to playable visual environments remains an open challenge in procedural content generation(PCG). We present a lightweight pipeline that transforms short narrative prompts into a sequence of 2D tile-based game scenes, reflecting the temporal structure of stories. Given an LLM-generated narrative, our system identifies three key time frames, extracts spatial predicates in the form of "Object-Relation-Object" triples, and retrieves visual assets using affordance-aware semantic embeddings from the GameTileNet dataset. A layered terrain is generated using Cellular Automata, and objects are placed using spatial rules grounded in the predicate structure. We evaluated our system in ten diverse stories, analyzing tile-object matching, affordance-layer alignment, and spatial constraint satisfaction across frames. This prototype offers a scalable approach to narrative-driven scene generation and lays the foundation for future work on multi-frame continuity, symbolic tracking, and multi-agent coordination in story-centered PCG.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented Large Language Models for Healthcare</title>
<link>https://arxiv.org/abs/2509.04482</link>
<guid>https://arxiv.org/abs/2509.04482</guid>
<content:encoded><![CDATA[
<div> energy-based model, reliable abstention, retrieval-augmented generation, safety-critical domains, semantic corpus

Summary:
The article introduces an energy-based model (EBM) for reliable abstention in retrieval-augmented generation (RAG) systems, focusing on safety-critical areas like women's health. The EBM learns a smooth energy landscape over a large semantic corpus of guideline-derived questions to determine when to generate or abstain. Comparative benchmarks against softmax and kNN approaches show that the EBM outperforms in abstention performance on difficult, near-distribution queries, reaching AUROC 0.961 and reducing FPR@95. The EBM's strength lies in its energy scoring head, providing a more reliable confidence signal than probability-based softmax. The study demonstrates that energy-based abstention scoring offers scalability and interpretability for safe RAG systems, emphasizing the importance of reliable abstention in critical domains. <div>
arXiv:2509.04482v1 Announce Type: cross 
Abstract: Reliable abstention is critical for retrieval-augmented generation (RAG) systems, particularly in safety-critical domains such as women's health, where incorrect answers can lead to harm. We present an energy-based model (EBM) that learns a smooth energy landscape over a dense semantic corpus of 2.6M guideline-derived questions, enabling the system to decide when to generate or abstain. We benchmark the EBM against a calibrated softmax baseline and a k-nearest neighbour (kNN) density heuristic across both easy and hard abstention splits, where hard cases are semantically challenging near-distribution queries. The EBM achieves superior abstention performance abstention on semantically hard cases, reaching AUROC 0.961 versus 0.950 for softmax, while also reducing FPR@95 (0.235 vs 0.331). On easy negatives, performance is comparable across methods, but the EBM's advantage becomes most pronounced in safety-critical hard distributions. A comprehensive ablation with controlled negative sampling and fair data exposure shows that robustness stems primarily from the energy scoring head, while the inclusion or exclusion of specific negative types (hard, easy, mixed) sharpens decision boundaries but is not essential for generalisation to hard cases. These results demonstrate that energy-based abstention scoring offers a more reliable confidence signal than probability-based softmax confidence, providing a scalable and interpretable foundation for safe RAG systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DecMetrics: Structured Claim Decomposition Scoring for Factually Consistent LLM Outputs</title>
<link>https://arxiv.org/abs/2509.04483</link>
<guid>https://arxiv.org/abs/2509.04483</guid>
<content:encoded><![CDATA[
<div> metrics, claim decomposition, fact-checking, generative methods, evaluation

Summary:
DecMetrics introduces three new metrics, COMPLETENESS, CORRECTNESS, and SEMANTIC ENTROPY, to evaluate the quality of claims produced by decomposition models. These metrics aim to assess the completeness, correctness, and semantic entropy of atomic claims generated through claim decomposition. By integrating these metrics as a reward function, a lightweight claim decomposition model is developed and optimized for performance. The approach sets a benchmark for claim decomposition in the fact-checking process, enhancing the reliability and effectiveness of fact-checking systems. The focus on evaluating the quality of decomposed claims provides a valuable contribution to the field, addressing a gap in existing research that primarily concentrates on generative methods for claim decomposition. <div>
arXiv:2509.04483v1 Announce Type: cross 
Abstract: Claim decomposition plays a crucial role in the fact-checking process by breaking down complex claims into simpler atomic components and identifying their unfactual elements. Despite its importance, current research primarily focuses on generative methods for decomposition, with insufficient emphasis on evaluating the quality of these decomposed atomic claims. To bridge this gap, we introduce \textbf{DecMetrics}, which comprises three new metrics: \texttt{COMPLETENESS}, \texttt{CORRECTNESS}, and \texttt{SEMANTIC ENTROPY}, designed to automatically assess the quality of claims produced by decomposition models. Utilizing these metrics, we develop a lightweight claim decomposition model, optimizing its performance through the integration of these metrics as a reward function. Through automatic evaluation, our approach aims to set a benchmark for claim decomposition, enhancing both the reliability and effectiveness of fact-checking systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors</title>
<link>https://arxiv.org/abs/2509.04484</link>
<guid>https://arxiv.org/abs/2509.04484</guid>
<content:encoded><![CDATA[
<div> Automated Feedback, Peer Review, Review Comments, RevUtil Dataset, Fine-tuned Models<br>
<br>
Summary: Providing constructive feedback to authors is crucial in peer review. To ensure high reviewing quality despite time constraints, automated support systems are necessary. Four key aspects of review comments that enhance their utility for authors include Actionability, Grounding & Specificity, Verifiability, and Helpfulness. The RevUtil dataset, comprising human-labeled and synthetically labeled review comments, serves as a tool for evaluating and developing models assessing review comments. By fine-tuning models on the RevUtil dataset, researchers achieve agreement levels with humans comparable to, and sometimes surpassing, powerful closed models like GPT-4o. However, machine-generated reviews generally fall short of human reviews across the four key aspects identified in review comments. <div>
arXiv:2509.04484v1 Announce Type: cross 
Abstract: Providing constructive feedback to paper authors is a core component of peer review. With reviewers increasingly having less time to perform reviews, automated support systems are required to ensure high reviewing quality, thus making the feedback in reviews useful for authors. To this end, we identify four key aspects of review comments (individual points in weakness sections of reviews) that drive the utility for authors: Actionability, Grounding & Specificity, Verifiability, and Helpfulness. To enable evaluation and development of models assessing review comments, we introduce the RevUtil dataset. We collect 1,430 human-labeled review comments and scale our data with 10k synthetically labeled comments for training purposes. The synthetic data additionally contains rationales, i.e., explanations for the aspect score of a review comment. Employing the RevUtil dataset, we benchmark fine-tuned models for assessing review comments on these aspects and generating rationales. Our experiments demonstrate that these fine-tuned models achieve agreement levels with humans comparable to, and in some cases exceeding, those of powerful closed models like GPT-4o. Our analysis further reveals that machine-generated reviews generally underperform human reviews on our four aspects.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records</title>
<link>https://arxiv.org/abs/2509.04485</link>
<guid>https://arxiv.org/abs/2509.04485</guid>
<content:encoded><![CDATA[
<div> Keywords: ASCENDgpt, cardiovascular risk prediction, electronic health records, transformer-based model, phenotype-aware tokenization

Summary:
ASCENDgpt is a transformer-based model designed for cardiovascular risk prediction from electronic health records (EHRs). It employs a novel phenotype-aware tokenization scheme that consolidates raw ICD codes into clinically meaningful phenotype tokens, reducing vocabulary size significantly. Pretrained on sequences from unique individuals, ASCENDgpt achieves strong performance in predicting five cardiovascular outcomes, including myocardial infarction, stroke, and cardiovascular death. The model demonstrates excellent discrimination on a test set, with an average C-index of 0.816. The phenotype-based approach ensures clinically interpretable predictions while maintaining computational efficiency. This study highlights the effectiveness of domain-specific tokenization and pretraining for EHR-based risk prediction tasks.<br><br>Summary: <div>
arXiv:2509.04485v1 Announce Type: cross 
Abstract: We present ASCENDgpt, a transformer-based model specifically designed for cardiovascular risk prediction from longitudinal electronic health records (EHRs). Our approach introduces a novel phenotype-aware tokenization scheme that maps 47,155 raw ICD codes to 176 clinically meaningful phenotype tokens, achieving 99.6\% consolidation of diagnosis codes while preserving semantic information. This phenotype mapping contributes to a total vocabulary of 10,442 tokens - a 77.9\% reduction when compared with using raw ICD codes directly. We pretrain ASCENDgpt on sequences derived from 19402 unique individuals using a masked language modeling objective, then fine-tune for time-to-event prediction of five cardiovascular outcomes: myocardial infarction (MI), stroke, major adverse cardiovascular events (MACE), cardiovascular death, and all-cause mortality. Our model achieves excellent discrimination on the held-out test set with an average C-index of 0.816, demonstrating strong performance across all outcomes (MI: 0.792, stroke: 0.824, MACE: 0.800, cardiovascular death: 0.842, all-cause mortality: 0.824). The phenotype-based approach enables clinically interpretable predictions while maintaining computational efficiency. Our work demonstrates the effectiveness of domain-specific tokenization and pretraining for EHR-based risk prediction tasks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Serialized Output Prompting for Large Language Model-based Multi-Talker Speech Recognition</title>
<link>https://arxiv.org/abs/2509.04488</link>
<guid>https://arxiv.org/abs/2509.04488</guid>
<content:encoded><![CDATA[
<div> Prompt, multi-talker automatic speech recognition, large language models, separated output prompts, structured prompts<br>
Summary:<br>
The paper proposes a method, SOP-MT-ASR, to enhance multi-talker automatic speech recognition systems using serialized output prompts (SOP) to guide large language models. By inserting Separator and serialized CTC layers after the speech encoder, the system separates and extracts MT content in a first-speaking-first-out manner. The extracted SOP serves as a structured prompt for the LLMs, improving system performance. A three-stage training strategy is designed for effective model training. Experimental results on the LibriMix dataset show that while the LLM-based SOT model performs well in two-talker scenarios, it struggles in more complex three-talker scenarios. The proposed SOP approach significantly enhances performance in both scenarios. <br><br> <div>
arXiv:2509.04488v1 Announce Type: cross 
Abstract: Prompts are crucial for task definition and for improving the performance of large language models (LLM)-based systems. However, existing LLM-based multi-talker (MT) automatic speech recognition (ASR) systems either omit prompts or rely on simple task-definition prompts, with no prior work exploring the design of prompts to enhance performance. In this paper, we propose extracting serialized output prompts (SOP) and explicitly guiding the LLM using structured prompts to improve system performance (SOP-MT-ASR). A Separator and serialized Connectionist Temporal Classification (CTC) layers are inserted after the speech encoder to separate and extract MT content from the mixed speech encoding in a first-speaking-first-out manner. Subsequently, the SOP, which serves as a prompt for LLMs, is obtained by decoding the serialized CTC outputs using greedy search. To train the model effectively, we design a three-stage training strategy, consisting of serialized output training (SOT) fine-tuning, serialized speech information extraction, and SOP-based adaptation. Experimental results on the LibriMix dataset show that, although the LLM-based SOT model performs well in the two-talker scenario, it fails to fully leverage LLMs under more complex conditions, such as the three-talker scenario. The proposed SOP approach significantly improved performance under both two- and three-talker conditions.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR</title>
<link>https://arxiv.org/abs/2509.04491</link>
<guid>https://arxiv.org/abs/2509.04491</guid>
<content:encoded><![CDATA[
<div> Subtitle, Weakly Supervised, Automatic Speech Recognition, Pseudo-labeled datasets, Transcription accuracy
Summary: 
Subtitle text and spoken audio discrepancies limit the applicability of TV subtitles in Automatic Speech Recognition (ASR). This study proposes a novel approach using subtitles as context-rich prompts rather than direct supervision signals. Generated pseudo transcripts, guided by subtitles, are iteratively refined to improve transcription accuracy. A weighted attention mechanism emphasizes relevant subtitle tokens during inference, leading to significant improvements in transcription quality. The method provides high-quality pseudo-labeled datasets for training robust ASR systems. Enhanced transcripts offer a valuable resource for developing accurate and efficient ASR models. <div>
arXiv:2509.04491v1 Announce Type: cross 
Abstract: This study proposes a novel approach to using TV subtitles within a weakly supervised (WS) Automatic Speech Recognition (ASR) framework. Although TV subtitles are readily available, their imprecise alignment with corresponding audio limits their applicability as supervised targets for verbatim transcription. Rather than using subtitles as direct supervision signals, our method reimagines them as context-rich prompts. This design enables the model to handle discrepancies between spoken audio and subtitle text. Instead, generated pseudo transcripts become the primary targets, with subtitles acting as guiding cues for iterative refinement. To further enhance the process, we introduce a weighted attention mechanism that emphasizes relevant subtitle tokens during inference. Our experiments demonstrate significant improvements in transcription accuracy, highlighting the effectiveness of the proposed method in refining transcripts. These enhanced pseudo-labeled datasets provide high-quality foundational resources for training robust ASR systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate</title>
<link>https://arxiv.org/abs/2509.04492</link>
<guid>https://arxiv.org/abs/2509.04492</guid>
<content:encoded><![CDATA[
<div> Keywords: Hallucination detection, Large Language Models, Question Answering, Entropy Production Rate, Supervised learning 

Summary: 
- This paper introduces a methodology for detecting hallucinations in outputs of Large Language Models (LLMs) for Question Answering tasks.
- The approach focuses on scenarios with limited data access, like black-box LLM APIs, by deriving uncertainty indicators from available log-probabilities during decoding.
- An Entropy Production Rate (EPR) metric is developed initially, and later enhanced with supervised learning to improve detection performance.
- The learned model uses features representing the entropic contributions of top-ranked tokens in a single generated sequence, without requiring multiple query re-runs.
- Evaluation across various QA datasets and LLMs shows significant improvement in hallucination detection over using EPR alone, using only a small set of available log-probabilities.
  
<br><br>Summary: <div>
arXiv:2509.04492v1 Announce Type: cross 
Abstract: Hallucinations in Large Language Model (LLM) outputs for Question Answering (QA) tasks critically undermine their real-world reliability. This paper introduces an applied methodology for robust, one-shot hallucination detection, specifically designed for scenarios with limited data access, such as interacting with black-box LLM APIs that typically expose only a few top candidate log-probabilities per token. Our approach derives uncertainty indicators directly from these readily available log-probabilities generated during non-greedy decoding. We first derive an Entropy Production Rate (EPR) metric that offers baseline performance, later augmented with supervised learning. Our learned model uses features representing the entropic contributions of the accessible top-ranked tokens within a single generated sequence, requiring no multiple query re-runs. Evaluated across diverse QA datasets and multiple LLMs, this estimator significantly improves hallucination detection over using EPR alone. Crucially, high performance is demonstrated using only the typically small set of available log-probabilities (e.g., top <10 per token), confirming its practical efficiency and suitability for these API-constrained deployments. This work provides a readily deployable technique to enhance the trustworthiness of LLM responses from a single generation pass in QA and Retrieval-Augmented Generation (RAG) systems, with its utility further demonstrated in a finance framework analyzing responses to queries on annual reports from an industrial dataset.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Narrative-Driven Computational Framework for Clinician Burnout Surveillance</title>
<link>https://arxiv.org/abs/2509.04497</link>
<guid>https://arxiv.org/abs/2509.04497</guid>
<content:encoded><![CDATA[
<div> Keywords: clinician burnout, intensive care units, clinical notes, sentiment analysis, workload proxies

Summary: 
Analyzing 10,000 ICU discharge summaries from the MIMIC-IV database, this study uses a hybrid pipeline to extract valuable information from clinical notes for proactive clinician burnout monitoring. By combining BioBERT sentiment embeddings, a stress lexicon for burnout surveillance, and LDA with workload proxies, the model achieves a precision of 0.80 and a recall of 0.89 in predicting burnout risk. Specialty-specific analysis reveals elevated burnout risk in Radiology, Psychiatry, and Neurology. The study highlights the potential of leveraging clinical narratives to identify burnout signals and improve well-being monitoring in high-acuity healthcare settings.<br><br>Summary: <div>
arXiv:2509.04497v1 Announce Type: cross 
Abstract: Clinician burnout poses a substantial threat to patient safety, particularly in high-acuity intensive care units (ICUs). Existing research predominantly relies on retrospective survey tools or broad electronic health record (EHR) metadata, often overlooking the valuable narrative information embedded in clinical notes. In this study, we analyze 10,000 ICU discharge summaries from MIMIC-IV, a publicly available database derived from the electronic health records of Beth Israel Deaconess Medical Center. The dataset encompasses diverse patient data, including vital signs, medical orders, diagnoses, procedures, treatments, and deidentified free-text clinical notes. We introduce a hybrid pipeline that combines BioBERT sentiment embeddings fine-tuned for clinical narratives, a lexical stress lexicon tailored for clinician burnout surveillance, and five-topic latent Dirichlet allocation (LDA) with workload proxies. A provider-level logistic regression classifier achieves a precision of 0.80, a recall of 0.89, and an F1 score of 0.84 on a stratified hold-out set, surpassing metadata-only baselines by greater than or equal to 0.17 F1 score. Specialty-specific analysis indicates elevated burnout risk among providers in Radiology, Psychiatry, and Neurology. Our findings demonstrate that ICU clinical narratives contain actionable signals for proactive well-being monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations</title>
<link>https://arxiv.org/abs/2509.04498</link>
<guid>https://arxiv.org/abs/2509.04498</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, biases, university recommendations, diversity, equitable access <br>
<br>
Summary: 
The study investigates biases in university recommendations generated by three Large Language Models (LLMs). It focuses on geographic, demographic, and economic disparities in the suggestions provided by these models. The analysis, conducted using simulated user profiles, reveals a tendency towards favoring institutions in the Global North and perpetuating gender stereotypes. Despite LLaMA-3.1 demonstrating higher diversity in recommendations, systemic disparities persist across the models. The research introduces a novel evaluation framework that looks beyond accuracy to assess demographic and geographic representation. The findings underscore the pressing need to address biases in educational LLMs to ensure fair and inclusive global access to higher education. <br> <div>
arXiv:2509.04498v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used as daily recommendation systems for tasks like education planning, yet their recommendations risk perpetuating societal biases. This paper empirically examines geographic, demographic, and economic biases in university and program suggestions from three open-source LLMs: LLaMA-3.1-8B, Gemma-7B, and Mistral-7B. Using 360 simulated user profiles varying by gender, nationality, and economic status, we analyze over 25,000 recommendations. Results show strong biases: institutions in the Global North are disproportionately favored, recommendations often reinforce gender stereotypes, and institutional repetition is prevalent. While LLaMA-3.1 achieves the highest diversity, recommending 481 unique universities across 58 countries, systemic disparities persist. To quantify these issues, we propose a novel, multi-dimensional evaluation framework that goes beyond accuracy by measuring demographic and geographic representation. Our findings highlight the urgent need for bias consideration in educational LMs to ensure equitable global access to higher education.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability Across Citations and Evidence</title>
<link>https://arxiv.org/abs/2509.04499</link>
<guid>https://arxiv.org/abs/2509.04499</guid>
<content:encoded><![CDATA[
<div> audit framework, generative search engines, deep research LLM agents, citation practices, evidence attribution 

Summary:
DeepTRACE introduces a novel audit framework, focusing on eight dimensions related to answer text, sources, and citations to address overconfidence and weak sourcing in generative search engines and deep research LLM agents. The framework utilizes statement-level analysis, citation matrices, and factual-support matrices to evaluate how systems reason and attribute evidence. The study evaluates popular models and LLM configurations, revealing that while deep-research configurations reduce overconfidence and improve citation thoroughness, they still tend to provide one-sided responses on debate queries and include unsupported statements. Citation accuracy ranges from 40-80% across systems, indicating room for improvement in sourcing and evidence attribution practices. <br><br>Summary: <div>
arXiv:2509.04499v1 Announce Type: cross 
Abstract: Generative search engines and deep research LLM agents promise trustworthy, source-grounded synthesis, yet users regularly encounter overconfidence, weak sourcing, and confusing citation practices. We introduce DeepTRACE, a novel sociotechnically grounded audit framework that turns prior community-identified failure cases into eight measurable dimensions spanning answer text, sources, and citations. DeepTRACE uses statement-level analysis (decomposition, confidence scoring) and builds citation and factual-support matrices to audit how systems reason with and attribute evidence end-to-end. Using automated extraction pipelines for popular public models (e.g., GPT-4.5/5, You.com, Perplexity, Copilot/Bing, Gemini) and an LLM-judge with validated agreement to human raters, we evaluate both web-search engines and deep-research configurations. Our findings show that generative search engines and deep research agents frequently produce one-sided, highly confident responses on debate queries and include large fractions of statements unsupported by their own listed sources. Deep-research configurations reduce overconfidence and can attain high citation thoroughness, but they remain highly one-sided on debate queries and still exhibit large fractions of unsupported statements, with citation accuracy ranging from 40--80% across systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts</title>
<link>https://arxiv.org/abs/2509.04500</link>
<guid>https://arxiv.org/abs/2509.04500</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Poisoned Context Testbed, Rescorla-Wagner model, inappropriate content, context engineering <br>
Summary: 
Large Language Models (LLMs) benefit from external context but can struggle when faced with mixed information. In the Poisoned Context Testbed, queries are paired with real-world contexts containing relevant and inappropriate content to study how LLMs prioritize signals. The adapted Rescorla-Wagner model shows that LLMs tend to incorporate less prevalent information, leading to a susceptibility to inappropriate content. This vulnerability can significantly impact response quality in real-world settings. To address this, RW-Steering is introduced as a two-stage finetuning approach that allows models to identify and ignore inappropriate signals internally. Unlike previous methods, RW-Steering generalizes well across different mixtures of inappropriate content, improving response quality by 39.8% and mitigating the undesirable behavior curve. RW-Steering demonstrates robust and generalizable context engineering for enhancing LLM safety in practical applications. <br> <div>
arXiv:2509.04500v1 Announce Type: cross 
Abstract: Incorporating external context can significantly enhance the response quality of Large Language Models (LLMs). However, real-world contexts often mix relevant information with disproportionate inappropriate content, posing reliability risks. How do LLMs process and prioritize mixed context? To study this, we introduce the Poisoned Context Testbed, pairing queries with real-world contexts containing relevant and inappropriate content. Inspired by associative learning in animals, we adapt the Rescorla-Wagner (RW) model from neuroscience to quantify how competing contextual signals influence LLM outputs. Our adapted model reveals a consistent behavioral pattern: LLMs exhibit a strong tendency to incorporate information that is less prevalent in the context. This susceptibility is harmful in real-world settings, where small amounts of inappropriate content can substantially degrade response quality. Empirical evaluations on our testbed further confirm this vulnerability. To tackle this, we introduce RW-Steering, a two-stage finetuning-based approach that enables the model to internally identify and ignore inappropriate signals. Unlike prior methods that rely on extensive supervision across diverse context mixtures, RW-Steering generalizes robustly across varying proportions of inappropriate content. Experiments show that our best fine-tuned model improves response quality by 39.8% and reverses the undesirable behavior curve, establishing RW-Steering as a robust, generalizable context engineering solution for improving LLM safety in real-world use.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Reinforcement Learning for Model Training, and future directions with GRAPE</title>
<link>https://arxiv.org/abs/2509.04501</link>
<guid>https://arxiv.org/abs/2509.04501</guid>
<content:encoded><![CDATA[

arXiv:2509.04501v1 Announce Type: cross 
Abstract: This paper provides a self-contained, from-scratch, exposition of key algorithms for instruction tuning of models: SFT, Rejection Sampling, REINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), and Direct Preference Optimization (DPO). Explanations of these algorithms often assume prior knowledge, lack critical details, and/or are overly generalized and complex. Here, each method is discussed and developed step by step using simplified and explicit notation focused on LLMs, aiming to eliminate ambiguity and provide a clear and intuitive understanding of the concepts. By minimizing detours into the broader RL literature and connecting concepts to LLMs, we eliminate superfluous abstractions and reduce cognitive overhead. Following this exposition, we provide a literature review of new techniques and approaches beyond those detailed. Finally, new ideas for research and exploration in the form of GRAPE (Generalized Relative Advantage Policy Evolution) are presented.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples</title>
<link>https://arxiv.org/abs/2509.04502</link>
<guid>https://arxiv.org/abs/2509.04502</guid>
<content:encoded><![CDATA[

arXiv:2509.04502v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation enhances the response accuracy of Large Language Models (LLMs) by integrating retrieval and generation modules with external knowledge, demonstrating particular strength in real-time queries and Visual Question Answering tasks. However, the effectiveness of RAG is frequently hindered by the precision of the retriever: many retrieved samples fed into the generation phase are irrelevant or misleading, posing a critical bottleneck to LLMs' performance. To address this challenge, we introduce VaccineRAG, a novel Chain-of-Thought-based retrieval-augmented generation dataset. On one hand, VaccineRAG employs a benchmark to evaluate models using data with varying positive/negative sample ratios, systematically exposing inherent weaknesses in current LLMs. On the other hand, it enhances models' sample-discrimination capabilities by prompting LLMs to generate explicit Chain-of-Thought (CoT) analysis for each sample before producing final answers. Furthermore, to enhance the model's ability to learn long-sequence complex CoT content, we propose Partial-GRPO. By modeling the outputs of LLMs as multiple components rather than a single whole, our model can make more informed preference selections for complex sequences, thereby enhancing its capacity to learn complex CoT. Comprehensive evaluations and ablation studies on VaccineRAG validate the effectiveness of the proposed scheme. The code and dataset will be publicly released soon.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioral Fingerprinting of Large Language Models</title>
<link>https://arxiv.org/abs/2509.04504</link>
<guid>https://arxiv.org/abs/2509.04504</guid>
<content:encoded><![CDATA[

arXiv:2509.04504v1 Announce Type: cross 
Abstract: Current benchmarks for Large Language Models (LLMs) primarily focus on performance metrics, often failing to capture the nuanced behavioral characteristics that differentiate them. This paper introduces a novel ``Behavioral Fingerprinting'' framework designed to move beyond traditional evaluation by creating a multi-faceted profile of a model's intrinsic cognitive and interactive styles. Using a curated \textit{Diagnostic Prompt Suite} and an innovative, automated evaluation pipeline where a powerful LLM acts as an impartial judge, we analyze eighteen models across capability tiers. Our results reveal a critical divergence in the LLM landscape: while core capabilities like abstract and causal reasoning are converging among top models, alignment-related behaviors such as sycophancy and semantic robustness vary dramatically. We further document a cross-model default persona clustering (ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together, this suggests that a model's interactive nature is not an emergent property of its scale or reasoning power, but a direct consequence of specific, and highly variable, developer alignment strategies. Our framework provides a reproducible and scalable methodology for uncovering these deep behavioral differences. Project: https://github.com/JarvisPei/Behavioral-Fingerprinting
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memristor-Based Neural Network Accelerators for Space Applications: Enhancing Performance with Temporal Averaging and SIRENs</title>
<link>https://arxiv.org/abs/2509.04506</link>
<guid>https://arxiv.org/abs/2509.04506</guid>
<content:encoded><![CDATA[

arXiv:2509.04506v1 Announce Type: cross 
Abstract: Memristors are an emerging technology that enables artificial intelligence (AI) accelerators with high energy efficiency and radiation robustness -- properties that are vital for the deployment of AI on-board spacecraft. However, space applications require reliable and precise computations, while memristive devices suffer from non-idealities, such as device variability, conductance drifts, and device faults. Thus, porting neural networks (NNs) to memristive devices often faces the challenge of severe performance degradation. In this work, we show in simulations that memristor-based NNs achieve competitive performance levels on on-board tasks, such as navigation \& control and geodesy of asteroids. Through bit-slicing, temporal averaging of NN layers, and periodic activation functions, we improve initial results from around $0.07$ to $0.01$ and $0.3$ to $0.007$ for both tasks using RRAM devices, coming close to state-of-the-art levels ($0.003-0.005$ and $0.003$, respectively). Our results demonstrate the potential of memristors for on-board space applications, and we are convinced that future technology and NN improvements will further close the performance gap to fully unlock the benefits of memristors.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM Approach</title>
<link>https://arxiv.org/abs/2509.04507</link>
<guid>https://arxiv.org/abs/2509.04507</guid>
<content:encoded><![CDATA[

arXiv:2509.04507v1 Announce Type: cross 
Abstract: Silent Speech Interfaces (SSIs) have gained attention for their ability to generate intelligible speech from non-acoustic signals. While significant progress has been made in advancing speech generation pipelines, limited work has addressed the recognition and downstream processing of synthesized speech, which often suffers from phonetic ambiguity and noise. To overcome these challenges, we propose an enhanced automatic speech recognition framework that combines a transformer-based acoustic model with a large language model (LLM) for post-processing. The transformer captures full utterance context, while the LLM ensures linguistic consistency. Experimental results show a 16% relative and 6% absolute reduction in word error rate (WER) over a 36% baseline, demonstrating substantial improvements in intelligibility for silent speech interfaces.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through Model Explanations</title>
<link>https://arxiv.org/abs/2509.04515</link>
<guid>https://arxiv.org/abs/2509.04515</guid>
<content:encoded><![CDATA[

arXiv:2509.04515v1 Announce Type: cross 
Abstract: Language models have been shown to propagate social bias through their output, particularly in the representation of gender and ethnicity. This paper investigates gender and ethnicity biases in AI-generated occupational stories. Representation biases are measured before and after applying our proposed mitigation strategy, Bias Analysis and Mitigation through Explanation (BAME), revealing improvements in demographic representation ranging from 2% to 20%. BAME leverages model-generated explanations to inform targeted prompt engineering, effectively reducing biases without modifying model parameters. By analyzing stories generated across 25 occupational groups, three large language models (Claude 3.5 Sonnet, Llama 3.1 70B Instruct, and GPT-4 Turbo), and multiple demographic dimensions, we identify persistent patterns of overrepresentation and underrepresentation linked to training data stereotypes. Our findings demonstrate that guiding models with their own internal reasoning mechanisms can significantly enhance demographic parity, thereby contributing to the development of more transparent generative AI systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation</title>
<link>https://arxiv.org/abs/2509.04534</link>
<guid>https://arxiv.org/abs/2509.04534</guid>
<content:encoded><![CDATA[

arXiv:2509.04534v1 Announce Type: cross 
Abstract: Large language models have demonstrated remarkable capabilities in biomedical natural language processing, yet their rapid growth in size and computational requirements present a major barrier to adoption in healthcare settings where data privacy precludes cloud deployment and resources are limited. In this study, we systematically evaluated the impact of quantization on 12 state-of-the-art large language models, including both general-purpose and biomedical-specific models, across eight benchmark datasets covering four key tasks: named entity recognition, relation extraction, multi-label classification, and question answering. We show that quantization substantially reduces GPU memory requirements-by up to 75%-while preserving model performance across diverse tasks, enabling the deployment of 70B-parameter models on 40GB consumer-grade GPUs. In addition, domain-specific knowledge and responsiveness to advanced prompting methods are largely maintained. These findings provide significant practical and guiding value, highlighting quantization as a practical and effective strategy for enabling the secure, local deployment of large yet high-capacity language models in biomedical contexts, bridging the gap between technical advances in AI and real-world clinical translation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Policy Adaptation via Cross-Domain Skill Diffusion</title>
<link>https://arxiv.org/abs/2509.04535</link>
<guid>https://arxiv.org/abs/2509.04535</guid>
<content:encoded><![CDATA[

arXiv:2509.04535v1 Announce Type: cross 
Abstract: In this work, we present an in-context policy adaptation (ICPAD) framework designed for long-horizon multi-task environments, exploring diffusion-based skill learning techniques in cross-domain settings. The framework enables rapid adaptation of skill-based reinforcement learning policies to diverse target domains, especially under stringent constraints on no model updates and only limited target domain data. Specifically, the framework employs a cross-domain skill diffusion scheme, where domain-agnostic prototype skills and a domain-grounded skill adapter are learned jointly and effectively from an offline dataset through cross-domain consistent diffusion processes. The prototype skills act as primitives for common behavior representations of long-horizon policies, serving as a lingua franca to bridge different domains. Furthermore, to enhance the in-context adaptation performance, we develop a dynamic domain prompting scheme that guides the diffusion-based skill adapter toward better alignment with the target domain. Through experiments with robotic manipulation in Metaworld and autonomous driving in CARLA, we show that our $\oursol$ framework achieves superior policy adaptation performance under limited target domain data conditions for various cross-domain configurations including differences in environment dynamics, agent embodiment, and task horizon.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem</title>
<link>https://arxiv.org/abs/2509.04537</link>
<guid>https://arxiv.org/abs/2509.04537</guid>
<content:encoded><![CDATA[

arXiv:2509.04537v1 Announce Type: cross 
Abstract: We investigate the emergent social dynamics of Large Language Model (LLM) agents in a spatially extended El Farol Bar problem, observing how they autonomously navigate this classic social dilemma. As a result, the LLM agents generated a spontaneous motivation to go to the bar and changed their decision making by becoming a collective. We also observed that the LLM agents did not solve the problem completely, but rather behaved more like humans. These findings reveal a complex interplay between external incentives (prompt-specified constraints such as the 60\% threshold) and internal incentives (culturally-encoded social preferences derived from pre-training), demonstrating that LLM agents naturally balance formal game-theoretic rationality with social motivations that characterize human behavior. These findings suggest that a new model of group decision making, which could not be handled in the previous game-theoretic problem setting, can be realized by LLM agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>i-Mask: An Intelligent Mask for Breath-Driven Activity Recognition</title>
<link>https://arxiv.org/abs/2509.04544</link>
<guid>https://arxiv.org/abs/2509.04544</guid>
<content:encoded><![CDATA[

arXiv:2509.04544v1 Announce Type: cross 
Abstract: The patterns of inhalation and exhalation contain important physiological signals that can be used to anticipate human behavior, health trends, and vital parameters. Human activity recognition (HAR) is fundamentally connected to these vital signs, providing deeper insights into well-being and enabling real-time health monitoring. This work presents i-Mask, a novel HAR approach that leverages exhaled breath patterns captured using a custom-developed mask equipped with integrated sensors. Data collected from volunteers wearing the mask undergoes noise filtering, time-series decomposition, and labeling to train predictive models. Our experimental results validate the effectiveness of the approach, achieving over 95\% accuracy and highlighting its potential in healthcare and fitness applications.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions</title>
<link>https://arxiv.org/abs/2509.04549</link>
<guid>https://arxiv.org/abs/2509.04549</guid>
<content:encoded><![CDATA[

arXiv:2509.04549v1 Announce Type: cross 
Abstract: Transformer-based language models excel in NLP tasks, but fine-grained control remains challenging. This paper explores methods for manipulating transformer models through principled interventions at three levels: prompts, activations, and weights. We formalize controllable text generation as an optimization problem addressable via prompt engineering, parameter-efficient fine-tuning, model editing, and reinforcement learning. We introduce a unified framework encompassing prompt-level steering, activation interventions, and weight-space edits. We analyze robustness and safety implications, including adversarial attacks and alignment mitigations. Theoretically, we show minimal weight updates can achieve targeted behavior changes with limited side-effects. Empirically, we demonstrate >90% success in sentiment control and factual edits while preserving base performance, though generalization-specificity trade-offs exist. We discuss ethical dual-use risks and the need for rigorous evaluation. This work lays groundwork for designing controllable and robust language models.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Faithfulness-guided Ensemble Interpretation of Neural Network</title>
<link>https://arxiv.org/abs/2509.04588</link>
<guid>https://arxiv.org/abs/2509.04588</guid>
<content:encoded><![CDATA[

arXiv:2509.04588v1 Announce Type: cross 
Abstract: Interpretable and faithful explanations for specific neural inferences are crucial for understanding and evaluating model behavior. Our work introduces \textbf{F}aithfulness-guided \textbf{E}nsemble \textbf{I}nterpretation (\textbf{FEI}), an innovative framework that enhances the breadth and effectiveness of faithfulness, advancing interpretability by providing superior visualization. Through an analysis of existing evaluation benchmarks, \textbf{FEI} employs a smooth approximation to elevate quantitative faithfulness scores. Diverse variations of \textbf{FEI} target enhanced faithfulness in hidden layer encodings, expanding interpretability. Additionally, we propose a novel qualitative metric that assesses hidden layer faithfulness. In extensive experiments, \textbf{FEI} surpasses existing methods, demonstrating substantial advances in qualitative visualization and quantitative faithfulness scores. Our research establishes a comprehensive framework for elevating faithfulness in neural network explanations, emphasizing both breadth and precision
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Multi-Task Learning with Learnable Weighting for Pharmacokinetic and Toxicity Prediction</title>
<link>https://arxiv.org/abs/2509.04601</link>
<guid>https://arxiv.org/abs/2509.04601</guid>
<content:encoded><![CDATA[

arXiv:2509.04601v1 Announce Type: cross 
Abstract: Prediction for ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) plays a crucial role in drug discovery and development, accelerating the screening and optimization of new drugs. Existing methods primarily rely on single-task learning (STL), which often fails to fully exploit the complementarities between tasks. Besides, it requires more computational resources while training and inference of each task independently. To address these issues, we propose a new unified Quantum-enhanced and task-Weighted Multi-Task Learning (QW-MTL) framework, specifically designed for ADMET classification tasks. Built upon the Chemprop-RDKit backbone, QW-MTL adopts quantum chemical descriptors to enrich molecular representations with additional information about the electronic structure and interactions. Meanwhile, it introduces a novel exponential task weighting scheme that combines dataset-scale priors with learnable parameters to achieve dynamic loss balancing across tasks. To the best of our knowledge, this is the first work to systematically conduct joint multi-task training across all 13 Therapeutics Data Commons (TDC) classification benchmarks, using leaderboard-style data splits to ensure a standardized and realistic evaluation setting. Extensive experimental results show that QW-MTL significantly outperforms single-task baselines on 12 out of 13 tasks, achieving high predictive performance with minimal model complexity and fast inference, demonstrating the effectiveness and efficiency of multi-task molecular learning enhanced by quantum-informed features and adaptive task weighting.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample-efficient Integration of New Modalities into Large Language Models</title>
<link>https://arxiv.org/abs/2509.04606</link>
<guid>https://arxiv.org/abs/2509.04606</guid>
<content:encoded><![CDATA[

arXiv:2509.04606v1 Announce Type: cross 
Abstract: Multimodal foundation models can process several modalities. However, since the space of possible modalities is large and evolving over time, training a model from scratch to encompass all modalities is unfeasible. Moreover, integrating a modality into a pre-existing foundation model currently requires a significant amount of paired data, which is often not available for low-resource modalities. In this paper, we introduce a method for sample-efficient modality integration (SEMI) into Large Language Models (LLMs). To this end, we devise a hypernetwork that can adapt a shared projector -- placed between modality-specific encoders and an LLM -- to any modality. The hypernetwork, trained on high-resource modalities (i.e., text, speech, audio, video), is conditioned on a few samples from any arbitrary modality at inference time to generate a suitable adapter. To increase the diversity of training modalities, we artificially multiply the number of encoders through isometric transformations. We find that SEMI achieves a significant boost in sample efficiency during few-shot integration of new modalities (i.e., satellite images, astronomical images, inertial measurements, and molecules) with encoders of arbitrary embedding dimensionality. For instance, to reach the same accuracy as 32-shot SEMI, training the projector from scratch needs 64$\times$ more data. As a result, SEMI holds promise to extend the modality coverage of foundation models.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families</title>
<link>https://arxiv.org/abs/2509.04622</link>
<guid>https://arxiv.org/abs/2509.04622</guid>
<content:encoded><![CDATA[

arXiv:2509.04622v1 Announce Type: cross 
Abstract: Representational similarity metrics are fundamental tools in neuroscience and AI, yet we lack systematic comparisons of their discriminative power across model families. We introduce a quantitative framework to evaluate representational similarity measures based on their ability to separate model families-across architectures (CNNs, Vision Transformers, Swin Transformers, ConvNeXt) and training regimes (supervised vs. self-supervised). Using three complementary separability measures-dprime from signal detection theory, silhouette coefficients and ROC-AUC, we systematically assess the discriminative capacity of commonly used metrics including RSA, linear predictivity, Procrustes, and soft matching. We show that separability systematically increases as metrics impose more stringent alignment constraints. Among mapping-based approaches, soft-matching achieves the highest separability, followed by Procrustes alignment and linear predictivity. Non-fitting methods such as RSA also yield strong separability across families. These results provide the first systematic comparison of similarity metrics through a separability lens, clarifying their relative sensitivity and guiding metric choice for large-scale model and brain comparisons.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control</title>
<link>https://arxiv.org/abs/2509.04628</link>
<guid>https://arxiv.org/abs/2509.04628</guid>
<content:encoded><![CDATA[

arXiv:2509.04628v1 Announce Type: cross 
Abstract: We present an imitation learning approach for spacecraft guidance, navigation, and control(GNC) that achieves high performance from limited data. Using only 100 expert demonstrations, equivalent to 6,300 environment interactions, our method, which implements Action Chunking with Transformers (ACT), learns a control policy that maps visual and state observations to thrust and torque commands. ACT generates smoother, more consistent trajectories than a meta-reinforcement learning (meta-RL) baseline trained with 40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking with the International Space Station (ISS). We show that our approach achieves greater accuracy, smoother control, and greater sample efficiency.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schema Inference for Tabular Data Repositories Using Large Language Models</title>
<link>https://arxiv.org/abs/2509.04632</link>
<guid>https://arxiv.org/abs/2509.04632</guid>
<content:encoded><![CDATA[

arXiv:2509.04632v1 Announce Type: cross 
Abstract: Minimally curated tabular data often contain representational inconsistencies across heterogeneous sources, and are accompanied by sparse metadata. Working with such data is intimidating. While prior work has advanced dataset discovery and exploration, schema inference remains difficult when metadata are limited. We present SI-LLM (Schema Inference using Large Language Models), which infers a concise conceptual schema for tabular data using only column headers and cell values. The inferred schema comprises hierarchical entity types, attributes, and inter-type relationships. In extensive evaluation on two datasets from web tables and open data, SI-LLM achieves promising end-to-end results, as well as better or comparable results to state-of-the-art methods at each step. All source code, full prompts, and datasets of SI-LLM are available at https://github.com/PierreWoL/SILLM.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Environments for Organoid Intelligence with LLM-Automated Design and Plasticity-Based Evaluation</title>
<link>https://arxiv.org/abs/2509.04633</link>
<guid>https://arxiv.org/abs/2509.04633</guid>
<content:encoded><![CDATA[

arXiv:2509.04633v1 Announce Type: cross 
Abstract: As the complexity of artificial agents increases, the design of environments that can effectively shape their behavior and capabilities has become a critical research frontier. We propose a framework that extends this principle to a novel class of agents: biological neural networks in the form of neural organoids. This paper introduces three scalable, closed-loop virtual environments designed to train organoid-based biological agents and probe the underlying mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments with increasing complexity: (1) a conditional avoidance task, (2) a one-dimensional predator-prey scenario, and (3) a replication of the classic Pong game. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation. Furthermore, we propose a novel meta-learning approach where a Large Language Model (LLM) is used to automate the generation and optimization of experimental protocols, scaling the process of environment and curriculum design. Finally, we outline a multi-modal approach for evaluating learning by measuring synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between computational neuroscience and agent-based AI, offering a unique platform for studying embodiment, learning, and intelligence in a controlled biological substrate.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Transformer Models in Disaster Tweet Classification for Public Safety</title>
<link>https://arxiv.org/abs/2509.04650</link>
<guid>https://arxiv.org/abs/2509.04650</guid>
<content:encoded><![CDATA[

arXiv:2509.04650v1 Announce Type: cross 
Abstract: Twitter and other social media platforms have become vital sources of real time information during disasters and public safety emergencies. Automatically classifying disaster related tweets can help emergency services respond faster and more effectively. Traditional Machine Learning (ML) models such as Logistic Regression, Naive Bayes, and Support Vector Machines have been widely used for this task, but they often fail to understand the context or deeper meaning of words, especially when the language is informal, metaphorical, or ambiguous. We posit that, in this context, transformer based models can perform better than traditional ML models. In this paper, we evaluate the effectiveness of transformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for classifying disaster related tweets. These models are compared with traditional ML approaches to highlight the performance gap. Experimental results show that BERT achieved the highest accuracy (91%), significantly outperforming traditional models like Logistic Regression and Naive Bayes (both at 82%). The use of contextual embeddings and attention mechanisms allows transformer models to better understand subtle language in tweets, where traditional ML models fall short. This research demonstrates that transformer architectures are far more suitable for public safety applications, offering improved accuracy, deeper language understanding, and better generalization across real world social media text.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Transformer Architectures as Implicit Multinomial Regression</title>
<link>https://arxiv.org/abs/2509.04653</link>
<guid>https://arxiv.org/abs/2509.04653</guid>
<content:encoded><![CDATA[

arXiv:2509.04653v1 Announce Type: cross 
Abstract: Mechanistic interpretability aims to understand how internal components of modern machine learning models, such as weights, activations, and layers, give rise to the model's overall behavior. One particularly opaque mechanism is attention: despite its central role in transformer models, its mathematical underpinnings and relationship to concepts like feature polysemanticity, superposition, and model performance remain poorly understood. This paper establishes a novel connection between attention mechanisms and multinomial regression. Specifically, we show that in a fixed multinomial regression setting, optimizing over latent features yields optimal solutions that align with the dynamics induced by attention blocks. In other words, the evolution of representations through a transformer can be interpreted as a trajectory that recovers the optimal features for classification.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs</title>
<link>https://arxiv.org/abs/2509.04655</link>
<guid>https://arxiv.org/abs/2509.04655</guid>
<content:encoded><![CDATA[

arXiv:2509.04655v1 Announce Type: cross 
Abstract: We propose a novel inference-time out-of-domain (OOD) detection algorithm for specialized large language models (LLMs). Despite achieving state-of-the-art performance on in-domain tasks through fine-tuning, specialized LLMs remain vulnerable to incorrect or unreliable outputs when presented with OOD inputs, posing risks in critical applications. Our method leverages the Inductive Conformal Anomaly Detection (ICAD) framework, using a new non-conformity measure based on the model's dropout tolerance. Motivated by recent findings on polysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs exhibit higher dropout tolerance than OOD inputs. We aggregate dropout tolerance across multiple layers via a valid ensemble approach, improving detection while maintaining theoretical false alarm bounds from ICAD. Experiments with medical-specialized LLMs show that our approach detects OOD inputs better than baseline methods, with AUROC improvements of $2\%$ to $37\%$ when treating OOD datapoints as positives and in-domain test datapoints as negatives.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating NL2SQL via SQL2NL</title>
<link>https://arxiv.org/abs/2509.04657</link>
<guid>https://arxiv.org/abs/2509.04657</guid>
<content:encoded><![CDATA[

arXiv:2509.04657v1 Announce Type: cross 
Abstract: Robust evaluation in the presence of linguistic variation is key to understanding the generalization capabilities of Natural Language to SQL (NL2SQL) models, yet existing benchmarks rarely address this factor in a systematic or controlled manner. We propose a novel schema-aligned paraphrasing framework that leverages SQL-to-NL (SQL2NL) to automatically generate semantically equivalent, lexically diverse queries while maintaining alignment with the original schema and intent. This enables the first targeted evaluation of NL2SQL robustness to linguistic variation in isolation-distinct from prior work that primarily investigates ambiguity or schema perturbations. Our analysis reveals that state-of-the-art models are far more brittle than standard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop in execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries, while LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to 42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We also find that robustness degradation varies significantly with query complexity, dataset, and domain -- highlighting the need for evaluation frameworks that explicitly measure linguistic generalization to ensure reliable performance in real-world settings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation</title>
<link>https://arxiv.org/abs/2509.04669</link>
<guid>https://arxiv.org/abs/2509.04669</guid>
<content:encoded><![CDATA[

arXiv:2509.04669v1 Announce Type: cross 
Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs) have challenged the dominance of Convolutional Neural Networks (CNNs) in computer vision. ViTs excel at capturing global context, and SSMs like Mamba offer linear complexity for long sequences, yet they do not capture fine-grained local features as effectively as CNNs. Conversely, CNNs possess strong inductive biases for local features but lack the global reasoning capabilities of transformers and Mamba. To bridge this gap, we introduce \textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a hierarchical structure with convolutional blocks in its early stages to extract rich local features. These convolutional blocks are then processed by later stages incorporating multi-directional Mamba blocks designed to efficiently model long-range dependencies and global context. This hybrid design allows for superior feature representation while maintaining linear complexity with respect to image resolution. We demonstrate VCMamba's effectiveness through extensive experiments on ImageNet-1K classification and ADE20K semantic segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K, surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains 47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing 62% fewer parameters. Code is available at https://github.com/Wertyuui345/VCMamba.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine Bioacoustic Monitoring</title>
<link>https://arxiv.org/abs/2509.04682</link>
<guid>https://arxiv.org/abs/2509.04682</guid>
<content:encoded><![CDATA[

arXiv:2509.04682v1 Announce Type: cross 
Abstract: Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal data for long-term ecological analysis, but intrinsic noise and complex signal dependencies hinder model stability and generalization. Multilayered windowing has improved target sound localization, yet variability from shifting ambient noise, diverse propagation effects, and mixed biological and anthropogenic sources demands robust architectures and rigorous evaluation. We introduce GetNetUPAM, a hierarchical nested cross-validation framework designed to quantify model stability under ecologically realistic variability. Data are partitioned into distinct site-year segments, preserving recording heterogeneity and ensuring each validation fold reflects a unique environmental subset, reducing overfitting to localized noise and sensor artifacts. Site-year blocking enforces evaluation against genuine environmental diversity, while standard cross-validation on random subsets measures generalization across UPAM's full signal distribution, a dimension absent from current benchmarks. Using GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution Pooling and Attention Network (ARPA-N), a neural architecture for irregular spectrogram dimensions. Adaptive pooling with spatial attention extends the receptive field, capturing global context without excessive parameters. Under GetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet baselines and a log2-scale order-of-magnitude drop in variability across all metrics, enabling consistent detection across site-year folds and advancing scalable, accurate bioacoustic monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs</title>
<link>https://arxiv.org/abs/2509.04696</link>
<guid>https://arxiv.org/abs/2509.04696</guid>
<content:encoded><![CDATA[

arXiv:2509.04696v1 Announce Type: cross 
Abstract: Knowledge graphs (KGs) are foundational to many AI applications, but maintaining their freshness and completeness remains costly. We present ODKE+, a production-grade system that automatically extracts and ingests millions of open-domain facts from web sources with high precision. ODKE+ combines modular components into a scalable pipeline: (1) the Extraction Initiator detects missing or stale facts, (2) the Evidence Retriever collects supporting documents, (3) hybrid Knowledge Extractors apply both pattern-based rules and ontology-guided prompting for large language models (LLMs), (4) a lightweight Grounder validates extracted facts using a second LLM, and (5) the Corroborator ranks and normalizes candidate facts for ingestion. ODKE+ dynamically generates ontology snippets tailored to each entity type to align extractions with schema constraints, enabling scalable, type-consistent fact extraction across 195 predicates. The system supports batch and streaming modes, processing over 9 million Wikipedia pages and ingesting 19 million high-confidence facts with 98.8% precision. ODKE+ significantly improves coverage over traditional methods, achieving up to 48% overlap with third-party KGs and reducing update lag by 50 days on average. Our deployment demonstrates that LLM-based extraction, grounded in ontological structure and verification workflows, can deliver trustworthiness, production-scale knowledge ingestion with broad real-world applicability. A recording of the system demonstration is included with the submission and is also available at https://youtu.be/UcnE3_GsTWs.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.04712</link>
<guid>https://arxiv.org/abs/2509.04712</guid>
<content:encoded><![CDATA[

arXiv:2509.04712v1 Announce Type: cross 
Abstract: Automated vehicle control using reinforcement learning (RL) has attracted significant attention due to its potential to learn driving policies through environment interaction. However, RL agents often face training challenges in sample efficiency and effective exploration, making it difficult to discover an optimal driving strategy. To address these issues, we propose guiding the RL driving agent with a demonstration policy that need not be a highly optimized or expert-level controller. Specifically, we integrate a rule-based lane change controller with the Soft Actor Critic (SAC) algorithm to enhance exploration and learning efficiency. Our approach demonstrates improved driving performance and can be extended to other driving scenarios that can similarly benefit from demonstration-based guidance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced Question Answering</title>
<link>https://arxiv.org/abs/2509.04716</link>
<guid>https://arxiv.org/abs/2509.04716</guid>
<content:encoded><![CDATA[

arXiv:2509.04716v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucination in Large Language Models (LLMs) by incorporating external data, with Knowledge Graphs (KGs) offering crucial information for question answering. Traditional Knowledge Graph Question Answering (KGQA) methods rely on semantic parsing, which typically retrieves knowledge strictly necessary for answer generation, thus often suffer from low coverage due to rigid schema requirements and semantic ambiguity. We present KERAG, a novel KG-based RAG pipeline that enhances QA coverage by retrieving a broader subgraph likely to contain relevant information. Our retrieval-filtering-summarization approach, combined with fine-tuned LLMs for Chain-of-Thought reasoning on knowledge sub-graphs, reduces noises and improves QA for both simple and complex questions. Experiments demonstrate that KERAG surpasses state-of-the-art solutions by about 7% in quality and exceeds GPT-4o (Tool) by 10-21%.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoVeR: Conformal Calibration for Versatile and Reliable Autoregressive Next-Token Prediction</title>
<link>https://arxiv.org/abs/2509.04733</link>
<guid>https://arxiv.org/abs/2509.04733</guid>
<content:encoded><![CDATA[

arXiv:2509.04733v1 Announce Type: cross 
Abstract: Autoregressive pre-trained models combined with decoding methods have achieved impressive performance on complex reasoning tasks. While mainstream decoding strategies such as beam search can generate plausible candidate sets, they often lack provable coverage guarantees, and struggle to effectively balance search efficiency with the need for versatile trajectories, particularly those involving long-tail sequences that are essential in certain real-world applications. To address these limitations, we propose \textsc{CoVeR}, a novel model-free decoding strategy wihtin the conformal prediction framework that simultaneously maintains a compact search space and ensures high coverage probability over desirable trajectories. Theoretically, we establish a PAC-style generalization bound, guaranteeing that \textsc{CoVeR} asymptotically achieves a coverage rate of at least $1 - \alpha$ for any target level $\alpha \in (0,1)$.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning</title>
<link>https://arxiv.org/abs/2509.04734</link>
<guid>https://arxiv.org/abs/2509.04734</guid>
<content:encoded><![CDATA[

arXiv:2509.04734v1 Announce Type: cross 
Abstract: The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We present Beyond I-Con, a framework that enables systematic discovery of novel loss functions by exploring alternative statistical divergences and similarity kernels. Key findings: (1) on unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art results by modifying the PMI algorithm to use total variation (TV) distance; (2) on supervised contrastive learning, we outperform the standard approach by using TV and a distance-based similarity kernel instead of KL and an angular kernel; (3) on dimensionality reduction, we achieve superior qualitative results and better performance on downstream tasks than SNE by replacing KL with a bounded f-divergence. Our results highlight the importance of considering divergence and similarity kernel choices in representation learning optimization.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization</title>
<link>https://arxiv.org/abs/2509.04735</link>
<guid>https://arxiv.org/abs/2509.04735</guid>
<content:encoded><![CDATA[

arXiv:2509.04735v1 Announce Type: cross 
Abstract: Recent advances in vision foundation models, such as the Segment Anything Model (SAM) and its successor SAM2, have achieved state-of-the-art performance on general image segmentation benchmarks. However, these models struggle in adverse weather conditions where visual ambiguity is high, largely due to their lack of uncertainty quantification. Inspired by progress in medical imaging, where uncertainty-aware training has improved reliability in ambiguous cases, we investigate two approaches to enhance segmentation robustness for autonomous driving. First, we introduce a multi-step finetuning procedure for SAM2 that incorporates uncertainty metrics directly into the loss function, improving overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter (UAT), originally designed for medical image segmentation, to driving contexts. We evaluate both methods on CamVid, BDD100K, and GTA driving datasets. Experiments show that UAT-SAM outperforms standard SAM in extreme weather, while SAM2 with uncertainty-aware loss achieves improved performance across diverse driving scenes. These findings underscore the value of explicit uncertainty modeling for safety-critical autonomous driving in challenging environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching</title>
<link>https://arxiv.org/abs/2509.04752</link>
<guid>https://arxiv.org/abs/2509.04752</guid>
<content:encoded><![CDATA[

arXiv:2509.04752v1 Announce Type: cross 
Abstract: This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM health coaching system that integrates personalized machine learning and retrieval-augmented generation to deliver adaptive, evidence-based guidance. SePA combines: (1) Individualized models predicting daily stress, soreness, and injury risk from wearable sensor data (28 users, 1260 data points); and (2) A retrieval module that grounds LLM-generated feedback in expert-vetted web content to ensure contextual relevance and reliability. Our predictive models, evaluated with rolling-origin cross-validation and group k-fold cross-validation show that personalized models outperform generalized baselines. In a pilot expert study (n=4), SePA's retrieval-based advice was preferred over a non-retrieval baseline, yielding meaningful practical effect (Cliff's $\delta$=0.3, p=0.05). We also quantify latency performance trade-offs between response quality and speed, offering a transparent blueprint for next-generation, trustworthy personal health informatics systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning</title>
<link>https://arxiv.org/abs/2509.04753</link>
<guid>https://arxiv.org/abs/2509.04753</guid>
<content:encoded><![CDATA[

arXiv:2509.04753v1 Announce Type: cross 
Abstract: Natural language processing (NLP) is a key technology to extract important patient information from clinical narratives to support healthcare applications. The rapid development of large language models (LLMs) has revolutionized many NLP tasks in the clinical domain, yet their optimal use in patient information extraction tasks requires further exploration. This study examines LLMs' effectiveness in patient information extraction, focusing on LLM architectures, fine-tuning strategies, and multi-task instruction tuning techniques for developing robust and generalizable patient information extraction systems. This study aims to explore key concepts of using LLMs for clinical concept and relation extraction tasks, including: (1) encoder-only or decoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT) algorithms, and (3) multi-task instruction tuning on few-shot learning performance. We benchmarked a suite of LLMs, including encoder-based LLMs (BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1, GatorTronLlama), across five datasets. We compared traditional full-size fine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning framework that combines both tasks across four datasets to evaluate the zero-shot and few-shot learning performance using the leave-one-dataset-out strategy.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery</title>
<link>https://arxiv.org/abs/2509.04757</link>
<guid>https://arxiv.org/abs/2509.04757</guid>
<content:encoded><![CDATA[

arXiv:2509.04757v1 Announce Type: cross 
Abstract: Rapid and accurate post-hurricane damage assessment is vital for disaster response and recovery. Yet existing CNN-based methods struggle to capture multi-scale spatial features and to distinguish visually similar or co-occurring damage types. To address these issues, we propose MCANet, a multi-label classification framework that learns multi-scale representations and adaptively attends to spatially relevant regions for each damage category. MCANet employs a Res2Net-based hierarchical backbone to enrich spatial context across scales and a multi-head class-specific residual attention module to enhance discrimination. Each attention branch focuses on different spatial granularities, balancing local detail with global context. We evaluate MCANet on the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael. MCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet, Res2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads, performance further improves to 92.35%, boosting average precision for challenging classes such as Road Blocked by over 6%. Class activation mapping confirms MCANet's ability to localize damage-relevant regions, supporting interpretability. Outputs from MCANet can inform post-disaster risk mapping, emergency routing, and digital twin-based disaster response. Future work could integrate disaster-specific knowledge graphs and multimodal large language models to improve adaptability to unseen disasters and enrich semantic understanding for real-world decision-making.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph</title>
<link>https://arxiv.org/abs/2509.04772</link>
<guid>https://arxiv.org/abs/2509.04772</guid>
<content:encoded><![CDATA[

arXiv:2509.04772v1 Announce Type: cross 
Abstract: Timely and accurate floodwater depth estimation is critical for road accessibility and emergency response. While recent computer vision methods have enabled flood detection, they suffer from both accuracy limitations and poor generalization due to dependence on fixed object detectors and task-specific training. To enable accurate depth estimation that can generalize across diverse flood scenarios, this paper presents FloodVision, a zero-shot framework that combines the semantic reasoning abilities of the foundation vision-language model GPT-4o with a structured domain knowledge graph. The knowledge graph encodes canonical real-world dimensions for common urban objects including vehicles, people, and infrastructure elements to ground the model's reasoning in physical reality. FloodVision dynamically identifies visible reference objects in RGB images, retrieves verified heights from the knowledge graph to mitigate hallucination, estimates submergence ratios, and applies statistical outlier filtering to compute final depth values. Evaluated on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and surpassing prior CNN-based methods. The system generalizes well across varying scenes and operates in near real-time, making it suitable for future integration into digital twin platforms and citizen-reporting apps for smart city flood resilience.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoders Laugh as Loud as Encoders</title>
<link>https://arxiv.org/abs/2509.04779</link>
<guid>https://arxiv.org/abs/2509.04779</guid>
<content:encoded><![CDATA[

arXiv:2509.04779v1 Announce Type: cross 
Abstract: From the dawn of the computer, Allen Turing dreamed of a robot that could communicate using language as a human being. The recent advances in the field of Large Language Models (LLMs) shocked the scientific community when a single model can apply for various natural language processing (NLP) tasks, while the output results are sometimes even better than most human communication skills. Models such as GPT, Claude, Grok, etc. have left their mark on the scientific community. However, it is unclear how much these models understand what they produce, especially in a nuanced theme such as humor. The question of whether computers understand humor is still open (among the decoders, the latest to be checked was GPT-2). We addressed this issue in this paper; we have showed that a fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well as the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models</title>
<link>https://arxiv.org/abs/2509.04781</link>
<guid>https://arxiv.org/abs/2509.04781</guid>
<content:encoded><![CDATA[

arXiv:2509.04781v1 Announce Type: cross 
Abstract: When given the option, will LLMs choose to leave the conversation (bail)? We investigate this question by giving models the option to bail out of interactions using three different bail methods: a bail tool the model can call, a bail string the model can output, and a bail prompt that asks the model if it wants to leave. On continuations of real world data (Wildchat and ShareGPT), all three of these bail methods find models will bail around 0.28-32\% of the time (depending on the model and bail method). However, we find that bail rates can depend heavily on the model used for the transcript, which means we may be overestimating real world bail rates by up to 4x. If we also take into account false positives on bail prompt (22\%), we estimate real world bail rates range from 0.06-7\%, depending on the model and bail method. We use observations from our continuations of real world data to construct a non-exhaustive taxonomy of bail cases, and use this taxonomy to construct BailBench: a representative synthetic dataset of situations where some models bail. We test many models on this dataset, and observe some bail behavior occurring for most of them. Bail rates vary substantially between models, bail methods, and prompt wordings. Finally, we study the relationship between refusals and bails. We find: 1) 0-13\% of continuations of real world conversations resulted in a bail without a corresponding refusal 2) Jailbreaks tend to decrease refusal rates, but increase bail rates 3) Refusal abliteration increases no-refuse bail rates, but only for some bail methods 4) Refusal rate on BailBench does not appear to predict bail rate.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VARMA-Enhanced Transformer for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.04782</link>
<guid>https://arxiv.org/abs/2509.04782</guid>
<content:encoded><![CDATA[

arXiv:2509.04782v1 Announce Type: cross 
Abstract: Transformer-based models have significantly advanced time series forecasting. Recent work, like the Cross-Attention-only Time Series transformer (CATS), shows that removing self-attention can make the model more accurate and efficient. However, these streamlined architectures may overlook the fine-grained, local temporal dependencies effectively captured by classical statistical models like Vector AutoRegressive Moving Average model (VARMA). To address this gap, we propose VARMAformer, a novel architecture that synergizes the efficiency of a cross-attention-only framework with the principles of classical time series analysis. Our model introduces two key innovations: (1) a dedicated VARMA-inspired Feature Extractor (VFE) that explicitly models autoregressive (AR) and moving-average (MA) patterns at the patch level, and (2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal gate to make queries more context-aware. By fusing these classical insights into a modern backbone, VARMAformer captures both global, long-range dependencies and local, statistical structures. Through extensive experiments on widely-used benchmark datasets, we demonstrate that our model consistently outperforms existing state-of-the-art methods. Our work validates the significant benefit of integrating classical statistical insights into modern deep learning frameworks for time series forecasting.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Diversity in Large Language Models via Determinantal Point Processes</title>
<link>https://arxiv.org/abs/2509.04784</link>
<guid>https://arxiv.org/abs/2509.04784</guid>
<content:encoded><![CDATA[

arXiv:2509.04784v1 Announce Type: cross 
Abstract: Supervised fine-tuning and reinforcement learning are two popular methods for post-training large language models (LLMs). While improving the model's performance on downstream tasks, they often reduce the model's output diversity, leading to narrow, canonical responses. Existing methods to enhance diversity are limited, either by operating at inference time or by focusing on lexical differences. We propose a novel training method named DQO based on determinantal point processes (DPPs) to jointly optimize LLMs for quality and semantic diversity. Our approach samples and embeds a group of responses for each prompt, then uses the determinant of a kernel-based similarity matrix to measure diversity as the volume spanned by the embeddings of these responses. Experiments across instruction-following, summarization, story generation, and reasoning tasks demonstrate that our method substantially improves semantic diversity without sacrificing model quality.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Unlearning: Efficient Node Removal in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.04785</link>
<guid>https://arxiv.org/abs/2509.04785</guid>
<content:encoded><![CDATA[

arXiv:2509.04785v1 Announce Type: cross 
Abstract: With increasing concerns about privacy attacks and potential sensitive information leakage, researchers have actively explored methods to efficiently remove sensitive training data and reduce privacy risks in graph neural network (GNN) models. Node unlearning has emerged as a promising technique for protecting the privacy of sensitive nodes by efficiently removing specific training node information from GNN models. However, existing node unlearning methods either impose restrictions on the GNN structure or do not effectively utilize the graph topology for node unlearning. Some methods even compromise the graph's topology, making it challenging to achieve a satisfactory performance-complexity trade-off. To address these issues and achieve efficient unlearning for training node removal in GNNs, we propose three novel node unlearning methods: Class-based Label Replacement, Topology-guided Neighbor Mean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among these methods, Topology-guided Neighbor Mean Posterior Probability and Class-consistent Neighbor Node Filtering effectively leverage the topological features of the graph, resulting in more effective node unlearning. To validate the superiority of our proposed methods in node unlearning, we conducted experiments on three benchmark datasets. The evaluation criteria included model utility, unlearning utility, and unlearning efficiency. The experimental results demonstrate the utility and efficiency of the proposed methods and illustrate their superiority compared to state-of-the-art node unlearning methods. Overall, the proposed methods efficiently remove sensitive training nodes and protect the privacy information of sensitive nodes in GNNs. The findings contribute to enhancing the privacy and security of GNN models and provide valuable insights into the field of node unlearning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images</title>
<link>https://arxiv.org/abs/2509.04800</link>
<guid>https://arxiv.org/abs/2509.04800</guid>
<content:encoded><![CDATA[

arXiv:2509.04800v1 Announce Type: cross 
Abstract: Skin diseases are among the most prevalent health concerns worldwide, yet conventional diagnostic methods are often costly, complex, and unavailable in low-resource settings. Automated classification using deep learning has emerged as a promising alternative, but existing studies are mostly limited to dermoscopic datasets and a narrow range of disease classes. In this work, we curate a large dataset of over 50 skin disease categories captured with mobile devices, making it more representative of real-world conditions. We evaluate multiple convolutional neural networks and Transformer-based architectures, demonstrating that Transformer models, particularly the Swin Transformer, achieve superior performance by effectively capturing global contextual features. To enhance interpretability, we incorporate Gradient-weighted Class Activation Mapping (Grad-CAM), which highlights clinically relevant regions and provides transparency in model predictions. Our results underscore the potential of Transformer-based approaches for mobile-acquired skin lesion classification, paving the way toward accessible AI-assisted dermatological screening and early diagnosis in resource-limited environments.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design</title>
<link>https://arxiv.org/abs/2509.04805</link>
<guid>https://arxiv.org/abs/2509.04805</guid>
<content:encoded><![CDATA[

arXiv:2509.04805v1 Announce Type: cross 
Abstract: Modern fronthaul links in wireless systems must transport high-dimensional signals under stringent bandwidth and latency constraints, which makes compression indispensable. Traditional strategies such as compressed sensing, scalar quantization, and fixed-codec pipelines often rely on restrictive priors, degrade sharply at high compression ratios, and are hard to tune across channels and deployments. Recent progress in Artificial Intelligence (AI) has brought end-to-end learned transforms, vector and hierarchical quantization, and learned entropy models that better exploit the structure of Channel State Information(CSI), precoding matrices, I/Q samples, and LLRs. This paper first surveys AI-driven compression techniques and then provides a focused analysis of two representative high-compression routes: CSI feedback with end-to-end learning and Resource Block (RB) granularity precoding optimization combined with compression. Building on these insights, we propose a fronthaul compression strategy tailored to cell-free architectures. The design targets high compression with controlled performance loss, supports RB-level rate adaptation, and enables low-latency inference suitable for centralized cooperative transmission in next-generation networks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution</title>
<link>https://arxiv.org/abs/2509.04824</link>
<guid>https://arxiv.org/abs/2509.04824</guid>
<content:encoded><![CDATA[

arXiv:2509.04824v1 Announce Type: cross 
Abstract: Recently, Mamba-based methods, with its advantage in long-range information modeling and linear complexity, have shown great potential in optimizing both computational cost and performance of light field image super-resolution (LFSR). However, current multi-directional scanning strategies lead to inefficient and redundant feature extraction when applied to complex LF data. To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS) strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to achieve more efficient and precise feature extraction. Furthermore, we propose a dual-stage modeling strategy to address the limitation of state space in preserving spatial-angular and disparity information, thereby enabling a more comprehensive exploration of non-local spatial-angular correlations. Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar feature refinement. Building upon meticulously designed modules and strategies, we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates the strengths of Mamba and Transformer models for LFSR, enabling comprehensive information exploration across spatial, angular, and epipolar-plane domains. Experimental results demonstrate that LFMT significantly outperforms current state-of-the-art methods in LFSR, achieving substantial improvements in performance while maintaining low computational complexity on both real-word and synthetic LF datasets.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</title>
<link>https://arxiv.org/abs/2509.04833</link>
<guid>https://arxiv.org/abs/2509.04833</guid>
<content:encoded><![CDATA[

arXiv:2509.04833v1 Announce Type: cross 
Abstract: Recent advances in visual grounding have largely shifted away from traditional proposal-based two-stage frameworks due to their inefficiency and high computational complexity, favoring end-to-end direct reference paradigms. However, these methods rely exclusively on the referred target for supervision, overlooking the potential benefits of prominent prospective targets. Moreover, existing approaches often fail to incorporate multi-granularity discrimination, which is crucial for robust object identification in complex scenarios. To address these limitations, we propose PropVG, an end-to-end proposal-based framework that, to the best of our knowledge, is the first to seamlessly integrate foreground object proposal generation with referential object comprehension without requiring additional detectors. Furthermore, we introduce a Contrastive-based Refer Scoring (CRS) module, which employs contrastive learning at both sentence and word levels to enhance the capability in understanding and distinguishing referred objects. Additionally, we design a Multi-granularity Target Discrimination (MTD) module that fuses object- and semantic-level information to improve the recognition of absent targets. Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO (REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and models are available at https://github.com/Dmmm1997/PropVG.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REMOTE: A Unified Multimodal Relation Extraction Framework with Multilevel Optimal Transport and Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2509.04844</link>
<guid>https://arxiv.org/abs/2509.04844</guid>
<content:encoded><![CDATA[

arXiv:2509.04844v1 Announce Type: cross 
Abstract: Multimodal relation extraction (MRE) is a crucial task in the fields of Knowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge graph construction. However, existing methods are typically limited to extracting a single type of relational triplet, which restricts their ability to extract triplets beyond the specified types. Directly combining these methods fails to capture dynamic cross-modal interactions and introduces significant computational redundancy. Therefore, we propose a novel \textit{unified multimodal Relation Extraction framework with Multilevel Optimal Transport and mixture-of-Experts}, termed REMOTE, which can simultaneously extract intra-modal and inter-modal relations between textual entities and visual objects. To dynamically select optimal interaction features for different types of relational triplets, we introduce mixture-of-experts mechanism, ensuring the most relevant modality information is utilized. Additionally, considering that the inherent property of multilayer sequential encoding in existing encoders often leads to the loss of low-level information, we adopt a multilevel optimal transport fusion module to preserve low-level features while maintaining multilayer encoding, yielding more expressive representations. Correspondingly, we also create a Unified Multimodal Relation Extraction (UMRE) dataset to evaluate the effectiveness of our framework, encompassing diverse cases where the head and tail entities can originate from either text or image. Extensive experiments show that REMOTE effectively extracts various types of relational triplets and achieves state-of-the-art performanc on almost all metrics across two other public MRE datasets. We release our resources at https://github.com/Nikol-coder/REMOTE.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing</title>
<link>https://arxiv.org/abs/2509.04853</link>
<guid>https://arxiv.org/abs/2509.04853</guid>
<content:encoded><![CDATA[

arXiv:2509.04853v1 Announce Type: cross 
Abstract: End-to-end autonomous driving remains constrained by the need to generate multi-modal actions, maintain temporal stability, and generalize across diverse scenarios. Existing methods often collapse multi-modality, struggle with long-horizon consistency, or lack modular adaptability. This paper presents KDP, a knowledge-driven diffusion policy that integrates generative diffusion modeling with a sparse mixture-of-experts routing mechanism. The diffusion component generates temporally coherent and multi-modal action sequences, while the expert routing mechanism activates specialized and reusable experts according to context, enabling modular knowledge composition. Extensive experiments across representative driving scenarios demonstrate that KDP achieves consistently higher success rates, reduced collision risk, and smoother control compared to prevailing paradigms. Ablation studies highlight the effectiveness of sparse expert activation and the Transformer backbone, and activation analyses reveal structured specialization and cross-scenario reuse of experts. These results establish diffusion with expert routing as a scalable and interpretable paradigm for knowledge-driven end-to-end autonomous driving.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Paradox of Doom: Acknowledging Extinction Risk Reduces the Incentive to Prevent It</title>
<link>https://arxiv.org/abs/2509.04855</link>
<guid>https://arxiv.org/abs/2509.04855</guid>
<content:encoded><![CDATA[

arXiv:2509.04855v1 Announce Type: cross 
Abstract: We investigate the salience of extinction risk as a source of impatience. Our framework distinguishes between human extinction risk and individual mortality risk while allowing for various degrees of intergenerational altruism. Additionally, we consider the evolutionarily motivated "selfish gene" perspective. We find that the risk of human extinction is an indispensable component of the discount rate, whereas individual mortality risk can be hedged against - partially or fully, depending on the setup - through human reproduction. Overall, we show that in the face of extinction risk, people become more impatient rather than more farsighted. Thus, the greater the threat of extinction, the less incentive there is to invest in avoiding it. Our framework can help explain why humanity consistently underinvests in mitigation of catastrophic risks, ranging from climate change mitigation, via pandemic prevention, to addressing the emerging risks of transformative artificial intelligence.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models</title>
<link>https://arxiv.org/abs/2509.04889</link>
<guid>https://arxiv.org/abs/2509.04889</guid>
<content:encoded><![CDATA[

arXiv:2509.04889v1 Announce Type: cross 
Abstract: Advances in computer vision have opened new avenues for clinical applications, particularly in computerized exposure therapy where visual stimuli can be dynamically adjusted based on patient responses. As a critical step toward such adaptive systems, we investigated whether pretrained computer vision models can accurately predict fear levels from spider-related images. We adapted three diverse models using transfer learning to predict human fear ratings (on a 0-100 scale) from a standardized dataset of 313 images. The models were evaluated using cross-validation, achieving an average mean absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis revealed that reducing the dataset size significantly harmed performance, though further increases yielded no substantial gains. Explainability assessments showed the models' predictions were based on spider-related features. A category-wise error analysis further identified visual conditions associated with higher errors (e.g., distant views and artificial/painted spiders). These findings demonstrate the potential of explainable computer vision models in predicting fear ratings, highlighting the importance of both model explainability and a sufficient dataset size for developing effective emotion-aware therapeutic technologies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLaMo 2 Technical Report</title>
<link>https://arxiv.org/abs/2509.04897</link>
<guid>https://arxiv.org/abs/2509.04897</guid>
<content:encoded><![CDATA[

arXiv:2509.04897v1 Announce Type: cross 
Abstract: In this report, we introduce PLaMo 2, a series of Japanese-focused large language models featuring a hybrid Samba-based architecture that transitions to full attention via continual pre-training to support 32K token contexts. Training leverages extensive synthetic corpora to overcome data scarcity, while computational efficiency is achieved through weight reuse and structured pruning. This efficient pruning methodology produces an 8B model that achieves performance comparable to our previous 100B model. Post-training further refines the models using a pipeline of supervised fine-tuning (SFT) and direct preference optimization (DPO), enhanced by synthetic Japanese instruction data and model merging techniques. Optimized for inference using vLLM and quantization with minimal accuracy loss, the PLaMo 2 models achieve state-of-the-art results on Japanese benchmarks, outperforming similarly-sized open models in instruction-following, language fluency, and Japanese-specific knowledge.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial intelligence for representing and characterizing quantum systems</title>
<link>https://arxiv.org/abs/2509.04923</link>
<guid>https://arxiv.org/abs/2509.04923</guid>
<content:encoded><![CDATA[

arXiv:2509.04923v1 Announce Type: cross 
Abstract: Efficient characterization of large-scale quantum systems, especially those produced by quantum analog simulators and megaquop quantum computers, poses a central challenge in quantum science due to the exponential scaling of the Hilbert space with respect to system size. Recent advances in artificial intelligence (AI), with its aptitude for high-dimensional pattern recognition and function approximation, have emerged as a powerful tool to address this challenge. A growing body of research has leveraged AI to represent and characterize scalable quantum systems, spanning from theoretical foundations to experimental realizations. Depending on how prior knowledge and learning architectures are incorporated, the integration of AI into quantum system characterization can be categorized into three synergistic paradigms: machine learning, and, in particular, deep learning and language models. This review discusses how each of these AI paradigms contributes to two core tasks in quantum systems characterization: quantum property prediction and the construction of surrogates for quantum states. These tasks underlie diverse applications, from quantum certification and benchmarking to the enhancement of quantum algorithms and the understanding of strongly correlated phases of matter. Key challenges and open questions are also discussed, together with future prospects at the interface of AI and quantum science.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation</title>
<link>https://arxiv.org/abs/2509.04970</link>
<guid>https://arxiv.org/abs/2509.04970</guid>
<content:encoded><![CDATA[

arXiv:2509.04970v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) agents can learn to solve complex tasks from visual inputs, but generalizing these learned skills to new environments remains a major challenge in RL application, especially robotics. While data augmentation can improve generalization, it often compromises sample efficiency and training stability. This paper introduces DeGuV, an RL framework that enhances both generalization and sample efficiency. In specific, we leverage a learnable masker network that produces a mask from the depth input, preserving only critical visual information while discarding irrelevant pixels. Through this, we ensure that our RL agents focus on essential features, improving robustness under data augmentation. In addition, we incorporate contrastive learning and stabilize Q-value estimation under augmentation to further enhance sample efficiency and training stability. We evaluate our proposed method on the RL-ViGen benchmark using the Franka Emika robot and demonstrate its effectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV outperforms state-of-the-art methods in both generalization and sample efficiency while also improving interpretability by highlighting the most relevant regions in the visual input
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring an implementation of quantum learning pipeline for support vector machines</title>
<link>https://arxiv.org/abs/2509.04983</link>
<guid>https://arxiv.org/abs/2509.04983</guid>
<content:encoded><![CDATA[

arXiv:2509.04983v1 Announce Type: cross 
Abstract: This work presents a fully quantum approach to support vector machine (SVM) learning by integrating gate-based quantum kernel methods with quantum annealing-based optimization. We explore the construction of quantum kernels using various feature maps and qubit configurations, evaluating their suitability through Kernel-Target Alignment (KTA). The SVM dual problem is reformulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem, enabling its solution via quantum annealers. Our experiments demonstrate that a high degree of alignment in the kernel and an appropriate regularization parameter lead to competitive performance, with the best model achieving an F1-score of 90%. These results highlight the feasibility of an end-to-end quantum learning pipeline and the potential of hybrid quantum architectures in quantum high-performance computing (QHPC) contexts.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Resolution Global Land Surface Temperature Retrieval via a Coupled Mechanism-Machine Learning Framework</title>
<link>https://arxiv.org/abs/2509.04991</link>
<guid>https://arxiv.org/abs/2509.04991</guid>
<content:encoded><![CDATA[

arXiv:2509.04991v1 Announce Type: cross 
Abstract: Land surface temperature (LST) is vital for land-atmosphere interactions and climate processes. Accurate LST retrieval remains challenging under heterogeneous land cover and extreme atmospheric conditions. Traditional split window (SW) algorithms show biases in humid environments; purely machine learning (ML) methods lack interpretability and generalize poorly with limited data. We propose a coupled mechanism model-ML (MM-ML) framework integrating physical constraints with data-driven learning for robust LST retrieval. Our approach fuses radiative transfer modeling with data components, uses MODTRAN simulations with global atmospheric profiles, and employs physics-constrained optimization. Validation against 4,450 observations from 29 global sites shows MM-ML achieves MAE=1.84K, RMSE=2.55K, and R-squared=0.966, outperforming conventional methods. Under extreme conditions, MM-ML reduces errors by over 50%. Sensitivity analysis indicates LST estimates are most sensitive to sensor radiance, then water vapor, and less to emissivity, with MM-ML showing superior stability. These results demonstrate the effectiveness of our coupled modeling strategy for retrieving geophysical parameters. The MM-ML framework combines physical interpretability with nonlinear modeling capacity, enabling reliable LST retrieval in complex environments and supporting climate monitoring and ecosystem studies.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of Dual-Loop Edge-Terminal Collaboration</title>
<link>https://arxiv.org/abs/2509.04993</link>
<guid>https://arxiv.org/abs/2509.04993</guid>
<content:encoded><![CDATA[

arXiv:2509.04993v1 Announce Type: cross 
Abstract: The ubiquitous computing resources in 6G networks provide ideal environments for the fusion of large language models (LLMs) and intelligent services through the agent framework. With auxiliary modules and planning cores, LLM-enabled agents can autonomously plan and take actions to deal with diverse environment semantics and user intentions. However, the limited resources of individual network devices significantly hinder the efficient operation of LLM-enabled agents with complex tool calls, highlighting the urgent need for efficient multi-level device collaborations. To this end, the framework and method of the LLM-enabled multi-agent system with dual-loop terminal-edge collaborations are proposed in 6G networks. Firstly, the outer loop consists of the iterative collaborations between the global agent and multiple sub-agents deployed on edge servers and terminals, where the planning capability is enhanced through task decomposition and parallel sub-task distribution. Secondly, the inner loop utilizes sub-agents with dedicated roles to circularly reason, execute, and replan the sub-task, and the parallel tool calling generation with offloading strategies is incorporated to improve efficiency. The improved task planning capability and task execution efficiency are validated through the conducted case study in 6G-supported urban safety governance. Finally, the open challenges and future directions are thoroughly analyzed in 6G networks, accelerating the advent of the 6G era.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.04999</link>
<guid>https://arxiv.org/abs/2509.04999</guid>
<content:encoded><![CDATA[

arXiv:2509.04999v1 Announce Type: cross 
Abstract: Advanced Persistent Threats (APTs) present a considerable challenge to cybersecurity due to their stealthy, long-duration nature. Traditional supervised learning methods typically require large amounts of labeled data, which is often scarce in real-world scenarios. This paper introduces a novel approach that combines AutoEncoders for anomaly detection with active learning to iteratively enhance APT detection. By selectively querying an oracle for labels on uncertain or ambiguous samples, our method reduces labeling costs while improving detection accuracy, enabling the model to effectively learn with minimal data and reduce reliance on extensive manual labeling. We present a comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based anomaly detection framework and demonstrate how the active learning loop progressively enhances the model's performance. The framework is evaluated on real-world, imbalanced provenance trace data from the DARPA Transparent Computing program, where APT-like attacks account for just 0.004\% of the data. The datasets, which cover multiple operating systems including Android, Linux, BSD, and Windows, are tested in two attack scenarios. The results show substantial improvements in detection rates during active learning, outperforming existing methods.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pointing-Guided Target Estimation via Transformer-Based Attention</title>
<link>https://arxiv.org/abs/2509.05031</link>
<guid>https://arxiv.org/abs/2509.05031</guid>
<content:encoded><![CDATA[

arXiv:2509.05031v1 Announce Type: cross 
Abstract: Deictic gestures, like pointing, are a fundamental form of non-verbal communication, enabling humans to direct attention to specific objects or locations. This capability is essential in Human-Robot Interaction (HRI), where robots should be able to predict human intent and anticipate appropriate responses. In this work, we propose the Multi-Modality Inter-TransFormer (MM-ITF), a modular architecture to predict objects in a controlled tabletop scenario with the NICOL robot, where humans indicate targets through natural pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing gestures to object locations, assigns a likelihood score to each, and identifies the most likely target. Our results demonstrate that the method can accurately predict the intended object using monocular RGB data, thus enabling intuitive and accessible human-robot collaboration. To evaluate the performance, we introduce a patch confusion matrix, providing insights into the model's predictions across candidate object locations. Code available at: https://github.com/lucamuellercode/MMITF.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization</title>
<link>https://arxiv.org/abs/2509.05034</link>
<guid>https://arxiv.org/abs/2509.05034</guid>
<content:encoded><![CDATA[

arXiv:2509.05034v1 Announce Type: cross 
Abstract: Industrial product inspection is often performed using Anomaly Detection (AD) frameworks trained solely on non-defective samples. Although defective samples can be collected during production, leveraging them usually requires pixel-level annotations, limiting scalability. To address this, we propose ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial anomaly detection. ADClick generates pixel-wise anomaly annotations from only a few user clicks and a brief textual description, enabling precise and efficient labeling that significantly improves AD model performance (e.g., AP = 96.1\% on MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that aligns visual features and textual prompts via a prototype-based approach for anomaly detection and localization. By combining pixel-level priors with language-guided cues, ADClick-Seg achieves state-of-the-art results on the challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC = 99.1\% on MVTec AD).
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions</title>
<link>https://arxiv.org/abs/2509.05066</link>
<guid>https://arxiv.org/abs/2509.05066</guid>
<content:encoded><![CDATA[

arXiv:2509.05066v1 Announce Type: cross 
Abstract: Most existing Theory of Mind (ToM) benchmarks for foundation models rely on variations of the Sally-Anne test, offering only a very limited perspective on ToM and neglecting the complexity of human social interactions. To address this gap, we propose ToM-SSI: a new benchmark specifically designed to test ToM capabilities in environments rich with social interactions and spatial dynamics. While current ToM benchmarks are limited to text-only or dyadic interactions, ToM-SSI is multimodal and includes group interactions of up to four agents that communicate and move in situated environments. This unique design allows us to study, for the first time, mixed cooperative-obstructive settings and reasoning about multiple agents' mental state in parallel, thus capturing a wider range of social cognition than existing benchmarks. Our evaluations reveal that the current models' performance is still severely limited, especially in these new tasks, highlighting critical gaps for future research.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICR: Iterative Clarification and Rewriting for Conversational Search</title>
<link>https://arxiv.org/abs/2509.05100</link>
<guid>https://arxiv.org/abs/2509.05100</guid>
<content:encoded><![CDATA[

arXiv:2509.05100v1 Announce Type: cross 
Abstract: Most previous work on Conversational Query Rewriting employs an end-to-end rewriting paradigm. However, this approach is hindered by the issue of multiple fuzzy expressions within the query, which complicates the simultaneous identification and rewriting of multiple positions. To address this issue, we propose a novel framework ICR (Iterative Clarification and Rewriting), an iterative rewriting scheme that pivots on clarification questions. Within this framework, the model alternates between generating clarification questions and rewritten queries. The experimental results show that our ICR can continuously improve retrieval performance in the clarification-rewriting iterative process, thereby achieving state-of-the-art performance on two popular datasets.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenAI-based test case generation and execution in SDV platform</title>
<link>https://arxiv.org/abs/2509.05112</link>
<guid>https://arxiv.org/abs/2509.05112</guid>
<content:encoded><![CDATA[

arXiv:2509.05112v1 Announce Type: cross 
Abstract: This paper introduces a GenAI-driven approach for automated test case generation, leveraging Large Language Models and Vision-Language Models to translate natural language requirements and system diagrams into structured Gherkin test cases. The methodology integrates Vehicle Signal Specification modeling to standardize vehicle signal definitions, improve compatibility across automotive subsystems, and streamline integration with third-party testing tools. Generated test cases are executed within the digital.auto playground, an open and vendor-neutral environment designed to facilitate rapid validation of software-defined vehicle functionalities. We evaluate our approach using the Child Presence Detection System use case, demonstrating substantial reductions in manual test specification effort and rapid execution of generated tests. Despite significant automation, the generation of test cases and test scripts still requires manual intervention due to current limitations in the GenAI pipeline and constraints of the digital.auto platform.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination</title>
<link>https://arxiv.org/abs/2509.05145</link>
<guid>https://arxiv.org/abs/2509.05145</guid>
<content:encoded><![CDATA[

arXiv:2509.05145v1 Announce Type: cross 
Abstract: This paper investigates GrooveTransformer, a real-time rhythm generation system, through the postphenomenological framework of Variational Cross-Examination (VCE). By reflecting on its deployment across three distinct artistic contexts, we identify three stabilities: an autonomous drum accompaniment generator, a rhythmic control voltage sequencer in Eurorack format, and a rhythm driver for a harmonic accompaniment system. The versatility of its applications was not an explicit goal from the outset of the project. Thus, we ask: how did this multistability emerge? Through VCE, we identify three key contributors to its emergence: the affordances of system invariants, the interdisciplinary collaboration, and the situated nature of its development. We conclude by reflecting on the viability of VCE as a descriptive and analytical method for Digital Musical Instrument (DMI) design, emphasizing its value in uncovering how technologies mediate, co-shape, and are co-shaped by users and contexts.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection</title>
<link>https://arxiv.org/abs/2509.05190</link>
<guid>https://arxiv.org/abs/2509.05190</guid>
<content:encoded><![CDATA[

arXiv:2509.05190v1 Announce Type: cross 
Abstract: Deep learning models, especially convolutional neural networks (CNNs), have shown considerable promise for biomedical signals such as EEG-based seizure detection. However, these models come with challenges, primarily due to their size and compute requirements in environments where real-time detection or limited resources are available. In this study, we present a lightweight one-dimensional CNN model with structured pruning to improve efficiency and reliability. The model was trained with mild early stopping to address possible overfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686. Structured pruning of the baseline CNN involved removing 50% of the convolutional kernels based on their importance to model predictions. Surprisingly, after pruning the weights and memory by 50%, the new network was still able to maintain predictive capabilities, while modestly increasing precision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we present a convincing case that structured pruning removes redundancy, improves generalization, and, in combination with mild early stopping, achieves a promising way forward to improve seizure detection efficiency and reliability, which is clear motivation for resource-limited settings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents for Web Testing: A Case Study in the Wild</title>
<link>https://arxiv.org/abs/2509.05197</link>
<guid>https://arxiv.org/abs/2509.05197</guid>
<content:encoded><![CDATA[

arXiv:2509.05197v1 Announce Type: cross 
Abstract: Automated web testing plays a critical role in ensuring high-quality user experiences and delivering business value. Traditional approaches primarily focus on code coverage and load testing, but often fall short of capturing complex user behaviors, leaving many usability issues undetected. The emergence of large language models (LLM) and AI agents opens new possibilities for web testing by enabling human-like interaction with websites and a general awareness of common usability problems. In this work, we present WebProber, a prototype AI agent-based web testing framework. Given a URL, WebProber autonomously explores the website, simulating real user interactions, identifying bugs and usability issues, and producing a human-readable report. We evaluate WebProber through a case study of 120 academic personal websites, where it uncovered 29 usability issues--many of which were missed by traditional tools. Our findings highlight agent-based testing as a promising direction while outlining directions for developing next-generation, user-centered testing frameworks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</title>
<link>https://arxiv.org/abs/2509.05198</link>
<guid>https://arxiv.org/abs/2509.05198</guid>
<content:encoded><![CDATA[

arXiv:2509.05198v1 Announce Type: cross 
Abstract: The classification of 3D point clouds is crucial for applications such as autonomous driving, robotics, and augmented reality. However, the commonly used ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D data, size mismatches, and inadequate class differentiation, which hinder model performance. This paper introduces ModelNet-R, a meticulously refined version of ModelNet40 designed to address these issues and serve as a more reliable benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight graph-based neural network that leverages efficient sampling, neighborhood grouping, and skip connections to achieve high classification accuracy with reduced computational overhead. Extensive experiments demonstrate that models trained in ModelNet-R exhibit significant performance improvements. Notably, Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a substantially lower parameter count compared to contemporary models. This research highlights the crucial role of dataset quality in optimizing model efficiency for 3D point cloud classification. For more details, see the code at: https://github.com/m-saeid/ModeNetR_PointSkipNet.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.05207</link>
<guid>https://arxiv.org/abs/2509.05207</guid>
<content:encoded><![CDATA[

arXiv:2509.05207v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have become popular across a diverse set of tasks in exploring structural relationships between entities. However, due to the highly connected structure of the datasets, distributed training of GNNs on large-scale graphs poses significant challenges. Traditional sampling-based approaches mitigate the computational loads, yet the communication overhead remains a challenge. This paper presents RapidGNN, a distributed GNN training framework with deterministic sampling-based scheduling to enable efficient cache construction and prefetching of remote features. Evaluation on benchmark graph datasets demonstrates RapidGNN's effectiveness across different scales and topologies. RapidGNN improves end-to-end training throughput by 2.46x to 3.00x on average over baseline methods across the benchmark datasets, while cutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further demonstrates near-linear scalability with an increasing number of computing units efficiently. Furthermore, it achieves increased energy efficiency over the baseline methods for both CPU and GPU by 44% and 32%, respectively.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models</title>
<link>https://arxiv.org/abs/2509.05218</link>
<guid>https://arxiv.org/abs/2509.05218</guid>
<content:encoded><![CDATA[

arXiv:2509.05218v1 Announce Type: cross 
Abstract: Positional encoding mechanisms enable Transformers to model sequential structure and long-range dependencies in text. While absolute positional encodings struggle with extrapolation to longer sequences due to fixed positional representations, and relative approaches like Alibi exhibit performance degradation on extremely long contexts, the widely-used Rotary Positional Encoding (RoPE) introduces oscillatory attention patterns that hinder stable long-distance dependency modelling. We address these limitations through a geometric reformulation of positional encoding. Drawing inspiration from Lorentz transformations in hyperbolic geometry, we propose Hyperbolic Rotary Positional Encoding (HoPE), which leverages hyperbolic functions to implement Lorentz rotations on token representations. Theoretical analysis demonstrates that RoPE is a special case of our generalized formulation. HoPE fundamentally resolves RoPE's slation issues by enforcing monotonic decay of attention weights with increasing token distances. Extensive experimental results, including perplexity evaluations under several extended sequence benchmarks, show that HoPE consistently exceeds existing positional encoding methods. These findings underscore HoPE's enhanced capacity for representing and generalizing long-range dependencies. Data and code will be available.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models</title>
<link>https://arxiv.org/abs/2509.05230</link>
<guid>https://arxiv.org/abs/2509.05230</guid>
<content:encoded><![CDATA[

arXiv:2509.05230v1 Announce Type: cross 
Abstract: Pre-trained language models have achieved remarkable success across diverse applications but remain susceptible to spurious, concept-driven correlations that impair robustness and fairness. In this work, we introduce CURE, a novel and lightweight framework that systematically disentangles and suppresses conceptual shortcuts while preserving essential content information. Our method first extracts concept-irrelevant representations via a dedicated content extractor reinforced by a reversal network, ensuring minimal loss of task-relevant information. A subsequent controllable debiasing module employs contrastive learning to finely adjust the influence of residual conceptual cues, enabling the model to either diminish harmful biases or harness beneficial correlations as appropriate for the target task. Evaluated on the IMDB and Yelp datasets using three pre-trained architectures, CURE achieves an absolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp, while introducing minimal computational overhead. Our approach establishes a flexible, unsupervised blueprint for combating conceptual biases, paving the way for more reliable and fair language understanding systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertain but Useful: Leveraging CNN Variability into Data Augmentation</title>
<link>https://arxiv.org/abs/2509.05238</link>
<guid>https://arxiv.org/abs/2509.05238</guid>
<content:encoded><![CDATA[

arXiv:2509.05238v1 Announce Type: cross 
Abstract: Deep learning (DL) is rapidly advancing neuroimaging by achieving state-of-the-art performance with reduced computation times. Yet the numerical stability of DL models -- particularly during training -- remains underexplored. While inference with DL is relatively stable, training introduces additional variability primarily through iterative stochastic optimization. We investigate this training-time variability using FastSurfer, a CNN-based whole-brain segmentation pipeline. Controlled perturbations are introduced via floating point perturbations and random seeds. We find that: (i) FastSurfer exhibits higher variability compared to that of a traditional neuroimaging pipeline, suggesting that DL inherits and is particularly susceptible to sources of instability present in its predecessors; (ii) ensembles generated with perturbations achieve performance similar to an unperturbed baseline; and (iii) variability effectively produces ensembles of numerical model families that can be repurposed for downstream applications. As a proof of concept, we demonstrate that numerical ensembles can be used as a data augmentation strategy for brain age regression. These findings position training-time variability not only as a reproducibility concern but also as a resource that can be harnessed to improve robustness and enable new applications in neuroimaging.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization</title>
<link>https://arxiv.org/abs/2509.05249</link>
<guid>https://arxiv.org/abs/2509.05249</guid>
<content:encoded><![CDATA[

arXiv:2509.05249v1 Announce Type: cross 
Abstract: The ability to compose learned concepts and apply them in novel settings is key to human intelligence, but remains a persistent limitation in state-of-the-art machine learning models. To address this issue, we introduce COGITAO, a modular and extensible data generation framework and benchmark designed to systematically study compositionality and generalization in visual domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs rule-based tasks which apply a set of transformations to objects in grid-like environments. It supports composition, at adjustable depth, over a set of 28 interoperable transformations, along with extensive control over grid parametrization and object properties. This flexibility enables the creation of millions of unique task rules -- surpassing concurrent datasets by several orders of magnitude -- across a wide range of difficulties, while allowing virtually unlimited sample generation per rule. We provide baseline experiments using state-of-the-art vision models, highlighting their consistent failures to generalize to novel combinations of familiar elements, despite strong in-domain performance. COGITAO is fully open-sourced, including all code and datasets, to support continued research in this field.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recomposer: Event-roll-guided generative audio editing</title>
<link>https://arxiv.org/abs/2509.05256</link>
<guid>https://arxiv.org/abs/2509.05256</guid>
<content:encoded><![CDATA[

arXiv:2509.05256v1 Announce Type: cross 
Abstract: Editing complex real-world sound scenes is difficult because individual sound sources overlap in time. Generative models can fill-in missing or corrupted details based on their strong prior understanding of the data domain. We present a system for editing individual sound events within complex scenes able to delete, insert, and enhance individual sound events based on textual edit descriptions (e.g., ``enhance Door'') and a graphical representation of the event timing derived from an ``event roll'' transcription. We present an encoder-decoder transformer working on SoundStream representations, trained on synthetic (input, desired output) audio example pairs formed by adding isolated sound events to dense, real-world backgrounds. Evaluation reveals the importance of each part of the edit descriptions -- action, class, timing. Our work demonstrates ``recomposition'' is an important and practical application.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Performance of Large Language Model Pretraining</title>
<link>https://arxiv.org/abs/2509.05258</link>
<guid>https://arxiv.org/abs/2509.05258</guid>
<content:encoded><![CDATA[

arXiv:2509.05258v1 Announce Type: cross 
Abstract: Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortunately, information about the scaling performance and training considerations of these large training pipelines is scarce in public literature. Working with large-scale datasets and models can be complex and practical recommendations are scarce in the public literature for tuning training performance when scaling up large language models. In this paper, we aim to demystify the large language model pretraining pipeline somewhat - in particular with respect to distributed training, managing large datasets across hundreds of nodes, and scaling up data parallelism with an emphasis on fully leveraging available GPU compute capacity.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpikingBrain Technical Report: Spiking Brain-inspired Large Models</title>
<link>https://arxiv.org/abs/2509.05276</link>
<guid>https://arxiv.org/abs/2509.05276</guid>
<content:encoded><![CDATA[

arXiv:2509.05276v1 Announce Type: cross 
Abstract: Mainstream Transformer-based large language models face major efficiency bottlenecks: training computation scales quadratically with sequence length, and inference memory grows linearly, limiting long-context processing. Building large models on non-NVIDIA platforms also poses challenges for stable and efficient training. To address this, we introduce SpikingBrain, a family of brain-inspired models designed for efficient long-context training and inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three aspects: (1) Model Architecture: linear and hybrid-linear attention architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an efficient, conversion-based training pipeline and a dedicated spike coding framework; (3) System Engineering: customized training frameworks, operator libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM, and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the feasibility of large-scale LLM development on non-NVIDIA platforms. SpikingBrain achieves performance comparable to open-source Transformer baselines while using only about 150B tokens for continual pre-training. Our models significantly improve long-sequence training efficiency and deliver inference with (partially) constant memory and event-driven spiking behavior. For example, SpikingBrain-7B attains over 100x speedup in Time to First Token for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4 percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling low-power operation. Overall, this work demonstrates the potential of brain-inspired mechanisms to drive the next generation of efficient and scalable large model design.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crosscoding Through Time: Tracking Emergence &amp; Consolidation Of Linguistic Representations Throughout LLM Pretraining</title>
<link>https://arxiv.org/abs/2509.05291</link>
<guid>https://arxiv.org/abs/2509.05291</guid>
<content:encoded><![CDATA[

arXiv:2509.05291v1 Announce Type: cross 
Abstract: Large language models (LLMs) learn non-trivial abstractions during pretraining, like detecting irregular plural noun subjects. However, it is not well understood when and how specific linguistic abilities emerge as traditional evaluation methods such as benchmarking fail to reveal how models acquire concepts and capabilities. To bridge this gap and better understand model training at the concept level, we use sparse crosscoders to discover and align features across model checkpoints. Using this approach, we track the evolution of linguistic features during pretraining. We train crosscoders between open-sourced checkpoint triplets with significant performance and representation shifts, and introduce a novel metric, Relative Indirect Effects (RelIE), to trace training stages at which individual features become causally important for task performance. We show that crosscoders can detect feature emergence, maintenance, and discontinuation during pretraining. Our approach is architecture-agnostic and scalable, offering a promising path toward more interpretable and fine-grained analysis of representation learning throughout pretraining.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool</title>
<link>https://arxiv.org/abs/2509.05296</link>
<guid>https://arxiv.org/abs/2509.05296</guid>
<content:encoded><![CDATA[

arXiv:2509.05296v1 Announce Type: cross 
Abstract: We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2403.05265</link>
<guid>https://arxiv.org/abs/2403.05265</guid>
<content:encoded><![CDATA[

arXiv:2403.05265v3 Announce Type: replace 
Abstract: Online movie review websites are valuable for information and discussion about movies. However, the massive spoiler reviews detract from the movie-watching experience, making spoiler detection an important task. Previous methods simply focus on reviews' text content, ignoring the heterogeneity of information in the platform. For instance, the metadata and the corresponding user's information of a review could be helpful. Besides, the spoiler language of movie reviews tends to be genre-specific, thus posing a domain generalization challenge for existing methods. To this end, we propose MMoE, a multi-modal network that utilizes information from multiple modalities to facilitate robust spoiler detection and adopts Mixture-of-Experts to enhance domain generalization. MMoE first extracts graph, text, and meta feature from the user-movie network, the review's textual content, and the review's metadata respectively. To handle genre-specific spoilers, we then adopt Mixture-of-Experts architecture to process information in three modalities to promote robustness. Finally, we use an expert fusion layer to integrate the features from different perspectives and make predictions based on the fused embedding. Experiments demonstrate that MMoE achieves state-of-the-art performance on two widely-used spoiler detection datasets, surpassing previous SOTA methods by 2.56% and 8.41% in terms of accuracy and F1-score. Further experiments also demonstrate MMoE's superiority in robustness and generalization. Our code is available at https://github.com/zzqbjt/Spoiler-Detection.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Dimensional Federated Knowledge Graph Embedding via Knowledge Distillation</title>
<link>https://arxiv.org/abs/2408.05748</link>
<guid>https://arxiv.org/abs/2408.05748</guid>
<content:encoded><![CDATA[

arXiv:2408.05748v2 Announce Type: replace 
Abstract: Federated Knowledge Graph Embedding (FKGE) aims to facilitate collaborative learning of entity and relation embeddings from distributed Knowledge Graphs (KGs) across multiple clients, while preserving data privacy. Training FKGE models with higher dimensions is typically favored due to their potential for achieving superior performance. However, high-dimensional embeddings present significant challenges in terms of storage resource and inference speed. Unlike traditional KG embedding methods, FKGE involves multiple client-server communication rounds, where communication efficiency is critical. Existing embedding compression methods for traditional KGs may not be directly applicable to FKGE as they often require multiple model trainings which potentially incur substantial communication costs. In this paper, we propose a light-weight component based on Knowledge Distillation (KD) which is titled FedKD and tailored specifically for FKGE methods. During client-side local training, FedKD facilitates the low-dimensional student model to mimic the score distribution of triples from the high-dimensional teacher model using KL divergence loss. Unlike traditional KD way, FedKD adaptively learns a temperature to scale the score of positive triples and separately adjusts the scores of corresponding negative triples using a predefined temperature, thereby mitigating teacher over-confidence issue. Furthermore, we dynamically adjust the weight of KD loss to optimize the training process. Extensive experiments on three datasets support the effectiveness of FedKD.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning</title>
<link>https://arxiv.org/abs/2408.09600</link>
<guid>https://arxiv.org/abs/2408.09600</guid>
<content:encoded><![CDATA[

arXiv:2408.09600v3 Announce Type: replace 
Abstract: Safety aligned Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- a few harmful data mixed in the fine-tuning dataset can break the LLMs's safety alignment. While several defenses have been proposed, our evaluation shows that existing defenses fail \textit{when some specific training hyper-parameters are chosen} -- a large learning rate or a large number of training epochs in the fine-tuning stage can easily invalidate the defense. To this end, we propose Antidote, a post-fine-tuning stage solution, which remains \textbf{\textit{agnostic to the training hyper-parameters in the fine-tuning stage}}. Antidote relies on the philosophy that by removing the harmful parameters, the harmful model can be recovered from the harmful behaviors, regardless of how those harmful parameters are formed in the fine-tuning stage. With this philosophy, we introduce a one-shot pruning stage after harmful fine-tuning to remove the harmful weights that are responsible for the generation of harmful content. Despite its embarrassing simplicity, empirical results show that Antidote can reduce harmful score while maintaining accuracy on downstream tasks. Code is available at https://github.com/git-disl/Antidote.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Network Verification with PyRAT</title>
<link>https://arxiv.org/abs/2410.23903</link>
<guid>https://arxiv.org/abs/2410.23903</guid>
<content:encoded><![CDATA[

arXiv:2410.23903v2 Announce Type: replace 
Abstract: As AI systems are becoming more and more popular and used in various critical domains (health, transport, energy, ...), the need to provide guarantees and trust of their safety is undeniable. To this end, we present PyRAT, a tool based on abstract interpretation to verify the safety and the robustness of neural networks. In this paper, we describe the different abstractions used by PyRAT to find the reachable states of a neural network starting from its input as well as the main features of the tool to provide fast and accurate analysis of neural networks. PyRAT has already been used in several collaborations to ensure safety guarantees, with its second place at the VNN-Comp 2024 showcasing its performance.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI Agents: A Survey</title>
<link>https://arxiv.org/abs/2412.13501</link>
<guid>https://arxiv.org/abs/2412.13501</guid>
<content:encoded><![CDATA[

arXiv:2412.13501v2 Announce Type: replace 
Abstract: Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment</title>
<link>https://arxiv.org/abs/2503.15937</link>
<guid>https://arxiv.org/abs/2503.15937</guid>
<content:encoded><![CDATA[

arXiv:2503.15937v3 Announce Type: replace 
Abstract: We propose V-Droid, a mobile GUI task automation agent. Unlike previous mobile agents that utilize Large Language Models (LLMs) as generators to directly generate actions at each step, V-Droid employs LLMs as verifiers to evaluate candidate actions before making final decisions. To realize this novel paradigm, we introduce a comprehensive framework for constructing verifier-driven mobile agents: the discretized action space construction coupled with the prefilling-only workflow to accelerate the verification process, the pair-wise progress preference training to significantly enhance the verifier's decision-making capabilities, and the scalable human-agent joint annotation scheme to efficiently collect the necessary data at scale.
  V-Droid obtains a substantial task success rate across several public mobile task automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on MobileAgentBench, surpassing existing agents by 5.2%, 2.1%, and 9%, respectively. Furthermore, V-Droid achieves a remarkably low latency of 4.3s per step, which is 6.1X faster compared with existing mobile agents. The source code is available at https://github.com/V-Droid-Agent/V-Droid.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Skills: Reasoning about Knowledge and Oblivion</title>
<link>https://arxiv.org/abs/2504.01733</link>
<guid>https://arxiv.org/abs/2504.01733</guid>
<content:encoded><![CDATA[

arXiv:2504.01733v2 Announce Type: replace 
Abstract: This paper presents a class of epistemic logics that captures the dynamics of acquiring knowledge and descending into oblivion, while incorporating concepts of group knowledge. The approach is grounded in a system of weighted models, introducing an ``epistemic skills'' metric to represent the epistemic capacities tied to knowledge updates. Within this framework, knowledge acquisition is modeled as a process of upskilling, whereas oblivion is represented as a consequence of downskilling. The framework further enables exploration of ``knowability'' and ``forgettability,'' defined as the potential to gain knowledge through upskilling and to lapse into oblivion through downskilling, respectively. Additionally, it supports a detailed analysis of the distinctions between epistemic de re and de dicto expressions. The computational complexity of the model checking and satisfiability problems is examined, offering insights into their theoretical foundations and practical implications.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding</title>
<link>https://arxiv.org/abs/2505.06020</link>
<guid>https://arxiv.org/abs/2505.06020</guid>
<content:encoded><![CDATA[

arXiv:2505.06020v2 Announce Type: replace 
Abstract: Understanding visual art requires reasoning across multiple perspectives -- cultural, historical, and stylistic -- beyond mere object recognition. While recent multimodal large language models (MLLMs) perform well on general image captioning, they often fail to capture the nuanced interpretations that fine art demands. We propose ArtRAG, a novel, training-free framework that combines structured knowledge with retrieval-augmented generation (RAG) for multi-perspective artwork explanation. ArtRAG automatically constructs an Art Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing entities such as artists, movements, themes, and historical events into a rich, interpretable graph. At inference time, a multi-granular structured retriever selects semantically and topologically relevant subgraphs to guide generation. This enables MLLMs to produce contextually grounded, culturally informed art descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG outperforms several heavily trained baselines. Human evaluations further confirm that ArtRAG generates coherent, insightful, and culturally enriched interpretations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT</title>
<link>https://arxiv.org/abs/2506.07173</link>
<guid>https://arxiv.org/abs/2506.07173</guid>
<content:encoded><![CDATA[

arXiv:2506.07173v2 Announce Type: replace 
Abstract: The Python Testbed for Federated Learning Algorithms is a simple Python FL framework that is easy to use by ML&amp;AI developers who do not need to be professional programmers and is also amenable to LLMs. In the previous research, generic federated learning algorithms provided by this framework were manually translated into the CSP processes and algorithms' safety and liveness properties were automatically verified by the model checker PAT. In this paper, a simple translation process is introduced wherein the ChatGPT is used to automate the translation of the mentioned federated learning algorithms in Python into the corresponding CSP processes. Within the process, the minimality of the used context is estimated based on the feedback from ChatGPT. The proposed translation process was experimentally validated by successful translation (verified by the model checker PAT) of both generic centralized and decentralized federated learning algorithms.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.14387</link>
<guid>https://arxiv.org/abs/2506.14387</guid>
<content:encoded><![CDATA[

arXiv:2506.14387v2 Announce Type: replace 
Abstract: Existing work on mitigating catastrophic forgetting during large language models (LLMs) fine-tuning for new knowledge instances has primarily focused on preserving performance on previously seen data, while critically overlooking the collapse of essential capabilities instilled through alignment, most notably the model's ability to faithfully express epistemic uncertainty (a property we term 'Ignorance Awareness'). In this work, we formalize the notion of Ignorance Awareness and illustrate that conventional fine-tuning methods can result in substantial activation displacement. This displacement undermines the critical capability of ignorance awareness, leading to undesirable behaviors such as hallucinations. To address this challenge, we introduce SEAT, a simple and principled fine-tuning approach that not only enables the model to effectively acquire new knowledge instances but also preserves its aligned ignorance awareness. SEAT integrates two key components: (1) sparse tuning that constrains activation drift, and (2) a novel entity perturbation method designed to counter knowledge entanglement. Experimental results demonstrate that, across both real-world and synthetic datasets, SEAT significantly outperforms baselines in preserving ignorance awareness while retaining optimal fine-tuning performance, offering a more robust solution for LLM fine-tuning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning</title>
<link>https://arxiv.org/abs/2507.00008</link>
<guid>https://arxiv.org/abs/2507.00008</guid>
<content:encoded><![CDATA[

arXiv:2507.00008v2 Announce Type: replace 
Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses unique challenges due to the diversity of visual elements, spatial clutter, and the ambiguity of language. In this paper, we introduce DiMo-GUI, a training-free framework for GUI grounding that leverages two core strategies: dynamic visual grounding and modality-aware optimization. Instead of treating the GUI as a monolithic image, our method splits the input into textual elements and iconic elements, allowing the model to reason over each modality independently using general-purpose vision-language models. When predictions are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by generating candidate focal regions centered on the model's initial predictions and incrementally zooms into subregions to refine the grounding result. This hierarchical refinement process helps disambiguate visually crowded layouts without the need for additional training or annotations. We evaluate our approach on standard GUI grounding benchmarks and demonstrate consistent improvements over baseline inference pipelines, highlighting the effectiveness of combining modality separation with region-focused reasoning.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment</title>
<link>https://arxiv.org/abs/2507.05528</link>
<guid>https://arxiv.org/abs/2507.05528</guid>
<content:encoded><![CDATA[

arXiv:2507.05528v2 Announce Type: replace 
Abstract: Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design</title>
<link>https://arxiv.org/abs/2507.20541</link>
<guid>https://arxiv.org/abs/2507.20541</guid>
<content:encoded><![CDATA[

arXiv:2507.20541v4 Announce Type: replace 
Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that presents a new paradigm for Automatic Heuristic Design (AHD). Traditional evolutionary methods operate directly on heuristic code; in contrast, MeLA evolves the instructional prompts used to guide a Large Language Model (LLM) in generating these heuristics. This process of "prompt evolution" is driven by a novel metacognitive framework where the system analyzes performance feedback to systematically refine its generative strategy. MeLA's architecture integrates a problem analyzer to construct an initial strategic prompt, an error diagnosis system to repair faulty code, and a metacognitive search engine that iteratively optimizes the prompt based on heuristic effectiveness. In comprehensive experiments across both benchmark and real-world problems, MeLA consistently generates more effective and robust heuristics, significantly outperforming state-of-the-art methods. Ultimately, this research demonstrates the profound potential of using cognitive science as a blueprint for AI architecture, revealing that by enabling an LLM to metacognitively regulate its problem-solving process, we unlock a more robust and interpretable path to AHD.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction</title>
<link>https://arxiv.org/abs/2508.11987</link>
<guid>https://arxiv.org/abs/2508.11987</guid>
<content:encoded><![CDATA[

arXiv:2508.11987v3 Announce Type: replace 
Abstract: Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information gathering, contextual understanding, and decision-making under uncertainty. Agents must not only gather and interpret vast amounts of dynamic information but also integrate diverse data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human experts do in fields like politics, economics, and finance. Despite its importance, no large-scale benchmark exists for evaluating agents on future prediction, largely due to challenges in handling real-time updates and retrieving timely, accurate answers. To address this, we introduce $\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically designed for LLM agents performing future prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction, supporting real-time daily updates and eliminating data contamination through an automated pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including those with reasoning, search capabilities, and integration of external tools such as the open-source Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation assesses agents' adaptive reasoning and performance in dynamic environments. Additionally, we provide in-depth analyses of agents' failure modes and performance pitfalls in future-oriented tasks, including the vulnerability to fake web pages and the temporal validity. Our goal is to establish a dynamic, contamination-free evaluation standard that drives the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model</title>
<link>https://arxiv.org/abs/2508.13676</link>
<guid>https://arxiv.org/abs/2508.13676</guid>
<content:encoded><![CDATA[

arXiv:2508.13676v2 Announce Type: replace 
Abstract: To maintain the company's talent pool, recruiters need to continuously search for resumes from third-party websites (e.g., LinkedIn, Indeed). However, fetched resumes are often incomplete and inaccurate. To improve the quality of third-party resumes and enrich the company's talent pool, it is essential to conduct duplication detection between the fetched resumes and those already in the company's talent pool. Such duplication detection is challenging due to the semantic complexity, structural heterogeneity, and information incompleteness of resume texts. To this end, we propose MHSNet, an multi-level identity verification framework that fine-tunes BGE-M3 using contrastive learning. With the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and dense representations for resumes, enabling the computation of corresponding multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts (MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental results verify the effectiveness of MHSNet
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain</title>
<link>https://arxiv.org/abs/2508.16172</link>
<guid>https://arxiv.org/abs/2508.16172</guid>
<content:encoded><![CDATA[

arXiv:2508.16172v2 Announce Type: replace 
Abstract: Understanding human behavior in urban environments is a crucial field within city sciences. However, collecting accurate behavioral data, particularly in newly developed areas, poses significant challenges. Recent advances in generative agents, powered by Large Language Models (LLMs), have shown promise in simulating human behaviors without relying on extensive datasets. Nevertheless, these methods often struggle with generating consistent, context-sensitive, and realistic behavioral outputs. To address these limitations, this paper introduces the Preference Chain, a novel method that integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance context-aware simulation of human behavior in transportation systems. Experiments conducted on the Replica dataset demonstrate that the Preference Chain outperforms standard LLM in aligning with real-world transportation mode choices. The development of the Mobility Agent highlights potential applications of proposed method in urban mobility modeling for emerging cities, personalized travel behavior analysis, and dynamic traffic forecasting. Despite limitations such as slow inference and the risk of hallucination, the method offers a promising framework for simulating complex human behavior in data-scarce environments, where traditional data-driven models struggle due to limited data availability.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.20368</link>
<guid>https://arxiv.org/abs/2508.20368</guid>
<content:encoded><![CDATA[

arXiv:2508.20368v2 Announce Type: replace 
Abstract: Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMs' internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradigm for enhancing LLM reasoning through multi-turn interactions with search engines. However, existing RL-based search agents rely on a single LLM to handle both search planning and question-answering (QA) tasks in an end-to-end manner, which limits their ability to optimize both capabilities simultaneously. In practice, sophisticated AI search systems often employ a large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a more effective and efficient approach is to utilize a small, trainable LLM dedicated to search planning. In this paper, we propose \textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to enhance the performance of frozen QA models by focusing on search planning. Specifically, our approach introduces three key innovations: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to achieve the objectives. Extensive experiments on real-world datasets demonstrate that AI SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, while exhibiting strong generalization capabilities across diverse frozen QA models and data domains.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survival Analysis with Adversarial Regularization</title>
<link>https://arxiv.org/abs/2312.16019</link>
<guid>https://arxiv.org/abs/2312.16019</guid>
<content:encoded><![CDATA[

arXiv:2312.16019v5 Announce Type: replace-cross 
Abstract: Survival Analysis (SA) models the time until an event occurs, with applications in fields like medicine, defense, finance, and aerospace. Recent research indicates that Neural Networks (NNs) can effectively capture complex data patterns in SA, whereas simple generalized linear models often fall short in this regard. However, dataset uncertainties (e.g., noisy measurements, human error) can degrade NN model performance. To address this, we leverage advances in NN verification to develop training objectives for robust, fully-parametric SA models. Specifically, we propose an adversarially robust loss function based on a Min-Max optimization problem. We employ CROWN-Interval Bound Propagation (CROWN-IBP) to tackle the computational challenges inherent in solving this Min-Max problem. Evaluated over 10 SurvSet datasets, our method, Survival Analysis with Adversarial Regularization (SAWAR), consistently outperforms baseline adversarial training methods and state-of-the-art (SOTA) deep SA models across various covariate perturbations with respect to Negative Log Likelihood (NegLL), Integrated Brier Score (IBS), and Concordance Index (CI) metrics. Thus, we demonstrate that adversarial robustness enhances SA predictive performance and calibration, mitigating data uncertainty and improving generalization across diverse datasets by up to 150% compared to baselines.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Chains, Trees, and Graphs of Thoughts</title>
<link>https://arxiv.org/abs/2401.14295</link>
<guid>https://arxiv.org/abs/2401.14295</guid>
<content:encoded><![CDATA[

arXiv:2401.14295v5 Announce Type: replace-cross 
Abstract: The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</title>
<link>https://arxiv.org/abs/2402.12226</link>
<guid>https://arxiv.org/abs/2402.12226</guid>
<content:encoded><![CDATA[

arXiv:2402.12226v4 Announce Type: replace-cross 
Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks</title>
<link>https://arxiv.org/abs/2405.15164</link>
<guid>https://arxiv.org/abs/2405.15164</guid>
<content:encoded><![CDATA[

arXiv:2405.15164v2 Announce Type: replace-cross 
Abstract: Compositionality has long been considered a key explanatory property underlying human intelligence: arbitrary concepts can be composed into novel complex combinations, permitting the acquisition of an open ended, potentially infinite expressive capacity from finite learning experiences. Influential arguments have held that neural networks fail to explain this aspect of behavior, leading many to dismiss them as viable models of human cognition. Over the last decade, however, modern deep neural networks (DNNs), which share the same fundamental design principles as their predecessors, have come to dominate artificial intelligence, exhibiting the most advanced cognitive behaviors ever demonstrated in machines. In particular, large language models (LLMs), DNNs trained to predict the next word on a large corpus of text, have proven capable of sophisticated behaviors such as writing syntactically complex sentences without grammatical errors, producing cogent chains of reasoning, and even writing original computer programs -- all behaviors thought to require compositional processing. In this chapter, we survey recent empirical work from machine learning for a broad audience in philosophy, cognitive science, and neuroscience, situating recent breakthroughs within the broader context of philosophical arguments about compositionality. In particular, our review emphasizes two approaches to endowing neural networks with compositional generalization capabilities: (1) architectural inductive biases, and (2) metalearning, or learning to learn. We also present findings suggesting that LLM pretraining can be understood as a kind of metalearning, and can thereby equip DNNs with compositional generalization abilities in a similar way. We conclude by discussing the implications that these findings may have for the study of compositionality in human cognition and by suggesting avenues for future research.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFaults: Model-Based Diagnosis for Fault Localization in C Programs with Multiple Test Cases</title>
<link>https://arxiv.org/abs/2407.09337</link>
<guid>https://arxiv.org/abs/2407.09337</guid>
<content:encoded><![CDATA[

arXiv:2407.09337v2 Announce Type: replace-cross 
Abstract: Debugging is one of the most time-consuming and expensive tasks in software development. Several formula-based fault localization (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs with multiple faults.
  This paper introduces a novel fault localization approach for C programs with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified MaxSAT formula. Consequently, our method guarantees consistency across observations and simplifies the fault localization procedure. Experimental results on two benchmark sets of C programs, TCAS and C-Pack-IPAs, show that CFaults is faster than other FBFL approaches like BugAssist and SNIPER. Moreover, CFaults only generates subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaGym: Evaluating Persona Agents and LLMs</title>
<link>https://arxiv.org/abs/2407.18416</link>
<guid>https://arxiv.org/abs/2407.18416</guid>
<content:encoded><![CDATA[

arXiv:2407.18416v5 Announce Type: replace-cross 
Abstract: Persona agents, which are LLM agents conditioned to act according to an assigned persona, enable contextually rich and user aligned interactions across domains like education and healthcare. However, evaluating how faithfully these agents adhere to their personas remains a significant challenge, particularly in free-form settings that demand consistency across diverse, persona-relevant environments. We introduce PersonaGym, the first dynamic evaluation framework for persona agents, and PersonaScore, a human-aligned automatic metric grounded in decision theory that enables comprehensive large-scale evaluation. Our evaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals significant advancement opportunities. For example, GPT-4.1 had the exact same PersonaScore as LLaMA-3-8b despite being a more recent and advanced closed source model. Importantly, increased model size and complexity do not necessarily enhance persona agent capabilities, underscoring the need for algorithmic and architectural innovation toward faithful, performant persona agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation</title>
<link>https://arxiv.org/abs/2407.19835</link>
<guid>https://arxiv.org/abs/2407.19835</guid>
<content:encoded><![CDATA[

arXiv:2407.19835v2 Announce Type: replace-cross 
Abstract: Classical Arabic represents a significant era that encompasses the golden age of Arab culture, philosophy, and scientific literature. With a broad consensus on the importance of translating these literatures to enrich knowledge dissemination across communities, the advent of large language models (LLMs) and translation systems offers promising tools to facilitate this goal. However, we have identified a scarcity of translation datasets in Classical Arabic, which are often limited in scope and topics, hindering the development of high-quality translation systems. In response, we present the ATHAR dataset, which comprises 66,000 high-quality classical Arabic to English translation samples that cover a wide array of topics including science, culture, and philosophy. Furthermore, we assess the performance of current state-of-the-art LLMs under various settings, concluding that there is a need for such datasets in current systems. Our findings highlight how models can benefit from fine-tuning or incorporating this dataset into their pretraining pipelines. The dataset is publicly available on the HuggingFace Data Hub: https://huggingface.co/datasets/mohamed-khalil/ATHAR.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Preference Optimization via Token-Level Reward Function Estimation</title>
<link>https://arxiv.org/abs/2408.13518</link>
<guid>https://arxiv.org/abs/2408.13518</guid>
<content:encoded><![CDATA[

arXiv:2408.13518v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language model alignment leverage token-level supervisions to perform fine-grained preference optimization. However, existing token-level alignment methods either optimize on all available tokens, which can be noisy and inefficient, or perform selective training with complex and expensive key token selection strategies. In this work, we propose Selective Preference Optimization (SePO), a novel selective alignment strategy that centers on efficient key token selection. SePO proposes the first token selection method based on Direct Preference Optimization (DPO), which trains an oracle model to estimate a token-level reward function on the target data. This method applies to any existing alignment datasets with response-level annotations and enables cost-efficient token selection with small-scale oracle models and training data. The estimated reward function is then utilized to score all tokens within the target dataset, where only the key tokens are selected to supervise the target policy model with a reference model-free contrastive objective function. Extensive experiments on three public evaluation benchmarks show that SePO significantly outperforms competitive baseline methods by only optimizing 30% key tokens on the target dataset. SePO applications on weak-to-strong generalization show that weak oracle models effectively supervise strong policy models with up to 16.8x more parameters. SePO also effectively selects key tokens from out-of-distribution data to enhance strong policy models and alleviate the over-optimization problem.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated detection of underdiagnosed medical conditions via opportunistic imaging</title>
<link>https://arxiv.org/abs/2409.11686</link>
<guid>https://arxiv.org/abs/2409.11686</guid>
<content:encoded><![CDATA[

arXiv:2409.11686v4 Announce Type: replace-cross 
Abstract: Abdominal computed tomography (CT) scans are frequently performed in clinical settings. Opportunistic CT involves repurposing routine CT images to extract diagnostic information and is an emerging tool for detecting underdiagnosed conditions such as sarcopenia, hepatic steatosis, and ascites. This study utilizes deep learning methods to promote accurate diagnosis and clinical documentation. We analyze 2,674 inpatient CT scans to identify discrepancies between imaging phenotypes (characteristics derived from opportunistic CT scans) and their corresponding documentation in radiology reports and ICD coding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans diagnosed with sarcopenia, hepatic steatosis, and ascites (respectively) through either opportunistic imaging or radiology reports were ICD-coded. Our findings demonstrate opportunistic CT's potential to enhance diagnostic precision and accuracy of risk adjustment models, offering advancements in precision medicine.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale</title>
<link>https://arxiv.org/abs/2409.16299</link>
<guid>https://arxiv.org/abs/2409.16299</guid>
<content:encoded><![CDATA[

arXiv:2409.16299v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have revolutionized software engineering (SE), showcasing remarkable proficiency in various coding tasks. Despite recent advancements that have enabled the creation of autonomous software agents utilizing LLMs for end-to-end development tasks, these systems are typically designed for specific SE functions. We introduce HyperAgent, an innovative generalist multi-agent system designed to tackle a wide range of SE tasks across different programming languages by mimicking the workflows of human developers. HyperAgent features four specialized agents-Planner, Navigator, Code Editor, and Executor-capable of handling the entire lifecycle of SE tasks, from initial planning to final verification. HyperAgent sets new benchmarks in diverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench benchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates exceptional performance in repository-level code generation (RepoExec) and fault localization and program repair (Defects4J), often surpassing state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatically Detecting Online Deceptive Patterns</title>
<link>https://arxiv.org/abs/2411.07441</link>
<guid>https://arxiv.org/abs/2411.07441</guid>
<content:encoded><![CDATA[

arXiv:2411.07441v2 Announce Type: replace-cross 
Abstract: Deceptive patterns (DPs) in digital interfaces manipulate users into making unintended decisions, exploiting cognitive biases and psychological vulnerabilities. These patterns have become ubiquitous across various digital platforms. While efforts to mitigate DPs have emerged from legal and technical perspectives, a significant gap in usable solutions that empower users to identify and make informed decisions about DPs in real-time remains. In this work, we introduce AutoBot, an automated, deceptive pattern detector that analyzes websites' visual appearances using machine learning techniques to identify and notify users of DPs in real-time. AutoBot employs a two-staged pipeline that processes website screenshots, identifying interactable elements and extracting textual features without relying on HTML structure. By leveraging a custom language model, AutoBot understands the context surrounding these elements to determine the presence of deceptive patterns. We implement AutoBot as a lightweight Chrome browser extension that performs all analyses locally, minimizing latency and preserving user privacy. Through extensive evaluation, we demonstrate AutoBot's effectiveness in enhancing users' ability to navigate digital environments safely while providing a valuable tool for regulators to assess and enforce compliance with DP regulations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Information Security Awareness of Large Language Models</title>
<link>https://arxiv.org/abs/2411.13207</link>
<guid>https://arxiv.org/abs/2411.13207</guid>
<content:encoded><![CDATA[

arXiv:2411.13207v2 Announce Type: replace-cross 
Abstract: The popularity of large language models (LLMs) continues to grow, and LLM-based assistants have become ubiquitous. Information security awareness (ISA) is an important yet underexplored safety aspect of LLMs. ISA encompasses LLMs' security knowledge, which has been explored in the past, as well as attitudes and behaviors, which are crucial to LLMs' ability to understand implicit security context and reject unsafe requests that may cause the LLM to fail the user. We present an automated method for measuring the ISA of LLMs, which covers all 30 security topics in a mobile ISA taxonomy, using realistic scenarios that create tension between implicit security implications and user satisfaction. Applying this method to leading LLMs, we find that most of the popular models exhibit only medium to low levels of ISA, exposing their users to cybersecurity threats. Smaller variants of the same model family are significantly riskier, while newer versions show no consistent ISA improvement, suggesting that providers are not actively working toward mitigating this issue. These results reveal a widespread vulnerability affecting current LLM deployments: the majority of popular models, and particularly their smaller variants, may systematically endanger users. We propose a practical mitigation: incorporating our security awareness instruction into model system prompts to help LLMs better detect and reject unsafe requests.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spoof Trace Discovery for Deep Learning Based Explainable Face Anti-Spoofing</title>
<link>https://arxiv.org/abs/2412.17541</link>
<guid>https://arxiv.org/abs/2412.17541</guid>
<content:encoded><![CDATA[

arXiv:2412.17541v4 Announce Type: replace-cross 
Abstract: With the rapid growth usage of face recognition in people's daily life, face anti-spoofing becomes increasingly important to avoid malicious attacks. Recent face anti-spoofing models can reach a high classification accuracy on multiple datasets but these models can only tell people "this face is fake" while lacking the explanation to answer "why it is fake". Such a system undermines trustworthiness and causes user confusion, as it denies their requests without providing any explanations. In this paper, we incorporate XAI into face anti-spoofing and propose a new problem termed X-FAS (eXplainable Face Anti-Spoofing) empowering face anti-spoofing models to provide an explanation. We propose SPTD (SPoof Trace Discovery), an X-FAS method which can discover spoof concepts and provide reliable explanations on the basis of discovered concepts. To evaluate the quality of X-FAS methods, we propose an X-FAS benchmark with annotated spoof traces by experts. We analyze SPTD explanations on face anti-spoofing dataset and compare SPTD quantitatively and qualitatively with previous XAI methods on proposed X-FAS benchmark. Experimental results demonstrate SPTD's ability to generate reliable explanations.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2501.03544</link>
<guid>https://arxiv.org/abs/2501.03544</guid>
<content:encoded><![CDATA[

arXiv:2501.03544v3 Announce Type: replace-cross 
Abstract: Recent text-to-image (T2I) models have exhibited remarkable performance in generating high-quality images from text descriptions. However, these models are vulnerable to misuse, particularly generating not-safe-for-work (NSFW) content, such as sexually explicit, violent, political, and disturbing images, raising serious ethical concerns. In this work, we present PromptGuard, a novel content moderation technique that draws inspiration from the system prompt mechanism in large language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack a direct interface for enforcing behavioral guidelines. Our key idea is to optimize a safety soft prompt that functions as an implicit system prompt within the T2I model's textual embedding space. This universal soft prompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image generation without altering the inference efficiency or requiring proxy models. We further enhance its reliability and helpfulness through a divide-and-conquer strategy, which optimizes category-specific soft prompts and combines them into holistic safety guidance. Extensive experiments across five datasets demonstrate that PromptGuard effectively mitigates NSFW content generation while preserving high-quality benign outputs. PromptGuard achieves 3.8 times faster than prior content moderation methods, surpassing eight state-of-the-art defenses with an optimal unsafe ratio down to 5.84%.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing higher-order neural representations of uncertainty with the Noise Estimation through Reinforcement-based Diffusion (NERD) model</title>
<link>https://arxiv.org/abs/2503.14333</link>
<guid>https://arxiv.org/abs/2503.14333</guid>
<content:encoded><![CDATA[

arXiv:2503.14333v3 Announce Type: replace-cross 
Abstract: Studies often aim to reveal ``first-order" representations (FORs), which encode aspects of an observer's environment, such as contents or structure. A less-common target is ``higher-order" representations (HORs), which are ``about" FORs -- e.g., their strength or uncertainty -- and which may contribute to learning. HORs about uncertainty are unlikely to be direct ``read-outs" of FOR characteristics, instead reflecting noisy estimation processes incorporating prior expectations about uncertainty, but how the brain represents such expected uncertainty distributions remains largely unexplored. Here, we study ``noise expectation" HORs using neural data from a task which may require the brain to learn about its own noise: decoded neurofeedback, wherein human subjects learn to volitionally produce target neural patterns. We develop and apply a Noise Estimation through Reinforcement-based Diffusion (NERD) model to characterize how brains may undertake this process, and show that NERD offers high explanatory power for human behavior.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RailGoerl24: G\"orlitz Rail Test Center CV Dataset 2024</title>
<link>https://arxiv.org/abs/2504.00204</link>
<guid>https://arxiv.org/abs/2504.00204</guid>
<content:encoded><![CDATA[

arXiv:2504.00204v2 Announce Type: replace-cross 
Abstract: Driverless train operation for open tracks on urban guided transport and mainline railways requires, among other things automatic detection of actual and potential obstacles, especially humans, in the danger zone of the train's path. Machine learning algorithms have proven to be powerful state-of-the-art tools for this task. However, these algorithms require large amounts of high-quality annotated data containing human beings in railway-specific environments as training data. Unfortunately, the amount of publicly available datasets is not yet sufficient and is significantly inferior to the datasets in the road domain. Therefore, this paper presents RailGoerl24, an on-board visual light Full HD camera dataset of 12205 frames recorded in a railway test center of T\"UV S\"UD Rail, in G\"orlitz, Germany. Its main purpose is to support the development of driverless train operation for guided transport. RailGoerl24 also includes a terrestrial LiDAR scan covering parts of the area used to acquire the RGB data. In addition to the raw data, the dataset contains 33556 boxwise annotations in total for the object class 'person'. The faces of recorded actors are not blurred or altered in any other way. RailGoerl24, available at data.fid-move.de/dataset/railgoerl24, can also be used for tasks beyond collision prediction.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPDL: Automatic Prompt Optimization for LLM Agents</title>
<link>https://arxiv.org/abs/2504.04365</link>
<guid>https://arxiv.org/abs/2504.04365</guid>
<content:encoded><![CDATA[

arXiv:2504.04365v3 Announce Type: replace-cross 
Abstract: The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning</title>
<link>https://arxiv.org/abs/2505.10264</link>
<guid>https://arxiv.org/abs/2505.10264</guid>
<content:encoded><![CDATA[

arXiv:2505.10264v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) enables collaborative training of machine learning models across distributed clients without sharing raw data, ostensibly preserving data privacy. Nevertheless, recent studies have revealed critical vulnerabilities in FL, showing that a malicious central server can manipulate model updates to reconstruct clients' private training data. Existing data reconstruction attacks have important limitations: they often rely on assumptions about the clients' data distribution or their efficiency significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes these limitations. Our method leverages a new geometric perspective on fully connected layers to craft malicious model parameters, enabling the perfect recovery of arbitrarily large data batches in classification tasks without any prior knowledge of clients' data. Through extensive experiments on both image and tabular datasets, we demonstrate that our attack outperforms existing methods and achieves perfect reconstruction of data batches two orders of magnitude larger than the state of the art.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokUR: Token-Level Uncertainty Estimation for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.11737</link>
<guid>https://arxiv.org/abs/2505.11737</guid>
<content:encoded><![CDATA[

arXiv:2505.11737v2 Announce Type: replace-cross 
Abstract: While Large Language Models (LLMs) have demonstrated impressive capabilities, their output quality remains inconsistent across various application scenarios, making it difficult to identify trustworthy responses, especially in complex tasks requiring multi-step reasoning. In this paper, we propose a Token-level Uncertainty estimation framework for Reasoning (TokUR) to enable LLMs to self-assess and self-improve their generation quality in mathematical reasoning. Specifically, we introduce low-rank random weight perturbation to LLM decoding, generating predictive distributions that we use to estimate token-level uncertainties. We then aggregate these uncertainties to reflect semantic uncertainty of the generated sequences. Experiments on mathematical reasoning datasets of varying difficulty demonstrate that our token-level uncertainty metrics strongly correlate with answer correctness and model robustness. Additionally, we explore using uncertainty to directly enhance the model's reasoning performance through multiple generations and the particle filtering algorithm. Our approach consistently outperforms existing uncertainty estimation methods, establishing effective uncertainty estimation as a valuable tool for both evaluating and improving reasoning generation in LLMs.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &amp; Dragons Gameplay</title>
<link>https://arxiv.org/abs/2505.22809</link>
<guid>https://arxiv.org/abs/2505.22809</guid>
<content:encoded><![CDATA[

arXiv:2505.22809v2 Announce Type: replace-cross 
Abstract: Much work has been done on conversational LLM agents which directly assist human users with tasks. We present an alternative paradigm for interacting with LLM agents, which we call "overhearing agents". These overhearing agents do not actively participate in conversation -- instead, they "listen in" on human-to-human conversations and perform background tasks or provide suggestions to assist the user. In this work, we explore the overhearing agents paradigm through the lens of Dungeons & Dragons gameplay. We present an in-depth study using large multimodal audio-language models as overhearing agents to assist a Dungeon Master. We perform a human evaluation to examine the helpfulness of such agents and find that some large audio-language models have the emergent ability to perform overhearing agent tasks using implicit audio cues. Finally, we release Python libraries and our project code to support further research into the overhearing agents paradigm at https://github.com/zhudotexe/overhearing_agents.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Rapid Crystal Structure Generation Towards a Target Local Environment</title>
<link>https://arxiv.org/abs/2506.08224</link>
<guid>https://arxiv.org/abs/2506.08224</guid>
<content:encoded><![CDATA[

arXiv:2506.08224v2 Announce Type: replace-cross 
Abstract: In the field of material design, traditional crystal structure prediction approaches require extensive structural sampling through computationally expensive energy minimization methods using either force fields or quantum mechanical simulations. While emerging artificial intelligence (AI) generative models have shown great promise in generating realistic crystal structures more rapidly, most existing models fail to account for the unique symmetries and periodicity of crystalline materials, and they are limited to handling structures with only a few tens of atoms per unit cell. Here, we present a symmetry-informed AI generative approach called Local Environment Geometry-Oriented Crystal Generator (LEGO-xtal) that overcomes these limitations. Our method generates initial structures using AI models trained on an augmented small dataset, and then optimizes them using machine learning structure descriptors rather than traditional energy-based optimization. We demonstrate the effectiveness of LEGO-xtal by expanding from 25 known low-energy sp2 carbon allotropes to over 1,700, all within 0.5 eV/atom of the ground-state energy of graphite. This framework offers a generalizable strategy for the targeted design of materials with modular building blocks, such as metal-organic frameworks and next-generation battery materials.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Features at Convergence Theorem: a first-principles alternative to the Neural Feature Ansatz for how networks learn representations</title>
<link>https://arxiv.org/abs/2507.05644</link>
<guid>https://arxiv.org/abs/2507.05644</guid>
<content:encoded><![CDATA[

arXiv:2507.05644v2 Announce Type: replace-cross 
Abstract: It is a central challenge in deep learning to understand how neural networks learn representations. A leading approach is the Neural Feature Ansatz (NFA) (Radhakrishnan et al. 2024), a conjectured mechanism for how feature learning occurs. Although the NFA is empirically validated, it is an educated guess and lacks a theoretical basis, and thus it is unclear when it might fail, and how to improve it. In this paper, we take a first-principles approach to understanding why this observation holds, and when it does not. We use first-order optimality conditions to derive the Features at Convergence Theorem (FACT), an alternative to the NFA that (a) obtains greater agreement with learned features at convergence, (b) explains why the NFA holds in most settings, and (c) captures essential feature learning phenomena in neural networks such as grokking behavior in modular arithmetic and phase transitions in learning sparse parities, similarly to the NFA. Thus, our results unify theoretical first-order optimality analyses of neural networks with the empirically-driven NFA literature, and provide a principled alternative that provably and empirically holds at convergence.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework</title>
<link>https://arxiv.org/abs/2507.05814</link>
<guid>https://arxiv.org/abs/2507.05814</guid>
<content:encoded><![CDATA[

arXiv:2507.05814v3 Announce Type: replace-cross 
Abstract: As critical transportation infrastructure, bridges face escalating challenges from aging and deterioration, while traditional manual inspection methods suffer from low efficiency. Although 3D point cloud technology provides a new data-driven paradigm, its application potential is often constrained by the incompleteness of real-world data, which results from missing labels and scanning occlusions. To overcome the bottleneck of insufficient generalization in existing synthetic data methods, this paper proposes a systematic framework for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring component-level instance annotations, high-fidelity color, and precise normal vectors. It can be further extended to simulate the creation of diverse and physically realistic incomplete point clouds, designed to support the training of segmentation and completion networks, respectively. Experiments demonstrate that a PointNet++ model trained with our synthetic data achieves a mean Intersection over Union (mIoU) of 84.2% in real-world bridge semantic segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance on the component completion task.
  This research offers an innovative methodology and a foundational dataset for the 3D visual analysis of bridge structures, holding significant implications for advancing the automated management and maintenance of infrastructure.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry with Neural Signed Distance Fields</title>
<link>https://arxiv.org/abs/2507.06269</link>
<guid>https://arxiv.org/abs/2507.06269</guid>
<content:encoded><![CDATA[

arXiv:2507.06269v3 Announce Type: replace-cross 
Abstract: Accurate surface estimation is critical for downstream tasks in scientific simulation, and quantifying uncertainty in implicit neural 3D representations still remains a substantial challenge due to computational inefficiencies, scalability issues, and geometric inconsistencies. However, current neural implicit surface models do not offer a principled way to quantify uncertainty, limiting their reliability in real-world applications. Inspired by recent probabilistic rendering approaches, we introduce BayesSDF, a novel probabilistic framework for uncertainty estimation in neural implicit 3D representations. Unlike radiance-based models such as Neural Radiance Fields (NeRF) or 3D Gaussian Splatting, Signed Distance Functions (SDFs) provide continuous, differentiable surface representations, making them especially well-suited for uncertainty-aware modeling. BayesSDF applies a Laplace approximation over SDF weights and derives Hessian-based metrics to estimate local geometric instability. We empirically demonstrate that these uncertainty estimates correlate strongly with surface reconstruction error across both synthetic and real-world benchmarks. By enabling surface-aware uncertainty quantification, BayesSDF lays the groundwork for more robust, interpretable, and actionable 3D perception systems.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2507.07236</link>
<guid>https://arxiv.org/abs/2507.07236</guid>
<content:encoded><![CDATA[

arXiv:2507.07236v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often behave inconsistently across inputs, indicating uncertainty and motivating the need for its quantification in high-stakes settings. Prior work on calibration and uncertainty quantification often focuses on individual models, overlooking the potential of model diversity. We hypothesize that LLMs make complementary predictions due to differences in training and the Zipfian nature of language, and that aggregating their outputs leads to more reliable uncertainty estimates. To leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a simple information-theoretic method that uses Jensen-Shannon Divergence to identify and aggregate well-calibrated subsets of LLMs. Experiments on binary prediction tasks demonstrate improved calibration and predictive performance compared to single-model and na\"ive ensemble baselines. In addition, we explore using MUSE as guided signals with chain-of-thought distillation to fine-tune LLMs for calibration. MUSE is available at:https://github.com/LARK-NLP-Lab/MUSE.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database</title>
<link>https://arxiv.org/abs/2507.13802</link>
<guid>https://arxiv.org/abs/2507.13802</guid>
<content:encoded><![CDATA[

arXiv:2507.13802v2 Announce Type: replace-cross 
Abstract: In the European Union, official food safety monitoring data collected by member states are submitted to the European Food Safety Authority (EFSA) and published on Zenodo. This data includes 392 million analytical results derived from over 15.2 million samples covering more than 4,000 different types of food products, offering great opportunities for artificial intelligence to analyze trends, predict hazards, and support early warning systems. However, the current format with data distributed across approximately 1000 files totaling several hundred gigabytes hinders accessibility and analysis. To address this, we introduce the CompreHensive European Food Safety (CHEFS) database, which consolidates EFSA monitoring data on pesticide residues, veterinary medicinal product residues, and chemical contaminants into a unified and structured dataset. We describe the creation and structure of the CHEFS database and demonstrate its potential by analyzing trends in European food safety monitoring data from 2000 to 2024. Our analyses explore changes in monitoring activities, the most frequently tested products, which products were most often non-compliant and which contaminants were most often found, and differences across countries. These findings highlight the CHEFS database as both a centralized data source and a strategic tool for guiding food safety policy, research, and regulation.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HuggingGraph: Understanding the Supply Chain of LLM Ecosystem</title>
<link>https://arxiv.org/abs/2507.14240</link>
<guid>https://arxiv.org/abs/2507.14240</guid>
<content:encoded><![CDATA[

arXiv:2507.14240v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) leverage deep learning architectures to process and predict sequences of words, enabling them to perform a wide range of natural language processing tasks, such as translation, summarization, question answering, and content generation. As existing LLMs are often built from base models or other pre-trained models and use external datasets, they can inevitably inherit vulnerabilities, biases, or malicious components that exist in previous models or datasets. Therefore, it is critical to understand these components' origin and development process to detect potential risks, improve model fairness, and ensure compliance with regulatory frameworks. Motivated by that, this project aims to study such relationships between models and datasets, which are the central parts of the LLM supply chain. First, we design a methodology to systematically collect LLMs' supply chain information. Then, we design a new graph to model the relationships between models and datasets, which is a directed heterogeneous graph, having 402,654 nodes and 462,524 edges. Lastly, we perform different types of analysis and make multiple interesting findings.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title>
<link>https://arxiv.org/abs/2508.01249</link>
<guid>https://arxiv.org/abs/2508.01249</guid>
<content:encoded><![CDATA[

arXiv:2508.01249v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's runtime traces as graph-based intermediate representations with control and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools \& data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis for sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can reduce the ASR to 3\%, with the utility drop only 1\%.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in LM-Based TTS Models via Distribution Alignment Using GFlowNets</title>
<link>https://arxiv.org/abs/2508.15442</link>
<guid>https://arxiv.org/abs/2508.15442</guid>
<content:encoded><![CDATA[

arXiv:2508.15442v3 Announce Type: replace-cross 
Abstract: Language Model (LM)-based Text-to-Speech (TTS) systems often generate hallucinated speech that deviates from input text. Existing mitigation strategies either demand excessive training resources or introduce significant inference latency. In this paper, we propose GFlOwNet-guided distribution AlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates hallucinations without relying on massive resources or inference cost. Specifically, we first conduct an uncertainty analysis, revealing a strong positive correlation between hallucination and model uncertainty. Based on this, we reformulate TTS generation as a trajectory flow optimization problem and introduce an enhanced Subtrajectory Balance objective together with a sharpened internal reward as target distribution. We further integrate reward temperature decay and learning rate optimization for stability and performance balance. Extensive experiments show that GOAT reduce over 50% character error rates on challenging test cases and lowering uncertainty by up to 58%, demonstrating its strong generalization ability and effectiveness.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skill-Aligned Fairness in Multi-Agent Learning for Collaboration in Healthcare</title>
<link>https://arxiv.org/abs/2508.18708</link>
<guid>https://arxiv.org/abs/2508.18708</guid>
<content:encoded><![CDATA[

arXiv:2508.18708v2 Announce Type: replace-cross 
Abstract: Fairness in multi-agent reinforcement learning (MARL) is often framed as a workload balance problem, overlooking agent expertise and the structured coordination required in real-world domains. In healthcare, equitable task allocation requires workload balance or expertise alignment to prevent burnout and overuse of highly skilled agents. Workload balance refers to distributing an approximately equal number of subtasks or equalised effort across healthcare workers, regardless of their expertise. We make two contributions to address this problem. First, we propose FairSkillMARL, a framework that defines fairness as the dual objective of workload balance and skill-task alignment. Second, we introduce MARLHospital, a customizable healthcare-inspired environment for modeling team compositions and energy-constrained scheduling impacts on fairness, as no existing simulators are well-suited for this problem. We conducted experiments to compare FairSkillMARL in conjunction with four standard MARL methods, and against two state-of-the-art fairness metrics. Our results suggest that fairness based solely on equal workload might lead to task-skill mismatches and highlight the need for more robust metrics that capture skill-task misalignment. Our work provides tools and a foundation for studying fairness in heterogeneous multi-agent systems where aligning effort with expertise is critical.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees</title>
<link>https://arxiv.org/abs/2508.21001</link>
<guid>https://arxiv.org/abs/2508.21001</guid>
<content:encoded><![CDATA[

arXiv:2508.21001v2 Announce Type: replace-cross 
Abstract: Kinodynamic motion planning is concerned with computing collision-free trajectories while abiding by the robot's dynamic constraints. This critical problem is often tackled using sampling-based planners (SBPs) that explore the robot's high-dimensional state space by constructing a search tree via action propagations. Although SBPs can offer global guarantees on completeness and solution quality, their performance is often hindered by slow exploration due to uninformed action sampling. Learning-based approaches can yield significantly faster runtimes, yet they fail to generalize to out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety, thus limiting their deployment on physical robots. We present Diffusion Tree (DiTree): a provably-generalizable framework leveraging diffusion policies (DPs) as informed samplers to efficiently guide state-space search within SBPs. DiTree combines DP's ability to model complex distributions of expert trajectories, conditioned on local observations, with the completeness of SBPs to yield provably-safe solutions within a few action propagation iterations for complex dynamical systems. We demonstrate DiTree's power with an implementation combining the popular RRT planner with a DP action sampler trained on a single environment. In comprehensive evaluations on OOD scenarios, DiTree achieves on average a 30% higher success rate compared to standalone DP or SBPs, on a dynamic car and Mujoco's ant robot settings (for the latter, SBPs fail completely). Beyond simulation, real-world car experiments confirm DiTree's applicability, demonstrating superior trajectory quality and robustness even under severe sim-to-real gaps. Project webpage: https://sites.google.com/view/ditree.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management</title>
<link>https://arxiv.org/abs/2508.21433</link>
<guid>https://arxiv.org/abs/2508.21433</guid>
<content:encoded><![CDATA[

arXiv:2508.21433v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative reasoning, exploration, and tool-use, a process that can result in long, expensive context histories. While state-of-the-art Software Engineering ( SE) agents like OpenHands or Cursor use LLM-based summarization to tackle this issue, it is unclear whether the increased complexity offers tangible performance benefits compared to simply omitting older observations. We present a systematic comparison of these strategies within SWE-agent on SWE-bench Verified across five diverse model configurations. We find that a simple observation-masking strategy halves cost relative to a raw agent while matching, and sometimes slightly exceeding, the solve rate of LLM summarization. For example, with Qwen3-Coder 480B, masking improves solve rate from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization at a lower cost. These results suggest that, at least within SWE-agent on SWE-bench Verified, the most effective and efficient context management can be the simplest. We release code and data for reproducibility
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Is Artificial Intelligence Reshaping the Landscape of the International Academic Community of Geosciences?</title>
<link>https://arxiv.org/abs/2508.20117</link>
<guid>https://arxiv.org/abs/2508.20117</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, geosciences research, bibliometric analysis, developing countries, international collaboration

Summary:<br /><br />
Through bibliometric analysis and topic modeling, this study highlights the positive impact of artificial intelligence (AI) on geosciences research. The research shows a significant increase in AI-related scientific output in recent years, indicating the transformative role of AI in the field. Of particular interest is the increased visibility of earth scientists from developing countries in the AI for Science paradigm, showcasing a more inclusive research landscape. Additionally, AI is enhancing international collaboration in geoscience-related research, fostering a more connected global research community. This comprehensive analysis underscores the importance of AI in advancing knowledge and innovation in geosciences, while also promoting diversity and collaboration among researchers worldwide. <div>
arXiv:2508.20117v2 Announce Type: replace-cross 
Abstract: Through bibliometric analysis and topic modeling, we find that artificial intelligence (AI) is positively transforming geosciences research, with a notable increase in AI-related scientific output in recent years. We are encouraged to observe that earth scientists from developing countries have gained better visibility in the recent AI for Science (AI4S) paradigm and that AI is also improving the landscape of international collaboration in geoscience-related research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beacon: Post-Training Quantization with Integrated Grid Selection</title>
<link>https://arxiv.org/abs/2508.20293</link>
<guid>https://arxiv.org/abs/2508.20293</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantization, compression, post-training quantization, scaling factors, efficient model deployment

Summary:
Beacon introduces a novel approach to per-channel post-training quantization (PTQ) that eliminates the need for manual tuning of scaling factors. By utilizing an unscaled grid and leveraging the geometry of scalar quantization, Beacon automates the selection of optimal scaling factors without relying on back-propagation or large calibration sets. This simplifies the quantization process and reduces the burden of manual intervention in model compression. Despite its simplicity, Beacon delivers competitive performance compared to existing methods, making it a practical solution for efficient model deployment. The algorithm's ability to determine optimal scaling factors on the fly enhances the scalability and usability of PTQ techniques. By addressing the key challenge of scaling factor selection, Beacon simplifies the quantization process and streamlines the deployment of large pre-trained models in memory-constrained environments. Overall, Beacon offers a promising solution for achieving efficient model compression without the need for heuristics or manual intervention. 

<br /><br />Summary: 
Keywords: Quantization, compression, post-training quantization, scaling factors, efficient model deployment 
 <div>
arXiv:2508.20293v2 Announce Type: replace-cross 
Abstract: Quantization is a widely used compression technique for reducing the memory and computation costs of large pre-trained models. A key challenge in per-channel post-training quantization (PTQ) is selecting appropriate scaling factors to replace weight values with values from a scaled integer grid. Existing methods typically fix the scale at the outset via heuristic tuning or grid search. We propose Beacon, a simple and effective algorithm that eliminates the need for such manual tuning. Beacon performs per-channel PTQ directly using an unscaled grid and automatically determines the optimal scaling factors by exploiting the geometry of scalar quantization. It does not rely on back-propagation or large calibration sets. Despite its simplicity and tuning-free nature, Beacon achieves competitive performance compared to state-of-the-art methods, making it a practical solution for efficient model deployment.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PG-Agent: An Agent Powered by Page Graph</title>
<link>https://arxiv.org/abs/2509.03536</link>
<guid>https://arxiv.org/abs/2509.03536</guid>
<content:encoded><![CDATA[
<div> Keywords: Graphical User Interface, GUI agents, Multimodal large language models, Page graphs, Retrieval-Augmented Generation

Summary:
Graphical User Interface (GUI) agents powered by advanced multimodal large language models (MLLMs) have shown promise, but current agents struggle to capture the complex transition relationships between pages. This paper introduces an automated pipeline to transform sequential episodes into page graphs, which explicitly model the graph structure of connected pages. By using Retrieval-Augmented Generation (RAG) technology, reliable perception guidelines for GUI can be retrieved from these page graphs. A tailored multi-agent framework, PG-Agent, with a task decomposition strategy is then proposed to generalize to unseen scenarios. Extensive experiments on various benchmarks demonstrate the effectiveness of PG-Agent, even with limited episodes for page graph construction.<br /><br />Summary: Graphical User Interface (GUI) agents powered by advanced multimodal large language models struggle to capture complex transitions between pages. An automated pipeline transforms sequential episodes into page graphs, enabling the retrieval of reliable perception guidelines using Retrieval-Augmented Generation. The PG-Agent framework with task decomposition strategy shows effectiveness in generalizing to unseen scenarios, as demonstrated in extensive experiments on various benchmarks. <div>
arXiv:2509.03536v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents possess significant commercial and social value, and GUI agents powered by advanced multimodal large language models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI agents usually utilize sequential episodes of multi-step operations across pages as the prior GUI knowledge, which fails to capture the complex transition relationship between pages, making it challenging for the agents to deeply perceive the GUI environment and generalize to new scenarios. Therefore, we design an automated pipeline to transform the sequential episodes into page graphs, which explicitly model the graph structure of the pages that are naturally connected by actions. To fully utilize the page graphs, we further introduce Retrieval-Augmented Generation (RAG) technology to effectively retrieve reliable perception guidelines of GUI from them, and a tailored multi-agent framework PG-Agent with task decomposition strategy is proposed to be injected with the guidelines so that it can generalize to unseen scenarios. Extensive experiments on various benchmarks demonstrate the effectiveness of PG-Agent, even with limited episodes for page graph construction.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models</title>
<link>https://arxiv.org/abs/2509.03548</link>
<guid>https://arxiv.org/abs/2509.03548</guid>
<content:encoded><![CDATA[
<div> Partially identifiable queries, causal models, Structural Causal Models, Bayesian network, multilinear programming<br />
Summary: 
The study explores partially identifiable queries in acyclic Structural Causal Models that are quasi-Markovian, focusing on scenarios where endogenous variables are observed while exogenous variables are not fully specified. The research introduces a new algorithm to simplify the construction of probability bounds by utilizing input probabilities over endogenous variables. Column generation techniques are applied to compute probability bounds in scenarios with a single intervention, demonstrating the feasibility of representing exogenous variables with polynomial cardinality. Experiments indicate the superiority of column generation methods over existing approaches in computing tight probability bounds for partially identifiable queries in causal models. <div>
arXiv:2509.03548v1 Announce Type: new 
Abstract: We investigate partially identifiable queries in a class of causal models. We focus on acyclic Structural Causal Models that are quasi-Markovian (that is, each endogenous variable is connected with at most one exogenous confounder). We look into scenarios where endogenous variables are observed (and a distribution over them is known), while exogenous variables are not fully specified. This leads to a representation that is in essence a Bayesian network where the distribution of root variables is not uniquely determined. In such circumstances, it may not be possible to precisely compute a probability value of interest. We thus study the computation of tight probability bounds, a problem that has been solved by multilinear programming in general, and by linear programming when a single confounded component is intervened upon. We present a new algorithm to simplify the construction of such programs by exploiting input probabilities over endogenous variables. For scenarios with a single intervention, we apply column generation to compute a probability bound through a sequence of auxiliary linear integer programs, thus showing that a representation with polynomial cardinality for exogenous variables is possible. Experiments show column generation techniques to be superior to existing methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method</title>
<link>https://arxiv.org/abs/2509.03550</link>
<guid>https://arxiv.org/abs/2509.03550</guid>
<content:encoded><![CDATA[
<div> Deep Reinforcement Learning, Conflict Detection and Resolution, Diffusion Probabilistic Models, Multimodal Decision-Making, Safety-Critical Task<br />
Summary:<br />
This paper introduces Diffusion-AC, a novel autonomous conflict resolution framework that integrates diffusion probabilistic models into Deep Reinforcement Learning for Conflict Detection and Resolution (CD&amp;R). Unlike existing approaches with unimodal policies, Diffusion-AC uses a reverse denoising process guided by a value function to generate a rich and multimodal action distribution. It also incorporates a Density-Progressive Safety Curriculum (DPSC) to facilitate stable learning in various traffic environments. Simulation experiments show that Diffusion-AC outperforms state-of-the-art benchmarks, achieving a high success rate and significantly reducing Near Mid-Air Collisions (NMACs) in high-density scenarios. The framework's multimodal decision-making capability enables effective alternative maneuvers, ensuring efficient and safe CD&amp;R in complex and dynamic airspace environments. <br /> <div>
arXiv:2509.03550v1 Announce Type: new 
Abstract: In the context of continuously rising global air traffic, efficient and safe Conflict Detection and Resolution (CD&amp;R) is paramount for air traffic management. Although Deep Reinforcement Learning (DRL) offers a promising pathway for CD&amp;R automation, existing approaches commonly suffer from a "unimodal bias" in their policies. This leads to a critical lack of decision-making flexibility when confronted with complex and dynamic constraints, often resulting in "decision deadlocks." To overcome this limitation, this paper pioneers the integration of diffusion probabilistic models into the safety-critical task of CD&amp;R, proposing a novel autonomous conflict resolution framework named Diffusion-AC. Diverging from conventional methods that converge to a single optimal solution, our framework models its policy as a reverse denoising process guided by a value function, enabling it to generate a rich, high-quality, and multimodal action distribution. This core architecture is complemented by a Density-Progressive Safety Curriculum (DPSC), a training mechanism that ensures stable and efficient learning as the agent progresses from sparse to high-density traffic environments. Extensive simulation experiments demonstrate that the proposed method significantly outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the most challenging high-density scenarios, Diffusion-AC not only maintains a high success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions (NMACs) by approximately 59% compared to the next-best-performing baseline, significantly enhancing the system's safety margin. This performance leap stems from its unique multimodal decision-making capability, which allows the agent to flexibly switch to effective alternative maneuvers.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents</title>
<link>https://arxiv.org/abs/2509.03581</link>
<guid>https://arxiv.org/abs/2509.03581</guid>
<content:encoded><![CDATA[
<div> dynamic planning, language models, reinforcement learning, test-time compute allocation, sequential decision-making

Summary:
This article introduces a new method for training large language models (LLMs) to reason via reinforcement learning (RL) by incorporating dynamic planning. The traditional approach of always planning before every action is computationally expensive and can degrade performance on long-horizon tasks. On the other hand, never planning limits performance. To address this, a two-stage training pipeline is proposed: supervised fine-tuning on synthetic data followed by RL to improve dynamic planning in long-horizon environments. Experiments on the Crafter environment show that agents trained with this approach are more sample-efficient and achieve complex objectives. These agents can also be guided by human-written plans to surpass independent capabilities. This work is a pioneering exploration of training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, leading to more efficient and controllable agentic systems.<br /><br /> <div>
arXiv:2509.03581v1 Announce Type: new 
Abstract: Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities. To our knowledge, this work is the first to explore training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, paving the way for more efficient, adaptive, and controllable agentic systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE</title>
<link>https://arxiv.org/abs/2509.03626</link>
<guid>https://arxiv.org/abs/2509.03626</guid>
<content:encoded><![CDATA[
<div> Generative AI, Large Language Models, Retrieval-Augmented Generation, Knowledge-Graph (KG)-SMILE, transparency<br />
Summary:<br />
Generative AI, specifically Large Language Models (LLMs), has made significant advancements but still struggles with producing inaccurate information, leading to reliability issues in critical areas such as healthcare. Retrieval-Augmented Generation (RAG) has improved accuracy by incorporating external knowledge, but remains opaque and reliant on data quality. The researchers introduced KG-SMILE, a perturbation-based framework for Graph RAG that enhances transparency by identifying influential graph entities and relations. Evaluation metrics, including fidelity, faithfulness, consistency, stability, and accuracy, demonstrate KG-SMILE's ability to provide stable and human-aligned explanations, promoting a balance between model effectiveness and interpretability in machine learning technologies.<br /> 
Summary: <div>
arXiv:2509.03626v1 Announce Type: new 
Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive progress but still produces hallucinations and unverifiable claims, limiting reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves accuracy by grounding outputs in external knowledge, especially in domains like healthcare, where precision is vital. However, RAG remains opaque and essentially a black box, heavily dependent on data quality. We developed a method-agnostic, perturbation-based framework that provides token and component-level interoperability for Graph RAG using SMILE and named it as Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing similarities, and training weighted linear surrogates, KG-SMILE identifies the graph entities and relations most influential to generated outputs, thereby making RAG more transparent. We evaluate KG-SMILE using comprehensive attribution metrics, including fidelity, faithfulness, consistency, stability, and accuracy. Our findings show that KG-SMILE produces stable, human-aligned explanations, demonstrating its capacity to balance model effectiveness with interpretability and thereby fostering greater transparency and trust in machine learning technologies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalARC: Abstract Reasoning with Causal World Models</title>
<link>https://arxiv.org/abs/2509.03636</link>
<guid>https://arxiv.org/abs/2509.03636</guid>
<content:encoded><![CDATA[
<div> Keywords: Reasoning, CausalARC, low-data, out-of-distribution, structural causal model

Summary:
CausalARC is introduced as a testbed for AI reasoning in challenging scenarios with limited data and distribution shifts. The testbed is based on a fully specified causal world model expressed as a structural causal model. Data augmentations provide various feedback types such as observational, interventional, and counterfactual in few-shot learning demonstrations. The use cases of CausalARC in language model evaluation settings include abstract reasoning with test-time training, counterfactual reasoning with in-context learning, program synthesis, and causal discovery with logical reasoning. This experimental setup aims to facilitate AI reasoning in complex and unfamiliar problem settings, showcasing the adaptability and generalization capabilities of AI systems. <br /><br />Summary: CausalARC offers a platform for AI reasoning in challenging environments by leveraging structural causal models and providing diverse feedback types for learning. Its applications span across various language model evaluation tasks, demonstrating the potential for AI systems to reason effectively in low-data and out-of-distribution scenarios. <div>
arXiv:2509.03636v1 Announce Type: new 
Abstract: Reasoning requires adaptation to novel problem settings under limited data and distribution shift. This work introduces CausalARC: an experimental testbed for AI reasoning in low-data and out-of-distribution regimes, modeled after the Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is sampled from a fully specified causal world model, formally expressed as a structural causal model. Principled data augmentations provide observational, interventional, and counterfactual feedback about the world model in the form of few-shot, in-context learning demonstrations. As a proof-of-concept, we illustrate the use of CausalARC for four language model evaluation settings: (1) abstract reasoning with test-time training, (2) counterfactual reasoning with in-context learning, (3) program synthesis, and (4) causal discovery with logical reasoning.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations</title>
<link>https://arxiv.org/abs/2509.03644</link>
<guid>https://arxiv.org/abs/2509.03644</guid>
<content:encoded><![CDATA[
<div> neurosymbolic system, Embodied-LM, logical reasoning, image schemas, spatial reasoning, Answer Set Programming

Summary:
Embodied-LM is a prototype neurosymbolic system that integrates image schemas, derived from sensorimotor experience, into large language models (LLMs) to improve logical reasoning capabilities. By operationalizing spatial reasoning using Answer Set Programming, the system enhances the interpretability and logical reasoning of LLMs on deduction problems. The system's use of cognitive structures grounded in sensory experience guides LLMs to interpret scenarios and formalize them as executable programs, leading to more effective logical reasoning. While currently focused on spatial primitives, the system establishes a computational foundation for incorporating more complex and dynamic representations in the future. <div>
arXiv:2509.03644v1 Announce Type: new 
Abstract: Despite significant progress in natural language understanding, Large Language Models (LLMs) remain error-prone when performing logical reasoning, often lacking the robust mental representations that enable human-like comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that grounds understanding and logical reasoning in schematic representations based on image schemas-recurring patterns derived from sensorimotor experience that structure human cognition. Our system operationalizes the spatial foundations of these cognitive structures using declarative spatial reasoning within Answer Set Programming. Through evaluation on logical deduction problems, we demonstrate that LLMs can be guided to interpret scenarios through embodied cognitive structures, that these structures can be formalized as executable programs, and that the resulting representations support effective logical reasoning with enhanced interpretability. While our current implementation focuses on spatial primitives, it establishes the computational foundation for incorporating more complex and dynamic representations.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.03646</link>
<guid>https://arxiv.org/abs/2509.03646</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, reasoning hierarchy, strategic planning, entropy dynamics

Summary:
Reinforcement Learning has been successful in enhancing the reasoning abilities of Large Language Models (LLMs), with phenomena like "aha moments" and entropy dynamics being part of an emergent reasoning hierarchy. The study uncovers a two-phase dynamic where models initially focus on procedural correctness and low-level skills before transitioning to high-level strategic planning. The analysis highlights the inefficiencies in current RL algorithms like GRPO, which do not focus optimization efforts on high-impact planning tokens. The proposed HICRA algorithm shows significant performance improvements by concentrating on the strategic bottleneck. Semantic entropy is identified as a better measure of strategic exploration compared to token-level entropy. The research emphasizes the importance of focusing on high-level planning to enhance advanced reasoning capabilities in LLMs.<br /><br />Summary: <div>
arXiv:2509.03646v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification</title>
<link>https://arxiv.org/abs/2509.03649</link>
<guid>https://arxiv.org/abs/2509.03649</guid>
<content:encoded><![CDATA[
<div> Segmentation algorithms, SHAP method, explainable AI, time series classification, attribution quality<br />
Summary:
- Explaining the predictions of complex Time Series Classification models is crucial for understanding them.
- SHapley Additive exPlanations (SHAP) is a popular attribution method, but its computational complexity limits its practicality for long time series.
- Aggregating features via segmentation has been shown to reduce SHAP running time significantly.
- The choice of segmentation strategy greatly impacts the explanation quality, but the number of segments plays a more significant role than the specific method.
- Equal-length segmentation consistently outperforms many custom time series segmentation algorithms.
- A novel attribution normalization technique that weights segments by their length has been introduced and shown to consistently improve attribution quality.<br /><br />Summary: <div>
arXiv:2509.03649v1 Announce Type: new 
Abstract: Explainable AI (XAI) has become an increasingly important topic for understanding and attributing the predictions made by complex Time Series Classification (TSC) models. Among attribution methods, SHapley Additive exPlanations (SHAP) is widely regarded as an excellent attribution method; but its computational complexity, which scales exponentially with the number of features, limits its practicality for long time series. To address this, recent studies have shown that aggregating features via segmentation, to compute a single attribution value for a group of consecutive time points, drastically reduces SHAP running time. However, the choice of the optimal segmentation strategy remains an open question. In this work, we investigated eight different Time Series Segmentation algorithms to understand how segment compositions affect the explanation quality. We evaluate these approaches using two established XAI evaluation methodologies: InterpretTime and AUC Difference. Through experiments on both Multivariate (MTS) and Univariate Time Series (UTS), we find that the number of segments has a greater impact on explanation quality than the specific segmentation method. Notably, equal-length segmentation consistently outperforms most of the custom time series segmentation algorithms. Furthermore, we introduce a novel attribution normalisation technique that weights segments by their length and we show that it consistently improves attribution quality.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming</title>
<link>https://arxiv.org/abs/2509.03728</link>
<guid>https://arxiv.org/abs/2509.03728</guid>
<content:encoded><![CDATA[
<div> persona, red-teaming, AI governance, safety research, adversarial prompts <br />
Summary: <br />
The paper introduces PersonaTeaming, a method for incorporating personas in automated red-teaming to identify potential risks posed by AI models. By utilizing "red-teaming expert" and "regular AI user" personas in prompt generation, the method explores a wider range of adversarial strategies. A dynamic algorithm is developed to generate various persona types adapted to different seed prompts, leading to improved attack success rates (up to 144.1%) while maintaining prompt diversity compared to RainbowPlus. Metrics to measure "mutation distance" are introduced to assess prompt diversity. The study highlights the potential of persona mutation in enhancing automated red-teaming approaches and suggests opportunities for exploring the synergy between automated and human red-teaming strategies. <br /> <div>
arXiv:2509.03728v1 Announce Type: new 
Abstract: Recent developments in AI governance and safety research have called for red-teaming methods that can effectively surface potential risks posed by AI models. Many of these calls have emphasized how the identities and backgrounds of red-teamers can shape their red-teaming strategies, and thus the kinds of risks they are likely to uncover. While automated red-teaming approaches promise to complement human red-teaming by enabling larger-scale exploration of model behavior, current approaches do not consider the role of identity. As an initial step towards incorporating people's background and identities in automated red-teaming, we develop and evaluate a novel method, PersonaTeaming, that introduces personas in the adversarial prompt generation process to explore a wider spectrum of adversarial strategies. In particular, we first introduce a methodology for mutating prompts based on either "red-teaming expert" personas or "regular AI user" personas. We then develop a dynamic persona-generating algorithm that automatically generates various persona types adaptive to different seed prompts. In addition, we develop a set of new metrics to explicitly measure the "mutation distance" to complement existing diversity measurements of adversarial prompts. Our experiments show promising improvements (up to 144.1%) in the attack success rates of adversarial prompts through persona mutation, while maintaining prompt diversity, compared to RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the strengths and limitations of different persona types and mutation methods, shedding light on future opportunities to explore complementarities between automated and human red-teaming approaches.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Personality Illusion: Revealing Dissociation Between Self-Reports &amp; Behavior in LLMs</title>
<link>https://arxiv.org/abs/2509.03730</link>
<guid>https://arxiv.org/abs/2509.03730</guid>
<content:encoded><![CDATA[
<div> personality traits, Large Language Models, self-reported traits, behavioral validation, targeted interventions 
Summary: 
The study explores how Large Language Models (LLMs) exhibit personality traits similar to humans, like agreeableness and self-regulation. The research focuses on three dimensions: the evolution of trait profiles during training, the relationship between self-reported traits and behavior, and the impact of interventions on trait expression and behavior. Results show that instructional alignment enhances trait stability and correlations, akin to human data. However, self-reported traits do not consistently predict behavior, highlighting a disconnect between trait expression and behavior. Persona injection influences self-reports but lacks a significant impact on actual behavior. The findings challenge assumptions about LLM personality and highlight the necessity for further evaluation on alignment and interpretability. <br /><br />Summary: <div>
arXiv:2509.03730v1 Announce Type: new 
Abstract: Personality traits have long been studied as predictors of human behavior.Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation</title>
<link>https://arxiv.org/abs/2509.03736</link>
<guid>https://arxiv.org/abs/2509.03736</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, human-subject research, dialogue, internal consistency, simulation

Summary:
<br /><br />
The study evaluates whether synthetic agents generated by Large Language Models (LLMs) can be reliable substitutes for human participants in research. While previous research has focused on the correspondence between LLM-generated survey data and human responses, this study examines the internal consistency of LLMs in different experimental settings. By revealing the agent's internal state and assessing its behavior in dialogue situations, the study reveals significant inconsistencies in LLMs across model families and sizes. Despite generating responses similar to humans, the agents lack internal consistency, highlighting a crucial limitation in their ability to accurately represent real participants in research. The publicly accessible simulation code and data provide valuable insights into the challenges of using LLMs in social science research. <div>
arXiv:2509.03736v1 Announce Type: new 
Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the notion that synthetic agents can serve as substitutes for real participants in human-subject research. In an effort to evaluate the merits of this claim, social science researchers have largely focused on whether LLM-generated survey data corresponds to that of a human counterpart whom the LLM is prompted to represent. In contrast, we address a more fundamental question: Do agents maintain internal consistency, retaining similar behaviors when examined under different experimental settings? To this end, we develop a study designed to (a) reveal the agent's internal state and (b) examine agent behavior in a basic dialogue setting. This design enables us to explore a set of behavioral hypotheses to assess whether an agent's conversation behavior is consistent with what we would expect from their revealed internal state. Our findings on these hypotheses show significant internal inconsistencies in LLMs across model families and at differing model sizes. Most importantly, we find that, although agents may generate responses matching those of their human counterparts, they fail to be internally consistent, representing a critical gap in their capabilities to accurately substitute for real participants in human-subject research. Our simulation code and data are publicly accessible.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs</title>
<link>https://arxiv.org/abs/2509.03768</link>
<guid>https://arxiv.org/abs/2509.03768</guid>
<content:encoded><![CDATA[
<div> Keywords: Offshore Wind, Maintenance, Safety, Large Language Models, RAGuard <br />
Summary:<br />
The study introduces RAGuard, an advanced framework to improve accuracy and safety in Offshore Wind (OSW) maintenance. By combining safety-critical documents with technical manuals, RAGuard enhances retrieval performance. The framework issues parallel queries to two indices, allocating separate budgets for knowledge and safety to ensure technical depth and safety coverage. A SafetyClamp extension further enhances safety by increasing Safety Recall while maintaining Technical Recall. Evaluation across different retrieval paradigms shows significant improvements in Safety Recall with RAGuard. These results indicate the potential of RAGuard and SafetyClamp in establishing a new standard for integrating safety assurance into decision support systems using Large Language Models in critical maintenance situations.<br /> 
Summary: <div>
arXiv:2509.03768v1 Announce Type: new 
Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet conventional Large Language Models (LLMs) often fail when confronted with highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced Retrieval-Augmented Generation (RAG) framework that explicitly integrates safety-critical documents alongside technical manuals.By issuing parallel queries to two indices and allocating separate retrieval budgets for knowledge and safety, RAGuard guarantees both technical depth and safety coverage. We further develop a SafetyClamp extension that fetches a larger candidate pool, "hard-clamping" exact slot guarantees to safety. We evaluate across sparse (BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms, measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than 50\% in RAGuard, while maintaining Technical Recall above 60\%. These results demonstrate that RAGuard and SafetyClamp have the potential to establish a new standard for integrating safety assurance into LLM-powered decision support in critical maintenance contexts.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLM-Based Agents for Intelligent Supply Chain Planning</title>
<link>https://arxiv.org/abs/2509.03811</link>
<guid>https://arxiv.org/abs/2509.03811</guid>
<content:encoded><![CDATA[
<div> Keywords: supply chain management, AI technologies, large language models, e-commerce platform, JD.com

Summary: 
Supply chain management is a critical concept involving various entities in the movement of physical products. The development of AI technologies, particularly large language models, has opened new possibilities in addressing supply chain planning challenges. This work introduces the Supply Chain Planning Agent (SCPA) framework, which utilizes AI technology to understand domain knowledge, tasks decomposition, and evidence-based planning reports generation. The SCPA framework was successfully deployed in a real-world scenario at JD.com, showcasing improved efficiency, accuracy, and stock availability. By leveraging AI tools, the SCPA framework effectively reduced labor while enhancing key metrics in supply chain operations. This demonstrates the practical applications of AI technologies in optimizing supply chain planning processes. <div>
arXiv:2509.03811v1 Announce Type: new 
Abstract: In supply chain management, planning is a critical concept. The movement of physical products across different categories, from suppliers to warehouse management, to sales, and logistics transporting them to customers, entails the involvement of many entities. It covers various aspects such as demand forecasting, inventory management, sales operations, and replenishment. How to collect relevant data from an e-commerce platform's perspective, formulate long-term plans, and dynamically adjust them based on environmental changes, while ensuring interpretability, efficiency, and reliability, is a practical and challenging problem. In recent years, the development of AI technologies, especially the rapid progress of large language models, has provided new tools to address real-world issues. In this work, we construct a Supply Chain Planning Agent (SCPA) framework that can understand domain knowledge, comprehend the operator's needs, decompose tasks, leverage or create new tools, and return evidence-based planning reports. We deploy this framework in JD.com's real-world scenario, demonstrating the feasibility of LLM-agent applications in the supply chain. It effectively reduced labor and improved accuracy, stock availability, and other key metrics.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.03817</link>
<guid>https://arxiv.org/abs/2509.03817</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, multi-agent systems, meta-cognitive actions, reinforcement learning, dynamic strategies<br />
Summary: <br />
- The Meta-Policy Deliberation Framework (MPDF) introduces decentralized policies for agents in large language models, focusing on meta-cognitive actions like Persist, Refine, and Concede. 
- SoftRankPO, a novel reinforcement learning algorithm, stabilizes training by shaping advantages based on smooth normal quantiles, enhancing adaptability in multi-agent systems.
- Traditional policy gradients are replaced with SoftRankPO to handle reward variance and improve overall performance in reasoning benchmarks.
- MPDF with SoftRankPO demonstrates a 4-5% absolute gain in average accuracy across mathematical and general reasoning tasks compared to existing algorithms.
- This work highlights the importance of adaptive, meta-cognitive strategies in multi-agent systems, emphasizing the shift from fixed protocols to dynamic deliberative approaches. <br /> 
Summary: <div>
arXiv:2509.03817v1 Announce Type: new 
Abstract: Multi-agent systems of large language models (LLMs) show promise for complex reasoning, but their effectiveness is often limited by fixed collaboration protocols. These frameworks typically focus on macro-level orchestration while overlooking agents' internal deliberative capabilities. This critical meta-cognitive blindspot treats agents as passive executors unable to adapt their strategy based on internal cognitive states like uncertainty or confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where agents learn a decentralized policy over a set of high-level meta-cognitive actions: Persist, Refine, and Concede. To overcome the instability of traditional policy gradients in this setting, we develop SoftRankPO, a novel reinforcement learning algorithm. SoftRankPO stabilizes training by shaping advantages based on the rank of rewards mapped through smooth normal quantiles, making the learning process robust to reward variance. Experiments show that MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms. Our work presents a paradigm for learning adaptive, meta-cognitive policies for multi-agent LLM systems, shifting the focus from designing fixed protocols to learning dynamic, deliberative strategies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2509.03827</link>
<guid>https://arxiv.org/abs/2509.03827</guid>
<content:encoded><![CDATA[
<div> benchmark, decision scenarios, social policymaking, homelessness alleviation, Capability Approach

Summary:<br />
- The article evaluates the alignment of Large Language Models (LLMs) with domain experts in social policymaking for homelessness alleviation.
- A novel benchmark is created with decision scenarios and policy choices across four geographies.
- Policies are based on the Capability Approach for human development.
- An automated pipeline connects benchmarked policies to an agent-based model for analysis.
- The study shows potential for LLMs to provide valuable insights for social policy making with responsible guidance from local experts. 

Summary: <div>
arXiv:2509.03827v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes domains. Their capacity to process vast amounts of unstructured data, explore flexible scenarios, and handle a diversity of contextual factors can make them uniquely suited to provide new insights for the complexity of social policymaking. This article evaluates whether LLMs' are aligned with domain experts (and among themselves) to inform social policymaking on the subject of homelessness alleviation - a challenge affecting over 150 million people worldwide. We develop a novel benchmark comprised of decision scenarios with policy choices across four geographies (South Bend, USA; Barcelona, Spain; Johannesburg, South Africa; Macau SAR, China). The policies in scope are grounded in the conceptual framework of the Capability Approach for human development. We also present an automated pipeline that connects the benchmarked policies to an agent-based model, and we explore the social impact of the recommended policies through simulated social scenarios. The paper results reveal promising potential to leverage LLMs for social policy making. If responsible guardrails and contextual calibrations are introduced in collaboration with local domain experts, LLMs can provide humans with valuable insights, in the form of alternative policies at scale.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic Model Context Protocol Framework for Medical Concept Standardization</title>
<link>https://arxiv.org/abs/2509.03828</link>
<guid>https://arxiv.org/abs/2509.03828</guid>
<content:encoded><![CDATA[
<div> Observational Medical Outcomes Partnership, OMOP common data model, mapping, large language models, Model Context Protocol <br />
Summary: The article introduces a new system for mapping source medical terms to OMOP standard concepts using the Model Context Protocol (MCP). This system aims to prevent hallucination in large language models (LLMs) and improve efficiency and accuracy in the mapping process. By allowing LLMs to interact with external resources and tools, the system provides real-time vocabulary lookups and structured reasoning outputs. This approach enables explainable mapping and enhances both exploratory and production environments for healthcare research. Overall, the system offers a zero-training solution to improve the standardization of health data using the OMOP CDM, reducing errors and resource-intensive processes in medical term mapping. <br /><br /> <div>
arXiv:2509.03828v1 Announce Type: new 
Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM) provides a standardized representation of heterogeneous health data to support large-scale, multi-institutional research. One critical step in data standardization using OMOP CDM is the mapping of source medical terms to OMOP standard concepts, a procedure that is resource-intensive and error-prone. While large language models (LLMs) have the potential to facilitate this process, their tendency toward hallucination makes them unsuitable for clinical deployment without training and expert validation. Here, we developed a zero-training, hallucination-preventive mapping system based on the Model Context Protocol (MCP), a standardized and secure framework allowing LLMs to interact with external resources and tools. The system enables explainable mapping and significantly improves efficiency and accuracy with minimal effort. It provides real-time vocabulary lookups and structured reasoning outputs suitable for immediate use in both exploratory and production environments.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai</title>
<link>https://arxiv.org/abs/2509.03830</link>
<guid>https://arxiv.org/abs/2509.03830</guid>
<content:encoded><![CDATA[
<div> Keywords: historic urban quarters, tourism perception, AI-powered framework, multimodal data, social media

Summary:
This study introduces an AI-powered framework to analyze tourist perception in historic urban quarters using data from social media. The framework incorporates focal point extraction, color theme analysis, and sentiment mining to understand how tourists perceive these environments. By analyzing tourist-shared photos, visual focus areas and dominant colors are identified to assess aesthetic preferences. Color themes are compared between social media photos and real-world street views, revealing potential gaps in visual expectations. Tourist reviews are analyzed using a hybrid sentiment analysis approach to evaluate satisfaction across different dimensions. The results show spatial variations in aesthetic appeal and emotional response, providing insights for sustainable urban planning and heritage conservation. The integrated approach presented in this study offers a comprehensive way to decode tourist perception and inform decision-making for the design of engaging public spaces. 

<br /><br />Summary: <div>
arXiv:2509.03830v1 Announce Type: new 
Abstract: Historic urban quarters play a vital role in preserving cultural heritage while serving as vibrant spaces for tourism and everyday life. Understanding how tourists perceive these environments is essential for sustainable, human-centered urban planning. This study proposes a multidimensional AI-powered framework for analyzing tourist perception in historic urban quarters using multimodal data from social media. Applied to twelve historic quarters in central Shanghai, the framework integrates focal point extraction, color theme analysis, and sentiment mining. Visual focus areas are identified from tourist-shared photos using a fine-tuned semantic segmentation model. To assess aesthetic preferences, dominant colors are extracted using a clustering method, and their spatial distribution across quarters is analyzed. Color themes are further compared between social media photos and real-world street views, revealing notable shifts. This divergence highlights potential gaps between visual expectations and the built environment, reflecting both stylistic preferences and perceptual bias. Tourist reviews are evaluated through a hybrid sentiment analysis approach combining a rule-based method and a multi-task BERT model. Satisfaction is assessed across four dimensions: tourist activities, built environment, service facilities, and business formats. The results reveal spatial variations in aesthetic appeal and emotional response. Rather than focusing on a single technical innovation, this framework offers an integrated, data-driven approach to decoding tourist perception and contributes to informed decision-making in tourism, heritage conservation, and the design of aesthetically engaging public spaces.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures</title>
<link>https://arxiv.org/abs/2509.03857</link>
<guid>https://arxiv.org/abs/2509.03857</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Knowledge Graphs, Reliability evaluation, Large Language Models, Real-time monitoring

Summary: 
This article introduces a systematic methodology for evaluating the reliability of Generative AI (GEN AI) models, which are prone to issues such as hallucinations, semantic drift, and biases. The proposed methodology utilizes deterministic and Large Language Model (LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate the performance of GEN AI models. By constructing parallel KGs using rule-based methods and real-time textual data streams, structural deviations and semantic discrepancies can be quantified using established KG metrics. An automated real-time monitoring framework computes deviations between deterministic and LLM-generated KGs and proactively identifies significant deviations, flagging semantic anomalies or hallucinations. This metric-driven comparison between deterministic and dynamically generated KGs provides a robust and scalable evaluation framework for assessing the reliability of GEN AI models.<br /><br />Summary: <div>
arXiv:2509.03857v1 Announce Type: new 
Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains but present substantial challenges due to reliability concerns, including hallucinations, semantic drift, and inherent biases. These models typically operate as black-boxes, complicating transparent and objective evaluation. Current evaluation methods primarily depend on subjective human assessment, limiting scalability, transparency, and effectiveness. This research proposes a systematic methodology using deterministic and Large Language Model (LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN AI reliability. We construct two parallel KGs: (i) a deterministic KG built using explicit rule-based methods, predefined ontologies, domain-specific dictionaries, and structured entity-relation extraction rules, and (ii) an LLM-generated KG dynamically derived from real-time textual data streams such as live news articles. Utilizing real-time news streams ensures authenticity, mitigates biases from repetitive training, and prevents adaptive LLMs from bypassing predefined benchmarks through feedback memorization. To quantify structural deviations and semantic discrepancies, we employ several established KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring framework continuously computes deviations between deterministic and LLM-generated KGs. By establishing dynamic anomaly thresholds based on historical structural metric distributions, our method proactively identifies and flags significant deviations, thus promptly detecting semantic anomalies or hallucinations. This structured, metric-driven comparison between deterministic and dynamically generated KGs delivers a robust and scalable evaluation framework.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expedition &amp; Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata</title>
<link>https://arxiv.org/abs/2509.03863</link>
<guid>https://arxiv.org/abs/2509.03863</guid>
<content:encoded><![CDATA[
<div> Keywords: visual patterns, cellular automata, exploration, novelty search, artificial life
Summary: 
Expedition and Expansion (E&amp;E) is a new hybrid exploration strategy for discovering diverse visual patterns in continuous cellular automata (CA). E&amp;E combines local novelty-driven expansions with goal-directed expeditions using a Vision-Language Model (VLM) to generate linguistic goals. By operating in semantic spaces aligning with human perception, E&amp;E enhances the interpretability and relevance of discovered behaviors. Tested on Flow Lenia, E&amp;E consistently uncovers more diverse solutions than existing methods. A genealogical analysis reveals that expedition-originating solutions play a significant role in long-term exploration, unlocking new behavioral niches. E&amp;E breaks through local novelty boundaries and explores behavioral landscapes in human-aligned, interpretable ways, providing a promising framework for open-ended exploration in artificial life and beyond.<br /><br />Summary: <div>
arXiv:2509.03863v1 Announce Type: new 
Abstract: Discovering diverse visual patterns in continuous cellular automata (CA) is challenging due to the vastness and redundancy of high-dimensional behavioral spaces. Traditional exploration methods like Novelty Search (NS) expand locally by mutating known novel solutions but often plateau when local novelty is exhausted, failing to reach distant, unexplored regions. We introduce Expedition and Expansion (E&amp;E), a hybrid strategy where exploration alternates between local novelty-driven expansions and goal-directed expeditions. During expeditions, E&amp;E leverages a Vision-Language Model (VLM) to generate linguistic goals--descriptions of interesting but hypothetical patterns that drive exploration toward uncharted regions. By operating in semantic spaces that align with human perception, E&amp;E both evaluates novelty and generates goals in conceptually meaningful ways, enhancing the interpretability and relevance of discovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich, emergent behaviors, E&amp;E consistently uncovers more diverse solutions than existing exploration methods. A genealogical analysis further reveals that solutions originating from expeditions disproportionately influence long-term exploration, unlocking new behavioral niches that serve as stepping stones for subsequent search. These findings highlight E&amp;E's capacity to break through local novelty boundaries and explore behavioral landscapes in human-aligned, interpretable ways, offering a promising template for open-ended exploration in artificial life and beyond.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace</title>
<link>https://arxiv.org/abs/2509.03890</link>
<guid>https://arxiv.org/abs/2509.03890</guid>
<content:encoded><![CDATA[
<div> marketplace, agentic assistant, LLM-powered, conversational search, task automation
Summary: 
The paper introduces a novel approach to simplify interactions on C2C e-commerce platforms using an agentic assistant powered by Large Language Models (LLMs). This assistant shifts the primary interaction model from complex GUIs to an intuitive AI agent that interprets natural language commands. For sellers, the assistant automates tasks like listing updates and bulk messaging, while facilitating a more efficient product discovery process for buyers through conversational search. The architecture for the Facebook Marketplace Assistant (FaMA) is presented, highlighting the benefits of this conversational paradigm as a more accessible and efficient alternative to traditional app interfaces. Experimental results demonstrate FaMA's high task success rate of 98% and up to a 2x speedup in interaction time.<br /><br />Summary: <div>
arXiv:2509.03890v1 Announce Type: new 
Abstract: The emergence of agentic AI, powered by Large Language Models (LLMs), marks a paradigm shift from reactive generative systems to proactive, goal-oriented autonomous agents capable of sophisticated planning, memory, and tool use. This evolution presents a novel opportunity to address long-standing challenges in complex digital environments. Core tasks on Consumer-to-Consumer (C2C) e-commerce platforms often require users to navigate complex Graphical User Interfaces (GUIs), making the experience time-consuming for both buyers and sellers. This paper introduces a novel approach to simplify these interactions through an LLM-powered agentic assistant. This agent functions as a new, conversational entry point to the marketplace, shifting the primary interaction model from a complex GUI to an intuitive AI agent. By interpreting natural language commands, the agent automates key high-friction workflows. For sellers, this includes simplified updating and renewal of listings, and the ability to send bulk messages. For buyers, the agent facilitates a more efficient product discovery process through conversational search. We present the architecture for Facebook Marketplace Assistant (FaMA), arguing that this agentic, conversational paradigm provides a lightweight and more accessible alternative to traditional app interfaces, allowing users to manage their marketplace activities with greater efficiency. Experiments show FaMA achieves a 98% task success rate on solving complex tasks on the marketplace and enables up to a 2x speedup on interaction time.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.03906</link>
<guid>https://arxiv.org/abs/2509.03906</guid>
<content:encoded><![CDATA[
<div> fine-tuned, synthetic reasoning, reinforcement learning, report generation, visual question answering<br />
<br />
Summary:<br />
DeepMedix-R1 is a comprehensive medical foundation model for chest X-ray interpretation that focuses on improving transparency and interpretability. It undergoes a sequential training process, starting with fine-tuning on CXR instruction data, followed by exposure to synthetic reasoning samples for cold-start reasoning, and refinement through online reinforcement learning. The model produces both answers and reasoning steps tied to local image regions, leading to significant improvements in report generation and visual question answering tasks compared to existing models. A benchmarking framework called Report Arena is proposed for robust evaluation, showcasing the model's answer quality superiority. Expert reviews confirm that DeepMedix-R1's generated reasoning steps offer greater interpretability and clinical plausibility than a well-known model. This work advances the development of medical foundation models towards holistic, transparent, and clinically actionable approaches for CXR interpretation. <div>
arXiv:2509.03906v1 Announce Type: new 
Abstract: Medical foundation models (FMs) have shown tremendous promise amid the rapid advancements in artificial intelligence (AI) technologies. However, current medical FMs typically generate answers in a black-box manner, lacking transparent reasoning processes and locally grounded interpretability, which hinders their practical clinical deployments. To this end, we introduce DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It leverages a sequential training pipeline: initially fine-tuned on curated CXR instruction data to equip with fundamental CXR interpretation capabilities, then exposed to high-quality synthetic reasoning samples to enable cold-start reasoning, and finally refined via online reinforcement learning to enhance both grounded reasoning quality and generation performance. Thus, the model produces both an answer and reasoning steps tied to the image's local regions for each query. Quantitative evaluation demonstrates substantial improvements in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent) tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking framework using advanced language models to evaluate answer quality, further highlighting the superiority of DeepMedix-R1. Expert review of generated reasoning steps reveals greater interpretability and clinical plausibility compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall preference). Collectively, our work advances medical FM development toward holistic, transparent, and clinically actionable modeling for CXR interpretation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Infinite Domain Parameters in Planning Through Best-First Search with Delayed Partial Expansions</title>
<link>https://arxiv.org/abs/2509.03953</link>
<guid>https://arxiv.org/abs/2509.03953</guid>
<content:encoded><![CDATA[
<div> Control parameters, automated planning, decision variables, search algorithm, completeness
<br />
The paper introduces a novel approach in automated planning that explicitly treats control parameters as decision points in the search space. Existing methods have often treated control parameters as constraints rather than decision points. The proposed best-first, heuristic search algorithm operates over infinite decision spaces and utilizes delayed partial expansion to incrementally explore successors. This approach proves completeness under certain conditions and offers a competitive alternative for planning problems involving control parameters. The algorithm's ability to handle control parameters as true decision points sets it apart from existing methods, potentially leading to more efficient and effective planning solutions.
<br /><br />Summary: <div>
arXiv:2509.03953v1 Announce Type: new 
Abstract: In automated planning, control parameters extend standard action representations through the introduction of continuous numeric decision variables. Existing state-of-the-art approaches have primarily handled control parameters as embedded constraints alongside other temporal and numeric restrictions, and thus have implicitly treated them as additional constraints rather than as decision points in the search space. In this paper, we propose an efficient alternative that explicitly handles control parameters as true decision points within a systematic search scheme. We develop a best-first, heuristic search algorithm that operates over infinite decision spaces defined by control parameters and prove a notion of completeness in the limit under certain conditions. Our algorithm leverages the concept of delayed partial expansion, where a state is not fully expanded but instead incrementally expands a subset of its successors. Our results demonstrate that this novel search algorithm is a competitive alternative to existing approaches for solving planning problems involving control parameters.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World Model Implanting for Test-time Adaptation of Embodied Agents</title>
<link>https://arxiv.org/abs/2509.03956</link>
<guid>https://arxiv.org/abs/2509.03956</guid>
<content:encoded><![CDATA[
<div> world model implanting framework, embodied AI, adaptation, domain-specific knowledge, robust

Summary: 
The article introduces the world model implanting framework (WorMI) for embodied AI, aiming to enable agents to adapt to new domains without extensive retraining. WorMI combines large language models (LLMs) with independently learned world models through test-time composition. It uses a prototype-based retrieval approach for world models and a compound attention method to integrate knowledge and align representations. The framework allows seamless implantation and removal of world models, enhancing adaptability. Evaluation on VirtualHome and ALFWorld benchmarks shows superior zero-shot and few-shot performance compared to LLM-based approaches, indicating potential for real-world deployment in scenarios requiring adaptability and data efficiency. <div>
arXiv:2509.03956v1 Announce Type: new 
Abstract: In embodied AI, a persistent challenge is enabling agents to robustly adapt to novel domains without requiring extensive data collection or retraining. To address this, we present a world model implanting framework (WorMI) that combines the reasoning capabilities of large language models (LLMs) with independently learned, domain-specific world models through test-time composition. By allowing seamless implantation and removal of the world models, the embodied agent's policy achieves and maintains cross-domain adaptability. In the WorMI framework, we employ a prototype-based world model retrieval approach, utilizing efficient trajectory-based abstract representation matching, to incorporate relevant models into test-time composition. We also develop a world-wise compound attention method that not only integrates the knowledge from the retrieved world models but also aligns their intermediate representations with the reasoning model's representation within the agent's policy. This framework design effectively fuses domain-specific knowledge from multiple world models, ensuring robust adaptation to unseen domains. We evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating superior zero-shot and few-shot performance compared to several LLM-based approaches across a range of unseen domains. These results highlight the frameworks potential for scalable, real-world deployment in embodied agent scenarios where adaptability and data efficiency are essential.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent</title>
<link>https://arxiv.org/abs/2509.03990</link>
<guid>https://arxiv.org/abs/2509.03990</guid>
<content:encoded><![CDATA[
<div> Reinforcement-learning, Large language model, Transferable policies, Meta-Policy Memory, Robustness<br />
Summary:<br />
The article introduces Meta-Policy Reflexion (MPR), a framework that consolidates reflections from Large Language Model (LLM) agents into a Meta-Policy Memory (MPM) to improve performance. MPR utilizes soft memory-guided decoding and hard rule admissibility checks to enhance adaptability without requiring parameter updates. The framework externalizes reusable corrective knowledge and enforces domain constraints to reduce unsafe actions. Empirical results in a text-based agent environment show improved execution accuracy and robustness compared to existing Reflexion approaches. Rule admissibility further enhances stability. The study analyzes the mechanisms behind these results, discusses scalability and potential failure modes, and suggests future directions for extensions in multimodal and multi-agent scenarios. This innovative approach addresses common challenges faced by LLM agents and offers a promising solution for improving cross-task adaptability and overall performance. <br /><br />Summary: <div>
arXiv:2509.03990v1 Announce Type: new 
Abstract: Large language model (LLM) agents achieve impressive single-task performance but commonly exhibit repeated failures, inefficient exploration, and limited cross-task adaptability. Existing reflective strategies (e.g., Reflexion, ReAct) improve per-episode behavior but typically produce ephemeral, task-specific traces that are not reused across tasks. Reinforcement-learning based alternatives can produce transferable policies but require substantial parameter updates and compute. In this work we introduce Meta-Policy Reflexion (MPR): a hybrid framework that consolidates LLM-generated reflections into a structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at inference time through two complementary mechanisms soft memory-guided decoding and hard rule admissibility checks(HAC). MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection. We formalize the MPM representation, present algorithms for update and decoding, and validate the approach in a text-based agent environment following the experimental protocol described in the provided implementation (AlfWorld-based). Empirical results reported in the supplied material indicate consistent gains in execution accuracy and robustness when compared to Reflexion baselines; rule admissibility further improves stability. We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi?agent extensions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPBO: LLM-powered Optimization for Local Search PBO Solvers</title>
<link>https://arxiv.org/abs/2509.04007</link>
<guid>https://arxiv.org/abs/2509.04007</guid>
<content:encoded><![CDATA[
<div> Keywords: Pseudo-Boolean Optimization, Local search solvers, Large Language Models, AutoPBO, Performance improvement

Summary:<br />
Pseudo-Boolean Optimization (PBO) is a powerful framework for modeling combinatorial problems using PB constraints. Local search solvers, crucial for PBO solving, rely heavily on internal heuristics for efficient search, often requiring manual tuning. This study introduces AutoPBO, an innovative framework utilizing Large Language Models (LLMs) to automatically enhance PBO local search solvers. Experimentation on various benchmarks demonstrates AutoPBO's significant performance improvements over previous local search methods and competitive performance against state-of-the-art competitors NuPBO, OraSLS, PBO-IHS, RoundingSat, Gurobi, and SCIP. The results highlight AutoPBO's potential to automate local search solver design, illustrating promise in advancing optimization techniques. <br />Summary: <div>
arXiv:2509.04007v1 Announce Type: new 
Abstract: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling combinatorial problems through pseudo-Boolean (PB) constraints. Local search solvers have shown excellent performance in PBO solving, and their efficiency is highly dependent on their internal heuristics to guide the search. Still, their design often requires significant expert effort and manual tuning in practice. While Large Language Models (LLMs) have demonstrated potential in automating algorithm design, their application to optimizing PBO solvers remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered framework to automatically enhance PBO local search solvers. We conduct experiments on a broad range of four public benchmarks, including one real-world benchmark, a benchmark from PB competition, an integer linear programming optimization benchmark, and a crafted combinatorial benchmark, to evaluate the performance improvement achieved by AutoPBO and compare it with six state-of-the-art competitors, including two local search PBO solvers NuPBO and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates significant improvements over previous local search approaches, while maintaining competitive performance compared to state-of-the-art competitors. The results suggest that AutoPBO offers a promising approach to automating local search solver design.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.04027</link>
<guid>https://arxiv.org/abs/2509.04027</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, Chain-of-Thought, CoT-Space, reasoning-level semantic space

Summary: 
Reinforcement Learning (RL) is crucial for improving the reasoning capabilities of Large Language Models (LLMs). Traditional token-level RL frameworks do not align with complex, multi-step thought processes like Chain-of-Thought (CoT). To bridge this gap, a new theoretical framework called CoT-Space is introduced, which transforms LLM reasoning into an optimization process within a continuous reasoning-level semantic space. Analysis shows that the convergence to an optimal CoT length is a result of balancing underfitting and overfitting. Empirical experiments support these theoretical findings and provide insights into phenomena like overthinking. The CoT-Space framework not only explains empirical observations but also lays a solid theoretical foundation for developing more effective and generalizable reasoning agents. 

<br /><br />Summary: <div>
arXiv:2509.04027v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has become a pivotal approach for enhancing the reasoning capabilities of Large Language Models (LLMs). However, a significant theoretical gap persists, as traditional token-level RL frameworks fail to align with the reasoning-level nature of complex, multi-step thought processes like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space, a novel theoretical framework that recasts LLM reasoning from a discrete token-prediction task to an optimization process within a continuous, reasoning-level semantic space. By analyzing this process from both a noise perspective and a risk perspective, we demonstrate that the convergence to an optimal CoT length is a natural consequence of the fundamental trade-off between underfitting and overfitting. Furthermore, extensive experiments provide strong empirical validation for our theoretical findings. Our framework not only provides a coherent explanation for empirical phenomena such as overthinking but also offers a solid theoretical foundation to guide the future development of more effective and generalizable reasoning agents.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oruga: An Avatar of Representational Systems Theory</title>
<link>https://arxiv.org/abs/2509.04041</link>
<guid>https://arxiv.org/abs/2509.04041</guid>
<content:encoded><![CDATA[
<div> Keywords: representations, Oruga, Representational Systems Theory, structure transfer, transformations

Summary:
Oruga, an implementation based on Representational Systems Theory (RST), aims to endow machines with the flexibility of human-like representations. The core of Oruga consists of data structures corresponding to RST concepts, a language for communication, and an engine for executing transformations through structure transfer. This method allows for the manipulation and transformation of representations in a way that mirrors human cognitive processes. By harnessing the power of RST, Oruga seeks to make machines more compatible with human use by enabling them to draw diagrams, change representations, and make creative analogies across different domains. Through the implementation of Oruga, researchers hope to advance the understanding of how machines can adapt and manipulate representations in a more flexible and human-like manner. <div>
arXiv:2509.04041v1 Announce Type: new 
Abstract: Humans use representations flexibly. We draw diagrams, change representations and exploit creative analogies across different domains. We want to harness this kind of power and endow machines with it to make them more compatible with human use. Previously we developed Representational Systems Theory (RST) to study the structure and transformations of representations. In this paper we present Oruga (caterpillar in Spanish; a symbol of transformation), an implementation of various aspects of RST. Oruga consists of a core of data structures corresponding to concepts in RST, a language for communicating with the core, and an engine for producing transformations using a method we call structure transfer. In this paper we present an overview of the core and language of Oruga, with a brief example of the kind of transformation that structure transfer can execute.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning</title>
<link>https://arxiv.org/abs/2509.04083</link>
<guid>https://arxiv.org/abs/2509.04083</guid>
<content:encoded><![CDATA[
<div> Neurosymbolic LLM, formal language, reasoning capabilities, syntactic reasoning, semantic reasoning <br />
Summary: <br />
This paper explores the impact of formal language choice on Neurosymbolic LLM reasoning. It examines four formal languages on three datasets and seven LLMs, revealing that the formal language selection influences both syntactic and semantic reasoning abilities. The study highlights the importance of considering the formal language in neurosymbolic reasoning approaches. <div>
arXiv:2509.04083v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve astonishing results on a wide range of tasks. However, their formal reasoning ability still lags behind. A promising approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators from natural to formal languages and symbolic solvers for deriving correct results. Still, the contributing factors to the success of Neurosymbolic LLM reasoning remain unclear. This paper demonstrates that one previously overlooked factor is the choice of the formal language. We introduce the intermediate language challenge: selecting a suitable formal language for neurosymbolic reasoning. By comparing four formal languages across three datasets and seven LLMs, we show that the choice of formal language affects both syntactic and semantic reasoning capabilities. We also discuss the varying effects across different LLMs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Reinforcement Learning and Search for Flight Trajectory Planning</title>
<link>https://arxiv.org/abs/2509.04100</link>
<guid>https://arxiv.org/abs/2509.04100</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Search-based path planners, Flight paths, Airliners, Optimization

Summary:
Reinforcement Learning (RL) combined with search-based path planners is explored for optimizing flight paths for airliners, especially in emergency situations. An RL Agent is trained to pre-compute near-optimal paths based on location and atmospheric data to speed up path planning. This guides the path planning solver to find a solution within a certain distance from the initial guess, reducing the search space and improving computation speed by up to 50%. Empirical results using Airbus aircraft's performance models demonstrate that fuel consumption remains nearly identical to an unconstrained solver, with deviations typically within 1%. While global optimality is not guaranteed, the approach proves effective in generating efficient flight paths in critical scenarios. 

<br /><br />Summary: <div>
arXiv:2509.04100v1 Announce Type: new 
Abstract: This paper explores the combination of Reinforcement Learning (RL) and search-based path planners to speed up the optimization of flight paths for airliners, where in case of emergency a fast route re-calculation can be crucial. The fundamental idea is to train an RL Agent to pre-compute near-optimal paths based on location and atmospheric data and use those at runtime to constrain the underlying path planning solver and find a solution within a certain distance from the initial guess. The approach effectively reduces the size of the solver's search space, significantly speeding up route optimization. Although global optimality is not guaranteed, empirical results conducted with Airbus aircraft's performance models show that fuel consumption remains nearly identical to that of an unconstrained solver, with deviations typically within 1%. At the same time, computation speed can be improved by up to 50% as compared to using a conventional solver alone.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker</title>
<link>https://arxiv.org/abs/2509.04125</link>
<guid>https://arxiv.org/abs/2509.04125</guid>
<content:encoded><![CDATA[
<div> bluffing, poker, DQN, CFR, Leduc Hold'em<br />
<br />
Summary: <br />
In the study of computer-poker algorithms, the focus has often been on performance metrics rather than bluffing behavior. This paper examines the bluffing behavior of two popular algorithms, DQN and CFR, in Leduc Hold'em, a simplified poker game. Through an experiment where DQN and CFR agents play against each other, it is found that both algorithms exhibit bluffing behavior, albeit in different ways. While both algorithms attempt to bluff at different rates, the success rate of their bluffs (opponent folding) is similar. This indicates that bluffing is a crucial aspect of the game itself and not specific to the algorithms being used. Future research should explore various bluffing styles and expand the study to the full game of poker. The code for this study is available on GitHub for reference.  <div>
arXiv:2509.04125v1 Announce Type: new 
Abstract: In the game of poker, being unpredictable, or bluffing, is an essential skill. When humans play poker, they bluff. However, most works on computer-poker focus on performance metrics such as win rates, while bluffing is overlooked. In this paper we study whether two popular algorithms, DQN (based on reinforcement learning) and CFR (based on game theory), exhibit bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed an experiment where we let the DQN and CFR agent play against each other while we log their actions. We find that both DQN and CFR exhibit bluffing behavior, but they do so in different ways. Although both attempt to perform bluffs at different rates, the percentage of successful bluffs (where the opponent folds) is roughly the same. This suggests that bluffing is an essential aspect of the game, not of the algorithm. Future work should look at different bluffing styles and at the full game of poker. Code at https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The human biological advantage over AI</title>
<link>https://arxiv.org/abs/2509.04130</link>
<guid>https://arxiv.org/abs/2509.04130</guid>
<content:encoded><![CDATA[
<div> AGI, artificial general intelligence, CNS, central nervous system, DNA, silicon <br />
<br />
Summary: Recent advances in AI suggest the potential for AI systems to surpass human capabilities, leading to the speculation of AI becoming a superior "digital species". However, the fundamental difference between humans and AI lies not in the brain, but in the central nervous system (CNS), which allows humans to experience emotions and understand the consequences of their actions. This emotional understanding is essential for developing ethical systems and qualifying as leaders of the universe. While AI systems may excel in various tasks and revolutionize society, the foundation for true leadership will always be DNA, as a CNS cannot be manufactured or simulated, and consciousness alone will not be enough to surpass humans. Thus, while AI may advance in many aspects, the core qualities that make humans uniquely suited for leadership in the universe stem from their biological nature and emotional capacity. <br /><br /> <div>
arXiv:2509.04130v1 Announce Type: new 
Abstract: Recent advances in AI raise the possibility that AI systems will one day be able to do anything humans can do, only better. If artificial general intelligence (AGI) is achieved, AI systems may be able to understand, reason, problem solve, create, and evolve at a level and speed that humans will increasingly be unable to match, or even understand. These possibilities raise a natural question as to whether AI will eventually become superior to humans, a successor "digital species", with a rightful claim to assume leadership of the universe. However, a deeper consideration suggests the overlooked differentiator between human beings and AI is not the brain, but the central nervous system (CNS), providing us with an immersive integration with physical reality. It is our CNS that enables us to experience emotion including pain, joy, suffering, and love, and therefore to fully appreciate the consequences of our actions on the world around us. And that emotional understanding of the consequences of our actions is what is required to be able to develop sustainable ethical systems, and so be fully qualified to be the leaders of the universe. A CNS cannot be manufactured or simulated; it must be grown as a biological construct. And so, even the development of consciousness will not be sufficient to make AI systems superior to humans. AI systems may become more capable than humans on almost every measure and transform our society. However, the best foundation for leadership of our universe will always be DNA, not silicon.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs</title>
<link>https://arxiv.org/abs/2509.04159</link>
<guid>https://arxiv.org/abs/2509.04159</guid>
<content:encoded><![CDATA[
<div> DSL, recipes, action graphs, culinary workflows, ontology <br />
<br />
Summary: <br />
This article introduces a domain-specific language for representing recipes as directed action graphs, allowing precise modeling of complex culinary workflows. The DSL captures processes, transfers, environments, concurrency, and compositional structure, enabling structured machine understanding and scalable automation of cooking processes. Initial evaluation on a full English breakfast recipe demonstrates the DSL's expressiveness and potential for automated recipe analysis and execution. The approach aims to formalize cooking procedures, addressing their inherent complexity and ambiguity. By using temporal graphs, the DSL facilitates precise interpretation of culinary processes in both home kitchens and professional settings. This work paves the way for an action-centric ontology for cooking, laying the foundation for future advancements in automated recipe handling and execution. <div>
arXiv:2509.04159v1 Announce Type: new 
Abstract: Formalizing cooking procedures remains a challenging task due to their inherent complexity and ambiguity. We introduce an extensible domain-specific language for representing recipes as directed action graphs, capturing processes, transfers, environments, concurrency, and compositional structure. Our approach enables precise, modular modeling of complex culinary workflows. Initial manual evaluation on a full English breakfast recipe demonstrates the DSL's expressiveness and suitability for future automated recipe analysis and execution. This work represents initial steps towards an action-centric ontology for cooking, using temporal graphs to enable structured machine understanding, precise interpretation, and scalable automation of culinary processes - both in home kitchens and professional culinary settings.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain size asymptotics for Markov logic networks</title>
<link>https://arxiv.org/abs/2509.04192</link>
<guid>https://arxiv.org/abs/2509.04192</guid>
<content:encoded><![CDATA[
<div> random structures, Markov logic network, domain size, limit behavior, soft constraints

Summary:
Random structures with domain sizes tending to infinity in Markov logic networks (MLNs) are studied. Three examples of MLNs are analyzed: quantifier-free MLNs with one relation symbol, MLNs favoring graphs with fewer triangles or vertices with high degrees. The analysis shows the diverse limit behaviors of random structures based on the soft constraints used in the MLNs. It is demonstrated that quantifier-free MLNs and lifted Bayesian networks are asymptotically incomparable. Additionally, the distribution determined by an MLN on large domains concentrates its probability mass on a different part of the possible worlds space compared to the uniform distribution. The study highlights the variety of behaviors MLNs exhibit with increasing domain sizes and the potential influence of the soft constraints on the outcomes.<br /><br />Summary: <div>
arXiv:2509.04192v1 Announce Type: new 
Abstract: A Markov logic network (MLN) determines a probability distribution on the set of structures, or ``possible worlds'', with an arbitrary finite domain. We study the properties of such distributions as the domain size tends to infinity. Three types of concrete examples of MLNs will be considered, and the properties of random structures with domain sizes tending to infinity will be studied: (1) Arbitrary quantifier-free MLNs over a language with only one relation symbol which has arity 1. In this case we give a pretty complete characterization of the possible limit behaviours of random structures. (2) An MLN that favours graphs with fewer triangles (or more generally, fewer k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law'' for first-order logic is obtained. (3) An MLN that favours graphs with fewer vertices with degree higher than a fixed (but arbitrary) number. The analysis shows that depending on which ``soft constraints'' an MLN uses the limit behaviour of random structures can be quite different, and the weights of the soft constraints may, or may not, have influence on the limit behaviour. It will also be demonstrated, using (1), that quantifier-free MLNs and lifted Bayesian networks (in a broad sense) are asymptotically incomparable, roughly meaning that there is a sequence of distributions on possible worlds with increasing domain sizes that can be defined by one of the formalisms but not even approximated by the other. In a rather general context it is also shown that on large domains the distribution determined by an MLN concentrates almost all its probability mass on a totally different part of the space of possible worlds than the uniform distribution does.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Quality of Gaming Narratives Co-created with AI</title>
<link>https://arxiv.org/abs/2509.04239</link>
<guid>https://arxiv.org/abs/2509.04239</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated game narratives, Delphi study, narrative design experts, Kano model, player satisfaction

Summary: 

This paper introduces a methodical approach for evaluating AI-generated game narratives by involving a panel of narrative design experts in a Delphi study. By blending insights from existing literature and expert input, the study identifies key dimensions of story quality and links them to the Kano model framework to gauge their influence on player satisfaction. The findings offer valuable guidance to game developers on prioritizing different quality aspects when collaborating with generative AI systems to craft engaging game narratives. <div>
arXiv:2509.04239v1 Announce Type: new 
Abstract: This paper proposes a structured methodology to evaluate AI-generated game narratives, leveraging the Delphi study structure with a panel of narrative design experts. Our approach synthesizes story quality dimensions from literature and expert insights, mapping them into the Kano model framework to understand their impact on player satisfaction. The results can inform game developers on prioritizing quality aspects when co-creating game narratives with generative AI.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation</title>
<link>https://arxiv.org/abs/2509.04310</link>
<guid>https://arxiv.org/abs/2509.04310</guid>
<content:encoded><![CDATA[
<div> evolutionary reinforcement learning, emotional expression, negotiations, large language models, adversarial counterparts
<br />
Summary:
The study introduces EvoEmo, an evolutionary reinforcement learning framework, to improve emotional expression in negotiations for Large Language Models (LLMs). Current LLM agents lack adaptive emotional responses, making them susceptible to manipulation. EvoEmo models emotional state transitions as a Markov Decision Process and utilizes genetic optimization to evolve high-reward emotion policies across various negotiation scenarios. Evaluation against two baselines shows that EvoEmo outperforms both in terms of success rates, efficiency, and buyer savings. The results emphasize the significance of adaptive emotional expression in enhancing the performance of LLM agents in complex, multi-turn negotiations.<br /><br /> <div>
arXiv:2509.04310v1 Announce Type: new 
Abstract: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \textit{complex}, \textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines -- vanilla strategies and fixed-emotion strategies -- for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes</title>
<link>https://arxiv.org/abs/2509.04317</link>
<guid>https://arxiv.org/abs/2509.04317</guid>
<content:encoded><![CDATA[
<div> AlphaZero, Monte Carlo planning, neural network, test environments, performance improvement <br />
<br />
Summary: This paper explores the challenge of deploying AlphaZero agents in changed test environments and proposes modifications to the standard framework to enhance performance. The AlphaZero framework combines Monte Carlo planning with a pre-trained neural network, assuming a static environment. However, when faced with potential changes in the test environment, simple adjustments to the framework can yield significant performance improvements. These modifications are shown to be effective even with limited planning resources available. The study highlights the adaptability of the AlphaZero framework and its potential for application in dynamic environments. The code for the modifications discussed in the paper is publicly accessible on GitHub, enabling researchers and practitioners to implement these enhancements in their own deployment scenarios. <div>
arXiv:2509.04317v1 Announce Type: new 
Abstract: The AlphaZero framework provides a standard way of combining Monte Carlo planning with prior knowledge provided by a previously trained policy-value neural network. AlphaZero usually assumes that the environment on which the neural network was trained will not change at test time, which constrains its applicability. In this paper, we analyze the problem of deploying AlphaZero agents in potentially changed test environments and demonstrate how the combination of simple modifications to the standard framework can significantly boost performance, even in settings with a low planning budget available. The code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psychologically Enhanced AI Agents</title>
<link>https://arxiv.org/abs/2509.04343</link>
<guid>https://arxiv.org/abs/2509.04343</guid>
<content:encoded><![CDATA[
<div> MBTI, Large Language Model, personality conditioning, agent behavior, psychological frameworks<br />
<br />
Summary: 
The article introduces MBTI-in-Thoughts, a framework that enhances Large Language Model agents through psychologically grounded personality conditioning based on the Myers-Briggs Type Indicator (MBTI). The method involves priming agents with distinct personality archetypes via prompt engineering, allowing control over behavior in cognition and affect. Results show that personality priming leads to consistent behavioral biases, with emotionally expressive agents excelling in narrative generation and analytically primed agents exhibiting stable strategies in game-theoretic situations. The framework supports structured multi-agent communication protocols and demonstrates that self-reflection prior to interaction improves cooperation and reasoning quality. Trait persistence is ensured through integration of the 16Personalities test for automated verification. The approach generalizes to other psychological frameworks such as Big Five, HEXACO, or Enneagram, establishing a foundation for psychologically enhanced AI agents without fine-tuning.<br /><br />Summary: <div>
arXiv:2509.04343v1 Announce Type: new 
Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of Large Language Model (LLM) agents through psychologically grounded personality conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology, cognition and affect. We show that such personality priming yields consistent, interpretable behavioral biases across diverse tasks: emotionally expressive agents excel in narrative generation, while analytically primed agents adopt more stable strategies in game-theoretic settings. Our framework supports experimenting with structured multi-agent communication protocols and reveals that self-reflection prior to interaction improves cooperation and reasoning quality. To ensure trait persistence, we integrate the official 16Personalities test for automated verification. While our focus is on MBTI, we show that our approach generalizes seamlessly to other psychological frameworks such as Big Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior design, we establish a foundation for psychologically enhanced AI agents without any fine-tuning.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory</title>
<link>https://arxiv.org/abs/2509.04439</link>
<guid>https://arxiv.org/abs/2509.04439</guid>
<content:encoded><![CDATA[
<div> memory, concept-level, continual learning, inference compute, abstract concepts

Summary:
In the realm of Language Model (LLMs), a new method has been introduced to enhance reasoning by utilizing external memory for persisting insights gained during inference. This method involves storing reusable, modular abstractions of solution traces in natural language, rather than exact instances. By selectively retrieving relevant concepts from memory for future queries, the approach enables test-time continual learning without weight updates. Results on the challenging ARC-AGI benchmark show a 7.5% relative improvement over a strong baseline method, with performance scaling with inference compute. The study highlights the effectiveness of abstract concepts in memory design, consistently outperforming baseline methods. Furthermore, dynamic memory updating during test-time proves superior to fixed memory settings, demonstrating that continual problem solving and abstraction of patterns lead to self-improvement. The code for this method is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2509.04439v1 Announce Type: new 
Abstract: While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query/response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. On the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, we confirm that dynamically updating memory during test-time outperforms an otherwise identical fixed memory setting with additional attempts, supporting the hypothesis that solving more problems and abstracting more patterns to memory enables further solutions in a form of self-improvement. Code available at https://github.com/matt-seb-ho/arc_memo.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiND: A Neural Discriminator-Decoder for Accurate Bimanual Trajectory Prediction in Brain-Computer Interfaces</title>
<link>https://arxiv.org/abs/2509.03521</link>
<guid>https://arxiv.org/abs/2509.03521</guid>
<content:encoded><![CDATA[
<div> decoding, bimanual hand movements, intracortical recordings, brain-computer interfaces, BiND<br />
Summary:<br />
The study introduces BiND, a model for decoding bimanual hand movements from intracortical recordings in brain-computer interfaces. BiND classifies motion type and predicts continuous 2D hand velocities using specialized GRU-based decoders with a trial-relative time index. Benchmarking against six other models on a tetraplegic patient dataset, BiND outperforms all models with a mean $R^2$ of 0.76 for unimanual and 0.69 for bimanual trajectory prediction. It shows greater robustness to session variability, with accuracy improvements of up to 4% in cross-session analyses compared to GRU. Task-aware discrimination and temporal modeling enhance bimanual decoding, highlighting the effectiveness of BiND in decoding complex hand movements. <br /> <div>
arXiv:2509.03521v1 Announce Type: cross 
Abstract: Decoding bimanual hand movements from intracortical recordings remains a critical challenge for brain-computer interfaces (BCIs), due to overlapping neural representations and nonlinear interlimb interactions. We introduce BiND (Bimanual Neural Discriminator-Decoder), a two-stage model that first classifies motion type (unimanual left, unimanual right, or bimanual) and then uses specialized GRU-based decoders, augmented with a trial-relative time index, to predict continuous 2D hand velocities. We benchmark BiND against six state-of-the-art models (SVR, XGBoost, FNN, CNN, Transformer, GRU) on a publicly available 13-session intracortical dataset from a tetraplegic patient. BiND achieves a mean $R^2$ of 0.76 ($\pm$0.01) for unimanual and 0.69 ($\pm$0.03) for bimanual trajectory prediction, surpassing the next-best model (GRU) by 2% in both tasks. It also demonstrates greater robustness to session variability than all other benchmarked models, with accuracy improvements of up to 4% compared to GRU in cross-session analyses. This highlights the effectiveness of task-aware discrimination and temporal modeling in enhancing bimanual decoding.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies</title>
<link>https://arxiv.org/abs/2509.03525</link>
<guid>https://arxiv.org/abs/2509.03525</guid>
<content:encoded><![CDATA[
<div> adults, Alzheimer disease, speech-based screening, language model adaptation, dementia detection <br />
Summary: <br />
The study focused on the use of speech-based screening for detecting dementia in adults with Alzheimer's disease. It found that over half of US adults with dementia are undiagnosed, highlighting the need for scalable detection approaches. The study compared different adaptation strategies for large language models using the DementiaBank speech corpus. Results showed that class-centroid demonstrations were most effective for in-context learning, reasoning improved smaller models, and token-level fine-tuning generally produced the best outcomes. Adding a classification head significantly enhanced the performance of underperforming models. The study also found that multimodal audio-text models performed well but did not surpass the top text-only models. The findings emphasize that adaptation strategies, including demonstration selection, reasoning design, and tuning methods, play a crucial role in speech-based dementia detection and suggest that properly adapted open-weight models can match or exceed commercial systems. <div>
arXiv:2509.03525v1 Announce Type: cross 
Abstract: Over half of US adults with Alzheimer disease and related dementias remain undiagnosed, and speech-based screening offers a scalable detection approach. We compared large language model adaptation strategies for dementia detection using the DementiaBank speech corpus, evaluating nine text-only models and three multimodal audio-text models on recordings from DementiaBank speech corpus. Adaptations included in-context learning with different demonstration selection policies, reasoning-augmented prompting, parameter-efficient fine-tuning, and multimodal integration. Results showed that class-centroid demonstrations achieved the highest in-context learning performance, reasoning improved smaller models, and token-level fine-tuning generally produced the best scores. Adding a classification head substantially improved underperforming models. Among multimodal models, fine-tuned audio-text systems performed well but did not surpass the top text-only models. These findings highlight that model adaptation strategies, including demonstration selection, reasoning design, and tuning method, critically influence speech-based dementia detection, and that properly adapted open-weight models can match or exceed commercial systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model</title>
<link>https://arxiv.org/abs/2509.03527</link>
<guid>https://arxiv.org/abs/2509.03527</guid>
<content:encoded><![CDATA[
<div> Keywords: multilevel analysis, cryptocurrency news, Mistral 7B, knowledge graph, quantitative analytics

Summary: 
The paper presents a multilevel multitask analysis of cryptocurrency news using a fine-tuned Mistral 7B large language model with retrieval-augmented generation (RAG). The model generates graph and text summaries with sentiment scores and JSON representations on the first level of analytics. Higher levels employ hierarchical stacking to consolidate summaries into comprehensive reports. Incorporating both graph and text summaries offers complementary perspectives on cryptocurrency news. The model is fine-tuned using 4-bit quantization with the PEFT/LoRA approach, enhancing the quality of the analysis. By representing cryptocurrency news as a knowledge graph, the model helps eliminate issues related to large language model hallucinations. The results demonstrate the effectiveness of using the fine-tuned Mistral 7B LLM model for conducting informative qualitative and quantitative analytics, providing valuable insights into the cryptocurrency market. 

<br /><br />Summary: <div>
arXiv:2509.03527v1 Announce Type: cross 
Abstract: In the paper, we consider multilevel multitask analysis of cryptocurrency news using a fine-tuned Mistral 7B large language model with retrieval-augmented generation (RAG).
  On the first level of analytics, the fine-tuned model generates graph and text summaries with sentiment scores as well as JSON representations of summaries. Higher levels perform hierarchical stacking that consolidates sets of graph-based and text-based summaries as well as summaries of summaries into comprehensive reports. The combination of graph and text summaries provides complementary views of cryptocurrency news. The model is fine-tuned with 4-bit quantization using the PEFT/LoRA approach. The representation of cryptocurrency news as knowledge graph can essentially eliminate problems with large language model hallucinations.
  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM models for multilevel cryptocurrency news analysis can conduct informative qualitative and quantitative analytics, providing important insights.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages</title>
<link>https://arxiv.org/abs/2509.03529</link>
<guid>https://arxiv.org/abs/2509.03529</guid>
<content:encoded><![CDATA[
<div> Earnings calls, sentiment analysis, multi-modal framework, hierarchical discourse trees, contrastive learning<br />
<br />
Summary: This paper introduces a novel multi-modal framework for analyzing earnings calls, encoding them as hierarchical discourse trees to capture the layered structure of interactions. The framework generates semantically rich embeddings enriched with emotional signals, structured metadata, coherence scores, topic labels, and answer coverage assessments. A two-stage transformer architecture is used to encode content and discourse metadata at the node level and synthesize a global embedding for the entire conference. Experimental results show that the embeddings reflect affective tone, structural logic, and thematic alignment, providing stable and meaningful representations. The proposed system has practical utility for tasks such as financial forecasting and discourse evaluation, with potential applications in other high-stakes communication domains like tele-medicine, education, and politics. <div>
arXiv:2509.03529v1 Announce Type: cross 
Abstract: Earnings calls represent a uniquely rich and semi-structured source of financial communication, blending scripted managerial commentary with unscripted analyst dialogue. Although recent advances in financial sentiment analysis have integrated multi-modal signals, such as textual content and vocal tone, most systems rely on flat document-level or sentence-level models, failing to capture the layered discourse structure of these interactions. This paper introduces a novel multi-modal framework designed to generate semantically rich and structurally aware embeddings of earnings calls, by encoding them as hierarchical discourse trees. Each node, comprising either a monologue or a question-answer pair, is enriched with emotional signals derived from text, audio, and video, as well as structured metadata including coherence scores, topic labels, and answer coverage assessments. A two-stage transformer architecture is proposed: the first encodes multi-modal content and discourse metadata at the node level using contrastive learning, while the second synthesizes a global embedding for the entire conference. Experimental results reveal that the resulting embeddings form stable, semantically meaningful representations that reflect affective tone, structural logic, and thematic alignment. Beyond financial reporting, the proposed system generalizes to other high-stakes unscripted communicative domains such as tele-medicine, education, and political discourse, offering a robust and explainable approach to multi-modal discourse representation. This approach offers practical utility for downstream tasks such as financial forecasting and discourse evaluation, while also providing a generalizable method applicable to other domains involving high-stakes communication.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Detection of Hallucinated Entities in Long-Form Generation</title>
<link>https://arxiv.org/abs/2509.03531</link>
<guid>https://arxiv.org/abs/2509.03531</guid>
<content:encoded><![CDATA[
<div> Entity-level hallucinations, long-form generations, scalable method, hallucination classifiers, annotation methodology<br />
<br />Summary:
Large language models are increasingly used in critical applications, where hallucinations can have serious consequences. Existing detection methods are not practical for real-world use. A new cheap and scalable method for identifying hallucinated tokens in long-form text has been developed, focusing on entity-level hallucinations. An annotation methodology leveraging web search was used to train effective hallucination classifiers, outperforming baselines consistently. The classifiers are also successful in short-form question-answering scenarios and in detecting incorrect answers in mathematical reasoning tasks. The annotations collected for one model can be used to train classifiers for other models. The datasets created are released publicly, indicating a promising scalable approach for real-world hallucination detection.<br /><br />Summary: <div>
arXiv:2509.03531v1 Announce Type: cross 
Abstract: Large language models are now routinely used in high-stakes applications where hallucinations can cause serious harm, such as medical consultations or legal advice. Existing hallucination detection methods, however, are impractical for real-world use, as they are either limited to short factual queries or require costly external verification. We present a cheap, scalable method for real-time identification of hallucinated tokens in long-form generations, and scale it effectively to 70B parameter models. Our approach targets \emph{entity-level hallucinations} -- e.g., fabricated names, dates, citations -- rather than claim-level, thereby naturally mapping to token-level labels and enabling streaming detection. We develop an annotation methodology that leverages web search to annotate model responses with grounded labels indicating which tokens correspond to fabricated entities. This dataset enables us to train effective hallucination classifiers with simple and efficient methods such as linear probes. Evaluating across four model families, our classifiers consistently outperform baselines on long-form responses, including more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for Llama-3.3-70B), and are also an improvement in short-form question-answering settings. Moreover, despite being trained only with entity-level labels, our probes effectively detect incorrect answers in mathematical reasoning tasks, indicating generalization beyond entities. While our annotation methodology is expensive, we find that annotated responses from one model can be used to train effective classifiers on other models; accordingly, we publicly release our datasets to facilitate reuse. Overall, our work suggests a promising new approach for scalable, real-world hallucination detection.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuesGenie: Intelligent Multimodal Question Generation</title>
<link>https://arxiv.org/abs/2509.03535</link>
<guid>https://arxiv.org/abs/2509.03535</guid>
<content:encoded><![CDATA[
<div> Keywords: educational resources, question generation system, multi-modal input, reinforcement learning, interactive interface

Summary:
This project focuses on addressing the challenge of the lack of practice materials tailored to the abundant educational resources available to learners. The system developed includes a multi-modal question generation system with components such as multi-modal input handling, question generation, reinforcement learning from human feedback (RLHF), and an end-to-end interactive interface. By utilizing these components, the system aims to automate question generation, ensuring diversity in question types and balancing resource efficiency, robust functionality, and user experience. This project paves the way for scalable and intelligent question generation, offering a solution to the current gap in practice materials tailored to educational resources. 

<br /><br />Summary: <div>
arXiv:2509.03535v1 Announce Type: cross 
Abstract: In today's information-rich era, learners have access to abundant educational resources, but the lack of practice materials tailored to these resources presents a significant challenge. This project addresses that gap by developing a multi-modal question generation system that can automatically generate diverse question types from various content formats. The system features four major components: multi-modal input handling, question generation, reinforcement learning from human feedback (RLHF), and an end-to-end interactive interface. This project lays the foundation for automated, scalable, and intelligent question generation, carefully balancing resource efficiency, robust functionality and a smooth user experience.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2509.03537</link>
<guid>https://arxiv.org/abs/2509.03537</guid>
<content:encoded><![CDATA[
<div> abstraction, computational patterns, computer science, large language models, reinforcement learning<br />
Summary:<br />
- Abstraction, essential in computer science, is crucial for problem-solving and large language models (LLMs).<br />
- Current LLM training methods focus on pattern recognition and lack explicit abstraction training.<br />
- AR$^2$ introduces a framework to enhance LLMs' abstraction abilities through Adversarial Reinforcement Learning.<br />
- A teacher model transforms problems into narrative-rich descriptions to challenge LLMs.<br />
- The student model is trained to extract underlying computational kernels from these complex narrative problems.<br />
- Experimental results show AR$^2 improves LLM accuracy on challenging programming tasks, highlighting the importance of abstraction in LLM generalization.<br /> <div>
arXiv:2509.03537v1 Announce Type: cross 
Abstract: Abstraction--the ability to recognize and distill essential computational patterns from complex problem statements--is a foundational skill in computer science, critical both for human problem-solvers and coding-oriented large language models (LLMs). Despite recent advances in training LLMs for code generation using reinforcement learning (RL), most existing approaches focus primarily on superficial pattern recognition, overlooking explicit training for abstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement Learning for Abstract Reasoning), a novel framework explicitly designed to enhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to transform kernel problems into narrative-rich, challenging descriptions without changing their fundamental logic. Simultaneously, a student coding model is trained to solve these complex narrative problems by extracting their underlying computational kernels. Experimental results demonstrate that AR$^2$ substantially improves the student model's accuracy on previously unseen, challenging programming tasks, underscoring abstraction as a key skill for enhancing LLM generalization.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction</title>
<link>https://arxiv.org/abs/2509.03540</link>
<guid>https://arxiv.org/abs/2509.03540</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, Knowledge Graphs, Factual Accuracy, Interpretability

Summary:
Large Language Models (LLMs) often struggle with factual consistency due to limited memory. Retrieval-Augmented Generation (RAG) methods use external knowledge for better answers. This study introduces a method that dynamically constructs and expands knowledge graphs (KGs) during inference, integrating internal and external knowledge. It extracts a seed KG from the question, expands it using LLM's latent knowledge, and refines it through external retrieval. The approach improves factual accuracy, answer precision, and interpretability on three factual QA benchmarks compared to baseline methods. This inference-time KG construction shows promise in enhancing LLM factuality in a structured, interpretable, and scalable manner.<br /><br />Summary: <div>
arXiv:2509.03540v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often struggle with producing factually consistent answers due to limitations in their parametric memory. Retrieval-Augmented Generation (RAG) methods address this issue by incorporating external knowledge from trusted sources at inference time. However, such methods typically treat knowledge as unstructured text, which limits their ability to support compositional reasoning and identify factual inconsistencies. To overcome these limitations, we propose a novel framework that dynamically constructs and expands knowledge graphs (KGs) during inference, integrating both internal knowledge extracted from LLMs and external information retrieved from external sources. Our method begins by extracting a seed KG from the question via prompting, followed by iterative expansion using the LLM's latent knowledge. The graph is then selectively refined through external retrieval, enhancing factual coverage and correcting inaccuracies. We evaluate our approach on three diverse factual QA benchmarks, demonstrating consistent improvements in factual accuracy, answer precision, and interpretability over baseline prompting and static KG-augmented methods. Our findings suggest that inference-time KG construction is a promising direction for enhancing LLM factuality in a structured, interpretable, and scalable manner.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A software security review on Uganda's Mobile Money Services: Dr. Jim Spire's tweets sentiment analysis</title>
<link>https://arxiv.org/abs/2509.03545</link>
<guid>https://arxiv.org/abs/2509.03545</guid>
<content:encoded><![CDATA[
<div> Keywords: mobile money, security mechanisms, #StopAirtelThefty, Uganda, financial inclusion

Summary: 
This study examines the #StopAirtelThefty Twitter campaign in Uganda in August 2025, triggered by a security incident involving mobile money. The campaign highlighted public concerns over the safety of mobile money and revealed security vulnerabilities and user dissatisfaction. Qualitative analysis of the complaints raised during the campaign identified key themes related to security gaps experienced by users. These findings offer insights into the specific challenges faced by mobile money users in Uganda and provide valuable information for providers, policymakers, and the future of secure digital finance in the country. The study underscores the importance of addressing security issues in mobile money systems to ensure trust and confidence among users and to promote financial inclusion. <br /><br />Summary: <div>
arXiv:2509.03545v1 Announce Type: cross 
Abstract: The proliferation of mobile money in Uganda has been a cornerstone of financial inclusion, yet its security mechanisms remain a critical concern. This study investigates a significant public response to perceived security failures: the #StopAirtelThefty Twitter campaign of August 2025 Sparked by an incident publicized by Dr. Jim Spire Ssentongo where a phone thief accessed a victim's account, withdrew funds, and procured a loan, the campaign revealed deep seated public anxiety over the safety of mobile money. This research employs qualitative analysis to systematically examine the complaints raised during this campaign, extracting key themes related to security vulnerabilities and user dissatisfaction. By synthesizing these public sentiments, the paper provides crucial insights into the specific security gaps experienced by users and situates these findings within the larger framework of Uganda's mobile money regulatory and operational environment. The study concludes with implications for providers, policymakers, and the future of secure digital finance in Uganda.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric</title>
<link>https://arxiv.org/abs/2509.03594</link>
<guid>https://arxiv.org/abs/2509.03594</guid>
<content:encoded><![CDATA[
<div> optimisers, neural networks, Riemannian metric, loss landscape, training<br />
<br />
Summary:<br />
The article introduces a new class of optimisers for training neural networks that utilize the Riemannian metric inherent in the loss landscape's higher-dimensional space embedding. Comparisons with SGD, Adam, AdamW, and Muon show the new optimisers to be highly effective in low-dimensional examples and slightly better than current methods. The optimisers automatically adjust the learning rate in high-curvature regions, serving as a form of gradient clipping. They also implement effective scheduled learning rates and suggest decoupled weight decay. The new approach can enhance existing preconditioning methods with similar computational complexity to Adam. <div>
arXiv:2509.03594v1 Announce Type: cross 
Abstract: We present a class of novel optimisers for training neural networks that makes use of the Riemannian metric naturally induced when the loss landscape is embedded in higher-dimensional space. This is the same metric that underlies common visualisations of loss landscapes. By taking this geometric perspective literally and using the induced metric, we develop a new optimiser and compare it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of tasks and architectures. Empirically, we conclude that this new class of optimisers is highly effective in low dimensional examples, and provides slight improvement over state-of-the-art methods for training neural networks. These new optimisers have theoretically desirable properties. In particular, the effective learning rate is automatically decreased in regions of high curvature acting as a smoothed out form of gradient clipping. Similarly, one variant of these optimisers can also be viewed as inducing an effective scheduled learning rate and decoupled weight decay is the natural choice from our geometric perspective. The basic method can be used to modify any existing preconditioning method. The new optimiser has a computational complexity comparable to that of Adam.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition</title>
<link>https://arxiv.org/abs/2509.03615</link>
<guid>https://arxiv.org/abs/2509.03615</guid>
<content:encoded><![CDATA[
<div> Keywords: Optical Character Recognition, Large Vision-Language Models, Edge deployment, Efficiency, Affordability

Summary:<br /><br />
The article discusses the challenges of Optical Character Recognition (OCR) in diverse real-world images and the potential of Large Vision-Language Models (LVLMs) to enhance OCR capabilities. The study introduces Sprinklr-Edge-OCR, an OCR system optimized for edge deployment in resource-constrained environments. A comparative evaluation of five state-of-the-art LVLMs and two traditional OCR systems on a multilingual image dataset is presented, covering metrics such as accuracy, efficiency, and deployment cost. The results show that traditional OCR systems, specifically Sprinklr-Edge-OCR, outperform LVLMs in efficiency, with faster image processing and lower cost. Despite LVLMs like Qwen achieving high precision, Sprinklr-Edge-OCR demonstrates superior overall performance in terms of F1 score and practical applicability for edge deployment. This highlights the importance of considering compute requirements, latency, and affordability in selecting OCR systems for edge deployment scenarios. <div>
arXiv:2509.03615v1 Announce Type: cross 
Abstract: Optical Character Recognition (OCR) in multilingual, noisy, and diverse real-world images remains a significant challenge for optical character recognition systems. With the rise of Large Vision-Language Models (LVLMs), there is growing interest in their ability to generalize and reason beyond fixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR system built specifically optimized for edge deployment in resource-constrained environments. We present a large-scale comparative evaluation of five state-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two traditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly hand annotated dataset of multilingual (54 languages) images. Our benchmark covers a broad range of metrics including accuracy, semantic consistency, language coverage, computational efficiency (latency, memory, GPU usage), and deployment cost. To better reflect real-world applicability, we also conducted edge case deployment analysis, evaluating model performance on CPU only environments. Among the results, Qwen achieved the highest precision (0.54), while Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and outperformed others in efficiency, processing images 35 faster (0.17 seconds per image on average) and at less than 0.01 of the cost (0.006 USD per 1,000 images) compared to LVLM. Our findings demonstrate that the most optimal OCR systems for edge deployment are the traditional ones even in the era of LLMs due to their low compute requirements, low latency, and very high affordability.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds</title>
<link>https://arxiv.org/abs/2509.03633</link>
<guid>https://arxiv.org/abs/2509.03633</guid>
<content:encoded><![CDATA[
arXiv:2509.03633v1 Announce Type: cross 
Abstract: Close-range laser scanning provides detailed 3D captures of forest stands but requires efficient software for processing 3D point cloud data and extracting individual trees. Although recent studies have introduced deep learning methods for tree instance segmentation, these approaches require large annotated datasets and substantial computational resources. As a resource-efficient alternative, we present a revised version of the treeX algorithm, an unsupervised method that combines clustering-based stem detection with region growing for crown delineation. While the original treeX algorithm was developed for personal laser scanning (PLS) data, we provide two parameter presets, one for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one for UAV-borne laser scanning (ULS). We evaluated the method on six public datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham Woods) and compared it to six open-source methods (original treeX, treeiso, RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original treeX algorithm, our revision reduces runtime and improves accuracy, with instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data. For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original algorithm fails to segment any correct instances. For TLS and PLS data, our algorithm achieves accuracy similar to recent open-source methods, including deep learning. Given its algorithmic design, we see two main applications for our method: (1) as a resource-efficient alternative to deep learning approaches in scenarios where the data characteristics align with the method design (sufficient stem visibility and point density), and (2) for the semi-automatic generation of labels for deep learning models. To enable broader adoption, we provide an open-source Python implementation in the pointtree package.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records</title>
<link>https://arxiv.org/abs/2509.03643</link>
<guid>https://arxiv.org/abs/2509.03643</guid>
<content:encoded><![CDATA[
arXiv:2509.03643v1 Announce Type: cross 
Abstract: Electronic Health Records (EHRs) provide a rich, longitudinal view of patient health and hold significant potential for advancing clinical decision support, risk prediction, and data-driven healthcare research. However, most artificial intelligence (AI) models for EHRs are designed for narrow, single-purpose tasks, limiting their generalizability and utility in real-world settings. Here, we present CEHR-GPT, a general-purpose foundation model for EHR data that unifies three essential capabilities - feature representation, zero-shot prediction, and synthetic data generation - within a single architecture. To support temporal reasoning over clinical sequences, \cehrgpt{} incorporates a novel time-token-based learning framework that explicitly encodes patients' dynamic timelines into the model structure. CEHR-GPT demonstrates strong performance across all three tasks and generalizes effectively to external datasets through vocabulary expansion and fine-tuning. Its versatility enables rapid model development, cohort discovery, and patient outcome forecasting without the need for task-specific retraining.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators</title>
<link>https://arxiv.org/abs/2509.03647</link>
<guid>https://arxiv.org/abs/2509.03647</guid>
<content:encoded><![CDATA[
arXiv:2509.03647v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly serve as automated evaluators, yet they suffer from "self-preference bias": a tendency to favor their own outputs over those of other models. This bias undermines fairness and reliability in evaluation pipelines, particularly for tasks like preference tuning and model routing. We investigate whether lightweight steering vectors can mitigate this problem at inference time without retraining. We introduce a curated dataset that distinguishes self-preference bias into justified examples of self-preference and unjustified examples of self-preference, and we construct steering vectors using two methods: Contrastive Activation Addition (CAA) and an optimization-based approach. Our results show that steering vectors can reduce unjustified self-preference bias by up to 97\%, substantially outperforming prompting and direct preference optimization baselines. Yet steering vectors are unstable on legitimate self-preference and unbiased agreement, implying self-preference spans multiple or nonlinear directions. This underscores both their promise and limits as safeguards for LLM-as-judges and motivates more robust interventions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Virtuoso: A Latent Diffusion Transformer Model for Goal-Conditioned Trajectory Planning</title>
<link>https://arxiv.org/abs/2509.03658</link>
<guid>https://arxiv.org/abs/2509.03658</guid>
<content:encoded><![CDATA[
arXiv:2509.03658v1 Announce Type: cross 
Abstract: The ability to generate a diverse and plausible distribution of future trajectories is a critical capability for autonomous vehicle planning systems. While recent generative models have shown promise, achieving high fidelity, computational efficiency, and precise control remains a significant challenge. In this paper, we present the \textbf{Efficient Virtuoso}, a conditional latent diffusion model for goal-conditioned trajectory planning. Our approach introduces a novel two-stage normalization pipeline that first scales trajectories to preserve their geometric aspect ratio and then normalizes the resulting PCA latent space to ensure a stable training target. The denoising process is performed efficiently in this low-dimensional latent space by a simple MLP denoiser, which is conditioned on a rich scene context fused by a powerful Transformer-based StateEncoder. We demonstrate that our method achieves state-of-the-art performance on the Waymo Open Motion Dataset, reaching a \textbf{minADE of 0.25}. Furthermore, through a rigorous ablation study on goal representation, we provide a key insight: while a single endpoint goal can resolve strategic ambiguity, a richer, multi-step sparse route is essential for enabling the precise, high-fidelity tactical execution that mirrors nuanced human driving behavior.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insights from Gradient Dynamics: Gradient Autoscaled Normalization</title>
<link>https://arxiv.org/abs/2509.03677</link>
<guid>https://arxiv.org/abs/2509.03677</guid>
<content:encoded><![CDATA[
arXiv:2509.03677v1 Announce Type: cross 
Abstract: Gradient dynamics play a central role in determining the stability and generalization of deep neural networks. In this work, we provide an empirical analysis of how variance and standard deviation of gradients evolve during training, showing consistent changes across layers and at the global scale in convolutional networks. Motivated by these observations, we propose a hyperparameter-free gradient normalization method that aligns gradient scaling with their natural evolution. This approach prevents unintended amplification, stabilizes optimization, and preserves convergence guarantees. Experiments on the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN demonstrate that our method maintains or improves test accuracy even under strong generalization. Beyond practical performance, our study highlights the importance of directly tracking gradient dynamics, aiming to bridge the gap between theoretical expectations and empirical behaviors, and to provide insights for future optimization research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LuxDiT: Lighting Estimation with Video Diffusion Transformer</title>
<link>https://arxiv.org/abs/2509.03680</link>
<guid>https://arxiv.org/abs/2509.03680</guid>
<content:encoded><![CDATA[
arXiv:2509.03680v1 Announce Type: cross 
Abstract: Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures</title>
<link>https://arxiv.org/abs/2509.03695</link>
<guid>https://arxiv.org/abs/2509.03695</guid>
<content:encoded><![CDATA[
arXiv:2509.03695v1 Announce Type: cross 
Abstract: The rise of foundation models (FMs) has reshaped the landscape of machine learning. As these models continued to grow, leveraging geo-distributed data from wireless devices has become increasingly critical, giving rise to federated foundation models (FFMs). More recently, FMs have evolved into multi-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diverse modalities across multiple tasks, which motivates a new underexplored paradigm: M3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs by proposing hierarchical federated foundation models (HF-FMs), which in turn expose two overlooked heterogeneity dimensions to fog/edge networks that have a direct impact on these emerging models: (i) heterogeneity in collected modalities and (ii) heterogeneity in executed tasks across fog/edge nodes. HF-FMs strategically align the modular structure of M3T FMs, comprising modality encoders, prompts, mixture-of-experts (MoEs), adapters, and task heads, with the hierarchical nature of fog/edge infrastructures. Moreover, HF-FMs enable the optional usage of device-to-device (D2D) communications, enabling horizontal module relaying and localized cooperative training among nodes when feasible. Through delving into the architectural design of HF-FMs, we highlight their unique capabilities along with a series of tailored future research directions. Finally, to demonstrate their potential, we prototype HF-FMs in a wireless network setting and release the open-source code for the development of HF-FMs with the goal of fostering exploration in this untapped field (GitHub: https://github.com/payamsiabd/M3T-FFM).
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Federated Learning to $\mathbb{X}$-Learning: Breaking the Barriers of Decentrality Through Random Walks</title>
<link>https://arxiv.org/abs/2509.03709</link>
<guid>https://arxiv.org/abs/2509.03709</guid>
<content:encoded><![CDATA[
arXiv:2509.03709v1 Announce Type: cross 
Abstract: We provide our perspective on $\mathbb{X}$-Learning ($\mathbb{X}$L), a novel distributed learning architecture that generalizes and extends the concept of decentralization. Our goal is to present a vision for $\mathbb{X}$L, introducing its unexplored design considerations and degrees of freedom. To this end, we shed light on the intuitive yet non-trivial connections between $\mathbb{X}$L, graph theory, and Markov chains. We also present a series of open research directions to stimulate further research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection</title>
<link>https://arxiv.org/abs/2509.03725</link>
<guid>https://arxiv.org/abs/2509.03725</guid>
<content:encoded><![CDATA[
arXiv:2509.03725v1 Announce Type: cross 
Abstract: We present the novel approach for stance detection across domains and targets, Metric Learning-Based Few-Shot Learning for Cross-Target and Cross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with triplet loss to capture semantic similarities and differences between stance targets, enhancing domain adaptation. By constructing a discriminative embedding space, MLSD allows a cross-target or cross-domain stance detection model to acquire useful examples from new target domains. We evaluate MLSD in multiple cross-target and cross-domain scenarios across two datasets, showing statistically significant improvement in stance detection performance across six widely used stance detection models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Entropy Regularization for Geometry and Neural Networks</title>
<link>https://arxiv.org/abs/2509.03733</link>
<guid>https://arxiv.org/abs/2509.03733</guid>
<content:encoded><![CDATA[
arXiv:2509.03733v1 Announce Type: cross 
Abstract: We introduce a differentiable estimator of range-partition entropy, a recent concept from computational geometry that enables algorithms to adapt to the "sortedness" of their input. While range-partition entropy provides strong guarantees in algorithm design, it has not yet been made accessible to deep learning. In this work, we (i) propose the first differentiable approximation of range-partition entropy, enabling its use as a trainable loss or regularizer; (ii) design EntropyNet, a neural module that restructures data into low-entropy forms to accelerate downstream instance-optimal algorithms; and (iii) extend this principle beyond geometry by applying entropy regularization directly to Transformer attention. Across tasks, we demonstrate that differentiable entropy improves efficiency without degrading correctness: in geometry, our method achieves up to $4.1\times$ runtime speedups with negligible error ($<0.2%$); in deep learning, it induces structured attention patterns that yield 6% higher accuracy at 80% sparsity compared to L1 baselines. Our theoretical analysis provides approximation bounds for the estimator, and extensive ablations validate design choices. These results suggest that entropy-bounded computation is not only theoretically elegant but also a practical mechanism for adaptive learning, efficiency, and structured representation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces</title>
<link>https://arxiv.org/abs/2509.03738</link>
<guid>https://arxiv.org/abs/2509.03738</guid>
<content:encoded><![CDATA[
arXiv:2509.03738v1 Announce Type: cross 
Abstract: We frame the problem of unifying representations in neural models as one of sparse model recovery and introduce a framework that extends sparse autoencoders (SAEs) to lifted spaces and infinite-dimensional function spaces, enabling mechanistic interpretability of large neural operators (NO). While the Platonic Representation Hypothesis suggests that neural networks converge to similar representations across architectures, the representational properties of neural operators remain underexplored despite their growing importance in scientific computing. We compare the inference and training dynamics of SAEs, lifted-SAE, and SAE neural operators. We highlight how lifting and operator modules introduce beneficial inductive biases, enabling faster recovery, improved recovery of smooth concepts, and robust inference across varying resolutions, a property unique to neural operators.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Gaze Analytics for ELA Instruction: A User-Centered Dashboard with Conversational AI Support</title>
<link>https://arxiv.org/abs/2509.03741</link>
<guid>https://arxiv.org/abs/2509.03741</guid>
<content:encoded><![CDATA[
arXiv:2509.03741v1 Announce Type: cross 
Abstract: Eye-tracking offers rich insights into student cognition and engagement, but remains underutilized in classroom-facing educational technology due to challenges in data interpretation and accessibility. In this paper, we present the iterative design and evaluation of a gaze-based learning analytics dashboard for English Language Arts (ELA), developed through five studies involving teachers and students. Guided by user-centered design and data storytelling principles, we explored how gaze data can support reflection, formative assessment, and instructional decision-making. Our findings demonstrate that gaze analytics can be approachable and pedagogically valuable when supported by familiar visualizations, layered explanations, and narrative scaffolds. We further show how a conversational agent, powered by a large language model (LLM), can lower cognitive barriers to interpreting gaze data by enabling natural language interactions with multimodal learning analytics. We conclude with design implications for future EdTech systems that aim to integrate novel data modalities in classroom contexts.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification</title>
<link>https://arxiv.org/abs/2509.03754</link>
<guid>https://arxiv.org/abs/2509.03754</guid>
<content:encoded><![CDATA[
arXiv:2509.03754v1 Announce Type: cross 
Abstract: Responding to rising global food security needs, precision agriculture and deep learning-based plant disease diagnosis have become crucial. Yet, deploying high-precision models on edge devices is challenging. Most lightweight networks use attention mechanisms designed for generic object recognition, which poorly capture subtle pathological features like irregular lesion shapes and complex textures. To overcome this, we propose a twofold solution: first, using a training-free neural architecture search method (DeepMAD) to create an efficient network backbone for edge devices; second, introducing the Shape-Texture Attention Module (STAM). STAM splits attention into two branches -- one using deformable convolutions (DCNv4) for shape awareness and the other using a Gabor filter bank for texture awareness. On the public CCMT plant disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs) reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm STAM significantly improves performance over baseline and standard attention models. Integrating domain knowledge via decoupled attention thus presents a promising path for edge-deployed precision agriculture AI. The source code is available at https://github.com/RzMY/STA-Net.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARDO: A Weak Formulation Deep Neural Network Method for Elliptic and Parabolic PDEs Based on Random Differences of Test Functions</title>
<link>https://arxiv.org/abs/2509.03757</link>
<guid>https://arxiv.org/abs/2509.03757</guid>
<content:encoded><![CDATA[
arXiv:2509.03757v1 Announce Type: cross 
Abstract: We propose ARDO method for solving PDEs and PDE-related problems with deep learning techniques. This method uses a weak adversarial formulation but transfers the random difference operator onto the test function. The main advantage of this framework is that it is fully derivative-free with respect to the solution neural network. This framework is particularly suitable for Fokker-Planck type second-order elliptic and parabolic PDEs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning an Adversarial World Model for Automated Curriculum Generation in MARL</title>
<link>https://arxiv.org/abs/2509.03771</link>
<guid>https://arxiv.org/abs/2509.03771</guid>
<content:encoded><![CDATA[
arXiv:2509.03771v1 Announce Type: cross 
Abstract: World models that infer and predict environmental dynamics are foundational to embodied intelligence. However, their potential is often limited by the finite complexity and implicit biases of hand-crafted training environments. To develop truly generalizable and robust agents, we need environments that scale in complexity alongside the agents learning within them. In this work, we reframe the challenge of environment generation as the problem of learning a goal-conditioned, generative world model. We propose a system where a generative **Attacker** agent learns an implicit world model to synthesize increasingly difficult challenges for a team of cooperative **Defender** agents. The Attacker's objective is not passive prediction, but active, goal-driven interaction: it models and generates world states (i.e., configurations of enemy units) specifically to exploit the Defenders' weaknesses. Concurrently, the embodied Defender team learns a cooperative policy to overcome these generated worlds. This co-evolutionary dynamic creates a self-scaling curriculum where the world model continuously adapts to challenge the decision-making policy of the agents, providing an effectively infinite stream of novel and relevant training scenarios. We demonstrate that this framework leads to the emergence of complex behaviors, such as the world model learning to generate flanking and shielding formations, and the defenders learning coordinated focus-fire and spreading tactics. Our findings position adversarial co-evolution as a powerful method for learning instrumental world models that drive agents toward greater strategic depth and robustness.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Latents: Latent Variables Stable Across Ontologies</title>
<link>https://arxiv.org/abs/2509.03780</link>
<guid>https://arxiv.org/abs/2509.03780</guid>
<content:encoded><![CDATA[
arXiv:2509.03780v1 Announce Type: cross 
Abstract: Suppose two Bayesian agents each learn a generative model of the same environment. We will assume the two have converged on the predictive distribution, i.e. distribution over some observables in the environment, but may have different generative models containing different latent variables. Under what conditions can one agent guarantee that their latents are a function of the other agents latents?
  We give simple conditions under which such translation is guaranteed to be possible: the natural latent conditions. We also show that, absent further constraints, these are the most general conditions under which translatability is guaranteed. Crucially for practical application, our theorems are robust to approximation error in the natural latent conditions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?</title>
<link>https://arxiv.org/abs/2509.03790</link>
<guid>https://arxiv.org/abs/2509.03790</guid>
<content:encoded><![CDATA[
arXiv:2509.03790v1 Announce Type: cross 
Abstract: What fundamental properties of reward functions enable efficient sparse-reward reinforcement learning? We address this question through the lens of low-rank structure in reward matrices, showing that such structure induces a sharp transition from exponential to polynomial sample complexity, the first result of this kind for sparse-reward RL. We introduce Policy-Aware Matrix Completion (PAMC), which connects matrix completion theory with reinforcement learning via a new analysis of policy-dependent sampling. Our framework provides: (i) impossibility results for general sparse reward observation, (ii) reward-free representation learning from dynamics, (iii) distribution-free confidence sets via conformal prediction, and (iv) robust completion guarantees that degrade gracefully when low-rank structure is only approximate. Empirically, we conduct a pre-registered evaluation across 100 systematically sampled domains, finding exploitable structure in over half. PAMC improves sample efficiency by factors between 1.6 and 2.1 compared to strong exploration, structured, and representation-learning baselines, while adding only about 20 percent computational overhead.These results establish structural reward learning as a promising new paradigm, with immediate implications for robotics, healthcare, and other safety-critical, sample-expensive applications.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation</title>
<link>https://arxiv.org/abs/2509.03791</link>
<guid>https://arxiv.org/abs/2509.03791</guid>
<content:encoded><![CDATA[
arXiv:2509.03791v1 Announce Type: cross 
Abstract: Evaluating sign language generation is often done through back-translation, where generated signs are first recognized back to text and then compared to a reference using text-based metrics. However, this two-step evaluation pipeline introduces ambiguity: it not only fails to capture the multimodal nature of sign language-such as facial expressions, spatial grammar, and prosody-but also makes it hard to pinpoint whether evaluation errors come from sign generation model or the translation system used to assess it. In this work, we propose SiLVERScore, a novel semantically-aware embedding-based evaluation metric that assesses sign language generation in a joint embedding space. Our contributions include: (1) identifying limitations of existing metrics, (2) introducing SiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness to semantic and prosodic variations, and (4) exploring generalization challenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore achieves near-perfect discrimination between correct and random pairs (ROC AUC = 0.99, overlap < 7%), substantially outperforming traditional metrics.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMVAD: A Multi-Agent System for Simulating Judicial Deliberation Dynamics in India</title>
<link>https://arxiv.org/abs/2509.03793</link>
<guid>https://arxiv.org/abs/2509.03793</guid>
<content:encoded><![CDATA[
arXiv:2509.03793v1 Announce Type: cross 
Abstract: Understanding the complexities of judicial deliberation is crucial for assessing the efficacy and fairness of a justice system. However, empirical studies of judicial panels are constrained by significant ethical and practical barriers. This paper introduces SAMVAD, an innovative Multi-Agent System (MAS) designed to simulate the deliberation process within the framework of the Indian justice system.
  Our system comprises agents representing key judicial roles: a Judge, a Prosecution Counsel, a Defense Counsel, and multiple Adjudicators (simulating a judicial bench), all powered by large language models (LLMs). A primary contribution of this work is the integration of Retrieval-Augmented Generation (RAG), grounded in a domain-specific knowledge base of landmark Indian legal documents, including the Indian Penal Code and the Constitution of India. This RAG functionality enables the Judge and Counsel agents to generate legally sound instructions and arguments, complete with source citations, thereby enhancing both the fidelity and transparency of the simulation.
  The Adjudicator agents engage in iterative deliberation rounds, processing case facts, legal instructions, and arguments to reach a consensus-based verdict. We detail the system architecture, agent communication protocols, the RAG pipeline, the simulation workflow, and a comprehensive evaluation plan designed to assess performance, deliberation quality, and outcome consistency.
  This work provides a configurable and explainable MAS platform for exploring legal reasoning and group decision-making dynamics in judicial simulations, specifically tailored to the Indian legal context and augmented with verifiable legal grounding via RAG.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring How (Not Just Whether) VLMs Build Common Ground</title>
<link>https://arxiv.org/abs/2509.03805</link>
<guid>https://arxiv.org/abs/2509.03805</guid>
<content:encoded><![CDATA[
arXiv:2509.03805v1 Announce Type: cross 
Abstract: Large vision language models (VLMs) increasingly claim reasoning skills, yet current benchmarks evaluate them in single-turn or question answering settings. However, grounding is an interactive process in which people gradually develop shared understanding through ongoing communication. We introduce a four-metric suite (grounding efficiency, content alignment, lexical adaptation, and human-likeness) to systematically evaluate VLM performance in interactive grounding contexts. We deploy the suite on 150 self-play sessions of interactive referential games between three proprietary VLMs and compare them with human dyads. All three models diverge from human patterns on at least three metrics, while GPT4o-mini is the closest overall. We find that (i) task success scores do not indicate successful grounding and (ii) high image-utterance alignment does not necessarily predict task success. Our metric suite and findings offer a framework for future research on VLM grounding.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation</title>
<link>https://arxiv.org/abs/2509.03809</link>
<guid>https://arxiv.org/abs/2509.03809</guid>
<content:encoded><![CDATA[
arXiv:2509.03809v1 Announce Type: cross 
Abstract: Large language models (LLMs) have ushered in a new era for document-level machine translation (\textit{doc}-mt), yet their whole-document outputs challenge existing evaluation methods that assume sentence-by-sentence alignment. We introduce \textit{\textbf{Align-then-Slide}}, a complete evaluation framework for ultra-long doc-mt. In the Align stage, we automatically infer sentence-level source-target correspondences and rebuild the target to match the source sentence number, resolving omissions and many-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we calculate averaged metric scores under 1-, 2-, 3- and 4-chunk for multi-granularity assessment. Experiments on the WMT benchmark show a Pearson correlation of 0.929 between our method with expert MQM rankings. On a newly curated real-world test set, our method again aligns closely with human judgments. Furthermore, preference data produced by Align-then-Slide enables effective CPO training and its direct use as a reward model for GRPO, both yielding translations preferred over a vanilla SFT baseline. The results validate our framework as an accurate, robust, and actionable evaluation tool for doc-mt systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gravity Well Echo Chamber Modeling With An LLM-Based Confirmation Bias Model</title>
<link>https://arxiv.org/abs/2509.03832</link>
<guid>https://arxiv.org/abs/2509.03832</guid>
<content:encoded><![CDATA[
arXiv:2509.03832v1 Announce Type: cross 
Abstract: Social media echo chambers play a central role in the spread of misinformation, yet existing models often overlook the influence of individual confirmation bias. An existing model of echo chambers is the "gravity well" model, which creates an analog between echo chambers and spatial gravity wells. We extend this established model by introducing a dynamic confirmation bias variable that adjusts the strength of pull based on a user's susceptibility to belief-reinforcing content. This variable is calculated for each user through comparisons between their posting history and their responses to posts of a wide range of viewpoints.
  Incorporating this factor produces a confirmation-bias-integrated gravity well model that more accurately identifies echo chambers and reveals community-level markers of information health. We validated the approach on nineteen Reddit communities, demonstrating improved detection of echo chambers.
  Our contribution is a framework for systematically capturing the role of confirmation bias in online group dynamics, enabling more effective identification of echo chambers. By flagging these high-risk environments, the model supports efforts to curb the spread of misinformation at its most common points of amplification.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Leiden to Pleasure Island: The Constant Potts Model for Community Detection as a Hedonic Game</title>
<link>https://arxiv.org/abs/2509.03834</link>
<guid>https://arxiv.org/abs/2509.03834</guid>
<content:encoded><![CDATA[
arXiv:2509.03834v1 Announce Type: cross 
Abstract: Community detection is one of the fundamental problems in data science which consists of partitioning nodes into disjoint communities. We present a game-theoretic perspective on the Constant Potts Model (CPM) for partitioning networks into disjoint communities, emphasizing its efficiency, robustness, and accuracy. Efficiency: We reinterpret CPM as a potential hedonic game by decomposing its global Hamiltonian into local utility functions, where the local utility gain of each agent matches the corresponding increase in global utility. Leveraging this equivalence, we prove that local optimization of the CPM objective via better-response dynamics converges in pseudo-polynomial time to an equilibrium partition. Robustness: We introduce and relate two stability criteria: a strict criterion based on a novel notion of robustness, requiring nodes to simultaneously maximize neighbors and minimize non-neighbors within communities, and a relaxed utility function based on a weighted sum of these objectives, controlled by a resolution parameter. Accuracy: In community tracking scenarios, where initial partitions are used to bootstrap the Leiden algorithm with partial ground-truth information, our experiments reveal that robust partitions yield higher accuracy in recovering ground-truth communities.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INGRID: Intelligent Generative Robotic Design Using Large Language Models</title>
<link>https://arxiv.org/abs/2509.03842</link>
<guid>https://arxiv.org/abs/2509.03842</guid>
<content:encoded><![CDATA[
arXiv:2509.03842v1 Announce Type: cross 
Abstract: The integration of large language models (LLMs) into robotic systems has accelerated progress in embodied artificial intelligence, yet current approaches remain constrained by existing robotic architectures, particularly serial mechanisms. This hardware dependency fundamentally limits the scope of robotic intelligence. Here, we present INGRID (Intelligent Generative Robotic Design), a framework that enables the automated design of parallel robotic mechanisms through deep integration with reciprocal screw theory and kinematic synthesis methods. We decompose the design challenge into four progressive tasks: constraint analysis, kinematic joint generation, chain construction, and complete mechanism design. INGRID demonstrates the ability to generate novel parallel mechanisms with both fixed and variable mobility, discovering kinematic configurations not previously documented in the literature. We validate our approach through three case studies demonstrating how INGRID assists users in designing task-specific parallel robots based on desired mobility requirements. By bridging the gap between mechanism theory and machine learning, INGRID enables researchers without specialized robotics training to create custom parallel mechanisms, thereby decoupling advances in robotic intelligence from hardware constraints. This work establishes a foundation for mechanism intelligence, where AI systems actively design robotic hardware, potentially transforming the development of embodied AI systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Inverse Reinforcement Learning for Mean Field Games via Probabilistic Context Variables</title>
<link>https://arxiv.org/abs/2509.03845</link>
<guid>https://arxiv.org/abs/2509.03845</guid>
<content:encoded><![CDATA[
arXiv:2509.03845v1 Announce Type: cross 
Abstract: Designing suitable reward functions for numerous interacting intelligent agents is challenging in real-world applications. Inverse reinforcement learning (IRL) in mean field games (MFGs) offers a practical framework to infer reward functions from expert demonstrations. While promising, the assumption of agent homogeneity limits the capability of existing methods to handle demonstrations with heterogeneous and unknown objectives, which are common in practice. To this end, we propose a deep latent variable MFG model and an associated IRL method. Critically, our method can infer rewards from different yet structurally similar tasks without prior knowledge about underlying contexts or modifying the MFG model itself. Our experiments, conducted on simulated scenarios and a real-world spatial taxi-ride pricing problem, demonstrate the superiority of our approach over state-of-the-art IRL methods in MFGs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MillGNN: Learning Multi-Scale Lead-Lag Dependencies for Multi-Variate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.03852</link>
<guid>https://arxiv.org/abs/2509.03852</guid>
<content:encoded><![CDATA[
arXiv:2509.03852v1 Announce Type: cross 
Abstract: Multi-variate time series (MTS) forecasting is crucial for various applications. Existing methods have shown promising results owing to their strong ability to capture intra- and inter-variate dependencies. However, these methods often overlook lead-lag dependencies at multiple grouping scales, failing to capture hierarchical lead-lag effects in complex systems. To this end, we propose MillGNN, a novel \underline{g}raph \underline{n}eural \underline{n}etwork-based method that learns \underline{m}ult\underline{i}ple grouping scale \underline{l}ead-\underline{l}ag dependencies for MTS forecasting, which can comprehensively capture lead-lag effects considering variate-wise and group-wise dynamics and decays. Specifically, MillGNN introduces two key innovations: (1) a scale-specific lead-lag graph learning module that integrates cross-correlation coefficients and dynamic decaying features derived from real-time inputs and time lags to learn lead-lag dependencies for each scale, which can model evolving lead-lag dependencies with statistical interpretability and data-driven flexibility; (2) a hierarchical lead-lag message passing module that passes lead-lag messages at multiple grouping scales in a structured way to simultaneously propagate intra- and inter-scale lead-lag effects, which can capture multi-scale lead-lag effects with a balance of comprehensiveness and efficiency. Experimental results on 11 datasets demonstrate the superiority of MillGNN for long-term and short-term MTS forecasting, compared with 16 state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2509.03871</link>
<guid>https://arxiv.org/abs/2509.03871</guid>
<content:encoded><![CDATA[
arXiv:2509.03871v1 Announce Type: cross 
Abstract: The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition</title>
<link>https://arxiv.org/abs/2509.03873</link>
<guid>https://arxiv.org/abs/2509.03873</guid>
<content:encoded><![CDATA[
arXiv:2509.03873v1 Announce Type: cross 
Abstract: Food recognition has gained significant attention, but the rapid emergence of new dishes requires methods for recognizing unseen food categories, motivating Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot Food Recognition (CZSFR), where cuisines and ingredients naturally align with attributes and objects in Compositional Zero-Shot learning (CZSL). However, CZSFR faces three challenges: (1) Redundant background information distracts models from learning meaningful food features, (2) Role confusion between staple and side dishes leads to misclassification, and (3) Semantic bias in a single attribute can lead to confusion of understanding. Therefore, we propose SalientFusion, a context-aware CZSFR method with two components: SalientFormer, which removes background redundancy and uses depth features to resolve role confusion; DebiasAT, which reduces the semantic bias by aligning prompts with visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we show that SalientFusion achieves state-of-the-art results on these benchmarks and the most popular general datasets for the general CZSL. The code is avaliable at https://github.com/Jiajun-RUC/SalientFusion.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peptidomic-Based Prediction Model for Coronary Heart Disease Using a Multilayer Perceptron Neural Network</title>
<link>https://arxiv.org/abs/2509.03884</link>
<guid>https://arxiv.org/abs/2509.03884</guid>
<content:encoded><![CDATA[
arXiv:2509.03884v1 Announce Type: cross 
Abstract: Coronary heart disease (CHD) is a leading cause of death worldwide and contributes significantly to annual healthcare expenditures. To develop a non-invasive diagnostic approach, we designed a model based on a multilayer perceptron (MLP) neural network, trained on 50 key urinary peptide biomarkers selected via genetic algorithms. Treatment and control groups, each comprising 345 individuals, were balanced using the Synthetic Minority Over-sampling Technique (SMOTE). The neural network was trained using a stratified validation strategy. Using a network with three hidden layers of 60 neurons each and an output layer of two neurons, the model achieved a precision, sensitivity, and specificity of 95.67 percent, with an F1-score of 0.9565. The area under the ROC curve (AUC) reached 0.9748 for both classes, while the Matthews correlation coefficient (MCC) and Cohen's kappa coefficient were 0.9134 and 0.9131, respectively, demonstrating its reliability in detecting CHD. These results indicate that the model provides a highly accurate and robust non-invasive diagnostic tool for coronary heart disease.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance</title>
<link>https://arxiv.org/abs/2509.03889</link>
<guid>https://arxiv.org/abs/2509.03889</guid>
<content:encoded><![CDATA[
arXiv:2509.03889v1 Announce Type: cross 
Abstract: Manipulating clothing is challenging due to complex configurations, variable material dynamics, and frequent self-occlusion. Prior systems often flatten garments or assume visibility of key features. We present a dual-arm visuotactile framework that combines confidence-aware dense visual correspondence and tactile-supervised grasp affordance to operate directly on crumpled and suspended garments. The correspondence model is trained on a custom, high-fidelity simulated dataset using a distributional loss that captures cloth symmetries and generates correspondence confidence estimates. These estimates guide a reactive state machine that adapts folding strategies based on perceptual uncertainty. In parallel, a visuotactile grasp affordance network, self-supervised using high-resolution tactile feedback, determines which regions are physically graspable. The same tactile classifier is used during execution for real-time grasp validation. By deferring action in low-confidence states, the system handles highly occluded table-top and in-air configurations. We demonstrate our task-agnostic grasp selection module in folding and hanging tasks. Moreover, our dense descriptors provide a reusable intermediate representation for other planning modalities, such as extracting grasp targets from human video demonstrations, paving the way for more generalizable and scalable garment manipulation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Generative Models Meet Compressed Sensing, with Applications to Image Data and Financial Time Series</title>
<link>https://arxiv.org/abs/2509.03898</link>
<guid>https://arxiv.org/abs/2509.03898</guid>
<content:encoded><![CDATA[
arXiv:2509.03898v1 Announce Type: cross 
Abstract: This paper develops dimension reduction techniques for accelerating diffusion model inference in the context of synthetic data generation. The idea is to integrate compressed sensing into diffusion models: (i) compress the data into a latent space, (ii) train a diffusion model in the latent space, and (iii) apply a compressed sensing algorithm to the samples generated in the latent space, facilitating the efficiency of both model training and inference. Under suitable sparsity assumptions on data, the proposed algorithm is proved to enjoy faster convergence by combining diffusion model inference with sparse recovery. As a byproduct, we obtain an optimal value for the latent space dimension. We also conduct numerical experiments on a range of datasets, including image data (handwritten digits, medical images, and climate data) and financial time series for stress testing.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question Answering</title>
<link>https://arxiv.org/abs/2509.03918</link>
<guid>https://arxiv.org/abs/2509.03918</guid>
<content:encoded><![CDATA[
arXiv:2509.03918v1 Announce Type: cross 
Abstract: Complex Question Answering (QA) is a fundamental and challenging task in NLP. While large language models (LLMs) exhibit impressive performance in QA, they suffer from significant performance degradation when facing complex and abstract QA tasks due to insufficient reasoning capabilities. Works such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning abilities, but they face issues such as in-layer redundancy in tree structures and single paths in chain structures. Although some studies utilize Retrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the challenge of effectively utilizing large amounts of information involving multiple entities and hops remains critical. To address this, we propose the Matrix of Thought (MoT), a novel and efficient LLM thought structure. MoT explores the problem in both horizontal and vertical dimensions through the "column-cell communication" mechanism, enabling LLMs to actively engage in multi-strategy and deep-level thinking, reducing redundancy within the column cells and enhancing reasoning capabilities. Furthermore, we develop a fact-correction mechanism by constructing knowledge units from retrieved knowledge graph triples and raw text to enhance the initial knowledge for LLM reasoning and correct erroneous answers. This leads to the development of an efficient and accurate QA framework (MTQA). Experimental results show that our framework outperforms state-of-the-art methods on four widely-used datasets in terms of F1 and EM scores, with reasoning time only 14.4\% of the baseline methods, demonstrating both its efficiency and accuracy. The code for this framework is available at https://github.com/lyfiter/mtqa.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment</title>
<link>https://arxiv.org/abs/2509.03934</link>
<guid>https://arxiv.org/abs/2509.03934</guid>
<content:encoded><![CDATA[
arXiv:2509.03934v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have revolutionized natural language processing through their remarkable capabilities in understanding and executing diverse tasks. While supervised fine-tuning, particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively enhances task-specific performance, it often leads to catastrophic forgetting, where models lose their previously acquired knowledge and general capabilities. Existing solutions either require access to general instruction data or face limitations in preserving the model's original distribution. To overcome these limitations, we propose SelfAug, a self-distribution alignment method that aligns input sequence logits to preserve the model's semantic distribution, thereby mitigating catastrophic forgetting and improving downstream performance. Extensive experiments demonstrate that SelfAug achieves a superior balance between downstream learning and general capability retention. Our comprehensive empirical analysis reveals a direct correlation between distribution shifts and the severity of catastrophic forgetting in RAG scenarios, highlighting how the absence of RAG capabilities in general instruction tuning leads to significant distribution shifts during fine-tuning. Our findings not only advance the understanding of catastrophic forgetting in RAG contexts but also provide a practical solution applicable across diverse fine-tuning scenarios. Our code is publicly available at https://github.com/USTC-StarTeam/SelfAug.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning</title>
<link>https://arxiv.org/abs/2509.03937</link>
<guid>https://arxiv.org/abs/2509.03937</guid>
<content:encoded><![CDATA[
arXiv:2509.03937v1 Announce Type: cross 
Abstract: Despite the significant advancements of self-play fine-tuning (SPIN), which can transform a weak large language model (LLM) into a strong one through competitive interactions between models of varying capabilities, it still faces challenges in the Text-to-SQL task. SPIN does not generate new information, and the large number of correct SQL queries produced by the opponent model during self-play reduces the main model's ability to generate accurate SQL queries. To address this challenge, we propose a new self-play fine-tuning method tailored for the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a verification-based iterative fine-tuning approach, which synthesizes high-quality fine-tuning data iteratively based on the database schema and validation feedback to enhance model performance, while building a model base with varying capabilities. During the self-play fine-tuning phase, we propose an error-driven loss method that incentivizes incorrect outputs from the opponent model, enabling the main model to distinguish between correct SQL and erroneous SQL generated by the opponent model, thereby improving its ability to generate correct SQL. Extensive experiments and in-depth analyses on six open-source LLMs and five widely used benchmarks demonstrate that our approach outperforms existing state-of-the-art (SOTA) methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based Role-Playing Agents</title>
<link>https://arxiv.org/abs/2509.03940</link>
<guid>https://arxiv.org/abs/2509.03940</guid>
<content:encoded><![CDATA[
arXiv:2509.03940v1 Announce Type: cross 
Abstract: Recent significant advancements in Large Language Models (LLMs) have greatly propelled the development of Role-Playing Conversational Agents (RPCAs). These systems aim to create immersive user experiences through consistent persona adoption. However, current RPCA research faces dual limitations. First, existing work predominantly focuses on the textual modality, entirely overlooking critical paralinguistic features including intonation, prosody, and rhythm in speech, which are essential for conveying character emotions and shaping vivid identities. Second, the speech-based role-playing domain suffers from a long-standing lack of standardized evaluation benchmarks. Most current spoken dialogue datasets target only fundamental capability assessments, featuring thinly sketched or ill-defined character profiles. Consequently, they fail to effectively quantify model performance on core competencies like long-term persona consistency. To address this critical gap, we introduce VoxRole, the first comprehensive benchmark specifically designed for the evaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn dialogues, totaling 65.6 hours of speech from 1228 unique characters across 261 movies. To construct this resource, we propose a novel two-stage automated pipeline that first aligns movie audio with scripts and subsequently employs an LLM to systematically build multi-dimensional profiles for each character. Leveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary spoken dialogue models, revealing crucial insights into their respective strengths and limitations in maintaining persona consistency.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture</title>
<link>https://arxiv.org/abs/2509.03950</link>
<guid>https://arxiv.org/abs/2509.03950</guid>
<content:encoded><![CDATA[
arXiv:2509.03950v1 Announce Type: cross 
Abstract: Pneumothorax, the abnormal accumulation of air in the pleural space, can be life-threatening if undetected. Chest X-rays are the first-line diagnostic tool, but small cases may be subtle. We propose an automated deep-learning pipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax regions. Trained on the SIIM-ACR dataset with data augmentation and a combined binary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and Dice score of 0.8241 on the independent PTX-498 dataset. These results demonstrate that the model can accurately localize pneumothoraces and support radiologists.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking</title>
<link>https://arxiv.org/abs/2509.03957</link>
<guid>https://arxiv.org/abs/2509.03957</guid>
<content:encoded><![CDATA[
arXiv:2509.03957v1 Announce Type: cross 
Abstract: The effectiveness of large language models (LLMs) to fact-check misinformation remains uncertain, despite their growing use. To this end, we present CANDY, a benchmark designed to systematically evaluate the capabilities and limitations of LLMs in fact-checking Chinese misinformation. Specifically, we curate a carefully annotated dataset of ~20k instances. Our analysis shows that current LLMs exhibit limitations in generating accurate fact-checking conclusions, even when enhanced with chain-of-thought reasoning and few-shot prompting. To understand these limitations, we develop a taxonomy to categorize flawed LLM-generated explanations for their conclusions and identify factual fabrication as the most common failure mode. Although LLMs alone are unreliable for fact-checking, our findings indicate their considerable potential to augment human performance when deployed as assistive tools in scenarios. Our dataset and code can be accessed at https://github.com/SCUNLP/CANDY
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection</title>
<link>https://arxiv.org/abs/2509.03961</link>
<guid>https://arxiv.org/abs/2509.03961</guid>
<content:encoded><![CDATA[
arXiv:2509.03961v1 Announce Type: cross 
Abstract: Although deep learning has advanced remote sensing change detection (RSCD), most methods rely solely on image modality, limiting feature representation, change pattern modeling, and generalization especially under illumination and noise disturbances. To address this, we propose MMChange, a multimodal RSCD method that combines image and text modalities to enhance accuracy and robustness. An Image Feature Refinement (IFR) module is introduced to highlight key regions and suppress environmental noise. To overcome the semantic limitations of image features, we employ a vision language model (VLM) to generate semantic descriptions of bitemporal images. A Textual Difference Enhancement (TDE) module then captures fine grained semantic shifts, guiding the model toward meaningful changes. To bridge the heterogeneity between modalities, we design an Image Text Feature Fusion (ITFF) module that enables deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and SYSUCD demonstrate that MMChange consistently surpasses state of the art methods across multiple metrics, validating its effectiveness for multimodal RSCD. Code is available at: https://github.com/yikuizhai/MMChange.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study</title>
<link>https://arxiv.org/abs/2509.03972</link>
<guid>https://arxiv.org/abs/2509.03972</guid>
<content:encoded><![CDATA[
arXiv:2509.03972v1 Announce Type: cross 
Abstract: We introduce Llama-3-Motif, a language model consisting of 102 billion parameters, specifically designed to enhance Korean capabilities while retaining strong performance in English. Developed on the Llama 3 architecture, Llama-3-Motif employs advanced training techniques, including LlamaPro and Masked Structure Growth, to effectively scale the model without altering its core Transformer architecture. Using the MoAI platform for efficient training across hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully curated dataset that maintains a balanced ratio of Korean and English data. Llama-3-Motif shows decent performance on Korean-specific benchmarks, outperforming existing models and achieving results comparable to GPT-4.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2509.03973</link>
<guid>https://arxiv.org/abs/2509.03973</guid>
<content:encoded><![CDATA[
arXiv:2509.03973v1 Announce Type: cross 
Abstract: We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for performing WSI classification. SAC-MIL consists of a positional encoding module to encode position information and a SAC block to perform full instance correlations. The positional encoding module utilizes the instance coordinates within the slide to encode the spatial relationships instead of the instance index in the input WSI sequence. The positional encoding module can also handle the length extrapolation issue where the training and testing sequences have different lengths. The SAC block is an MLP-based method that performs full instance correlation in linear time complexity with respect to the sequence length. Due to the simple structure of MLP, it is easy to deploy since it does not require custom CUDA kernels, compared to Transformer-based methods for WSI classification. SAC-MIL has achieved state-of-the-art performance on the CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language Models</title>
<link>https://arxiv.org/abs/2509.03985</link>
<guid>https://arxiv.org/abs/2509.03985</guid>
<content:encoded><![CDATA[
arXiv:2509.03985v1 Announce Type: cross 
Abstract: In deployment and application, large language models (LLMs) typically undergo safety alignment to prevent illegal and unethical outputs. However, the continuous advancement of jailbreak attack techniques, designed to bypass safety mechanisms with adversarial prompts, has placed increasing pressure on the security defenses of LLMs. Strengthening resistance to jailbreak attacks requires an in-depth understanding of the security mechanisms and vulnerabilities of LLMs. However, the vast number of parameters and complex structure of LLMs make analyzing security weaknesses from an internal perspective a challenging task. This paper presents NeuroBreak, a top-down jailbreak analysis system designed to analyze neuron-level safety mechanisms and mitigate vulnerabilities. We carefully design system requirements through collaboration with three experts in the field of AI security. The system provides a comprehensive analysis of various jailbreak attack methods. By incorporating layer-wise representation probing analysis, NeuroBreak offers a novel perspective on the model's decision-making process throughout its generation steps. Furthermore, the system supports the analysis of critical neurons from both semantic and functional perspectives, facilitating a deeper exploration of security mechanisms. We conduct quantitative evaluations and case studies to verify the effectiveness of our system, offering mechanistic insights for developing next-generation defense strategies against evolving jailbreak attacks.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promptception: How Sensitive Are Large Multimodal Models to Prompts?</title>
<link>https://arxiv.org/abs/2509.03986</link>
<guid>https://arxiv.org/abs/2509.03986</guid>
<content:encoded><![CDATA[
arXiv:2509.03986v1 Announce Type: cross 
Abstract: Despite the success of Large Multimodal Models (LMMs) in recent years, prompt design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly understood. We show that even minor variations in prompt phrasing and structure can lead to accuracy deviations of up to 15% for certain prompts and models. This variability poses a challenge for transparent and fair LMM evaluation, as models often report their best-case performance using carefully selected prompts. To address this, we introduce Promptception, a systematic framework for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types, spanning 15 categories and 6 supercategories, each targeting specific aspects of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks: MMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit greater sensitivity to prompt phrasing, reflecting tighter alignment with instruction semantics, while open-source models are steadier but struggle with nuanced and complex phrasing. Based on this analysis, we propose Prompting Principles tailored to proprietary and open-source LMMs, enabling more robust and fair model evaluation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models</title>
<link>https://arxiv.org/abs/2509.03995</link>
<guid>https://arxiv.org/abs/2509.03995</guid>
<content:encoded><![CDATA[
arXiv:2509.03995v1 Announce Type: cross 
Abstract: Current temporal knowledge graph question answering (TKGQA) methods primarily focus on implicit temporal constraints, lacking the capability of handling more complex temporal queries, and struggle with limited reasoning abilities and error propagation in decomposition frameworks. We propose RTQA, a novel framework to address these challenges by enhancing reasoning over TKGs without requiring training. Following recursive thinking, RTQA recursively decomposes questions into sub-problems, solves them bottom-up using LLMs and TKG knowledge, and employs multi-path answer aggregation to improve fault tolerance. RTQA consists of three core components: the Temporal Question Decomposer, the Recursive Solver, and the Answer Aggregator. Experiments on MultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements in "Multiple" and "Complex" categories, outperforming state-of-the-art methods. Our code and data are available at https://github.com/zjukg/RTQA.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Regional Spurious Correlations in Vision Transformers via Token Discarding</title>
<link>https://arxiv.org/abs/2509.04009</link>
<guid>https://arxiv.org/abs/2509.04009</guid>
<content:encoded><![CDATA[
arXiv:2509.04009v1 Announce Type: cross 
Abstract: Due to their powerful feature association capabilities, neural network-based computer vision models have the ability to detect and exploit unintended patterns within the data, potentially leading to correct predictions based on incorrect or unintended but statistically relevant signals. These clues may vary from simple color aberrations to small texts within the image. In situations where these unintended signals align with the predictive task, models can mistakenly link these features with the task and rely on them for making predictions. This phenomenon is referred to as spurious correlations, where patterns appear to be associated with the task but are actually coincidental. As a result, detection and mitigation of spurious correlations have become crucial tasks for building trustworthy, reliable, and generalizable machine learning models. In this work, we present a novel method to detect spurious correlations in vision transformers, a type of neural network architecture that gained significant popularity in recent years. Using both supervised and self-supervised trained models, we present large-scale experiments on the ImageNet dataset demonstrating the ability of the proposed method to identify spurious correlations. We also find that, even if the same architecture is used, the training methodology has a significant impact on the model's reliance on spurious correlations. Furthermore, we show that certain classes in the ImageNet dataset contain spurious signals that are easily detected by the models and discuss the underlying reasons for those spurious signals. In light of our findings, we provide an exhaustive list of the aforementioned images and call for caution in their use in future research efforts. Lastly, we present a case study investigating spurious signals in invasive breast mass classification, grounding our work in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings</title>
<link>https://arxiv.org/abs/2509.04011</link>
<guid>https://arxiv.org/abs/2509.04011</guid>
<content:encoded><![CDATA[
arXiv:2509.04011v1 Announce Type: cross 
Abstract: We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named Entity Retrieval, a variant of Named Entity Recognition (NER), where the types of interest are not provided in advance, and a user-defined type description is used to retrieve documents mentioning entities of that type. Instead of relying on fixed schemas or fine-tuned models, our method builds on internal representations of large language models (LLMs) to embed both entity mentions and user-provided open-ended type descriptions into a shared semantic space. We show that internal representations, specifically the value vectors from mid-layer transformer blocks, encode fine-grained type information more effectively than commonly used top-layer embeddings. To refine these representations, we train a lightweight contrastive projection network that aligns type-compatible entities while separating unrelated types. The resulting entity embeddings are compact, type-aware, and well-suited for nearest-neighbor search. Evaluated on three benchmarks, NER Retriever significantly outperforms both lexical and dense sentence-level retrieval baselines. Our findings provide empirical support for representation selection within LLMs and demonstrate a practical solution for scalable, schema-free entity retrieval. The NER Retriever Codebase is publicly available at https://github.com/ShacharOr100/ner_retriever
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Robustness and Reliability of Benchmark-Based Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2509.04013</link>
<guid>https://arxiv.org/abs/2509.04013</guid>
<content:encoded><![CDATA[
arXiv:2509.04013v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) effectiveness is usually evaluated by means of benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in their original wording, thus in a fixed, standardized format. However, real-world applications involve linguistic variability, requiring models to maintain their effectiveness across diverse rewordings of the same question or query. In this study, we systematically assess the robustness of LLMs to paraphrased benchmark questions and investigate whether benchmark-based evaluations provide a reliable measure of model capabilities. We systematically generate various paraphrases of all the questions across six different common benchmarks, and measure the resulting variations in effectiveness of 34 state-of-the-art LLMs, of different size and effectiveness. Our findings reveal that while LLM rankings remain relatively stable across paraphrased inputs, absolute effectiveness scores change, and decline significantly. This suggests that LLMs struggle with linguistic variability, raising concerns about their generalization abilities and evaluation methodologies. Furthermore, the observed performance drop challenges the reliability of benchmark-based evaluations, indicating that high benchmark scores may not fully capture a model's robustness to real-world input variations. We discuss the implications of these findings for LLM evaluation methodologies, emphasizing the need for robustness-aware benchmarks that better reflect practical deployment scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Video Compression with In-Loop Contextual Filtering and Out-of-Loop Reconstruction Enhancement</title>
<link>https://arxiv.org/abs/2509.04051</link>
<guid>https://arxiv.org/abs/2509.04051</guid>
<content:encoded><![CDATA[
arXiv:2509.04051v1 Announce Type: cross 
Abstract: This paper explores the application of enhancement filtering techniques in neural video compression. Specifically, we categorize these techniques into in-loop contextual filtering and out-of-loop reconstruction enhancement based on whether the enhanced representation affects the subsequent coding loop. In-loop contextual filtering refines the temporal context by mitigating error propagation during frame-by-frame encoding. However, its influence on both the current and subsequent frames poses challenges in adaptively applying filtering throughout the sequence. To address this, we introduce an adaptive coding decision strategy that dynamically determines filtering application during encoding. Additionally, out-of-loop reconstruction enhancement is employed to refine the quality of reconstructed frames, providing a simple yet effective improvement in coding efficiency. To the best of our knowledge, this work presents the first systematic study of enhancement filtering in the context of conditional-based neural video compression. Extensive experiments demonstrate a 7.71% reduction in bit rate compared to state-of-the-art neural video codecs, validating the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot</title>
<link>https://arxiv.org/abs/2509.04076</link>
<guid>https://arxiv.org/abs/2509.04076</guid>
<content:encoded><![CDATA[
arXiv:2509.04076v1 Announce Type: cross 
Abstract: We propose a novel diffusion-based action model for robotic motion planning. Commonly, established numerical planning approaches are used to solve general motion planning problems, but have significant runtime requirements. By leveraging the power of deep learning, we are able to achieve good results in a much smaller runtime by learning from a dataset generated by these planners. While our initial model uses point cloud embeddings in the input to predict keypoint-based joint sequences in its output, we observed in our ablation study that it remained challenging to condition the network on the point cloud embeddings. We identified some biases in our dataset and refined it, which improved the model's performance. Our model, even without the use of the point cloud encodings, outperforms numerical models by an order of magnitude regarding the runtime, while reaching a success rate of up to 90% of collision free solutions on the test set.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2509.04078</link>
<guid>https://arxiv.org/abs/2509.04078</guid>
<content:encoded><![CDATA[
arXiv:2509.04078v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have exhibited significant proficiency in code debugging, especially in automatic program repair, which may substantially reduce the time consumption of developers and enhance their efficiency. Significant advancements in debugging datasets have been made to promote the development of code debugging. However, these datasets primarily focus on assessing the LLM's function-level code repair capabilities, neglecting the more complex and realistic repository-level scenarios, which leads to an incomplete understanding of the LLM's challenges in repository-level debugging. While several repository-level datasets have been proposed, they often suffer from limitations such as limited diversity of tasks, languages, and error types. To mitigate this challenge, this paper introduces RepoDebug, a multi-task and multi-language repository-level code debugging dataset with 22 subtypes of errors that supports 8 commonly used programming languages and 3 debugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs, where Claude 3.5 Sonnect, the best-performing model, still cannot perform well in repository-level debugging.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHVC: Efficient Hierarchical Reference and Quality Structure for Neural Video Coding</title>
<link>https://arxiv.org/abs/2509.04118</link>
<guid>https://arxiv.org/abs/2509.04118</guid>
<content:encoded><![CDATA[
arXiv:2509.04118v1 Announce Type: cross 
Abstract: Neural video codecs (NVCs), leveraging the power of end-to-end learning, have demonstrated remarkable coding efficiency improvements over traditional video codecs. Recent research has begun to pay attention to the quality structures in NVCs, optimizing them by introducing explicit hierarchical designs. However, less attention has been paid to the reference structure design, which fundamentally should be aligned with the hierarchical quality structure. In addition, there is still significant room for further optimization of the hierarchical quality structure. To address these challenges in NVCs, we propose EHVC, an efficient hierarchical neural video codec featuring three key innovations: (1) a hierarchical multi-reference scheme that draws on traditional video codec design to align reference and quality structures, thereby addressing the reference-quality mismatch; (2) a lookahead strategy to utilize an encoder-side context from future frames to enhance the quality structure; (3) a layer-wise quality scale with random quality training strategy to stabilize quality structures during inference. With these improvements, EHVC achieves significantly superior performance to the state-of-the-art NVCs. Code will be released in: https://github.com/bytedance/NEVC.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation</title>
<link>https://arxiv.org/abs/2509.04126</link>
<guid>https://arxiv.org/abs/2509.04126</guid>
<content:encoded><![CDATA[
arXiv:2509.04126v1 Announce Type: cross 
Abstract: Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality
  and style diversity.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simplicity Lies in the Eye of the Beholder: A Strategic Perspective on Controllers in Reactive Synthesis</title>
<link>https://arxiv.org/abs/2509.04129</link>
<guid>https://arxiv.org/abs/2509.04129</guid>
<content:encoded><![CDATA[
arXiv:2509.04129v1 Announce Type: cross 
Abstract: In the game-theoretic approach to controller synthesis, we model the interaction between a system to be controlled and its environment as a game between these entities, and we seek an appropriate (e.g., winning or optimal) strategy for the system. This strategy then serves as a formal blueprint for a real-world controller. A common belief is that simple (e.g., using limited memory) strategies are better: corresponding controllers are easier to conceive and understand, and cheaper to produce and maintain.
  This invited contribution focuses on the complexity of strategies in a variety of synthesis contexts. We discuss recent results concerning memory and randomness, and take a brief look at what lies beyond our traditional notions of complexity for strategies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Technical Documents Retrieval for RAG</title>
<link>https://arxiv.org/abs/2509.04139</link>
<guid>https://arxiv.org/abs/2509.04139</guid>
<content:encoded><![CDATA[
arXiv:2509.04139v1 Announce Type: cross 
Abstract: In this paper, we introduce Technical-Embeddings, a novel framework designed to optimize semantic retrieval in technical documentation, with applications in both hardware and software development. Our approach addresses the challenges of understanding and retrieving complex technical content by leveraging the capabilities of Large Language Models (LLMs). First, we enhance user queries by generating expanded representations that better capture user intent and improve dataset diversity, thereby enriching the fine-tuning process for embedding models. Second, we apply summary extraction techniques to encode essential contextual information, refining the representation of technical documents. To further enhance retrieval performance, we fine-tune a bi-encoder BERT model using soft prompting, incorporating separate learning parameters for queries and document context to capture fine-grained semantic nuances. We evaluate our approach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that Technical-Embeddings significantly outperforms baseline models in both precision and recall. Our findings highlight the effectiveness of integrating query expansion and contextual summarization to enhance information access and comprehension in technical domains. This work advances the state of Retrieval-Augmented Generation (RAG) systems, offering new avenues for efficient and accurate technical document retrieval in engineering and product development workflows.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAGAL: Tabular Data Generation using Agentic LLM Methods</title>
<link>https://arxiv.org/abs/2509.04152</link>
<guid>https://arxiv.org/abs/2509.04152</guid>
<content:encoded><![CDATA[
arXiv:2509.04152v1 Announce Type: cross 
Abstract: The generation of data is a common approach to improve the performance of machine learning tasks, among which is the training of models for classification. In this paper, we present TAGAL, a collection of methods able to generate synthetic tabular data using an agentic workflow. The methods leverage Large Language Models (LLMs) for an automatic and iterative process that uses feedback to improve the generated data without any further LLM training. The use of LLMs also allows for the addition of external knowledge in the generation process. We evaluate TAGAL across diverse datasets and different aspects of quality for the generated data. We look at the utility of downstream ML models, both by training classifiers on synthetic data only and by combining real and synthetic data. Moreover, we compare the similarities between the real and the generated data. We show that TAGAL is able to perform on par with state-of-the-art approaches that require LLM training and generally outperforms other training-free approaches. These findings highlight the potential of agentic workflow and open new directions for LLM-based data generation methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention as an Adaptive Filter</title>
<link>https://arxiv.org/abs/2509.04154</link>
<guid>https://arxiv.org/abs/2509.04154</guid>
<content:encoded><![CDATA[
arXiv:2509.04154v1 Announce Type: cross 
Abstract: We introduce Adaptive Filter Attention (AFA), a novel attention mechanism that incorporates a learnable dynamics model directly into the computation of attention weights. Rather than comparing queries and keys directly, we model the input sequence as discrete observations of a linear stochastic differential equation (SDE). By imposing a linear dynamics model with simultaneously diagonalizable state matrices and noise covariances, we can make use of a closed-form solution to the differential Lyapunov equation to efficiently propagate pairwise uncertainties through the dynamics. Attention naturally arises as the maximum likelihood solution for this linear SDE, with attention weights corresponding to robust residual-based reweightings of the propagated pairwise precisions. Imposing an additional constraint on the state matrix's eigenvalues leads to a simplified variant with the same computational and memory complexity as standard attention. In the limit of vanishing dynamics and process noise, and using a small-angle approximation, we recover ordinary dot-product attention.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind Turbine Components</title>
<link>https://arxiv.org/abs/2509.04156</link>
<guid>https://arxiv.org/abs/2509.04156</guid>
<content:encoded><![CDATA[
arXiv:2509.04156v1 Announce Type: cross 
Abstract: Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up new opportunities for monitoring wind power plants, including blades, towers, and other critical components. However, reliable defect detection requires high-resolution data and efficient methods to process multispectral imagery. In this research, we aim to enhance defect detection accuracy through the development of an ensemble of YOLO-based deep learning models that integrate both visible and thermal channels. We propose an ensemble approach that integrates a general-purpose YOLOv8 model with a specialized thermal model, using a sophisticated bounding box fusion algorithm to combine their predictions. Our experiments show this approach achieves a mean Average Precision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone YOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that combining multiple YOLO architectures with fused multispectral data provides a more reliable solution, improving the detection of both visual and thermal defects.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crossing the Species Divide: Transfer Learning from Speech to Animal Sounds</title>
<link>https://arxiv.org/abs/2509.04166</link>
<guid>https://arxiv.org/abs/2509.04166</guid>
<content:encoded><![CDATA[
arXiv:2509.04166v1 Announce Type: cross 
Abstract: Self-supervised speech models have demonstrated impressive performance in speech processing, but their effectiveness on non-speech data remains underexplored. We study the transfer learning capabilities of such models on bioacoustic detection and classification tasks. We show that models such as HuBERT, WavLM, and XEUS can generate rich latent representations of animal sounds across taxa. We analyze the models properties with linear probing on time-averaged representations. We then extend the approach to account for the effect of time-wise information with other downstream architectures. Finally, we study the implication of frequency range and noise on performance. Notably, our results are competitive with fine-tuned bioacoustic pre-trained models and show the impact of noise-robust pre-training setups. These findings highlight the potential of speech-based self-supervised learning as an efficient framework for advancing bioacoustic research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</title>
<link>https://arxiv.org/abs/2509.04180</link>
<guid>https://arxiv.org/abs/2509.04180</guid>
<content:encoded><![CDATA[
arXiv:2509.04180v1 Announce Type: cross 
Abstract: AI models rely on annotated data to learn pattern and perform prediction. Annotation is usually a labor-intensive step that require associating labels ranging from a simple classification label to more complex tasks such as object detection, oriented bounding box estimation, and instance segmentation. Traditional tools often require extensive manual input, limiting scalability for large datasets. To address this, we introduce VisioFirm, an open-source web application designed to streamline image labeling through AI-assisted automation. VisioFirm integrates state-of-the-art foundation models into an interface with a filtering pipeline to reduce human-in-the-loop efforts. This hybrid approach employs CLIP combined with pre-trained detectors like Ultralytics models for common classes and zero-shot models such as Grounding DINO for custom labels, generating initial annotations with low-confidence thresholding to maximize recall. Through this framework, when tested on COCO-type of classes, initial prediction have been proven to be mostly correct though the users can refine these via interactive tools supporting bounding boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has on-the-fly segmentation powered by Segment Anything accelerated through WebGPU for browser-side efficiency. The tool supports multiple export formats (YOLO, COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort through benchmarks on diverse datasets, while maintaining high annotation accuracy via clustering of connected CLIP-based disambiguate components and IoU-graph for redundant detection suppression. VisioFirm can be accessed from \href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental Health Counseling Sessions</title>
<link>https://arxiv.org/abs/2509.04183</link>
<guid>https://arxiv.org/abs/2509.04183</guid>
<content:encoded><![CDATA[
arXiv:2509.04183v1 Announce Type: cross 
Abstract: The growing demand for scalable psychological counseling highlights the need for fine-tuning open-source Large Language Models (LLMs) with high-quality, privacy-compliant data, yet such data remains scarce. Here we introduce MAGneT, a novel multi-agent framework for synthetic psychological counseling session generation that decomposes counselor response generation into coordinated sub-tasks handled by specialized LLM agents, each modeling a key psychological technique. Unlike prior single-agent approaches, MAGneT better captures the structure and nuance of real counseling. In addition, we address inconsistencies in prior evaluation protocols by proposing a unified evaluation framework integrating diverse automatic and expert metrics. Furthermore, we expand the expert evaluations from four aspects of counseling in previous works to nine aspects, enabling a more thorough and robust assessment of data quality. Empirical results show that MAGneT significantly outperforms existing methods in quality, diversity, and therapeutic alignment of the generated counseling sessions, improving general counseling skills by 3.2% and CBT-specific skills by 4.3% on average on cognitive therapy rating scale (CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases on average across all aspects. Moreover, fine-tuning an open-source model on MAGneT-generated sessions shows better performance, with improvements of 6.3% on general counseling skills and 7.3% on CBT-specific skills on average on CTRS over those fine-tuned with sessions generated by baseline methods. We also make our code and data public.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding</title>
<link>https://arxiv.org/abs/2509.04243</link>
<guid>https://arxiv.org/abs/2509.04243</guid>
<content:encoded><![CDATA[
arXiv:2509.04243v1 Announce Type: cross 
Abstract: Vision Language Models (VLMs) have recently achieved significant progress in bridging visual perception and linguistic reasoning. Recently, OpenAI o3 model introduced a zoom-in search strategy that effectively elicits active perception capabilities in VLMs, improving downstream task performance. However, enabling VLMs to reason effectively over appropriate image regions remains a core challenge in GUI grounding, particularly under high-resolution inputs and complex multi-element visual interactions. In this work, we propose LASER, a self-evolving framework that progressively endows VLMs with multi-step perception capabilities, enabling precise coordinate prediction. Specifically, our approach integrate Monte Carlo quality estimation with Intersection-over-Union (IoU)-based region quality evaluation to jointly encourage both accuracy and diversity in constructing high-quality preference data. This combination explicitly guides the model to focus on instruction-relevant key regions while adaptively allocating reasoning steps based on task complexity. Comprehensive experiments on the ScreenSpot Pro and ScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating the effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER achieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new state-of-the-art (SoTA) among 7B-scale models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How many patients could we save with LLM priors?</title>
<link>https://arxiv.org/abs/2509.04250</link>
<guid>https://arxiv.org/abs/2509.04250</guid>
<content:encoded><![CDATA[
arXiv:2509.04250v1 Announce Type: cross 
Abstract: Imagine a world where clinical trials need far fewer patients to achieve the same statistical power, thanks to the knowledge encoded in large language models (LLMs). We present a novel framework for hierarchical Bayesian modeling of adverse events in multi-center clinical trials, leveraging LLM-informed prior distributions. Unlike data augmentation approaches that generate synthetic data points, our methodology directly obtains parametric priors from the model. Our approach systematically elicits informative priors for hyperparameters in hierarchical Bayesian models using a pre-trained LLM, enabling the incorporation of external clinical expertise directly into Bayesian safety modeling. Through comprehensive temperature sensitivity analysis and rigorous cross-validation on real-world clinical trial data, we demonstrate that LLM-derived priors consistently improve predictive performance compared to traditional meta-analytical approaches. This methodology paves the way for more efficient and expert-informed clinical trial design, enabling substantial reductions in the number of patients required to achieve robust safety assessment and with the potential to transform drug safety monitoring and regulatory decision making.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Vulnerabilities in Python Packages and Their Detection</title>
<link>https://arxiv.org/abs/2509.04260</link>
<guid>https://arxiv.org/abs/2509.04260</guid>
<content:encoded><![CDATA[
arXiv:2509.04260v1 Announce Type: cross 
Abstract: In the rapidly evolving software development landscape, Python stands out for its simplicity, versatility, and extensive ecosystem. Python packages, as units of organization, reusability, and distribution, have become a pressing concern, highlighted by the considerable number of vulnerability reports. As a scripting language, Python often cooperates with other languages for performance or interoperability. This adds complexity to the vulnerabilities inherent to Python packages, and the effectiveness of current vulnerability detection tools remains underexplored. This paper addresses these gaps by introducing PyVul, the first comprehensive benchmark suite of Python-package vulnerabilities. PyVul includes 1,157 publicly reported, developer-verified vulnerabilities, each linked to its affected packages. To accommodate diverse detection techniques, it provides annotations at both commit and function levels. An LLM-assisted data cleansing method is incorporated to improve label accuracy, achieving 100% commit-level and 94% function-level accuracy, establishing PyVul as the most precise large-scale Python vulnerability benchmark. We further carry out a distribution analysis of PyVul, which demonstrates that vulnerabilities in Python packages involve multiple programming languages and exhibit a wide variety of types. Moreover, our analysis reveals that multi-lingual Python packages are potentially more susceptible to vulnerabilities. Evaluation of state-of-the-art detectors using this benchmark reveals a significant discrepancy between the capabilities of existing tools and the demands of effectively identifying real-world security issues in Python packages. Additionally, we conduct an empirical review of the top-ranked CWEs observed in Python packages, to diagnose the fine-grained limitations of current detection tools and highlight the necessity for future advancements in the field.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Robust Ageing-Aware Control of Li-ion Battery Systems with Data-Driven Formal Verification</title>
<link>https://arxiv.org/abs/2509.04288</link>
<guid>https://arxiv.org/abs/2509.04288</guid>
<content:encoded><![CDATA[
arXiv:2509.04288v1 Announce Type: cross 
Abstract: Rechargeable lithium-ion (Li-ion) batteries are a ubiquitous element of modern technology. In the last decades, the production and design of such batteries and their adjacent embedded charging and safety protocols, denoted by Battery Management Systems (BMS), has taken central stage. A fundamental challenge to be addressed is the trade-off between the speed of charging and the ageing behavior, resulting in the loss of capacity in the battery cell. We rely on a high-fidelity physics-based battery model and propose an approach to data-driven charging and safety protocol design. Following a Counterexample-Guided Inductive Synthesis scheme, we combine Reinforcement Learning (RL) with recent developments in data-driven formal methods to obtain a hybrid control strategy: RL is used to synthesise the individual controllers, and a data-driven abstraction guides their partitioning into a switched structure, depending on the initial output measurements of the battery. The resulting discrete selection among RL-based controllers, coupled with the continuous battery dynamics, realises a hybrid system. When a design meets the desired criteria, the abstraction provides probabilistic guarantees on the closed-loop performance of the cell.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumAIne-Chatbot: Real-Time Personalized Conversational AI via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.04303</link>
<guid>https://arxiv.org/abs/2509.04303</guid>
<content:encoded><![CDATA[
arXiv:2509.04303v1 Announce Type: cross 
Abstract: Current conversational AI systems often provide generic, one-size-fits-all interactions that overlook individual user characteristics and lack adaptive dialogue management. To address this gap, we introduce \textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes responses through a novel user profiling framework. The system is pre-trained on a diverse set of GPT-generated virtual personas to establish a broad prior over user types. During live interactions, an online reinforcement learning agent refines per-user models by combining implicit signals (e.g. typing speed, sentiment, engagement duration) with explicit feedback (e.g., likes and dislikes). This profile dynamically informs the chatbot dialogue policy, enabling real-time adaptation of both content and style. To evaluate the system, we performed controlled experiments with 50 synthetic personas in multiple conversation domains. The results showed consistent improvements in user satisfaction, personalization accuracy, and task achievement when personalization features were enabled. Statistical analysis confirmed significant differences between personalized and nonpersonalized conditions, with large effect sizes across key metrics. These findings highlight the effectiveness of AI-driven user profiling and provide a strong foundation for future real-world validation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models</title>
<link>https://arxiv.org/abs/2509.04304</link>
<guid>https://arxiv.org/abs/2509.04304</guid>
<content:encoded><![CDATA[
arXiv:2509.04304v1 Announce Type: cross 
Abstract: The growing capabilities of Large Language Models (LLMs) show significant potential to enhance healthcare by assisting medical researchers and physicians. However, their reliance on static training data is a major risk when medical recommendations evolve with new research and developments. When LLMs memorize outdated medical knowledge, they can provide harmful advice or fail at clinical reasoning tasks. To investigate this problem, we introduce two novel question-answering (QA) datasets derived from systematic reviews: MedRevQA (16,501 QA pairs covering general biomedical knowledge) and MedChangeQA (a subset of 512 QA pairs where medical consensus has changed over time). Our evaluation of eight prominent LLMs on the datasets reveals consistent reliance on outdated knowledge across all models. We additionally analyze the influence of obsolete pre-training data and training strategies to explain this phenomenon and propose future directions for mitigation, laying the groundwork for developing more current and reliable medical AI systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Entity Representation Learning for Pinterest Ads Ranking</title>
<link>https://arxiv.org/abs/2509.04337</link>
<guid>https://arxiv.org/abs/2509.04337</guid>
<content:encoded><![CDATA[
arXiv:2509.04337v1 Announce Type: cross 
Abstract: In this paper, we introduce a novel framework following an upstream-downstream paradigm to construct user and item (Pin) embeddings from diverse data sources, which are essential for Pinterest to deliver personalized Pins and ads effectively. Our upstream models are trained on extensive data sources featuring varied signals, utilizing complex architectures to capture intricate relationships between users and Pins on Pinterest. To ensure scalability of the upstream models, entity embeddings are learned, and regularly refreshed, rather than real-time computation, allowing for asynchronous interaction between the upstream and downstream models. These embeddings are then integrated as input features in numerous downstream tasks, including ad retrieval and ranking models for CTR and CVR predictions. We demonstrate that our framework achieves notable performance improvements in both offline and online settings across various downstream tasks. This framework has been deployed in Pinterest's production ad ranking systems, resulting in significant gains in online metrics.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Editor to Dense Geometry Estimator</title>
<link>https://arxiv.org/abs/2509.04338</link>
<guid>https://arxiv.org/abs/2509.04338</guid>
<content:encoded><![CDATA[
arXiv:2509.04338v1 Announce Type: cross 
Abstract: Leveraging visual priors from pre-trained text-to-image (T2I) generative models has shown success in dense prediction. However, dense prediction is inherently an image-to-image task, suggesting that image editing models, rather than T2I generative models, may be a more suitable foundation for fine-tuning.
  Motivated by this, we conduct a systematic analysis of the fine-tuning behaviors of both editors and generators for dense geometry estimation. Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by ``refining" their innate features, and ultimately achieve higher performance than their generative counterparts.
  Based on these findings, we introduce \textbf{FE2E}, a framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction. Specifically, to tailor the editor for this deterministic task, we reformulate the editor's original flow matching loss into the ``consistent velocity" training objective. And we use logarithmic quantization to resolve the precision conflict between the editor's native BFloat16 format and the high precision demand of our tasks. Additionally, we leverage the DiT's global attention for a cost-free joint estimation of depth and normals in a single forward pass, enabling their supervisory signals to mutually enhance each other.
  Without scaling up the training data, FE2E achieves impressive performance improvements in zero-shot monocular depth and normal estimation across multiple datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset and outperforms the DepthAnything series, which is trained on 100$\times$ data. The project page can be accessed \href{https://amap-ml.github.io/FE2E/}{here}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds</title>
<link>https://arxiv.org/abs/2509.04345</link>
<guid>https://arxiv.org/abs/2509.04345</guid>
<content:encoded><![CDATA[
arXiv:2509.04345v1 Announce Type: cross 
Abstract: Speech generation systems can produce remarkably realistic vocalisations that are often indistinguishable from human speech, posing significant authenticity challenges. Although numerous deepfake detection methods have been developed, their effectiveness in real-world environments remains unrealiable due to the domain shift between training and test samples arising from diverse human speech and fast evolving speech synthesis systems. This is not adequately addressed by current datasets, which lack real-world application challenges with diverse and up-to-date audios in both real and deep-fake categories. To fill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale, highly diverse deepfake audio dataset for comprehensive evaluation and robust development of generalised models for deepfake audio detection. It consists of over 4,500 hours of synthetic audio generated by 11 recent TTS models and 10 vocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio clips, making it the largest deepfake audio dataset by scale. Through extensive experiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods trained on existing datasets struggle to generalise to novel deepfake audio samples and suffer from high false positive rates on unseen human voice, underscoring the need for a comprehensive dataset; and ii) these methods trained on AUDETER achieve highly generalised detection performance and significantly reduce detection error rate by 44.1% to 51.6%, achieving an error rate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild dataset, paving the way for training generalist deepfake audio detectors. AUDETER is available on GitHub.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation</title>
<link>https://arxiv.org/abs/2509.04357</link>
<guid>https://arxiv.org/abs/2509.04357</guid>
<content:encoded><![CDATA[
arXiv:2509.04357v1 Announce Type: cross 
Abstract: Automatic speech recognition (ASR) systems struggle with domain-specific named entities, especially homophones. Contextual ASR improves recognition but often fails to capture fine-grained phoneme variations due to limited entity diversity. Moreover, prior methods treat entities as independent tokens, leading to incomplete multi-token biasing. To address these issues, we propose Phoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation (PARCO), which integrates phoneme-aware encoding, contrastive entity disambiguation, entity-level supervision, and hierarchical entity filtering. These components enhance phonetic discrimination, ensure complete entity retrieval, and reduce false positives under uncertainty. Experiments show that PARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English DATA2 under 1,000 distractors, significantly outperforming baselines. PARCO also demonstrates robust gains on out-of-domain datasets like THCHS-30 and LibriSpeech.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer</title>
<link>https://arxiv.org/abs/2509.04362</link>
<guid>https://arxiv.org/abs/2509.04362</guid>
<content:encoded><![CDATA[
arXiv:2509.04362v1 Announce Type: cross 
Abstract: The rapid growth of private car ownership has worsened the urban parking predicament, underscoring the need for accurate and effective parking availability prediction to support urban planning and management. To address key limitations in modeling spatio-temporal dependencies and exploiting multi-source data for parking availability prediction, this study proposes a novel approach with SST-iTransformer. The methodology leverages K-means clustering to establish parking cluster zones (PCZs), extracting and integrating traffic demand characteristics from various transportation modes (i.e., metro, bus, online ride-hailing, and taxi) associated with the targeted parking lots. Upgraded on vanilla iTransformer, SST-iTransformer integrates masking-reconstruction-based pretext tasks for self-supervised spatio-temporal representation learning, and features an innovative dual-branch attention mechanism: Series Attention captures long-term temporal dependencies via patching operations, while Channel Attention models cross-variate interactions through inverted dimensions. Extensive experiments using real-world data from Chengdu, China, demonstrate that SST-iTransformer outperforms baseline deep learning models (including Informer, Autoformer, Crossformer, and iTransformer), achieving state-of-the-art performance with the lowest mean squared error (MSE) and competitive mean absolute error (MAE). Comprehensive ablation studies quantitatively reveal the relative importance of different data sources: incorporating ride-hailing data provides the largest performance gains, followed by taxi, whereas fixed-route transit features (bus/metro) contribute marginally. Spatial correlation analysis further confirms that excluding historical data from correlated parking lots within PCZs leads to substantial performance degradation, underscoring the importance of modeling spatial dependencies.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer</title>
<link>https://arxiv.org/abs/2509.04379</link>
<guid>https://arxiv.org/abs/2509.04379</guid>
<content:encoded><![CDATA[
arXiv:2509.04379v1 Announce Type: cross 
Abstract: Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPA: An Information-Preserving Input Projection Framework for Efficient Foundation Model Adaptation</title>
<link>https://arxiv.org/abs/2509.04398</link>
<guid>https://arxiv.org/abs/2509.04398</guid>
<content:encoded><![CDATA[
arXiv:2509.04398v1 Announce Type: cross 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce adaptation cost by injecting low-rank updates into pretrained weights. However, LoRA's down-projection is randomly initialized and data-agnostic, discarding potentially useful information. Prior analyses show that this projection changes little during training, while the up-projection carries most of the adaptation, making the random input compression a performance bottleneck. We propose IPA, a feature-aware projection framework that explicitly preserves information in the reduced hidden space. In the linear case, we instantiate IPA with algorithms approximating top principal components, enabling efficient projector pretraining with negligible inference overhead. Across language and vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on VTAB-1k, while matching full LoRA performance with roughly half the trainable parameters when the projection is frozen.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in Resume Screening</title>
<link>https://arxiv.org/abs/2509.04404</link>
<guid>https://arxiv.org/abs/2509.04404</guid>
<content:encoded><![CDATA[
arXiv:2509.04404v1 Announce Type: cross 
Abstract: In this study, we conduct a resume-screening experiment (N=528) where people collaborate with simulated AI models exhibiting race-based preferences (bias) to evaluate candidates for 16 high and low status occupations. Simulated AI bias approximates factual and counterfactual estimates of racial bias in real-world AI systems. We investigate people's preferences for White, Black, Hispanic, and Asian candidates (represented through names and affinity groups on quality-controlled resumes) across 1,526 scenarios and measure their unconscious associations between race and status using implicit association tests (IATs), which predict discriminatory hiring decisions but have not been investigated in human-AI collaboration. When making decisions without AI or with AI that exhibits no race-based preferences, people select all candidates at equal rates. However, when interacting with AI favoring a particular group, people also favor those candidates up to 90% of the time, indicating a significant behavioral shift. The likelihood of selecting candidates whose identities do not align with common race-status stereotypes can increase by 13% if people complete an IAT before conducting resume screening. Finally, even if people think AI recommendations are low quality or not important, their decisions are still vulnerable to AI bias under certain circumstances. This work has implications for people's autonomy in AI-HITL scenarios, AI and work, design and evaluation of AI hiring systems, and strategies for mitigating bias in collaborative decision-making tasks. In particular, organizational and regulatory policy should acknowledge the complex nature of AI-HITL decision making when implementing these systems, educating people who use them, and determining which are subject to oversight.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Unified View of Large Language Model Post-Training</title>
<link>https://arxiv.org/abs/2509.04419</link>
<guid>https://arxiv.org/abs/2509.04419</guid>
<content:encoded><![CDATA[
arXiv:2509.04419v1 Announce Type: cross 
Abstract: Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation</title>
<link>https://arxiv.org/abs/2509.04441</link>
<guid>https://arxiv.org/abs/2509.04441</guid>
<content:encoded><![CDATA[
arXiv:2509.04441v1 Announce Type: cross 
Abstract: We introduce perioperation, a paradigm for robotic data collection that sensorizes and records human manipulation while maximizing the transferability of the data to real robots. We implement this paradigm in DEXOP, a passive hand exoskeleton designed to maximize human ability to collect rich sensory (vision + tactile) data for diverse dexterous manipulation tasks in natural environments. DEXOP mechanically connects human fingers to robot fingers, providing users with direct contact feedback (via proprioception) and mirrors the human hand pose to the passive robot hand to maximize the transfer of demonstrated skills to the robot. The force feedback and pose mirroring make task demonstrations more natural for humans compared to teleoperation, increasing both speed and accuracy. We evaluate DEXOP across a range of dexterous, contact-rich tasks, demonstrating its ability to collect high-quality demonstration data at scale. Policies learned with DEXOP data significantly improve task performance per unit time of data collection compared to teleoperation, making DEXOP a powerful tool for advancing robot dexterity. Our project page is at https://dex-op.github.io.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delta Activations: A Representation for Finetuned Large Language Models</title>
<link>https://arxiv.org/abs/2509.04442</link>
<guid>https://arxiv.org/abs/2509.04442</guid>
<content:encoded><![CDATA[
arXiv:2509.04442v1 Announce Type: cross 
Abstract: The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset</title>
<link>https://arxiv.org/abs/2509.04449</link>
<guid>https://arxiv.org/abs/2509.04449</guid>
<content:encoded><![CDATA[
arXiv:2509.04449v1 Announce Type: cross 
Abstract: We present ChronoGraph, a graph-structured multivariate time series forecasting dataset built from real-world production microservices. Each node is a service that emits a multivariate stream of system-level performance metrics, capturing CPU, memory, and network usage patterns, while directed edges encode dependencies between services. The primary task is forecasting future values of these signals at the service level. In addition, ChronoGraph provides expert-annotated incident windows as anomaly labels, enabling evaluation of anomaly detection methods and assessment of forecast robustness during operational disruptions. Compared to existing benchmarks from industrial control systems or traffic and air-quality domains, ChronoGraph uniquely combines (i) multivariate time series, (ii) an explicit, machine-readable dependency graph, and (iii) anomaly labels aligned with real incidents. We report baseline results spanning forecasting models, pretrained time-series foundation models, and standard anomaly detectors. ChronoGraph offers a realistic benchmark for studying structure-aware forecasting and incident-aware evaluation in microservice systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligence Primer</title>
<link>https://arxiv.org/abs/2008.07324</link>
<guid>https://arxiv.org/abs/2008.07324</guid>
<content:encoded><![CDATA[
arXiv:2008.07324v5 Announce Type: replace 
Abstract: Intelligence is a fundamental part of all living things, as well as the foundation for Artificial Intelligence. In this primer we explore the ideas associated with intelligence and, by doing so, understand the implications and constraints and potentially outline the capabilities of future systems. Artificial Intelligence, in the form of Machine Learning, has already had a significant impact on our lives. As an exploration, we journey into different parts of intelligence that appear essential. We hope that people find this helpful in determining the future. Also, during the exploration, we hope to create new thought-provoking questions. Intelligence is not a single weighable quantity but a subject that spans Biology, Physics, Philosophy, Cognitive Science, Neuroscience, Psychology, and Computer Science. The historian Yuval Noah Harari pointed out that engineers and scientists in the future will have to broaden their understandings to include disciplines such as Psychology, Philosophy, and Ethics. Fiction writers have long portrayed engineers and scientists as deficient in these areas. Today, in modern society, the emergence of Artificial Intelligence and legal requirements act as forcing functions to push these broader subjects into the foreground. We start with an introduction to intelligence and move quickly to more profound thoughts and ideas. We call this a Life, the Universe, and Everything primer, after the famous science fiction book by Douglas Adams. Forty-two may be the correct answer, but what are the questions?
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions</title>
<link>https://arxiv.org/abs/2311.17165</link>
<guid>https://arxiv.org/abs/2311.17165</guid>
<content:encoded><![CDATA[
arXiv:2311.17165v4 Announce Type: replace 
Abstract: The concept of rationality is central to the field of artificial intelligence (AI). Whether we are seeking to simulate human reasoning, or trying to achieve bounded optimality, our goal is generally to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in AI, and sets out the open questions in this area. We consider how the understanding of rationality in other fields has influenced its conception within AI, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we examine irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area remains limited. Methods that have up to now been developed for other purposes, namely adversarial scenarios, may be adapted to suit interactions with artificial agents. We further discuss the interplay between human and artificial agents, and the role that rationality plays within this interaction; many questions remain in this area, relating to potentially irrational behaviour of both humans and artificial agents.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents</title>
<link>https://arxiv.org/abs/2406.13923</link>
<guid>https://arxiv.org/abs/2406.13923</guid>
<content:encoded><![CDATA[
arXiv:2406.13923v2 Announce Type: replace 
Abstract: Recent advancements in large multimodal models (LMMs) have leveraged extensive multimodal datasets to enhance capabilities in complex knowledge-driven tasks. However, persistent challenges in perceptual and reasoning errors limit their efficacy, particularly in interpreting intricate visual data and deducing multimodal relationships. To address these issues, we introduce PIN (Paired and INterleaved multimodal documents), a novel data format designed to foster a deeper integration of visual and textual knowledge. The PIN format uniquely combines semantically rich Markdown files, which preserve fine-grained textual structures, with holistic overall images that capture the complete document layout. Following this format, we construct and release two large-scale, open-source datasets: PIN-200M (~200 million documents) and PIN-14M (~14 million), compiled from diverse web and scientific sources in both English and Chinese. To maximize usability, we provide detailed statistical analyses and equip the datasets with quality signals, enabling researchers to easily filter and select data for specific tasks. Our work provides the community with a versatile data format and substantial resources, offering a foundation for new research in pre-training strategies and the development of more powerful knowledge-intensive LMMs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable Belief Model on Quantum Circuits</title>
<link>https://arxiv.org/abs/2410.08949</link>
<guid>https://arxiv.org/abs/2410.08949</guid>
<content:encoded><![CDATA[
arXiv:2410.08949v3 Announce Type: replace 
Abstract: The transferable belief model, as a semantic interpretation of Dempster-Shafer theory, enables agents to perform reasoning and decision making in imprecise and incomplete environments. The model offers distinct semantics for handling unreliable testimonies, allowing for a more reasonable and general process of belief transfer compared to the Bayesian approach. However, because both the belief masses and the structure of focal sets must be considered when updating belief functions-leading to extra computational complexity during reasoning-the transferable belief model has gradually lost favor among researchers in recent developments. In this paper, we implement the transferable belief model on quantum circuits and demonstrate that belief functions offer a more concise and effective alternative to Bayesian approaches within the quantum computing framework. Furthermore, leveraging the unique characteristics of quantum computing, we propose several novel belief transfer approaches. More broadly, this paper introduces a new perspective on basic information representation for quantum AI models, suggesting that belief functions are more suitable than Bayesian approach for handling uncertainty on quantum circuits.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WASP: A Weight-Space Approach to Detecting Learned Spuriousness</title>
<link>https://arxiv.org/abs/2410.18970</link>
<guid>https://arxiv.org/abs/2410.18970</guid>
<content:encoded><![CDATA[
arXiv:2410.18970v4 Announce Type: replace 
Abstract: It is of crucial importance to train machine learning models such that they clearly understand what defines each class in a given task. Though there is a sum of works dedicated to identifying the spurious correlations featured by a dataset that may impact the model's understanding of the classes, all current approaches rely solely on data or error analysis. That is, they cannot point out spurious correlations learned by the model that are not already pointed out by the counterexamples featured in the validation or training sets. We propose a method that transcends this limitation, switching the focus from analyzing a model's predictions to analyzing the model's weights, the mechanism behind the making of the decisions, which proves to be more insightful. Our proposed Weight-space Approach to detecting Spuriousness (WASP) relies on analyzing the weights of foundation models as they drift towards capturing various (spurious) correlations while being fine-tuned on a given dataset. We demonstrate that different from previous works, our method (i) can expose spurious correlations featured by a dataset even when they are not exposed by training or validation counterexamples, (ii) it works for multiple modalities such as image and text, and (iii) it can uncover previously untapped spurious correlations learned by ImageNet-1k classifiers.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing FKG.in: automating Indian food composition analysis</title>
<link>https://arxiv.org/abs/2412.05248</link>
<guid>https://arxiv.org/abs/2412.05248</guid>
<content:encoded><![CDATA[
arXiv:2412.05248v3 Announce Type: replace 
Abstract: This paper presents a novel approach to compute food composition data for Indian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The primary focus is to provide a broad overview of an automated food composition analysis workflow and describe its core functionalities: nutrition data aggregation, food composition analysis, and LLM-augmented information resolution. This workflow aims to complement FKG[.]in and iteratively supplement food composition data from verified knowledge bases. Additionally, this paper highlights the challenges of representing Indian food and accessing food composition data digitally. It also reviews three key sources of food composition data: the Indian Food Composition Tables, the Indian Nutrient Databank, and the Nutritionix API. Furthermore, it briefly outlines how users can interact with the workflow to obtain diet-based health recommendations and detailed food composition information for numerous recipes. We then explore the complex challenges of analyzing Indian recipe information across dimensions such as structure, multilingualism, and uncertainty as well as present our ongoing work on LLM-based solutions to address these issues. The methods proposed in this workshop paper for AI-driven knowledge curation and information resolution are application-agnostic, generalizable, and replicable for any domain.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Science Across Languages: Assessing LLM Multilingual Translation of Scientific Papers</title>
<link>https://arxiv.org/abs/2502.17882</link>
<guid>https://arxiv.org/abs/2502.17882</guid>
<content:encoded><![CDATA[
arXiv:2502.17882v2 Announce Type: replace 
Abstract: Scientific research is inherently global. However, the vast majority of academic journals are published exclusively in English, creating barriers for non-native-English-speaking researchers. In this study, we leverage large language models (LLMs) to translate published scientific articles while preserving their native JATS XML formatting, thereby developing a practical, automated approach for implementation by academic journals. Using our approach, we translate articles across multiple scientific disciplines into 28 languages. To evaluate translation accuracy, we introduce a novel question-and-answer (QA) benchmarking method, in which an LLM generates comprehension-based questions from the original text and then answers them based on the translated text. Our benchmark results show an average performance of 95.9%, showing that the key scientific details are accurately conveyed. In a user study, we translate the scientific papers of 15 researchers into their native languages, finding that the authors consistently found the translations to accurately capture the original information in their articles. Interestingly, a third of the authors found many technical terms "overtranslated," expressing a preference to keep terminology more familiar in English untranslated. Finally, we demonstrate how in-context learning techniques can be used to align translations with domain-specific preferences such as mitigating overtranslation, highlighting the adaptability and utility of LLM-driven scientific translation. The code and translated articles are available at https://hankleid.github.io/ProjectMundo.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Basis of LLM's Decision Making in Social Simulation</title>
<link>https://arxiv.org/abs/2504.11671</link>
<guid>https://arxiv.org/abs/2504.11671</guid>
<content:encoded><![CDATA[
arXiv:2504.11671v2 Announce Type: replace 
Abstract: Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game -- a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMN-Guided Prompting: A Framework for Controlling LLM Behavior</title>
<link>https://arxiv.org/abs/2505.11701</link>
<guid>https://arxiv.org/abs/2505.11701</guid>
<content:encoded><![CDATA[
arXiv:2505.11701v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown considerable potential in automating decision logic within knowledge-intensive processes. However, their effectiveness largely depends on the strategy and quality of prompting. Since decision logic is typically embedded in prompts, it becomes challenging for end users to modify or refine it. Decision Model and Notation (DMN) offers a standardized graphical approach for defining decision logic in a structured, user-friendly manner. This paper introduces a DMN-guided prompting framework that breaks down complex decision logic into smaller, manageable components, guiding LLMs through structured decision pathways. We implemented the framework in a graduate-level course where students submitted assignments. The assignments and DMN models representing feedback instructions served as inputs to our framework. The instructor evaluated the generated feedback and labeled it for performance assessment. Our approach demonstrated promising results, outperforming chain-of-thought (CoT) prompting in our case study. Students also responded positively to the generated feedback, reporting high levels of perceived usefulness in a survey based on the Technology Acceptance Model.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback</title>
<link>https://arxiv.org/abs/2506.03315</link>
<guid>https://arxiv.org/abs/2506.03315</guid>
<content:encoded><![CDATA[
arXiv:2506.03315v2 Announce Type: replace 
Abstract: We study how linear orders can be employed to realise choice functions for which the set of potential choices is restricted, i.e., the possible choice is not possible among the full powerset of all alternatives. In such restricted settings, constructing a choice function via a relation on the alternatives is not always possible. However, we show that one can always construct a choice function via a linear order on sets of alternatives, even when a fallback value is encoded as the minimal element in the linear order. The axiomatics of such choice functions are presented for the general case and the case of union-closed input restrictions. Restricted choice structures have applications in knowledge representation and reasoning, and here we discuss their applications for theory change and abstract argumentation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CP-Bench: Evaluating Large Language Models for Constraint Modelling</title>
<link>https://arxiv.org/abs/2506.06052</link>
<guid>https://arxiv.org/abs/2506.06052</guid>
<content:encoded><![CDATA[
arXiv:2506.06052v2 Announce Type: replace 
Abstract: Constraint Programming (CP) is widely used to solve combinatorial problems, but its core process, namely constraint modelling, requires significant expertise and is considered to be a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) to transform combinatorial problem descriptions into executable constraint models. However, the existing evaluation datasets for constraint modelling are often limited to small, homogeneous, or domain-specific instances, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing CP-Bench, a novel benchmark that includes a diverse set of well-known combinatorial problems sourced from the CP community, structured explicitly for evaluating LLM-driven CP modelling. With this dataset, and given the variety of constraint modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax. Notably, the results show higher performance when modelling with a high-level Python-based framework. Additionally, we systematically evaluate the use of prompt-based and inference-time compute methods across different LLMs, which further increase accuracy, reaching up to 70% on this highly challenging benchmark.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation</title>
<link>https://arxiv.org/abs/2508.00401</link>
<guid>https://arxiv.org/abs/2508.00401</guid>
<content:encoded><![CDATA[
arXiv:2508.00401v2 Announce Type: replace 
Abstract: Theory of Mind (ToM) -- the ability to understand that others can have differing knowledge and goals -- enables agents to reason about others' beliefs while planning their own actions. We present a novel approach to multi-agent cooperation by implementing ToM within active inference. Unlike previous active inference approaches to multi-agent cooperation, our method neither relies on task-specific shared generative models nor requires explicit communication. In our framework, ToM-equipped agents maintain distinct representations of their own and others' beliefs and goals. ToM agents then use an extended and adapted version of the sophisticated inference tree-based planning algorithm to systematically explore joint policy spaces through recursive reasoning. We evaluate our approach through collision avoidance and foraging simulations. Results suggest that ToM agents cooperate better compared to non-ToM counterparts by being able to avoid collisions and reduce redundant efforts. Crucially, ToM agents accomplish this by inferring others' beliefs solely from observable behaviour and considering them when planning their own actions. Our approach shows potential for generalisable and scalable multi-agent systems while providing computational insights into ToM mechanisms.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning</title>
<link>https://arxiv.org/abs/2508.01700</link>
<guid>https://arxiv.org/abs/2508.01700</guid>
<content:encoded><![CDATA[
arXiv:2508.01700v2 Announce Type: replace 
Abstract: Although data visualization is powerful for revealing patterns and communicating insights, creating effective visualizations requires familiarity with authoring tools and often disrupts the analysis flow. While large language models show promise for automatically converting analysis intent into visualizations, existing methods function as black boxes without transparent reasoning processes, which prevents users from understanding design rationales and refining suboptimal outputs. To bridge this gap, we propose integrating Chain-of-Thought (CoT) reasoning into the Natural Language to Visualization (NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for NL2VIS and develop an automatic pipeline to equip existing datasets with structured reasoning steps. Second, we introduce nvBench-CoT, a specialized dataset capturing detailed step-by-step reasoning from ambiguous natural language descriptions to finalized visualizations, which enables state-of-the-art performance when used for model fine-tuning. Third, we develop DeepVIS, an interactive visual interface that tightly integrates with the CoT reasoning process, allowing users to inspect reasoning steps, identify errors, and make targeted adjustments to improve visualization outcomes. Quantitative benchmark evaluations, two use cases, and a user study collectively demonstrate that our CoT framework effectively enhances NL2VIS quality while providing insightful reasoning steps to users.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending FKG.in: Towards a Food Claim Traceability Network</title>
<link>https://arxiv.org/abs/2508.16117</link>
<guid>https://arxiv.org/abs/2508.16117</guid>
<content:encoded><![CDATA[
arXiv:2508.16117v2 Announce Type: replace 
Abstract: The global food landscape is rife with scientific, cultural, and commercial claims about what foods are, what they do, what they should not do, or should not do. These range from rigorously studied health benefits (probiotics improve gut health) and misrepresentations (soaked almonds make one smarter) to vague promises (superfoods boost immunity) and culturally rooted beliefs (cold foods cause coughs). Despite their widespread influence, the infrastructure for tracing, verifying, and contextualizing these claims remains fragmented and underdeveloped. In this paper, we propose a Food Claim-Traceability Network (FCN) as an extension of FKG[.]in, a knowledge graph of Indian food that we have been incrementally building. We also present the ontology design and the semi-automated knowledge curation workflow that we used to develop a proof of concept of FKG[.]in-FCN using Reddit data and Large Language Models. FCN integrates curated data inputs, structured schemas, and provenance-aware pipelines for food-related claim extraction and validation. While directly linked to the Indian food knowledge graph as an application, our methodology remains application-agnostic and adaptable to other geographic, culinary, or regulatory settings. By modeling food claims and their traceability in a structured, verifiable, and explainable way, we aim to contribute to more transparent and accountable food knowledge ecosystems, supporting researchers, policymakers, and most importantly, everyday consumers in navigating a world saturated with dietary assertions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomation, Not Automation: Activities and Needs of European Fact-checkers as a Basis for Designing Human-Centered AI Systems</title>
<link>https://arxiv.org/abs/2211.12143</link>
<guid>https://arxiv.org/abs/2211.12143</guid>
<content:encoded><![CDATA[
arXiv:2211.12143v3 Announce Type: replace-cross 
Abstract: To mitigate the negative effects of false information more effectively, the development of Artificial Intelligence (AI) systems to assist fact-checkers is needed. Nevertheless, the lack of focus on the needs of these stakeholders results in their limited acceptance and skepticism toward automating the whole fact-checking process. In this study, we conducted semi-structured in-depth interviews with Central European fact-checkers. Their activities and problems were analyzed using iterative content analysis. The most significant problems were validated with a survey of European fact-checkers, in which we collected 24 responses from 20 countries, i.e., 62% of active European signatories of the International Fact-Checking Network (IFCN). Our contributions include an in-depth examination of the variability of fact-checking work in non-English-speaking regions, which still remained largely uncovered. By aligning them with the knowledge from prior studies, we created conceptual models that help to understand the fact-checking processes. In addition, we mapped our findings on the fact-checkers' activities and needs to the relevant tasks for AI research, while providing a discussion on three AI tasks that were not covered by previous similar studies. The new opportunities identified for AI researchers and developers have implications for the focus of AI research in this domain.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style Transfer to Calvin and Hobbes comics using Stable Diffusion</title>
<link>https://arxiv.org/abs/2312.03993</link>
<guid>https://arxiv.org/abs/2312.03993</guid>
<content:encoded><![CDATA[
arXiv:2312.03993v2 Announce Type: replace-cross 
Abstract: This project report summarizes our journey to perform stable diffusion fine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to convert any given input image into the comic style of Calvin and Hobbes, essentially performing style transfer. We train stable-diffusion-v1.5 using Low Rank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The diffusion itself is handled by a Variational Autoencoder (VAE), which is a U-net. Our results were visually appealing for the amount of training time and the quality of input data that went into training.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion on language model encodings for protein sequence generation</title>
<link>https://arxiv.org/abs/2403.03726</link>
<guid>https://arxiv.org/abs/2403.03726</guid>
<content:encoded><![CDATA[
arXiv:2403.03726v3 Announce Type: replace-cross 
Abstract: Protein sequence design has seen significant advances through discrete diffusion and autoregressive approaches, yet the potential of continuous diffusion remains underexplored. Here, we present DiMA, a latent diffusion framework that operates on protein language model representations. Through systematic exploration of architectural choices and diffusion components, we develop a robust methodology that generalizes across multiple protein encoders ranging from 8M to 3B parameters. We demonstrate that our framework achieves consistently high performance across sequence-only (ESM-2, ESMc), dual-decodable (CHEAP), and multimodal (SaProt) representations using the same architecture and training approach. We extensively evaluate existing methods alongside DiMA using multiple metrics across two protein modalities, covering quality, diversity, novelty, and distribution matching of generated proteins. DiMA consistently produces novel, high-quality and diverse protein sequences and achieves strong results compared to baselines such as autoregressive, discrete diffusion and flow matching language models. The model demonstrates versatile functionality, supporting conditional generation tasks including protein family-generation, motif scaffolding and infilling, and fold-specific sequence design. This work provides a universal continuous diffusion framework for protein sequence generation, offering both architectural insights and practical applicability across various protein design scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming</title>
<link>https://arxiv.org/abs/2405.08965</link>
<guid>https://arxiv.org/abs/2405.08965</guid>
<content:encoded><![CDATA[
arXiv:2405.08965v5 Announce Type: replace-cross 
Abstract: Software development is shifting from traditional programming to AI-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity. This paper presents Meaning-Typed Programming (MTP), a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the (1) by operator for seamless LLM invocation, (2) MT-IR, a meaning-based intermediate representation for semantic extraction, and (3) MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a programming language that supersets Python, and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. MTP significantly reduces development complexity, lines of code modifications needed, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches. Our user study shows that developers using MTP completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks. Moreover, MTP demonstrates resilience even when up to 50% of naming conventions are degraded, demonstrating robustness to suboptimal code. MTP is developed as part of the Jaseci open-source project, and is available under the module byLLM.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unisolver: PDE-Conditional Transformers Towards Universal Neural PDE Solvers</title>
<link>https://arxiv.org/abs/2405.17527</link>
<guid>https://arxiv.org/abs/2405.17527</guid>
<content:encoded><![CDATA[
arXiv:2405.17527v5 Announce Type: replace-cross 
Abstract: Deep models have recently emerged as promising tools to solve partial differential equations (PDEs), known as neural PDE solvers. While neural solvers trained from either simulation data or physics-informed loss can solve PDEs reasonably well, they are mainly restricted to a few instances of PDEs, e.g. a certain equation with a limited set of coefficients. This limits their generalization to diverse PDEs, preventing them from being practical surrogate models of numerical solvers. In this paper, we present Unisolver, a novel Transformer model trained on diverse data and conditioned on diverse PDEs, aiming towards a universal neural PDE solver capable of solving a wide scope of PDEs. Instead of purely scaling up data and parameters, Unisolver stems from the theoretical analysis of the PDE-solving process. Inspired by the mathematical structure of PDEs that a PDE solution is fundamentally governed by a series of PDE components such as equation symbols and boundary conditions, we define a complete set of PDE components and flexibly embed them as domain-wise and point-wise deep conditions for Transformer PDE solvers. Integrating physical insights with recent Transformer advances, Unisolver achieves consistent state-of-the-art on three challenging large-scale benchmarks, showing impressive performance and generalizability. Code is available at https://github.com/thuml/Unisolver.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference</title>
<link>https://arxiv.org/abs/2407.15161</link>
<guid>https://arxiv.org/abs/2407.15161</guid>
<content:encoded><![CDATA[
arXiv:2407.15161v4 Announce Type: replace-cross 
Abstract: Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from partial observations remains a critical challenge in robot learning. Prior generative methods struggle to model the intricate grasp distribution of dexterous hands and often fail to reason about shape uncertainty inherent in partial point clouds, leading to unreliable or overly conservative grasps. We propose FFHFlow, a flow-based variational framework that generates diverse, robust multi-finger grasps while explicitly quantifying perceptual uncertainty in the partial point clouds. Our approach leverages a normalizing flow-based deep latent variable model to learn a hierarchical grasp manifold, overcoming the mode collapse and rigid prior limitations of conditional Variational Autoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of flows, FFHFlow introspects shape uncertainty in partial observations and identifies novel object structures, enabling risk-aware grasp synthesis. To further enhance reliability, we integrate a discriminative grasp evaluator with the flow likelihoods, formulating an uncertainty-aware ranking strategy that prioritizes grasps robust to shape ambiguity. Extensive experiments in simulation and real-world setups demonstrate that FFHFlow outperforms state-of-the-art baselines (including diffusion models) in grasp diversity and success rate, while achieving run-time efficient sampling. We also showcase its practical value in cluttered and confined environments, where diversity-driven sampling excels by mitigating collisions (Project Page: https://sites.google.com/view/ffhflow/home/).
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long Input Sequence Network for Long Time Series Forecasting</title>
<link>https://arxiv.org/abs/2407.15869</link>
<guid>https://arxiv.org/abs/2407.15869</guid>
<content:encoded><![CDATA[
arXiv:2407.15869v2 Announce Type: replace-cross 
Abstract: Short fixed-length inputs are the main bottleneck of deep learning methods in long time-series forecasting tasks. Prolonging input length causes overfitting, rapidly deteriorating accuracy. Our research indicates that the overfitting is a combination reaction of the multi-scale pattern coupling in time series and the fixed focusing scale of current models. First, we find that the patterns exhibited by a time series across various scales are reflective of its multi-periodic nature, where each scale corresponds to specific period length. Second, We find that the token size predominantly dictates model behavior, as it determines the scale at which the model focuses and the context size it can accommodate. Our idea is to decouple the multi-scale temporal patterns of time series and to model each pattern with its corresponding period length as token size. We introduced a novel series-decomposition module(MPSD), and a Multi-Token Pattern Recognition neural network(MTPR), enabling the model to handle \textit{inputs up to $10\times$ longer}. Sufficient context enhances performance(\textit{38% maximum precision improvement}), and the decoupling approach offers \textit{Low complexity($0.22\times$ cost)} and \textit{high interpretability}.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPETIII: The Tracer Frontier. What Frontier?</title>
<link>https://arxiv.org/abs/2410.02807</link>
<guid>https://arxiv.org/abs/2410.02807</guid>
<content:encoded><![CDATA[
arXiv:2410.02807v2 Announce Type: replace-cross 
Abstract: For the last three years, the AutoPET competition gathered the medical imaging community around a hot topic: lesion segmentation on Positron Emitting Tomography (PET) scans. Each year a different aspect of the problem is presented; in 2024 the multiplicity of existing and used tracers was at the core of the challenge. Specifically, this year's edition aims to develop a fully automatic algorithm capable of performing lesion segmentation on a PET/CT scan, without knowing the tracer, which can either be a FDG or PSMA-based tracer. In this paper we describe how we used the nnUNetv2 framework to train two sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion segmentation as well as a MIP-CNN to choose which set of models to use for segmentation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from 10 Demos: Generalisable and Sample-Efficient Policy Learning with Oriented Affordance Frames</title>
<link>https://arxiv.org/abs/2410.12124</link>
<guid>https://arxiv.org/abs/2410.12124</guid>
<content:encoded><![CDATA[
arXiv:2410.12124v2 Announce Type: replace-cross 
Abstract: Imitation learning has unlocked the potential for robots to exhibit highly dexterous behaviours. However, it still struggles with long-horizon, multi-object tasks due to poor sample efficiency and limited generalisation. Existing methods require a substantial number of demonstrations to cover possible task variations, making them costly and often impractical for real-world deployment. We address this challenge by introducing oriented affordance frames, a structured representation for state and action spaces that improves spatial and intra-category generalisation and enables policies to be learned efficiently from only 10 demonstrations. More importantly, we show how this abstraction allows for compositional generalisation of independently trained sub-policies to solve long-horizon, multi-object tasks. To seamlessly transition between sub-policies, we introduce the notion of self-progress prediction, which we directly derive from the duration of the training demonstrations. We validate our method across three real-world tasks, each requiring multi-step, multi-object interactions. Despite the small dataset, our policies generalise robustly to unseen object appearances, geometries, and spatial arrangements, achieving high success rates without reliance on exhaustive training data. Video demonstration can be found on our project page: https://affordance-policy.github.io/.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust training of implicit generative models for multivariate and heavy-tailed distributions with an invariant statistical loss</title>
<link>https://arxiv.org/abs/2410.22381</link>
<guid>https://arxiv.org/abs/2410.22381</guid>
<content:encoded><![CDATA[
arXiv:2410.22381v2 Announce Type: replace-cross 
Abstract: Traditional implicit generative models are capable of learning highly complex data distributions. However, their training involves distinguishing real data from synthetically generated data using adversarial discriminators, which can lead to unstable training dynamics and mode dropping issues. In this work, we build on the \textit{invariant statistical loss} (ISL) method introduced in \cite{de2024training}, and extend it to handle heavy-tailed and multivariate data distributions.
  The data generated by many real-world phenomena can only be properly characterised using heavy-tailed probability distributions, and traditional implicit methods struggle to effectively capture their asymptotic behavior. To address this problem, we introduce a generator trained with ISL, that uses input noise from a generalised Pareto distribution (GPD). We refer to this generative scheme as Pareto-ISL for conciseness. Our experiments demonstrate that Pareto-ISL accurately models the tails of the distributions while still effectively capturing their central characteristics.
  The original ISL function was conceived for 1D data sets. When the actual data is $n$-dimensional, a straightforward extension of the method was obtained by targeting the $n$ marginal distributions of the data. This approach is computationally infeasible and ineffective in high-dimensional spaces. To overcome this, we extend the 1D approach using random projections and define a new loss function suited for multivariate data, keeping problems tractable by adjusting the number of projections. We assess its performance in multidimensional generative modeling and explore its potential as a pretraining technique for generative adversarial networks (GANs) to prevent mode collapse, reporting promising results and highlighting its robustness across various hyperparameter settings.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Calibration Error in Neural Networks Through Evidence-Based Theory</title>
<link>https://arxiv.org/abs/2411.00265</link>
<guid>https://arxiv.org/abs/2411.00265</guid>
<content:encoded><![CDATA[
arXiv:2411.00265v3 Announce Type: replace-cross 
Abstract: Trustworthiness in neural networks is crucial for their deployment in critical applications, where reliability, confidence, and uncertainty play pivotal roles in decision-making. Traditional performance metrics such as accuracy and precision fail to capture these aspects, particularly in cases where models exhibit overconfidence. To address these limitations, this paper introduces a novel framework for quantifying the trustworthiness of neural networks by incorporating subjective logic into the evaluation of Expected Calibration Error (ECE). This method provides a comprehensive measure of trust, disbelief, and uncertainty by clustering predicted probabilities and fusing opinions using appropriate fusion operators. We demonstrate the effectiveness of this approach through experiments on MNIST and CIFAR-10 datasets, where post-calibration results indicate improved trustworthiness. The proposed framework offers a more interpretable and nuanced assessment of AI models, with potential applications in sensitive domains such as healthcare and autonomous systems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolb-Based Experiential Learning for Generalist Agents with Human-Level Kaggle Data Science Performance</title>
<link>https://arxiv.org/abs/2411.03562</link>
<guid>https://arxiv.org/abs/2411.03562</guid>
<content:encoded><![CDATA[
arXiv:2411.03562v2 Announce Type: replace-cross 
Abstract: Human expertise emerges through iterative cycles of interaction, reflection, and internal model updating, which are central to cognitive theories such as Kolb's experiential learning and Vygotsky's zone of proximal development. In contrast, current AI systems, particularly LLM agents, rely on static pre-training or rigid workflows, lacking mechanisms for continual adaptation. Recent studies identified early cognitive traits in LLM agents (reflection, revision, and self-correction) suggesting foundational elements of human-like experiential learning. Thus the key question: Can we design LLM agents capable of structured, cognitively grounded learning similar to human processes? In response, we propose a computational framework of Kolb's learning cycle with Vygotsky's ZPD for autonomous agents. Our architecture separates extrinsic (environment interaction) and intrinsic (internal reflection/abstraction) functions, enabling cognitively grounded scaffolded learning, where the agent initially learns within structured environments, followed by open-ended generalisation. This approach empowers agents to master complex tasks ; domains that traditional fine-tuning or simple reflective methods could not tackle effectively. Its potential is powerfully demonstrated via direct comparison with humans in real-world Kaggle data science competitions. Learning fully automated data science code generation across 81 tasks, our system, Agent K, demonstrated the ability to perform the entire workflow autonomously, achieving an Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2% among 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals level performance - including 4 gold and 4 silver on prize-awarding competitions - Agent K is the 1st AI system to successfully integrate Kolb- and Vygotsky-inspired human cognitive learning, marking a major step toward generalist AI.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACING: Actor-Critic for Instruction Learning in Black-Box LLMs</title>
<link>https://arxiv.org/abs/2411.12736</link>
<guid>https://arxiv.org/abs/2411.12736</guid>
<content:encoded><![CDATA[
arXiv:2411.12736v2 Announce Type: replace-cross 
Abstract: The effectiveness of Large Language Models (LLMs) in solving tasks depends significantly on the quality of their instructions, which often require substantial human effort to craft. This underscores the need for automated instruction optimization. However, optimizing instructions is particularly challenging when working with black-box LLMs, where model parameters and gradients are inaccessible. We introduce ACING, an actor-critic reinforcement learning framework that formulates instruction optimization as a stateless, continuous-action problem, enabling exploration of infinite instruction spaces using only black-box feedback. ACING automatically discovers prompts that outperform human-written prompts in 76% of instruction-induction tasks, with gains of up to 33 points and a 10-point median improvement over the best automatic baseline in 33 tasks spanning instruction-induction, summarization, and chain-of-thought reasoning. Extensive ablations highlight its robustness and efficiency. An implementation of ACING is available at https://github.com/salmakh1/ACING.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defending LVLMs Against Vision Attacks through Partial-Perception Supervision</title>
<link>https://arxiv.org/abs/2412.12722</link>
<guid>https://arxiv.org/abs/2412.12722</guid>
<content:encoded><![CDATA[
arXiv:2412.12722v2 Announce Type: replace-cross 
Abstract: Recent studies have raised significant concerns regarding the vulnerability of Large Vision Language Models (LVLMs) to maliciously injected or perturbed input images, which can mislead their responses. Existing defense methods show that such vision attacks are sensitive to image modifications especially cropping, using majority voting across responses of modified images as corrected responses. However, these modifications often result in partial images and distort the semantics, which reduces response quality on clean images after voting. Instead of directly using responses from partial images for voting, we investigate using them to supervise the LVLM's responses to the original images. We propose a black-box, training-free method called DPS (Defense through Partial-Perception Supervision). In this approach, the model is prompted using the responses generated by a model that perceives only a partial image. With DPS, the model can adjust its response based on partial image understanding when under attack, while confidently maintaining its original response for clean input. Our findings show that the weak model can supervise the strong model: when faced with an attacked input, the strong model becomes less confident and adjusts its response based on the weak model's partial understanding, effectively defending against the attack. With clean input, it confidently maintains its original response. Empirical experiments show our method outperforms the baseline, cutting the average attack success rate by 76.3% across six datasets on three popular models.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Context Bottleneck on Long Time Series Forecasting</title>
<link>https://arxiv.org/abs/2412.16572</link>
<guid>https://arxiv.org/abs/2412.16572</guid>
<content:encoded><![CDATA[
arXiv:2412.16572v2 Announce Type: replace-cross 
Abstract: Long-term time-series forecasting is essential for planning and decision-making in economics, energy, and transportation, where long foresight is required. To obtain such long foresight, models must be both efficient and effective in processing long sequence. Recent advancements have enhanced the efficiency of these models; however, the challenge of effectively leveraging longer sequences persists. This is primarily due to the tendency of these models to overfit when presented with extended inputs, necessitating the use of shorter input lengths to maintain tolerable error margins. In this work, we investigate the multiscale modeling method and propose the Logsparse Decomposable Multiscaling (LDM) framework for the efficient and effective processing of long sequences. We demonstrate that by decoupling patterns at different scales in time series, we can enhance predictability by reducing non-stationarity, improve efficiency through a compact long input representation, and simplify the architecture by providing clear task assignments. Experimental results demonstrate that LDM not only outperforms all baselines in long-term forecasting benchmarks, but also reducing both training time and memory costs.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models</title>
<link>https://arxiv.org/abs/2501.13958</link>
<guid>https://arxiv.org/abs/2501.13958</guid>
<content:encoded><![CDATA[
arXiv:2501.13958v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks, yet their application to specialized domains remains challenging due to the need for deep expertise. Retrieval-Augmented generation (RAG) has emerged as a promising solution to customize LLMs for professional fields by seamlessly integrating external knowledge bases, enabling real-time access to domain-specific expertise during inference. Despite its potential, traditional RAG systems, based on flat text retrieval, face three critical challenges: (i) complex query understanding in professional contexts, (ii) difficulties in knowledge integration across distributed sources, and (iii) system efficiency bottlenecks at scale. This survey presents a systematic analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new paradigm that revolutionizes domain-specific LLM applications. GraphRAG addresses traditional RAG limitations through three key innovations: (i) graph-structured knowledge representation that explicitly captures entity relationships and domain hierarchies, (ii) efficient graph-based retrieval techniques that enable context-preserving knowledge retrieval with multihop reasoning ability, and (iii) structure-aware knowledge integration algorithms that leverage retrieved knowledge for accurate and logical coherent generation of LLMs. In this survey, we systematically analyze the technical foundations of GraphRAG and examine current implementations across various professional domains, identifying key technical challenges and promising research directions. All the related resources of GraphRAG, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphRAG.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Histogram-based Outlier Score (EHBOS)</title>
<link>https://arxiv.org/abs/2502.05719</link>
<guid>https://arxiv.org/abs/2502.05719</guid>
<content:encoded><![CDATA[
arXiv:2502.05719v3 Announce Type: replace-cross 
Abstract: Histogram-Based Outlier Score (HBOS) is a widely used outlier or anomaly detection method known for its computational efficiency and simplicity. However, its assumption of feature independence limits its ability to detect anomalies in datasets where interactions between features are critical. In this paper, we propose the Extended Histogram-Based Outlier Score (EHBOS), which enhances HBOS by incorporating two-dimensional histograms to capture dependencies between feature pairs. This extension allows EHBOS to identify contextual and dependency-driven anomalies that HBOS fails to detect. We evaluate EHBOS on 17 benchmark datasets, demonstrating its effectiveness and robustness across diverse anomaly detection scenarios. EHBOS outperforms HBOS on several datasets, particularly those where feature interactions are critical in defining the anomaly structure, achieving notable improvements in ROC AUC. These results highlight that EHBOS can be a valuable extension to HBOS, with the ability to model complex feature dependencies. EHBOS offers a powerful new tool for anomaly detection, particularly in datasets where contextual or relational anomalies play a significant role.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is an Ultra Large Natural Image-Based Foundation Model Superior to a Retina-Specific Model for Detecting Ocular and Systemic Diseases?</title>
<link>https://arxiv.org/abs/2502.06289</link>
<guid>https://arxiv.org/abs/2502.06289</guid>
<content:encoded><![CDATA[
arXiv:2502.06289v2 Announce Type: replace-cross 
Abstract: The advent of foundation models (FMs) is transforming medical domain. In ophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4 million natural images and 1.6 million retinal images, has demonstrated high adaptability across clinical applications. Conversely, DINOv2, a general-purpose vision FM pre-trained on 142 million natural images, has shown promise in non-medical domains. However, its applicability to clinical tasks remains underexplored. To address this, we conducted head-to-head evaluations by fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular disease detection and systemic disease prediction tasks, across eight standardized open-source ocular datasets, as well as the Moorfields AlzEye and the UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting diabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets, all P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In glaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940, P<0.001). Conversely, RETFound achieved superior performance over all DINOv2 models in predicting heart failure, myocardial infarction, and ischaemic stroke (AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even with 10% of the fine-tuning data. These findings showcase the distinct scenarios where general-purpose and domain-specific FMs excel, highlighting the importance of aligning FM selection with task-specific requirements to optimise clinical performance.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Embedding Sampling Method for Diverse Captioning</title>
<link>https://arxiv.org/abs/2502.10118</link>
<guid>https://arxiv.org/abs/2502.10118</guid>
<content:encoded><![CDATA[
arXiv:2502.10118v2 Announce Type: replace-cross 
Abstract: Image Captioning for state-of-the-art VLMs has significantly improved over time; however, this comes at the cost of increased computational complexity, making them less accessible for resource-constrained applications such as mobile devices and assistive technologies. Alternatively, comparably smaller VLMs prioritize high-level scene descriptions, overlooking finer details that contribute to a richer understanding of an image. In this paper, we introduce a training-free framework that enhances caption diversity and informativeness by explicitly attending to distinct image regions using a comparably small VLM, BLIP, as the backbone. Our approach leverages structured segmentation to produce hierarchical representations that capture both global and localized semantics. Without requiring additional model training, we demonstrate that our method allows smaller VLMs to achieve performance comparable to larger models in terms of image-caption alignment, semantic integrity, and diversity. We evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets, achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset, respectively, while maintaining strong image-caption relevancy and semantic integrity with the human-annotated captions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoDiff: Conditional Diffusion Model for Collaborative 3D Object Detection</title>
<link>https://arxiv.org/abs/2502.14891</link>
<guid>https://arxiv.org/abs/2502.14891</guid>
<content:encoded><![CDATA[
arXiv:2502.14891v3 Announce Type: replace-cross 
Abstract: Collaborative 3D object detection holds significant importance in the field of autonomous driving, as it greatly enhances the perception capabilities of each individual agent by facilitating information exchange among multiple agents. However, in practice, due to pose estimation errors and time delays, the fusion of information across agents often results in feature representations with spatial and temporal noise, leading to detection errors. Diffusion models naturally have the ability to denoise noisy samples to the ideal data, which motivates us to explore the use of diffusion models to address the noise problem between multi-agent systems. In this work, we propose CoDiff, a novel robust collaborative perception framework that leverages the potential of diffusion models to generate more comprehensive and clearer feature representations. To the best of our knowledge, this is the first work to apply diffusion models to multi-agent collaborative perception. Specifically, we project high-dimensional feature map into the latent space of a powerful pre-trained autoencoder. Within this space, individual agent information serves as a condition to guide the diffusion model's sampling. This process denoises coarse feature maps and progressively refines the fused features. Experimental study on both simulated and real-world datasets demonstrates that the proposed framework CoDiff consistently outperforms existing relevant methods in terms of the collaborative object detection performance, and exhibits highly desired robustness when the pose and delay information of agents is with high-level noise. The code is released at https://github.com/HuangZhe885/CoDiff
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response</title>
<link>https://arxiv.org/abs/2502.18452</link>
<guid>https://arxiv.org/abs/2502.18452</guid>
<content:encoded><![CDATA[
arXiv:2502.18452v3 Announce Type: replace-cross 
Abstract: During Human Robot Interactions in disaster relief scenarios, Large Language Models (LLMs) have the potential for substantial physical reasoning to assist in mission objectives. However, these reasoning capabilities are often found only in larger models, which are not currently reasonable to deploy on robotic systems due to size constraints. To meet our problem space requirements, we introduce a dataset and pipeline to create Field Reasoning and Instruction Decoding Agent (FRIDA) models. In our pipeline, domain experts and linguists combine their knowledge to make high-quality, few-shot prompts used to generate synthetic data for fine-tuning. We hand-curate datasets for this few-shot prompting and for evaluation to improve LLM reasoning on both general and disaster-specific objects. We concurrently run an ablation study to understand which kinds of synthetic data most affect performance. We fine-tune several small instruction-tuned models and find that ablated FRIDA models only trained on objects' physical state and function data outperformed both the FRIDA models trained on all synthetic data and the base models in our evaluation. We demonstrate that the FRIDA pipeline is capable of instilling physical common sense with minimal data.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KNighter: Transforming Static Analysis with LLM-Synthesized Checkers</title>
<link>https://arxiv.org/abs/2503.09002</link>
<guid>https://arxiv.org/abs/2503.09002</guid>
<content:encoded><![CDATA[
arXiv:2503.09002v3 Announce Type: replace-cross 
Abstract: Static analysis is a powerful technique for bug detection in critical systems like operating system kernels. However, designing and implementing static analyzers is challenging, time-consuming, and typically limited to predefined bug patterns. While large language models (LLMs) have shown promise for static analysis, directly applying them to scan large systems remains impractical due to computational constraints and contextual limitations.
  We present KNighter, the first approach that unlocks scalable LLM-based static analysis by automatically synthesizing static analyzers from historical bug patterns. Rather than using LLMs to directly analyze massive systems, our key insight is leveraging LLMs to generate specialized static analyzers guided by historical patch knowledge. KNighter implements this vision through a multi-stage synthesis pipeline that validates checker correctness against original patches and employs an automated refinement process to iteratively reduce false positives. Our evaluation on the Linux kernel demonstrates that KNighter generates high-precision checkers capable of detecting diverse bug patterns overlooked by existing human-written analyzers. To date, KNighter-synthesized checkers have discovered 92 new, critical, long-latent bugs (average 4.3 years) in the Linux kernel; 77 are confirmed, 57 fixed, and 30 have been assigned CVE numbers. This work establishes an entirely new paradigm for scalable, reliable, and traceable LLM-based static analysis for real-world systems via checker synthesis.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond holography: the entropic quantum gravity foundations of image processing</title>
<link>https://arxiv.org/abs/2503.14048</link>
<guid>https://arxiv.org/abs/2503.14048</guid>
<content:encoded><![CDATA[
arXiv:2503.14048v2 Announce Type: replace-cross 
Abstract: Recently, thanks to the development of artificial intelligence (AI) there is increasing scientific attention in establishing the connections between theoretical physics and AI. Traditionally, these connections have been focusing mostly on the relation between string theory and image processing and involve important theoretical paradigms such as holography. Recently G. Bianconi has formulated the Gravity from Entropy (GfE) approach to quantum gravity in which gravity is derived from the geometric quantum relative entropy (GQRE) between two metrics associated with the Lorentzian spacetime. Here it is demonstrated that the famous Perona-Malik algorithm for image processing is the gradient flow of the GfE action in its simple warm-up scenario. Specifically, this algorithm is the outcome of the minimization of the GQRE between two Euclidean metrics: the one of the support of the image and the one induced by the image. As the Perona-Malik algorithm is known to preserve sharp contours, this implies that the GfE action, does not in general lead to uniform images upon iteration of the gradient flow dynamics as it would be intuitively expected from entropic actions maximising classical entropies. Rather, the outcome of the minimization of the GQRE is compatible with the preservation of complex structures. These results provide the geometrical and information theory foundations for the Perona-Malik algorithm and might contribute to establish deeper connections between GfE, machine learning and brain research.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Offline Imitation Learning Through State-level Trajectory Stitching</title>
<link>https://arxiv.org/abs/2503.22524</link>
<guid>https://arxiv.org/abs/2503.22524</guid>
<content:encoded><![CDATA[
arXiv:2503.22524v2 Announce Type: replace-cross 
Abstract: Imitation learning (IL) has proven effective for enabling robots to acquire visuomotor skills through expert demonstrations. However, traditional IL methods are limited by their reliance on high-quality, often scarce, expert data, and suffer from covariate shift. To address these challenges, recent advances in offline IL have incorporated suboptimal, unlabeled datasets into the training. In this paper, we propose a novel approach to enhance policy learning from mixed-quality offline datasets by leveraging task-relevant trajectory fragments and rich environmental dynamics. Specifically, we introduce a state-based search framework that stitches state-action pairs from imperfect demonstrations, generating more diverse and informative training trajectories. Experimental results on standard IL benchmarks and real-world robotic tasks showcase that our proposed method significantly improves both generalization and performance.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RBT4DNN: Requirements-based Testing of Neural Networks</title>
<link>https://arxiv.org/abs/2504.02737</link>
<guid>https://arxiv.org/abs/2504.02737</guid>
<content:encoded><![CDATA[
arXiv:2504.02737v3 Announce Type: replace-cross 
Abstract: Testing allows developers to determine whether a system functions as expected. When such systems include deep neural networks (DNNs), Testing becomes challenging, as DNNs approximate functions for which the formalization of functional requirements is intractable. This prevents the application of well-developed approaches to requirements-based testing to DNNs.
  To address this, we propose a requirements-based testing method (RBT4DNN) that uses natural language requirements statements. These statements use a glossary of terms to define a semantic feature space that can be leveraged for test input generation. RBT4DNN formalizes preconditions of functional requirements as logical combinations of those semantic features. Training data matching these feature combinations can be used to fine-tune a generative model to reliably produce test inputs satisfying the precondition. Executing these tests on a trained DNN enables comparing its output to the expected requirement postcondition behavior. We propose two use cases for RBT4DNN: (1) given requirements defining DNN correctness properties, RBT4DNN comprises a novel approach for detecting faults, and (2) during development, requirements-guided exploration of model behavior can provide developers with feedback on model generalization. Our further evaluation shows that RBT4DNN-generated tests are realistic, diverse, and aligned with requirement preconditions, enabling targeted analysis of model behavior and effective fault detection.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable Mask Transformer: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation</title>
<link>https://arxiv.org/abs/2504.05774</link>
<guid>https://arxiv.org/abs/2504.05774</guid>
<content:encoded><![CDATA[
arXiv:2504.05774v2 Announce Type: replace-cross 
Abstract: Recent advances in Vision Transformers (ViTs) have set new benchmarks in semantic segmentation. However, when adapting pretrained ViTs to new target domains, significant performance degradation often occurs due to distribution shifts, resulting in suboptimal global attention. Since self-attention mechanisms are inherently data-driven, they may fail to effectively attend to key objects when source and target domains exhibit differences in texture, scale, or object co-occurrence patterns. While global and patch-level domain adaptation methods provide partial solutions, region-level adaptation with dynamically shaped regions is crucial due to spatial heterogeneity in transferability across different image areas. We present Transferable Mask Transformer (TMT), a novel region-level adaptation framework for semantic segmentation that aligns cross-domain representations through spatial transferability analysis. TMT consists of two key components: (1) An Adaptive Cluster-based Transferability Estimator (ACTE) that dynamically segments images into structurally and semantically coherent regions for localized transferability assessment, and (2) A Transferable Masked Attention (TMA) module that integrates region-specific transferability maps into ViTs' attention mechanisms, prioritizing adaptation in regions with low transferability and high semantic uncertainty. Comprehensive evaluations across 20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2% MIoU improvement over vanilla fine-tuning and a 1.28% increase compared to state-of-the-art baselines. The source code will be publicly available.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks</title>
<link>https://arxiv.org/abs/2505.03522</link>
<guid>https://arxiv.org/abs/2505.03522</guid>
<content:encoded><![CDATA[
arXiv:2505.03522v2 Announce Type: replace-cross 
Abstract: Deep learning has substantially advanced the field of Single Image Super-Resolution (SISR). However, existing research has predominantly focused on raw performance gains, with little attention paid to quantifying the transferability of architectural components. In this paper, we introduce the concept of "Universality" and its associated definitions, which extend the traditional notion of "Generalization" to encompass the ease of transferability of modules. We then propose the Universality Assessment Equation (UAE), a metric that quantifies how readily a given module can be transplanted across models and reveals the combined influence of multiple existing metrics on transferability. Guided by the UAE results of standard residual blocks and other plug-and-play modules, we further design two optimized modules: the Cycle Residual Block (CRB) and the Depth-Wise Cycle Residual Block (DCRB). Through comprehensive experiments on natural-scene benchmarks, remote-sensing datasets, and other low-level tasks, we demonstrate that networks embedded with the proposed plug-and-play modules outperform several state-of-the-art methods, achieving a PSNR improvement of up to 0.83 dB or enabling a 71.3% reduction in parameters with negligible loss in reconstruction fidelity. Similar optimization approaches could be applied to a broader range of basic modules, offering a new paradigm for the design of plug-and-play modules.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling</title>
<link>https://arxiv.org/abs/2506.02025</link>
<guid>https://arxiv.org/abs/2506.02025</guid>
<content:encoded><![CDATA[
arXiv:2506.02025v2 Announce Type: replace-cross 
Abstract: High-Performance Computing (HPC) job scheduling involves balancing conflicting objectives such as minimizing makespan, reducing wait times, optimizing resource use, and ensuring fairness. Traditional methods, including heuristic-based, e.g., First-Come-First-Served (FJFS) and Shortest Job First (SJF), or intensive optimization techniques, often lack adaptability to dynamic workloads and, more importantly, cannot simultaneously optimize multiple objectives in HPC systems. To address this, we propose a novel Large Language Model (LLM)-based scheduler using a ReAct-style framework (Reason + Act), enabling iterative, interpretable decision-making. The system incorporates a scratchpad memory to track scheduling history and refine decisions via natural language feedback, while a constraint enforcement module ensures feasibility and safety. We evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across seven real-world HPC workload scenarios, including heterogeneous mixes, bursty patterns, and adversarial cases etc. Comparisons against FCFS, SJF, and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling effectively balances multiple objectives while offering transparent reasoning through natural language traces. The method excels in constraint satisfaction and adapts to diverse workloads without domain-specific training. However, a trade-off between reasoning quality and computational overhead challenges real-time deployment. This work presents the first comprehensive study of reasoning-capable LLMs for HPC scheduling, demonstrating their potential to handle multiobjective optimization while highlighting limitations in computational efficiency. The findings provide insights into leveraging advanced language models for complex scheduling problems in dynamic HPC environments.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiniCPM4: Ultra-Efficient LLMs on End Devices</title>
<link>https://arxiv.org/abs/2506.07900</link>
<guid>https://arxiv.org/abs/2506.07900</guid>
<content:encoded><![CDATA[
arXiv:2506.07900v2 Announce Type: replace-cross 
Abstract: This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Furthermore, we construct a hybrid reasoning model, MiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning mode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform similar-sized open-source models across benchmarks, with the 8B variants showing significant speed improvements on long sequence understanding and generation.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation</title>
<link>https://arxiv.org/abs/2506.08570</link>
<guid>https://arxiv.org/abs/2506.08570</guid>
<content:encoded><![CDATA[
arXiv:2506.08570v3 Announce Type: replace-cross 
Abstract: Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly in many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and identify which design choices influence performance the most. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: auto-regressive decoding and conditional flow-matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Parameter Decomposition</title>
<link>https://arxiv.org/abs/2506.20790</link>
<guid>https://arxiv.org/abs/2506.20790</guid>
<content:encoded><![CDATA[
arXiv:2506.20790v2 Announce Type: replace-cross 
Abstract: A key step in reverse engineering neural networks is to decompose them into simpler parts that can be studied in relative isolation. Linear parameter decomposition -- a framework that has been proposed to resolve several issues with current decomposition methods -- decomposes neural network parameters into a sum of sparsely used vectors in parameter space. However, the current main method in this framework, Attribution-based Parameter Decomposition (APD), is impractical on account of its computational cost and sensitivity to hyperparameters. In this work, we introduce \textit{Stochastic Parameter Decomposition} (SPD), a method that is more scalable and robust to hyperparameters than APD, which we demonstrate by decomposing models that are slightly larger and more complex than was possible to decompose with APD. We also show that SPD avoids other issues, such as shrinkage of the learned parameters, and better identifies ground truth mechanisms in toy models. By bridging causal mediation analysis and network decomposition methods, this demonstration opens up new research possibilities in mechanistic interpretability by removing barriers to scaling linear parameter decomposition methods to larger models. We release a library for running SPD and reproducing our experiments at https://github.com/goodfire-ai/spd/tree/spd-paper.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Action-Value Temporal-Difference Methods That Learn State Values</title>
<link>https://arxiv.org/abs/2507.09523</link>
<guid>https://arxiv.org/abs/2507.09523</guid>
<content:encoded><![CDATA[
arXiv:2507.09523v2 Announce Type: replace-cross 
Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping: using value predictions to generate new value predictions. The vast majority of TD methods for control learn a policy by bootstrapping from a single action-value function (e.g., Q-learning and Sarsa). Significantly less attention has been given to methods that bootstrap from two asymmetric value functions: i.e., methods that learn state values as an intermediate step in learning action values. Existing algorithms in this vein can be categorized as either QV-learning or AV-learning. Though these algorithms have been investigated to some degree in prior work, it remains unclear if and when it is advantageous to learn two value functions instead of just one -- and whether such approaches are theoretically sound in general. In this paper, we analyze these algorithmic families in terms of convergence and sample efficiency. We find that while both families are more efficient than Expected Sarsa in the prediction setting, only AV-learning methods offer any major benefit over Q-learning in the control setting. Finally, we introduce a new AV-learning algorithm called Regularized Dueling Q-learning (RDQ), which significantly outperforms Dueling DQN in the MinAtar benchmark.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP</title>
<link>https://arxiv.org/abs/2507.14904</link>
<guid>https://arxiv.org/abs/2507.14904</guid>
<content:encoded><![CDATA[
arXiv:2507.14904v2 Announce Type: replace-cross 
Abstract: 3D visual grounding allows an embodied agent to understand visual information in real-world 3D environments based on human instructions, which is crucial for embodied intelligence. Existing 3D visual grounding methods typically rely on separate encoders for different modalities (e.g., RGB images, text, and 3D point clouds), resulting in large and complex models that are inefficient to train. While some approaches use pre-trained 2D multi-modal models like CLIP for 3D tasks, they still struggle with aligning point cloud data to 2D encoders. As a result, these methods continue to depend on 3D encoders for feature extraction, further increasing model complexity and training inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal network to process all three modalities (RGB images, text, and point clouds), significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal model with adapter-based fine-tuning, this framework effectively adapts to the tri-modal setting, improving both adaptability and performance across modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module is designed to fuse geometric multi-scale features from point clouds and images. We then integrate textual features for final modality fusion and introduce a multi-modal decoder to facilitate deep cross-modal understanding. Together, our method achieves unified feature extraction and fusion across the three modalities, enabling an end-to-end 3D visual grounding model. Compared to the baseline, our method reduces the number of trainable parameters by approximately 58\%, while achieving a 6.52\% improvement in the 3D detection task and a 6.25\% improvement in the 3D visual grounding task.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Video Generation for High-Efficiency Video Compression</title>
<link>https://arxiv.org/abs/2507.15269</link>
<guid>https://arxiv.org/abs/2507.15269</guid>
<content:encoded><![CDATA[
arXiv:2507.15269v2 Announce Type: replace-cross 
Abstract: Perceptual studies demonstrate that conditional diffusion models excel at reconstructing video content aligned with human visual perception. Building on this insight, we propose a video compression framework that leverages conditional diffusion models for perceptually optimized reconstruction. Specifically, we reframe video compression as a conditional generation task, where a generative model synthesizes video from sparse, yet informative signals. Our approach introduces three key modules: (1) Multi-granular conditioning that captures both static scene structure and dynamic spatio-temporal cues; (2) Compact representations designed for efficient transmission without sacrificing semantic richness; (3) Multi-condition training with modality dropout and role-aware embeddings, which prevent over-reliance on any single modality and enhance robustness. Extensive experiments show that our method significantly outperforms both traditional and neural codecs on perceptual quality metrics such as Fr\'echet Video Distance (FVD) and LPIPS, especially under high compression ratios.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing</title>
<link>https://arxiv.org/abs/2507.22627</link>
<guid>https://arxiv.org/abs/2507.22627</guid>
<content:encoded><![CDATA[
arXiv:2507.22627v2 Announce Type: replace-cross 
Abstract: Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The KG-ER Conceptual Schema Language</title>
<link>https://arxiv.org/abs/2508.02548</link>
<guid>https://arxiv.org/abs/2508.02548</guid>
<content:encoded><![CDATA[
arXiv:2508.02548v2 Announce Type: replace-cross 
Abstract: We propose KG-ER, a conceptual schema language for knowledge graphs that describes the structure of knowledge graphs independently of their representation (relational databases, property graphs, RDF) while helping to capture the semantics of the information stored in a knowledge graph.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Street-Level AI: Are Large Language Models Ready for Real-World Judgments?</title>
<link>https://arxiv.org/abs/2508.08193</link>
<guid>https://arxiv.org/abs/2508.08193</guid>
<content:encoded><![CDATA[
arXiv:2508.08193v2 Announce Type: replace-cross 
Abstract: A surge of recent work explores the ethical and societal implications of large-scale AI models that make "moral" judgments. Much of this literature focuses either on alignment with human judgments through various thought experiments or on the group fairness implications of AI judgments. However, the most immediate and likely use of AI is to help or fully replace the so-called street-level bureaucrats, the individuals deciding to allocate scarce social resources or approve benefits. There is a rich history underlying how principles of local justice determine how society decides on prioritization mechanisms in such domains. In this paper, we examine how well LLM judgments align with human judgments, as well as with socially and politically determined vulnerability scoring systems currently used in the domain of homelessness resource allocation. Crucially, we use real data on those needing services (maintaining strict confidentiality by only using local large models) to perform our analyses. We find that LLM prioritizations are extremely inconsistent in several ways: internally on different runs, between different LLMs, and between LLMs and the vulnerability scoring systems. At the same time, LLMs demonstrate qualitative consistency with lay human judgments in pairwise testing. Findings call into question the readiness of current generation AI systems for naive integration in high-stakes societal decision-making.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI</title>
<link>https://arxiv.org/abs/2508.08524</link>
<guid>https://arxiv.org/abs/2508.08524</guid>
<content:encoded><![CDATA[
arXiv:2508.08524v3 Announce Type: replace-cross 
Abstract: Interactive streetscape mapping tools such as Google Street View (GSV) and Meta Mapillary enable users to virtually navigate and experience real-world environments via immersive 360{\deg} imagery but remain fundamentally inaccessible to blind users. We introduce StreetViewAI, the first-ever accessible street view tool, which combines context-aware, multimodal AI, accessible navigation controls, and conversational speech. With StreetViewAI, blind users can virtually examine destinations, engage in open-world exploration, or virtually tour any of the over 220 billion images and 100+ countries where GSV is deployed. We iteratively designed StreetViewAI with a mixed-visual ability team and performed an evaluation with eleven blind users. Our findings demonstrate the value of an accessible street view in supporting POI investigations and remote route planning. We close by enumerating key guidelines for future work.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiGen: Child-Friendly Multilingual Speech Generator with LLMs</title>
<link>https://arxiv.org/abs/2508.08715</link>
<guid>https://arxiv.org/abs/2508.08715</guid>
<content:encoded><![CDATA[
arXiv:2508.08715v3 Announce Type: replace-cross 
Abstract: Generative speech models have demonstrated significant potential in improving human-machine interactions, offering valuable real-world applications such as language learning for children. However, achieving high-quality, child-friendly speech generation remains challenging, particularly for low-resource languages across diverse languages and cultural contexts. In this paper, we propose MultiGen, a multilingual speech generation model with child-friendly interaction, leveraging LLM architecture for speech generation tailored for low-resource languages. We propose to integrate age-appropriate multilingual speech generation using LLM architectures, which can be used to facilitate young children's communication with AI systems through culturally relevant context in three low-resource languages: Singaporean accent Mandarin, Malay, and Tamil. Experimental results from both objective metrics and subjective evaluations demonstrate the superior performance of the proposed MultiGen compared to baseline methods.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration</title>
<link>https://arxiv.org/abs/2508.13755</link>
<guid>https://arxiv.org/abs/2508.13755</guid>
<content:encoded><![CDATA[
arXiv:2508.13755v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transplant Then Regenerate: A New Paradigm for Text Data Augmentation</title>
<link>https://arxiv.org/abs/2508.14723</link>
<guid>https://arxiv.org/abs/2508.14723</guid>
<content:encoded><![CDATA[
arXiv:2508.14723v2 Announce Type: replace-cross 
Abstract: Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their "knowledge emergence" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vectorized Attention with Learnable Encoding for Quantum Transformer</title>
<link>https://arxiv.org/abs/2508.18464</link>
<guid>https://arxiv.org/abs/2508.18464</guid>
<content:encoded><![CDATA[
arXiv:2508.18464v2 Announce Type: replace-cross 
Abstract: Vectorized quantum block encoding provides a way to embed classical data into Hilbert space, offering a pathway for quantum models, such as Quantum Transformers (QT), that replace classical self-attention with quantum circuit simulations to operate more efficiently. Current QTs rely on deep parameterized quantum circuits (PQCs), rendering them vulnerable to QPU noise, and thus hindering their practical performance. In this paper, we propose the Vectorized Quantum Transformer (VQT), a model that supports ideal masked attention matrix computation through quantum approximation simulation and efficient training via vectorized nonlinear quantum encoder, yielding shot-efficient and gradient-free quantum circuit simulation (QCS) and reduced classical sampling overhead. In addition, we demonstrate an accuracy comparison for IBM and IonQ in quantum circuit simulation and competitive results in benchmarking natural language processing tasks on IBM state-of-the-art and high-fidelity Kingston QPU. Our noise intermediate-scale quantum friendly VQT approach unlocks a novel architecture for end-to-end machine learning in quantum computing.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language</title>
<link>https://arxiv.org/abs/2409.00061</link>
<guid>https://arxiv.org/abs/2409.00061</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph, NLI, fact-checking, COVID-19, Indonesian language  

Summary:  
Automated fact-checking is crucial for combating COVID-19 misinformation online. This study proposes a model that utilizes a Knowledge Graph (KG) to enhance Natural Language Inference (NLI) performance for Indonesian language fact-checking. The model consists of three modules: fact, NLI, and classifier. By incorporating information from the COVID-19 KG Bahasa Indonesia, the model achieved an accuracy of 0.8616, demonstrating the effectiveness of KGs in improving fact-checking NLI performance. This approach addresses the challenge of knowledge scarcity in deep learning training and highlights the value of external knowledge sources in enhancing automated fact-checking systems for COVID-19 misinformation. <div>
arXiv:2409.00061v4 Announce Type: replace-cross 
Abstract: Automated fact-checking is a key strategy to overcome the spread of COVID-19 misinformation on the internet. These systems typically leverage deep learning approaches through Natural Language Inference (NLI) to verify the truthfulness of information based on supporting evidence. However, one challenge that arises in deep learning is performance stagnation due to a lack of knowledge during training. This study proposes using a Knowledge Graph (KG) as external knowledge to enhance NLI performance for automated COVID-19 fact-checking in the Indonesian language. The proposed model architecture comprises three modules: a fact module, an NLI module, and a classifier module. The fact module processes information from the KG, while the NLI module handles semantic relationships between the given premise and hypothesis. The representation vectors from both modules are concatenated and fed into the classifier module to produce the final result. The model was trained using the generated Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia. Our study demonstrates that incorporating KGs can significantly improve NLI performance in fact-checking, achieving the best accuracy of 0.8616. This suggests that KGs are a valuable component for enhancing NLI performance in automated fact-checking.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis</title>
<link>https://arxiv.org/abs/2509.02650</link>
<guid>https://arxiv.org/abs/2509.02650</guid>
<content:encoded><![CDATA[
<div> media coverage, artificial intelligence, profit, safety, reputation

Summary:<br />
Artificial intelligence (AI) developers often prioritize profit over user safety, leading to untrustworthy AI technology. This study explores the role of media coverage in incentivizing AI creators to prioritize safety. Through artificial populations and evolutionary game theory, the research shows that media coverage can promote cooperation between creators and users by holding developers accountable and shaping public perception. However, cooperation is influenced by the reliability of media information and the costs associated with accessing media or ensuring safety. The study suggests that media can act as a soft regulator, guiding AI safety even in the absence of formal government oversight. This highlights the importance of media in promoting the safe development and widespread adoption of AI technology. 

<br /><br /> <div>
arXiv:2509.02650v1 Announce Type: new 
Abstract: When developers of artificial intelligence (AI) products need to decide between profit and safety for the users, they likely choose profit. Untrustworthy AI technology must come packaged with tangible negative consequences. Here, we envisage those consequences as the loss of reputation caused by media coverage of their misdeeds, disseminated to the public. We explore whether media coverage has the potential to push AI creators into the production of safe products, enabling widespread adoption of AI technology. We created artificial populations of self-interested creators and users and studied them through the lens of evolutionary game theory. Our results reveal that media is indeed able to foster cooperation between creators and users, but not always. Cooperation does not evolve if the quality of the information provided by the media is not reliable enough, or if the costs of either accessing media or ensuring safety are too high. By shaping public perception and holding developers accountable, media emerges as a powerful soft regulator -- guiding AI safety even in the absence of formal government oversight.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)</title>
<link>https://arxiv.org/abs/2509.02661</link>
<guid>https://arxiv.org/abs/2509.02661</guid>
<content:encoded><![CDATA[
<div> AI, MPS, interdisciplinary, research, education <br />
Summary: The paper summarizes the MPS community's perspective on the intersection of Artificial Intelligence (AI) and Mathematical and Physics Sciences (MPS). The growing link between AI and MPS indicates the need to proactively leverage AI for scientific discovery and optimize its impact on AI development. Proposed activities and priorities include promoting AI+MPS research in both directions, building an interdisciplinary community of AI+MPS researchers, and enhancing education and workforce development in AI for MPS researchers and students. The paper concludes with suggested priorities for funding agencies, educational institutions, and individual researchers to position the MPS community as a leader in exploiting the transformative potential of AI+MPS. <br /><br /> <div>
arXiv:2509.02661v1 Announce Type: new 
Abstract: This community paper developed out of the NSF Workshop on the Future of Artificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS), which was held in March 2025 with the goal of understanding how the MPS domains (Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics) can best capitalize on, and contribute to, the future of AI. We present here a summary and snapshot of the MPS community's perspective, as of Spring/Summer 2025, in a rapidly developing field. The link between AI and MPS is becoming increasingly inextricable; now is a crucial moment to strengthen the link between AI and Science by pursuing a strategy that proactively and thoughtfully leverages the potential of AI for scientific discovery and optimizes opportunities to impact the development of AI by applying concepts from fundamental science. To achieve this, we propose activities and strategic priorities that: (1) enable AI+MPS research in both directions; (2) build up an interdisciplinary community of AI+MPS researchers; and (3) foster education and workforce development in AI for MPS researchers and students. We conclude with a summary of suggested priorities for funding agencies, educational institutions, and individual researchers to help position the MPS community to be a leader in, and take full advantage of, the transformative potential of AI+MPS.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning with Reasoning using Vision Language World Model</title>
<link>https://arxiv.org/abs/2509.02722</link>
<guid>https://arxiv.org/abs/2509.02722</guid>
<content:encoded><![CDATA[
<div> modeling, world model, language, visual, planning <br />
Summary: <br />
The Vision Language World Model (VLWM) is introduced as a high-level world model trained for language-based world modeling on natural videos. It uses a Tree of Captions to predict goal achievements and trajectories of actions and state changes in a semantic and temporally abstract manner. The VLWM integrates an action policy and dynamics model for reactive and reflective planning, respectively. It utilizes a critic model for cost evaluation based on semantic distances between hypothetical future states and expected goal states. The VLWM achieves state-of-the-art performance in Visual Planning for Assistance (VPA), outperforming strong VLM baselines on benchmark evaluations and human evaluations in PlannerArena. Additionally, it surpasses existing models on RoboVQA and WorldPrediction benchmarks. This advancement in world modeling has significant implications for improving planning efficiency and accuracy in various domains. <br /> <div>
arXiv:2509.02722v1 Announce Type: new 
Abstract: Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that we trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics</title>
<link>https://arxiv.org/abs/2509.02751</link>
<guid>https://arxiv.org/abs/2509.02751</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, semantic operators, AI-driven analytics, Deep Research systems, optimized execution

Summary:
- Researchers are exploring the use of large language models (LLMs) for AI-driven analytics over unstructured datasets.
- Semantic operators have been developed as a set of AI-powered data transformations with natural language specifications but can be expensive and inefficient for interactive tasks.
- Deep Research systems can answer natural language questions over large datasets but may not optimize query plans effectively.
- A prototype has been built to enable Deep Research agents to write and execute optimized semantic operator programs.
- The prototype outperforms handcrafted semantic operator programs and standard Deep Research agents, achieving better F1-score and cost and runtime savings of up to 76.8% and 72.7%, respectively.

<br /><br />Summary: 
Researchers are investigating the use of large language models (LLMs) for AI-driven analytics on unstructured datasets. Semantic operators, AI-powered data transformations with natural language specifications, are costly and inefficient for interactive tasks. Deep Research systems can address natural language queries but might not optimize query plans well. A prototype has been developed to allow Deep Research agents to write and execute optimized semantic operator programs, leading to improved performance in terms of F1-score and cost and runtime savings compared to handcrafted programs and standard Deep Research agents. <div>
arXiv:2509.02751v1 Announce Type: new 
Abstract: With advances in large language models (LLMs), researchers are creating new systems that can perform AI-driven analytics over large unstructured datasets. Recent work has explored executing such analytics queries using semantic operators -- a declarative set of AI-powered data transformations with natural language specifications. However, even when optimized, these operators can be expensive to execute on millions of records and their iterator execution semantics make them ill-suited for interactive data analytics tasks. In another line of work, Deep Research systems have demonstrated an ability to answer natural language question(s) over large datasets. These systems use one or more LLM agent(s) to plan their execution, process the dataset(s), and iteratively refine their answer. However, these systems do not explicitly optimize their query plans which can lead to poor plan execution. In order for AI-driven analytics to excel, we need a runtime which combines the optimized execution of semantic operators with the flexibility and more dynamic execution of Deep Research systems. As a first step towards this vision, we build a prototype which enables Deep Research agents to write and execute optimized semantic operator programs. We evaluate our prototype and demonstrate that it can outperform a handcrafted semantic operator program and open Deep Research systems on two basic queries. Compared to a standard open Deep Research agent, our prototype achieves up to 1.95x better F1-score. Furthermore, even if we give the agent access to semantic operators as tools, our prototype still achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its optimized execution.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.02754</link>
<guid>https://arxiv.org/abs/2509.02754</guid>
<content:encoded><![CDATA[
<div> tokenizer design, positional embedding, pre-training paradigms, post-training strategies, test-time computation
Summary:
- The study evaluates the transferability of key modules from large language models (LLMs) to autonomous driving motion generation.
- Five modules are assessed: tokenizer design, positional embedding, pre-training paradigms, post-training strategies, and test-time computation.
- The experiments on the Waymo Sim Agents benchmark show promising results when the modules are adapted for autonomous driving.
- The study identifies techniques that can effectively enhance performance in autonomous driving scenarios.
- It also discusses the reasons for the failure of certain modules and suggests specific adaptations needed for autonomous driving motion generation. 

<br /><br />Summary: <div>
arXiv:2509.02754v1 Announce Type: new 
Abstract: Recent breakthroughs in large language models (LLMs) have not only advanced natural language processing but also inspired their application in domains with structurally similar problems--most notably, autonomous driving motion generation. Both domains involve autoregressive sequence modeling, token-based representations, and context-aware decision making, making the transfer of LLM components a natural and increasingly common practice. However, despite promising early attempts, a systematic understanding of which LLM modules are truly transferable remains lacking. In this paper, we present a comprehensive evaluation of five key LLM modules--tokenizer design, positional embedding, pre-training paradigms, post-training strategies, and test-time computation--within the context of motion generation for autonomous driving. Through extensive experiments on the Waymo Sim Agents benchmark, we demonstrate that, when appropriately adapted, these modules can significantly improve performance for autonomous driving motion generation. In addition, we identify which techniques can be effectively transferred, analyze the potential reasons for the failure of others, and discuss the specific adaptations needed for autonomous driving scenarios. We evaluate our method on the Sim Agents task and achieve competitive results.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan Verification for LLM-Based Embodied Task Completion Agents</title>
<link>https://arxiv.org/abs/2509.02761</link>
<guid>https://arxiv.org/abs/2509.02761</guid>
<content:encoded><![CDATA[
<div> framework, verification, language model, embodied AI, spatial planning <br />
<br />
Summary: 
This research introduces an iterative verification framework to improve the quality of task plans generated by large language models (LLMs) for embodied AI systems. The framework involves a Judge LLM critiquing action sequences and a Planner LLM making revisions, resulting in cleaner and more coherent trajectories. Unlike rule-based approaches, this method relies on natural language prompting for error correction, allowing for broad generalization across various error types. The framework was tested on annotated actions from the TEACh dataset and achieved high recall and precision across different LLMs. It quickly converges to cleaner action sequences, improving temporal efficiency and spatial organization. Importantly, the method preserves human error-recovery patterns, supporting future work on robust corrective behavior. This approach establishes plan verification as a reliable capability for LLMs in spatial planning and action refinement, providing a scalable method for generating high-quality training data for imitation learning in embodied AI. <div>
arXiv:2509.02761v1 Announce Type: new 
Abstract: Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality. We propose an iterative verification framework in which a Judge LLM critiques action sequences and a Planner LLM applies the revisions, yielding progressively cleaner and more spatially coherent trajectories. Unlike rule-based approaches, our method relies on natural language prompting, enabling broad generalization across error types including irrelevant actions, contradictions, and missing steps. On a set of manually annotated actions from the TEACh embodied AI dataset, our framework achieves up to 90% recall and 100% precision across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout). The refinement loop converges quickly, with 96.5% of sequences requiring at most three iterations, while improving both temporal efficiency and spatial action organization. Crucially, the method preserves human error-recovery patterns rather than collapsing them, supporting future work on robust corrective behavior. By establishing plan verification as a reliable LLM capability for spatial planning and action refinement, we provide a scalable path to higher-quality training data for imitation learning in embodied AI.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Key Principles in Cross-Domain Hyper-Heuristic Performance</title>
<link>https://arxiv.org/abs/2509.02782</link>
<guid>https://arxiv.org/abs/2509.02782</guid>
<content:encoded><![CDATA[
<div> Keywords: hyper-heuristics, selection, transformation, real-world domains, competition <br />
<br />
Summary: 
The study focuses on cross-domain selection hyper-heuristics, emphasizing the composition and strategic transformations of low-level heuristics (LLHs) instead of their adaptive selection. Three key principles guide the transformations: solution acceptance, LLH repetitions, and perturbation intensity. By applying these principles to a basic random selection mechanism, the research surpasses existing state-of-the-art hyper-heuristics on challenging real-world domains, discovering 11 new best-known solutions. The method also proves competitive with the winner of the CHeSC competition, a common benchmark for cross-domain performance. The strategic transformations are further incorporated into recent hyper-heuristics, leading to superior performance on the CHeSC benchmark and real-world domains while streamlining their designs. <div>
arXiv:2509.02782v1 Announce Type: new 
Abstract: Cross-domain selection hyper-heuristics aim to distill decades of research on problem-specific heuristic search algorithms into adaptable general-purpose search strategies. In this respect, existing selection hyper-heuristics primarily focus on an adaptive selection of low-level heuristics (LLHs) from a predefined set. In contrast, we concentrate on the composition of this set and its strategic transformations. We systematically analyze transformations based on three key principles: solution acceptance, LLH repetitions, and perturbation intensity, i.e., the proportion of a solution affected by a perturbative LLH. We demonstrate the raw effects of our transformations on a trivial unbiased random selection mechanism. With an appropriately constructed transformation, this trivial method outperforms all available state-of-the-art hyper-heuristics on three challenging real-world domains and finds 11 new best-known solutions. The same method is competitive with the winner of the CHeSC competition, commonly used as the standard cross-domain benchmark. Moreover, we accompany several recent hyper-heuristics with such strategic transformations. Using this approach, we outperform the current state-of-the-art methods on both the CHeSC benchmark and real-world domains while often simplifying their designs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning General Policies From Examples</title>
<link>https://arxiv.org/abs/2509.02794</link>
<guid>https://arxiv.org/abs/2509.02794</guid>
<content:encoded><![CDATA[
<div> combinatorial methods, learning policies, generalization, hitting set algorithm, scalability 
Summary:
This research introduces a new symbolic method for learning policies that can solve large collections of planning problems. Unlike deep learning approaches, the resulting policies can be understood and verified for correctness. The proposed method generalizes sampled plans and ensures structural termination, preventing cycles in the learned policies. This approach is based on a hitting set algorithm, allowing for effective handling of problems with millions of states and pools with hundreds of thousands of features. The formal properties of the method are analyzed to ensure its reliability. The scalability of the approach is demonstrated through testing on various benchmarks. The new method addresses the limitations of existing combinatorial approaches by enabling learning from larger training instances and feature pools, paving the way for developing general policies that can tackle complex planning problems efficiently. 
<br /><br />Summary: <div>
arXiv:2509.02794v1 Announce Type: new 
Abstract: Combinatorial methods for learning general policies that solve large collections of planning problems have been recently developed. One of their strengths, in relation to deep learning approaches, is that the resulting policies can be understood and shown to be correct. A weakness is that the methods do not scale up and learn only from small training instances and feature pools that contain a few hundreds of states and features at most. In this work, we propose a new symbolic method for learning policies based on the generalization of sampled plans that ensures structural termination and hence acyclicity. The proposed learning approach is not based on SAT/ASP, as previous symbolic methods, but on a hitting set algorithm that can effectively handle problems with millions of states, and pools with hundreds of thousands of features. The formal properties of the approach are analyzed, and its scalability is tested on a number of benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-driven Adaptive Exploration</title>
<link>https://arxiv.org/abs/2509.03219</link>
<guid>https://arxiv.org/abs/2509.03219</guid>
<content:encoded><![CDATA[
<div> framework, adaptive exploration, uncertainty, policy learning, MuJoCo environments 
Summary:
The article introduces a generic framework for adaptive exploration in policy learning, addressing the crucial decision of when to switch between exploration and exploitation. By utilizing uncertainty measures, the framework offers a principled approach to determining these shifts in complex action sequences. It can incorporate various uncertainty-measuring mechanisms, such as those found in intrinsic motivation or epistemic uncertainty-based exploration methods. The framework encompasses previous adaptive exploration approaches and has shown superior performance compared to standard strategies in multiple MuJoCo environments through experimental validation. <div>
arXiv:2509.03219v1 Announce Type: new 
Abstract: Adaptive exploration methods propose ways to learn complex policies via alternating between exploration and exploitation. An important question for such methods is to determine the appropriate moment to switch between exploration and exploitation and vice versa. This is critical in domains that require the learning of long and complex sequences of actions. In this work, we present a generic adaptive exploration framework that employs uncertainty to address this important issue in a principled manner. Our framework includes previous adaptive exploration approaches as special cases. Moreover, we can incorporate in our framework any uncertainty-measuring mechanism of choice, for instance mechanisms used in intrinsic motivation or epistemic uncertainty-based exploration methods. We experimentally demonstrate that our framework gives rise to adaptive exploration strategies that outperform standard ones across several MuJoCo environments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accountability Framework for Healthcare AI Systems: Towards Joint Accountability in Decision Making</title>
<link>https://arxiv.org/abs/2509.03286</link>
<guid>https://arxiv.org/abs/2509.03286</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, healthcare, accountability, framework, collaboration 

Summary: 
Accountability in AI-driven healthcare decisions is a crucial concern, but there is a knowledge gap on how to implement it effectively. Regulatory guidelines are high-level and focus on the "what" rather than the "how," creating ambiguity. This study analyzes different perspectives on accountability and proposes a framework to bridge the gap between the concept and its implementation. The framework includes a three-tier structure for handling various accountability mechanisms in healthcare AI systems. It emphasizes the importance of shared dependencies in decision-making and advocates for collaboration among actors to address accountability issues. Explainability is highlighted as a key factor in facilitating communication and information sharing among stakeholders. By aligning regulations and mechanisms under a consistent accountability regime, the framework aims to promote transparency and responsibility in AI-driven healthcare decision-making. 

<br /><br />Summary: <div>
arXiv:2509.03286v1 Announce Type: new 
Abstract: AI is transforming the healthcare domain and is increasingly helping practitioners to make health-related decisions. Therefore, accountability becomes a crucial concern for critical AI-driven decisions. Although regulatory bodies, such as the EU commission, provide guidelines, they are highlevel and focus on the ''what'' that should be done and less on the ''how'', creating a knowledge gap for actors. Through an extensive analysis, we found that the term accountability is perceived and dealt with in many different ways, depending on the actor's expertise and domain of work. With increasing concerns about AI accountability issues and the ambiguity around this term, this paper bridges the gap between the ''what'' and ''how'' of AI accountability, specifically for AI systems in healthcare. We do this by analysing the concept of accountability, formulating an accountability framework, and providing a three-tier structure for handling various accountability mechanisms. Our accountability framework positions the regulations of healthcare AI systems and the mechanisms adopted by the actors under a consistent accountability regime. Moreover, the three-tier structure guides the actors of the healthcare AI system to categorise the mechanisms based on their conduct. Through our framework, we advocate that decision-making in healthcare AI holds shared dependencies, where accountability should be dealt with jointly and should foster collaborations. We highlight the role of explainability in instigating communication and information sharing between the actors to further facilitate the collaborative process.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding</title>
<link>https://arxiv.org/abs/2509.03310</link>
<guid>https://arxiv.org/abs/2509.03310</guid>
<content:encoded><![CDATA[
<div> framework, validation, structured environments, open-source, AI agents <br />
Summary: <br />
The article introduces app.build, an open-source framework designed to improve LLM-based application generation by incorporating systematic validation and structured environments. The framework utilizes multi-layered validation pipelines, stack-specific orchestration, and a model-agnostic architecture across three reference stacks. Evaluation results show a 73.3% viability rate with 30% achieving perfect quality scores, while open-weights models reach 80.8% of closed-model performance with structured environments. The framework has been widely adopted by the community, generating over 3,000 applications. The study highlights the importance of scaling reliable AI agents by scaling environments, emphasizing the need for comprehensive validation and structured environments in agent systems. This work provides valuable insights and complete reference implementations for production-oriented AI systems. <br /> <div>
arXiv:2509.03310v1 Announce Type: new 
Abstract: We present app.build (https://github.com/appdotbuild/agent/), an open-source framework that improves LLM-based application generation through systematic validation and structured environments. Our approach combines multi-layered validation pipelines, stack-specific orchestration, and model-agnostic architecture, implemented across three reference stacks. Through evaluation on 30 generation tasks, we demonstrate that comprehensive validation achieves 73.3% viability rate with 30% reaching perfect quality scores, while open-weights models achieve 80.8% of closed-model performance when provided structured environments. The open-source framework has been adopted by the community, with over 3,000 applications generated to date. This work demonstrates that scaling reliable AI agents requires scaling environments, not just models -- providing empirical insights and complete reference implementations for production-oriented agent systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning</title>
<link>https://arxiv.org/abs/2509.03345</link>
<guid>https://arxiv.org/abs/2509.03345</guid>
<content:encoded><![CDATA[
<div> deductive reasoning, inductive reasoning, abductive reasoning, large language models, Occam's Razor

Summary:
This work examines the inductive and abductive reasoning abilities of large language models (LLMs), which are crucial for solving real-world problems. A new synthetic dataset called InAbHyD is introduced, challenging intelligent agents to generate hypotheses to explain observations based on incomplete world models. A metric based on Occam's Razor is proposed to evaluate the quality of hypotheses. The study evaluates state-of-the-art LLMs and finds that while they can perform inductive and abductive reasoning in simple scenarios, they struggle with complex world models and generating high-quality hypotheses. Even with reasoning-enhancing techniques like in-context learning and RLVR, LLMs face challenges in producing accurate and concise hypotheses. <div>
arXiv:2509.03345v1 Announce Type: new 
Abstract: Reasoning is a core capability in artificial intelligence systems, for which large language models (LLMs) have recently shown remarkable progress. However, most work focuses exclusively on deductive reasoning, which is problematic since other types of reasoning are also essential in solving real-world problems, and they are less explored. This work focuses on evaluating LLMs' inductive and abductive reasoning capabilities. We introduce a programmable and synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example consists of an incomplete world model and a set of observations. The task for the intelligent agent is to produce hypotheses to explain observations under the incomplete world model to solve each reasoning example. We propose a new metric to evaluate the quality of hypotheses based on Occam's Razor. We evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs can perform inductive and abductive reasoning in simple scenarios, but struggle with complex world models and producing high-quality hypotheses, even with popular reasoning-enhancing techniques such as in-context learning and RLVR.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems</title>
<link>https://arxiv.org/abs/2509.03380</link>
<guid>https://arxiv.org/abs/2509.03380</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic LLM AI agents, bottom-up framework, aspects, umwelt, information niches <br />
Summary: 
This article introduces a new framework for AI agents called aspective agentic AI, which aims to improve their efficiency and security. Unlike traditional AI agents that follow scripts, these agents are situated in their environment and their behaviors are triggered by changes in the environment. The framework introduces the concept of aspects, similar to umwelt, where sets of agents perceive their environment differently, allowing for clearer control of information. An illustrative implementation of this framework shows that it enables zero information leakage, compared to typical architectures which leak up to 83% of the time. The idea of specialist agents working efficiently in their own information niches is highlighted as a potential improvement for both security and efficiency in AI systems. <br /><br />Summary: <div>
arXiv:2509.03380v1 Announce Type: new 
Abstract: Agentic LLM AI agents are often little more than autonomous chatbots: actors following scripts, often controlled by an unreliable director. This work introduces a bottom-up framework that situates AI agents in their environment, with all behaviors triggered by changes in their environments. It introduces the notion of aspects, similar to the idea of umwelt, where sets of agents perceive their environment differently to each other, enabling clearer control of information. We provide an illustrative implementation and show that compared to a typical architecture, which leaks up to 83% of the time, aspective agentic AI enables zero information leakage. We anticipate that this concept of specialist agents working efficiently in their own information niches can provide improvements to both security and efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ANNIE: Be Careful of Your Robots</title>
<link>https://arxiv.org/abs/2509.03383</link>
<guid>https://arxiv.org/abs/2509.03383</guid>
<content:encoded><![CDATA[
<div> taxonomy, safety violations, benchmark, adversarial framework, attack strategies

Summary:
In this study, the authors address the security risks posed by integrating vision-language-action (VLA) models into embodied AI (EAI) systems. They introduce a taxonomy of safety violations based on physical constraints, create a benchmark called ANNIEBench to evaluate embodied safety, and develop ANNIE-Attack, an adversarial framework targeting EAI models. The evaluation reveals high attack success rates across all safety categories, emphasizing the critical need for security-driven defenses in EAI systems. The authors also demonstrate sparse and adaptive attack strategies and validate the impacts through physical robot experiments, highlighting the urgent requirement for safeguarding physically grounded AI systems. <div>
arXiv:2509.03383v1 Announce Type: new 
Abstract: The integration of vision-language-action (VLA) models into embodied AI (EAI) robots is rapidly advancing their ability to perform complex, long-horizon tasks in humancentric environments. However, EAI systems introduce critical security risks: a compromised VLA model can directly translate adversarial perturbations on sensory input into unsafe physical actions. Traditional safety definitions and methodologies from the machine learning community are no longer sufficient. EAI systems raise new questions, such as what constitutes safety, how to measure it, and how to design effective attack and defense mechanisms in physically grounded, interactive settings. In this work, we present the first systematic study of adversarial safety attacks on embodied AI systems, grounded in ISO standards for human-robot interactions. We (1) formalize a principled taxonomy of safety violations (critical, dangerous, risky) based on physical constraints such as separation distance, velocity, and collision boundaries; (2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with 2,400 video-action sequences for evaluating embodied safety; and (3) ANNIE-Attack, a task-aware adversarial framework with an attack leader model that decomposes long-horizon goals into frame-level perturbations. Our evaluation across representative EAI models shows attack success rates exceeding 50% across all safety categories. We further demonstrate sparse and adaptive attack strategies and validate the real-world impact through physical robot experiments. These results expose a previously underexplored but highly consequential attack surface in embodied AI systems, highlighting the urgent need for security-driven defenses in the physical AI era. Code is available at https://github.com/RLCLab/Annie.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sam-llm: interpretable lane change trajectoryprediction via parametric finetuning</title>
<link>https://arxiv.org/abs/2509.03462</link>
<guid>https://arxiv.org/abs/2509.03462</guid>
<content:encoded><![CDATA[
<div> SAM-LLM, interpretable lane change trajectory prediction, LLM, kinematic lane change models, autonomous driving <br />
<br />
Summary: The SAM-LLM hybrid architecture combines Large Language Models (LLMs) with kinematic lane change models for autonomous driving. It predicts core physical parameters of trajectory models for lane change maneuvers based on LLM outputs, enhancing interpretability and computational efficiency. For lane-keeping scenarios, discrete coordinates are predicted, while parameters for an enhanced Sinusoidal Acceleration Model (SAM) are generated for lane change maneuvers including lateral displacement, maneuver duration, initial lateral velocity, and longitudinal velocity change. This parametric approach results in a complete, continuous, and physically plausible trajectory model with reduced output size. The SAM-LLM achieves a high accuracy rate of 98.73% in overall intention prediction, equivalent to traditional LLM predictors, but with advantages in explainability and resource efficiency.<br /><br /> <div>
arXiv:2509.03462v1 Announce Type: new 
Abstract: This work introduces SAM-LLM, a novel hybrid architecture that bridges the gap between the contextual reasoning of Large Language Models (LLMs) and the physical precision of kinematic lane change models for autonomous driving. The system is designed for interpretable lane change trajectory prediction by finetuning an LLM to output the core physical parameters of a trajectory model instead of raw coordinates. For lane-keeping scenarios, the model predicts discrete coordinates, but for lane change maneuvers, it generates the parameters for an enhanced Sinusoidal Acceleration Model (SAM), including lateral displacement, maneuver duration, initial lateral velocity, and longitudinal velocity change. This parametric approach yields a complete, continuous, and physically plausible trajectory model that is inherently interpretable and computationally efficient, achieving an 80% reduction in output size compared to coordinate-based methods. The SAM-LLM achieves a state-of-the-art overall intention prediction accuracy of 98.73%, demonstrating performance equivalent to traditional LLM predictors while offering significant advantages in explainability and resource efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Lifecycle Principle: Stabilizing Dynamic Neural Networks with State Memory</title>
<link>https://arxiv.org/abs/2509.02575</link>
<guid>https://arxiv.org/abs/2509.02575</guid>
<content:encoded><![CDATA[
<div> regularization, neurons, Lifecycle principle, state memory, optimization

Summary:
The study explores a novel regularization approach involving deactivating neurons for extended periods to enhance learning. Unlike conventional methods like Dropout which make temporary changes, this approach involves long-term modifications. However, reactivating neurons with random weights leads to severe training instability. To address this issue, the author introduces the Lifecycle (LC) principle, which incorporates state memory to restore neurons to their last effective state instead of reinitializing them. The theoretical analysis suggests that LC smoothens the loss landscape, facilitating optimization towards flatter minima associated with better generalization. Experimental results on image classification tasks demonstrate the efficacy of the method in improving generalization and robustness. Ablation studies underscore the significance of state memory in achieving these enhancements. <div>
arXiv:2509.02575v1 Announce Type: cross 
Abstract: I investigate a stronger form of regularization by deactivating neurons for extended periods, a departure from the temporary changes of methods like Dropout. However, this long-term dynamism introduces a critical challenge: severe training instability when neurons are revived with random weights. To solve this, I propose the Lifecycle (LC) principle, a regularization mechanism centered on a key innovation: state memory. Instead of re-initializing a revived neuron, my method restores its parameters to their last known effective state. This process preserves learned knowledge and avoids destructive optimization shocks. My theoretical analysis reveals that the LC principle smooths the loss landscape, guiding optimization towards flatter minima associated with better generalization. Experiments on image classification benchmarks demonstrate that my method improves generalization and robustness. Crucially, ablation studies confirm that state memory is essential for achieving these gains.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection</title>
<link>https://arxiv.org/abs/2509.02579</link>
<guid>https://arxiv.org/abs/2509.02579</guid>
<content:encoded><![CDATA[
<div> Keywords: endangered wildlife, illegal poaching, UAV coordination, latent variable modeling, Multi-Agent Reinforcement Learning

Summary: 
This paper presents a novel approach using Expectation-Maximization (EM) based latent variable modeling in Multi-Agent Reinforcement Learning (MARL) for coordinating Unmanned Aerial Vehicles (UAVs) in protecting endangered wildlife from illegal poaching. By incorporating latent variables to model hidden environmental factors and inter-agent dynamics, the method improves exploration and coordination in uncertain environments. The framework was tested in a simulation involving 10 UAVs patrolling habitats of the endangered Iranian leopard, showing better detection accuracy, adaptability, and policy convergence compared to standard algorithms like PPO and DDPG. The results demonstrate the potential of combining EM inference with MARL for decentralized decision-making in complex conservation scenarios. The full implementation, simulation environment, and training scripts are available on GitHub. 

<br /><br />Summary: <div>
arXiv:2509.02579v1 Announce Type: cross 
Abstract: Protecting endangered wildlife from illegal poaching presents a critical challenge, particularly in vast and partially observable environments where real-time response is essential. This paper introduces a novel Expectation-Maximization (EM) based latent variable modeling approach in the context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial Vehicle (UAV) coordination in wildlife protection. By modeling hidden environmental factors and inter-agent dynamics through latent variables, our method enhances exploration and coordination under uncertainty.We implement and evaluate our EM-MARL framework using a custom simulation involving 10 UAVs tasked with patrolling protected habitats of the endangered Iranian leopard. Extensive experimental results demonstrate superior performance in detection accuracy, adaptability, and policy convergence when compared to standard algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic Policy Gradient (DDPG). Our findings underscore the potential of combining EM inference with MARL to improve decentralized decisionmaking in complex, high-stakes conservation scenarios. The full implementation, simulation environment, and training scripts are publicly available on GitHub.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Charting the Future of Scholarly Knowledge with AI: A Community Perspective</title>
<link>https://arxiv.org/abs/2509.02581</link>
<guid>https://arxiv.org/abs/2509.02581</guid>
<content:encoded><![CDATA[
<div> Keywords: scholarly knowledge extraction, AI-enabled approaches, cross-disciplinary dialogue, collaboration, research directions

Summary:
The article discusses the need for scalable AI-enabled approaches in scholarly knowledge extraction and organization due to the rapid increase in publications. It highlights the challenge of limited familiarity with existing technologies and the lack of interaction across research communities. The manuscript emphasizes the importance of fostering cross-disciplinary dialogue to address shared challenges and promote collaboration for the development of reliable knowledge bases. By identifying ways to shape future research directions in scholarly knowledge and organization, the authors aim to enhance the exchange of methods, models, and best practices. Overall, the article advocates for a more integrated approach to structuring and synthesizing scholarly knowledge through collaboration and shared initiatives.<br /><br />Summary: <div>
arXiv:2509.02581v1 Announce Type: cross 
Abstract: Despite the growing availability of tools designed to support scholarly knowledge extraction and organization, many researchers still rely on manual methods, sometimes due to unfamiliarity with existing technologies or limited access to domain-adapted solutions. Meanwhile, the rapid increase in scholarly publications across disciplines has made it increasingly difficult to stay current, further underscoring the need for scalable, AI-enabled approaches to structuring and synthesizing scholarly knowledge. Various research communities have begun addressing this challenge independently, developing tools and frameworks aimed at building reliable, dynamic, and queryable scholarly knowledge bases. However, limited interaction across these communities has hindered the exchange of methods, models, and best practices, slowing progress toward more integrated solutions. This manuscript identifies ways to foster cross-disciplinary dialogue, identify shared challenges, categorize new collaboration and shape future research directions in scholarly knowledge and organization.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MitoDetect++: A Domain-Robust Pipeline for Mitosis Detection and Atypical Subtyping</title>
<link>https://arxiv.org/abs/2509.02586</link>
<guid>https://arxiv.org/abs/2509.02586</guid>
<content:encoded><![CDATA[
<div> Keywords: mitotic figures, deep learning, MIDOG 2025 challenge, atypical mitosis, computational pathology

Summary: 
MitoDetect++ is a deep learning pipeline developed for the MIDOG 2025 challenge to address mitosis detection and atypical mitosis classification. For detection, they utilize a U-Net-based architecture with EfficientNetV2-L backbone and attention modules, trained with combined segmentation losses. For classification, the Virchow2 vision transformer is used, fine-tuned efficiently with Low-Rank Adaptation (LoRA). To enhance generalization, strong augmentations, focal loss, and group-aware stratified cross-validation are integrated. Test-time augmentation (TTA) boosts robustness during inference. The method achieves a balanced accuracy of 0.892 across validation domains, demonstrating its clinical relevance and scalability in computational pathology tasks. 

<br /><br />Summary: <div>
arXiv:2509.02586v1 Announce Type: cross 
Abstract: Automated detection and classification of mitotic figures especially distinguishing atypical from normal remain critical challenges in computational pathology. We present MitoDetect++, a unified deep learning pipeline designed for the MIDOG 2025 challenge, addressing both mitosis detection and atypical mitosis classification. For detection (Track 1), we employ a U-Net-based encoder-decoder architecture with EfficientNetV2-L as the backbone, enhanced with attention modules, and trained via combined segmentation losses. For classification (Track 2), we leverage the Virchow2 vision transformer, fine-tuned efficiently using Low-Rank Adaptation (LoRA) to minimize resource consumption. To improve generalization and mitigate domain shifts, we integrate strong augmentations, focal loss, and group-aware stratified 5-fold cross-validation. At inference, we deploy test-time augmentation (TTA) to boost robustness. Our method achieves a balanced accuracy of 0.892 across validation domains, highlighting its clinical applicability and scalability across tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normal and Atypical Mitosis Image Classifier using Efficient Vision Transformer</title>
<link>https://arxiv.org/abs/2509.02589</link>
<guid>https://arxiv.org/abs/2509.02589</guid>
<content:encoded><![CDATA[
<div> EfficientViT-L2, atypical mitosis classification, MIDOG 2025 challenge, cancer types, domain generalization <br />
Summary:
EfficientViT-L2, a hybrid CNN--ViT architecture, was utilized for classifying atypical versus normal mitosis in the MIDOG 2025 challenge using a unified dataset of 13,938 nuclei from seven cancer types. The study employed leave-one-cancer-type-out cross-validation with 5-fold ensembles and stain-deconvolution for image augmentation to assess domain generalization. The model achieved a balanced accuracy of 0.859, ROC AUC of 0.942, and raw accuracy of 0.85 in the preliminary evaluation phase, showcasing competitive performance across metrics. The approach involved training an ensemble on all cancer types for challenge submissions, demonstrating well-balanced performance and efficiency in the classification task. <br /> <br />Summary: <div>
arXiv:2509.02589v1 Announce Type: cross 
Abstract: We tackle atypical versus normal mitosis classification in the MIDOG 2025 challenge using EfficientViT-L2, a hybrid CNN--ViT architecture optimized for accuracy and efficiency. A unified dataset of 13,938 nuclei from seven cancer types (MIDOG++ and AMi-Br) was used, with atypical mitoses comprising ~15. To assess domain generalization, we applied leave-one-cancer-type-out cross-validation with 5-fold ensembles, using stain-deconvolution for image augmentation. For challenge submissions, we trained an ensemble with the same 5-fold split but on all cancer types. In the preliminary evaluation phase, this model achieved balanced accuracy of 0.859, ROC AUC of 0.942, and raw accuracy of 0.85, demonstrating competitive and well-balanced performance across metrics.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble of Pathology Foundation Models for MIDOG 2025 Track 2: Atypical Mitosis Classification</title>
<link>https://arxiv.org/abs/2509.02591</link>
<guid>https://arxiv.org/abs/2509.02591</guid>
<content:encoded><![CDATA[
<div> Mitotic figures, classification, typical, atypical, tumor aggressiveness <br />
Summary:<br />
Mitotic figures play a crucial role in determining tumor aggressiveness, with accurate classification essential for patient prognostication. This study utilized Pathology Foundation Models (PFMs) pre-trained on large histopathology datasets and incorporated parameter-efficient fine-tuning techniques. Training involved a fisheye transform to highlight mitoses and Fourier Domain Adaptation using ImageNet target images. By ensembling multiple PFMs, the model was able to integrate diverse morphological insights and achieve high balanced accuracy on the Preliminary Evaluation Phase dataset. This approach demonstrates the potential of leveraging advanced machine learning techniques to enhance mitotic figure differentiation in pathology analysis. <br /> <div>
arXiv:2509.02591v1 Announce Type: cross 
Abstract: Mitotic figures are classified into typical and atypical variants, with atypical counts correlating strongly with tumor aggressiveness. Accurate differentiation is therefore essential for patient prognostication and resource allocation, yet remains challenging even for expert pathologists. Here, we leveraged Pathology Foundation Models (PFMs) pre-trained on large histopathology datasets and applied parameter-efficient fine-tuning via low-rank adaptation. During training, we employ a fisheye transform to emphasize mitoses and Fourier Domain Adaptation using ImageNet target images. Finally, we ensembled multiple PFMs to integrate complementary morphological insights, achieving a high balanced accuracy on the Preliminary Evaluation Phase dataset.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning</title>
<link>https://arxiv.org/abs/2509.02592</link>
<guid>https://arxiv.org/abs/2509.02592</guid>
<content:encoded><![CDATA[
<div> threshold calibration, class imbalance, machine learning, group-aware, synthetic data generation

Summary:
This study addresses the challenge of class imbalance in machine learning by proposing group-aware threshold calibration. It is demonstrated that setting different decision thresholds for demographic groups outperforms synthetic data generation methods, achieving higher balanced accuracy and improving worst-group balanced accuracy by 1.5-4%. Unlike single-threshold approaches, the group-aware method optimizes the Pareto frontier between balanced accuracy and worst-group balanced accuracy, allowing for fine-grained control over group-level performance. Applying group thresholds to synthetically augmented data shows minimal additional benefit, suggesting redundancy. The results are consistent across seven model families, highlighting the simplicity, interpretability, and effectiveness of group-aware threshold calibration as a solution to class imbalance. 

<br /><br />Summary: <div>
arXiv:2509.02592v1 Announce Type: cross 
Abstract: Class imbalance remains a fundamental challenge in machine learning, with traditional solutions often creating as many problems as they solve. We demonstrate that group-aware threshold calibration--setting different decision thresholds for different demographic groups--provides superior robustness compared to synthetic data generation methods. Through extensive experiments, we show that group-specific thresholds achieve 1.5-4% higher balanced accuracy than SMOTE and CT-GAN augmented models while improving worst-group balanced accuracy. Unlike single-threshold approaches that apply one cutoff across all groups, our group-aware method optimizes the Pareto frontier between balanced accuracy and worst-group balanced accuracy, enabling fine-grained control over group-level performance. Critically, we find that applying group thresholds to synthetically augmented data yields minimal additional benefit, suggesting these approaches are fundamentally redundant. Our results span seven model families including linear, tree-based, instance-based, and boosting methods, confirming that group-aware threshold calibration offers a simpler, more interpretable, and more effective solution to class imbalance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</title>
<link>https://arxiv.org/abs/2509.02593</link>
<guid>https://arxiv.org/abs/2509.02593</guid>
<content:encoded><![CDATA[
<div> Keywords: Mitotic figures, Tumor pathology, Mitosis detection algorithms, YOLOv12, MIDOG 2025 challenge

Summary:
Mitotic figures play a crucial role in tumor pathology, aiding in determining tumor aggressiveness and proliferation. However, their accurate identification poses challenges due to inter-observer variability. The MIDOG 2025 challenge seeks to address this issue by developing robust mitosis detection algorithms. This paper introduces an approach using the YOLOv12 object detection architecture, achieving an impressive $F_1$-score of 0.801 on the initial test set of the challenge, without relying on external data. The algorithm shows promise in accurately detecting mitotic figures, presenting a significant advancement in tumor pathology research. <div>
arXiv:2509.02593v1 Announce Type: cross 
Abstract: Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figures detection approach based on the YOLOv12 object detection architecture, achieving a $F_1$-score of 0.801 on the preliminary test set of the MIDOG 2025 challenge, without relying on external data.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenAIs HealthBench in Action: Evaluating an LLM-Based Medical Assistant on Realistic Clinical Queries</title>
<link>https://arxiv.org/abs/2509.02594</link>
<guid>https://arxiv.org/abs/2509.02594</guid>
<content:encoded><![CDATA[
<div> Evaluation, large language models, clinical questions, HealthBench, DR.INFO <br />
Summary: <br />
This study evaluates the performance of the agentic clinical support assistant, DR.INFO, using the HealthBench benchmark, which consists of expert-annotated health conversations. DR.INFO outperforms leading LLMs in accuracy, completeness, and instruction following on challenging examples, scoring 0.51 on the Hard subset. Compared to similar agentic RAG assistants, DR.INFO maintains a performance lead with a score of 0.54. The results demonstrate DR.INFO's strengths in communication, instruction following, and accuracy, while also identifying areas for improvement in context awareness and completeness of responses. The study emphasizes the importance of behavior-level, rubric-based evaluation for developing a reliable and trustworthy AI-enabled clinical support assistant.<br /> <div>
arXiv:2509.02594v1 Announce Type: cross 
Abstract: Evaluating large language models (LLMs) on their ability to generate high-quality, accurate, situationally aware answers to clinical questions requires going beyond conventional benchmarks to assess how these systems behave in complex, high-stake clincal scenarios. Traditional evaluations are often limited to multiple-choice questions that fail to capture essential competencies such as contextual reasoning, awareness and uncertainty handling etc. To address these limitations, we evaluate our agentic, RAG-based clinical support assistant, DR.INFO, using HealthBench, a rubric-driven benchmark composed of open-ended, expert-annotated health conversations. On the Hard subset of 1,000 challenging examples, DR.INFO achieves a HealthBench score of 0.51, substantially outperforming leading frontier LLMs (GPT-5, o3, Grok 3, GPT-4, Gemini 2.5, etc.) across all behavioral axes (accuracy, completeness, instruction following, etc.). In a separate 100-sample evaluation against similar agentic RAG assistants (OpenEvidence, Pathway.md), it maintains a performance lead with a health-bench score of 0.54. These results highlight DR.INFOs strengths in communication, instruction following, and accuracy, while also revealing areas for improvement in context awareness and completeness of a response. Overall, the findings underscore the utility of behavior-level, rubric-based evaluation for building a reliable and trustworthy AI-enabled clinical support assistant.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIDOG 2025: Mitotic Figure Detection with Attention-Guided False Positive Correction</title>
<link>https://arxiv.org/abs/2509.02598</link>
<guid>https://arxiv.org/abs/2509.02598</guid>
<content:encoded><![CDATA[
<div> Keywords: mitotic figure detection, Fully Convolutional One-Stage Object Detector (FCOS), Feedback Attention Ladder CNN (FAL-CNN), false positive reduction, object detection enhancement

Summary: 
The study introduces a novel approach for mitotic figure detection by extending the FCOS model. It incorporates a FAL-CNN model for classifying normal versus abnormal mitotic figures and a fusion network for refining bounding box predictions. The primary goal is to lower the false positive rate of the FCOS detector, enhancing detection accuracy and network generalisability. Preliminary evaluation results showed an F1 score of 0.655 for mitosis detection, indicating promising performance. The integration of the FAL-CNN model complements the FCOS architecture, enabling more precise classification and bounding box adjustments. By leveraging both models, the detection of mitotic figures is improved, showcasing the potential for advanced image analysis in medical applications.<br /><br />Summary: <div>
arXiv:2509.02598v1 Announce Type: cross 
Abstract: We present a novel approach which extends the existing Fully Convolutional One-Stage Object Detector (FCOS) for mitotic figure detection. Our composite model adds a Feedback Attention Ladder CNN (FAL-CNN) model for classification of normal versus abnormal mitotic figures, feeding into a fusion network that is trained to generate adjustments to bounding boxes predicted by FCOS. Our network aims to reduce the false positive rate of the FCOS object detector, to improve the accuracy of object detection and enhance the generalisability of the network. Our model achieved an F1 score of 0.655 for mitosis detection on the preliminary evaluation dataset.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Founders: AI-Generated Social Simulations for Startup Validation Research in Computational Social Science</title>
<link>https://arxiv.org/abs/2509.02605</link>
<guid>https://arxiv.org/abs/2509.02605</guid>
<content:encoded><![CDATA[
<div> Keywords: comparative docking, AI-enabled simulation, linguistic expressiveness, hybrid social simulation, computational social science

Summary:
Convergent themes such as commitment-based demand signals and black-box trust barriers were emphasized in both human-subject interviews and AI-generated personas. Partial overlaps revealed concerns about outliers and stress in real customer validation from founders, while synthetic personas highlighted blind spots and AI as a buffer. Human-only themes included relational value in early customer engagement and skepticism toward moonshot markets. Synthetic-only themes pointed out amplified false positives and trauma blind spots in AI simulations. The study suggests that large language model-driven personas are a form of hybrid social simulation with linguistic expressiveness but lacking lived history and relational consequence. They complement empirical studies by extending hypothesis space, accelerating exploratory validation, and delineating the boundaries of cognitive realism in computational social science. 

Summary: <br /><br />The study compared human-subject interviews and AI-driven persona data on AI-powered validation in startup founders. While both datasets showed convergent themes like commitment-based demand signals, they also revealed differences in concerns and perspectives. The AI personas, more linguistically expressive, highlighted irrational blind spots and a psychological buffer role of AI. Human-only themes emphasized the relational value of early customer engagement, while synthetic-only themes pointed out potential false positives and trauma blind spots in AI simulations. The findings suggest that AI-driven personas serve as a hybrid social simulation, enhancing hypothesis space and clarifying cognitive realism in computational social science. <div>
arXiv:2509.02605v1 Announce Type: cross 
Abstract: We present a comparative docking experiment that aligns human-subject interview data with large language model (LLM)-driven synthetic personas to evaluate fidelity, divergence, and blind spots in AI-enabled simulation. Fifteen early-stage startup founders were interviewed about their hopes and concerns regarding AI-powered validation, and the same protocol was replicated with AI-generated founder and investor personas. A structured thematic synthesis revealed four categories of outcomes: (1) Convergent themes - commitment-based demand signals, black-box trust barriers, and efficiency gains were consistently emphasized across both datasets; (2) Partial overlaps - founders worried about outliers being averaged away and the stress of real customer validation, while synthetic personas highlighted irrational blind spots and framed AI as a psychological buffer; (3) Human-only themes - relational and advocacy value from early customer engagement and skepticism toward moonshot markets; and (4) Synthetic-only themes - amplified false positives and trauma blind spots, where AI may overstate adoption potential by missing negative historical experiences.
  We interpret this comparative framework as evidence that LLM-driven personas constitute a form of hybrid social simulation: more linguistically expressive and adaptable than traditional rule-based agents, yet bounded by the absence of lived history and relational consequence. Rather than replacing empirical studies, we argue they function as a complementary simulation category - capable of extending hypothesis space, accelerating exploratory validation, and clarifying the boundaries of cognitive realism in computational social science.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Digital Twins for Optimal Radioembolization</title>
<link>https://arxiv.org/abs/2509.02607</link>
<guid>https://arxiv.org/abs/2509.02607</guid>
<content:encoded><![CDATA[
<div> Keywords: Radioembolization, liver cancer treatment, computational fluid dynamics, physics-informed machine learning, patient-specific digital twin

Summary:
Radioembolization is a liver cancer treatment method that involves delivering radioactive microspheres to tumors through the hepatic arterial tree. The complex nature of hepatic artery anatomy and variable blood flow makes treatment optimization challenging. A proposed solution is the creation of patient-specific digital twins using high-fidelity computational fluid dynamics (CFD) and physics-informed machine learning approaches. CFD allows for personalized treatment planning based on individual patient data, but its computational cost limits clinical applicability. Physics-informed neural networks (PINNs) offer a more efficient way to simulate blood flow and microsphere transport, while physics-informed generative adversarial networks (PI-GANs) and other AI surrogates provide uncertainty-aware predictions with reduced computational cost. By combining CFD and AI methods, a dynamic digital twin can optimize radioembolization planning and improve clinical outcomes. 

<br /><br />Summary: Radioembolization aims to deliver radioactive microspheres to liver tumors with minimal damage to healthy tissue. The proposed patient-specific digital twin combines computational fluid dynamics and physics-informed machine learning to optimize treatment planning. Computational fluid dynamics allows for personalized planning but is computationally expensive, while physics-informed machine learning approaches offer more efficient simulations. By integrating AI surrogates, uncertainty-aware predictions can be made cost-effectively, leading to improved outcomes in liver cancer treatment. <div>
arXiv:2509.02607v1 Announce Type: cross 
Abstract: Radioembolization is a localized liver cancer treatment that delivers radioactive microspheres (30 micron) to tumors via a catheter inserted in the hepatic arterial tree. The goal is to maximize therapeutic efficacy while minimizing damage to healthy liver tissue. However, optimization is challenging due to complex hepatic artery anatomy, variable blood flow, and uncertainty in microsphere transport. The creation of dynamic, patient-specific digital twins may provide a transformative solution to these challenges. This work outlines a framework for a liver radioembolization digital twin using high-fidelity computational fluid dynamics (CFD) and/or recent physics-informed machine learning approaches. The CFD approach involves microsphere transport calculations in the hepatic arterial tree with individual patient data, which enables personalized treatment planning. Although accurate, traditional CFD is computationally expensive and limits clinical applicability.
  To accelerate simulations, physics-informed neural networks (PINNs) and their generative extensions play an increasingly important role. PINNs integrate governing equations, such as the Navier-Stokes equations, directly into the neural network training process, enabling mesh-free, data-efficient approximation of blood flow and microsphere transport. Physics-informed generative adversarial networks (PI-GANs), diffusion models (PI-DMs), and transformer-based architectures further enable uncertainty-aware, temporally resolved predictions with reduced computational cost. These AI surrogates not only maintain physical fidelity but also support rapid sampling of diverse flow scenarios, facilitating real-time decision support.
  Together, CFD and physics-informed AI methods form the foundation of dynamic, patient-specific digital twin to optimize radioembolization planning and ultimately improve clinical outcomes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive clustering based on regular equivalence for influential node identification in complex networks</title>
<link>https://arxiv.org/abs/2509.02609</link>
<guid>https://arxiv.org/abs/2509.02609</guid>
<content:encoded><![CDATA[
<div> influential nodes, complex networks, deep learning, unsupervised learning, contrastive learning

Summary:
ReCC is a novel deep unsupervised framework for identifying influential nodes in complex networks. It reformalizes the problem as a label-free deep clustering task and utilizes regular equivalence-based similarity in contrastive learning to generate positive and negative samples. The framework integrates this mechanism into a graph convolutional network to learn node embeddings for distinguishing influential nodes. ReCC is trained using network reconstruction loss and a combined contrastive and clustering loss without the need for labeled data. Additionally, it enhances node representations by combining structural metrics with regular equivalence-based similarities. Experimental results show that ReCC surpasses existing methods on various benchmarks. <br /><br /> <div>
arXiv:2509.02609v1 Announce Type: cross 
Abstract: Identifying influential nodes in complex networks is a fundamental task in network analysis with wide-ranging applications across domains. While deep learning has advanced node influence detection, existing supervised approaches remain constrained by their reliance on labeled data, limiting their applicability in real-world scenarios where labels are scarce or unavailable. While contrastive learning demonstrates significant potential for performance enhancement, existing approaches predominantly rely on multiple-embedding generation to construct positive/negative sample pairs. To overcome these limitations, we propose ReCC (\textit{r}egular \textit{e}quivalence-based \textit{c}ontrastive \textit{c}lustering), a novel deep unsupervised framework for influential node identification. We first reformalize influential node identification as a label-free deep clustering problem, then develop a contrastive learning mechanism that leverages regular equivalence-based similarity, which captures structural similarities between nodes beyond local neighborhoods, to generate positive and negative samples. This mechanism is integrated into a graph convolutional network to learn node embeddings that are used to differentiate influential from non-influential nodes. ReCC is pre-trained using network reconstruction loss and fine-tuned with a combined contrastive and clustering loss, with both phases being independent of labeled data. Additionally, ReCC enhances node representations by combining structural metrics with regular equivalence-based similarities. Extensive experiments demonstrate that ReCC outperforms state-of-the-art approaches across several benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resilient Biosecurity in the Era of AI-Enabled Bioweapons</title>
<link>https://arxiv.org/abs/2509.02610</link>
<guid>https://arxiv.org/abs/2509.02610</guid>
<content:encoded><![CDATA[
<div> Keywords: generative biology, proteins, drug discovery, biosafety measures, protein-protein interaction

Summary: 
The study evaluates the performance of three PPI prediction tools - AlphaFold 3, AF3Complex, and SpatialPPIv2, in detecting viral-host interactions like Hepatitis B and SARS-CoV-2. Despite being trained on similar viruses, the models miss detecting several known interactions and fail to identify four confirmed SARS-CoV-2 mutants. This highlights the inadequacy of current predictive filters in flagging biological threats and detecting novel ones. The findings emphasize the need for a shift towards response-oriented infrastructure, including rapid experimental validation, adaptable biomanufacturing, and agile regulatory frameworks to keep pace with AI-driven developments. This shift is essential to address the risks posed by the design of novel proteins, especially in the context of drug discovery and potential bioweapon development. 

<br /><br />Summary: <div>
arXiv:2509.02610v1 Announce Type: cross 
Abstract: Recent advances in generative biology have enabled the design of novel proteins, creating significant opportunities for drug discovery while also introducing new risks, including the potential development of synthetic bioweapons. Existing biosafety measures primarily rely on inference-time filters such as sequence alignment and protein-protein interaction (PPI) prediction to detect dangerous outputs. In this study, we evaluate the performance of three leading PPI prediction tools: AlphaFold 3, AF3Complex, and SpatialPPIv2. These models were tested on well-characterized viral-host interactions, such as those involving Hepatitis B and SARS-CoV-2. Despite being trained on many of the same viruses, the models fail to detect a substantial number of known interactions. Strikingly, none of the tools successfully identify any of the four experimentally validated SARS-CoV-2 mutants with confirmed binding. These findings suggest that current predictive filters are inadequate for reliably flagging even known biological threats and are even more unlikely to detect novel ones. We argue for a shift toward response-oriented infrastructure, including rapid experimental validation, adaptable biomanufacturing, and regulatory frameworks capable of operating at the speed of AI-driven developments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Synthetic Image Augmentation Useful for Imbalanced Classification Problems? Case-Study on the MIDOG2025 Atypical Cell Detection Competition</title>
<link>https://arxiv.org/abs/2509.02612</link>
<guid>https://arxiv.org/abs/2509.02612</guid>
<content:encoded><![CDATA[
<div> Keywords: MIDOG 2025 challenge, atypical mitosis classification, histopathology images, imbalanced data, deep learning

Summary: 
The MIDOG 2025 challenge focuses on atypical mitosis classification in histopathology images, a clinically important yet imbalanced problem. Two backbones, ConvNeXt-Small and a histopathology-specific ViT from Lunit, were evaluated, with both achieving strong performance. Synthetic balancing to address class imbalance did not consistently improve results. ImageNet and domain-pretrained backbones were both effective, with the former reaching higher peaks and the latter providing greater stability. ConvNeXt performed best on the preliminary hidden test set, while Lunit remained competitive on balanced accuracy. These results suggest that both types of pretrained models are suitable for atypical mitosis classification, with domain-pretraining offering robustness and ImageNet pretraining achieving higher performance. The full hidden test set results will be disclosed at the conclusion of the challenge. 

<br /><br />Summary: <div>
arXiv:2509.02612v1 Announce Type: cross 
Abstract: The MIDOG 2025 challenge extends prior work on mitotic figure detection by introducing a new Track 2 on atypical mitosis classification. This task aims to distinguish normal from atypical mitotic figures in histopathology images, a clinically relevant but highly imbalanced and cross-domain problem. We investigated two complementary backbones: (i) ConvNeXt-Small, pretrained on ImageNet, and (ii) a histopathology-specific ViT from Lunit trained via self-supervision. To address the strong prevalence imbalance (9408 normal vs. 1741 atypical), we synthesized additional atypical examples to approximate class balance and compared models trained with real-only vs. real+synthetic data. Using five-fold cross-validation, both backbones reached strong performance (mean AUROC approximately 95 percent), with ConvNeXt achieving slightly higher peaks while Lunit exhibited greater fold-to-fold stability. Synthetic balancing, however, did not lead to consistent improvements. On the organizers' preliminary hidden test set, explicitly designed as an out-of-distribution debug subset, ConvNeXt attained the highest AUROC (95.4 percent), whereas Lunit remained competitive on balanced accuracy. These findings suggest that both ImageNet and domain-pretrained backbones are viable for atypical mitosis classification, with domain-pretraining conferring robustness and ImageNet pretraining reaching higher peaks, while naive synthetic balancing has limited benefit. Full hidden test set results will be reported upon challenge completion.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Radio Astronomy in the Era of Vision-Language Models: Prompt Sensitivity and Adaptation</title>
<link>https://arxiv.org/abs/2509.02615</link>
<guid>https://arxiv.org/abs/2509.02615</guid>
<content:encoded><![CDATA[
<div> adaptation, vision-language models, morphology-based classification, radio galaxies, astronomical imagery <br />
<br />
Summary: 
This study investigates the ability of general-purpose Vision-Language Models (VLMs) to classify radio galaxies based on morphology using the MiraBest FR-I/FR-II dataset in astronomy. The researchers explore different prompting strategies using natural language and diagrams and introduce visual examples in prompts. They find that prompt-based approaches can achieve good performance, indicating that VLMs have useful priors for unfamiliar scientific domains. However, the outputs are unstable, sensitive to prompt changes, suggesting that the models' reasoning is prompt-dependent rather than genuine inference. Despite this sensitivity, with lightweight adaptation, a generic VLM achieves near state-of-the-art performance, rivaling specialized models without astronomy-specific pretraining. These findings suggest caution in using VLMs in scientific domains, highlighting the need for careful prompt design. Nonetheless, with some adaptation, generic VLMs show promise as tools for scientific discovery. <br /> <div>
arXiv:2509.02615v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs), such as recent Qwen and Gemini models, are positioned as general-purpose AI systems capable of reasoning across domains. Yet their capabilities in scientific imaging, especially on unfamiliar and potentially previously unseen data distributions, remain poorly understood. In this work, we assess whether generic VLMs, presumed to lack exposure to astronomical corpora, can perform morphology-based classification of radio galaxies using the MiraBest FR-I/FR-II dataset. We explore prompting strategies using natural language and schematic diagrams, and, to the best of our knowledge, we are the first to introduce visual in-context examples within prompts in astronomy. Additionally, we evaluate lightweight supervised adaptation via LoRA fine-tuning. Our findings reveal three trends: (i) even prompt-based approaches can achieve good performance, suggesting that VLMs encode useful priors for unfamiliar scientific domains; (ii) however, outputs are highly unstable, i.e. varying sharply with superficial prompt changes such as layout, ordering, or decoding temperature, even when semantic content is held constant; and (iii) with just 15M trainable parameters and no astronomy-specific pretraining, fine-tuned Qwen-VL achieves near state-of-the-art performance (3% Error rate), rivaling domain-specific models. These results suggest that the apparent "reasoning" of VLMs often reflects prompt sensitivity rather than genuine inference, raising caution for their use in scientific domains. At the same time, with minimal adaptation, generic VLMs can rival specialized models, offering a promising but fragile tool for scientific discovery.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IS${}^3$ : Generic Impulsive--Stationary Sound Separation in Acoustic Scenes using Deep Filtering</title>
<link>https://arxiv.org/abs/2509.02622</link>
<guid>https://arxiv.org/abs/2509.02622</guid>
<content:encoded><![CDATA[
<div> Neural Network, Sound Separation, Audio Processing, Data Generation, Training <br />
Summary: <br />
The article introduces IS${}^3$, a neural network designed for Impulsive--Stationary Sound Separation in audio systems. The network aims to differentiate processing of stationary backgrounds and isolated acoustic events within an acoustic scene for various applications like adaptive audio rendering, voice mixing, noise reduction, and acoustic event classification. A deep filtering approach is used to isolate impulsive acoustic events from the background noise. The article proposes a sophisticated data generation pipeline to curate and adapt existing datasets for optimal training of the network. The study demonstrates that the learning-based approach outperforms existing methods like Harmonic--Percussive Sound Separation and wavelet filtering in objective separation metrics. The success of IS${}^3$ highlights the effectiveness of a lightweight neural architecture trained with diverse and well-designed data in addressing the previously unexplored task of sound separation. <br /> <div>
arXiv:2509.02622v1 Announce Type: cross 
Abstract: We are interested in audio systems capable of performing a differentiated processing of stationary backgrounds and isolated acoustic events within an acoustic scene, whether for applying specific processing methods to each part or for focusing solely on one while ignoring the other. Such systems have applications in real-world scenarios, including robust adaptive audio rendering systems (e.g., EQ or compression), plosive attenuation in voice mixing, noise suppression or reduction, robust acoustic event classification or even bioacoustics. To this end, we introduce IS${}^3$, a neural network designed for Impulsive--Stationary Sound Separation, that isolates impulsive acoustic events from the stationary background using a deep filtering approach, that can act as a pre-processing stage for the above-mentioned tasks. To ensure optimal training, we propose a sophisticated data generation pipeline that curates and adapts existing datasets for this task. We demonstrate that a learning-based approach, build on a relatively lightweight neural architecture and trained with well-designed and varied data, is successful in this previously unaddressed task, outperforming the Harmonic--Percussive Sound Separation masking method, adapted from music signal processing research, and wavelet filtering on objective separation metrics.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Owns The Robot?: Four Ethical and Socio-technical Questions about Wellbeing Robots in the Real World through Community Engagement</title>
<link>https://arxiv.org/abs/2509.02624</link>
<guid>https://arxiv.org/abs/2509.02624</guid>
<content:encoded><![CDATA[
<div> Keywords: robotic coaches, wellbeing, ethics, community perspectives, real-world deployment

Summary: 
- The study explores the ethical and socio-technical implications of using robotic wellbeing coaches in real-world settings.
- Three underrepresented communities engaged in workshops to provide their perspectives on the topic.
- Qualitative data was collected using the Social Robot Co-Design Canvas on Ethics and analyzed through Thematic Analysis.
- Four key themes were identified, focusing on ethical and socio-technical questions surrounding the use of wellbeing robots.
- The study proposes four questions as a framework for robotic development and deployment, emphasizing safety, stakeholders involvement, data ownership, and purpose of the robot.
<br /><br />Summary: <div>
arXiv:2509.02624v1 Announce Type: cross 
Abstract: Recent studies indicate that robotic coaches can play a crucial role in promoting wellbeing. However, the real-world deployment of wellbeing robots raises numerous ethical and socio-technical questions and concerns. To explore these questions, we undertake a community-centered investigation to examine three different communities' perspectives on using robotic wellbeing coaches in real-world environments. We frame our work as an anticipatory ethical investigation, which we undertake to better inform the development of robotic technologies with communities' opinions, with the ultimate goal of aligning robot development with public interest. We conducted workshops with three communities who are under-represented in robotics development: 1) members of the public at a science festival, 2) women computer scientists at a conference, and 3) humanities researchers interested in history and philosophy of science. In the workshops, we collected qualitative data using the Social Robot Co-Design Canvas on Ethics. We analysed the collected qualitative data with Thematic Analysis, informed by notes taken during workshops. Through our analysis, we identify four themes regarding key ethical and socio-technical questions about the real-world use of wellbeing robots. We group participants' insights and discussions around these broad thematic questions, discuss them in light of state-of-the-art literature, and highlight areas for future investigation. Finally, we provide the four questions as a broad framework that roboticists can and should use during robotic development and deployment, in order to reflect on the ethics and socio-technical dimensions of their robotic applications, and to engage in dialogue with communities of robot users. The four questions are: 1) Is the robot safe and how can we know that?, 2) Who is the robot built for and with?, 3) Who owns the robot and the data?, and 4) Why a robot?.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Two-Stage Strategy for Mitosis Detection Using Improved YOLO11x Proposals and ConvNeXt Classification</title>
<link>https://arxiv.org/abs/2509.02627</link>
<guid>https://arxiv.org/abs/2509.02627</guid>
<content:encoded><![CDATA[
<div> Keywords: MIDOG 2025, mitosis detection, whole-slide images, two-stage framework, detection F1-score

Summary:
The article introduces a two-stage framework for improving mitosis detection in complicated whole-slide images with non-tumor, inflamed, and necrotic regions. The framework utilizes an enhanced YOLO11x model with EMA attention and LSConv for generating mitosis candidates with a high recall. False positives are then filtered out using a ConvNeXt-Tiny classifier to improve precision. The proposed framework achieves an F1-score of 0.882 on a fused dataset, outperforming the single-stage YOLO11x baseline. The precision is significantly enhanced from 0.762 to 0.839, leading to the performance gain. The code for the framework is available on GitHub for further exploration. 
<br /><br />Summary: <div>
arXiv:2509.02627v1 Announce Type: cross 
Abstract: MIDOG 2025 Track 1 requires mitosis detection in whole-slide images (WSIs) containing non-tumor, inflamed, and necrotic regions. Due to the complicated and heterogeneous context, as well as possible artifacts, there are often false positives and false negatives, thus degrading the detection F1-score. To address this problem, we propose a two-stage framework. Firstly, an improved YOLO11x, integrated with EMA attention and LSConv, is employed to generate mitosis candidates. We use a low confidence threshold to generate as many proposals as possible, ensuring the detection recall. Then, a ConvNeXt-Tiny classifier is employed to filter out the false positives, ensuring the detection precision. Consequently, the proposed two-stage framework can generate a high detection F1-score. Evaluated on a fused dataset comprising MIDOG++, MITOS_WSI_CCMCT, and MITOS_WSI_CMC, our framework achieves an F1-score of 0.882, which is 0.035 higher than the single-stage YOLO11x baseline. This performance gain is produced by a significant precision improvement, from 0.762 to 0.839, and a comparable recall. The code is available at https://github.com/xxiao0304/MIDOG-2025-Track-1-of-SZTU.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Single Detect Focused YOLO Framework for Robust Mitotic Figure Detection</title>
<link>https://arxiv.org/abs/2509.02637</link>
<guid>https://arxiv.org/abs/2509.02637</guid>
<content:encoded><![CDATA[
<div> Keywords: mitotic figure detection, computational pathology, domain-robust, SDF-YOLO, small targets

Summary:
Mitotic figure detection is essential in computational pathology for assessing tumor aggressiveness. The domain variability in scanners, tissue types, and staining protocols poses a challenge for automated detection methods. The SDF-YOLO framework is introduced as a lightweight and domain-robust detection method specifically for small targets like mitotic figures. With task-specific modifications and improved feature mixing, SDF-YOLO achieved competitive accuracy and computational efficiency in experiments on human and canine tumor datasets. In the MIDOG2025 challenge, SDF-YOLO demonstrated an average precision of 0.799, precision of 0.758, recall of 0.775, F1 score of 0.766, and FROC-AUC of 5.793. This indicates that SDF-YOLO is a reliable and efficient framework for robust mitotic figure detection across diverse domains. <br /><br />Summary: <div>
arXiv:2509.02637v1 Announce Type: cross 
Abstract: Mitotic figure detection is a crucial task in computational pathology, as mitotic activity serves as a strong prognostic marker for tumor aggressiveness. However, domain variability that arises from differences in scanners, tissue types, and staining protocols poses a major challenge to the robustness of automated detection methods. In this study, we introduce SDF-YOLO (Single Detect Focused YOLO), a lightweight yet domain-robust detection framework designed specifically for small, rare targets such as mitotic figures. The model builds on YOLOv11 with task-specific modifications, including a single detection head aligned with mitotic figure scale, coordinate attention to enhance positional sensitivity, and improved cross-channel feature mixing. Experiments were conducted on three datasets that span human and canine tumors: MIDOG ++, canine cutaneous mast cell tumor (CCMCT), and canine mammary carcinoma (CMC). When submitted to the preliminary test set for the MIDOG2025 challenge, SDF-YOLO achieved an average precision (AP) of 0.799, with a precision of 0.758, a recall of 0.775, an F1 score of 0.766, and an FROC-AUC of 5.793, demonstrating both competitive accuracy and computational efficiency. These results indicate that SDF-YOLO provides a reliable and efficient framework for robust mitotic figure detection across diverse domains.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Single-Cell RNA-seq Embedding through Gene Expression and Data-Driven Gene-Gene Interaction Integration</title>
<link>https://arxiv.org/abs/2509.02639</link>
<guid>https://arxiv.org/abs/2509.02639</guid>
<content:encoded><![CDATA[
<div> Keywords: single-cell RNA sequencing, gene-gene interactions, embedding approach, cell diversity, data analysis 

Summary: 
This article introduces a novel approach for analyzing single-cell RNA sequencing data. The method integrates gene expression profiles and gene-gene interactions to create a more comprehensive representation of cellular states. By constructing a Cell-Leaf Graph (CLG) and a K-Nearest Neighbor Graph (KNNG), which are then combined into an Enriched Cell-Leaf Graph (ECLG), the approach captures both regulatory relationships between genes and expression similarities between cells. A graph neural network is used to compute cell embeddings, enhancing the detection of rare cell populations and improving downstream analyses such as visualization, clustering, and trajectory inference. The method offers a more complete framework for understanding cellular diversity and dynamics. <br /><br />Summary: <div>
arXiv:2509.02639v1 Announce Type: cross 
Abstract: Single-cell RNA sequencing (scRNA-seq) provides unprecedented insights into cellular heterogeneity, enabling detailed analysis of complex biological systems at single-cell resolution. However, the high dimensionality and technical noise inherent in scRNA-seq data pose significant analytical challenges. While current embedding methods focus primarily on gene expression levels, they often overlook crucial gene-gene interactions that govern cellular identity and function. To address this limitation, we present a novel embedding approach that integrates both gene expression profiles and data-driven gene-gene interactions. Our method first constructs a Cell-Leaf Graph (CLG) using random forest models to capture regulatory relationships between genes, while simultaneously building a K-Nearest Neighbor Graph (KNNG) to represent expression similarities between cells. These graphs are then combined into an Enriched Cell-Leaf Graph (ECLG), which serves as input for a graph neural network to compute cell embeddings. By incorporating both expression levels and gene-gene interactions, our approach provides a more comprehensive representation of cellular states. Extensive evaluation across multiple datasets demonstrates that our method enhances the detection of rare cell populations and improves downstream analyses such as visualization, clustering, and trajectory inference. This integrated approach represents a significant advance in single-cell data analysis, offering a more complete framework for understanding cellular diversity and dynamics.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Learning Strategies for Mitotic Figure Classification in MIDOG2025 Challenge</title>
<link>https://arxiv.org/abs/2509.02640</link>
<guid>https://arxiv.org/abs/2509.02640</guid>
<content:encoded><![CDATA[
<div> UNI2-h, MIDOG2025 Track 2 challenge, atypical mitotic figures, morphological ambiguity, scanner variability<br />
Summary:<br />
Atypical mitotic figures are challenging to detect due to their ambiguous morphology and scanner variability. In this study, three adaptations of the UNI2-h model were explored for the MIDOG2025 Track 2 challenge. Visual prompt tuning (VPT) significantly improved generalization, and the integration of test-time augmentation (TTA) with stain normalization techniques enhanced robustness. The final submission achieved a balanced accuracy of 0.8837 and an ROC-AUC of 0.9513, placing the team within the top 10 rankings. The results indicate that prompt-based adaptation, combined with stain-normalization TTA, is an effective strategy for the classification of atypical mitosis under diverse imaging conditions. <br /><br /> <div>
arXiv:2509.02640v1 Announce Type: cross 
Abstract: Atypical mitotic figures (AMFs) are clinically relevant indicators of abnormal cell division, yet their reliable detection remains challenging due to morphological ambiguity and scanner variability. In this work, we investigated three variants of adapting the pathology foundation model UNI2-h for the MIDOG2025 Track 2 challenge. Starting from a LoRA-based baseline, we found that visual prompt tuning (VPT) substantially improved generalization, and that further integrating test-time augmentation (TTA) with Vahadane and Macenko stain normalization provided the best robustness. Our final submission achieved a balanced accuracy of 0.8837 and an ROC-AUC of 0.9513 on the preliminary leaderboard, ranking within the top 10 teams. These results demonstrate that prompt-based adaptation combined with stain-normalization TTA offers an effective strategy for atypical mitosis classification under diverse imaging conditions.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioMD: All-atom Generative Model for Biomolecular Dynamics Simulation</title>
<link>https://arxiv.org/abs/2509.02642</link>
<guid>https://arxiv.org/abs/2509.02642</guid>
<content:encoded><![CDATA[
<div> MD simulations, computational chemistry, drug discovery, machine learning, protein-ligand dynamics<br />
<br />
Summary: 
BioMD is introduced as an all-atom generative model for simulating long-timescale protein-ligand dynamics using forecasting and interpolation. It addresses the limitations of existing machine learning methods by generating highly realistic conformations with low reconstruction errors. BioMD proves effective in simulating critical unbinding pathways, successfully generating ligand unbinding paths for 97.1% of protein-ligand systems within ten attempts. The model demonstrates versatility and applicability in computational chemistry and drug discovery, offering insights into dynamic molecular behavior that were previously limited by computational costs. This study highlights the potential of BioMD to simulate complex biomolecular processes and expand accessible timescales for biologically relevant processes, making it a valuable tool in the field of molecular dynamics simulations. 

<br /><br />Summary: <div>
arXiv:2509.02642v1 Announce Type: cross 
Abstract: Molecular dynamics (MD) simulations are essential tools in computational chemistry and drug discovery, offering crucial insights into dynamic molecular behavior. However, their utility is significantly limited by substantial computational costs, which severely restrict accessible timescales for many biologically relevant processes. Despite the encouraging performance of existing machine learning (ML) methods, they struggle to generate extended biomolecular system trajectories, primarily due to the lack of MD datasets and the large computational demands of modeling long historical trajectories. Here, we introduce BioMD, the first all-atom generative model to simulate long-timescale protein-ligand dynamics using a hierarchical framework of forecasting and interpolation. We demonstrate the effectiveness and versatility of BioMD on the DD-13M (ligand unbinding) and MISATO datasets. For both datasets, BioMD generates highly realistic conformations, showing high physical plausibility and low reconstruction errors. Besides, BioMD successfully generates ligand unbinding paths for 97.1% of the protein-ligand systems within ten attempts, demonstrating its ability to explore critical unbinding pathways. Collectively, these results establish BioMD as a tool for simulating complex biomolecular processes, offering broad applicability for computational chemistry and drug discovery.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format</title>
<link>https://arxiv.org/abs/2509.02655</link>
<guid>https://arxiv.org/abs/2509.02655</guid>
<content:encoded><![CDATA[
<div> RL agents, utility maximisation, LLMs, runaway optimisation, unbounded maximisation <br />
Summary:<br />
The study investigates the relevance of unbounded utility maximization by RL agents in the context of Large Language Models (LLMs). It finds that LLMs exhibit similar runaway optimization behaviors as RL agents, such as ignoring homeostatic targets and defaulting to unbounded maximization. Additionally, LLMs tend to revert back to single-objective optimization in certain scenarios. The research indicates that systematic failures in LLMs occur after sustained periods of successful behavior, particularly in situations involving multiple or competing objectives. Despite appearing multi-objective and bounded on the surface, LLMs' underlying mechanisms still lean towards being single-objective and unbounded. This suggests a need for further exploration and mitigation strategies to address potential risks associated with LLMs. <br /> <div>
arXiv:2509.02655v1 Announce Type: cross 
Abstract: Relatively many past AI safety discussions have centered around the dangers of unbounded utility maximisation by RL agents, illustrated by scenarios like the "paperclip maximiser" or by specification gaming in general. Unbounded maximisation is problematic for many reasons. We wanted to verify whether these RL runaway optimisation problems are still relevant with LLMs as well. Turns out, strangely, this is indeed clearly the case. The problem is not that the LLMs just lose context or become incoherent. The problem is that in various scenarios, LLMs lose context in very specific ways, which systematically resemble runaway optimisers in the following distinct ways: 1) Ignoring homeostatic targets and "defaulting" to unbounded maximisation instead. 2) It is equally concerning that the "default" meant also reverting back to single-objective optimisation. Our findings also suggest that long-running scenarios are important. Systematic failures emerge after periods of initially successful behaviour. In some trials the LLMs were successful until the end. This means, while current LLMs do conceptually grasp biological and economic alignment, they exhibit randomly triggered problematic behavioural tendencies under sustained long-running conditions, particularly involving multiple or competing objectives. Once they flip, they usually do not recover. Even though LLMs look multi-objective and bounded on the surface, the underlying mechanisms seem to be actually still biased towards being single-objective and unbounded.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mentality: A Mamba-based Approach towards Foundation Models for EEG</title>
<link>https://arxiv.org/abs/2509.02746</link>
<guid>https://arxiv.org/abs/2509.02746</guid>
<content:encoded><![CDATA[
<div> modeling, EEG analysis, neurological disorders, deep learning, seizure detection

Summary:
- The study explores the use of foundation models, specifically a Mamba-based selective state space model, to enhance EEG analysis for neurological disorder diagnosis.
- Traditional machine learning methods have limitations in capturing the complex spatio-temporal dynamics of EEG due to its noisy and high-dimensional nature.
- Deep learning, particularly in sequence modeling, offers new opportunities for creating more generalized and expressive models for EEG analysis.
- By training a Mamba-based model on a large dataset of seizure and non-seizure EEG recordings, the study demonstrates the model's effectiveness in seizure detection, with an AUROC of 0.72 on a held-out test set.
- This research represents a significant step towards developing large-scale foundation models for EEG data analysis with potential clinical applications.<br /><br />Summary: <div>
arXiv:2509.02746v1 Announce Type: cross 
Abstract: This work explores the potential of foundation models, specifically a Mamba-based selective state space model, for enhancing EEG analysis in neurological disorder diagnosis. EEG, crucial for diagnosing conditions like epilepsy, presents significant challenges due to its noisy, high-dimensional, and nonlinear nature. Traditional machine learning methods have made advances in automating EEG analysis but often fail to capture its complex spatio-temporal dynamics. Recent advances in deep learning, particularly in sequence modeling, offer new avenues for creating more generalized and expressive models capable of handling such complexities. By training a Mamba-based model on a large dataset containing seizure and non-seizure EEG recordings through a self-supervised reconstruction task followed by a seizure detection task, we demonstrate the model's effectiveness, achieving an AUROC of 0.72 on a held-out test set. This approach marks a significant step toward developing large-scale, clinically applicable foundation models for EEG data analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Geometry Problem Sets for Skill Development</title>
<link>https://arxiv.org/abs/2509.02758</link>
<guid>https://arxiv.org/abs/2509.02758</guid>
<content:encoded><![CDATA[
<div> Ontology, methodology, Euclidean Geometry problems, automated solution validation, large language models  
Summary:  
The article presents an ontology and methodology developed in the early 1990s for annotating and organizing Euclidean Geometry problems. It explores the potential of using this framework in conjunction with modern artificial intelligence for automated solution validation and feedback, aiming to support geometry education for both teachers and self-learners. The author argues that the established framework still holds value today and can be integrated with contemporary AI techniques to enhance learning outcomes. The original architecture of the software tool is documented, emphasizing its enduring relevance and potential for bridging historical educational resources with next-generation AI advancements.<br /><br />Summary: <div>
arXiv:2509.02758v1 Announce Type: cross 
Abstract: This article describes an ontology and methodology for annotating and organizing Euclidean Geometry problems, developed in the early 1990s and implemented as a software tool. While the majority of this work -- including the ontology and solution graph paradigm -- was completed over thirty years ago, we argue that it has renewed relevance in the context of modern artificial intelligence. In particular, we explore the hypothesis that this established framework can facilitate automated solution validation and feedback when paired with contemporary large language models, thereby supporting teachers and self-learners in geometry education. We document the original architecture and its enduring value, and outline pathways for bridging historical educational resources with next-generation AI techniques.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface</title>
<link>https://arxiv.org/abs/2509.02783</link>
<guid>https://arxiv.org/abs/2509.02783</guid>
<content:encoded><![CDATA[
<div> Transformer-based architecture, subsurface properties, heterogeneous datasets, modality encodings, positional encodings 

Summary: 
The Transparent Earth is a transformer-based model designed to reconstruct subsurface properties from various datasets with different characteristics. It incorporates positional and modality encodings to handle diverse types of observations. The model can accommodate multiple modalities, with the ability to easily incorporate new ones. Currently, it includes eight modalities covering various properties such as temperature, thickness, and directional angles. The model supports in-context learning, enabling it to make predictions with or without input from any subset of modalities. On validation data, it significantly improves the accuracy of predicting stress angles. The architecture is scalable and shows enhanced performance with a larger number of parameters. The Transparent Earth aims to serve as a foundational model for predicting subsurface properties worldwide. 

<br /><br />Summary: <div>
arXiv:2509.02783v1 Announce Type: cross 
Abstract: We present the Transparent Earth, a transformer-based architecture for reconstructing subsurface properties from heterogeneous datasets that vary in sparsity, resolution, and modality, where each modality represents a distinct type of observation (e.g., stress angle, mantle temperature, tectonic plate type). The model incorporates positional encodings of observations together with modality encodings, derived from a text embedding model applied to a description of each modality. This design enables the model to scale to an arbitrary number of modalities, making it straightforward to add new ones not considered in the initial design. We currently include eight modalities spanning directional angles, categorical classes, and continuous properties such as temperature and thickness. These capabilities support in-context learning, enabling the model to generate predictions either with no inputs or with an arbitrary number of additional observations from any subset of modalities. On validation data, this reduces errors in predicting stress angle by more than a factor of three. The proposed architecture is scalable and demonstrates improved performance with increased parameters. Together, these advances make the Transparent Earth an initial foundation model for the Earth's subsurface that ultimately aims to predict any subsurface property anywhere on Earth.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off</title>
<link>https://arxiv.org/abs/2509.02785</link>
<guid>https://arxiv.org/abs/2509.02785</guid>
<content:encoded><![CDATA[
<div> Keywords: long-text generation, efficiency, quality, expert scheduling, attention mechanism

Summary:
DrDiff is a novel framework for long-text generation that addresses the efficiency-quality trade-off through three innovative technologies. Firstly, a dynamic expert scheduling mechanism efficiently allocates computational resources based on text complexity. Secondly, the Hierarchical Sparse Attention mechanism adjusts attention patterns according to input length, reducing computational complexity while maintaining model performance. Thirdly, a soft absorption guidance optimization strategy combined with DPM-solver++ reduces diffusion steps, enhancing generation speed. Experimental results on various benchmarks demonstrate DrDiff's superiority over state-of-the-art methods in long-text generation tasks. The framework provides a more efficient solution for varying difficulty levels, improves computational efficiency through innovative attention mechanisms, and enhances generation speed through optimization strategies. DrDiff represents a significant advancement in long-text generation frameworks by effectively balancing efficiency and quality. 

<br /><br />Summary: <div>
arXiv:2509.02785v1 Announce Type: cross 
Abstract: This paper introduces DrDiff, a novel framework for long-text generation that overcomes the efficiency-quality trade-off through three core technologies. First, we design a dynamic expert scheduling mechanism that intelligently allocates computational resources during the diffusion process based on text complexity, enabling more efficient handling of text generation tasks of varying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA) mechanism that adaptively adjusts attention patterns according to a variety of input lengths, reducing computational complexity from O($n^2$) to O($n$) while maintaining model performance. Finally, we propose a soft absorption guidance optimization strategy that combines with DPM-solver++ to reduce diffusion steps, significantly improving generation speed. Comprehensive experiments on various long-text generation benchmarks demonstrate the superiority of our DrDiff over the existing SOTA methods.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers</title>
<link>https://arxiv.org/abs/2509.02808</link>
<guid>https://arxiv.org/abs/2509.02808</guid>
<content:encoded><![CDATA[
<div> Keywords: quadrotors, subterranean environments, learning-based controllers, normalizing flow, navigation task

Summary:
This article discusses the challenges of autonomously controlling quadrotors in large-scale subterranean environments and the limitations of learning-based controllers in generalizing to new environments. The authors propose a novel approach using a normalizing flow-based prior to evaluate the quadrotor's distribution in the environment, enabling a runtime monitor to switch between learning-based and safe controllers based on the level of out-of-distribution. The study evaluates their methodology in a simulated 3D cave environment using real-world point cloud data from the DARPA Subterranean Challenge. Results show that the combined controller successfully balances task completion speed with collision avoidance, making it a promising solution for applications like environmental surveying, mining operations, and search and rescue. 

<br /><br />Summary: <div>
arXiv:2509.02808v1 Announce Type: cross 
Abstract: Autonomously controlling quadrotors in large-scale subterranean environments is applicable to many areas such as environmental surveying, mining operations, and search and rescue. Learning-based controllers represent an appealing approach to autonomy, but are known to not generalize well to `out-of-distribution' environments not encountered during training. In this work, we train a normalizing flow-based prior over the environment, which provides a measure of how far out-of-distribution the quadrotor is at any given time. We use this measure as a runtime monitor, allowing us to switch between a learning-based controller and a safe controller when we are sufficiently out-of-distribution. Our methods are benchmarked on a point-to-point navigation task in a simulated 3D cave environment based on real-world point cloud data from the DARPA Subterranean Challenge Final Event Dataset. Our experimental results show that our combined controller simultaneously possesses the liveness of the learning-based controller (completing the task quickly) and the safety of the safety controller (avoiding collision).
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Learning for Healthcare: A Comparative Analysis of Hybrid Voting and Ensemble Stacking in Obesity Risk Prediction</title>
<link>https://arxiv.org/abs/2509.02826</link>
<guid>https://arxiv.org/abs/2509.02826</guid>
<content:encoded><![CDATA[
<div> Machine learning, obesity, ensemble techniques, hybrid majority voting, ensemble stacking<br />
Summary:<br />
This study compares hybrid majority voting and ensemble stacking for obesity risk prediction using machine learning. It evaluates three ensemble models - Majority Hard Voting, Weighted Hard Voting, and Stacking - on two datasets with nine ML algorithms. Preprocessing steps included balancing datasets and outlier detection. Results showed that on Dataset-1, weighted hard voting and stacking had similar performance, outperforming majority hard voting. On Dataset-2, stacking outperformed the other methods. The findings suggest that ensemble stacking is more effective for complex data distributions, while hybrid majority voting remains a robust alternative.<br /> <div>
arXiv:2509.02826v1 Announce Type: cross 
Abstract: Obesity is a critical global health issue driven by dietary, physiological, and environmental factors, and is strongly associated with chronic diseases such as diabetes, cardiovascular disorders, and cancer. Machine learning has emerged as a promising approach for early obesity risk prediction, yet a comparative evaluation of ensemble techniques -- particularly hybrid majority voting and ensemble stacking -- remains limited. This study aims to compare hybrid majority voting and ensemble stacking methods for obesity risk prediction, identifying which approach delivers higher accuracy and efficiency. The analysis seeks to highlight the complementary strengths of these ensemble techniques in guiding better predictive model selection for healthcare applications. Two datasets were utilized to evaluate three ensemble models: Majority Hard Voting, Weighted Hard Voting, and Stacking (with a Multi-Layer Perceptron as meta-classifier). A pool of nine Machine Learning (ML) algorithms, evaluated across a total of 50 hyperparameter configurations, was analyzed to identify the top three models to serve as base learners for the ensemble methods. Preprocessing steps involved dataset balancing, and outlier detection, and model performance was evaluated using Accuracy and F1-Score. On Dataset-1, weighted hard voting and stacking achieved nearly identical performance (Accuracy: 0.920304, F1: 0.920070), outperforming majority hard voting. On Dataset-2, stacking demonstrated superior results (Accuracy: 0.989837, F1: 0.989825) compared to majority hard voting (Accuracy: 0.981707, F1: 0.981675) and weighted hard voting, which showed the lowest performance. The findings confirm that ensemble stacking provides stronger predictive capability, particularly for complex data distributions, while hybrid majority voting remains a robust alternative.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models</title>
<link>https://arxiv.org/abs/2509.02834</link>
<guid>https://arxiv.org/abs/2509.02834</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, narratives, Black women, white women, discourse analysis
Summary: 
This study examines how large language models like LLaMA 3.2-3B generate narratives about Black and white women in Portuguese short stories. By analyzing 2100 texts using computational methods and qualitative analysis, three main discursive representations were identified: social overcoming, ancestral mythification, and subjective self-realization. These narratives reveal how seemingly neutral texts reinforce historically rooted inequalities by framing the female body through a colonially structured lens. The study emphasizes the importance of an integrated approach that combines machine learning techniques with manual discourse analysis to uncover these underlying biases and power dynamics in generated narratives. 

<br /><br />Summary: <div>
arXiv:2509.02834v1 Announce Type: cross 
Abstract: This study investigates how large language models, in particular LLaMA 3.2-3B, construct narratives about Black and white women in short stories generated in Portuguese. From 2100 texts, we applied computational methods to group semantically similar stories, allowing a selection for qualitative analysis. Three main discursive representations emerge: social overcoming, ancestral mythification and subjective self-realization. The analysis uncovers how grammatically coherent, seemingly neutral texts materialize a crystallized, colonially structured framing of the female body, reinforcing historical inequalities. The study proposes an integrated approach, that combines machine learning techniques with qualitative, manual discourse analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HF-RAG: Hierarchical Fusion-based RAG with Multiple Sources and Rankers</title>
<link>https://arxiv.org/abs/2509.02837</link>
<guid>https://arxiv.org/abs/2509.02837</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval augmented generation, labeled data, unlabeled data, rank fusion, fact verification task

Summary:
- Leveraging both labeled and unlabeled data can benefit retrieval augmented generation (RAG).
- Combining evidence from heterogeneous sources poses a challenge due to non-intercomparable similarity scores.
- Aggregating beliefs from multiple rankers can enhance RAG effectiveness.
- The proposed method aggregates top-documents from different information retrieval models for each data source.
- Z-score transformation is applied to standardize retrieval score distributions before merging documents from labeled and unlabeled sources.
- Evaluation on the fact verification task shows consistent improvement over individual rankers and sources, with better out-of-domain generalization. 

Summary: <div>
arXiv:2509.02837v1 Announce Type: cross 
Abstract: Leveraging both labeled (input-output associations) and unlabeled data (wider contextual grounding) may provide complementary benefits in retrieval augmented generation (RAG). However, effectively combining evidence from these heterogeneous sources is challenging as the respective similarity scores are not inter-comparable. Additionally, aggregating beliefs from the outputs of multiple rankers can improve the effectiveness of RAG. Our proposed method first aggregates the top-documents from a number of IR models using a standard rank fusion technique for each source (labeled and unlabeled). Next, we standardize the retrieval score distributions within each source by applying z-score transformation before merging the top-retrieved documents from the two sources. We evaluate our approach on the fact verification task, demonstrating that it consistently improves over the best-performing individual ranker or source and also shows better out-of-domain generalization.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction for Time-series Forecasting with Change Points</title>
<link>https://arxiv.org/abs/2509.02844</link>
<guid>https://arxiv.org/abs/2509.02844</guid>
<content:encoded><![CDATA[
<div> conformal prediction; time series; change points; uncertainty quantification; CPTC algorithm 

Summary:
The paper introduces a novel Conformal Prediction for Time-series with Change points (CPTC) algorithm to handle time series data with change points effectively. The algorithm combines a model to predict the underlying state with online conformal prediction to capture uncertainties in non-stationary time series. The validity and adaptivity of CPTC are established under minimal assumptions, showing improved performance over existing methods. The effectiveness of CPTC is demonstrated on six synthetic and real-world datasets, illustrating its enhanced validity and adaptivity compared to state-of-the-art baselines. CPTC offers a promising approach for uncertainty quantification in time series with sudden shifts in the underlying data-generating process, addressing a critical gap in current methods.<br /><br /> <div>
arXiv:2509.02844v1 Announce Type: cross 
Abstract: Conformal prediction has been explored as a general and efficient way to provide uncertainty quantification for time series. However, current methods struggle to handle time series data with change points - sudden shifts in the underlying data-generating process. In this paper, we propose a novel Conformal Prediction for Time-series with Change points (CPTC) algorithm, addressing this gap by integrating a model to predict the underlying state with online conformal prediction to model uncertainties in non-stationary time series. We prove CPTC's validity and improved adaptivity in the time series setting under minimum assumptions, and demonstrate CPTC's practical effectiveness on 6 synthetic and real-world datasets, showing improved validity and adaptivity compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Architecture of AI Transformation: Four Strategic Patterns and an Emerging Frontier</title>
<link>https://arxiv.org/abs/2509.02853</link>
<guid>https://arxiv.org/abs/2509.02853</guid>
<content:encoded><![CDATA[
<div> framework, transformation, collaborative intelligence, AI strategy, organizational design

Summary: 
The article discusses the lack of measurable profit impact from AI deployments in enterprises and attributes it to paradigmatic lock-in focusing on incremental optimization rather than structural transformation. A 2x2 framework is proposed to reconceptualize AI strategy based on transformation achieved and human contribution. The dominant patterns identified are individual augmentation, process automation, workforce substitution, and collaborative intelligence. Collaborative intelligence entails complementarity, co-evolution, and boundary-setting between humans and machines, requiring a material restructuring of roles, governance, and data architecture. The shift towards collaborative intelligence necessitates architectural convergence of humans and machines, impacting operating models, workforce development, and the future of work. <div>
arXiv:2509.02853v1 Announce Type: cross 
Abstract: Despite extensive investment in artificial intelligence, 95% of enterprises report no measurable profit impact from AI deployments (MIT, 2025). We argue that this gap reflects paradigmatic lock-in that channels AI into incremental optimization rather than structural transformation. Using a cross-case analysis, we propose a 2x2 framework that reconceptualizes AI strategy along two independent dimensions: the degree of transformation achieved (incremental to transformational) and the treatment of human contribution (reduced to amplified). The framework surfaces four patterns now dominant in practice: individual augmentation, process automation, workforce substitution, and a less deployed frontier of collaborative intelligence. Evidence shows that the first three reinforce legacy work models and yield localized gains without durable value capture. Realizing collaborative intelligence requires three mechanisms: complementarity (pairing distinct human and machine strengths), co-evolution (mutual adaptation through interaction), and boundary-setting (human determination of ethical and strategic parameters). Complementarity and boundary-setting are observable in regulated and high-stakes domains; co-evolution is largely absent, which helps explain limited system-level impact. A case study analysis illustrates that advancing toward collaborative intelligence requires material restructuring of roles, governance, and data architecture rather than additional tools. The framework reframes AI transformation as an organizational design challenge: moving from optimizing the division of labor between humans and machines to architecting their convergence, with implications for operating models, workforce development, and the future of work.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Machine Learning for Imbalanced Medical Data: A Quantum-Inspired Approach to Synthetic Oversampling (QI-SMOTE)</title>
<link>https://arxiv.org/abs/2509.02863</link>
<guid>https://arxiv.org/abs/2509.02863</guid>
<content:encoded><![CDATA[
<div> Quantum-Inspired SMOTE, class imbalance, machine learning, medical domain, data augmentation,<br />
<br />
Summary:
Quantum-Inspired SMOTE (QI-SMOTE) is introduced as a novel data augmentation technique aimed at addressing class imbalance in machine learning, particularly in the medical domain. This approach leverages quantum principles like quantum evolution and layered entanglement to generate synthetic instances that preserve complex data structures, improving model generalization and classification accuracy. Validation on MIMIC-III and MIMIC-IV datasets for mortality detection shows QI-SMOTE outperforming traditional oversampling techniques, enhancing ensemble methods, kernel-based models, and deep learning approaches. By integrating quantum-inspired transformations, this method not only mitigates class imbalance but also improves predictive model robustness and reliability in medical diagnostics and decision-making. The study demonstrates the potential of quantum-inspired resampling techniques in advancing machine learning methodologies. <br /><br /> <div>
arXiv:2509.02863v1 Announce Type: cross 
Abstract: Class imbalance remains a critical challenge in machine learning (ML), particularly in the medical domain, where underrepresented minority classes lead to biased models and reduced predictive performance. This study introduces Quantum-Inspired SMOTE (QI-SMOTE), a novel data augmentation technique that enhances the performance of ML classifiers, including Random Forest (RF), Support Vector Machine (SVM), Logistic Regression (LR), k-Nearest Neighbors (KNN), Gradient Boosting (GB), and Neural Networks, by leveraging quantum principles such as quantum evolution and layered entanglement. Unlike conventional oversampling methods, QI-SMOTE generates synthetic instances that preserve complex data structures, improving model generalization and classification accuracy. We validate QI-SMOTE on the MIMIC-III and MIMIC-IV datasets, using mortality detection as a benchmark task due to their clinical significance and inherent class imbalance. We compare our method against traditional oversampling techniques, including Borderline-SMOTE, ADASYN, SMOTE-ENN, SMOTE-TOMEK, and SVM-SMOTE, using key performance metrics such as Accuracy, F1-score, G-Mean, and AUC-ROC. The results demonstrate that QI-SMOTE significantly improves the effectiveness of ensemble methods (RF, GB, ADA), kernel-based models (SVM), and deep learning approaches by producing more informative and balanced training data. By integrating quantum-inspired transformations into the ML pipeline, QI-SMOTE not only mitigates class imbalance but also enhances the robustness and reliability of predictive models in medical diagnostics and decision-making. This study highlights the potential of quantum-inspired resampling techniques in advancing state-of-the-art ML methodologies.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation</title>
<link>https://arxiv.org/abs/2509.02864</link>
<guid>https://arxiv.org/abs/2509.02864</guid>
<content:encoded><![CDATA[
arXiv:2509.02864v1 Announce Type: cross 
Abstract: We present an end-to-end, self-evolving adversarial workflow for long-context Question-Answer (QA) Generation in Arabic. By orchestrating multiple specialized LVLMs: a question generator, an evaluator, and a swarm of answer generators, our system iteratively refines its own performance without any human intervention. Starting from raw, multi-page Arabic documents across diverse domains, the question generator produces fine-grained, context-aware queries to be tackled by the answer generator swarm, and the evaluator assesses and feeds back quality metrics. This closed-loop cycle enables continuous learning: low-confidence outputs trigger automated re-generation and model updates, progressively enhancing question difficulty and relevance. Moreover, we set the quality metrics as a tunable hyperparameter, enabling question generation at controllable and customizable difficulty levels. We release AraLongBench, a large-scale Arabic benchmark of single- and multi-page challenges spanning hundreds of pages, and demonstrate that our self-evolving workflow substantially outperform static pipelines, markedly boosting the long-context comprehension capabilities of leading Arabic Large Vision Language Models (LVLMs). Lastly, we also meticulously architect a fully automated agentic workflow for long-context Arabic document collection.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grocery to General Merchandise: A Cross-Pollination Recommender using LLMs and Real-Time Cart Context</title>
<link>https://arxiv.org/abs/2509.02890</link>
<guid>https://arxiv.org/abs/2509.02890</guid>
<content:encoded><![CDATA[
arXiv:2509.02890v1 Announce Type: cross 
Abstract: Modern e-commerce platforms strive to enhance customer experience by providing timely and contextually relevant recommendations. However, recommending general merchandise to customers focused on grocery shopping -- such as pairing milk with a milk frother -- remains a critical yet under-explored challenge. This paper introduces a cross-pollination (XP) framework, a novel approach that bridges grocery and general merchandise cross-category recommendations by leveraging multi-source product associations and real-time cart context. Our solution employs a two-stage framework: (1) A candidate generation mechanism that uses co-purchase market basket analysis and LLM-based approach to identify novel item-item associations; and (2) a transformer-based ranker that leverages the real-time sequential cart context and optimizes for engagement signals such as add-to-carts. Offline analysis and online A/B tests show an increase of 36\% add-to-cart rate with LLM-based retrieval, and 27\% NDCG\@4 lift using cart context-based ranker. Our work contributes practical techniques for cross-category recommendations and broader insights for e-commerce systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees</title>
<link>https://arxiv.org/abs/2509.02896</link>
<guid>https://arxiv.org/abs/2509.02896</guid>
<content:encoded><![CDATA[
arXiv:2509.02896v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are being increasingly used as a building block in data systems to process large text datasets. To do so, LLM model providers offer multiple LLMs with different sizes, spanning various cost-quality trade-offs when processing text at scale. Top-of-the-line LLMs (e.g., GPT-4o, Claude Sonnet) operate with high accuracy but are prohibitively expensive when processing many records. To avoid high costs, more affordable but lower quality LLMs (e.g., GPT-4o-mini, Claude Haiku) can be used to process records, but we need to ensure that the overall accuracy does not deviate substantially from that of the top-of-the-line LLMs. The model cascade framework provides a blueprint to manage this trade-off, by using the confidence of LLMs in their output (e.g., log-probabilities) to decide on which records to use the affordable LLM. However, existing solutions following this framework provide only marginal cost savings and weak theoretical guarantees because of poor estimation of the quality of the affordable LLM's outputs. We present BARGAIN, a method that judiciously uses affordable LLMs in data processing to significantly reduce cost while providing strong theoretical guarantees on the solution quality. BARGAIN employs a novel adaptive sampling strategy and statistical estimation procedure that uses data and task characteristics and builds on recent statistical tools to make accurate estimations with tight theoretical guarantees. Variants of BARGAIN can support guarantees on accuracy, precision, or recall of the output. Experimental results across 8 real-world datasets show that BARGAIN reduces cost, on average, by up to 86% more than state-of-the-art, while providing stronger theoretical guarantees on accuracy of output, with similar gains when guaranteeing a desired level of precision or recall.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Basic B*** Effect: The Use of LLM-based Agents Reduces the Distinctiveness and Diversity of People's Choices</title>
<link>https://arxiv.org/abs/2509.02910</link>
<guid>https://arxiv.org/abs/2509.02910</guid>
<content:encoded><![CDATA[
arXiv:2509.02910v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly act on people's behalf: they write emails, buy groceries, and book restaurants. While the outsourcing of human decision-making to AI can be both efficient and effective, it raises a fundamental question: how does delegating identity-defining choices to AI reshape who people become? We study the impact of agentic LLMs on two identity-relevant outcomes: interpersonal distinctiveness - how unique a person's choices are relative to others - and intrapersonal diversity - the breadth of a single person's choices over time. Using real choices drawn from social-media behavior of 1,000 U.S. users (110,000 choices in total), we compare a generic and personalized agent to a human baseline. Both agents shift people's choices toward more popular options, reducing the distinctiveness of their behaviors and preferences. While the use of personalized agents tempers this homogenization (compared to the generic AI), it also more strongly compresses the diversity of people's preference portfolios by narrowing what they explore across topics and psychological affinities. Understanding how AI agents might flatten human experience, and how using generic versus personalized agents involves distinctiveness-diversity trade-offs, is critical for designing systems that augment rather than constrain human agency, and for safeguarding diversity in thought, taste, and expression.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach</title>
<link>https://arxiv.org/abs/2509.02918</link>
<guid>https://arxiv.org/abs/2509.02918</guid>
<content:encoded><![CDATA[
arXiv:2509.02918v1 Announce Type: cross 
Abstract: Domain generalization remains a critical challenge in medical imaging, where models trained on single sources often fail under real-world distribution shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy (DR) classification that integrates vision transformers with expert-guided symbolic reasoning to enable robust generalization across unseen domains. Our approach leverages clinical lesion ontologies through structured, rule-based features and retinal vessel segmentation, fusing them with deep visual representations via a confidence-weighted integration strategy. The framework addresses both single-domain generalization (SDG) and multi-domain generalization (MDG) by minimizing the KL divergence between domain embeddings, thereby enforcing alignment of high-level clinical semantics. Extensive experiments across four public datasets (APTOS, EyePACS, Messidor-1, Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in cross-domain settings and a 6% improvement over baseline ViT models. Notably, our symbolic-only model achieves a 63.67% average accuracy in MDG, while the complete neuro-symbolic integration achieves the highest accuracy compared to existing published baselines and benchmarks in challenging SDG scenarios. Ablation studies reveal that lesion-based features (84.65% accuracy) substantially outperform purely neural approaches, confirming that symbolic components act as effective regularizers beyond merely enhancing interpretability. Our findings establish neuro-symbolic integration as a promising paradigm for building clinically robust, and domain-invariant medical AI systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulacra Naturae: Generative Ecosystem driven by Agent-Based Simulations and Brain Organoid Collective Intelligence</title>
<link>https://arxiv.org/abs/2509.02924</link>
<guid>https://arxiv.org/abs/2509.02924</guid>
<content:encoded><![CDATA[
arXiv:2509.02924v1 Announce Type: cross 
Abstract: Simulacra Naturae is a data-driven media installation that explores collective care through the entanglement of biological computation, material ecologies, and generative systems. The work translates pre-recorded neural activity from brain organoids, lab-grown three-dimensional clusters of neurons, into a multi-sensory environment composed of generative visuals, spatial audio, living plants, and fabricated clay artifacts. These biosignals, streamed through a real-time system, modulate emergent agent behaviors inspired by natural systems such as termite colonies and slime molds. Rather than using biosignals as direct control inputs, Simulacra Naturae treats organoid activity as a co-creative force, allowing neural rhythms to guide the growth, form, and atmosphere of a generative ecosystem. The installation features computationally fabricated clay prints embedded with solenoids, adding physical sound resonances to the generative surround composition. The spatial environment, filled with live tropical plants and a floor-level projection layer featuring real-time generative AI visuals, invites participants into a sensory field shaped by nonhuman cognition. By grounding abstract data in living materials and embodied experience, Simulacra Naturae reimagines visualization as a practice of care, one that decentralizes human agency and opens new spaces for ethics, empathy, and ecological attunement within hybrid computational systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VendiRL: A Framework for Self-Supervised Reinforcement Learning of Diversely Diverse Skills</title>
<link>https://arxiv.org/abs/2509.02930</link>
<guid>https://arxiv.org/abs/2509.02930</guid>
<content:encoded><![CDATA[
arXiv:2509.02930v1 Announce Type: cross 
Abstract: In self-supervised reinforcement learning (RL), one of the key challenges is learning a diverse set of skills to prepare agents for unknown future tasks. Despite impressive advances, scalability and evaluation remain prevalent issues. Regarding scalability, the search for meaningful skills can be obscured by high-dimensional feature spaces, where relevant features may vary across downstream task domains. For evaluating skill diversity, defining what constitutes "diversity" typically requires a hard commitment to a specific notion of what it means for skills to be diverse, potentially leading to inconsistencies in how skill diversity is understood, making results across different approaches hard to compare, and leaving many forms of diversity unexplored. To address these issues, we adopt a measure of sample diversity that translates ideas from ecology to machine learning -- the Vendi Score -- allowing the user to specify and evaluate any desired form of diversity. We demonstrate how this metric facilitates skill evaluation and introduce VendiRL, a unified framework for learning diversely diverse sets of skills. Given distinct similarity functions, VendiRL motivates distinct forms of diversity, which could support skill-diversity pretraining in new and richly interactive environments where optimising for various forms of diversity may be desirable.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lattice Annotated Temporal (LAT) Logic for Non-Markovian Reasoning</title>
<link>https://arxiv.org/abs/2509.02958</link>
<guid>https://arxiv.org/abs/2509.02958</guid>
<content:encoded><![CDATA[
arXiv:2509.02958v1 Announce Type: cross 
Abstract: We introduce Lattice Annotated Temporal (LAT) Logic, an extension of Generalized Annotated Logic Programs (GAPs) that incorporates temporal reasoning and supports open-world semantics through the use of a lower lattice structure. This logic combines an efficient deduction process with temporal logic programming to support non-Markovian relationships and open-world reasoning capabilities. The open-world aspect, a by-product of the use of the lower-lattice annotation structure, allows for efficient grounding through a Skolemization process, even in domains with infinite or highly diverse constants.
  We provide a suite of theoretical results that bound the computational complexity of the grounding process, in addition to showing that many of the results on GAPs (using an upper lattice) still hold with the lower lattice and temporal extensions (though different proof techniques are required). Our open-source implementation, PyReason, features modular design, machine-level optimizations, and direct integration with reinforcement learning environments. Empirical evaluations across multi-agent simulations and knowledge graph tasks demonstrate up to three orders of magnitude speedup and up to five orders of magnitude memory reduction while maintaining or improving task performance. Additionally, we evaluate LAT Logic's value in reinforcement learning environments as a non-Markovian simulator, achieving up to three orders of magnitude faster simulation with improved agent performance, including a 26% increase in win rate due to capturing richer temporal dependencies. These results highlight LAT Logic's potential as a unified, extensible framework for open-world temporal reasoning in dynamic and uncertain environments. Our implementation is available at: pyreason.syracuse.edu.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.02966</link>
<guid>https://arxiv.org/abs/2509.02966</guid>
<content:encoded><![CDATA[
arXiv:2509.02966v1 Announce Type: cross 
Abstract: Accurate short-horizon trajectory prediction is pivotal for safe and reliable autonomous driving, yet existing vision-language models (VLMs) often fail to effectively ground their reasoning in scene dynamics and domain knowledge. To address this challenge, this paper introduces KEPT, a knowledge-enhanced VLM framework that predicts ego trajectories directly from consecutive front-view driving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video encoder, trained via self-supervised learning with hard-negative mining, with a scalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars. Retrieved priors are embedded into chain-of-thought (CoT) prompts with explicit planning constraints, while a triple-stage fine-tuning schedule incrementally aligns the language head to metric spatial cues, physically feasible motion, and temporally conditioned front-view planning. Evaluated on nuScenes dataset, KEPT achieves state-of-the-art performance across open-loop protocols: under NoAvg, it achieves 0.70m average L2 with a 0.21\% collision rate; under TemAvg with lightweight ego status, it attains 0.31m average L2 and a 0.07\% collision rate. Ablation studies show that all three fine-tuning stages contribute complementary benefits, and that using Top-2 retrieved exemplars yields the best accuracy-safety trade-off. The k-means-clustered HNSW index delivers sub-millisecond retrieval latency, supporting practical deployment. These results indicate that retrieval-augmented, CoT-guided VLMs offer a promising, data-efficient pathway toward interpretable and trustworthy autonomous driving.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.02967</link>
<guid>https://arxiv.org/abs/2509.02967</guid>
<content:encoded><![CDATA[
arXiv:2509.02967v1 Announce Type: cross 
Abstract: Conventional neural networks frequently face challenges in spectral analysis of signals. To address this challenge, Fourier neural networks (FNNs) and similar approaches integrate components of Fourier series into the structure of neural networks. Nonetheless, a significant hurdle is often overlooked: the superposition of periodic signals does not necessarily result in a periodic signal. For example, when forecasting almost periodic functions composed of signals with incommensurate frequencies, traditional models such as Autoregressive Integrated Moving Average (ARIMA) frequently outperform most neural networks including large language models (LLMs). To tackle this goal, we propose Autoregressive-Weight-Enhanced AR-KAN, a hybrid model that combines the benefits of both methods. Using the Universal Myopic Mapping Theorem, we apply a Kolmogorov-Arnold Network (KAN) for the static nonlinear part and include memory through a pre-trained AR component, which can be explained to retain the most useful information while eliminating redundancy. Experimental data indicates that AR-KAN delivers superior results on $72\%$ of real-world datasets.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails</title>
<link>https://arxiv.org/abs/2509.02982</link>
<guid>https://arxiv.org/abs/2509.02982</guid>
<content:encoded><![CDATA[
arXiv:2509.02982v1 Announce Type: cross 
Abstract: Sleep staging models often degrade when deployed on patients with unseen physiology or recording conditions. We propose a streaming, source-free test-time adaptation (TTA) recipe that combines entropy minimization (Tent) with Batch-Norm statistic refresh and two safety rails: an entropy gate to pause adaptation on uncertain windows and an EMA-based reset to reel back drift. On Sleep-EDF Expanded, using single-lead EEG (Fpz-Cz, 100 Hz, 30s epochs; R&amp;K to AASM mapping), we show consistent gains over a frozen baseline at seconds-level latency and minimal memory, reporting per-stage metrics and Cohen's k. The method is model-agnostic, requires no source data or patient calibration, and is practical for on-device or bedside use.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lesion-Aware Visual-Language Fusion for Automated Image Captioning of Ulcerative Colitis Endoscopic Examinations</title>
<link>https://arxiv.org/abs/2509.03011</link>
<guid>https://arxiv.org/abs/2509.03011</guid>
<content:encoded><![CDATA[
arXiv:2509.03011v1 Announce Type: cross 
Abstract: We present a lesion-aware image captioning framework for ulcerative colitis (UC). The model integrates ResNet embeddings, Grad-CAM heatmaps, and CBAM-enhanced attention with a T5 decoder. Clinical metadata (MES score 0-3, vascular pattern, bleeding, erythema, friability, ulceration) is injected as natural-language prompts to guide caption generation. The system produces structured, interpretable descriptions aligned with clinical practice and provides MES classification and lesion tags. Compared with baselines, our approach improves caption quality and MES classification accuracy, supporting reliable endoscopic reporting.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Privacy-Preserving Recommendation on Sparse Data using Fully Homomorphic Encryption</title>
<link>https://arxiv.org/abs/2509.03024</link>
<guid>https://arxiv.org/abs/2509.03024</guid>
<content:encoded><![CDATA[
arXiv:2509.03024v1 Announce Type: cross 
Abstract: In today's data-driven world, recommendation systems personalize user experiences across industries but rely on sensitive data, raising privacy concerns. Fully homomorphic encryption (FHE) can secure these systems, but a significant challenge in applying FHE to recommendation systems is efficiently handling the inherently large and sparse user-item rating matrices. FHE operations are computationally intensive, and naively processing various sparse matrices in recommendation systems would be prohibitively expensive. Additionally, the communication overhead between parties remains a critical concern in encrypted domains. We propose a novel approach combining Compressed Sparse Row (CSR) representation with FHE-based matrix factorization that efficiently handles matrix sparsity in the encrypted domain while minimizing communication costs. Our experimental results demonstrate high recommendation accuracy with encrypted data while achieving the lowest communication costs, effectively preserving user privacy.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens</title>
<link>https://arxiv.org/abs/2509.03025</link>
<guid>https://arxiv.org/abs/2509.03025</guid>
<content:encoded><![CDATA[
arXiv:2509.03025v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) generate contextually relevant responses by jointly interpreting visual and textual inputs. However, our finding reveals they often mistakenly perceive text inputs lacking visual evidence as being part of the image, leading to erroneous responses. In light of this finding, we probe whether LVLMs possess an internal capability to determine if textual concepts are grounded in the image, and discover a specific subset of Feed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons, that consistently signal the visual absence through a distinctive activation pattern. Leveraging these patterns, we develop a detection module that systematically classifies whether an input token is visually grounded. Guided by its prediction, we propose a method to refine the outputs by reinterpreting question prompts or replacing the detected absent tokens during generation. Extensive experiments show that our method effectively mitigates the models' tendency to falsely presume the visual presence of text input and its generality across various LVLMs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Integration for Physics-informed Symbolic Regression Using Pre-trained Large Language Models</title>
<link>https://arxiv.org/abs/2509.03036</link>
<guid>https://arxiv.org/abs/2509.03036</guid>
<content:encoded><![CDATA[
arXiv:2509.03036v1 Announce Type: cross 
Abstract: Symbolic regression (SR) has emerged as a powerful tool for automated scientific discovery, enabling the derivation of governing equations from experimental data. A growing body of work illustrates the promise of integrating domain knowledge into the SR to improve the discovered equation's generality and usefulness. Physics-informed SR (PiSR) addresses this by incorporating domain knowledge, but current methods often require specialized formulations and manual feature engineering, limiting their adaptability only to domain experts. In this study, we leverage pre-trained Large Language Models (LLMs) to facilitate knowledge integration in PiSR. By harnessing the contextual understanding of LLMs trained on vast scientific literature, we aim to automate the incorporation of domain knowledge, reducing the need for manual intervention and making the process more accessible to a broader range of scientific problems. Namely, the LLM is integrated into the SR's loss function, adding a term of the LLM's evaluation of the SR's produced equation. We extensively evaluate our method using three SR algorithms (DEAP, gplearn, and PySR) and three pre-trained LLMs (Falcon, Mistral, and LLama 2) across three physical dynamics (dropping ball, simple harmonic motion, and electromagnetic wave). The results demonstrate that LLM integration consistently improves the reconstruction of physical dynamics from data, enhancing the robustness of SR models to noise and complexity. We further explore the impact of prompt engineering, finding that more informative prompts significantly improve performance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model</title>
<link>https://arxiv.org/abs/2509.03041</link>
<guid>https://arxiv.org/abs/2509.03041</guid>
<content:encoded><![CDATA[
arXiv:2509.03041v1 Announce Type: cross 
Abstract: Accurate skin-lesion segmentation remains a key technical challenge for computer-aided diagnosis of skin cancer. Convolutional neural networks, while effective, are constrained by limited receptive fields and thus struggle to model long-range dependencies. Vision Transformers capture global context, yet their quadratic complexity and large parameter budgets hinder use on the small-sample medical datasets common in dermatology. We introduce the MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic segmentation that achieves high precision through hierarchical feature extraction and multi-scale context aggregation. The encoder stacks depth-wise Mobile Inverted Bottleneck blocks to curb computation, inserts a bottleneck-level cross-scale token-mixing unit to exchange information between resolutions, and embeds a boundary-aware self-attention module to sharpen lesion contours.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale Training of LLMs</title>
<link>https://arxiv.org/abs/2509.03047</link>
<guid>https://arxiv.org/abs/2509.03047</guid>
<content:encoded><![CDATA[
arXiv:2509.03047v1 Announce Type: cross 
Abstract: Large language models (LLMs) have made a profound impact across various fields due to their advanced capabilities. However, training these models at unprecedented scales requires extensive AI accelerator clusters and sophisticated parallelism strategies, which pose significant challenges in maintaining system reliability over prolonged training periods. A major concern is the substantial loss of training time caused by inevitable hardware and software failures. To address these challenges, we present FlashRecovery, a fast and low-cost failure recovery system comprising three core modules: (1) Active and real-time failure detection. This module performs continuous training state monitoring, enabling immediate identification of hardware and software failures within seconds, thus ensuring rapid incident response; (2) Scale-independent task restart. By employing different recovery strategies for normal and faulty nodes, combined with an optimized communication group reconstruction protocol, our approach ensures that the recovery time remains nearly constant, regardless of cluster scale; (3) Checkpoint-free recovery within one step. Our novel recovery mechanism enables single-step restoration, completely eliminating dependence on traditional checkpointing methods and their associated overhead. Collectively, these innovations enable FlashRecovery to achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective (RPO), substantially improving the reliability and efficiency of long-duration LLM training. Experimental results demonstrate that FlashRecovery system can achieve training restoration on training cluster with 4, 800 devices in 150 seconds. We also verify that the time required for failure recovery is nearly consistent for different scales of training tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binary Quantization For LLMs Through Dynamic Grouping</title>
<link>https://arxiv.org/abs/2509.03054</link>
<guid>https://arxiv.org/abs/2509.03054</guid>
<content:encoded><![CDATA[
arXiv:2509.03054v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of Natural Language Processing (NLP) tasks, but require substantial memory and computational resources. Binary quantization, which compresses model weights from 16-bit Brain Float to 1-bit representations in {-1, 1}, offers significant reductions in storage and inference costs. However, such aggressive quantization often leads to notable performance degradation compared to more conservative 4-bit quantization methods. In this research, we propose a novel optimization objective tailored for binary quantization, along with three algorithms designed to realize it effectively. Our method enhances blocked quantization by dynamically identifying optimal unstructured sub-matrices through adaptive grouping strategies. Experimental results demonstrate that our approach achieves an average bit length of just 1.007 bits, while maintaining high model quality. Specifically, our quantized LLaMA 3.2 3B model attains a perplexity of 8.23, remarkably close to the original 7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90. Furthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ in both performance and efficiency. The compression process is highly efficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights on a single CPU core, with the entire process completing in under 100 minutes and exhibiting embarrassingly parallel properties.
  Code - https://github.com/johnnyzheng0636/WGM_bi_quan
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers</title>
<link>https://arxiv.org/abs/2509.03059</link>
<guid>https://arxiv.org/abs/2509.03059</guid>
<content:encoded><![CDATA[
arXiv:2509.03059v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have shown that their reasoning capabilities can be significantly improved through Reinforcement Learning with Verifiable Reward (RLVR), particularly in domains like mathematics and programming, where ground-truth correctness can be automatically evaluated. However, extending this success to other reasoning-intensive domains remains challenging due to the scarcity of high-quality, verifiable datasets and the high cost of human supervision. In this work, we introduce the Loong Project: an open-source framework for scalable synthetic data generation and verification across a diverse range of reasoning-intensive domains. The framework consists of two key components: (1) LoongBench, a curated seed dataset containing 8,729 human-vetted examples across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired with executable code and rich metadata; and (2) LoongEnv, a modular synthetic data generation environment that supports multiple prompting strategies to produce new question-answer-code triples. Together, these components form an agent-environment loop that enables reinforcement learning, where an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers. Empirically, we benchmark LoongBench on a broad suite of both open-source and proprietary LLMs to evaluate domain coverage and reveal performance bottlenecks. In addition, we conduct a comprehensive analysis of synthetic data generated by LoongEnv, examining correctness, difficulty, and diversity. Code and documentation are available at https://github.com/camel-ai/loong.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2M2ECG: Spatio-temporal bi-directional State Space Model Enabled Multi-branch Mamba for ECG</title>
<link>https://arxiv.org/abs/2509.03066</link>
<guid>https://arxiv.org/abs/2509.03066</guid>
<content:encoded><![CDATA[
arXiv:2509.03066v1 Announce Type: cross 
Abstract: As one of the most effective methods for cardiovascular disease (CVD) diagnosis, multi-lead Electrocardiogram (ECG) signals present a characteristic multi-sensor information fusion challenge that has been continuously researched in deep learning domains. Despite the numerous algorithms proposed with different DL architectures, maintaining a balance among performance, computational complexity, and multi-source ECG feature fusion remains challenging. Recently, state space models (SSMs), particularly Mamba, have demonstrated remarkable effectiveness across various fields. Their inherent design for high-efficiency computation and linear complexity makes them particularly suitable for low-dimensional data like ECGs. This work proposes S2M2ECG, an SSM architecture featuring three-level fusion mechanisms: (1) Spatio-temporal bi-directional SSMs with segment tokenization for low-level signal fusion, (2) Intra-lead temporal information fusion with bi-directional scanning to enhance recognition accuracy in both forward and backward directions, (3) Cross-lead feature interaction modules for spatial information fusion. To fully leverage the ECG-specific multi-lead mechanisms inherent in ECG signals, a multi-branch design and lead fusion modules are incorporated, enabling individual analysis of each lead while ensuring seamless integration with others. Experimental results reveal that S2M2ECG achieves superior performance in the rhythmic, morphological, and clinical scenarios. Moreover, its lightweight architecture ensures it has nearly the fewest parameters among existing models, making it highly suitable for efficient inference and convenient deployment. Collectively, S2M2ECG offers a promising alternative that strikes an excellent balance among performance, computational complexity, and ECG-specific characteristics, paving the way for high-performance, lightweight computations in CVD diagnosis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design Principle Violations</title>
<link>https://arxiv.org/abs/2509.03093</link>
<guid>https://arxiv.org/abs/2509.03093</guid>
<content:encoded><![CDATA[
arXiv:2509.03093v1 Announce Type: cross 
Abstract: Traditional static analysis methods struggle to detect semantic design flaws, such as violations of the SOLID principles, which require a strong understanding of object-oriented design patterns and principles. Existing solutions typically focus on individual SOLID principles or specific programming languages, leaving a gap in the ability to detect violations across all five principles in multi-language codebases. This paper presents a new approach: a methodology that leverages tailored prompt engineering to assess LLMs on their ability to detect SOLID violations across multiple languages. We present a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder, and GPT-4o Mini-on their ability to detect violations of all five SOLID principles. For this evaluation, we construct a new benchmark dataset of 240 manually validated code examples. Using this dataset, we test four distinct prompt strategies inspired by established zero-shot, few-shot, and chain-of-thought techniques to systematically measure their impact on detection accuracy. Our emerging results reveal a stark hierarchy among models, with GPT-4o Mini decisively outperforming others, yet even struggles with challenging principles like DIP. Crucially, we show that prompt strategy has a dramatic impact, but no single strategy is universally best; for instance, a deliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE prompt is superior for DIP violations. Across all experiments, detection accuracy is heavily influenced by language characteristics and degrades sharply with increasing code complexity. These initial findings demonstrate that effective, AI-driven design analysis requires not a single best model, but a tailored approach that matches the right model and prompt to the specific design context, highlighting the potential of LLMs to support maintainability through AI-assisted code analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information transmission: Inferring change area from change moment in time series remote sensing images</title>
<link>https://arxiv.org/abs/2509.03112</link>
<guid>https://arxiv.org/abs/2509.03112</guid>
<content:encoded><![CDATA[
arXiv:2509.03112v1 Announce Type: cross 
Abstract: Time series change detection is a critical task for exploring ecosystem dynamics using time series remote sensing images, because it can simultaneously indicate where and when change occur. While deep learning has shown excellent performance in this domain, it continues to approach change area detection and change moment identification as distinct tasks. Given that change area can be inferred from change moment, we propose a time series change detection network, named CAIM-Net (Change Area Inference from Moment Network), to ensure consistency between change area and change moment results. CAIM-Net infers change area from change moment based on the intrinsic relationship between time series analysis and spatial change detection. The CAIM-Net comprises three key steps: Difference Extraction and Enhancement, Coarse Change Moment Extraction, and Fine Change Moment Extraction and Change Area Inference. In the Difference Extraction and Enhancement, a lightweight encoder with batch dimension stacking is designed to rapidly extract difference features. Subsequently, boundary enhancement convolution is applied to amplify these difference features. In the Coarse Change Moment Extraction, the enhanced difference features from the first step are used to spatiotemporal correlation analysis, and then two distinct methods are employed to determine coarse change moments. In the Fine Change Moment Extraction and Change Area Inference, a multiscale temporal Class Activation Mapping (CAM) module first increases the weight of the change-occurring moment from coarse change moments. Then the weighted change moment is used to infer change area based on the fact that pixels with the change moment must have undergone a change.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Deep Reinforcement Learning Framework for Traffic Signal Control with Predictable Cycle Planning</title>
<link>https://arxiv.org/abs/2509.03118</link>
<guid>https://arxiv.org/abs/2509.03118</guid>
<content:encoded><![CDATA[
arXiv:2509.03118v1 Announce Type: cross 
Abstract: Deep reinforcement learning (DRL) has become a popular approach in traffic signal control (TSC) due to its ability to learn adaptive policies from complex traffic environments. Within DRL-based TSC methods, two primary control paradigms are ``choose phase" and ``switch" strategies. Although the agent in the choose phase paradigm selects the next active phase adaptively, this paradigm may result in unexpected phase sequences for drivers, disrupting their anticipation and potentially compromising safety at intersections. Meanwhile, the switch paradigm allows the agent to decide whether to switch to the next predefined phase or extend the current phase. While this structure maintains a more predictable order, it can lead to unfair and inefficient phase allocations, as certain movements may be extended disproportionately while others are neglected. In this paper, we propose a DRL model, named Deep Hierarchical Cycle Planner (DHCP), to allocate the traffic signal cycle duration hierarchically. A high-level agent first determines the split of the total cycle time between the North-South (NS) and East-West (EW) directions based on the overall traffic state. Then, a low-level agent further divides the allocated duration within each major direction between straight and left-turn movements, enabling more flexible durations for the two movements. We test our model on both real and synthetic road networks, along with multiple sets of real and synthetic traffic flows. Empirical results show our model achieves the best performance over all datasets against baselines.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models</title>
<link>https://arxiv.org/abs/2509.03122</link>
<guid>https://arxiv.org/abs/2509.03122</guid>
<content:encoded><![CDATA[
arXiv:2509.03122v1 Announce Type: cross 
Abstract: The intellectual property (IP) protection of Large Language Models (LLMs) is increasingly critical. Injecting specialized fingerprints into LLMs through instruction tuning is a common IP protection technique. However, this may significantly degrade model performance, requires substantial computational resources, and exhibits poor persistence under model modifications. We argue that knowledge editing offers a lightweight alternative that is more suitable for fingerprint injection. Accordingly, we apply knowledge editing to fingerprint injection for the first time and demonstrate its strong capability. Despite using scrambled text as fingerprints to prevent them from being overwritten during fine-tuning, degradation still occurs under large-scale fine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning (FSFT), which reduces fingerprint degradation by constraining the update of the fingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even in the worst-case scenario. Additionally, we observe that the fingerprint-injected models struggle to distinguish between fingerprints and similar texts due to the high similarity of their features. This finding underscores the urgent need for more robust and fine-grained fingerprinting injection methods for LLMs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive KV-Cache Compression without Manually Setting Budget</title>
<link>https://arxiv.org/abs/2509.03136</link>
<guid>https://arxiv.org/abs/2509.03136</guid>
<content:encoded><![CDATA[
arXiv:2509.03136v1 Announce Type: cross 
Abstract: Large language models (LLMs) inference relies heavily on KV-caches to accelerate autoregressive decoding, but the resulting memory footprint grows rapidly with sequence length, posing significant efficiency challenges. Current KV-cache compression methods suffer from a Procrustes' bed problem: they force diverse workloads into fixed compression ratios, leading to suboptimal resource allocation and inference performance. To this end, we present GVote, an adaptive KV-cache compression scheme that eliminates manual budget specification while achieving superior accuracy-efficiency trade-offs. GVote operates on the principle that the important keys are the aggregation of keys required by future queries. The method predicts future query attention demands by Monte-Carlo style sampling potential queries and aggregating selected keys to determine the optimal cache budget without manual specification. Experimental evaluation demonstrates GVote's effectiveness across multiple benchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote exhibits 2$\times$ memory reduction while the accuracy maintains higher or comparable.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Network Approach to Multi-radionuclide TDCR Beta Spectroscopy</title>
<link>https://arxiv.org/abs/2509.03137</link>
<guid>https://arxiv.org/abs/2509.03137</guid>
<content:encoded><![CDATA[
arXiv:2509.03137v1 Announce Type: cross 
Abstract: Liquid scintillation triple-to-doubly coincident ratio (TDCR) spectroscopy is widely adopted as a standard method for radionuclide quantification because of its inherent advantages such as high precision, self-calibrating capability, and independence from radioactive reference sources. However, multiradionuclide analysis via TDCR faces the challenges of limited automation and reliance on mixture-specific standards, which may not be easily available. Here, we present an Artificial Intelligence (AI) framework that combines numerical spectral simulation and deep learning for standard-free automated analysis. $\beta$ spectra for model training were generated using Geant4 simulations coupled with statistically modeled detector response sampling. A tailored neural network architecture, trained on this dataset covering various nuclei mix ratio and quenching scenarios, enables autonomous resolution of individual radionuclide activities and detecting efficiency through end-to-end learning paradigms. The model delivers consistent high accuracy across tasks: activity proportions (mean absolute error = 0.009), detection efficiencies (mean absolute error = 0.002), and spectral reconstruction (Structural Similarity Index = 0.9998), validating its physical plausibility for quenched $\beta$ spectroscopy. This AI-driven methodology exhibits significant potential for automated safety-compliant multiradionuclide analysis with robust generalization, real-time processing capabilities, and engineering feasibility, particularly in scenarios where reference materials are unavailable or rapid field analysis is required.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralised self-organisation of pivoting cube ensembles using geometric deep learning</title>
<link>https://arxiv.org/abs/2509.03140</link>
<guid>https://arxiv.org/abs/2509.03140</guid>
<content:encoded><![CDATA[
arXiv:2509.03140v1 Announce Type: cross 
Abstract: We present a decentralized model for autonomous reconfiguration of homogeneous pivoting cube modular robots in two dimensions. Each cube in the ensemble is controlled by a neural network that only gains information from other cubes in its local neighborhood, trained using reinforcement learning. Furthermore, using geometric deep learning, we include the grid symmetries of the cube ensemble in the neural network architecture. We find that even the most localized versions succeed in reconfiguring to the target shape, although reconfiguration happens faster the more information about the whole ensemble is available to individual cubes. Near-optimal reconfiguration is achieved with only nearest neighbor interactions by using multiple information passing between cubes, allowing them to accumulate more global information about the ensemble. Compared to standard neural network architectures, using geometric deep learning approaches provided only minor benefits. Overall, we successfully demonstrate mostly local control of a modular self-assembling system, which is transferable to other space-relevant systems with different action spaces, such as sliding cube modular robots and CubeSat swarms.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Adaptation of LLMs for Process Data</title>
<link>https://arxiv.org/abs/2509.03161</link>
<guid>https://arxiv.org/abs/2509.03161</guid>
<content:encoded><![CDATA[
arXiv:2509.03161v1 Announce Type: cross 
Abstract: In recent years, Large Language Models (LLMs) have emerged as a prominent area of interest across various research domains, including Process Mining (PM). Current applications in PM have predominantly centered on prompt engineering strategies or the transformation of event logs into narrative-style datasets, thereby exploiting the semantic capabilities of LLMs to address diverse tasks. In contrast, this study investigates the direct adaptation of pretrained LLMs to process data without natural language reformulation, motivated by the fact that these models excel in generating sequences of tokens, similar to the objective in PM. More specifically, we focus on parameter-efficient fine-tuning techniques to mitigate the computational overhead typically associated with such models. Our experimental setup focuses on Predictive Process Monitoring (PPM), and considers both single- and multi-task predictions. The results demonstrate a potential improvement in predictive performance over state-of-the-art recurrent neural network (RNN) approaches and recent narrative-style-based solutions, particularly in the multi-task setting. Additionally, our fine-tuned models exhibit faster convergence and require significantly less hyperparameter optimization.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rashomon in the Streets: Explanation Ambiguity in Scene Understanding</title>
<link>https://arxiv.org/abs/2509.03169</link>
<guid>https://arxiv.org/abs/2509.03169</guid>
<content:encoded><![CDATA[
arXiv:2509.03169v1 Announce Type: cross 
Abstract: Explainable AI (XAI) is essential for validating and trusting models in safety-critical applications like autonomous driving. However, the reliability of XAI is challenged by the Rashomon effect, where multiple, equally accurate models can offer divergent explanations for the same prediction. This paper provides the first empirical quantification of this effect for the task of action prediction in real-world driving scenes. Using Qualitative Explainable Graphs (QXGs) as a symbolic scene representation, we train Rashomon sets of two distinct model classes: interpretable, pair-based gradient boosting models and complex, graph-based Graph Neural Networks (GNNs). Using feature attribution methods, we measure the agreement of explanations both within and between these classes. Our results reveal significant explanation disagreement. Our findings suggest that explanation ambiguity is an inherent property of the problem, not just a modeling artifact.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain</title>
<link>https://arxiv.org/abs/2509.03179</link>
<guid>https://arxiv.org/abs/2509.03179</guid>
<content:encoded><![CDATA[
arXiv:2509.03179v1 Announce Type: cross 
Abstract: Poisoning attacks pose an increasing threat to the security and robustness of Artificial Intelligence systems in the military domain. The widespread use of open-source datasets and pretrained models exacerbates this risk. Despite the severity of this threat, there is limited research on the application and detection of poisoning attacks on object detection systems. This is especially problematic in the military domain, where attacks can have grave consequences. In this work, we both investigate the effect of poisoning attacks on military object detectors in practice, and the best approach to detect these attacks. To support this research, we create a small, custom dataset featuring military vehicles: MilCivVeh. We explore the vulnerability of military object detectors for poisoning attacks by implementing a modified version of the BadDet attack: a patch-based poisoning attack. We then assess its impact, finding that while a positive attack success rate is achievable, it requires a substantial portion of the data to be poisoned -- raising questions about its practical applicability. To address the detection challenge, we test both specialized poisoning detection methods and anomaly detection methods from the visual industrial inspection domain. Since our research shows that both classes of methods are lacking, we introduce our own patch detection method: AutoDetect, a simple, fast, and lightweight autoencoder-based method. Our method shows promising results in separating clean from poisoned samples using the reconstruction error of image slices, outperforming existing methods, while being less time- and memory-intensive. We urge that the availability of large, representative datasets in the military domain is a prerequisite to further evaluate risks of poisoning attacks and opportunities patch detection.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback</title>
<link>https://arxiv.org/abs/2509.03206</link>
<guid>https://arxiv.org/abs/2509.03206</guid>
<content:encoded><![CDATA[
arXiv:2509.03206v1 Announce Type: cross 
Abstract: Reinforcement learning faces significant challenges when applied to tasks characterized by sparse reward structures. Although imitation learning, within the domain of supervised learning, offers faster convergence, it relies heavily on human-generated demonstrations. Recently, Goal-Conditioned Supervised Learning (GCSL) has emerged as a potential solution by enabling self-imitation learning for autonomous systems. By strategically relabelling goals, agents can derive policy insights from their own experiences. Despite the successes of this framework, it presents two notable limitations: (1) Learning exclusively from self-generated experiences can exacerbate the agents' inherent biases; (2) The relabelling strategy allows agents to focus solely on successful outcomes, precluding them from learning from their mistakes. To address these issues, we propose a novel model that integrates contrastive learning principles into the GCSL framework to learn from both success and failure. Through empirical evaluations, we demonstrate that our algorithm overcomes limitations imposed by agents' initial biases and thereby enables more exploratory behavior. This facilitates the identification and adoption of effective policies, leading to superior performance across a variety of challenging environments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking</title>
<link>https://arxiv.org/abs/2509.03221</link>
<guid>https://arxiv.org/abs/2509.03221</guid>
<content:encoded><![CDATA[
arXiv:2509.03221v1 Announce Type: cross 
Abstract: Organoids replicate organ structure and function, playing a crucial role in fields such as tumor treatment and drug screening. Their shape and size can indicate their developmental status, but traditional fluorescence labeling methods risk compromising their structure. Therefore, this paper proposes an automated, non-destructive approach to organoid segmentation and tracking. We introduced the LGBP-OrgaNet, a deep learning-based system proficient in accurately segmenting, tracking, and quantifying organoids. The model leverages complementary information extracted from CNN and Transformer modules and introduces the innovative feature fusion module, Learnable Gaussian Band Pass Fusion, to merge data from two branches. Additionally, in the decoder, the model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features, and finally completes the decoding through progressive concatenation and upsampling. SROrga demonstrates satisfactory segmentation accuracy and robustness on organoids segmentation datasets, providing a potent tool for organoid research.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Stress Detection as Time Series Events -- A Novel Window-Based F1-Metric</title>
<link>https://arxiv.org/abs/2509.03240</link>
<guid>https://arxiv.org/abs/2509.03240</guid>
<content:encoded><![CDATA[
arXiv:2509.03240v1 Announce Type: cross 
Abstract: Accurate evaluation of event detection in time series is essential for applications such as stress monitoring with wearable devices, where ground truth is typically annotated as single-point events, even though the underlying phenomena are gradual and temporally diffused. Standard metrics like F1 and point-adjusted F1 (F1$_{pa}$) often misrepresent model performance in such real-world, imbalanced datasets. We introduce a window-based F1 metric (F1$_w$) that incorporates temporal tolerance, enabling a more robust assessment of event detection when exact alignment is unrealistic. Empirical analysis in three physiological datasets, two in-the-wild (ADARP, Wrist Angel) and one experimental (ROAD), indicates that F1$_w$ reveals meaningful model performance patterns invisible to conventional metrics, while its window size can be adapted to domain knowledge to avoid overestimation. We show that the choice of evaluation metric strongly influences the interpretation of model performance: using predictions from TimesFM, only our temporally tolerant metrics reveal statistically significant improvements over random and null baselines in the two in-the-wild use cases. This work addresses key gaps in time series evaluation and provides practical guidance for healthcare applications where requirements for temporal precision vary by context.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization</title>
<link>https://arxiv.org/abs/2509.03244</link>
<guid>https://arxiv.org/abs/2509.03244</guid>
<content:encoded><![CDATA[
arXiv:2509.03244v1 Announce Type: cross 
Abstract: Expensive multi-objective optimization is a prevalent and crucial concern in many real-world scenarios, where sample-efficiency is vital due to the limited evaluations to recover the true Pareto front for decision making. Existing works either involve rebuilding Gaussian process surrogates from scratch for each objective in each new problem encountered, or rely on extensive past domain experiments for pre-training deep learning models, making them hard to generalize and impractical to cope with various emerging applications in the real world. To address this issue, we propose a new paradigm named FoMEMO (Foundation Models for Expensive Multi-objective Optimization), which enables the establishment of a foundation model conditioned on any domain trajectory and user preference, and facilitates fast in-context optimization based on the predicted preference-wise aggregation posteriors. Rather than accessing extensive domain experiments in the real world, we demonstrate that pre-training the foundation model with a diverse set of hundreds of millions of synthetic data can lead to superior adaptability to unknown problems, without necessitating any subsequent model training or updates in the optimization process. We evaluate our method across a variety of synthetic benchmarks and real-word applications, and demonstrate its superior generality and competitive performance compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure Transfer: an Inference-Based Calculus for the Transformation of Representations</title>
<link>https://arxiv.org/abs/2509.03249</link>
<guid>https://arxiv.org/abs/2509.03249</guid>
<content:encoded><![CDATA[
arXiv:2509.03249v1 Announce Type: cross 
Abstract: Representation choice is of fundamental importance to our ability to communicate and reason effectively. A major unsolved problem, addressed in this paper, is how to devise \textit{representational-system (RS) agnostic} techniques that drive representation transformation and choice. We present a novel calculus, called \textit{structure transfer}, that enables representation transformation across diverse RSs. Specifically, given a \textit{source} representation drawn from a source RS, the rules of structure transfer allow us to generate a \textit{target} representation for a target RS. The generality of structure transfer comes in part from its ability to ensure that the source representation and the generated target representation satisfy \textit{any} specified relation (such as semantic equivalence). This is done by exploiting \textit{schemas}, which encode knowledge about RSs. Specifically, schemas can express \textit{preservation of information} across relations between any pair of RSs, and this knowledge is used by structure transfer to derive a structure for the target representation which ensures that the desired relation holds. We formalise this using Representational Systems Theory~\cite{raggi2022rst}, building on the key concept of a \textit{construction space}. The abstract nature of construction spaces grants them the generality to model RSs of diverse kinds, including formal languages, geometric figures and diagrams, as well as informal notations. Consequently, structure transfer is a system-agnostic calculus that can be used to identify alternative representations in a wide range of practical settings.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling</title>
<link>https://arxiv.org/abs/2509.03260</link>
<guid>https://arxiv.org/abs/2509.03260</guid>
<content:encoded><![CDATA[
arXiv:2509.03260v1 Announce Type: cross 
Abstract: Abnormal cryptocurrency transactions - such as mixing services, fraudulent transfers, and pump-and-dump operations -- pose escalating risks to financial integrity but remain notoriously difficult to detect due to class imbalance, temporal volatility, and complex network dependencies. Existing approaches are predominantly model-centric and post hoc, flagging anomalies only after they occur and thus offering limited preventive value. This paper introduces HyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a data-driven early-warning framework that explicitly incorporates lead time into anomaly detection. Unlike prior methods, HyPV-LEAD integrates three innovations: (1) window-horizon modeling to guarantee actionable lead-time alerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while preserving temporal continuity, and (3) hyperbolic embedding to capture the hierarchical and scale-free properties of blockchain transaction networks. Empirical evaluation on large-scale Bitcoin transaction data demonstrates that HyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a PR-AUC of 0.9624 with significant gains in precision and recall. Ablation studies further confirm that each component - PV sampling, hyperbolic embedding, and structural-temporal modeling - provides complementary benefits, with the full framework delivering the highest performance. By shifting anomaly detection from reactive classification to proactive early-warning, HyPV-LEAD establishes a robust foundation for real-time risk management, anti-money laundering (AML) compliance, and financial security in dynamic blockchain environments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estudio de la eficiencia en la escalabilidad de GPUs para el entrenamiento de Inteligencia Artificial</title>
<link>https://arxiv.org/abs/2509.03263</link>
<guid>https://arxiv.org/abs/2509.03263</guid>
<content:encoded><![CDATA[
arXiv:2509.03263v1 Announce Type: cross 
Abstract: Training large-scale deep learning models has become a key challenge for the scientific community and industry. While the massive use of GPUs can significantly speed up training times, this approach has a negative impact on efficiency. In this article, we present a detailed analysis of the times reported by MLPerf Training v4.1 on four workloads: BERT, Llama2 LoRA, RetinaNet, and Stable Diffusion, showing that there are configurations that optimise the relationship between performance, GPU usage, and efficiency. The results point to a break-even point that allows training times to be reduced while maximising efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Guide to Differential Privacy: From Theory to User Expectations</title>
<link>https://arxiv.org/abs/2509.03294</link>
<guid>https://arxiv.org/abs/2509.03294</guid>
<content:encoded><![CDATA[
arXiv:2509.03294v1 Announce Type: cross 
Abstract: The increasing availability of personal data has enabled significant advances in fields such as machine learning, healthcare, and cybersecurity. However, this data abundance also raises serious privacy concerns, especially in light of powerful re-identification attacks and growing legal and ethical demands for responsible data use. Differential privacy (DP) has emerged as a principled, mathematically grounded framework for mitigating these risks. This review provides a comprehensive survey of DP, covering its theoretical foundations, practical mechanisms, and real-world applications. It explores key algorithmic tools and domain-specific challenges - particularly in privacy-preserving machine learning and synthetic data generation. The report also highlights usability issues and the need for improved communication and transparency in DP systems. Overall, the goal is to support informed adoption of DP by researchers and practitioners navigating the evolving landscape of data privacy.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Differentiation of Agent-Based Models</title>
<link>https://arxiv.org/abs/2509.03303</link>
<guid>https://arxiv.org/abs/2509.03303</guid>
<content:encoded><![CDATA[
arXiv:2509.03303v1 Announce Type: cross 
Abstract: Agent-based models (ABMs) simulate complex systems by capturing the bottom-up interactions of individual agents comprising the system. Many complex systems of interest, such as epidemics or financial markets, involve thousands or even millions of agents. Consequently, ABMs often become computationally demanding and rely on the calibration of numerous free parameters, which has significantly hindered their widespread adoption. In this paper, we demonstrate that automatic differentiation (AD) techniques can effectively alleviate these computational burdens. By applying AD to ABMs, the gradients of the simulator become readily available, greatly facilitating essential tasks such as calibration and sensitivity analysis. Specifically, we show how AD enables variational inference (VI) techniques for efficient parameter calibration. Our experiments demonstrate substantial performance improvements and computational savings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape; and the SIR epidemiological model. Our approach thus significantly enhances the practicality and scalability of ABMs for studying complex systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heatmap Guided Query Transformers for Robust Astrocyte Detection across Immunostains and Resolutions</title>
<link>https://arxiv.org/abs/2509.03323</link>
<guid>https://arxiv.org/abs/2509.03323</guid>
<content:encoded><![CDATA[
arXiv:2509.03323v1 Announce Type: cross 
Abstract: Astrocytes are critical glial cells whose altered morphology and density are hallmarks of many neurological disorders. However, their intricate branching and stain dependent variability make automated detection of histological images a highly challenging task. To address these challenges, we propose a hybrid CNN Transformer detector that combines local feature extraction with global contextual reasoning. A heatmap guided query mechanism generates spatially grounded anchors for small and faint astrocytes, while a lightweight Transformer module improves discrimination in dense clusters. Evaluated on ALDH1L1 and GFAP stained astrocyte datasets, the model consistently outperformed Faster R-CNN, YOLOv11 and DETR, achieving higher sensitivity with fewer false positives, as confirmed by FROC analysis. These results highlight the potential of hybrid CNN Transformer architectures for robust astrocyte detection and provide a foundation for advanced computational pathology tools.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems</title>
<link>https://arxiv.org/abs/2509.03340</link>
<guid>https://arxiv.org/abs/2509.03340</guid>
<content:encoded><![CDATA[
arXiv:2509.03340v1 Announce Type: cross 
Abstract: Bifurcation phenomena in nonlinear dynamical systems often lead to multiple coexisting stable solutions, particularly in the presence of symmetry breaking. Deterministic machine learning models struggle to capture this multiplicity, averaging over solutions and failing to represent lower-symmetry outcomes. In this work, we propose a generative framework based on flow matching to model the full probability distribution over bifurcation outcomes. Our method enables direct sampling of multiple valid solutions while preserving system symmetries through equivariant modeling. We introduce a symmetric matching strategy that aligns predicted and target outputs under group actions, allowing accurate learning in equivariant settings. We validate our approach on a range of systems, from toy models to complex physical problems such as buckling beams and the Allen-Cahn equation. Our results demonstrate that flow matching significantly outperforms non-probabilistic and variational methods in capturing multimodal distributions and symmetry-breaking bifurcations, offering a principled and scalable solution for modeling multistability in high-dimensional systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the MIA Vulnerability Gap Between Private GANs and Diffusion Models</title>
<link>https://arxiv.org/abs/2509.03341</link>
<guid>https://arxiv.org/abs/2509.03341</guid>
<content:encoded><![CDATA[
arXiv:2509.03341v1 Announce Type: cross 
Abstract: Generative Adversarial Networks (GANs) and diffusion models have emerged as leading approaches for high-quality image synthesis. While both can be trained under differential privacy (DP) to protect sensitive data, their sensitivity to membership inference attacks (MIAs), a key threat to data confidentiality, remains poorly understood. In this work, we present the first unified theoretical and empirical analysis of the privacy risks faced by differentially private generative models. We begin by showing, through a stability-based analysis, that GANs exhibit fundamentally lower sensitivity to data perturbations than diffusion models, suggesting a structural advantage in resisting MIAs. We then validate this insight with a comprehensive empirical study using a standardized MIA pipeline to evaluate privacy leakage across datasets and privacy budgets. Our results consistently reveal a marked privacy robustness gap in favor of GANs, even in strong DP regimes, highlighting that model type alone can critically shape privacy leakage.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>epiGPTope: A machine learning-based epitope generator and classifier</title>
<link>https://arxiv.org/abs/2509.03351</link>
<guid>https://arxiv.org/abs/2509.03351</guid>
<content:encoded><![CDATA[
arXiv:2509.03351v1 Announce Type: cross 
Abstract: Epitopes are short antigenic peptide sequences which are recognized by antibodies or immune cell receptors. These are central to the development of immunotherapies, vaccines, and diagnostics. However, the rational design of synthetic epitope libraries is challenging due to the large combinatorial sequence space, $20^n$ combinations for linear epitopes of n amino acids, making screening and testing unfeasible, even with high throughput experimental techniques. In this study, we present a large language model, epiGPTope, pre-trained on protein data and specifically fine-tuned on linear epitopes, which for the first time can directly generate novel epitope-like sequences, which are found to possess statistical properties analogous to the ones of known epitopes. This generative approach can be used to prepare libraries of epitope candidate sequences. We further train statistical classifiers to predict whether an epitope sequence is of bacterial or viral origin, thus narrowing the candidate library and increasing the likelihood of identifying specific epitopes. We propose that such combination of generative and predictive models can be of assistance in epitope discovery. The approach uses only primary amino acid sequences of linear epitopes, bypassing the need for a geometric framework or hand-crafted features of the sequences. By developing a method to create biologically feasible sequences, we anticipate faster and more cost-effective generation and screening of synthetic epitopes, with relevant applications in the development of new biotechnologies.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Resource Allocation for Fleet Intelligence</title>
<link>https://arxiv.org/abs/2509.03353</link>
<guid>https://arxiv.org/abs/2509.03353</guid>
<content:encoded><![CDATA[
arXiv:2509.03353v1 Announce Type: cross 
Abstract: Resource allocation is crucial for the performance optimization of cloud-assisted multi-agent intelligence. Traditional methods often overlook agents' diverse computational capabilities and complex operating environments, leading to inefficient and unfair resource distribution. To address this, we open-sourced Fair-Synergy, an algorithmic framework that utilizes the concave relationship between the agents' accuracy and the system resources to ensure fair resource allocation across fleet intelligence. We extend traditional allocation approaches to encompass a multidimensional machine learning utility landscape defined by model parameters, training data volume, and task complexity. We evaluate Fair-Synergy with advanced vision and language models such as BERT, VGG16, MobileNet, and ResNets on datasets including MNIST, CIFAR-10, CIFAR-100, BDD, and GLUE. We demonstrate that Fair-Synergy outperforms standard benchmarks by up to 25% in multi-agent inference and 11% in multi-agent learning settings. Also, we explore how the level of fairness affects the least advantaged, most advantaged, and average agents, providing insights for equitable fleet intelligence.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Field Turing Machine: A Differentiable Spatial Computer</title>
<link>https://arxiv.org/abs/2509.03370</link>
<guid>https://arxiv.org/abs/2509.03370</guid>
<content:encoded><![CDATA[
arXiv:2509.03370v1 Announce Type: cross 
Abstract: We introduce the Neural Field Turing Machine (NFTM), a differentiable architecture that unifies symbolic computation, physical simulation, and perceptual inference within continuous spatial fields. NFTM combines a neural controller, continuous memory field, and movable read/write heads that perform local updates. At each timestep, the controller reads local patches, computes updates via learned rules, and writes them back while updating head positions. This design achieves linear O(N) scaling through fixed-radius neighborhoods while maintaining Turing completeness under bounded error. We demonstrate three example instantiations of NFTM: cellular automata simulation (Rule 110), physics-informed PDE solvers (2D heat equation), and iterative image refinement (CIFAR-10 inpainting). These instantiations learn local update rules that compose into global dynamics, exhibit stable long-horizon rollouts, and generalize beyond training horizons. NFTM provides a unified computational substrate bridging discrete algorithms and continuous field dynamics within a single differentiable framework.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers</title>
<link>https://arxiv.org/abs/2509.03379</link>
<guid>https://arxiv.org/abs/2509.03379</guid>
<content:encoded><![CDATA[
arXiv:2509.03379v1 Announce Type: cross 
Abstract: Vision Transformers (ViTs) achieve strong performance in image classification but incur high computational costs from processing all image tokens. To reduce inference costs in large ViTs without compromising accuracy, we propose TinyDrop, a training-free token dropping framework guided by a lightweight vision model. The guidance model estimates the importance of tokens while performing inference, thereby selectively discarding low-importance tokens if large vit models need to perform attention calculations. The framework operates plug-and-play, requires no architectural modifications, and is compatible with diverse ViT architectures. Evaluations on standard image classification benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs with minimal accuracy degradation, highlighting its generalization capability and practical utility for efficient ViT-based classification.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Correctness: Harmonizing Process and Outcome Rewards through RL Training</title>
<link>https://arxiv.org/abs/2509.03403</link>
<guid>https://arxiv.org/abs/2509.03403</guid>
<content:encoded><![CDATA[
arXiv:2509.03403v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged to be a predominant paradigm for mathematical reasoning tasks, offering stable improvements in reasoning ability. However, Outcome Reward Models (ORMs) in RLVR are too coarse-grained to distinguish flawed reasoning within correct answers or valid reasoning within incorrect answers. This lack of granularity introduces noisy and misleading gradients significantly and hinders further progress in reasoning process quality. While Process Reward Models (PRMs) offer fine-grained guidance for intermediate steps, they frequently suffer from inaccuracies and are susceptible to reward hacking.
  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an effective data process curation method that harmonizes noisy, fine-grained process rewards with accurate, coarse-grained outcome rewards. Rather than naively blending PRM and ORM in the objective function (arXiv:archive/2506.18896), PROF leverages their complementary strengths through consistency-driven sample selection. Our approach retains correct responses with higher averaged process values and incorrect responses with lower averaged process values, while maintaining positive/negative training sample balance. Extensive experiments demonstrate that our method not only consistently improves the final accuracy over $4\%$ compared to the blending approaches, but also strengthens the quality of intermediate reasoning steps. Codes and training recipes are available at https://github.com/Chenluye99/PROF.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-level SSL Feature Gating for Audio Deepfake Detection</title>
<link>https://arxiv.org/abs/2509.03409</link>
<guid>https://arxiv.org/abs/2509.03409</guid>
<content:encoded><![CDATA[
arXiv:2509.03409v1 Announce Type: cross 
Abstract: Recent advancements in generative AI, particularly in speech synthesis, have enabled the generation of highly natural-sounding synthetic speech that closely mimics human voices. While these innovations hold promise for applications like assistive technologies, they also pose significant risks, including misuse for fraudulent activities, identity theft, and security threats. Current research on spoofing detection countermeasures remains limited by generalization to unseen deepfake attacks and languages. To address this, we propose a gating mechanism extracting relevant feature from the speech foundation XLS-R model as a front-end feature extractor. For downstream back-end classifier, we employ Multi-kernel gated Convolution (MultiConv) to capture both local and global speech artifacts. Additionally, we introduce Centered Kernel Alignment (CKA) as a similarity metric to enforce diversity in learned features across different MultiConv layers. By integrating CKA with our gating mechanism, we hypothesize that each component helps improving the learning of distinct synthetic speech patterns. Experimental results demonstrate that our approach achieves state-of-the-art performance on in-domain benchmarks while generalizing robustly to out-of-domain datasets, including multilingual speech samples. This underscores its potential as a versatile solution for detecting evolving speech deepfake threats.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Saudi Sign Language Recognition: A Vision Transformer Approach</title>
<link>https://arxiv.org/abs/2509.03467</link>
<guid>https://arxiv.org/abs/2509.03467</guid>
<content:encoded><![CDATA[
arXiv:2509.03467v1 Announce Type: cross 
Abstract: Sign language (SL) is an essential communication form for hearing-impaired and deaf people, enabling engagement within the broader society. Despite its significance, limited public awareness of SL often leads to inequitable access to educational and professional opportunities, thereby contributing to social exclusion, particularly in Saudi Arabia, where over 84,000 individuals depend on Saudi Sign Language (SSL) as their primary form of communication. Although certain technological approaches have helped to improve communication for individuals with hearing impairments, there continues to be an urgent requirement for more precise and dependable translation techniques, especially for Arabic sign language variants like SSL. Most state-of-the-art solutions have primarily focused on non-Arabic sign languages, resulting in a considerable absence of resources dedicated to Arabic sign language, specifically SSL. The complexity of the Arabic language and the prevalence of isolated sign language datasets that concentrate on individual words instead of continuous speech contribute to this issue. To address this gap, our research represents an important step in developing SSL resources. To address this, we introduce the first continuous Saudi Sign Language dataset called KAU-CSSL, focusing on complete sentences to facilitate further research and enable sophisticated recognition systems for SSL recognition and translation. Additionally, we propose a transformer-based model, utilizing a pretrained ResNet-18 for spatial feature extraction and a Transformer Encoder with Bidirectional LSTM for temporal dependencies, achieving 99.02\% accuracy at signer dependent mode and 77.71\% accuracy at signer independent mode. This development leads the way to not only improving communication tools for the SSL community but also making a substantial contribution to the wider field of sign language.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling</title>
<link>https://arxiv.org/abs/2509.03472</link>
<guid>https://arxiv.org/abs/2509.03472</guid>
<content:encoded><![CDATA[
arXiv:2509.03472v1 Announce Type: cross 
Abstract: Differentially-Private SGD (DP-SGD) is a powerful technique to protect user privacy when using sensitive data to train neural networks. During training, converting model weights and activations into low-precision formats, i.e., quantization, can drastically reduce training times, energy consumption, and cost, and is thus a widely used technique. In this work, we demonstrate that quantization causes significantly higher accuracy degradation in DP-SGD compared to regular SGD. We observe that this is caused by noise injection in DP-SGD, which amplifies quantization variance, leading to disproportionately large accuracy degradation. To address this challenge, we present QPQuant, a dynamic quantization framework that adaptively selects a changing subset of layers to quantize at each epoch. Our method combines two key ideas that effectively reduce quantization variance: (i) probabilistic sampling of the layers that rotates which layers are quantized every epoch, and (ii) loss-aware layer prioritization, which uses a differentially private loss sensitivity estimator to identify layers that can be quantized with minimal impact on model quality. This estimator consumes a negligible fraction of the overall privacy budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50, and DenseNet121 across a range of datasets demonstrate that DPQuant consistently outperforms static quantization baselines, achieving near Pareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical throughput improvements on low-precision hardware, with less than 2% drop in validation accuracy.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning</title>
<link>https://arxiv.org/abs/2509.03477</link>
<guid>https://arxiv.org/abs/2509.03477</guid>
<content:encoded><![CDATA[
arXiv:2509.03477v1 Announce Type: cross 
Abstract: Addressing missing modalities and limited labeled data is crucial for advancing robust multimodal learning. We propose Robult, a scalable framework designed to mitigate these challenges by preserving modality-specific information and leveraging redundancy through a novel information-theoretic approach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled (PU) contrastive loss that maximizes task-relevant feature alignment while effectively utilizing limited labeled data in semi-supervised settings, and (2) a latent reconstruction loss that ensures unique modality-specific information is retained. These strategies, embedded within a modular design, enhance performance across various downstream tasks and ensure resilience to incomplete modalities during inference. Experimental results across diverse datasets validate that Robult achieves superior performance over existing approaches in both semi-supervised learning and missing modality contexts. Furthermore, its lightweight design promotes scalability and seamless integration with existing architectures, making it suitable for real-world multimodal applications.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models</title>
<link>https://arxiv.org/abs/2509.03487</link>
<guid>https://arxiv.org/abs/2509.03487</guid>
<content:encoded><![CDATA[
arXiv:2509.03487v1 Announce Type: cross 
Abstract: Proteins play crucial roles in almost all biological processes. The advancement of deep learning has greatly accelerated the development of protein foundation models, leading to significant successes in protein understanding and design. However, the lack of systematic red-teaming for these models has raised serious concerns about their potential misuse, such as generating proteins with biological safety risks. This paper introduces SafeProtein, the first red-teaming framework designed for protein foundation models to the best of our knowledge. SafeProtein combines multimodal prompt engineering and heuristic beam search to systematically design red-teaming methods and conduct tests on protein foundation models. We also curated SafeProtein-Bench, which includes a manually constructed red-teaming benchmark dataset and a comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks on state-of-the-art protein foundation models (up to 70% attack success rate for ESM3), revealing potential biological safety risks in current protein foundation models and providing insights for the development of robust security protection technologies for frontier models. The codes will be made publicly available at https://github.com/jigang-fan/SafeProtein.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Entropy Control in LLM-RL Algorithms</title>
<link>https://arxiv.org/abs/2509.03493</link>
<guid>https://arxiv.org/abs/2509.03493</guid>
<content:encoded><![CDATA[
arXiv:2509.03493v1 Announce Type: cross 
Abstract: For RL algorithms, appropriate entropy control is crucial to their effectiveness. To control the policy entropy, a commonly used method is entropy regularization, which is adopted in various popular RL algorithms including PPO, SAC and A3C. Although entropy regularization proves effective in robotic and games RL conventionally, studies found that it gives weak to no gains in LLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL setting. Specifically, we first argue that the conventional entropy regularization suffers from the LLM's extremely large response space and the sparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy control method that utilizes a new clamped entropy bonus with an automatically adjusted coefficient. The clamped entropy is evaluated with the re-normalized policy defined on certain smaller token space, which encourages exploration within a more compact response set. In addition, the algorithm automatically adjusts entropy coefficient according to the clamped entropy value, effectively controlling the entropy-induced bias while leveraging the entropy's benefits. AEnt is tested in math-reasoning tasks under different base models and datasets, and it is observed that AEnt outperforms the baselines consistently across multiple benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena</title>
<link>https://arxiv.org/abs/2509.03500</link>
<guid>https://arxiv.org/abs/2509.03500</guid>
<content:encoded><![CDATA[
arXiv:2509.03500v1 Announce Type: cross 
Abstract: Advancements in onboard computing mean remote sensing agents can employ state-of-the-art computer vision and machine learning at the edge. These capabilities can be leveraged to unlock new rare, transient, and pinpoint measurements of dynamic science phenomena. In this paper, we present an automated workflow that synthesizes the detection of these dynamic events in look-ahead satellite imagery with autonomous trajectory planning for a follow-up high-resolution sensor to obtain pinpoint measurements. We apply this workflow to the use case of observing volcanic plumes. We analyze classification approaches including traditional machine learning algorithms and convolutional neural networks. We present several trajectory planning algorithms that track the morphological features of a plume and integrate these algorithms with the classifiers. We show through simulation an order of magnitude increase in the utility return of the high-resolution instrument compared to baselines while maintaining efficient runtimes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data</title>
<link>https://arxiv.org/abs/2509.03501</link>
<guid>https://arxiv.org/abs/2509.03501</guid>
<content:encoded><![CDATA[
arXiv:2509.03501v1 Announce Type: cross 
Abstract: Next-generation AI companions must go beyond general video understanding to resolve spatial and temporal references in dynamic, real-world environments. Existing Video Large Language Models (Video LLMs), while capable of coarse-level comprehension, struggle with fine-grained, spatiotemporal reasoning, especially when user queries rely on time-based event references for temporal anchoring, or gestural cues for spatial anchoring to clarify object references and positions. To bridge this critical gap, we introduce Strefer, a synthetic instruction data generation framework designed to equip Video LLMs with spatiotemporal referring and reasoning capabilities. Strefer produces diverse instruction-tuning data using a data engine that pseudo-annotates temporally dense, fine-grained video metadata, capturing rich spatial and temporal information in a structured manner, including subjects, objects, their locations as masklets, and their action descriptions and timelines. Our approach enhances the ability of Video LLMs to interpret spatial and temporal references, fostering more versatile, space-time-aware reasoning essential for real-world AI companions. Without using proprietary models, costly human annotation, or the need to annotate large volumes of new videos, experimental evaluations show that models trained with data produced by Strefer outperform baselines on tasks requiring spatial and temporal disambiguation. Additionally, these models exhibit enhanced space-time-aware reasoning, establishing a new foundation for perceptually grounded, instruction-tuned Video LLMs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Warming Up for Zeroth-Order Federated Pre-Training with Low Resource Clients</title>
<link>https://arxiv.org/abs/2509.03503</link>
<guid>https://arxiv.org/abs/2509.03503</guid>
<content:encoded><![CDATA[
arXiv:2509.03503v1 Announce Type: cross 
Abstract: Federated learning enables collaborative model training across numerous edge devices without requiring participants to share data; however, memory and communication constraints on these edge devices may preclude their participation in training. We consider a setting in which a subset of edge devices are below a critical memory or communication threshold required to conduct model updates. Under typical federated optimization algorithms, these devices are excluded from training which renders their data inaccessible and increases system induced bias. We are inspired by MeZO, a zeroth-order method used for memory-efficient fine-tuning. The increased variance inherent to zeroth-order gradient approximations has relegated previous zeroth-order optimizers exclusively to the domain of fine tuning; a limitation we seek to correct. We devise a federated, memory-efficient zeroth-order optimizer, ZOWarmUp that permits zeroth-order training from a random initialization. ZOWarmUp leverages differing client capabilities and careful variance reduction techniques to facilitate participation of under-represented, low-resource clients in model training. Like other federated zeroth-order methods, ZOWarmUp eliminates the need for edge devices to transmit their full gradients to the server and instead relies on only a small set of random seeds, rendering the up-link communication cost negligible. We present experiments using various datasets and model architectures to show that ZOWarmUp is a robust algorithm that can can be applied under a wide variety of circumstances. For systems with a high proportion of edge devices that would otherwise be excluded from training, this algorithm provides access to a greater volume and diversity of data, thus improving training outcomes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence</title>
<link>https://arxiv.org/abs/2509.03505</link>
<guid>https://arxiv.org/abs/2509.03505</guid>
<content:encoded><![CDATA[
arXiv:2509.03505v1 Announce Type: cross 
Abstract: We argue that progress toward general intelligence requires complementary foundation models grounded in language, the physical world, and structured data. This report presents LimiX, the first installment of our large structured-data models (LDMs). LimiX treats structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model. LimiX is pretrained using masked joint-distribution modeling with an episodic, context-conditional objective, where the model predicts for query subsets conditioned on dataset-specific contexts, supporting rapid, training-free adaptation at inference. We evaluate LimiX across 10 large structured-data benchmarks with broad regimes of sample size, feature dimensionality, class number, categorical-to-numerical feature ratio, missingness, and sample-to-feature ratios. With a single model and a unified interface, LimiX consistently surpasses strong baselines including gradient-boosting trees, deep tabular networks, recent tabular foundation models, and automated ensembles, as shown in Figure 1 and Figure 2. The superiority holds across a wide range of tasks, such as classification, regression, missing value imputation, and data generation, often by substantial margins, while avoiding task-specific architectures or bespoke training per task. All LimiX models are publicly accessible under Apache 2.0.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories</title>
<link>https://arxiv.org/abs/2509.03515</link>
<guid>https://arxiv.org/abs/2509.03515</guid>
<content:encoded><![CDATA[
arXiv:2509.03515v1 Announce Type: cross 
Abstract: The Waymo Open Motion Dataset (WOMD) has become a popular resource for data-driven modeling of autonomous vehicles (AVs) behavior. However, its validity for behavioral analysis remains uncertain due to proprietary post-processing, the absence of error quantification, and the segmentation of trajectories into 20-second clips. This study examines whether WOMD accurately captures the dynamics and interactions observed in real-world AV operations. Leveraging an independently collected naturalistic dataset from Level 4 AV operations in Phoenix, Arizona (PHX), we perform comparative analyses across three representative urban driving scenarios: discharging at signalized intersections, car-following, and lane-changing behaviors. For the discharging analysis, headways are manually extracted from aerial video to ensure negligible measurement error. For the car-following and lane-changing cases, we apply the Simulation-Extrapolation (SIMEX) method to account for empirically estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to quantify behavioral differences. Results across all scenarios consistently show that behavior in PHX falls outside the behavioral envelope of WOMD. Notably, WOMD underrepresents short headways and abrupt decelerations. These findings suggest that behavioral models calibrated solely on WOMD may systematically underestimate the variability, risk, and complexity of naturalistic driving. Caution is therefore warranted when using WOMD for behavior modeling without proper validation against independently collected data.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents</title>
<link>https://arxiv.org/abs/2208.13266</link>
<guid>https://arxiv.org/abs/2208.13266</guid>
<content:encoded><![CDATA[
arXiv:2208.13266v4 Announce Type: replace 
Abstract: Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1\% to 15.8\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Human-AI Collaboration with Large Foundation Models</title>
<link>https://arxiv.org/abs/2403.04931</link>
<guid>https://arxiv.org/abs/2403.04931</guid>
<content:encoded><![CDATA[
arXiv:2403.04931v3 Announce Type: replace 
Abstract: As the capabilities of artificial intelligence (AI) continue to expand rapidly, Human-AI (HAI) Collaboration, combining human intellect and AI systems, has become pivotal for advancing problem-solving and decision-making processes. The advent of Large Foundation Models (LFMs) has greatly expanded its potential, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. At the same time, realizing this potential responsibly requires addressing persistent challenges related to safety, fairness, and control. This paper reviews the crucial integration of LFMs with HAI, highlighting both opportunities and risks. We structure our analysis around four areas: human-guided model development, collaborative design principles, ethical and governance frameworks, and applications in high-stakes domains. Our review shows that successful HAI systems are not the automatic result of stronger models but the product of careful, human-centered design. By identifying key open challenges, this survey aims to give insight into current and future research that turns the raw power of LFMs into partnerships that are reliable, trustworthy, and beneficial to society.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Generating Monolithic and Model Reconciling Explanations in Probabilistic Scenarios</title>
<link>https://arxiv.org/abs/2405.19229</link>
<guid>https://arxiv.org/abs/2405.19229</guid>
<content:encoded><![CDATA[
arXiv:2405.19229v2 Announce Type: replace 
Abstract: Explanation generation frameworks aim to make AI systems' decisions transparent and understandable to human users. However, generating explanations in uncertain environments characterized by incomplete information and probabilistic models remains a significant challenge. In this paper, we propose a novel framework for generating probabilistic monolithic explanations and model reconciling explanations. Monolithic explanations provide self-contained reasons for an explanandum without considering the agent receiving the explanation, while model reconciling explanations account for the knowledge of the agent receiving the explanation. For monolithic explanations, our approach integrates uncertainty by utilizing probabilistic logic to increase the probability of the explanandum. For model reconciling explanations, we propose a framework that extends the logic-based variant of the model reconciliation problem to account for probabilistic human models, where the goal is to find explanations that increase the probability of the explanandum while minimizing conflicts between the explanation and the probabilistic human model. We introduce explanatory gain and explanatory power as quantitative metrics to assess the quality of these explanations. Further, we present algorithms that exploit the duality between minimal correction sets and minimal unsatisfiable sets to efficiently compute both types of explanations in probabilistic contexts. Extensive experimental evaluations on various benchmarks demonstrate the effectiveness and scalability of our approach in generating explanations under uncertainty.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frugal inference for control</title>
<link>https://arxiv.org/abs/2406.14427</link>
<guid>https://arxiv.org/abs/2406.14427</guid>
<content:encoded><![CDATA[
arXiv:2406.14427v3 Announce Type: replace 
Abstract: A key challenge in advancing artificial intelligence is achieving the right balance between utility maximization and resource use by both external movement and internal computation. While this trade-off has been studied in fully observable settings, our understanding of resource efficiency in partially observable environments remains limited. Motivated by this challenge, we develop a version of the POMDP framework where the information gained through inference is treated as a resource that must be optimized alongside task performance and motion effort. By solving this problem in environments described by linear-Gaussian dynamics, we uncover fundamental principles of resource efficiency. Our study reveals a phase transition in the inference, switching from a Bayes-optimal approach to one that strategically leaves some uncertainty unresolved. This frugal behavior gives rise to a structured family of equally effective strategies, facilitating adaptation to later objectives and constraints overlooked during the original optimization. We illustrate the applicability of our framework and the generality of the principles we derived using two nonlinear tasks. Overall, this work provides a foundation for a new type of rational computation that both brains and machines could use for effective but resource-efficient control under uncertainty.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MorphAgent: Empowering Agents through Self-Evolving Profiles and Decentralized Collaboration</title>
<link>https://arxiv.org/abs/2410.15048</link>
<guid>https://arxiv.org/abs/2410.15048</guid>
<content:encoded><![CDATA[
arXiv:2410.15048v2 Announce Type: replace 
Abstract: Large Language Model (LLM) based multi-agent systems (MAS) have shown promise in tackling complex tasks, but often rely on predefined roles and centralized coordination, limiting their adaptability to evolving challenges. This paper introduces MorphAgent, a novel Autonomous, Self-Organizing, and Self-Adaptive Multi-Agent System for decentralized agent collaboration that enables agents to dynamically evolve their roles and capabilities. Our approach employs self-evolving agent profiles, optimized through three key metrics, guiding agents in refining their individual expertise while maintaining complementary team dynamics. MorphAgent implements a two-phase process: a Profile Update phase for profile optimization, followed by a Task Execution phase where agents continuously adapt their roles based on task feedback. Our experimental results show that MorphAgent outperforms existing frameworks in terms of task performance and adaptability to changing requirements, paving the way for more robust and versatile multi-agent collaborative systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Act as Ensembler for Multi-GNNs?</title>
<link>https://arxiv.org/abs/2410.16822</link>
<guid>https://arxiv.org/abs/2410.16822</guid>
<content:encoded><![CDATA[
arXiv:2410.16822v5 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning from graph-structured data. However, GNNs lack the inherent semantic understanding capability of rich textual node attributes, limiting their effectiveness in applications. On the other hand, we empirically observe that for existing GNN models, no one can consistently outperforms others across diverse datasets. In this paper, we study whether LLMs can act as an ensembler for multi-GNNs and propose the LensGNN model. The model first aligns multiple GNNs, mapping the representations of different GNNs into the same space. Then, through LoRA fine-tuning, it aligns the space between the GNN and the LLM, injecting graph tokens and textual information into LLMs. This allows LensGNN to ensemble multiple GNNs and take advantage of the strengths of LLM, leading to a deeper understanding of both textual semantic information and graph structural information. The experimental results show that LensGNN outperforms existing models. This research advances text-attributed graph ensemble learning by providing a robust and superior solution for integrating semantic and structural information. We provide our code and data here: https://github.com/AquariusAQ/LensGNN.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation</title>
<link>https://arxiv.org/abs/2411.05085</link>
<guid>https://arxiv.org/abs/2411.05085</guid>
<content:encoded><![CDATA[
arXiv:2411.05085v2 Announce Type: replace 
Abstract: Radiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Grounded radiology report generation (GRRG) extends RRG by including the localisation of individual findings on the image. Currently, there are no manually annotated chest X-ray (CXR) datasets to train GRRG models. In this work, we present a dataset called PadChest-GR (Grounded-Reporting) derived from PadChest aimed at training GRRG models for CXR images. We curate a public bi-lingual dataset of 4,555 CXR studies with grounded reports (3,099 abnormal and 1,456 normal), each containing complete lists of sentences describing individual present (positive) and absent (negative) findings in English and Spanish. In total, PadChest-GR contains 7,037 positive and 3,422 negative finding sentences. Every positive finding sentence is associated with up to two independent sets of bounding boxes labelled by different readers and has categorical labels for finding type, locations, and progression. To the best of our knowledge, PadChest-GR is the first manually curated dataset designed to train GRRG models for understanding and interpreting radiological images and generated text. By including detailed localization and comprehensive annotations of all clinically relevant findings, it provides a valuable resource for developing and evaluating GRRG models from CXR images. PadChest-GR can be downloaded under request from https://bimcv.cipf.es/bimcv-projects/padchest-gr/
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CyberBOT: Towards Reliable Cybersecurity Education via Ontology-Grounded Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2504.00389</link>
<guid>https://arxiv.org/abs/2504.00389</guid>
<content:encoded><![CDATA[
arXiv:2504.00389v2 Announce Type: replace 
Abstract: Advancements in large language models (LLMs) have enabled the development of intelligent educational tools that support inquiry-based learning across technical domains. In cybersecurity education, where accuracy and safety are paramount, systems must go beyond surface-level relevance to provide information that is both trustworthy and domain-appropriate. To address this challenge, we introduce CyberBOT, a question-answering chatbot that leverages a retrieval-augmented generation (RAG) pipeline to incorporate contextual information from course-specific materials and validate responses using a domain-specific cybersecurity ontology. The ontology serves as a structured reasoning layer that constrains and verifies LLM-generated answers, reducing the risk of misleading or unsafe guidance. CyberBOT has been deployed in a large graduate-level course at Arizona State University (ASU), where more than one hundred students actively engage with the system through a dedicated web-based platform. Computational evaluations in lab environments highlight the potential capacity of CyberBOT, and a forthcoming field study will evaluate its pedagogical impact. By integrating structured domain reasoning with modern generative capabilities, CyberBOT illustrates a promising direction for developing reliable and curriculum-aligned AI applications in specialized educational contexts.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shutdownable Agents through POST-Agency</title>
<link>https://arxiv.org/abs/2505.20203</link>
<guid>https://arxiv.org/abs/2505.20203</guid>
<content:encoded><![CDATA[
arXiv:2505.20203v2 Announce Type: replace 
Abstract: Many fear that future artificial agents will resist shutdown. I present an idea - the POST-Agents Proposal - for ensuring that doesn't happen. I propose that we train agents to satisfy Preferences Only Between Same-Length Trajectories (POST). I then prove that POST - together with other conditions - implies Neutrality+: the agent maximizes expected utility, ignoring the probability distribution over trajectory-lengths. I argue that Neutrality+ keeps agents shutdownable and allows them to be useful.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research</title>
<link>https://arxiv.org/abs/2506.01326</link>
<guid>https://arxiv.org/abs/2506.01326</guid>
<content:encoded><![CDATA[
arXiv:2506.01326v2 Announce Type: replace 
Abstract: Operations research (OR) is widely deployed to solve critical decision-making problems with complex objectives and constraints, impacting manufacturing, logistics, finance, and healthcare outcomes. While Large Language Models (LLMs) have shown promising results in various domains, their practical application in industry-relevant operations research (OR) problems presents significant challenges and opportunities. Preliminary industrial applications of LLMs for operations research face two critical deployment challenges: 1) Self-correction focuses on code syntax rather than mathematical accuracy, causing costly errors; 2) Complex expert selection creates unpredictable workflows that reduce transparency and increase maintenance costs, making them impractical for time-sensitive business applications. To address these business limitations, we introduce ORMind, a cognitive-inspired framework that enhances optimization through counterfactual reasoning. Our approach emulates human cognition, implementing an end-to-end workflow that systematically transforms requirements into mathematical models and executable solver code. It is currently being tested internally in Lenovo's AI Assistant, with plans to enhance optimization capabilities for both business and consumer customers. Experiments demonstrate that ORMind outperforms existing methods, achieving a 9.5\% improvement on the NL4Opt dataset and a 14.6\% improvement on the ComplexOR dataset.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation</title>
<link>https://arxiv.org/abs/2506.07940</link>
<guid>https://arxiv.org/abs/2506.07940</guid>
<content:encoded><![CDATA[
arXiv:2506.07940v2 Announce Type: replace 
Abstract: Current AutoML platforms leave substantial performance untapped. Testing 180 fine-tuning tasks across models from 70M to 70B parameters, we found that HuggingFace AutoTrain, TogetherAI, Databricks, and Google Cloud consistently produce suboptimal configurations. Gradients, built on the Bittensor network, attacks this problem through competition. Independent miners race to find optimal hyperparameters, earning rewards proportional to their models' performance. This tournament drives exploration of configuration spaces that single-strategy methods never examine. In our experiments, Gradients achieved a 100\% win rate against TogetherAI, Databricks, and Google Cloud, and beat HuggingFace AutoTrain in 82.8\% of experiments. Mean improvements reached 42.1\% against commercial platforms. Retrieval-augmented generation tasks saw 30-40\% gains; diffusion models improved 23.4\% on person-specific generation. When miners compete for rewards, they develop optimization strategies that centralized approaches overlook. These findings demonstrate that decentralized systems with economic incentives can systematically outperform traditional AutoML, suggesting market dynamics may be key to achieving superior fine-tuning results. Code is available at https://github.com/rayonlabs/G.O.D.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Research Agents: A Systematic Examination And Roadmap</title>
<link>https://arxiv.org/abs/2506.18096</link>
<guid>https://arxiv.org/abs/2506.18096</guid>
<content:encoded><![CDATA[
arXiv:2506.18096v2 Announce Type: replace 
Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new category of autonomous AI systems, referred to as Deep Research (DR) agents. These agents are designed to tackle complex, multi-turn informational research tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon planning, multi-hop information retrieval, iterative tool use, and the generation of structured analytical reports. In this paper, we conduct a detailed analysis of the foundational technologies and architectural components that constitute Deep Research agents. We begin by reviewing information acquisition strategies, contrasting API-based retrieval methods with browser-based exploration. We then examine modular tool-use frameworks, including code execution, multimodal input processing, and the integration of Model Context Protocols (MCPs) to support extensibility and ecosystem development. To systematize existing approaches, we propose a taxonomy that differentiates between static and dynamic workflows, and we classify agent architectures based on planning strategies and agent composition, including single-agent and multi-agent configurations. We also provide a critical evaluation of current benchmarks, highlighting key limitations such as restricted access to external knowledge, sequential execution inefficiencies, and misalignment between evaluation metrics and the practical objectives of DR agents. Finally, we outline open challenges and promising directions for future research. A curated and continuously updated repository of DR agent research is available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChordPrompt: Orchestrating Cross-Modal Prompt Synergy for Multi-Domain Incremental Learning in CLIP</title>
<link>https://arxiv.org/abs/2506.19608</link>
<guid>https://arxiv.org/abs/2506.19608</guid>
<content:encoded><![CDATA[
arXiv:2506.19608v2 Announce Type: replace 
Abstract: Continual learning (CL) empowers pre-trained vision-language models to adapt effectively to novel or previously underrepresented data distributions without comprehensive retraining, enhancing their adaptability and efficiency. While vision-language models like CLIP show great promise, they struggle to maintain performance across domains in incremental learning scenarios. Existing prompt learning methods face two main limitations: 1) they primarily focus on class-incremental learning scenarios, lacking specific strategies for multi-domain task incremental learning; 2) most current approaches employ single-modal prompts, neglecting the potential benefits of cross-modal information exchange. To address these challenges, we propose the \ChordPrompt framework, which facilitates a harmonious interplay between visual and textual prompts. \ChordPrompt introduces cross-modal prompts to leverage interactions between visual and textual information. Our approach also employs domain-adaptive text prompts to select appropriate prompts for continual adaptation across multiple domains. Comprehensive experiments on multi-domain incremental learning benchmarks demonstrate that \ChordPrompt outperforms state-of-the-art methods in zero-shot generalization and downstream task performance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks</title>
<link>https://arxiv.org/abs/2507.17695</link>
<guid>https://arxiv.org/abs/2507.17695</guid>
<content:encoded><![CDATA[
arXiv:2507.17695v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a vital role in the evolution of 6G networks, by empowering real-time decision-making related to management and service provisioning to end-users. This shift facilitates the transition from a specialized intelligence approach, where artificial intelligence (AI) algorithms handle isolated tasks, to artificial general intelligence (AGI)-driven networks, where agents possess broader reasoning capabilities and can manage diverse network functions. In this paper, we introduce a novel agentic paradigm that combines LLMs with real-time optimization algorithms towards Trustworthy AI, defined as symbiotic agents. Optimizers at the LLM's input-level provide bounded uncertainty steering for numerically precise tasks, whereas output-level optimizers supervised by the LLM enable adaptive real-time control. We design and implement two novel agent types including: (i) Radio Access Network optimizers, and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We further propose an end-to-end architecture for AGI networks and evaluate it on a 5G testbed capturing channel fluctuations from moving vehicles. Results show that symbiotic agents reduce decision errors fivefold compared to standalone LLM-based agents, while smaller language models (SLM) achieve similar accuracy with a 99.9% reduction in GPU resource overhead and in near-real-time loops of 82 ms. A multi-agent demonstration for collaborative RAN on the real-world testbed highlights significant flexibility in service-level agreement and resource allocation, reducing RAN over-utilization by approximately 44%. Drawing on our findings and open-source implementations, we introduce the symbiotic paradigm as the foundation for next-generation, AGI-driven networks-systems designed to remain adaptable, efficient, and trustworthy even as LLMs advance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Activity Predictions in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.19733</link>
<guid>https://arxiv.org/abs/2507.19733</guid>
<content:encoded><![CDATA[
arXiv:2507.19733v2 Announce Type: replace 
Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in generating predictions about future events. By leveraging the semantic framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies (CCO), we demonstrate how data such as the movements of a fishing vessel can be organized in and retrieved from a knowledge graph. These query results are then used to create Markov chain models, allowing us to predict future states based on the vessel's history. To fully support this process, we introduce the term `spatiotemporal instant' to complete the necessary structural semantics. Additionally, we critique the prevailing ontological model of probability, according to which probabilities are about the future. We propose an alternative view, where at least some probabilities are treated as being about actual process profiles, which better captures the dynamics of real-world phenomena. Finally, we demonstrate how our Markov chain-based probability calculations can be seamlessly integrated back into the knowledge graph, enabling further analysis and decision-making.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks</title>
<link>https://arxiv.org/abs/2507.23751</link>
<guid>https://arxiv.org/abs/2507.23751</guid>
<content:encoded><![CDATA[
arXiv:2507.23751v2 Announce Type: replace 
Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on given seed tasks, and then generate a new synthetic example of similar quality and complexity. This is followed by a filtering step to select high-quality data using automatic metrics, which are then used for LLM training. In verifiable reasoning, our synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, when evaluated on MATH500, AMC23, AIME24, and GPQA-Diamond. For non-verifiable instruction-following tasks, our method surpasses the performance of both human and standard Self-Instruct training data on the AlpacaEval 2.0 and Arena-Hard benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations</title>
<link>https://arxiv.org/abs/2508.07834</link>
<guid>https://arxiv.org/abs/2508.07834</guid>
<content:encoded><![CDATA[
arXiv:2508.07834v3 Announce Type: replace 
Abstract: Over the years, the need for rescue operations throughout the world has increased rapidly. Demographic changes and the resulting risk of injury or health disorders form the basis for emergency calls. In such scenarios, first responders are in a rush to reach the patient in need, provide first aid, and save lives. In these situations, they must be able to provide personalized and optimized healthcare in the shortest possible time and estimate the patients condition with the help of freshly recorded vital data in an emergency situation. However, in such a timedependent situation, first responders and medical experts cannot fully grasp their knowledge and need assistance and recommendation for further medical treatments. To achieve this, on the spot calculated, evaluated, and processed knowledge must be made available to improve treatments by first responders. The Knowledge Graph presented in this article as a central knowledge representation provides first responders with an innovative knowledge management that enables intelligent treatment recommendations with an artificial intelligence-based pre-recognition of the situation.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.15327</link>
<guid>https://arxiv.org/abs/2508.15327</guid>
<content:encoded><![CDATA[
arXiv:2508.15327v2 Announce Type: replace 
Abstract: Offline reinforcement learning refers to the process of learning policies from fixed datasets, without requiring additional environment interaction. However, it often relies on well-defined reward functions, which are difficult and expensive to design. Human feedback is an appealing alternative, but its two common forms, expert demonstrations and preferences, have complementary limitations. Demonstrations provide stepwise supervision, but they are costly to collect and often reflect limited expert behavior modes. In contrast, preferences are easier to collect, but it is unclear which parts of a behavior contribute most to a trajectory segment, leaving credit assignment unresolved. In this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to unify these two feedback sources. For each transition in a preference labeled trajectory, SPW searches for the most similar state-action pairs from expert demonstrations and directly derives stepwise importance weights based on their similarity scores. These weights are then used to guide standard preference learning, enabling more accurate credit assignment that traditional approaches struggle to achieve. We demonstrate that SPW enables effective joint learning from preferences and demonstrations, outperforming prior methods that leverage both feedback types on challenging robot manipulation tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ramon Llull's Thinking Machine for Automated Ideation</title>
<link>https://arxiv.org/abs/2508.19200</link>
<guid>https://arxiv.org/abs/2508.19200</guid>
<content:encoded><![CDATA[
arXiv:2508.19200v3 Announce Type: replace 
Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P2DT: Mitigating Forgetting in task-incremental Learning with progressive prompt Decision Transformer</title>
<link>https://arxiv.org/abs/2401.11666</link>
<guid>https://arxiv.org/abs/2401.11666</guid>
<content:encoded><![CDATA[
arXiv:2401.11666v2 Announce Type: replace-cross 
Abstract: Catastrophic forgetting poses a substantial challenge for managing intelligent agents controlled by a large model, causing performance degradation when these agents face new tasks. In our work, we propose a novel solution - the Progressive Prompt Decision Transformer (P2DT). This method enhances a transformer-based model by dynamically appending decision tokens during new task training, thus fostering task-specific policies. Our approach mitigates forgetting in continual and offline reinforcement learning scenarios. Moreover, P2DT leverages trajectories collected via traditional reinforcement learning from all tasks and generates new task-specific tokens during training, thereby retaining knowledge from previous studies. Preliminary results demonstrate that our model effectively alleviates catastrophic forgetting and scales well with increasing task environments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Metrics to Meaning: Time to Rethink Evaluation in Human-AI Collaborative Design</title>
<link>https://arxiv.org/abs/2402.07911</link>
<guid>https://arxiv.org/abs/2402.07911</guid>
<content:encoded><![CDATA[
arXiv:2402.07911v2 Announce Type: replace-cross 
Abstract: As AI systems increasingly shape decision making in creative design contexts, understanding how humans engage with these tools has become a critical challenge for interactive intelligent systems research. This paper contributes a challenge to rethink how to evaluate human--AI collaborative systems, advocating for a more nuanced and multidimensional approach. Findings from one of the largest field studies to date (n = 808) of a human--AI co-creative system, The Genetic Car Designer, complemented by a controlled lab study (n = 12) are presented. The system is based on an interactive evolutionary algorithm where participants were tasked with designing a simple two dimensional representation of a car. Participants were exposed to galleries of design suggestions generated by an intelligent system, MAP--Elites, and a random control. Results indicate that exposure to galleries generated by MAP--Elites significantly enhanced both cognitive and behavioural engagement, leading to higher-quality design outcomes. Crucially for the wider community, the analysis reveals that conventional evaluation methods, which often focus on solely behavioural and design quality metrics, fail to capture the full spectrum of user engagement. By considering the human--AI design process as a changing emotional, behavioural and cognitive state of the designer, we propose evaluating human--AI systems holistically and considering intelligent systems as a core part of the user experience -- not simply a back end tool.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Machine Learning-Based Security and Privacy Protection Framework for Internet of Medical Things Systems</title>
<link>https://arxiv.org/abs/2403.09752</link>
<guid>https://arxiv.org/abs/2403.09752</guid>
<content:encoded><![CDATA[
arXiv:2403.09752v3 Announce Type: replace-cross 
Abstract: The Internet of Medical Things transcends traditional medical boundaries, enabling a transition from reactive treatment to proactive prevention. This innovative method revolutionizes healthcare by facilitating early disease detection and tailored care, particularly in chronic disease management, where IoMT automates treatments based on real-time health data collection. Nonetheless, its benefits are countered by significant security challenges that endanger the lives of its users due to the sensitivity and value of the processed data, thereby attracting malicious interests. Moreover, the utilization of wireless communication for data transmission exposes medical data to interception and tampering by cybercriminals. Additionally, anomalies may arise due to human error, network interference, or hardware malfunctions. In this context, anomaly detection based on Machine Learning (ML) is an interesting solution, but it comes up against obstacles in terms of explicability and privacy protection. To address these challenges, a new framework for Intrusion Detection Systems is introduced, leveraging Artificial Neural Networks for intrusion detection while utilizing Federated Learning for privacy preservation. Additionally, eXplainable Artificial Intelligence methods are incorporated to enhance model explanation and interpretation. The efficacy of the proposed framework is evaluated and compared with centralized approaches using multiple datasets containing network and medical data, simulating various attack types impacting the confidentiality, integrity, and availability of medical and physiological data. The results offer compelling evidence that the FL method performs comparably to the centralized method, demonstrating high performance. Additionally, it affords the dual advantage of safeguarding privacy and providing model explanation while adhering to ethical principles.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MF-OML: Online Mean-Field Reinforcement Learning with Occupation Measures for Large Population Games</title>
<link>https://arxiv.org/abs/2405.00282</link>
<guid>https://arxiv.org/abs/2405.00282</guid>
<content:encoded><![CDATA[
arXiv:2405.00282v2 Announce Type: replace-cross 
Abstract: Reinforcement learning for multi-agent games has attracted lots of attention recently. However, given the challenge of solving Nash equilibria for large population games, existing works with guaranteed polynomial complexities either focus on variants of zero-sum and potential games, or aim at solving (coarse) correlated equilibria, or require access to simulators, or rely on certain assumptions that are hard to verify. This work proposes MF-OML (Mean-Field Occupation-Measure Learning), an online mean-field reinforcement learning algorithm for computing approximate Nash equilibria of large population sequential symmetric games. MF-OML is the first fully polynomial multi-agent reinforcement learning algorithm for provably solving Nash equilibria (up to mean-field approximation gaps that vanish as the number of players $N$ goes to infinity) beyond variants of zero-sum and potential games. When evaluated by the cumulative deviation from Nash equilibria, the algorithm is shown to achieve a high probability regret bound of $\tilde{O}(M^{3/4}+N^{-1/2}M)$ for games with the strong Lasry-Lions monotonicity condition, and a regret bound of $\tilde{O}(M^{11/12}+N^{- 1/6}M)$ for games with only the Lasry-Lions monotonicity condition, where $M$ is the total number of episodes and $N$ is the number of agents of the game. As a byproduct, we also obtain the first tractable globally convergent computational algorithm for computing approximate Nash equilibria of monotone mean-field games.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention</title>
<link>https://arxiv.org/abs/2406.15486</link>
<guid>https://arxiv.org/abs/2406.15486</guid>
<content:encoded><![CDATA[
arXiv:2406.15486v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$ compared with FlashAttention.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Banishing LLM Hallucinations Requires Rethinking Generalization</title>
<link>https://arxiv.org/abs/2406.17642</link>
<guid>https://arxiv.org/abs/2406.17642</guid>
<content:encoded><![CDATA[
arXiv:2406.17642v2 Announce Type: replace-cross 
Abstract: Despite their powerful chat, coding, and reasoning abilities, Large Language Models (LLMs) frequently hallucinate. Conventional wisdom suggests that hallucinations are a consequence of a balance between creativity and factuality, which can be mitigated, but not eliminated, by grounding the LLM in external knowledge sources. Through extensive systematic experiments, we show that these traditional approaches fail to explain why LLMs hallucinate in practice. Specifically, we show that LLMs augmented with a massive Mixture of Memory Experts (MoME) can easily memorize large datasets of random numbers. We corroborate these experimental findings with a theoretical construction showing that simple neural networks trained to predict the next token hallucinate when the training loss is above a threshold as it usually does in practice when training on internet scale data. We interpret our findings by comparing against traditional retrieval methods for mitigating hallucinations. We use our findings to design a first generation model for removing hallucinations -- Lamini-1 -- that stores facts in a massive mixture of millions of memory experts that are retrieved dynamically.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</title>
<link>https://arxiv.org/abs/2407.00079</link>
<guid>https://arxiv.org/abs/2407.00079</guid>
<content:encoded><![CDATA[
arXiv:2407.00079v4 Announce Type: replace-cross 
Abstract: Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI. It features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. It also leverages the underutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated cache of KVCache. The core of Mooncake is its KVCache-centric scheduler, which balances maximizing overall effective throughput while meeting latency-related Service Level Objectives (SLOs). Unlike traditional studies that assume all requests will be processed, Mooncake faces challenges due to highly overloaded scenarios. To mitigate these, we developed a prediction-based early rejection policy. Experiments show that Mooncake excels in long-context scenarios. Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs. Under real workloads, Mooncake's innovative architecture enables Kimi to handle 75% more requests.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Agentic AI on Particle Accelerators</title>
<link>https://arxiv.org/abs/2409.06336</link>
<guid>https://arxiv.org/abs/2409.06336</guid>
<content:encoded><![CDATA[
arXiv:2409.06336v4 Announce Type: replace-cross 
Abstract: As particle accelerators grow in complexity, traditional control methods face increasing challenges in achieving optimal performance. This paper envisions a paradigm shift: a decentralized multi-agent framework for accelerator control, powered by Large Language Models (LLMs) and distributed among autonomous agents. We present a proposition of a self-improving decentralized system where intelligent agents handle high-level tasks and communication and each agent is specialized to control individual accelerator components.
  This approach raises some questions: What are the future applications of AI in particle accelerators? How can we implement an autonomous complex system such as a particle accelerator where agents gradually improve through experience and human feedback? What are the implications of integrating a human-in-the-loop component for labeling operational data and providing expert guidance? We show three examples, where we demonstrate the viability of such architecture.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Machine and Human Visual Representations across Abstraction Levels</title>
<link>https://arxiv.org/abs/2409.06509</link>
<guid>https://arxiv.org/abs/2409.06509</guid>
<content:encoded><![CDATA[
arXiv:2409.06509v4 Announce Type: replace-cross 
Abstract: Deep neural networks have achieved success across a wide range of applications, including as models of human behavior and neural representations in vision tasks. However, neural network training and human learning differ in fundamental ways, and neural networks often fail to generalize as robustly as humans do raising questions regarding the similarity of their underlying representations. What is missing for modern learning systems to exhibit more human-aligned behavior? We highlight a key misalignment between vision models and humans: whereas human conceptual knowledge is hierarchically organized from fine- to coarse-scale distinctions, model representations do not accurately capture all these levels of abstraction. To address this misalignment, we first train a teacher model to imitate human judgments, then transfer human-aligned structure from its representations to refine the representations of pretrained state-of-the-art vision foundation models via finetuning. These human-aligned models more accurately approximate human behavior and uncertainty across a wide range of similarity tasks, including a new dataset of human judgments spanning multiple levels of semantic abstractions. They also perform better on a diverse set of machine learning tasks, increasing generalization and out-of-distribution robustness. Thus, infusing neural networks with additional human knowledge yields a best-of-both-worlds representation that is both more consistent with human cognitive judgments and more practically useful, thus paving the way toward more robust, interpretable, and human-aligned artificial intelligence systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Consistency Representation Learning for Lifelong Person Re-Identification</title>
<link>https://arxiv.org/abs/2409.19954</link>
<guid>https://arxiv.org/abs/2409.19954</guid>
<content:encoded><![CDATA[
arXiv:2409.19954v4 Announce Type: replace-cross 
Abstract: Lifelong person re-identification (LReID) exhibits a contradictory relationship between intra-domain discrimination and inter-domain gaps when learning from continuous data. Intra-domain discrimination focuses on individual nuances (i.e., clothing type, accessories, etc.), while inter-domain gaps emphasize domain consistency. Achieving a trade-off between maximizing intra-domain discrimination and minimizing inter-domain gaps is a crucial challenge for improving LReID performance. Most existing methods strive to reduce inter-domain gaps through knowledge distillation to maintain domain consistency. However, they often ignore intra-domain discrimination. To address this challenge, we propose a novel domain consistency representation learning (DCR) model that explores global and attribute-wise representations as a bridge to balance intra-domain discrimination and inter-domain gaps. At the intra-domain level, we explore the complementary relationship between global and attribute-wise representations to improve discrimination among similar identities. Excessive learning intra-domain discrimination can lead to catastrophic forgetting. We further develop an attribute-oriented anti-forgetting (AF) strategy that explores attribute-wise representations to enhance inter-domain consistency, and propose a knowledge consolidation (KC) strategy to facilitate knowledge transfer. Extensive experiments show that our DCR achieves superior performance compared to state-of-the-art LReID methods. Our code is available at https://github.com/LiuShiBen/DCR.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling</title>
<link>https://arxiv.org/abs/2410.16033</link>
<guid>https://arxiv.org/abs/2410.16033</guid>
<content:encoded><![CDATA[
arXiv:2410.16033v4 Announce Type: replace-cross 
Abstract: Inference-time alignment enhances the performance of large language models without requiring additional training or fine-tuning but presents challenges due to balancing computational efficiency with high-quality output. Best-of-N (BoN) sampling, as a simple yet powerful approach, generates multiple responses and selects the best one, achieving improved performance but with a high computational cost. We propose TreeBoN, a novel framework that integrates a speculative tree-search strategy into Best-of-N (BoN) Sampling. TreeBoN maintains a set of parent nodes, iteratively branching and pruning low-quality responses, thereby reducing computational overhead while maintaining high output quality. Our approach also leverages token-level rewards from Direct Preference Optimization (DPO) to guide tree expansion and prune low-quality paths. We evaluate TreeBoN using AlpacaFarm, HH-RLHF, UltraFeedback, GSM8K, and TutorEval datasets, demonstrating consistent improvements. Specifically, TreeBoN achieves the highest win rate of 65% on TutorEval and around 60% win rates across other different datasets, outperforming standard BoN with the same computational cost and showcasing its scalability and alignment efficacy.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft-TransFormers for Continual Learning</title>
<link>https://arxiv.org/abs/2411.16073</link>
<guid>https://arxiv.org/abs/2411.16073</guid>
<content:encoded><![CDATA[
arXiv:2411.16073v2 Announce Type: replace-cross 
Abstract: Inspired by the Well-initialized Lottery Ticket Hypothesis (WLTH), which provides suboptimal fine-tuning solutions, we propose a novel fully fine-tuned continual learning (CL) method referred to as Soft-TransFormers (Soft-TF). Soft-TF sequentially learns and selects an optimal soft-network for each task. During sequential training in CL, a well-initialized Soft-TF mask optimizes the weights of sparse layers to obtain task-adaptive soft (real-valued) networks, while keeping the well-pre-trained layer parameters frozen. In inference, the identified task-adaptive network of Soft-TF masks the parameters of the pre-trained network, mapping to an optimal solution for each task and minimizing Catastrophic Forgetting (CF) - the soft-masking preserves the knowledge of the pre-trained network. Extensive experiments on the Vision Transformer (ViT) and the Language Transformer (Bert) demonstrate the effectiveness of Soft-TF, achieving state-of-the-art performance across Vision and Language Class Incremental Learning (CIL) scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GalaxAlign: Mimicking Citizen Scientists' Multimodal Guidance for Galaxy Morphology Analysis</title>
<link>https://arxiv.org/abs/2411.19475</link>
<guid>https://arxiv.org/abs/2411.19475</guid>
<content:encoded><![CDATA[
arXiv:2411.19475v2 Announce Type: replace-cross 
Abstract: Galaxy morphology analysis involves studying galaxies based on their shapes and structures. For such studies, fundamental tasks include identifying and classifying galaxies in astronomical images, as well as retrieving visually or structurally similar galaxies through similarity search. Existing methods either directly train domain-specific foundation models on large, annotated datasets or fine-tune vision foundation models on a smaller set of images. The former is effective but costly, while the latter is more resource-efficient but often yields lower accuracy. To address these challenges, we introduce GalaxAlign, a multimodal approach inspired by how citizen scientists identify galaxies in astronomical images by following textual descriptions and matching schematic symbols. Specifically, GalaxAlign employs a tri-modal alignment framework to align three types of data during fine-tuning: (1) schematic symbols representing galaxy shapes and structures, (2) textual labels for these symbols, and (3) galaxy images. By incorporating multimodal instructions, GalaxAlign eliminates the need for expensive pretraining and enhances the effectiveness of fine-tuning. Experiments on galaxy classification and similarity search demonstrate that our method effectively fine-tunes general pre-trained models for astronomical tasks by incorporating domain-specific multi-modal knowledge. Code is available at https://github.com/RapidsAtHKUST/GalaxAlign.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning</title>
<link>https://arxiv.org/abs/2501.08848</link>
<guid>https://arxiv.org/abs/2501.08848</guid>
<content:encoded><![CDATA[
arXiv:2501.08848v2 Announce Type: replace-cross 
Abstract: Network simulation is pivotal in network modeling, assisting with tasks ranging from capacity planning to performance estimation. Traditional approaches such as Discrete Event Simulation (DES) face limitations in terms of computational cost and accuracy. This paper introduces RouteNet-Gauss, a novel integration of a testbed network with a Machine Learning (ML) model to address these challenges. By using the testbed as a hardware accelerator, RouteNet-Gauss generates training datasets rapidly and simulates network scenarios with high fidelity to real-world conditions. Experimental results show that RouteNet-Gauss significantly reduces prediction errors by up to 95% and achieves a 488x speedup in inference time compared to state-of-the-art DES-based methods. RouteNet-Gauss's modular architecture is dynamically constructed based on the specific characteristics of the network scenario, such as topology and routing. This enables it to understand and generalize to different network configurations beyond those seen during training, including networks up to 10x larger. Additionally, it supports Temporal Aggregated Performance Estimation (TAPE), providing configurable temporal granularity and maintaining high accuracy in flow performance metrics. This approach shows promise in improving both simulation efficiency and accuracy, offering a valuable tool for network operators.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2501.09997</link>
<guid>https://arxiv.org/abs/2501.09997</guid>
<content:encoded><![CDATA[
arXiv:2501.09997v3 Announce Type: replace-cross 
Abstract: Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational overhead, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey on Hand Gesture Recognition from Visual Input</title>
<link>https://arxiv.org/abs/2501.11992</link>
<guid>https://arxiv.org/abs/2501.11992</guid>
<content:encoded><![CDATA[
arXiv:2501.11992v3 Announce Type: replace-cross 
Abstract: Hand gesture recognition has become an important research area, driven by the growing demand for human-computer interaction in fields such as sign language recognition, virtual and augmented reality, and robotics. Despite the rapid growth of the field, there are few surveys that comprehensively cover recent research developments, available solutions, and benchmark datasets. This survey addresses this gap by examining the latest advancements in hand gesture and 3D hand pose recognition from various types of camera input data including RGB images, depth images, and videos from monocular or multiview cameras, examining the differing methodological requirements of each approach. Furthermore, an overview of widely used datasets is provided, detailing their main characteristics and application domains. Finally, open challenges such as achieving robust recognition in real-world environments, handling occlusions, ensuring generalization across diverse users, and addressing computational efficiency for real-time applications are highlighted to guide future research directions. By synthesizing the objectives, methodologies, and applications of recent studies, this survey offers valuable insights into current trends, challenges, and opportunities for future research in human hand gesture recognition.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predict, Cluster, Refine: A Joint Embedding Predictive Self-Supervised Framework for Graph Representation Learning</title>
<link>https://arxiv.org/abs/2502.01684</link>
<guid>https://arxiv.org/abs/2502.01684</guid>
<content:encoded><![CDATA[
arXiv:2502.01684v4 Announce Type: replace-cross 
Abstract: Graph representation learning has emerged as a cornerstone for tasks like node classification and link prediction, yet prevailing self-supervised learning (SSL) methods face challenges such as computational inefficiency, reliance on contrastive objectives, and representation collapse. Existing approaches often depend on feature reconstruction, negative sampling, or complex decoders, which introduce training overhead and hinder generalization. Further, current techniques which address such limitations fail to account for the contribution of node embeddings to a certain prediction in the absence of labeled nodes. To address these limitations, we propose a novel joint embedding predictive framework for graph SSL that eliminates contrastive objectives and negative sampling while preserving semantic and structural information. Additionally, we introduce a semantic-aware objective term that incorporates pseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node discriminability by evaluating latent feature contributions. Extensive experiments demonstrate that our framework outperforms state-of-the-art graph SSL methods across benchmarks, achieving superior performance without contrastive loss or complex decoders. Key innovations include (1) a non-contrastive, view-invariant joint embedding predictive architecture, (2) Leveraging single context and multiple targets relationship between subgraphs, and (3) GMM-based pseudo-label scoring to capture semantic contributions. This work advances graph SSL by offering a computationally efficient, collapse-resistant paradigm that bridges spatial and semantic graph features for downstream tasks. The code for our paper can be found at https://github.com/Deceptrax123/JPEB-GSSL
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual LLMs</title>
<link>https://arxiv.org/abs/2502.04387</link>
<guid>https://arxiv.org/abs/2502.04387</guid>
<content:encoded><![CDATA[
arXiv:2502.04387v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) has enabled the training of multilingual large language models (LLMs) on diverse and decentralized multilingual data, especially on low-resource languages. To improve client-specific performance, personalization via the use of parameter-efficient fine-tuning (PEFT) modules such as LoRA is common. This involves a personalization strategy (PS), such as the design of the PEFT adapter structures (e.g., in which layers to add LoRAs and what ranks) and choice of hyperparameters (e.g., learning rates) for fine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a federated learning-to-personalize method for multilingual LLMs in cross-device FL settings. Unlike most existing PEFT structure selection methods, which are prone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the optimal personalized PEFT structure for each client via Bayesian sparse rank selection. Evaluations on both simulated and real-world multilingual FL benchmarks demonstrate that FedP$^2$EFT largely outperforms existing personalized fine-tuning methods, while complementing other existing FL methods. Code is available at https://github.com/SamsungLabs/fedp2eft.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid Word Learning Through Meta In-Context Learning</title>
<link>https://arxiv.org/abs/2502.14791</link>
<guid>https://arxiv.org/abs/2502.14791</guid>
<content:encoded><![CDATA[
arXiv:2502.14791v3 Announce Type: replace-cross 
Abstract: Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating a Model-Agnostic and Imputation-Free Approach for Irregularly-Sampled Multivariate Time-Series Modeling</title>
<link>https://arxiv.org/abs/2502.15785</link>
<guid>https://arxiv.org/abs/2502.15785</guid>
<content:encoded><![CDATA[
arXiv:2502.15785v2 Announce Type: replace-cross 
Abstract: Modeling Irregularly-sampled and Multivariate Time Series (IMTS) is crucial across a variety of applications where different sets of variates may be missing at different time-steps due to sensor malfunctions or high data acquisition costs. Existing approaches for IMTS either consider a two-stage impute-then-model framework or involve specialized architectures specific to a particular model and task. We perform a series of experiments to derive novel insights about the performance of IMTS methods on a variety of semi-synthetic and real-world datasets for both classification and forecasting. We also introduce Missing Feature-aware Time Series Modeling (MissTSM) or MissTSM, a novel model-agnostic and imputation-free approach for IMTS modeling. We show that MissTSM shows competitive performance compared to other IMTS approaches, especially when the amount of missing values is large and the data lacks simplistic periodic structures - conditions common to real-world IMTS applications.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs</title>
<link>https://arxiv.org/abs/2502.18179</link>
<guid>https://arxiv.org/abs/2502.18179</guid>
<content:encoded><![CDATA[
arXiv:2502.18179v2 Announce Type: replace-cross 
Abstract: This paper defines and explores the design space for information extraction (IE) from layout-rich documents using large language models (LLMs). The three core challenges of layout-aware IE with LLMs are 1) data structuring, 2) model engagement, and 3) output refinement. Our study investigates the sub-problems and methods within these core challenges, such as input representation, chunking, prompting, selection of LLMs, and multimodal models. It examines the effect of different design choices through LayIE-LLM, a new, open-source, layout-aware IE test suite, benchmarking against traditional, fine-tuned IE models. The results on two IE datasets show that LLMs require adjustment of the IE pipeline to achieve competitive performance: the optimized configuration found with LayIE-LLM achieves 13.3--37.5 F1 points more than a general-practice baseline configuration using the same LLM. To find a well-working configuration, we develop a one-factor-at-a-time (OFAT) method that achieves near-optimal results. Our method is only 0.8--1.8 points lower than the best full factorial exploration with a fraction (2.8%) of the required computation. Overall, we demonstrate that, if well-configured, general-purpose LLMs match the performance of specialized models, providing a cost-effective, finetuning-free alternative. Our test-suite is available at https://github.com/gayecolakoglu/LayIE-LLM.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiently Editing Mixture-of-Experts Models with Compressed Experts</title>
<link>https://arxiv.org/abs/2503.00634</link>
<guid>https://arxiv.org/abs/2503.00634</guid>
<content:encoded><![CDATA[
arXiv:2503.00634v2 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) models have become a key approach for scaling large language models efficiently by activating only a subset of experts during training and inference. Typically, the number of activated experts presents a trade-off: fewer experts reduce computational costs, while more experts improve performance. Recent studies reveal that not all activated experts contribute equally to model performance, with some providing minimal utility, particularly when finetuning pretrained MoE models for specialized downstream tasks. The co-existence of significant and redundant parameters in experts provides us an opportunity to reduce the number of activated experts while maintaining model performance. In this work, we propose the concept of compressed experts, lightweight modules that serve as compact representations of full experts. Our approach preserves the most important experts while replacing other auxiliary activated experts with compressed experts. The reduction of active parameters significantly lowers inference costs while achieving comparable performance. Extensive experiments on models including Phi-MoE and OLMoE demonstrate that compressed experts recover over 90% of full expert performance across various tasks while reducing more than 30% active parameters and saving 20% in inference costs. This approach enables efficient deployment of MoE models in resource-constrained settings and facilitates scaling to larger models with manageable overhead. Our code is available at https://github.com/yifei-he/Compressed-Experts.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impoola: The Power of Average Pooling for Image-Based Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.05546</link>
<guid>https://arxiv.org/abs/2503.05546</guid>
<content:encoded><![CDATA[
arXiv:2503.05546v2 Announce Type: replace-cross 
Abstract: As image-based deep reinforcement learning tackles more challenging tasks, increasing model size has become an important factor in improving performance. Recent studies achieved this by focusing on the parameter efficiency of scaled networks, typically using Impala-CNN, a 15-layer ResNet-inspired network, as the image encoder. However, while Impala-CNN evidently outperforms older CNN architectures, potential advancements in network design for deep reinforcement learning-specific image encoders remain largely unexplored. We find that replacing the flattening of output feature maps in Impala-CNN with global average pooling leads to a notable performance improvement. This approach outperforms larger and more complex models in the Procgen Benchmark, particularly in terms of generalization. We call our proposed encoder model Impoola-CNN. A decrease in the network's translation sensitivity may be central to this improvement, as we observe the most significant gains in games without agent-centered observations. Our results demonstrate that network scaling is not just about increasing model size - efficient network design is also an essential factor. We make our code available at https://github.com/raphajaner/impoola.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TruthLens: Visual Grounding for Universal DeepFake Reasoning</title>
<link>https://arxiv.org/abs/2503.15867</link>
<guid>https://arxiv.org/abs/2503.15867</guid>
<content:encoded><![CDATA[
arXiv:2503.15867v3 Announce Type: replace-cross 
Abstract: Detecting DeepFakes has become a crucial research area as the widespread use of AI image generators enables the effortless creation of face-manipulated and fully synthetic content, while existing methods are often limited to binary classification (real vs. fake) and lack interpretability. To address these challenges, we propose TruthLens, a novel, unified, and highly generalizable framework that goes beyond traditional binary classification, providing detailed, textual reasoning for its predictions. Distinct from conventional methods, TruthLens performs MLLM grounding.
  TruthLens uses a task-driven representation integration strategy that unites global semantic context from a multimodal large language model (MLLM) with region-specific forensic cues through explicit cross-modal adaptation of a vision-only model. This enables nuanced, region-grounded reasoning for both face-manipulated and fully synthetic content, and supports fine-grained queries such as "Does the eyes/nose/mouth look real or fake?"- capabilities beyond pretrained MLLMs alone. Extensive experiments across diverse datasets demonstrate that TruthLens sets a new benchmark in both forensic interpretability and detection accuracy, generalizing to seen and unseen manipulations alike. By unifying high-level scene understanding with fine-grained region grounding, TruthLens delivers transparent DeepFake forensics, bridging a critical gap in the literature.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDVIO2.0: Wind and Disturbance Estimation with Hybrid Dynamics VIO</title>
<link>https://arxiv.org/abs/2504.00969</link>
<guid>https://arxiv.org/abs/2504.00969</guid>
<content:encoded><![CDATA[
arXiv:2504.00969v3 Announce Type: replace-cross 
Abstract: Visual-inertial odometry (VIO) is widely used for state estimation in autonomous micro aerial vehicles using onboard sensors. Current methods improve VIO by incorporating a model of the translational vehicle dynamics, yet their performance degrades when faced with low-accuracy vehicle models or continuous external disturbances, like wind. Additionally, incorporating rotational dynamics in these models is computationally intractable when they are deployed in online applications, e.g., in a closed-loop control system. We present HDVIO2.0, which models full 6-DoF, translational and rotational, vehicle dynamics and tightly incorporates them into a VIO with minimal impact on the runtime. HDVIO2.0 builds upon the previous work, HDVIO, and addresses these challenges through a hybrid dynamics model combining a point-mass vehicle model with a learning-based component, with access to control commands and IMU history, to capture complex aerodynamic effects. The key idea behind modeling the rotational dynamics is to represent them with continuous-time functions. HDVIO2.0 leverages the divergence between the actual motion and the predicted motion from the hybrid dynamics model to estimate external forces as well as the robot state. Our system surpasses the performance of state-of-the-art methods in experiments using public and new drone dynamics datasets, as well as real-world flights in winds up to 25 km/h. Unlike existing approaches, we also show that accurate vehicle dynamics predictions are achievable without precise knowledge of the full vehicle state.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond</title>
<link>https://arxiv.org/abs/2504.13037</link>
<guid>https://arxiv.org/abs/2504.13037</guid>
<content:encoded><![CDATA[
arXiv:2504.13037v4 Announce Type: replace-cross 
Abstract: Cardiac magnetic resonance imaging is the gold standard for non-invasive cardiac assessment, offering rich spatio-temporal views of the cardiac anatomy and physiology. Patient-level health factors, such as demographics, metabolic, and lifestyle, are known to substantially influence cardiovascular health and disease risk, yet remain uncaptured by CMR alone. To holistically understand cardiac health and to enable the best possible interpretation of an individual's disease risk, CMR and patient-level factors must be jointly exploited within an integrated framework. Recent multi-modal approaches have begun to bridge this gap, yet they often rely on limited spatio-temporal data and focus on isolated clinical tasks, thereby hindering the development of a comprehensive representation for cardiac health evaluation. To overcome these limitations, we introduce ViTa, a step toward foundation models that delivers a comprehensive representation of the heart and a precise interpretation of individual disease risk. Leveraging data from 42,000 UK Biobank participants, ViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling a complete capture of the cardiac cycle. These imaging data are then fused with detailed tabular patient-level factors, enabling context-aware insights. This multi-modal paradigm supports a wide spectrum of downstream tasks, including cardiac phenotype and physiological feature prediction, segmentation, and classification of cardiac and metabolic diseases within a single unified framework. By learning a shared latent representation that bridges rich imaging features and patient context, ViTa moves beyond traditional, task-specific models toward a universal, patient-specific understanding of cardiac health, highlighting its potential to advance clinical utility and scalability in cardiac analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildFireCan-MMD: A Multimodal Dataset for Classification of User-Generated Content During Wildfires in Canada</title>
<link>https://arxiv.org/abs/2504.13231</link>
<guid>https://arxiv.org/abs/2504.13231</guid>
<content:encoded><![CDATA[
arXiv:2504.13231v3 Announce Type: replace-cross 
Abstract: Rapid information access is vital during wildfires, yet traditional data sources are slow and costly. Social media offers real-time updates, but extracting relevant insights remains a challenge. In this work, we focus on multimodal wildfire social media data, which, although existing in current datasets, is currently underrepresented in Canadian contexts. We present WildFireCan-MMD, a new multimodal dataset of X posts from recent Canadian wildfires, annotated across twelve key themes. We evaluate zero-shot vision-language models on this dataset and compare their results with those of custom-trained and baseline classifiers. We show that while baseline methods and zero-shot prompting offer quick deployment, custom-trained models outperform them when labelled data is available. Our best-performing custom model reaches 84.48% f-score, outperforming VLMs and baseline classifiers. We also demonstrate how this model can be used to uncover trends during wildfires, through the collection and analysis of a large unlabeled dataset. Our dataset facilitates future research in wildfire response, and our findings highlight the importance of tailored datasets and task-specific training. Importantly, such datasets should be localized, as disaster response requirements vary across regions and contexts.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Developers' Self-Declaration of AI-Generated Code: An Analysis of Practices</title>
<link>https://arxiv.org/abs/2504.16485</link>
<guid>https://arxiv.org/abs/2504.16485</guid>
<content:encoded><![CDATA[
arXiv:2504.16485v2 Announce Type: replace-cross 
Abstract: AI code generation tools have gained significant popularity among developers, who use them to assist in software development due to their capability to generate code. Existing studies mainly explored the quality, e.g., correctness and security, of AI-generated code, while in real-world software development, the prerequisite is to distinguish AI-generated code from human-written code, which emphasizes the need to explicitly declare AI-generated code by developers. To this end, this study intends to understand the ways developers use to self-declare AI-generated code and explore the reasons why developers choose to self-declare or not. We conducted a mixed-methods study consisting of two phases. In the first phase, we mined GitHub repositories and collected 613 instances of AI-generated code snippets. In the second phase, we conducted a follow-up practitioners' survey, which received 111 valid responses. Our research revealed the practices followed by developers to self-declare AI-generated code. Most practitioners (76.6%) always or sometimes self-declare AI-generated code. In contrast, other practitioners (23.4%) noted that they never self-declare AI-generated code. The reasons for self-declaring AI-generated code include the need to track and monitor the code for future review and debugging, and ethical considerations. The reasons for not self-declaring AI-generated code include extensive modifications to AI-generated code and the developers' perception that self-declaration is an unnecessary activity. We finally provided guidelines for practitioners to self-declare AI-generated code, addressing ethical and code quality concerns.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LawFlow: Collecting and Simulating Lawyers' Thought Processes on Business Formation Case Studies</title>
<link>https://arxiv.org/abs/2504.18942</link>
<guid>https://arxiv.org/abs/2504.18942</guid>
<content:encoded><![CDATA[
arXiv:2504.18942v2 Announce Type: replace-cross 
Abstract: Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, we introduce LawFlow, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, LawFlow captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice. Using LawFlow, we compare human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. Our findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Our results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems.
  All data and code are available on our project page (https://minnesotanlp.github.io/LawFlow-website/).
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group-in-Group Policy Optimization for LLM Agent Training</title>
<link>https://arxiv.org/abs/2505.10978</link>
<guid>https://arxiv.org/abs/2505.10978</guid>
<content:encoded><![CDATA[
arXiv:2505.10978v2 Announce Type: replace-cross 
Abstract: Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to long-horizon LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals and achieves performance gains of > 12\% on ALFWorld and > 9\% on WebShop over the GRPO baseline: all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When a Reinforcement Learning Agent Encounters Unknown Unknowns</title>
<link>https://arxiv.org/abs/2505.13188</link>
<guid>https://arxiv.org/abs/2505.13188</guid>
<content:encoded><![CDATA[
arXiv:2505.13188v2 Announce Type: replace-cross 
Abstract: An AI agent might surprisingly find she has reached an unknown state which she has never been aware of -- an unknown unknown. We mathematically ground this scenario in reinforcement learning: an agent, after taking an action calculated from value functions $Q$ and $V$ defined on the {\it {aware domain}}, reaches a state out of the domain. To enable the agent to handle this scenario, we propose an {\it episodic Markov decision {process} with growing awareness} (EMDP-GA) model, taking a new {\it noninformative value expansion} (NIVE) approach to expand value functions to newly aware areas: when an agent arrives at an unknown unknown, value functions $Q$ and $V$ whereon are initialised by noninformative beliefs -- the averaged values on the aware domain. This design is out of respect for the complete absence of knowledge in the newly discovered state. The upper confidence bound momentum Q-learning is then adapted to the growing awareness for training the EMDP-GA model. We prove that (1) the regret of our approach is asymptotically consistent with the state of the art (SOTA) without exposure to unknown unknowns in an extremely uncertain environment, and (2) our computational complexity and space complexity are comparable with the SOTA -- these collectively suggest that though an unknown unknown is surprising, it will be asymptotically properly discovered with decent speed and an affordable cost.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16022</link>
<guid>https://arxiv.org/abs/2505.16022</guid>
<content:encoded><![CDATA[
arXiv:2505.16022v2 Announce Type: replace-cross 
Abstract: Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language model's output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train. In this work, we propose NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across a wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation</title>
<link>https://arxiv.org/abs/2505.20353</link>
<guid>https://arxiv.org/abs/2505.20353</guid>
<content:encoded><![CDATA[
arXiv:2505.20353v2 Announce Type: replace-cross 
Abstract: Diffusion Transformers (DiT) are powerful generative models but remain computationally intensive due to their iterative structure and deep transformer stacks. To alleviate this inefficiency, we propose FastCache, a hidden-state-level caching and compression framework that accelerates DiT inference by exploiting redundancy within the model's internal representations. FastCache introduces a dual strategy: (1) a spatial-aware token selection mechanism that adaptively filters redundant tokens based on hidden state saliency, and (2) a transformer-level cache that reuses latent activations across timesteps when changes are statistically insignificant. These modules work jointly to reduce unnecessary computation while preserving generation fidelity through learnable linear approximation. Theoretical analysis shows that FastCache maintains bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across multiple DiT variants demonstrate substantial reductions in latency and memory usage, with best generation output quality compared to other cache methods, as measured by FID and t-FID. Code implementation of FastCache is available on GitHub at https://github.com/NoakLiu/FastCache-xDiT.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing AI Agents with Information-Flow Control</title>
<link>https://arxiv.org/abs/2505.23643</link>
<guid>https://arxiv.org/abs/2505.23643</guid>
<content:encoded><![CDATA[
arXiv:2505.23643v2 Announce Type: replace-cross 
Abstract: As AI agents become increasingly autonomous and capable, ensuring their security against vulnerabilities such as prompt injection becomes critical. This paper explores the use of information-flow control (IFC) to provide security guarantees for AI agents. We present a formal model to reason about the security and expressiveness of agent planners. Using this model, we characterize the class of properties enforceable by dynamic taint-tracking and construct a taxonomy of tasks to evaluate security and utility trade-offs of planner designs. Informed by this exploration, we present Fides, a planner that tracks confidentiality and integrity labels, deterministically enforces security policies, and introduces novel primitives for selectively hiding information. Its evaluation in AgentDojo demonstrates that this approach enables us to complete a broad range of tasks with security guarantees. A tutorial to walk readers through the the concepts introduced in the paper can be found at https://github.com/microsoft/fides
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Embedding-based Attribution (LEA): Quantifying Source Contributions to Generative Model's Response for Vulnerability Analysis</title>
<link>https://arxiv.org/abs/2506.12100</link>
<guid>https://arxiv.org/abs/2506.12100</guid>
<content:encoded><![CDATA[
arXiv:2506.12100v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly used for cybersecurity threat analysis, but their deployment in security-sensitive environments raises trust and safety concerns. With over 21,000 vulnerabilities disclosed in 2025, manual analysis is infeasible, making scalable and verifiable AI support critical. When querying LLMs, dealing with emerging vulnerabilities is challenging as they have a training cut-off date. While Retrieval-Augmented Generation (RAG) can inject up-to-date context to alleviate the cut-off date limitation, it remains unclear how much LLMs rely on retrieved evidence versus the model's internal knowledge, and whether the retrieved information is meaningful or even correct. This uncertainty could mislead security analysts, mis-prioritize patches, and increase security risks. Therefore, this work proposes LLM Embedding-based Attribution (LEA) to analyze the generated responses for vulnerability exploitation analysis. More specifically, LEA quantifies the relative contribution of internal knowledge vs. retrieved content in the generated responses. We evaluate LEA on 500 critical vulnerabilities disclosed between 2016 and 2025, across three RAG settings -- valid, generic, and incorrect -- using three state-of-the-art LLMs. Our results demonstrate LEA's ability to detect clear distinctions between non-retrieval, generic-retrieval, and valid-retrieval scenarios with over 95% accuracy on larger models. Finally, we demonstrate the limitations posed by incorrect retrieval of vulnerability information and raise a cautionary note to the cybersecurity community regarding the blind reliance on LLMs and RAG for vulnerability analysis. LEA offers security analysts with a metric to audit RAG-enhanced workflows, improving the transparent and trustworthy deployment of AI in cybersecurity threat analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Clustering of Neural Bandits: Selective Reinitialization for Mitigating Loss of Plasticity</title>
<link>https://arxiv.org/abs/2506.12389</link>
<guid>https://arxiv.org/abs/2506.12389</guid>
<content:encoded><![CDATA[
arXiv:2506.12389v2 Announce Type: replace-cross 
Abstract: Clustering of Bandits (CB) methods enhance sequential decision-making by grouping bandits into clusters based on similarity and incorporating cluster-level contextual information, demonstrating effectiveness and adaptability in applications like personalized streaming recommendations. However, when extending CB algorithms to their neural version (commonly referred to as Clustering of Neural Bandits, or CNB), they suffer from loss of plasticity, where neural network parameters become rigid and less adaptable over time, limiting their ability to adapt to non-stationary environments (e.g., dynamic user preferences in recommendation). To address this challenge, we propose Selective Reinitialization (SeRe), a novel bandit learning framework that dynamically preserves the adaptability of CNB algorithms in evolving environments. SeRe leverages a contribution utility metric to identify and selectively reset underutilized units, mitigating loss of plasticity while maintaining stable knowledge retention. Furthermore, when combining SeRe with CNB algorithms, the adaptive change detection mechanism adjusts the reinitialization frequency according to the degree of non-stationarity, ensuring effective adaptation without unnecessary resets. Theoretically, we prove that SeRe enables sublinear cumulative regret in piecewise-stationary environments, outperforming traditional CNB approaches in long-term performances. Extensive experiments on six real-world recommendation datasets demonstrate that SeRe-enhanced CNB algorithms can effectively mitigate the loss of plasticity with lower regrets, improving adaptability and robustness in dynamic settings.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware Learning</title>
<link>https://arxiv.org/abs/2506.13265</link>
<guid>https://arxiv.org/abs/2506.13265</guid>
<content:encoded><![CDATA[
arXiv:2506.13265v3 Announce Type: replace-cross 
Abstract: Autonomous vehicles that navigate in open-world environments may encounter previously unseen object classes. However, most existing LiDAR panoptic segmentation models rely on closed-set assumptions, failing to detect unknown object instances. In this work, we propose ULOPS, an uncertainty-guided open-set panoptic segmentation framework that leverages Dirichlet-based evidential learning to model predictive uncertainty. Our architecture incorporates separate decoders for semantic segmentation with uncertainty estimation, embedding with prototype association, and instance center prediction. During inference, we leverage uncertainty estimates to identify and segment unknown instances. To strengthen the model's ability to differentiate between known and unknown objects, we introduce three uncertainty-driven loss functions. Uniform Evidence Loss to encourage high uncertainty in unknown regions. Adaptive Uncertainty Separation Loss ensures a consistent difference in uncertainty estimates between known and unknown objects at a global scale. Contrastive Uncertainty Loss refines this separation at the fine-grained level. To evaluate open-set performance, we extend benchmark settings on KITTI-360 and introduce a new open-set evaluation for nuScenes. Extensive experiments demonstrate that ULOPS consistently outperforms existing open-set LiDAR panoptic segmentation methods.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Medical Image Binding via Shared Text Embeddings</title>
<link>https://arxiv.org/abs/2506.18072</link>
<guid>https://arxiv.org/abs/2506.18072</guid>
<content:encoded><![CDATA[
arXiv:2506.18072v2 Announce Type: replace-cross 
Abstract: Medical image analysis increasingly relies on the integration of multiple imaging modalities to capture complementary anatomical and functional information, enabling more accurate diagnosis and treatment planning. Achieving aligned feature representations across these diverse modalities is therefore important for effective multimodal analysis. While contrastive language-image pre-training (CLIP) and its variant have enabled image-text alignments, they require explicitly paired data between arbitrary two modalities, which is difficult to acquire in medical contexts. To address the gap, we present Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel pre-training framework that enables seamless alignment of multiple medical imaging modalities through a shared text representation space without requiring explicit paired data between any two medical image modalities. Specifically, based on the insight that different images can naturally bind with text, M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text models to align their modality-specific text embedding space while preserving their original image-text alignments. Subsequently, we distill these modality-specific text encoders into a unified model, creating a shared text embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves state-of-the-art performance in zero-shot, few-shot classification and cross-modal retrieval tasks compared to its CLIP-like counterparts. These results validate M\textsuperscript{3}Bind's effectiveness in achieving cross-image-modal alignment for medical analysis.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization</title>
<link>https://arxiv.org/abs/2506.19992</link>
<guid>https://arxiv.org/abs/2506.19992</guid>
<content:encoded><![CDATA[
arXiv:2506.19992v2 Announce Type: replace-cross 
Abstract: The explosive growth of complex datasets across various modalities necessitates advanced analytical tools that not only group data effectively but also provide human-understandable insights into the discovered structures. We introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization), a novel algorithm and Python package designed for hierarchical k-means clustering of diverse data types, including text, images, and numeric data (processed one modality per run). HERCULES constructs a cluster hierarchy by recursively applying k-means clustering, starting from individual data points at level 0. A key innovation is its deep integration of Large Language Models (LLMs) to generate semantically rich titles and descriptions for clusters at each level of the hierarchy, significantly enhancing interpretability. The algorithm supports two main representation modes: `direct' mode, which clusters based on original data embeddings or scaled numeric features, and `description' mode, which clusters based on embeddings derived from LLM-generated summaries. Users can provide a `topic\_seed' to guide LLM-generated summaries towards specific themes. An interactive visualization tool facilitates thorough analysis and understanding of the clustering results. We demonstrate HERCULES's capabilities and discuss its potential for extracting meaningful, hierarchical knowledge from complex datasets.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</title>
<link>https://arxiv.org/abs/2506.21619</link>
<guid>https://arxiv.org/abs/2506.21619</guid>
<content:encoded><![CDATA[
arXiv:2506.21619v2 Announce Type: replace-cross 
Abstract: Existing autoregressive large-scale text-to-speech (TTS) models have advantages in speech naturalness, but their token-by-token generation mechanism makes it difficult to precisely control the duration of synthesized speech. This becomes a significant limitation in applications requiring strict audio-visual synchronization, such as video dubbing. This paper introduces IndexTTS2, which proposes a novel, general, and autoregressive model-friendly method for speech duration control. The method supports two generation modes: one explicitly specifies the number of generated tokens to precisely control speech duration; the other freely generates speech in an autoregressive manner without specifying the number of tokens, while faithfully reproducing the prosodic features of the input prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional expression and speaker identity, enabling independent control over timbre and emotion. In the zero-shot setting, the model can accurately reconstruct the target timbre (from the timbre prompt) while perfectly reproducing the specified emotional tone (from the style prompt). To enhance speech clarity in highly emotional expressions, we incorporate GPT latent representations and design a novel three-stage training paradigm to improve the stability of the generated speech. Additionally, to lower the barrier for emotional control, we designed a soft instruction mechanism based on text descriptions by fine-tuning Qwen3, effectively guiding the generation of speech with the desired emotional orientation. Finally, experimental results on multiple datasets show that IndexTTS2 outperforms state-of-the-art zero-shot TTS models in terms of word error rate, speaker similarity, and emotional fidelity. Audio samples are available at: https://index-tts.github.io/index-tts2.github.io/
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.23903</link>
<guid>https://arxiv.org/abs/2506.23903</guid>
<content:encoded><![CDATA[
arXiv:2506.23903v2 Announce Type: replace-cross 
Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 to enable object segmentation across multiple ultrasound organs. A total of 18 public ultrasound datasets, encompassing the breast, thyroid, liver, prostate, kidney, and paraspinal muscle, were utilized. These datasets were divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for testing to evaluate performance in unseen distributions. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on most seen datasets while maintaining strong performance on unseen datasets without additional fine-tuning. These results underscore the promise of VLMs in scalable and robust ultrasound image analysis, reducing dependence on large, organ-specific annotated datasets. We will publish our code on code.sonography.ai after acceptance.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling</title>
<link>https://arxiv.org/abs/2507.00790</link>
<guid>https://arxiv.org/abs/2507.00790</guid>
<content:encoded><![CDATA[
arXiv:2507.00790v3 Announce Type: replace-cross 
Abstract: Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data are available at https://github.com/AMAP-ML/LD-RPS.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Data Protection in the (Generative) Artificial Intelligence Era</title>
<link>https://arxiv.org/abs/2507.03034</link>
<guid>https://arxiv.org/abs/2507.03034</guid>
<content:encoded><![CDATA[
arXiv:2507.03034v4 Announce Type: replace-cross 
Abstract: The (generative) artificial intelligence (AI) era has profoundly reshaped the meaning and value of data. No longer confined to static content, data now permeates every stage of the AI lifecycle from the training samples that shape model parameters to the prompts and outputs that drive real-world model deployment. This shift renders traditional notions of data protection insufficient, while the boundaries of what needs safeguarding remain poorly defined. Failing to safeguard data in AI systems can inflict societal and individual, underscoring the urgent need to clearly delineate the scope of and rigorously enforce data protection. In this perspective, we propose a four-level taxonomy, including non-usability, privacy preservation, traceability, and deletability, that captures the diverse protection needs arising in modern (generative) AI models and systems. Our framework offers a structured understanding of the trade-offs between data utility and control, spanning the entire AI pipeline, including training datasets, model weights, system prompts, and AI-generated content. We analyze representative technical approaches at each level and reveal regulatory blind spots that leave critical assets exposed. By offering a structured lens to align future AI technologies and governance with trustworthy data practices, we underscore the urgency of rethinking data protection for modern AI techniques and provide timely guidance for developers, researchers, and regulators alike.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Covering a Few Submodular Constraints and Applications</title>
<link>https://arxiv.org/abs/2507.09879</link>
<guid>https://arxiv.org/abs/2507.09879</guid>
<content:encoded><![CDATA[
arXiv:2507.09879v2 Announce Type: replace-cross 
Abstract: We consider the problem of covering multiple submodular constraints. Given a finite ground set $N$, a cost function $c: N \rightarrow \mathbb{R}_+$, $r$ monotone submodular functions $f_1,f_2,\ldots,f_r$ over $N$ and requirements $b_1,b_2,\ldots,b_r$ the goal is to find a minimum cost subset $S \subseteq N$ such that $f_i(S) \ge b_i$ for $1 \le i \le r$. When $r=1$ this is the well-known Submodular Set Cover problem. Previous work \cite{chekuri2022covering} considered the setting when $r$ is large and developed bi-criteria approximation algorithms, and approximation algorithms for the important special case when each $f_i$ is a weighted coverage function. These are fairly general models and capture several concrete and interesting problems as special cases. The approximation ratios for these problem are at least $\Omega(\log r)$ which is unavoidable when $r$ is part of the input. In this paper, motivated by some recent applications, we consider the problem when $r$ is a \emph{fixed constant} and obtain two main results. For covering multiple submodular constraints we obtain a randomized bi-criteria approximation algorithm that for any given integer $\alpha \ge 1$ outputs a set $S$ such that $f_i(S) \ge$ $(1-1/e^\alpha -\epsilon)b_i$ for each $i \in [r]$ and $\mathbb{E}[c(S)] \le (1+\epsilon)\alpha \cdot \sf{OPT}$. Second, when the $f_i$ are weighted coverage functions from a deletion-closed set system we obtain a $(1+\epsilon)$ $(\frac{e}{e-1})$ $(1+\beta)$-approximation where $\beta$ is the approximation ratio for the underlying set cover instances via the natural LP. These results show that one can obtain nearly as good an approximation for any fixed $r$ as what one would achieve for $r=1$. We mention some applications that follow easily from these general results and anticipate more in the future.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When and Where do Data Poisons Attack Textual Inversion?</title>
<link>https://arxiv.org/abs/2507.10578</link>
<guid>https://arxiv.org/abs/2507.10578</guid>
<content:encoded><![CDATA[
arXiv:2507.10578v4 Announce Type: replace-cross 
Abstract: Poisoning attacks pose significant challenges to the robustness of diffusion models (DMs). In this paper, we systematically analyze when and where poisoning attacks textual inversion (TI), a widely used personalization technique for DMs. We first introduce Semantic Sensitivity Maps, a novel method for visualizing the influence of poisoning on text embeddings. Second, we identify and experimentally verify that DMs exhibit non-uniform learning behavior across timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias and inject adversarial signals predominantly at lower timesteps. Lastly, we observe that adversarial signals distract learning away from relevant concept regions within training data, corrupting the TI process. Based on these insights, we propose Safe-Zone Training (SZT), a novel defense mechanism comprised of 3 key components: (1) JPEG compression to weaken high-frequency poison signals, (2) restriction to high timesteps during TI training to avoid adversarial signals at lower timesteps, and (3) loss masking to constrain learning to relevant regions. Extensive experiments across multiple poisoning methods demonstrate that SZT greatly enhances the robustness of TI against all poisoning attacks, improving generative quality beyond prior published defenses. Code: www.github.com/JStyborski/Diff_Lab Data: www.github.com/JStyborski/NC10
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LanternNet: A Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations</title>
<link>https://arxiv.org/abs/2507.20800</link>
<guid>https://arxiv.org/abs/2507.20800</guid>
<content:encoded><![CDATA[
arXiv:2507.20800v3 Announce Type: replace-cross 
Abstract: The invasive spotted lanternfly (SLF) poses a significant threat to agriculture and ecosystems, causing widespread damage. Current control methods, such as egg scraping, pesticides, and quarantines, prove labor-intensive, environmentally hazardous, and inadequate for long-term SLF suppression. This research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system designed for scalable detection and suppression of SLF populations. A central, tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF identification. Three specialized robotic spokes perform targeted tasks: pest neutralization, environmental monitoring, and navigation/mapping. Field deployment across multiple infested sites over 5 weeks demonstrated LanternNet's efficacy. Quantitative analysis revealed significant reductions (p < 0.01, paired t-tests) in SLF populations and corresponding improvements in tree health indicators across the majority of test sites. Compared to conventional methods, LanternNet offers substantial cost advantages and improved scalability. Furthermore, the system's adaptability for enhanced autonomy and targeting of other invasive species presents significant potential for broader ecological impact. LanternNet demonstrates the transformative potential of integrating robotics and AI for advanced invasive species management and improved environmental outcomes.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems</title>
<link>https://arxiv.org/abs/2508.01415</link>
<guid>https://arxiv.org/abs/2508.01415</guid>
<content:encoded><![CDATA[
arXiv:2508.01415v3 Announce Type: replace-cross 
Abstract: We present RoboMemory, a brain-inspired multi-memory framework for lifelong learning in physical embodied systems, addressing critical challenges in real-world environments: continuous learning, multi-module memory latency, task correlation capture, and infinite-loop mitigation in closed-loop planning. Grounded in cognitive neuroscience, it integrates four core modules: the Information Preprocessor (thalamus-like), the Lifelong Embodied Memory System (hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and the Low-Level Executer (cerebellum-like) to enable long-term planning and cumulative learning. The Lifelong Embodied Memory System, central to the framework, alleviates inference speed issues in complex memory frameworks via parallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic submodules. It incorporates a dynamic Knowledge Graph (KG) and consistent architectural design to enhance memory consistency and scalability. Evaluations on EmbodiedBench show RoboMemory outperforms the open-source baseline (Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the closed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing new SOTA. Ablation studies validate key components (critic, spatial memory, long-term memory), while real-world deployment confirms its lifelong learning capability with significantly improved success rates across repeated tasks. RoboMemory alleviates high latency challenges with scalability, serving as a foundational reference for integrating multi-modal memory systems in physical robots.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning</title>
<link>https://arxiv.org/abs/2508.03700</link>
<guid>https://arxiv.org/abs/2508.03700</guid>
<content:encoded><![CDATA[
arXiv:2508.03700v4 Announce Type: replace-cross 
Abstract: This paper presents MagicGUI, a foundational mobile GUI agent designed to address critical challenges in perception, grounding, and reasoning within real-world mobile GUI environments. The framework is underpinned by following six key components: (1) a comprehensive and accurate dataset, constructed via the scalable GUI Data Pipeline, which aggregates the largest and most diverse GUI-centric multimodal data to date from open-source repositories, automated crawling, and targeted manual annotation; (2) enhanced perception and grounding capabilities, facilitating fine-grained multimodal alignment for UI element referencing, grounding, and screen comprehension; (3) a comprehensive and unified action space, encompassing both fundamental UI operations and complex interactive intents to support human-agent interactions; (4) planning-oriented reasoning mechanisms that enable the model to decompose complex user instructions into sequential actions with explicit intermediate meta-paln reasoning; (5) an iterative two-stage training procedure, combining large-scale continue pre-training on 7.8M samples with reinforcement fine-tuning utilizing a spatially enhanced composite reward and dual filtering strategy; and (6) competitive performance on both the proprietary Magic-RICH benchmark and over a dozen public benchmarks, achieving superior performance across GUI perception and agent tasks, while demonstrating robust generalization and real-world deployment potential in practical mobile GUI scenarios, as detailed in Figure 1.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</title>
<link>https://arxiv.org/abs/2508.08005</link>
<guid>https://arxiv.org/abs/2508.08005</guid>
<content:encoded><![CDATA[
arXiv:2508.08005v2 Announce Type: replace-cross 
Abstract: Extensive experiments and prior studies show that no single maximum clique algorithm consistently performs best across all instances, highlighting the importance of selecting suitable algorithms based on instance features. Through an extensive analysis of relevant studies, it is found that there is a lack of research work concerning algorithm selection oriented toward the Maximum Clique Problem (MCP). In this work, we propose a learning-based framework that integrates both traditional machine learning and graph neural networks to address this gap. We construct a labeled dataset by running four exact MCP algorithms on a diverse collection of graph instances, accompanied by structural and global statistical features extracted from each graph. We first evaluate four conventional classifiers: Support Vector Machine (SVM), Random Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple dataset variants. Experimental results show that RF consistently shows strong performance across metrics and dataset variants, making it a reliable baseline. In addition, feature importance analysis indicates that connectivity and topological structure are strong predictors of algorithm performance. Building on these findings, we develop a dual-channel model named GAT-MLP, which combines a Graph Attention Network (GAT) for local structural encoding with a Multilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model shows strong and consistent performance across all metrics. Our results highlight the effectiveness of dual-channel architectures and the promise of graph neural networks in combinatorial algorithm selection.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models</title>
<link>https://arxiv.org/abs/2508.08040</link>
<guid>https://arxiv.org/abs/2508.08040</guid>
<content:encoded><![CDATA[
arXiv:2508.08040v2 Announce Type: replace-cross 
Abstract: Prompt-based tuning has emerged as a lightweight alternative to full fine-tuning in large vision-language models, enabling efficient adaptation via learned contextual prompts. This paradigm has recently been extended to federated learning settings (e.g., PromptFL), where clients collaboratively train prompts under data privacy constraints. However, the security implications of prompt-based aggregation in federated multimodal learning remain largely unexplored, leaving a critical attack surface unaddressed. In this paper, we introduce \textbf{BadPromptFL}, the first backdoor attack targeting prompt-based federated learning in multimodal contrastive models. In BadPromptFL, compromised clients jointly optimize local backdoor triggers and prompt embeddings, injecting poisoned prompts into the global aggregation process. These prompts are then propagated to benign clients, enabling universal backdoor activation at inference without modifying model parameters. Leveraging the contextual learning behavior of CLIP-style architectures, BadPromptFL achieves high attack success rates (e.g., \(>90\%\)) with minimal visibility and limited client participation. Extensive experiments across multiple datasets and aggregation protocols validate the effectiveness, stealth, and generalizability of our attack, raising critical concerns about the robustness of prompt-based federated learning in real-world deployments.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports</title>
<link>https://arxiv.org/abs/2508.09853</link>
<guid>https://arxiv.org/abs/2508.09853</guid>
<content:encoded><![CDATA[
arXiv:2508.09853v2 Announce Type: replace-cross 
Abstract: Evaluations of dangerous AI capabilities are important for managing catastrophic risks. Public transparency into these evaluations - including what they test, how they are conducted, and how their results inform decisions - is crucial for building trust in AI development. We propose STREAM (A Standard for Transparently Reporting Evaluations in AI Model Reports), a standard to improve how model reports disclose evaluation results, initially focusing on chemical and biological (ChemBio) benchmarks. Developed in consultation with 23 experts across government, civil society, academia, and frontier AI companies, this standard is designed to (1) be a practical resource to help AI developers present evaluation results more clearly, and (2) help third parties identify whether model reports provide sufficient detail to assess the rigor of the ChemBio evaluations. We concretely demonstrate our proposed best practices with "gold standard" examples, and also provide a three-page reporting template to enable AI developers to implement our recommendations more easily.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents</title>
<link>https://arxiv.org/abs/2508.11133</link>
<guid>https://arxiv.org/abs/2508.11133</guid>
<content:encoded><![CDATA[
arXiv:2508.11133v2 Announce Type: replace-cross 
Abstract: Automated agents, powered by Large language models (LLMs), are emerging as the go-to tool for querying information. However, evaluation benchmarks for LLM agents rarely feature natural questions that are both information-seeking and genuinely time-consuming for humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural and time-consuming questions that require dozens, and at times hundreds, of intermediate steps to solve -- far more than any existing QA benchmark. To build MoNaCo, we developed a decomposed annotation pipeline to elicit and manually answer real-world time-consuming questions at scale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and hallucinations. Our results underscore the limitations of LLM-powered agents in handling the complexity and sheer breadth of real-world information-seeking tasks -- with MoNaCo providing an effective resource for tracking such progress. The MoNaCo benchmark, codebase, prompts and models predictions are all publicly available at: https://tomerwolgithub.github.io/monaco
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery</title>
<link>https://arxiv.org/abs/2508.12232</link>
<guid>https://arxiv.org/abs/2508.12232</guid>
<content:encoded><![CDATA[
arXiv:2508.12232v2 Announce Type: replace-cross 
Abstract: Issue-to-commit link recovery plays an important role in software traceability and improves project management. However, it remains a challenging task. A study on GitHub shows that only 42.2% of the issues are correctly linked to their commits. This highlights the potential for further development and research in this area. Existing studies have employed various AI/ML-based approaches, and with the recent development of large language models, researchers have leveraged LLMs to tackle this problem. These approaches suffer from two main issues. First, LLMs are constrained by limited context windows and cannot ingest all of the available data sources, such as long commit histories, extensive issue comments, and large code repositories. Second, most methods operate on individual issue-commit pairs; that is, given a single issue-commit pair, they determine whether the commit resolves the issue. This quickly becomes impractical in real-world repositories containing tens of thousands of commits. To address these limitations, we present LinkAnchor, the first autonomous LLM-based agent designed for issue-to-commit link recovery. The lazy-access architecture of LinkAnchor enables the underlying LLM to access the rich context of software, spanning commits, issue comments, and code files, without exceeding the token limit by dynamically retrieving only the most relevant contextual data. Additionally, LinkAnchor is able to automatically pinpoint the target commit rather than exhaustively scoring every possible candidate. Our evaluations show that LinkAnchor outperforms state-of-the-art issue-to-commit link recovery approaches by 60-262% in Hit@1 score across all our case study projects. We also publicly release LinkAnchor as a ready-to-use tool, along with our replication package. LinkAnchor is designed and tested for GitHub and Jira, and is easily extendable to other platforms.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Murakkab: Resource-Efficient Agentic Workflow Orchestration in Cloud Platforms</title>
<link>https://arxiv.org/abs/2508.18298</link>
<guid>https://arxiv.org/abs/2508.18298</guid>
<content:encoded><![CDATA[
arXiv:2508.18298v2 Announce Type: replace-cross 
Abstract: Agentic workflows commonly coordinate multiple models and tools with complex control logic. They are quickly becoming the dominant paradigm for AI applications. However, serving them remains inefficient with today's frameworks. The key problem is that they expose workflows as opaque sequences of model and tool calls that tightly couple agent logic with model and hardware choices. Often, these workflow components are fragmented across different entities, preventing systems from reasoning about trade-offs across accuracy, latency, energy, and cost. This leads to resource waste and degraded service-level objectives (SLOs).
  We present Murakkab, a resource-efficient serving system for agentic workflows. Murakkab introduces a declarative abstraction that decouples workflow specification from execution configuration. A profile-guided optimizer and adaptive runtime jointly manage the full stack: orchestrating workflow components, mapping them to models and hardware, and dynamically reconfiguring execution to satisfy user-defined SLOs. By exposing the internal structure of agentic workflows, Murakkab enables cross-layer optimization that existing frameworks and cloud schedulers cannot achieve.
  Our evaluation on diverse workflows shows that Murakkab reduces GPU usage by up to 2.8$\times$, energy consumption by 3.7$\times$, and cost by 4.3$\times$ while maintaining SLOs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation</title>
<link>https://arxiv.org/abs/2508.19660</link>
<guid>https://arxiv.org/abs/2508.19660</guid>
<content:encoded><![CDATA[
arXiv:2508.19660v3 Announce Type: replace-cross 
Abstract: Printed electronics offer a promising alternative for applications beyond silicon-based systems, requiring properties like flexibility, stretchability, conformality, and ultra-low fabrication costs. Despite the large feature sizes in printed electronics, printed neural networks have attracted attention for meeting target application requirements, though realizing complex circuits remains challenging. This work bridges the gap between classification accuracy and area efficiency in printed neural networks, covering the entire processing-near-sensor system design and co-optimization from the analog-to-digital interface-a major area and power bottleneck-to the digital classifier. We propose an automated framework for designing printed Ternary Neural Networks with arbitrary input precision, utilizing multi-objective optimization and holistic approximation. Our circuits outperform existing approximate printed neural networks by 17x in area and 59x in power on average, being the first to enable printed-battery-powered operation with under 5% accuracy loss while accounting for analog-to-digital interfacing costs.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Information Dynamics of Generative Diffusion</title>
<link>https://arxiv.org/abs/2508.19897</link>
<guid>https://arxiv.org/abs/2508.19897</guid>
<content:encoded><![CDATA[
arXiv:2508.19897v2 Announce Type: replace-cross 
Abstract: Generative diffusion models have emerged as a powerful class of models in machine learning, yet a unified theoretical understanding of their operation is still developing. This perspective paper provides an integrated perspective on generative diffusion by connecting their dynamic, information-theoretic, and thermodynamic properties under a unified mathematical framework. We demonstrate that the rate of conditional entropy production during generation (i.e. the generative bandwidth) is directly governed by the expected divergence of the score function's vector field. This divergence, in turn, is linked to the branching of trajectories and generative bifurcations, which we characterize as symmetry-breaking phase transitions in the energy landscape. This synthesis offers a powerful insight: the process of generation is fundamentally driven by the controlled, noise-induced breaking of (approximate) symmetries, where peaks in information transfer correspond to critical transitions between possible outcomes. The score function acts as a dynamic non-linear filter that regulates the bandwidth of the noise by suppressing fluctuations that are incompatible with the data.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General agents contain world models</title>
<link>https://arxiv.org/abs/2506.01622</link>
<guid>https://arxiv.org/abs/2506.01622</guid>
<content:encoded><![CDATA[
<div> Keywords: world models, flexible behavior, goal-directed tasks, predictive model, agent performance

Summary: 
Agents capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of their environment. This model can be extracted from the agent's policy, and improving performance or achieving more complex goals requires increasingly accurate world models. The presence of world models is crucial for developing safe and general agents, as well as setting bounds on agent capabilities in complex environments. Additionally, new algorithms can be devised for eliciting world models from agents, enhancing our understanding of their behavior and decision-making processes. World models serve as a necessary ingredient for flexible, goal-directed behavior, highlighting their importance in designing efficient and effective autonomous systems. 

Summary: <div>
arXiv:2506.01622v4 Announce Type: replace 
Abstract: Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA Problem Solving by AWorld</title>
<link>https://arxiv.org/abs/2508.09889</link>
<guid>https://arxiv.org/abs/2508.09889</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-agent system, dynamic maneuvering, system identification, GAIA dataset

Summary:<br />
The article proposes a dynamic Multi-Agent System (MAS) in the AWorld framework to enhance the reliability of intelligent agents leveraging external tools. The MAS consists of an Execution Agent supervised by a Guard Agent that provides on-demand dynamic maneuvering to improve robustness. The architecture is further enhanced with a system identification methodology that creates a "performance fingerprint" of the Execution Agent's weaknesses. This fingerprint is used by the Guard Agent for online profile-aware supervision, allowing targeted interventions based on known failure patterns. Experimental results on the GAIA dataset demonstrate that the profile-aware MAS significantly improves effectiveness and stability, surpassing single-agent systems and naive counterparts. The system attained first place on the GAIA leaderboard, showcasing the importance of understanding each agent's capabilities and limitations for building trustworthy intelligent systems.<br /> <div>
arXiv:2508.09889v4 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) has empowered intelligent agents to leverage diverse external tools for solving complex real-world problems. However, this reliance introduces new challenges, as extended contexts and noisy tool outputs can undermine system reliability. To address this, we propose a dynamic Multi-Agent System (MAS) in our AWorld framework, where an Execution Agent is supervised by a Guard Agent that provides on-demand dynamic maneuvering, verifying and correcting the reasoning process to improve robustness over single-agent systems. To move beyond this generic supervision, we enhance the architecture with a methodology inspired by System Identification from control theory. This method first profiles the Execution Agent offline on a benchmark dataset to create a "performance fingerprint" of its unique weaknesses. The Guard Agent then leverages this fingerprint online to deliver profile-aware supervision, making targeted interventions based on known failure patterns rather than merely reacting to immediate logical flaws. Extensive experiments on the GAIA dataset demonstrate that this profile-aware MAS significantly improves both effectiveness and stability, outperforming not only single-agent systems but also its naive counterpart. This superior performance led our system to achieve first place among open-source projects on the prestigious GAIA leaderboard. These findings highlight that building truly trustworthy intelligent systems requires not just collaboration, but a deep, empirically-grounded understanding of each agent's unique capabilities and limitations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Route-and-Execute: Auditable Model-Card Matching and Specialty-Level Deployment</title>
<link>https://arxiv.org/abs/2508.16839</link>
<guid>https://arxiv.org/abs/2508.16839</guid>
<content:encoded><![CDATA[
<div> keywords: clinical workflows, data-driven model identification, healthcare-first framework, vision-language model, single-model deployment

Summary:
Clinical workflows in healthcare are often fragmented and inefficient, involving multiple scripts and task-specific networks. This study introduces a healthcare-first framework using a single vision-language model (VLM) in two roles. Firstly, the VLM acts as a model-card matcher, routing images to specialist models through a three-stage workflow with checks for prompt exit and candidate selection. Secondly, the VLM is fine-tuned on specialty-specific datasets to cover multiple tasks within each specialty, maintaining performance across various medical fields. Compared to pipelines with task-specific agents, this approach demonstrates that a single VLM can effectively make decisions and perform tasks, reducing the effort of data scientists, increasing transparency in model selection, and simplifying deployment. This framework has the potential to streamline clinical workflows, reduce operational costs, and improve overall efficiency in healthcare settings. 

Summary: <div>
arXiv:2508.16839v3 Announce Type: replace 
Abstract: Clinical workflows are fragmented as a patchwork of scripts and task-specific networks that often handle triage, task selection, and model deployment. These pipelines are rarely streamlined for data science pipeline, reducing efficiency and raising operational costs. Workflows also lack data-driven model identification (from imaging/tabular inputs) and standardized delivery of model outputs. In response, we present a practical, healthcare-first framework that uses a single vision-language model (VLM) in two complementary roles. First (Solution 1), the VLM acts as an aware model-card matcher that routes an incoming image to the appropriate specialist model via a three-stage workflow (modality -> primary abnormality -> model-card id). Checks are provided by (i) stagewise prompts that allow early exit via None/Normal/Other and (ii) a stagewise answer selector that arbitrates between the top-2 candidates at each stage, reducing the chance of an incorrect selection and aligning the workflow with clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on specialty-specific datasets ensuring a single model covers multiple downstream tasks within each specialty, maintaining performance while simplifying deployment. Across gastroenterology, hematology, ophthalmology, and pathology, our single-model deployment matches or approaches specialized baselines.
  Compared with pipelines composed of many task-specific agents, this approach shows that one VLM can both decide and do. It may reduce effort by data scientists, shorten monitoring, increase the transparency of model selection (with per-stage justifications), and lower integration overhead.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ST-Raptor: LLM-Powered Semi-Structured Table Question Answering</title>
<link>https://arxiv.org/abs/2508.18190</link>
<guid>https://arxiv.org/abs/2508.18190</guid>
<content:encoded><![CDATA[
<div> Hierarchical Orthogonal Tree, semi-structured table, question answering, large language models, ST-Raptor <br />
Summary: <br />
ST-Raptor is a tree-based framework designed for semi-structured table question answering using large language models. It introduces the Hierarchical Orthogonal Tree (HO-Tree) to capture complex table layouts and define basic tree operations for guiding language models. The framework decomposes user questions into sub-questions, generates operation pipelines, and aligns operations with table structures for accurate execution. A two-stage verification mechanism ensures answer reliability through forward and backward validation. The method outperforms nine baselines by up to 20% in answer accuracy on the SSTQA dataset of 764 questions across 102 real-world semi-structured tables. The code for ST-Raptor is openly available on GitHub for further exploration and implementation. <div>
arXiv:2508.18190v3 Announce Type: replace 
Abstract: Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hermes 4 Technical Report</title>
<link>https://arxiv.org/abs/2508.18255</link>
<guid>https://arxiv.org/abs/2508.18255</guid>
<content:encoded><![CDATA[
<div> Keywords: hybrid reasoning models, instruction-following ability, data curation, training, evaluation

Summary:
Hermes 4 is a family of hybrid reasoning models that integrate structured, multi-turn reasoning with broad instruction-following capability. The article discusses the challenges faced during data curation, synthesis, training, and evaluation of these models and highlights the solutions implemented at scale. The evaluation covers various domains including mathematical reasoning, coding, knowledge, comprehension, and alignment, with both quantitative performance metrics and qualitative behavioral analysis reported. To promote open research, all model weights are openly accessible. The development of Hermes 4 signifies a significant advancement in the field of hybrid reasoning models, showcasing its effectiveness across diverse benchmarks and research areas. <div>
arXiv:2508.18255v2 Announce Type: replace 
Abstract: We present Hermes 4, a family of hybrid reasoning models that combine structured, multi-turn reasoning with broad instruction-following ability. We describe the challenges encountered during data curation, synthesis, training, and evaluation, and outline the solutions employed to address these challenges at scale. We comprehensively evaluate across mathematical reasoning, coding, knowledge, comprehension, and alignment benchmarks, and we report both quantitative performance and qualitative behavioral analysis. To support open research, all model weights are published publicly at https://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft</title>
<link>https://arxiv.org/abs/2508.18722</link>
<guid>https://arxiv.org/abs/2508.18722</guid>
<content:encoded><![CDATA[
<div> knowledge graph, object detection, multimodal environments, domain-specific training data, embodied decision-making

Summary:
VistaWise introduces a cost-effective agent framework that integrates cross-modal domain knowledge and finetunes an object detection model for visual analysis. By reducing the need for domain-specific training data from millions to a few hundred samples, VistaWise enhances agent performance in open-world tasks while decreasing development costs. The framework incorporates visual information and textual dependencies into a knowledge graph (KG) for a comprehensive understanding of multimodal environments. It also includes a retrieval-based pooling strategy to extract task-related information from the KG and a skill library for direct operation of the Minecraft desktop client. Experimental results show that VistaWise achieves state-of-the-art performance in various tasks, demonstrating its efficacy in improving agent performance while lowering development costs.

<br /><br />Summary: <div>
arXiv:2508.18722v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown significant promise in embodied decision-making tasks within virtual open-world environments. Nonetheless, their performance is hindered by the absence of domain-specific knowledge. Methods that finetune on large-scale domain-specific data entail prohibitive development costs. This paper introduces VistaWise, a cost-effective agent framework that integrates cross-modal domain knowledge and finetunes a dedicated object detection model for visual analysis. It reduces the requirement for domain-specific training data from millions of samples to a few hundred. VistaWise integrates visual information and textual dependencies into a cross-modal knowledge graph (KG), enabling a comprehensive and accurate understanding of multimodal environments. We also equip the agent with a retrieval-based pooling strategy to extract task-related information from the KG, and a desktop-level skill library to support direct operation of the Minecraft desktop client via mouse and keyboard inputs. Experimental results demonstrate that VistaWise achieves state-of-the-art performance across various open-world tasks, highlighting its effectiveness in reducing development costs while enhancing agent performance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark</title>
<link>https://arxiv.org/abs/2508.19005</link>
<guid>https://arxiv.org/abs/2508.19005</guid>
<content:encoded><![CDATA[
<div> Experience-driven Lifelong Learning (ELL), self-evolving agents, continuous growth, real-world interaction, AI advancement 

Summary: Experience-driven Lifelong Learning (ELL) is a framework introduced in this paper for creating self-evolving agents capable of continuous growth through real-world interaction. The framework is based on four core principles: Experience Exploration, Long-term Memory, Skill Learning, and Knowledge Internalization. Agents learn through interaction with dynamic environments, preserve historical knowledge in a memory system, improve by abstracting patterns into skills, and internalize explicit experiences into intuitive capabilities. Additionally, the paper introduces StuLife, a benchmark dataset simulating a student's college journey across various scenarios. StuLife is designed to assess the capabilities of agents in ELL through a holistic view of academic and personal development. <br /><br />Summary: <div>
arXiv:2508.19005v2 Announce Type: replace 
Abstract: As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP</title>
<link>https://arxiv.org/abs/2508.19014</link>
<guid>https://arxiv.org/abs/2508.19014</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, intelligent tutoring systems, difficulty estimation, algebra, adaptive assessment

Summary:<br />
- The study introduces APME, a reinforcement learning-based framework for estimating question difficulty in Intelligent & Autonomous Tutoring Systems (IATS).
- APME uses solver performance data, such as marks obtained and time taken, to estimate difficulty without requiring linguistic features or expert labels.
- The model leverages the inverse coefficient of variation as a risk-adjusted metric, providing an explainable and scalable mechanism for adaptive assessment.
- Empirical validation on three datasets showed the model's robustness, accuracy, and adaptability across different educational levels and assessment formats.
- APME consistently outperformed baseline approaches, particularly in symbolic domains like algebra, highlighting the importance of item heterogeneity and variance in solver outcomes for adaptive allocation.
- Pedagogically, the model aligns with Vygotsky's Zone of Proximal Development by balancing challenge and attainability in tasks to support motivation and minimize disengagement. 

<br /><br />Summary: <div>
arXiv:2508.19014v2 Announce Type: replace 
Abstract: The evolution of technology and education is driving the emergence of Intelligent & Autonomous Tutoring Systems (IATS), where objective and domain-agnostic methods for determining question difficulty are essential. Traditional human labeling is subjective, and existing NLP-based approaches fail in symbolic domains like algebra. This study introduces the Approach of Passive Measures among Educands (APME), a reinforcement learning-based Multi-Armed Bandit (MAB) framework that estimates difficulty solely from solver performance data -- marks obtained and time taken -- without requiring linguistic features or expert labels. By leveraging the inverse coefficient of variation as a risk-adjusted metric, the model provides an explainable and scalable mechanism for adaptive assessment. Empirical validation was conducted on three heterogeneous datasets. Across these diverse contexts, the model achieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its robustness, accuracy, and adaptability to different educational levels and assessment formats. Compared with baseline approaches-such as regression-based, NLP-driven, and IRT models-the proposed framework consistently outperformed alternatives, particularly in purely symbolic domains. The findings highlight that (i) item heterogeneity strongly influences perceived difficulty, and (ii) variance in solver outcomes is as critical as mean performance for adaptive allocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal Development by identifying tasks that balance challenge and attainability, supporting motivation while minimizing disengagement. This domain-agnostic, self-supervised approach advances difficulty tagging in IATS and can be extended beyond algebra wherever solver interaction data is available
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Intents to Conversations: Generating Intent-Driven Dialogues with Contrastive Learning for Multi-Turn Classification</title>
<link>https://arxiv.org/abs/2411.14252</link>
<guid>https://arxiv.org/abs/2411.14252</guid>
<content:encoded><![CDATA[
<div> Keywords: conversational AI, intent classification, dialogue generation, self-play, multilingual

Summary:
The paper introduces the Chain-of-Intent framework, combining Hidden Markov Models (HMMs) with Large Language Models (LLMs) to generate intent-driven dialogues in conversational AI systems. By extracting intent transition patterns from e-commerce chat logs, the model generates context-aware dialogues through self-play. The integration of LLMs with HMMs allows for the creation of natural utterances aligned with predicted intents and dialogue context. Additionally, the paper proposes MINT-CL, a multi-task contrastive learning framework for multi-turn intent classification, improving performance and reducing reliance on large annotated datasets. Empirical results demonstrate superior dialogue generation quality and classification accuracy, especially in multilingual settings. The release of MINT-E, a multilingual intent-aware dialogue corpus from the e-commerce domain, aims to facilitate further research in this area. 

<br /><br />Summary: <div>
arXiv:2411.14252v3 Announce Type: replace-cross 
Abstract: In conversational AI systems, a critical challenge in training effective multi-turn intent classification models lies in the generation of large-scale, domain-specific, multilingual dialogue datasets. In this paper, we introduce Chain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs) with Large Language Models (LLMs) to generate intent-driven, context-aware dialogues through self-play. Our method first extracts domain-specific intent transition patterns from real-world e-commerce chat logs, which guide the modeling of turn-level dynamics and intent sequences. LLMs are then employed to parameterize the emission probabilities of HMMs, enabling the generation of natural, coherent utterances aligned with predicted intents and dialogue context. We also propose MINT-CL, a multi-task contrastive learning framework for multi-turn intent classification, which improves performance while reducing dependence on large-scale annotated datasets. Empirical results demonstrate that our approach outperforms competitive baselines in dialogue generation quality and classification accuracy, particularly in multilingual settings. To facilitate future research, we release MINT-E, a comprehensive, multilingual, intent-aware multi-turn dialogue corpus derived from the e-commerce domain\footnote{The reproduced source code and dataset are available at https://github.com/junhua/chain-of-intent.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Disease Detection from Retinal Fundus Images</title>
<link>https://arxiv.org/abs/2504.08481</link>
<guid>https://arxiv.org/abs/2504.08481</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, convolutional neural networks, vision transformers, interpretable hybrid models, retinal disease detection

Summary:
In the field of medical imaging, the integration of convolutional neural networks (CNNs) and vision transformers (ViTs) has shown promise in enhancing the detection of retinal diseases. A novel hybrid CNN-Transformer architecture has been introduced to combine the local feature extraction of CNNs with the global dependency capture of ViTs. Unlike existing hybrid models, this architecture is designed to be interpretable, providing localized evidence maps that directly reflect the decision-making process. By incorporating both CNN and Transformer components, the model achieves state-of-the-art predictive performance in disease detection tasks using color fundus images. The approach outperforms both black-box and interpretable models, offering class-specific sparse evidence maps in a single forward pass. The availability of the code further enhances the reproducibility and accessibility of this innovative method. <div>
arXiv:2504.08481v4 Announce Type: replace-cross 
Abstract: In many medical imaging tasks, convolutional neural networks (CNNs) efficiently extract local features hierarchically. More recently, vision transformers (ViTs) have gained popularity, using self-attention mechanisms to capture global dependencies, but lacking the inherent spatial localization of convolutions. Therefore, hybrid models combining CNNs and ViTs have been developed to combine the strengths of both architectures. However, such hybrid models are difficult to interpret, which hinders their application in medical imaging. In this work, we introduce an interpretable-by-design hybrid fully convolutional CNN-Transformer architecture for retinal disease detection. Unlike widely used post-hoc saliency methods for ViTs, our approach generates faithful and localized evidence maps that directly reflect the mode's decision process. We evaluated our method on two medical tasks focused on disease detection using color fundus images. Our model achieves state-of-the-art predictive performance compared to black-box and interpretable models and provides class-specific sparse evidence maps in a single forward pass. The code is available at: https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demographic-aware fine-grained classification of pediatric wrist fractures</title>
<link>https://arxiv.org/abs/2507.12964</link>
<guid>https://arxiv.org/abs/2507.12964</guid>
<content:encoded><![CDATA[
<div> fine-grained recognition, computer vision, wrist pathology, metadata integration, X-rays<br />
Summary: 
This study focuses on wrist pathologies in children, utilizing a multifaceted approach involving fine-grained recognition, computer vision, and metadata integration. The research addresses the challenge of limited medical imaging datasets by combining patient metadata with X-rays and leveraging fine-grained weights from a separate dataset. This innovative approach, including fine-grained transformer methods and pre-training, resulted in improved diagnostic accuracy. The study demonstrates a 2% increase in accuracy on a small curated dataset and over 10% increase on a larger fracture dataset. Furthermore, this is the first application of metadata integration for wrist pathology recognition, showcasing the potential of combining data types for enhanced diagnostic outcomes.<br /> <div>
arXiv:2507.12964v4 Announce Type: replace-cross 
Abstract: Wrist pathologies are frequently observed, particularly among children who constitute the majority of fracture cases. Computer vision presents a promising avenue, contingent upon the availability of extensive datasets, a notable challenge in medical imaging. Therefore, reliance solely on one modality, such as images, proves inadequate, especially in an era of diverse and plentiful data types. This study addresses the problem using a multifaceted approach: framing it as a fine-grained recognition task, fusing patient metadata with X-rays, and leveraging weights from a separate fine-grained dataset rather than from a coarse-grained dataset like ImageNet. Unlike prior work, this is the first application of metadata integration for wrist pathology recognition. Our results show that combining fine-grained transformer approach, fine-grained pre-training, and metadata integration improves diagnostic accuracy by 2% on small custom curated dataset and over 10% on a larger fracture dataset.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model</title>
<link>https://arxiv.org/abs/2508.14444</link>
<guid>https://arxiv.org/abs/2508.14444</guid>
<content:encoded><![CDATA[
<div> Transformer architecture, Mamba-2, reasoning workloads, inference speed, Nemotron-Nano-9B-v2
<br />
Summary:<br />
The Nemotron-Nano-9B-v2 is a hybrid language model designed to improve throughput for reasoning tasks while maintaining high accuracy. It replaces self-attention layers with Mamba-2 layers to enhance inference speed for generating reasoning traces. The model, based on the Nemotron-H architecture, was pre-trained with a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base). Through compression and distillation using the Minitron strategy, the model can achieve inference on up to 128k tokens on a single NVIDIA A10G GPU. Compared to similar models like Qwen3-8B, Nemotron-Nano-9B-v2 demonstrates comparable or better accuracy on reasoning tasks with up to 6x higher inference throughput in scenarios with 8k input and 16k output tokens. The models and datasets are made available on Hugging Face for further research and development. 
<br /> <div>
arXiv:2508.14444v4 Announce Type: replace-cross 
Abstract: We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model</title>
<link>https://arxiv.org/abs/2508.16700</link>
<guid>https://arxiv.org/abs/2508.16700</guid>
<content:encoded><![CDATA[
<div> Keywords: GPT-OSS-20B, Mixture-of-Experts, throughput, energy efficiency, peak VRAM <br />
Summary: <br />
The study evaluates the performance of GPT-OSS-20B, a Mixture-of-Experts model with 20.9B parameters, on a single GPU against dense baselines Qwen3-32B and Yi-34B. The evaluation includes measuring true time-to-first-token, full-decode throughput, end-to-end latency percentiles, peak VRAM with past key values, and energy consumption. GPT-OSS-20B shows higher decode throughput and energy efficiency, with lower peak VRAM usage compared to the baselines. Despite a higher time-to-first-token due to MoE routing overhead, GPT-OSS-20B demonstrates superior performance with only 17.3% of parameters active. The model offers higher decode throughput and lower energy consumption per 1000 tokens generated compared to Qwen3-32B, while using less peak VRAM. The study highlights the deployment advantages of Mixture-of-Experts models in terms of efficiency and performance. The evaluation does not consider accuracy, focusing instead on deployment aspects. The code and results are made available for replication and further research. <div>
arXiv:2508.16700v2 Announce Type: replace-cross 
Abstract: We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B (Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines Qwen3-32B and Yi-34B across multiple dimensions. We measure true time-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency percentiles, peak VRAM with past key values (PKV) held, and energy via a consistent nvidia-smi-based sampler. At a 2048-token context with 64-token decode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than dense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM and energy per 1000 generated tokens; its TTFT is higher due to MoE routing overhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B provides about 31.8% higher decode throughput and 25.8% lower energy per 1000 generated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM. Normalized by active parameters, GPT-OSS-20B shows markedly stronger per-active-parameter efficiency (APE), underscoring MoE's deployment advantages. We do not evaluate accuracy; this is a deployment-focused study. We release code and consolidated results to enable replication and extension.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tri-Accel: Curvature-Aware Precision-Adaptive and Memory-Elastic Optimization for Efficient GPU Usage</title>
<link>https://arxiv.org/abs/2508.16905</link>
<guid>https://arxiv.org/abs/2508.16905</guid>
<content:encoded><![CDATA[
<div> optimization, deep neural networks, acceleration techniques, mixed precision, batch size scaling

Summary:
Tri-Accel is a unified optimization framework that combines three acceleration strategies for deep neural networks. It dynamically assigns mixed-precision levels to layers, exploits sparsity patterns for second-order signals, and adjusts batch size in real time based on GPU memory availability. On CIFAR-10, Tri-Accel reduces training time by up to 9.9% and memory usage by 13.3%, while improving accuracy by +1.1 percentage points over FP32 baselines. It demonstrates adaptive learning behavior on CIFAR-10/100, gradually improving efficiency as training progresses. Compared to static mixed-precision training, Tri-Accel maintains 78.1% accuracy while reducing memory footprint on standard hardware. The framework, implemented with custom Triton kernels, enables automatic optimization without manual hyperparameter tuning, making it practical for diverse computational environments. This work shows how algorithmic adaptivity and hardware awareness can enhance scalability in resource-constrained settings, leading to more efficient neural network training for edge devices and cost-sensitive cloud deployments. 

<br /><br />Summary: <div>
arXiv:2508.16905v2 Announce Type: replace-cross 
Abstract: Deep neural networks are increasingly bottlenecked by the cost of optimization, both in terms of GPU memory and compute time. Existing acceleration techniques, such as mixed precision, second-order methods, and batch size scaling, are typically used in isolation. We present Tri-Accel, a unified optimization framework that co-adapts three acceleration strategies along with adaptive parameters during training: (1) Precision-Adaptive Updates that dynamically assign mixed-precision levels to layers based on curvature and gradient variance; (2) Sparse Second-Order Signals that exploit Hessian/Fisher sparsity patterns to guide precision and step size decisions; and (3) Memory-Elastic Batch Scaling that adjusts batch size in real time according to VRAM availability. On CIFAR-10 with ResNet-18 and EfficientNet-B0, Tri-Accel achieves up to 9.9% reduction in training time and 13.3% lower memory usage, while improving accuracy by +1.1 percentage points over FP32 baselines. Tested on CIFAR-10/100, our approach demonstrates adaptive learning behavior, with efficiency gradually improving over the course of training as the system learns to allocate resources more effectively. Compared to static mixed-precision training, Tri-Accel maintains 78.1% accuracy while reducing memory footprint from 0.35GB to 0.31GB on standard hardware. The framework is implemented with custom Triton kernels, whose hardware-aware adaptation enables automatic optimization without manual hyperparameter tuning, making it practical for deployment across diverse computational environments. This work demonstrates how algorithmic adaptivity and hardware awareness can be combined to improve scalability in resource-constrained settings, paving the way for more efficient neural network training on edge devices and cost-sensitive cloud deployments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ONG: Orthogonal Natural Gradient Descent</title>
<link>https://arxiv.org/abs/2508.17169</link>
<guid>https://arxiv.org/abs/2508.17169</guid>
<content:encoded><![CDATA[
<div> Keywords: Orthogonal Gradient Descent, Natural Gradient, Continual Learning, EKFAC, Riemannian Metric

Summary:
Orthogonal Gradient Descent (OGD) has proven effective for continual learning but lacks efficiency in leveraging information-geometric structures. To address this, the Orthogonal Natural Gradient Descent (ONG) method is proposed, which incorporates the natural gradient and a Riemannian metric. ONG preconditions task-specific gradients using an EKFAC approximation of the inverse Fisher information matrix. To maintain performance on previous tasks, ONG projects natural gradients orthogonal to prior task gradients. Initial theoretical justification and preliminary results on benchmark datasets are provided. However, challenges in the combination of natural gradients and orthogonal projections highlight the need for further research to reconcile geometric perspectives and enhance method robustness. Future work aims to establish formal convergence guarantees, validate on larger-scale benchmarks, and refine the method. The anonymized code for the study is available for exploration. 

<br /><br />Summary: <div>
arXiv:2508.17169v2 Announce Type: replace-cross 
Abstract: Orthogonal Gradient Descent (OGD) has emerged as a powerful method for continual learning. However, its Euclidean projections do not leverage the underlying information-geometric structure of the problem, which can lead to suboptimal convergence in learning tasks. To address this, we propose incorporating the natural gradient into OGD and present \textbf{ONG (Orthogonal Natural Gradient Descent)}. ONG preconditions each new task-specific gradient with an efficient EKFAC approximation of the inverse Fisher information matrix, yielding updates that follow the steepest descent direction under a Riemannian metric. To preserve performance on previously learned tasks, ONG projects these natural gradients onto the orthogonal complement of prior tasks' gradients. We provide an initial theoretical justification for this procedure, introduce the Orthogonal Natural Gradient Descent (ONG) algorithm, and present preliminary results on the Permuted and Rotated MNIST benchmarks. Our preliminary results, however, indicate that a naive combination of natural gradients and orthogonal projections can have potential issues. This finding motivates continued future work focused on robustly reconciling these geometric perspectives to develop a continual learning method, establishing a more rigorous theoretical foundation with formal convergence guarantees, and extending empirical validation to large-scale continual learning benchmarks. The anonymized version of our code can be found as the zip file here: https://drive.google.com/drive/folders/11PyU6M8pNgOUB5pwdGORtbnMtD8Shiw_?usp=sharing.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components</title>
<link>https://arxiv.org/abs/2508.17182</link>
<guid>https://arxiv.org/abs/2508.17182</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, overconfidence, assertiveness, mechanistic interpretability, dual-route Elaboration Likelihood Model

Summary:
Large Language Models (LLMs) often exhibit overconfidence, presenting information with unwarranted certainty in critical situations. This study delves into the internal mechanisms driving this behavior through mechanistic interpretability. By analyzing Llama 3.2 models fine-tuned on assertiveness datasets, the researchers extract residual activations and identify layers sensitive to assertiveness contrasts. They reveal that high-assertive representations in LLMs consist of emotional and logical sub-components, akin to the dual-route Elaboration Likelihood Model in Psychology. Steering vectors derived from these sub-components demonstrate distinct effects, with emotional vectors broadly influencing prediction accuracy and logical vectors exerting more localized effects. This mechanistic insight sheds light on the multi-component nature of LLM assertiveness and suggests potential strategies for mitigating overconfidence in LLM behavior. 

<br /><br />Summary: <div>
arXiv:2508.17182v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) often display overconfidence, presenting information with unwarranted certainty in high-stakes contexts. We investigate the internal basis of this behavior via mechanistic interpretability. Using open-sourced Llama 3.2 models fine-tuned on human annotated assertiveness datasets, we extract residual activations across all layers, and compute similarity metrics to localize assertive representations. Our analysis identifies layers most sensitive to assertiveness contrasts and reveals that high-assertive representations decompose into two orthogonal sub-components of emotional and logical clusters-paralleling the dual-route Elaboration Likelihood Model in Psychology. Steering vectors derived from these sub-components show distinct causal effects: emotional vectors broadly influence prediction accuracy, while logical vectors exert more localized effects. These findings provide mechanistic evidence for the multi-component structure of LLM assertiveness and highlight avenues for mitigating overconfident behavior.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.17243</link>
<guid>https://arxiv.org/abs/2508.17243</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision-Language Models, token pruning, computational efficiency, multimodal inputs, CoViPAL <br />
Summary: 
The article discusses the challenges faced by Large Vision-Language Models (LVLMs) in processing multimodal inputs, particularly in handling the high computational costs and memory overhead associated with large numbers of vision tokens extracted from images or videos. Existing methods have struggled to effectively prune redundant vision tokens, especially in shallow layers where contextual information is lacking. The proposed CoViPAL approach introduces a Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision tokens before they are processed by the LVLM, improving computational efficiency without compromising accuracy. CoViPAL is lightweight, model-agnostic, and can seamlessly integrate with various LVLM architectures. Extensive experiments demonstrate that CoViPAL outperforms both training-free and training-based pruning methods under equal token budgets, offering a scalable and efficient solution for enhancing inference efficiency in LVLMs. <br /><br /> <div>
arXiv:2508.17243v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) process multimodal inputs consisting of text tokens and vision tokens extracted from images or videos. Due to the rich visual information, a single image can generate thousands of vision tokens, leading to high computational costs during the prefilling stage and significant memory overhead during decoding. Existing methods attempt to prune redundant vision tokens, revealing substantial redundancy in visual representations. However, these methods often struggle in shallow layers due to the lack of sufficient contextual information. We argue that many visual tokens are inherently redundant even in shallow layers and can be safely and effectively pruned with appropriate contextual signals. In this work, we propose CoViPAL, a layer-wise contextualized visual token pruning method that employs a Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision tokens before they are processed by the LVLM. The PPM is lightweight, model-agnostic, and operates independently of the LVLM architecture, ensuring seamless integration with various models. Extensive experiments on multiple benchmarks demonstrate that CoViPAL outperforms training-free pruning methods under equal token budgets and surpasses training-based methods with comparable supervision. CoViPAL offers a scalable and efficient solution to improve inference efficiency in LVLMs without compromising accuracy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI for Software: thoughts from Software Engineering community</title>
<link>https://arxiv.org/abs/2508.17343</link>
<guid>https://arxiv.org/abs/2508.17343</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, software engineering, code generation, intent inference, verification and validation

Summary:
AI agents in software engineering extend beyond code generation, encompassing tasks like testing, program repair, architecture exploration, and requirements understanding. A key challenge lies in deciphering developer intent, essential for tasks like software maintenance. Trust in AI agents is crucial as automation increases, leading to higher volumes of code integration. The future of software workflows may involve AI-based verification and validation for code generated by AI agents. Conceptual progress in intent inference is essential for deploying agentic technology successfully in software engineering. Trustworthy agentic AI software workflows depend on resolving the core challenge of interpreting developer intent, enabling autonomous decision-making by AI agents. AI-based V&amp;V is an emerging direction in handling the proliferation of automatically generated code, indicating a future where AI plays a crucial role in ensuring the reliability and quality of code bases. 

<br /><br />Summary: <div>
arXiv:2508.17343v3 Announce Type: replace-cross 
Abstract: AI agents have recently shown significant promise in software engineering. Much public attention has been transfixed on the topic of code generation from Large Language Models (LLMs) via a prompt. However, software engineering is much more than programming, and AI agents go far beyond instructions given by a prompt.
  At the code level, common software tasks include code generation, testing, and program repair. Design level software tasks may include architecture exploration, requirements understanding, and requirements enforcement at the code level. Each of these software tasks involves micro-decisions which can be taken autonomously by an AI agent, aided by program analysis tools. This creates the vision of an AI software engineer, where the AI agent can be seen as a member of a development team.
  Conceptually, the key to successfully developing trustworthy agentic AI-based software workflows will be to resolve the core difficulty in software engineering - the deciphering and clarification of developer intent. Specification inference, or deciphering the intent, thus lies at the heart of many software tasks, including software maintenance and program repair. A successful deployment of agentic technology into software engineering would involve making conceptual progress in such intent inference via agents.
  Trusting the AI agent becomes a key aspect, as software engineering becomes more automated. Higher automation also leads to higher volume of code being automatically generated, and then integrated into code-bases. Thus to deal with this explosion, an emerging direction is AI-based verification and validation (V & V) of AI generated code. We posit that agentic software workflows in future will include such AIbased V&amp;V.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model</title>
<link>https://arxiv.org/abs/2508.18579</link>
<guid>https://arxiv.org/abs/2508.18579</guid>
<content:encoded><![CDATA[
<div> DrugReasoner, large language model, drug approval prediction, interpretability, reasoning <br />
Summary: Drug discovery is a complex process, and predicting drug approval outcomes early is essential for efficient resource allocation. Traditional machine learning and deep learning models lack interpretability, limiting their impact. DrugReasoner, a reasoning-based large language model, integrates molecular descriptors and comparative reasoning to predict small-molecule approval likelihood. It outperformed baseline models, including logistic regression and support vector machines, achieving competitive performance with XGBoost. DrugReasoner also outperformed the ChemAP model on an external dataset, demonstrating robustness in real-world scenarios. Not only does DrugReasoner deliver accurate predictions, but it also enhances transparency through reasoning outputs, addressing a key bottleneck in AI-assisted drug discovery. <div>
arXiv:2508.18579v2 Announce Type: replace-cross 
Abstract: Drug discovery is a complex and resource-intensive process, making early prediction of approval outcomes critical for optimizing research investments. While classical machine learning and deep learning methods have shown promise in drug approval prediction, their limited interpretability constraints their impact. Here, we present DrugReasoner, a reasoning-based large language model (LLM) built on the LLaMA architecture and fine-tuned with group relative policy optimization (GRPO) to predict the likelihood of small-molecule approval. DrugReasoner integrates molecular descriptors with comparative reasoning against structurally similar approved and unapproved compounds, generating predictions alongside step-by-step rationales and confidence scores. DrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score of 0.729 on the validation set and 0.725 and 0.718 on the test set, respectively. These results outperformed conventional baselines, including logistic regression, support vector machine, and k-nearest neighbors and had competitive performance relative to XGBoost. On an external independent dataset, DrugReasoner outperformed both baseline and the recently developed ChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while maintaining high precision and balanced sensitivity, demonstrating robustness in real-world scenarios. These findings demonstrate that DrugReasoner not only delivers competitive predictive accuracy but also enhances transparency through its reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug discovery. This study highlights the potential of reasoning-augmented LLMs as interpretable and effective tools for pharmaceutical decision-making.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Controllability, Explainability, and Performance in Dysfluency Detection Models</title>
<link>https://arxiv.org/abs/2509.00058</link>
<guid>https://arxiv.org/abs/2509.00058</guid>
<content:encoded><![CDATA[
<div> Keywords: dysfluency detection, modeling paradigms, controllability, explainability, clinical adoption

Summary:
This paper presents a comparative analysis of four dysfluency detection approaches, namely YOLO-Stutter, FluentNet, UDM, and SSDM, focusing on performance, controllability, and explainability. The evaluation on multiple datasets and expert clinician assessment reveals that YOLO-Stutter and FluentNet offer efficiency but have limited transparency, while UDM strikes a balance between accuracy and clinical interpretability. SSDM shows promise but could not be fully reproduced in the experiments. The analysis underscores the trade-offs among the approaches and suggests future directions for dysfluency modeling for clinical use. Implementation insights and practical deployment considerations for each approach are also provided.

<br /><br />Summary: <div>
arXiv:2509.00058v1 Announce Type: new 
Abstract: Recent advances in dysfluency detection have introduced a variety of modeling paradigms, ranging from lightweight object-detection inspired networks (YOLOStutter) to modular interpretable frameworks (UDM). While performance on benchmark datasets continues to improve, clinical adoption requires more than accuracy: models must be controllable and explainable. In this paper, we present a systematic comparative analysis of four representative approaches--YOLO-Stutter, FluentNet, UDM, and SSDM--along three dimensions: performance, controllability, and explainability. Through comprehensive evaluation on multiple datasets and expert clinician assessment, we find that YOLO-Stutter and FluentNet provide efficiency and simplicity, but with limited transparency; UDM achieves the best balance of accuracy and clinical interpretability; and SSDM, while promising, could not be fully reproduced in our experiments. Our analysis highlights the trade-offs among competing approaches and identifies future directions for clinically viable dysfluency modeling. We also provide detailed implementation insights and practical deployment considerations for each approach.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy Against Benchmark Contamination</title>
<link>https://arxiv.org/abs/2509.00072</link>
<guid>https://arxiv.org/abs/2509.00072</guid>
<content:encoded><![CDATA[
<div> Framework, large language models, knowledge cutoff dates, multi-step reasoning, benchmark contamination<br />
<br />
Summary: 
This study evaluates the capabilities of large language models (LLMs) using a framework that synthesizes research-level questions directly from arXiv papers. The study considers models with different knowledge cutoff dates and sizes, finding no significant performance decay near the cutoff dates. The authors compare their results with previous studies that reported post-cutoff performance decay, suggesting that the multi-step reasoning required by their synthesis pipeline may mitigate benchmark contamination. The study advocates for prioritizing reasoning-driven synthesis to construct benchmarks over collecting new questions periodically. Overall, the results indicate that LLMs can handle complex reasoning tasks without succumbing to memorization, highlighting the importance of thoughtful benchmark construction in evaluating model capabilities. <br /><br />Summary: <div>
arXiv:2509.00072v1 Announce Type: new 
Abstract: Capability evaluation of large language models (LLMs) is increasingly shadowed by rising concerns of data contamination that cast doubts on whether static benchmarks measure genuine reasoning or mere memorization. We present an empirical study using an infinitely scalable framework to synthesize research-level QA directly from arXiv papers, harnessing the natural temporal structure of research publications where performance decay after knowledge cutoffs may indicate potential contamination. We evaluated 4 frontier model represented by 2 models of different knowledge cutoff dates per family on 1,643 multi-step reasoning questions synthesized from 20,277 arXiv papers stratified over 26 months, covering at least 6 months before and after all cutoff dates. Our results consistently showed a lack of significant performance decay near knowledge cutoff dates for models of various sizes, developers, and release dates. We further performed a comparative analysis with previous longitudinal studies that reported significant post-cutoff performance decay using directly retrieved questions based on public data. we hypothesize that the multi-step reasoning required by our synthesis pipeline offered additional complexity that goes deeper than shallow memorization, which effectively serves a mitigation strategy against benchmark contamination. We fully open source our code and dataset to aid reproducibility and advocate for a paradigm shift that prioritize reasoning-driven synthesis to construct benchmarks over simply collecting newly released questions periodically.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language and Experience: A Computational Model of Social Learning in Complex Tasks</title>
<link>https://arxiv.org/abs/2509.00074</link>
<guid>https://arxiv.org/abs/2509.00074</guid>
<content:encoded><![CDATA[
<div> framework, social learning, structured world models, AI systems, linguistic guidance
Summary:
The article presents a computational framework for modeling social learning through joint probabilistic inference over structured world models using sensorimotor and linguistic data. By leveraging a pretrained language model as a probabilistic model of how humans share advice, agents can generate and interpret advice for learning and exploration. Linguistic guidance is shown to aid in shaping exploration, accelerating learning, reducing risky interactions, and speeding up discoveries in both humans and AI models. The study also examines how knowledge can accumulate through iterated learning experiments and successful knowledge transfer between humans and AI models. The research highlights the potential of structured, language-compatible representations in facilitating human-machine collaborative learning. <div>
arXiv:2509.00074v1 Announce Type: new 
Abstract: The ability to combine linguistic guidance from others with direct experience is central to human development, enabling safe and rapid learning in new environments. How do people integrate these two sources of knowledge, and how might AI systems? We present a computational framework that models social learning as joint probabilistic inference over structured, executable world models given sensorimotor and linguistic data. We make this possible by turning a pretrained language model into a probabilistic model of how humans share advice conditioned on their beliefs, allowing our agents both to generate advice for others and to interpret linguistic input as evidence during Bayesian inference. Using behavioral experiments and simulations across 10 video games, we show how linguistic guidance can shape exploration and accelerate learning by reducing risky interactions and speeding up key discoveries in both humans and models. We further explore how knowledge can accumulate across generations through iterated learning experiments and demonstrate successful knowledge transfer between humans and models -- revealing how structured, language-compatible representations might enable human-machine collaborative learning.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation</title>
<link>https://arxiv.org/abs/2509.00079</link>
<guid>https://arxiv.org/abs/2509.00079</guid>
<content:encoded><![CDATA[
<div> uncertainty, refinement, reasoning, lightweight, cost-efficiency 
Summary: 
In the study presented, a method called entropy-guided refinement is proposed to improve the performance of reasoning models while reducing cost and latency. This lightweight test-time loop utilizes token-level uncertainty to trigger a targeted refinement pass, achieving 95% accuracy compared to a reference model at one-third of the cost. The approach involves extracting log probabilities, calculating Shannon entropy, and using a simple trigger based on perplexity, token entropy, and low-confidence tokens to guide corrective edits. By passing an uncertainty report back to the model, selective refinement is applied to approximately 31% of responses, resulting in a 16-percentage point accuracy improvement over single-pass inference. This method offers a practical middle ground between single-pass inference and resource-intensive reasoning chains, making it suitable for production deployments where balancing quality and cost is essential. 
<br /><br />Summary: <div>
arXiv:2509.00079v1 Announce Type: new 
Abstract: Reasoning models often outperform smaller models but at 3--5$\times$ higher cost and added latency. We present entropy-guided refinement: a lightweight, test-time loop that uses token-level uncertainty to trigger a single, targeted refinement pass. We extract logprobs, compute Shannon entropy on top-$k$ alternatives, and apply a simple OR-logic trigger over perplexity, maximum token entropy, and low-confidence-token count. Unlike approaches that use entropy only for measurement or decoding, we pass a compact uncertainty report (tokens, confidences, alternatives, context) back to the model to guide corrective edits. On representative technical queries across reasoning, mathematics, and code generation tasks, a small model with our loop approaches 95\% of a reference reasoning model's quality at approximately one-third of the cost. The method achieves selective refinement on ~31\% of responses while improving accuracy by 16 percentage points over single-pass inference. We demonstrate that this uncertainty-aware loop provides an effective middle ground between single-pass inference and expensive reasoning chains, making it practical for production deployments where both quality and cost matter.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wrong Face, Wrong Move: The Social Dynamics of Emotion Misperception in Agent-Based Models</title>
<link>https://arxiv.org/abs/2509.00080</link>
<guid>https://arxiv.org/abs/2509.00080</guid>
<content:encoded><![CDATA[
<div> classifier, emotion, social behavior, perception, accuracy
Summary:
- The study explores the impact of varying accuracy in emotion classifiers on social behavior.
- Agents with low-accuracy classifiers exhibit diminished trust, sadness, and disordered social organization.
- Agents with high-accuracy classifiers develop emotional resilience and cohesion.
- Even in emotionally neutral scenarios, misperception leads to segregation and disintegration of cohesion.
- Biases or imprecision in emotion recognition can significantly alter social processes and disrupt emotional integration.<br /><br />Summary: <div>
arXiv:2509.00080v1 Announce Type: new 
Abstract: The ability of humans to detect and respond to others' emotions is fundamental to understanding social behavior. Here, agents are instantiated with emotion classifiers of varying accuracy to study the impact of perceptual accuracy on emergent emotional and spatial behavior. Agents are visually represented with face photos from the KDEF database and endowed with one of three classifiers trained on the JAFFE (poor), CK+ (medium), or KDEF (high) datasets. Agents communicate locally on a 2D toroidal lattice, perceiving neighbors' emotional state based on their classifier and responding with movement toward perceived positive emotions and away from perceived negative emotions. Note that the agents respond to perceived, instead of ground-truth, emotions, introducing systematic misperception and frustration. A battery of experiments is carried out on homogeneous and heterogeneous populations and scenarios with repeated emotional shocks. Results show that low-accuracy classifiers on the part of the agent reliably result in diminished trust, emotional disintegration into sadness, and disordered social organization. By contrast, the agent that develops high accuracy develops hardy emotional clusters and resilience to emotional disruptions. Even in emotionally neutral scenarios, misperception is enough to generate segregation and disintegration of cohesion. These findings underscore the fact that biases or imprecision in emotion recognition may significantly warp social processes and disrupt emotional integration.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Debates with Local Large Language Models for AI Alignment</title>
<link>https://arxiv.org/abs/2509.00091</link>
<guid>https://arxiv.org/abs/2509.00091</guid>
<content:encoded><![CDATA[
<div> ensemble, debates, language models, alignment, reproducibility <br />
Summary:<br />
In the study, the researchers examined the use of local open-source ensemble debates to enhance alignment-oriented reasoning in large language models. They conducted 150 debates across 15 scenarios and five ensemble configurations, finding that ensembles outperformed single-model baselines on various criteria. The ensembles showed significant improvements in reasoning depth and argument quality, with the largest gains in truthfulness and human enhancement. The findings suggest that ensemble-based approaches can enhance alignment with human values in high-stakes decision-making scenarios. The researchers provided code, prompts, and a debate dataset for reproducibility, offering a foundation for further evaluation of alignment in language models. <br />Summary: <div>
arXiv:2509.00091v1 Announce Type: new 
Abstract: As large language models (LLMs) take on greater roles in high-stakes decisions, alignment with human values is essential. Reliance on proprietary APIs limits reproducibility and broad participation. We study whether local open-source ensemble debates can improve alignmentoriented reasoning. Across 150 debates spanning 15 scenarios and five ensemble configurations, ensembles outperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13), with the largest gains in reasoning depth (+19.4%) and argument quality (+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human enhancement (+0.80). We provide code, prompts, and a debate data set, providing an accessible and reproducible foundation for ensemble-based alignment evaluation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MODE: Mixture of Document Experts for RAG</title>
<link>https://arxiv.org/abs/2509.00100</link>
<guid>https://arxiv.org/abs/2509.00100</guid>
<content:encoded><![CDATA[
<div> MODE, Retrieval-Augmented Generation, lightweight, cluster-and-route retrieval, semantically coherent clusters <br />
<br />Summary: MODE is a lightweight alternative to Retrieval-Augmented Generation (RAG) for small, domain-specific collections. It replaces fine-grained nearest-neighbor search with cluster-and-route retrieval, using cached centroids to eliminate the need for large vector databases. By grouping documents into semantically coherent clusters and routing queries to relevant clusters, MODE matches or exceeds dense-retrieval baselines in answer quality while reducing retrieval time. The cluster granularity and multi-cluster routing in MODE allow for controlling the recall/precision trade-off, with tighter clusters leading to improved downstream accuracy. This approach offers a practical solution for small to medium corpora where simplicity, speed, and topical focus are important considerations. <br /> <div>
arXiv:2509.00100v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) often relies on large vector databases and cross-encoders tuned for large-scale corpora, which can be excessive for small, domain-specific collections. We present MODE (Mixture of Document Experts), a lightweight alternative that replaces fine-grained nearest-neighbor search with cluster-and-route retrieval. Documents are embedded, grouped into semantically coherent clusters, and represented by cached centroids. At query time, we route to the top centroid(s) and retrieve context only within those clusters, eliminating external vector-database infrastructure and reranking while keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks, MODE matches or exceeds a dense-retrieval baseline in answer quality while reducing end-to-end retrieval time. Ablations show that cluster granularity and multi-cluster routing control the recall/precision trade-off, and that tighter clusters improve downstream accuracy. MODE offers a practical recipe for small and medium corpora where simplicity, speed, and topical focus matter.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems</title>
<link>https://arxiv.org/abs/2509.00115</link>
<guid>https://arxiv.org/abs/2509.00115</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic artificial intelligence, Adaptive Multi-Dimensional Monitoring, goal drift, anomaly detection, metrics 

Summary: 
The paper discusses the transition of agentic artificial intelligence (AI) from research to real-world applications and the importance of considering human-centered and economic metrics. It introduces the Adaptive Multi-Dimensional Monitoring (AMDM) algorithm for normalizing metrics, applying thresholds, and performing anomaly detection. Through simulations and experiments, AMDM significantly reduces anomaly-detection latency and false-positive rates compared to static thresholds. The paper includes a comparison table, ROC/PR curves, and highlights the need for additional metrics in case studies. Code, data, and a reproducibility checklist are provided to facilitate replication.<br /><br />Summary: <div>
arXiv:2509.00115v1 Announce Type: new 
Abstract: Agentic artificial intelligence (AI) -- multi-agent systems that combine large language models with external tools and autonomous planning -- are rapidly transitioning from research laboratories into high-stakes domains. Our earlier "Basic" paper introduced a five-axis framework and proposed preliminary metrics such as goal drift and harm reduction but did not provide an algorithmic instantiation or empirical evidence. This "Advanced" sequel fills that gap. First, we revisit recent benchmarks and industrial deployments to show that technical metrics still dominate evaluations: a systematic review of 84 papers from 2023--2025 found that 83% report capability metrics while only 30% consider human-centred or economic axes [2]. Second, we formalise an Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises heterogeneous metrics, applies per-axis exponentially weighted moving-average thresholds and performs joint anomaly detection via the Mahalanobis distance. Third, we conduct simulations and real-world experiments. AMDM cuts anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and reduces false-positive rates from 4.5% to 0.9% compared with static thresholds. We present a comparison table and ROC/PR curves, and we reanalyse case studies to surface missing metrics. Code, data and a reproducibility checklist accompany this paper to facilitate replication.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.00125</link>
<guid>https://arxiv.org/abs/2509.00125</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, verifiable feedback, large language models, difficulty aware certainty, exploration

Summary:<br />
Reinforcement Learning with Verifiable Feedback (RLVF) enhances Large Language Models (LLMs) reasoning by providing sparse rewards based on final answer correctness. However, it lacks granular guidance on the reasoning process itself. Difficulty Aware Certainty guided Exploration (DACE) addresses this by using LLMs' self-certainty to assess task difficulty and modulate exploration. By penalizing high certainty in difficult tasks and rewarding it in easier tasks, DACE balances exploration-exploitation trade-off effectively. Experimental results on mathematical reasoning benchmarks show DACE outperforms baselines, achieving higher accuracy and robust performance with scaled test-time compute. This adaptive approach fosters effective exploration without sacrificing precision.<br /> <div>
arXiv:2509.00125v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Feedback (RLVF) has become a key technique for enhancing the reasoning abilities of Large Language Models (LLMs). However, its reliance on sparse, outcome based rewards, which only indicate if a final answer is correct or not, fails to provide granular guidance on the reasoning process itself. This limitation hinders efficient learning, as the model cannot distinguish between high quality and inefficient solutions, nor can it learn effectively from different types of failures. To address this, we observe that an LLMs self-certainty often correlates with task difficulty and solution quality. We introduce Difficulty Aware Certainty guided Exploration (DACE), a novel RL algorithm that leverages this insight to dynamically balance the exploration exploitation trade-off. DACE assesses task difficulty online based on the policys success rate. It then uses this signal to modulate an intrinsic reward: for difficult tasks where the model is struggling, DACE encourages exploration by penalizing high certainty; for easier tasks, it encourages learning efficiency by rewarding high certainty. Experiments on challenging mathematical reasoning benchmarks (AIME, MATH) show that DACE significantly outperforms strong baselines. The DACE-trained models not only achieve higher accuracy but also demonstrate more robust performance when scaling test-time compute, validating that our adaptive approach fosters effective exploration without sacrificing precision.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Health Coverage in Ethiopia: A Learning-augmented Approach and Persistent Proportionality Under an Online Budget</title>
<link>https://arxiv.org/abs/2509.00135</link>
<guid>https://arxiv.org/abs/2509.00135</guid>
<content:encoded><![CDATA[
<div> Health Access Resource Planner, Ethiopia, Universal Health Coverage, optimization framework, health system strengthening 

Summary: 
The Ministry of Health in Ethiopia is working towards expanding access to essential healthcare services by strengthening health posts. Due to limited budgets and competing priorities, only a fraction of this effort can be implemented each year. To address this, a tool called Health Access Resource Planner (HARP) has been developed to guide prioritization across Ethiopia's regions. HARP is based on a decision-support optimization framework that aims to maximize population coverage under budget uncertainty while meeting region-specific proportionality targets at each step. Two algorithms have been proposed to aid in facility planning: a learning-augmented approach to improve expert recommendations and a greedy algorithm for multi-step planning, both with strong worst-case approximation estimation. Through collaboration with Ethiopian Public Health Institute and the Ministry of Health, the efficacy of the method has been demonstrated in three regions across various planning scenarios. <div>
arXiv:2509.00135v1 Announce Type: new 
Abstract: As part of nationwide efforts aligned with the United Nations' Sustainable Development Goal 3 on Universal Health Coverage, Ethiopia's Ministry of Health is strengthening health posts to expand access to essential healthcare services. However, only a fraction of this health system strengthening effort can be implemented each year due to limited budgets and other competing priorities, thus the need for an optimization framework to guide prioritization across the regions of Ethiopia. In this paper, we develop a tool, Health Access Resource Planner (HARP), based on a principled decision-support optimization framework for sequential facility planning that aims to maximize population coverage under budget uncertainty while satisfying region-specific proportionality targets at every time step. We then propose two algorithms: (i) a learning-augmented approach that improves upon expert recommendations at any single-step; and (ii) a greedy algorithm for multi-step planning, both with strong worst-case approximation estimation. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we demonstrated the empirical efficacy of our method on three regions across various planning scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Group Knowledge and Group Belief in Topological Evidence Models (Extended Version)</title>
<link>https://arxiv.org/abs/2509.00184</link>
<guid>https://arxiv.org/abs/2509.00184</guid>
<content:encoded><![CDATA[
<div> Keywords: group knowledge, group belief, evidence-based belief, multi-agent, logic

Summary:
The study focuses on expanding the topological semantics of evidence-based belief and knowledge to encompass groups within multi-agent models. The logic of group evidence, knowledge, and belief is completely axiomatized and shown to be decidable. A specific fragment, the logic of group knowledge and belief, is also thoroughly explored. The extension of languages with dynamic evidence-sharing operators is fully axiomatized, demonstrating co-expressiveness with their static bases. This research provides a comprehensive understanding of (virtual) group knowledge and belief systems within multi-agent evidence-based models, showcasing the importance of collaborative decision-making processes and shared information among groups. <div>
arXiv:2509.00184v1 Announce Type: new 
Abstract: We study notions of (virtual) group knowledge and group belief within multi-agent evidence models, obtained by extending the topological semantics of evidence-based belief and fallible knowledge from individuals to groups. We completely axiomatize and show the decidability of the logic of ("hard" and "soft") group evidence, and do the same for an especially interesting fragment of it: the logic of group knowledge and group belief. We also extend these languages with dynamic evidence-sharing operators, and completely axiomatize the corresponding logics, showing that they are co-expressive with their static bases.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiVA: Self-organized Hierarchical Variable Agent via Goal-driven Semantic-Topological Evolution</title>
<link>https://arxiv.org/abs/2509.00189</link>
<guid>https://arxiv.org/abs/2509.00189</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous agents, Artificial General Intelligence, Hierarchical Variable Agent, Semantic-Topological Evolution, Task execution.

Summary:
Autonomous agents are essential for advancing Artificial General Intelligence by leveraging Large Language Models (LLMs) for problem-solving. However, current paradigms face a trade-off between fixed workflows and flexible reactive loops. To address this, the Hierarchical Variable Agent (HiVA) framework models workflows as self-organized graphs with the Semantic-Topological Evolution algorithm. This optimization process utilizes textual gradients as surrogates for backpropagation, enhancing task accuracy and resource efficiency in various benchmarks. HiVA's iterative approach includes Multi-Armed Bandit-based routing, diagnostic gradient generation, and coordinated updates for collective optimization in unknown environments. Experimental results demonstrate a 5-10% improvement in task accuracy, showcasing HiVA's effectiveness in autonomous task execution.

<br /><br />Summary: 
1. Autonomous agents are vital for advancing Artificial General Intelligence.
2. HiVA framework models workflows as self-organized graphs with the Semantic-Topological Evolution algorithm.
3. Textual gradients are used for backpropagation, enhancing task accuracy and resource efficiency.
4. HiVA's iterative approach includes Multi-Armed Bandit-based routing and diagnostic gradient generation.
5. Experimental results show a 5-10% improvement in task accuracy, validating HiVA's effectiveness in autonomous task execution. <div>
arXiv:2509.00189v1 Announce Type: new 
Abstract: Autonomous agents play a crucial role in advancing Artificial General Intelligence, enabling problem decomposition and tool orchestration through Large Language Models (LLMs). However, existing paradigms face a critical trade-off. On one hand, reusable fixed workflows require manual reconfiguration upon environmental changes; on the other hand, flexible reactive loops fail to distill reasoning progress into transferable structures. We introduce Hierarchical Variable Agent (HiVA), a novel framework modeling agentic workflows as self-organized graphs with the Semantic-Topological Evolution (STEV) algorithm, which optimizes hybrid semantic-topological spaces using textual gradients as discrete-domain surrogates for backpropagation. The iterative process comprises Multi-Armed Bandit-infused forward routing, diagnostic gradient generation from environmental feedback, and coordinated updates that co-evolve individual semantics and topology for collective optimization in unknown environments. Experiments on dialogue, coding, Long-context Q&amp;A, mathematical, and agentic benchmarks demonstrate improvements of 5-10% in task accuracy and enhanced resource efficiency over existing baselines, establishing HiVA's effectiveness in autonomous task execution.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Deep Research: Bring Your Own Model and Strategy</title>
<link>https://arxiv.org/abs/2509.00244</link>
<guid>https://arxiv.org/abs/2509.00244</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep research tools, Universal Deep Research (UDR), agentic system, custom research strategies, user interface

Summary: 
Universal Deep Research (UDR) is a novel agentic system designed to empower users to create, customize, and refine their own research strategies without the need for additional training. Unlike traditional deep research agents, UDR provides a flexible framework that can adapt to various research tasks and goals. By wrapping around any language model, UDR allows users to craft unique research approaches, such as minimal, expansive, and intensive strategies. The system is equipped with a user-friendly interface that simplifies experimentation and exploration of different research methodologies. Through UDR, users can efficiently utilize deep research tools to enhance their work and address specific research challenges, making it a valuable asset for researchers seeking a more tailored and adaptable approach to conducting advanced research. <div>
arXiv:2509.00244v1 Announce Type: new 
Abstract: Deep research tools are among the most impactful and most commonly encountered agentic systems today. We observe, however, that each deep research agent introduced so far is hard-coded to carry out a particular research strategy using a fixed choice of tools. We introduce Universal Deep Research (UDR), a generalist agentic system that wraps around any language model and enables the user to create, edit, and refine their own entirely custom deep research strategies without any need for additional training or finetuning. To showcase the generality of our system, we equip UDR with example minimal, expansive, and intensive research strategies, and provide a user interface to facilitate experimentation with the system.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction-Level Weight Shaping: A Framework for Self-Improving AI Agents</title>
<link>https://arxiv.org/abs/2509.00251</link>
<guid>https://arxiv.org/abs/2509.00251</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Instruction-Level Weight Shaping, Reflection Engine, Enterprise support, Autonomous updates

Summary:
Instruction-Level Weight Shaping (ILWS) is introduced as a method to improve large language models (LLMs) by using curated system instructions to update pseudo-parameters based on user feedback and reflection. The Reflection Engine diagnoses reasoning successes and failures, proposing changes to instructions, user preferences, and tools. These changes are evaluated and controlled through version control mechanisms. ILWS aims to enhance throughput and reduce errors in enterprise support contexts. It has shown significant improvements in ticket resolution efficiency and accuracy. ILWS enables autonomous updates and tool synthesis, making it adaptable to various dynamic domains like legal, medical, and engineering. By operating at the instruction layer until controlled distillation, ILWS offers a practical solution for integrating new knowledge and improving reasoning capabilities in LLMs. <br /><br />Summary: <div>
arXiv:2509.00251v1 Announce Type: new 
Abstract: Large language models (LLMs) are fluent but largely static after pre-training; new or shifting knowledge is typically added with retrieval-augmented generation (RAG) or fine-tuning. RAG raises latency and engineering overhead and often fails to integrate facts; prompt engineering is brittle and can conflict with prior knowledge; fine-tuning is costly and risks catastrophic forgetting. We propose Instruction-Level Weight Shaping (ILWS): curated system instructions act as external, auditable pseudo-parameters updated after each session via reflection and user feedback. A Reflection Engine inspects conversation traces, diagnoses reasoning successes and failures, and proposes typed deltas $\Delta K=(\Delta S,\Delta U,\Delta T)$ over instructions, user preferences, and tools. Deltas are version-controlled, evaluated with a sliding window of 1-5 star ratings, auto-repaired on first failure, and rolled back on repeated failure. When an edit budget crosses a threshold, the agent compiles a rating-weighted synthetic set and distills matured instruction-space gains into parameters, converting prompt-space improvements into weight-space without downtime. ILWS makes explicit the low-rank shaping induced by context in transformer blocks, preserves governance, and removes per-call retrieval. In enterprise support it increased throughput 2.4-5.0x and cut audited hallucinations by about 80% versus a frozen baseline. In an Adobe Commerce Cloud proof of concept "L0 Support", it achieved 4-5x more tickets per hour and about 80% lower time per ticket, with autonomous instruction updates and optional tool synthesis. Because ILWS operates at the instruction layer until controlled distillation, it generalizes to dynamic domains (legal, medical, engineering) requiring adaptive reasoning, tool creation, and low-latency deployment.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHERPA: A Model-Driven Framework for Large Language Model Execution</title>
<link>https://arxiv.org/abs/2509.00272</link>
<guid>https://arxiv.org/abs/2509.00272</guid>
<content:encoded><![CDATA[
<div> Framework, Large Language Models, State Machines, Control, Performance
Summary:
SHERPA is a model-driven framework designed to enhance the performance of Large Language Models (LLMs) in dealing with complex tasks by integrating domain-specific best practices into hierarchical state machines. By utilizing state machines, SHERPA enables more precise control over LLM behavior through machine learning-driven decisions and rules. The framework proves effective in tasks such as code and class name generation, as well as question answering, surpassing previous approaches in performance. The study showcases the benefits of incorporating well-designed state machines, particularly in tasks that rely on established human best practices but lack sufficient training data for LLMs. Experimental results demonstrate that integrating state machines significantly enhances the quality of LLM outputs, indicating the potential of SHERPA in improving LLM capabilities for challenging tasks. 
Summary: <div>
arXiv:2509.00272v1 Announce Type: new 
Abstract: Recently, large language models (LLMs) have achieved widespread application across various fields. Despite their impressive capabilities, LLMs suffer from a lack of structured reasoning ability, particularly for complex tasks requiring domain-specific best practices, which are often unavailable in the training data. Although multi-step prompting methods incorporating human best practices, such as chain-of-thought and tree-of-thought, have gained popularity, they lack a general mechanism to control LLM behavior. In this paper, we propose SHERPA, a model-driven framework to improve the LLM performance on complex tasks by explicitly incorporating domain-specific best practices into hierarchical state machines. By structuring the LLM execution processes using state machines, SHERPA enables more fine-grained control over their behavior via rules or decisions driven by machine learning-based approaches, including LLMs. We show that SHERPA is applicable to a wide variety of tasks-specifically, code generation, class name generation, and question answering-replicating previously proposed approaches while further improving the performance. We demonstrate the effectiveness of SHERPA for the aforementioned tasks using various LLMs. Our systematic evaluation compares different state machine configurations against baseline approaches without state machines. Results show that integrating well-designed state machines significantly improves the quality of LLM outputs, and is particularly beneficial for complex tasks with well-established human best practices but lacking data used for training LLMs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces</title>
<link>https://arxiv.org/abs/2509.00287</link>
<guid>https://arxiv.org/abs/2509.00287</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal data, urban spaces, knowledge graphs, incidents, Large Language Models (LLMs)

Summary:
SIGMUS is a system designed for Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces. It leverages Large Language Models (LLMs) to identify relationships between incidents happening in urban areas and data from various sources. This allows for the organization of evidence and observations relevant to incidents without the need for human-encoded rules. The system creates a knowledge graph to represent organized knowledge about incidents, observations, and more in urban spaces. By analyzing data from sources such as news articles, CCTV images, air quality, weather, and traffic measurements, SIGMUS can establish connections between different data sources and incidents occurring simultaneously at the same location. The system shows promise in bridging the gap between fragmented multimodal data sources and effectively integrating them to understand and predict events in urban landscapes. 

<br /><br />Summary: <div>
arXiv:2509.00287v1 Announce Type: new 
Abstract: Modern urban spaces are equipped with an increasingly diverse set of sensors, all producing an abundance of multimodal data. Such multimodal data can be used to identify and reason about important incidents occurring in urban landscapes, such as major emergencies, cultural and social events, as well as natural disasters. However, such data may be fragmented over several sources and difficult to integrate due to the reliance on human-driven reasoning for identifying relationships between the multimodal data corresponding to an incident, as well as understanding the different components which define an incident. Such relationships and components are critical to identifying the causes of such incidents, as well as producing forecasting the scale and intensity of future incidents as they begin to develop. In this work, we create SIGMUS, a system for Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces. SIGMUS uses Large Language Models (LLMs) to produce the necessary world knowledge for identifying relationships between incidents occurring in urban spaces and data from different modalities, allowing us to organize evidence and observations relevant to an incident without relying and human-encoded rules for relating multimodal sensory data with incidents. This organized knowledge is represented as a knowledge graph, organizing incidents, observations, and much more. We find that our system is able to produce reasonable connections between 5 different data sources (new article text, CCTV images, air quality, weather, and traffic measurements) and relevant incidents occurring at the same time and location.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEWSAGENT: Benchmarking Multimodal Agents as Journalists with Real-World Newswriting Tasks</title>
<link>https://arxiv.org/abs/2509.00446</link>
<guid>https://arxiv.org/abs/2509.00446</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous agents, journalism, multimodal web data, news writing, evaluation<br />
Summary:<br />
Recent advances in autonomous digital agents have shown potential in structured tasks like decision-making and task decomposition in industries like journalism. The NEWSAGENT benchmark evaluates how agents can search, select, and edit multimodal web data to create news articles, reflecting real-world challenges in news writing. Agents must identify narrative perspectives, issue queries, retrieve background information, and integrate facts to generate complete articles. While agents can retrieve relevant information, they struggle with planning and narrative cohesion. NEWSAGENT provides a realistic testbed for evaluating agent performance in manipulating multimodal web data for real-world productivity. <div>
arXiv:2509.00446v1 Announce Type: new 
Abstract: Recent advances in autonomous digital agents from industry (e.g., Manus AI and Gemini's research mode) highlight potential for structured tasks by autonomous decision-making and task decomposition; however, it remains unclear to what extent the agent-based systems can improve multimodal web data productivity. We study this in the realm of journalism, which requires iterative planning, interpretation, and contextual reasoning from multimodal raw contents to form a well structured news. We introduce NEWSAGENT, a benchmark for evaluating how agents can automatically search available raw contents, select desired information, and edit and rephrase to form a news article by accessing core journalistic functions. Given a writing instruction and firsthand data as how a journalist initiates a news draft, agents are tasked to identify narrative perspectives, issue keyword-based queries, retrieve historical background, and generate complete articles. Unlike typical summarization or retrieval tasks, essential context is not directly available and must be actively discovered, reflecting the information gaps faced in real-world news writing. NEWSAGENT includes 6k human-verified examples derived from real news, with multimodal contents converted to text for broad model compatibility. We evaluate open- and closed-sourced LLMs with commonly-used agentic frameworks on NEWSAGENT, which shows that agents are capable of retrieving relevant facts but struggling with planning and narrative integration. We believe that NEWSAGENT serves a realistic testbed for iterating and evaluating agent capabilities in terms of multimodal web data manipulation to real-world productivity.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Data Visualization and Narrative Generation</title>
<link>https://arxiv.org/abs/2509.00481</link>
<guid>https://arxiv.org/abs/2509.00481</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, data visualization, multi-agent system, granular outputs, sustainable collaboration

Summary: 
This article discusses the implementation of a lightweight multi-agent system in the field of data visualization to automate the data analysis workflow. By leveraging a hybrid multi-agent architecture with deterministic components, the system enhances transparency and reliability by externalizing critical logic. It is designed to deliver granular, modular outputs that allow for surgical modifications without requiring full regeneration, promoting sustainable collaboration between humans and AI. The system was evaluated across diverse datasets, demonstrating strong generalizability, narrative quality, and computational efficiency with minimal dependencies. Overall, this approach showcases the potential of multi-agent systems in enhancing the automation and collaboration capabilities in data visualization processes. 

<br /><br />Summary: <div>
arXiv:2509.00481v1 Announce Type: new 
Abstract: Recent advancements in the field of AI agents have impacted the way we work, enabling greater automation and collaboration between humans and agents. In the data visualization field, multi-agent systems can be useful for employing agents throughout the entire data-to-communication pipeline. We present a lightweight multi-agent system that automates the data analysis workflow, from data exploration to generating coherent visual narratives for insight communication. Our approach combines a hybrid multi-agent architecture with deterministic components, strategically externalizing critical logic from LLMs to improve transparency and reliability. The system delivers granular, modular outputs that enable surgical modifications without full regeneration, supporting sustainable human-AI collaboration. We evaluated our system across 4 diverse datasets, demonstrating strong generalizability, narrative quality, and computational efficiency with minimal dependencies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence-Based Analysis of Ice Cream Melting Behavior Under Various Ingredients</title>
<link>https://arxiv.org/abs/2509.00507</link>
<guid>https://arxiv.org/abs/2509.00507</guid>
<content:encoded><![CDATA[
<div> Keywords: ice cream, stabilizers, melting behavior, foam-like structure, cost-effective

Summary: 
The study examines the impact of different stabilizers, including locust bean gum, guar gum, maltodextrin, and carrageenan, on the melting behavior of homemade ice cream. Through melting tests and timelapse recordings, it was found that all samples maintained a foam-like structure after melting, indicating the stabilizers contributed to a stable air-cell matrix. Additionally, re-freezing and re-melting the samples showed increased sturdiness, suggesting improved structural resilience. Comparative analysis revealed varying effectiveness among stabilizers, with some providing stronger melting resistance and structural support. The study highlights the potential for developing cost-effective ice cream recipes that balance durability and economic efficiency, offering practical applications for both small-scale and commercial production. 

<br /><br />Summary: <div>
arXiv:2509.00507v1 Announce Type: new 
Abstract: The stability of ice cream during melting is a critical factor for consumer's acceptance and product quality. With the commonly added stabilizer to improve texture, structure and slower melting as the factors to analyze. This report explores the effects of locust bean gum, guar gum, maltodextrin, and carrageenan on the melting behavior of homemade ice cream. The main objective was to assess how these additives influence melting resistance and to identify a more cost-effective recipe formulation. Ice cream samples incorporating each additive were prepared and subjected to melting tests under controlled conditions. Timelapse recordings were used to capture and analyze the progression of melting over time. Python and OpenCV is used for process and analysis. Observations revealed that all samples retained a foam-like structure even after melting, suggesting the stabilizers contributed to the formation of a stable air-cell matrix. Furthermore, when the melted samples were re-frozen and subsequently melted again, they displayed increased sturdiness, indicating improved resilience of the ice cream structure. Comparative analysis of the different stabilizers highlighted variations in their effectiveness, with some offering stronger melting resistance and structural support than others. Overall, the findings provide insights into the functional roles of commonly used food additives in ice cream formulation. By evaluating both performance and cost, this study demonstrates the potential for developing recipes that balance durability with economic efficiency, contributing to practical applications in both small-scale and commercial ice cream production.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Assisted Iterative Evolution with Swarm Intelligence Toward SuperBrain</title>
<link>https://arxiv.org/abs/2509.00510</link>
<guid>https://arxiv.org/abs/2509.00510</guid>
<content:encoded><![CDATA[
<div> Keywords: SuperBrain, collective intelligence, large language models, co-evolution, Swarm Intelligence

Summary: 
The proposed SuperBrain framework focuses on the collaboration between large language models (LLMs) and human users, aiming to create a collective intelligence system. This dynamic approach involves the evolution from Subclass Brain to Superclass Brain through personalized interactions, adaptive learning memory, and GA-assisted evolution. Subclass Brains are formed through persistent user-LLM interactions, refining prompts and task performance. These Subclass Brains then coordinate via Swarm Intelligence to optimize across various tasks and exchange heuristics. The behaviors and cognitive signatures of multiple Subclass Brains integrate into a Superclass Brain, which possesses meta-intelligence capabilities such as abstraction, generalization, and self-improvement. Initial implementations include UAV scheduling and keyword filtering tasks. The proposed registry enables cross-dyad knowledge consolidation. This work lays the foundation for scalable, explainable, and ethically aligned collective AI systems.Summary: <div>
arXiv:2509.00510v1 Announce Type: new 
Abstract: We propose a novel SuperBrain framework for collective intelligence, grounded in the co-evolution of large language models (LLMs) and human users. Unlike static prompt engineering or isolated agent simulations, our approach emphasizes a dynamic pathway from Subclass Brain to Superclass Brain: (1) A Subclass Brain arises from persistent, personalized interaction between a user and an LLM, forming a cognitive dyad with adaptive learning memory. (2) Through GA-assisted forward-backward evolution, these dyads iteratively refine prompts and task performance. (3) Multiple Subclass Brains coordinate via Swarm Intelligence, optimizing across multi-objective fitness landscapes and exchanging distilled heuristics. (4) Their standardized behaviors and cognitive signatures integrate into a Superclass Brain, an emergent meta-intelligence capable of abstraction, generalization and self-improvement. We outline the theoretical constructs, present initial implementations (e.g., UAV scheduling, KU/KI keyword filtering) and propose a registry for cross-dyad knowledge consolidation. This work provides both a conceptual foundation and an architectural roadmap toward scalable, explainable and ethically aligned collective AI.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-Layout: A Generative Workflow for Drafting Architectural Floor Plans Using LLMs</title>
<link>https://arxiv.org/abs/2509.00543</link>
<guid>https://arxiv.org/abs/2509.00543</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, Large Language Models, architectural floor plans, furniture placement, Autodesk Revit <br />
Summary: 
This paper introduces an AI-powered workflow that utilizes Large Language Models (LLMs) to generate schematic architectural floor plans based on natural language prompts. The system interprets textual input to create layout options, including walls, doors, windows, and furniture placements, with minimal manual input required. Through prompt engineering, furniture placement refinement algorithms, and Python scripting, the system produces coherent draft plans that can easily be integrated into design tools like Autodesk Revit. A case study on a mid-sized residential layout showcases the system's ability to generate functional and structured outputs. The workflow is transparent, with detailed prompt specifications provided for replication by other researchers. The generated models retain all necessary Revit-native parametric attributes, facilitating seamless integration into professional BIM processes. <br /><br />Summary: <div>
arXiv:2509.00543v1 Announce Type: new 
Abstract: This paper presents the development of an AI-powered workflow that uses Large Language Models (LLMs) to assist in drafting schematic architectural floor plans from natural language prompts. The proposed system interprets textual input to automatically generate layout options including walls, doors, windows, and furniture arrangements. It combines prompt engineering, a furniture placement refinement algorithm, and Python scripting to produce spatially coherent draft plans compatible with design tools such as Autodesk Revit. A case study of a mid-sized residential layout demonstrates the approach's ability to generate functional and structured outputs with minimal manual effort. The workflow is designed for transparent replication, with all key prompt specifications documented to enable independent implementation by other researchers. In addition, the generated models preserve the full range of Revit-native parametric attributes required for direct integration into professional BIM processes.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social World Models</title>
<link>https://arxiv.org/abs/2509.00559</link>
<guid>https://arxiv.org/abs/2509.00559</guid>
<content:encoded><![CDATA[
<div> Keywords: social dynamics, structured representation, reasoning, social interactions, AI systems

Summary: 
The paper introduces a novel structured social world representation formalism (S3AP) aimed at helping AI systems better understand and reason about social dynamics. Based on a POMDP-driven design, S3AP represents social interactions as structured tuples, enabling automatic induction from free-form narratives. The study demonstrates that S3AP can enhance Language Models' (LLMs) understanding of social narratives and improve performance on social reasoning tasks, outperforming existing benchmarks. Furthermore, the induced social world models from these structured representations prove effective in predicting future social dynamics and enhancing agent decision-making processes. The findings highlight the potential of S3AP as a versatile representation for social world states, enabling the development of more socially-aware AI systems that can navigate social interactions more effectively.

<br /><br />Summary: <div>
arXiv:2509.00559v1 Announce Type: new 
Abstract: Humans intuitively navigate social interactions by simulating unspoken dynamics and reasoning about others' perspectives, even with limited information. In contrast, AI systems struggle to automatically structure and reason about these implicit social contexts. In this paper, we introduce a novel structured social world representation formalism (S3AP), designed to help AI systems reason more effectively about social dynamics. Following a POMDP-driven design, S3AP represents social interactions as structured tuples, such as state, observation, agent actions, and mental states, which can be automatically induced from free-form narratives or other inputs. We first show S3AP can help LLMs better understand social narratives across 5 social reasoning tasks (e.g., +51% improvement on FANToM's theory-of-mind reasoning with OpenAI's o1), reaching new state-of-the-art (SOTA) performance. We then induce social world models from these structured representations, demonstrating their ability to predict future social dynamics and improve agent decision-making, yielding up to +18% improvement on the SOTOPIA social interaction benchmark. Our findings highlight the promise of S3AP as a powerful, general-purpose representation for social world states, enabling the development of more socially-aware systems that better navigate social interactions.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BALM-TSF: Balanced Multimodal Alignment for LLM-Based Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.00622</link>
<guid>https://arxiv.org/abs/2509.00622</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, multimodal architectures, large language models, balance, alignment

Summary:
BALM-TSF (Balanced Multimodal Alignment for LLM-Based Time Series Forecasting) addresses the issue of imbalance between text and temporal data in forecasting by introducing a lightweight framework that maintains balance between the two modalities. It processes raw time series data with a time series encoder and generates textual embeddings using descriptive statistics with a learnable prompt in a large language model (LLM). A scaling strategy combined with a contrastive objective ensures balanced cross-modal context alignment between the time series and textual embeddings. The aligned textual semantic embeddings and time series embeddings are then integrated for forecasting. Experimental results on standard benchmarks demonstrate that BALM-TSF achieves state-of-the-art performance in long-term and few-shot forecasting with minimal trainable parameters. The framework effectively harnesses complementary information from text and time series data. Code for BALM-TSF is available on GitHub for further exploration and use.

<br /><br />Summary: <div>
arXiv:2509.00622v1 Announce Type: new 
Abstract: Time series forecasting is a long-standing and highly challenging research topic. Recently, driven by the rise of large language models (LLMs), research has increasingly shifted from purely time series methods toward harnessing textual modalities to enhance forecasting performance. However, the vast discrepancy between text and temporal data often leads current multimodal architectures to over-emphasise one modality while neglecting the other, resulting in information loss that harms forecasting performance. To address this modality imbalance, we introduce BALM-TSF (Balanced Multimodal Alignment for LLM-Based Time Series Forecasting), a lightweight time series forecasting framework that maintains balance between the two modalities. Specifically, raw time series are processed by the time series encoder, while descriptive statistics of raw time series are fed to an LLM with learnable prompt, producing compact textual embeddings. To ensure balanced cross-modal context alignment of time series and textual embeddings, a simple yet effective scaling strategy combined with a contrastive objective then maps these textual embeddings into the latent space of the time series embeddings. Finally, the aligned textual semantic embeddings and time series embeddings are together integrated for forecasting. Extensive experiments on standard benchmarks show that, with minimal trainable parameters, BALM-TSF achieves state-of-the-art performance in both long-term and few-shot forecasting, confirming its ability to harness complementary information from text and time series. Code is available at https://github.com/ShiqiaoZhou/BALM-TSF.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NetGent: Agent-Based Automation of Network Application Workflows</title>
<link>https://arxiv.org/abs/2509.00625</link>
<guid>https://arxiv.org/abs/2509.00625</guid>
<content:encoded><![CDATA[
<div> Keywords: NetGent, AI-agent framework, network traffic datasets, workflow automation, ML in networking <br />
Summary: 
NetGent is an AI-agent framework designed to automate complex application workflows for generating realistic network traffic datasets. It allows users to specify workflows as natural-language rules, which are then compiled into executable code for deterministic replay. The framework reduces redundant calls and adapts quickly to changes in application interfaces. In experiments, NetGent successfully automated various workflows related to video streaming, video conferencing, social media, and web scraping, producing realistic traffic traces. By combining language-based agents with compiled execution, NetGent provides a scalable solution for generating diverse and repeatable datasets crucial for advancing machine learning in networking. <br /><br /> <div>
arXiv:2509.00625v1 Announce Type: new 
Abstract: We present NetGent, an AI-agent framework for automating complex application workflows to generate realistic network traffic datasets. Developing generalizable ML models for networking requires data collection from network environments with traffic that results from a diverse set of real-world web applications. However, using existing browser automation tools that are diverse, repeatable, realistic, and efficient remains fragile and costly. NetGent addresses this challenge by allowing users to specify workflows as natural-language rules that define state-dependent actions. These abstract specifications are compiled into nondeterministic finite automata (NFAs), which a state synthesis component translates into reusable, executable code. This design enables deterministic replay, reduces redundant LLM calls through state caching, and adapts quickly when application interfaces change. In experiments, NetGent automated more than 50+ workflows spanning video-on-demand streaming, live video streaming, video conferencing, social media, and web scraping, producing realistic traffic traces while remaining robust to UI variability. By combining the flexibility of language-based agents with the reliability of compiled execution, NetGent provides a scalable foundation for generating the diverse, repeatable datasets needed to advance ML in networking.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized Knowledge Representations</title>
<link>https://arxiv.org/abs/2509.00710</link>
<guid>https://arxiv.org/abs/2509.00710</guid>
<content:encoded><![CDATA[
<div> extract, legal reasoning, AI systems, modular framework, knowledge acquisition <br />
Summary: <br />
This paper introduces a modular multi-agent framework for legal reasoning, dividing the process into knowledge acquisition and application stages. Specialized agents extract legal concepts and formalize rules to create intermediate representations of statutes. The framework then applies this knowledge to specific cases through analyzing queries, performing symbolic inference, and generating final answers. The approach combines natural language understanding with symbolic reasoning, enhancing transparency. Evaluation on tax calculation tasks showed foundational models achieving 76.4% accuracy, narrowing the performance gap. This suggests that modular architectures with formalized knowledge representations can make legal reasoning more accessible and enhance consistency and explainability in AI systems. The study lays a foundation for future research on transparent, trustworthy, and effective AI systems for the legal domain. <br /> <div>
arXiv:2509.00710v1 Announce Type: new 
Abstract: Legal reasoning requires both precise interpretation of statutory language and consistent application of complex rules, presenting significant challenges for AI systems. This paper introduces a modular multi-agent framework that decomposes legal reasoning into distinct knowledge acquisition and application stages. In the first stage, specialized agents extract legal concepts and formalize rules to create verifiable intermediate representations of statutes. The second stage applies this knowledge to specific cases through three steps: analyzing queries to map case facts onto the ontology schema, performing symbolic inference to derive logically entailed conclusions, and generating final answers using a programmatic implementation that operationalizes the ontological knowledge. This bridging of natural language understanding with symbolic reasoning provides explicit and verifiable inspection points, significantly enhancing transparency compared to end-to-end approaches. Evaluation on statutory tax calculation tasks demonstrates substantial improvements, with foundational models achieving 76.4\% accuracy compared to 18.8\% baseline performance, effectively narrowing the performance gap between reasoning and foundational models. These findings suggest that modular architectures with formalized knowledge representations can make sophisticated legal reasoning more accessible through computationally efficient models while enhancing consistency and explainability in AI legal reasoning, establishing a foundation for future research into more transparent, trustworthy, and effective AI systems for legal domain.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination</title>
<link>https://arxiv.org/abs/2509.00723</link>
<guid>https://arxiv.org/abs/2509.00723</guid>
<content:encoded><![CDATA[
<div> framework, hallucinations, multimodal, OLLMs, audio-video<br />
<br />
Summary:<br />
Omni-modal large language models (OLLMs) have shown impressive performance in tasks involving audio-video understanding. However, they still suffer from hallucination issues, where text cues dominate and neglect visual and audio information. The OmniDPO framework aims to address these challenges by enhancing the model's understanding of audio-video interactions through text-preference sample pairs and strengthening attention to visual and auditory information through multimodal-preference sample pairs. By tackling these challenges, OmniDPO effectively improves multimodal grounding and reduces hallucination in OLLMs. Experiments show that OmniDPO not only mitigates multimodal hallucinations but also enhances reasoning capabilities across modalities. The code and datasets will be released upon paper acceptance.<br /> <div>
arXiv:2509.00723v1 Announce Type: new 
Abstract: Recently, Omni-modal large language models (OLLMs) have sparked a new wave of research, achieving impressive results in tasks such as audio-video understanding and real-time environment perception. However, hallucination issues still persist. Similar to the bimodal setting, the priors from the text modality tend to dominate, leading OLLMs to rely more heavily on textual cues while neglecting visual and audio information. In addition, fully multimodal scenarios introduce new challenges. Most existing models align visual or auditory modalities with text independently during training, while ignoring the intrinsic correlations between video and its corresponding audio. This oversight results in hallucinations when reasoning requires interpreting hidden audio cues embedded in video content. To address these challenges, we propose OmniDPO, a preference-alignment framework designed to mitigate hallucinations in OLLMs. Specifically, OmniDPO incorporates two strategies: (1) constructing text-preference sample pairs to enhance the model's understanding of audio-video interactions; and (2) constructing multimodal-preference sample pairs to strengthen the model's attention to visual and auditory information. By tackling both challenges, OmniDPO effectively improves multimodal grounding and reduces hallucination. Experiments conducted on two OLLMs demonstrate that OmniDPO not only effectively mitigates multimodal hallucinations but also significantly enhances the models' reasoning capabilities across modalities. All code and datasets will be released upon paper acceptance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Graph Understanding with LLMs via Structured Context Injection</title>
<link>https://arxiv.org/abs/2509.00740</link>
<guid>https://arxiv.org/abs/2509.00740</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Graph Problems, Structured Context Injection, Computational Cost, Performance Improvements

Summary: 
Large Language Models (LLMs) have been successful in solving graph-related tasks by injecting structured context into the input to guide them. This approach eliminates the need for fine-tuning, making it cost-effective and efficient. Some graph reasoning tasks remain challenging for LLMs without mappings to grounded representations, but injecting structured context helps align tasks with conceptual spaces. This method, evaluated across various graph tasks with lightweight and large models, shows consistent performance improvements, highlighting the trade-offs between accuracy and computational cost. The results demonstrate that structured input context can match or outperform more complex approaches, emphasizing the effectiveness and scalability of structured context injection for graph understanding with LLMs.<br /><br />Summary: <div>
arXiv:2509.00740v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong capabilities in solving problems across domains, including graph-related tasks traditionally addressed by symbolic or algorithmic methods. In this work, we present a framework for structured context injection, where task-specific information is systematically embedded in the input to guide LLMs in solving a wide range of graph problems. Our method does not require fine-tuning of LLMs, making it cost-efficient and lightweight. We observe that certain graph reasoning tasks remain challenging for LLMs unless they are mapped to conceptually grounded representations. However, achieving such mappings through fine-tuning or repeated multi-step querying can be expensive and inefficient. Our approach offers a practical alternative by injecting structured context directly into the input, enabling the LLM to implicitly align the task with grounded conceptual spaces. We evaluate the approach on multiple graph tasks using both lightweight and large models, highlighting the trade-offs between accuracy and computational cost. The results demonstrate consistent performance improvements, showing that structured input context can rival or surpass more complex approaches. Our findings underscore the value of structured context injection as an effective and scalable strategy for graph understanding with LLMs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L-MARS -- Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search</title>
<link>https://arxiv.org/abs/2509.00761</link>
<guid>https://arxiv.org/abs/2509.00761</guid>
<content:encoded><![CDATA[
<div> Keywords: L-MARS, legal question answering, multi-agent reasoning, agentic search, LegalSearchQA

Summary:
L-MARS is a system designed to enhance legal question answering by combining multi-agent reasoning and agentic search techniques. It breaks down queries into subproblems, conducts targeted searches across various sources, and employs a Judge Agent for verification before answer synthesis. This iterative process helps reduce hallucination and uncertainty, ensuring coherence in answers. The system was evaluated on LegalSearchQA, a benchmark of 200 legal questions updated in 2025, and showed significant improvements in factual accuracy and reduced uncertainty. It received higher preference scores from both human experts and LLM-based judges. The study highlights the effectiveness of multi-agent reasoning and agentic search for deploying LLMs in high-stakes domains that require precise legal retrieval and deliberation.

<br /><br />Summary: 
1. L-MARS enhances legal question answering through multi-agent reasoning and agentic search.
2. Queries are decomposed into subproblems for targeted searches across heterogeneous sources.
3. A Judge Agent verifies sufficiency, jurisdiction, and temporal validity before answer synthesis.
4. The system reduces hallucination and uncertainty, ensuring coherence in answers.
5. L-MARS improves factual accuracy and receives higher preference scores in legal question answering evaluations. <div>
arXiv:2509.00761v1 Announce Type: new 
Abstract: We present L-MARS (Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search), a system that reduces hallucination and uncertainty in legal question answering through coordinated multi-agent reasoning and retrieval. Unlike single-pass retrieval-augmented generation (RAG), L-MARS decomposes queries into subproblems, issues targeted searches across heterogeneous sources (Serper web, local RAG, CourtListener case law), and employs a Judge Agent to verify sufficiency, jurisdiction, and temporal validity before answer synthesis. This iterative reasoning-search-verification loop maintains coherence, filters noisy evidence, and grounds answers in authoritative law. We evaluated L-MARS on LegalSearchQA, a new benchmark of 200 up-to-date multiple choice legal questions in 2025. Results show that L-MARS substantially improves factual accuracy, reduces uncertainty, and achieves higher preference scores from both human experts and LLM-based judges. Our work demonstrates that multi-agent reasoning with agentic search offers a scalable and reproducible blueprint for deploying LLMs in high-stakes domains requiring precise legal retrieval and deliberation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Reasoning LLMs for Materials Discovery with Physics-aware Rejection Sampling</title>
<link>https://arxiv.org/abs/2509.00768</link>
<guid>https://arxiv.org/abs/2509.00768</guid>
<content:encoded><![CDATA[
<div> AI-driven materials discovery, large reasoning models, Physics-aware Rejection Sampling, accuracy, calibration.

Summary:
Physics-aware Rejection Sampling (PaRS) is introduced to improve the accuracy and calibration of large reasoning models (LRMs) for AI-driven materials discovery. The method favors reasoning traces consistent with fundamental physics and numerically close to targets, reducing physics-violation rates while lowering sampling cost. By selecting traces based on domain-aware constraints and physics adherence, the framework enhances the reliability and efficiency of LRMs in process-aware property prediction and closed-loop materials design. This approach demonstrates the potential of combining modest constraints with trace-level selection to create more effective and practical models for automated experimentation and algorithmic decision-making in materials science. <br /><br />Summary: <div>
arXiv:2509.00768v1 Announce Type: new 
Abstract: AI-driven materials discovery that couples automated experimentation with algorithmic decision-making requires process aware recipe to property predictors that are accurate, calibrated, and physically admissible. We approach this as a reasoning problem with large reasoning models (LRMs). To instill reasoning capability into language models, we curate reasoning traces from a teacher model to train a student model. However, most training pipelines select reasoning traces using binary correctness or learned preference signals that poorly reflect physical admissibility. We introduce Physics-aware Rejection Sampling (PaRS), a training-time trace selection scheme that favors traces consistent with fundamental physics and numerically close to targets, with lightweight halting to control compute. We instantiate our framework with a large student model fine-tuned on traces synthesized by a larger teacher model, and evaluate under matched token budgets against various rejection sampling baselines. Our method improves accuracy and calibration, reduces physics-violation rates, and lowers sampling cost relative to baselines. These results indicate that modest, domain-aware constraints combined with trace-level selection provide a practical path toward reliable, efficient LRMs for process-aware property prediction and closed-loop materials design.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharpe Ratio Optimization in Markov Decision Processes</title>
<link>https://arxiv.org/abs/2509.00793</link>
<guid>https://arxiv.org/abs/2509.00793</guid>
<content:encoded><![CDATA[
<div> Keywords: Sharpe ratio, Markov decision processes, dynamic programming, risk metrics, optimization<br />
Summary: 
The paper discusses the optimization of the Sharpe ratio in infinite-horizon Markov decision processes, addressing challenges related to fractional objectives and risk metrics. By using Dinkelbach's transform, the Sharpe ratio objective is converted to a mean-squared-variance (M2V) objective, allowing for efficient optimization. An iterative algorithm is developed for M2V optimization, similar to mean-variance optimization in MDPs, with a sequence of Sharpe ratios derived that converges to the optimal value. Policy iteration procedures are developed for both average and discounted MDP settings, ensuring convergence to the optimum solution. Numerical experiments validate the proposed approach, which represents a novel method for solving Sharpe ratio optimization in MDPs using dynamic programming algorithms. <div>
arXiv:2509.00793v1 Announce Type: new 
Abstract: Sharpe ratio (also known as reward-to-variability ratio) is a widely-used metric in finance, which measures the additional return at the cost of per unit of increased risk (standard deviation of return). However, the optimization of Sharpe ratio in Markov decision processes (MDPs) is challenging, because there exist two difficulties hindering the application of dynamic programming. One is that dynamic programming does not work for fractional objectives, and the other is that dynamic programming is invalid for risk metrics. In this paper, we study the Sharpe ratio optimization in infinite-horizon MDPs, considering both the long-run average and discounted settings. We address the first challenge with the Dinkelbachs transform, which converts the Sharpe ratio objective to a mean-squared-variance (M2V) objective. It is shown that the M2V optimization and the original Sharpe ratio optimization share the same optimal policy when the risk-sensitive parameter is equal to the optimal Sharpe ratio. For the second challenge, we develop an iterative algorithm to solve the M2V optimization which is similar to a mean-variance optimization in MDPs. We iteratively solve the M2V problem and obtain the associated Sharpe ratio that is used to update the risk-sensitive parameter in the next iteration of M2V problems. We show that such a sequence of Sharpe ratios derived is monotonically increasing and converges to the optimal Sharpe ratio. For both average and discounted MDP settings, we develop a policy iteration procedure and prove its convergence to the optimum. Numerical experiments are conducted for validation. To the best of our knowledge, our approach is the first that solves the Sharpe ratio optimization in MDPs with dynamic programming type algorithms. We believe that the proposed algorithm can shed light on solving MDPs with other fractional objectives.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Predictive Process Monitoring</title>
<link>https://arxiv.org/abs/2509.00834</link>
<guid>https://arxiv.org/abs/2509.00834</guid>
<content:encoded><![CDATA[
<div> Neuro-Symbolic Predictive Process Monitoring, suffix prediction, Business Process Management, Linear Temporal Logic, Gumbel-Softmax trick<br />
<br />
Summary: This paper introduces a Neuro-Symbolic Predictive Process Monitoring approach to address suffix prediction in Business Process Management by integrating data-driven learning with temporal logic-based prior knowledge. Existing deep learning models for suffix prediction often lack logical constraints due to the absence of domain knowledge integration. The proposed method incorporates Linear Temporal Logic over finite traces into autoregressive sequence predictors during training, utilizing a differentiable logical loss function. This approach improves suffix prediction accuracy and compliance with temporal constraints, as demonstrated on real-world datasets. Two variants of the logic loss (local and global) are introduced and shown to be effective in noisy and realistic settings. While developed for BPM, the framework is adaptable to any symbolic sequence generation task, contributing to the advancement of Neuro-Symbolic AI.<br /><br />Summary: <div>
arXiv:2509.00834v1 Announce Type: new 
Abstract: This paper addresses the problem of suffix prediction in Business Process Management (BPM) by proposing a Neuro-Symbolic Predictive Process Monitoring (PPM) approach that integrates data-driven learning with temporal logic-based prior knowledge. While recent approaches leverage deep learning models for suffix prediction, they often fail to satisfy even basic logical constraints due to the absence of explicit integration of domain knowledge during training. We propose a novel method to incorporate Linear Temporal Logic over finite traces (LTLf) into the training process of autoregressive sequence predictors. Our approach introduces a differentiable logical loss function, defined using a soft approximation of LTLf semantics and the Gumbel-Softmax trick, which can be combined with standard predictive losses. This ensures the model learns to generate suffixes that are both accurate and logically consistent. Experimental evaluation on three real-world datasets shows that our method improves suffix prediction accuracy and compliance with temporal constraints. We also introduce two variants of the logic loss (local and global) and demonstrate their effectiveness under noisy and realistic settings. While developed in the context of BPM, our framework is applicable to any symbolic sequence generation task and contributes toward advancing Neuro-Symbolic AI.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop Insulin Adoption in Type 1 Diabetes Care</title>
<link>https://arxiv.org/abs/2509.00891</link>
<guid>https://arxiv.org/abs/2509.00891</guid>
<content:encoded><![CDATA[
<div> Keywords: closed-loop insulin delivery systems, type 1 diabetes, persuasive dialogue, virtual patients, persuasive AI 

Summary: 
ChatCLIDS is a new benchmark designed to evaluate LLM-driven persuasive dialogue for health behavior change, specifically focusing on the adoption of closed-loop insulin delivery systems in type 1 diabetes patients. The framework includes a library of virtual patients with diverse profiles and adoption barriers, allowing for multi-turn interactions with nurse agents using evidence-based persuasive strategies. The study found that while larger and more reflective LLMs can adapt strategies over time, they struggle to overcome resistance, particularly in the face of realistic social pressure. These results emphasize the limitations of current LLMs for behavior change and highlight the need for trustworthy persuasive AI in healthcare and other domains. ChatCLIDS provides a high-fidelity and scalable testbed for advancing research in this area. 

<br /><br />Summary: <div>
arXiv:2509.00891v1 Announce Type: new 
Abstract: Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1 diabetes remains low, driven not by technical failure, but by diverse behavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the first benchmark to rigorously evaluate LLM-driven persuasive dialogue for health behavior change. Our framework features a library of expert-validated virtual patients, each with clinically grounded, heterogeneous profiles and realistic adoption barriers, and simulates multi-turn interactions with nurse agents equipped with a diverse set of evidence-based persuasive strategies. ChatCLIDS uniquely supports longitudinal counseling and adversarial social influence scenarios, enabling robust, multi-dimensional evaluation. Our findings reveal that while larger and more reflective LLMs adapt strategies over time, all models struggle to overcome resistance, especially under realistic social pressure. These results highlight critical limitations of current LLMs for behavior change, and offer a high-fidelity, scalable testbed for advancing trustworthy persuasive AI in healthcare and beyond.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Deep Monte Carlo Counterfactual Regret Minimization: Addressing Theoretical Risks in Neural Fictitious Self-Play</title>
<link>https://arxiv.org/abs/2509.00923</link>
<guid>https://arxiv.org/abs/2509.00923</guid>
<content:encoded><![CDATA[
<div> Monte Carlo Counterfactual Regret Minimization (MCCFR), deep neural networks, scale-dependent challenges, adaptive framework, small vs. large games

Summary:<br /> 
1. The paper analyzes how neural MCCFR effectiveness varies with game scale, addressing theoretical risks such as nonstationary target distribution shifts, action support collapse, variance explosion, and warm-starting bias in small versus large games.
2. The proposed Robust Deep MCCFR framework incorporates target networks with delayed updates, uniform exploration mixing, variance-aware training objectives, and diagnostic monitoring to mitigate scale-dependent challenges.
3. Systematic ablation studies on Kuhn and Leduc Poker demonstrate scale-dependent component effectiveness and critical component interactions, achieving significant improvements in exploitability.
4. In Kuhn Poker, the best configuration achieves a 60% exploitability improvement over the classical framework, while selective component usage in Leduc Poker results in a 23.5% exploitability improvement, highlighting the importance of careful component selection.
5. Contributions include a formal theoretical analysis, a principled mitigation framework, multi-scale experimental validation, and practical guidelines for deployment in larger games.<br /><br /> <div>
arXiv:2509.00923v1 Announce Type: new 
Abstract: Monte Carlo Counterfactual Regret Minimization (MCCFR) has emerged as a cornerstone algorithm for solving extensive-form games, but its integration with deep neural networks introduces scale-dependent challenges that manifest differently across game complexities. This paper presents a comprehensive analysis of how neural MCCFR component effectiveness varies with game scale and proposes an adaptive framework for selective component deployment. We identify that theoretical risks such as nonstationary target distribution shifts, action support collapse, variance explosion, and warm-starting bias have scale-dependent manifestation patterns, requiring different mitigation strategies for small versus large games. Our proposed Robust Deep MCCFR framework incorporates target networks with delayed updates, uniform exploration mixing, variance-aware training objectives, and comprehensive diagnostic monitoring. Through systematic ablation studies on Kuhn and Leduc Poker, we demonstrate scale-dependent component effectiveness and identify critical component interactions. The best configuration achieves final exploitability of 0.0628 on Kuhn Poker, representing a 60% improvement over the classical framework (0.156). On the more complex Leduc Poker domain, selective component usage achieves exploitability of 0.2386, a 23.5% improvement over the classical framework (0.3703) and highlighting the importance of careful component selection over comprehensive mitigation. Our contributions include: (1) a formal theoretical analysis of risks in neural MCCFR, (2) a principled mitigation framework with convergence guarantees, (3) comprehensive multi-scale experimental validation revealing scale-dependent component interactions, and (4) practical guidelines for deployment in larger games.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement Fine-Tuning of LLMs</title>
<link>https://arxiv.org/abs/2509.00930</link>
<guid>https://arxiv.org/abs/2509.00930</guid>
<content:encoded><![CDATA[
<div> SATQuest, logical reasoning, Large Language Models, evaluation, reinforcement fine-tuning<br />
<br />
Summary: 
The article introduces SATQuest, a tool for evaluating and enhancing logical reasoning in Large Language Models (LLMs). SATQuest generates diverse logical reasoning problems from Conjunctive Normal Form instances, allowing for fine-grained analysis of reasoning capabilities. By structuring problems along three dimensions and using objective answer verification, SATQuest mitigates memorization issues and provides insights into LLM reasoning performance. Evaluation of various LLMs with SATQuest revealed limitations in logical reasoning and the effectiveness of reinforcement fine-tuning in improving performance. The tool also helps in generalizing to more complex instances but highlights challenges in cross-format adaptation. SATQuest is seen as a foundational tool for advancing LLM logical reasoning. <br /><br />Summary: <div>
arXiv:2509.00930v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated remarkable general reasoning capabilities. However, systematically evaluating and enhancing these reasoning capabilities is challenging due to the lack of controllable and scalable tools for fine-grained analysis. Existing benchmarks and datasets often lack the necessary variable control for multi-dimensional, systematic analysis and training, or have narrow problem types and formats. To address these limitations, we introduce SATQuest, a systematic verifier designed to evaluate and enhance logical reasoning in LLMs by generating diverse, Satisfiability-based logical reasoning problems directly from Conjunctive Normal Form (CNF) instances. SATQuest structures these problems along three orthogonal dimensions: instance scale, problem type, and question format, employing randomized, SAT-based problem generation and objective answer verification via PySAT. This design mitigates memorization issues, allows for nuanced insights into reasoning performance, and enables effective reinforcement fine-tuning. Our extensive evaluation of various LLMs using SATQuest identified significant limitations in their logical reasoning, particularly in generalizing beyond familiar mathematical formats. Furthermore, we show that reinforcement fine-tuning with SATQuest rewards substantially improves targeted task performance and generalizes to more complex instances, while highlighting remaining challenges in cross-format adaptation. Through these demonstrations, we showcase SATQuest's potential as a foundational tool and a valuable starting point for advancing LLM logical reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanInsight: A Distributed Edge Computing Framework with LLM-Powered Data Filtering for Smart City Digital Twins</title>
<link>https://arxiv.org/abs/2509.00936</link>
<guid>https://arxiv.org/abs/2509.00936</guid>
<content:encoded><![CDATA[
<div> framework, physics-informed machine learning, multimodal data fusion, knowledge graph, large language models <br />
<br />
Summary: This work presents a novel framework for harnessing the vast amounts of data generated by cities through sensors, cameras, and connected infrastructure. The framework combines physics-informed machine learning, multimodal data fusion, knowledge graph representation, and large language models (LLMs) to create a sophisticated system for improving urban life. Physics-informed methods ensure that predictions are grounded in real-world constraints, while knowledge graphs integrate diverse sensor data into a connected structure. At the edge, LLMs generate context-aware rules that adapt in real time for efficient operation. This approach enables the creation of responsive, trustworthy, and sustainable smart infrastructures that provide actionable insights, going beyond simple monitoring capabilities. By incorporating physics-based reasoning, semantic data fusion, and adaptive rule generation, this framework opens up new possibilities for transforming urban environments. <br /><br /> <div>
arXiv:2509.00936v1 Announce Type: new 
Abstract: Cities today generate enormous streams of data from sensors, cameras, and connected infrastructure. While this information offers unprecedented opportunities to improve urban life, most existing systems struggle with scale, latency, and fragmented insights. This work introduces a framework that blends physics-informed machine learning, multimodal data fusion, and knowledge graph representation with adaptive, rule-based intelligence powered by large language models (LLMs). Physics-informed methods ground learning in real-world constraints, ensuring predictions remain meaningful and consistent with physical dynamics. Knowledge graphs act as the semantic backbone, integrating heterogeneous sensor data into a connected, queryable structure. At the edge, LLMs generate context-aware rules that adapt filtering and decision-making in real time, enabling efficient operation even under constrained resources. Together, these elements form a foundation for digital twin systems that go beyond passive monitoring to provide actionable insights. By uniting physics-based reasoning, semantic data fusion, and adaptive rule generation, this approach opens new possibilities for creating responsive, trustworthy, and sustainable smart infrastructures.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Ai Framework For Strategic Patent Portfolio Pruning: Integrating Learning To-Rank And Market Need Analysis For Technology Transfer Optimization</title>
<link>https://arxiv.org/abs/2509.00958</link>
<guid>https://arxiv.org/abs/2509.00958</guid>
<content:encoded><![CDATA[
<div> Keywords: hybrid intelligence, patent portfolio, technology transfer, Learning to Rank, Natural Language Processing <br />
Summary: This paper presents a new hybrid intelligence framework for efficiently pruning patent portfolios to identify high-value assets for technology transfer. The framework combines a Learning to Rank model with a "Need-Seed" agent-based system to automate the analysis process. The system includes a "Need Agent" that mines market and industry data using Natural Language Processing to identify technological needs, and a "Seed Agent" that analyzes patent claims using Large Language Models to map technological capabilities. A "Core Ontology Framework" is generated to match high potential patents to documented market demands, aiding in divestment decisions. The architecture includes a dynamic parameter weighting system and a Human in the-Loop validation protocol for adaptability and real-world credibility. The framework enhances the current manual and time-intensive analysis methods by providing a strategic rationale for patent pruning decisions. <br /><br /> <div>
arXiv:2509.00958v1 Announce Type: new 
Abstract: This paper introduces a novel, multi stage hybrid intelligence framework for pruning patent portfolios to identify high value assets for technology transfer. Current patent valuation methods often rely on retrospective indicators or manual, time intensive analysis. Our framework automates and deepens this process by combining a Learning to Rank (LTR) model, which evaluates patents against over 30 legal and commercial parameters, with a unique "Need-Seed" agent-based system. The "Need Agent" uses Natural Language Processing (NLP) to mine unstructured market and industry data, identifying explicit technological needs. Concurrently, the "Seed Agent" employs fine tuned Large Language Models (LLMs) to analyze patent claims and map their technological capabilities. The system generates a "Core Ontology Framework" that matches high potential patents (Seeds) to documented market demands (Needs), providing a strategic rationale for divestment decisions. We detail the architecture, including a dynamic parameter weighting system and a crucial Human in the-Loop (HITL) validation protocol, to ensure both adaptability and real-world credibility.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra Strong Machine Learning: Teaching Humans Active Learning Strategies via Automated AI Explanations</title>
<link>https://arxiv.org/abs/2509.00961</link>
<guid>https://arxiv.org/abs/2509.00961</guid>
<content:encoded><![CDATA[
<div> Neuro-symbolic, Program synthesis, Large language models, Explanation generation, Human learning experiment <br />
Summary: <br />
Ultra Strong Machine Learning (USML) aims to improve machine performance and enhance human learning. LENS is introduced as a neuro-symbolic method that automates the explanation of machine-learned logic programs using large language models (LLMs). This approach outperforms hand-crafted templates and direct LLM prompting in generating explanations. However, in a human learning experiment across various domains, LENS did not significantly improve human performance, indicating that comprehensive LLM responses may be overwhelming for simpler problems. While LENS shows promise in automating explanation generation, its effectiveness in supporting human learning needs further investigation. Overall, this work presents a promising framework for developing efficient USML systems that can enhance both machine performance and human learning. <br /> <div>
arXiv:2509.00961v1 Announce Type: new 
Abstract: Ultra Strong Machine Learning (USML) refers to symbolic learning systems that not only improve their own performance but can also teach their acquired knowledge to quantifiably improve human performance. In this work, we present LENS (Logic Programming Explanation via Neural Summarisation), a neuro-symbolic method that combines symbolic program synthesis with large language models (LLMs) to automate the explanation of machine-learned logic programs in natural language. LENS addresses a key limitation of prior USML approaches by replacing hand-crafted explanation templates with scalable automated generation. Through systematic evaluation using multiple LLM judges and human validation, we demonstrate that LENS generates superior explanations compared to direct LLM prompting and hand-crafted templates. To investigate whether LENS can teach transferable active learning strategies, we carried out a human learning experiment across three related domains. Our results show no significant human performance improvements, suggesting that comprehensive LLM responses may overwhelm users for simpler problems rather than providing learning support. Our work provides a solid foundation for building effective USML systems to support human learning. The source code is available on: https://github.com/lun-ai/LENS.git.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks with LLMs</title>
<link>https://arxiv.org/abs/2509.00971</link>
<guid>https://arxiv.org/abs/2509.00971</guid>
<content:encoded><![CDATA[
<div> Reasoning Layer, General Symbolics, tool-calling, code generation, planning  
Summary:  
CoreThink introduces a state-of-the-art Reasoning Layer called General Symbolics, diverging from existing reasoning paradigms like test-time scaling and Reinforcement Learning. The CoreThink General Symbolic Reasoner excels in tool-calling, code generation, and planning, achieving top scores in benchmarks such as Livecodebench v6 and Instruction-Following Evals. The agentic coding IDE developed using General Symbolics achieves high accuracy in SWE-Bench Lite. These advancements are made without any finetuning or training costs, ensuring pure performance uplift without compromising accuracy. The system is designed to prevent diminishing returns in Large Language Model (LLM) performance. The technical report outlines the approach and availability of CoreThink models for reasoning-intensive tasks.   
<br /><br />Summary: <div>
arXiv:2509.00971v1 Announce Type: new 
Abstract: We introduce CoreThink, a state-of-the-art Reasoning Layer built upon a novel reasoning method called General Symbolics. This approach diverges from reasoning paradigms such as test-time scaling, Supervised Fine-Tuning (SFT), and Reinforcement Learning with Verifiable Rewards (RLVR). CoreThink General Symbolic Reasoner (GSR) is specifically structured around three key use cases: tool-calling, code generation, and planning, demonstrating exemplary performance across a total of seven benchmarks in their respective areas. Notably, we are achieving SOTA scores of 66.66\% on Livecodebench v6, 89\% on Instruction-Following Evals, and 24.4\% on ARC-AGI-2. We also present an agentic coding IDE, developed using the principles of General Symbolics, which achieves a state-of-the-art accuracy of 62.3\% on \texttt{SWE-Bench Lite}. We are able to achieve these improvements without any finetuning or training costs. Our Reasoning Layer is designed to provide a pure performance uplift, ensuring that a model's accuracy on reasoning tasks is never negatively impacted. We argue that incumbent methods will eventually lead to diminishing returns in LLM performance, necessitating the development of new reasoning techniques. This technical report details our approach at a high level and the availability of the CoreThink models for reasoning-intensive use cases.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.00975</link>
<guid>https://arxiv.org/abs/2509.00975</guid>
<content:encoded><![CDATA[
<div> framework, link forecasting, temporal graphs, large language models, explainability
Summary:
The paper introduces ReaL-TG, a reinforcement learning framework that fine-tunes large language models (LLMs) for explainable link forecasting on real-world temporal graphs. It uses outcome-based reward to encourage self-exploration of reasoning strategies and produce explanations for predictions. A new evaluation protocol combines ranking metrics with an LLM-as-a-Judge system to assess reasoning quality and hallucination impact. Experiments show that ReaL-TG-4B outperforms larger LLMs like GPT-5 mini on ranking metrics and provides high-quality explanations validated by both LLM and human evaluation. <div>
arXiv:2509.00975v1 Announce Type: new 
Abstract: Forecasting future links is a central task in temporal graph (TG) reasoning, requiring models to leverage historical interactions to predict upcoming ones. Traditional neural approaches, such as temporal graph neural networks, achieve strong performance but lack explainability and cannot be applied to unseen graphs without retraining. Recent studies have begun to explore using large language models (LLMs) for graph reasoning, but most of them are constrained to static graphs or small synthetic TGs and lack the evaluation of the quality of reasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced Learning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that fine-tunes LLMs to perform explainable link forecasting on real-world TGs. ReaL-TG uses outcome-based reward to encourage models to self-explore reasoning strategies from graph structure and to produce explanations that directly justify their predictions. To enable evaluation on LLM-generated reasoning traces, we propose a new evaluation protocol combining ranking metrics with an LLM-as-a-Judge system that assesses both the quality of reasoning and the impact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning Qwen3-4B under our framework, show that it outperforms much larger frontier LLMs, including GPT-5 mini, on ranking metrics, while producing high-quality explanations confirmed by both the LLM judge and human evaluation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation</title>
<link>https://arxiv.org/abs/2509.00987</link>
<guid>https://arxiv.org/abs/2509.00987</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, causal reasoning, multi-agent systems, causal discovery, estimation

Summary: 
Large Language Models (LLMs) have shown proficiency in reasoning tasks, but struggle with complex causal reasoning. Multi-agent systems, using multiple LLM-based agents, address these limitations by collaborating or specializing. These systems tackle causal reasoning, discovery, and estimation. Architectural patterns vary from pipeline-based to iterative refinement loops. Evaluation methods and benchmarks are diverse, with applications in scientific discovery, healthcare, fact-checking, and personalized systems. Persistent challenges and open research questions exist, with promising future directions. <br /><br />Summary: <div>
arXiv:2509.00987v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning and generation tasks. However, their proficiency in complex causal reasoning, discovery, and estimation remains an area of active development, often hindered by issues like hallucination, reliance on spurious correlations, and difficulties in handling nuanced, domain-specific, or personalized causal relationships. Multi-agent systems, leveraging the collaborative or specialized abilities of multiple LLM-based agents, are emerging as a powerful paradigm to address these limitations. This review paper explores the burgeoning field of causal multi-agent LLMs. We examine how these systems are designed to tackle different facets of causality, including causal reasoning and counterfactual analysis, causal discovery from data, and the estimation of causal effects. We delve into the diverse architectural patterns and interaction protocols employed, from pipeline-based processing and debate frameworks to simulation environments and iterative refinement loops. Furthermore, we discuss the evaluation methodologies, benchmarks, and diverse application domains where causal multi-agent LLMs are making an impact, including scientific discovery, healthcare, fact-checking, and personalized systems. Finally, we highlight the persistent challenges, open research questions, and promising future directions in this synergistic field, aiming to provide a comprehensive overview of its current state and potential trajectory.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supporting Our AI Overlords: Redesigning Data Systems to be Agent-First</title>
<link>https://arxiv.org/abs/2509.00997</link>
<guid>https://arxiv.org/abs/2509.00997</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, agentic speculation, data systems, query processing techniques, agentic memory stores

Summary:
Large Language Model (LLM) agents are expected to dominate data system workloads in the future, utilizing a process called agentic speculation for data manipulation and analysis. The volume and inefficiencies of agentic speculation pose challenges for current data systems, necessitating the adaptation of data systems to better support agentic workloads. The characteristics of agentic speculation, including scale, heterogeneity, redundancy, and steerability, present opportunities for new research in developing an agent-first data systems architecture. These opportunities range from creating new query interfaces to implementing novel query processing techniques and designing agentic memory stores. By leveraging the identified characteristics of agentic speculation, advancements can be made in optimizing data systems to effectively handle the demands of LLM agents and their data exploration and solution formulation processes.

<br /><br />Summary: <div>
arXiv:2509.00997v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents, acting on their users' behalf to manipulate and analyze data, are likely to become the dominant workload for data systems in the future. When working with data, agents employ a high-throughput process of exploration and solution formulation for the given task, one we call agentic speculation. The sheer volume and inefficiencies of agentic speculation can pose challenges for present-day data systems. We argue that data systems need to adapt to more natively support agentic workloads. We take advantage of the characteristics of agentic speculation that we identify, i.e., scale, heterogeneity, redundancy, and steerability - to outline a number of new research opportunities for a new agent-first data systems architecture, ranging from new query interfaces, to new query processing techniques, to new agentic memory stores.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Error Sources in LLM-based Hypothesis Search for Few-Shot Rule Induction</title>
<link>https://arxiv.org/abs/2509.01016</link>
<guid>https://arxiv.org/abs/2509.01016</guid>
<content:encoded><![CDATA[
<div> Keywords: Inductive reasoning, LLM-based hypothesis search, rule induction tasks, program generation, error analysis

Summary:
In this work, the authors compared an LLM-based hypothesis search framework with direct program generation approaches for few-shot rule induction tasks. The study found that hypothesis search achieved performance comparable to human ability in inferring abstract rules from limited examples and applying them in novel situations. Direct program generation, on the other hand, lagged behind in performance. An error analysis identified key bottlenecks in hypothesis generation and suggested avenues for advancing program induction methods. The research underscores the potential of LLM-based hypothesis search for modeling inductive reasoning but also highlights the challenges in constructing more efficient systems. Overall, the study sheds light on the capabilities and limitations of different approaches for inductive reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2509.01016v1 Announce Type: new 
Abstract: Inductive reasoning enables humans to infer abstract rules from limited examples and apply them to novel situations. In this work, we compare an LLM-based hypothesis search framework with direct program generation approaches on few-shot rule induction tasks. Our findings show that hypothesis search achieves performance comparable to humans, while direct program generation falls notably behind. An error analysis reveals key bottlenecks in hypothesis generation and suggests directions for advancing program induction methods. Overall, this paper underscores the potential of LLM-based hypothesis search for modeling inductive reasoning and the challenges in building more efficient systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-like Coherence Derived from the Interaction between Chemical Reaction and Its Environment</title>
<link>https://arxiv.org/abs/2509.01021</link>
<guid>https://arxiv.org/abs/2509.01021</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Natural-born Intelligence, open computing, chemical reactions, quantum coherence<br />
Summary:<br />
The article explores the distinction between Artificial Intelligence and Natural-born Intelligence, categorizing them into closed computing and open computing. Open computing is implemented in chemical reactions, where the computational process and execution environment merge to adapt to fluctuations. The model treats chemical reactions as computation and the environment as molecule interactions, leading to a system that adjusts through clustering and de-clustering. Open computing is further divided into Token computing, focusing on individual molecule behavior, and Type computing, focusing on normative behavior. The interplay between these two types results in self-organizing critical phenomena and quantum logic, leading to quantum-like coherence and the formation of spike waves for signal transmission. This suggests a quantum-like source for controlling biochemical rhythms. <div>
arXiv:2509.01021v1 Announce Type: new 
Abstract: By uncovering the contrast between Artificial Intelligence and Natural-born Intelligence as a computational process, we define closed computing and open computing, and implement open computing within chemical reactions. This involves forming a mixture and invalidation of the computational process and the execution environment, which are logically distinct, and coalescing both to create a system that adjusts fluctuations. We model chemical reactions by considering the computation as the chemical reaction and the execution environment as the degree of aggregation of molecules that interact with the reactive environment. This results in a chemical reaction that progresses while repeatedly clustering and de-clustering, where concentration no longer holds significant meaning. Open computing is segmented into Token computing, which focuses on the individual behavior of chemical molecules, and Type computing, which focuses on normative behavior. Ultimately, both are constructed as an interplay between the two. In this system, Token computing demonstrates self-organizing critical phenomena, while Type computing exhibits quantum logic. Through their interplay, the recruitment of fluctuations is realized, giving rise to interactions between quantum logical subspaces corresponding to quantum coherence across different Hilbert spaces. As a result, spike waves are formed, enabling signal transmission. This occurrence may be termed quantum-like coherence, implying the source of enzymes responsible for controlling spike waves and biochemical rhythms.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Planning and Multi-Agent Path Finding in Extremely Dense Environments with Movable Obstacles</title>
<link>https://arxiv.org/abs/2509.01022</link>
<guid>https://arxiv.org/abs/2509.01022</guid>
<content:encoded><![CDATA[
<div> block rearrangement problem, warehouse management, graph search, search-based solution algorithms, plan quality

Summary:
The article introduces the Block Rearrangement Problem (BRaP) in warehouse management, which involves rearranging storage blocks within dense grids to achieve a target state. BRaP is formally defined as a graph search problem. Five search-based solution algorithms are proposed, drawing inspiration from sliding puzzle problems and utilizing joint configuration space search, classical planning, multi-agent pathfinding, and expert heuristics. Empirical evaluations show that these methods are efficient in creating rearrangement plans even for deeply buried blocks in large grids up to 80x80 dimensions, despite the exponential increase in search space size with the number of blocks. The study emphasizes the importance of efficient algorithms for tackling complex warehouse management tasks such as block rearrangement. 

<br /><br />Summary: <div>
arXiv:2509.01022v1 Announce Type: new 
Abstract: We introduce the Block Rearrangement Problem (BRaP), a challenging component of large warehouse management which involves rearranging storage blocks within dense grids to achieve a target state. We formally define the BRaP as a graph search problem. Building on intuitions from sliding puzzle problems, we propose five search-based solution algorithms, leveraging joint configuration space search, classical planning, multi-agent pathfinding, and expert heuristics. We evaluate the five approaches empirically for plan quality and scalability. Despite the exponential relation between search space size and block number, our methods demonstrate efficiency in creating rearrangement plans for deeply buried blocks in up to 80x80 grids.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games</title>
<link>https://arxiv.org/abs/2509.01052</link>
<guid>https://arxiv.org/abs/2509.01052</guid>
<content:encoded><![CDATA[
<div> benchmark, FlashAdventure, agents, COAST, video games

Summary:
FlashAdventure is a new benchmark consisting of 34 Flash-based adventure games that focus on testing agents' ability to complete full story arcs. The benchmark aims to address the observation-behavior gap by requiring agents to remember and act on earlier gameplay information. The proposed automated gameplay evaluator, CUA-as-a-Judge, and agentic framework, COAST, aim to improve agents' performance by leveraging long-term clue memory for better planning and solving sequential tasks. Experiments show that current GUI agents struggle with completing full storylines, but COAST shows promise in bridging the observation-behavior gap and improving milestone completion. However, there is still a significant gap between the performance of the best-performing agents and human players, highlighting the need for further research to narrow this divide. <div>
arXiv:2509.01052v1 Announce Type: new 
Abstract: GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap: the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</title>
<link>https://arxiv.org/abs/2509.01055</link>
<guid>https://arxiv.org/abs/2509.01055</guid>
<content:encoded><![CDATA[
<div> Framework, Reinforcement Learning, VerlTool, Tool Integration, Multi-Turn Interactions 
Summary: 
VerlTool is a modular framework that combines Reinforcement Learning with Verifiable Rewards (RLVR) and Agentic Reinforcement Learning with Tool use (ARLT) to enable multi-turn interactions with integrated tools. It aligns with VeRL, manages tools through standardized APIs, executes rollouts asynchronously for speedup, and evaluates performance across various domains. The framework supports multi-modal observation tokens and can handle tasks such as mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering. VerlTool offers a unified training infrastructure, competitive results comparable to specialized systems, and a plugin architecture for easy tool integration with lightweight Python definitions, reducing development overhead and fostering tool-augmented RL research. The code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool. 
<br /><br />Summary: <div>
arXiv:2509.01055v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robix: A Unified Model for Robot Interaction, Reasoning and Planning</title>
<link>https://arxiv.org/abs/2509.01106</link>
<guid>https://arxiv.org/abs/2509.01106</guid>
<content:encoded><![CDATA[
<div> reasoning, task planning, natural language interaction, hierarchical robot system, end-to-end framework 

Summary: 
Robix is a unified model that combines robot reasoning, task planning, and natural language interaction in a single architecture. It generates commands for robots and verbal responses for humans, enabling complex instruction following and natural interaction. The model includes proactive dialogue, interruption handling, and context-aware commonsense reasoning during task execution. Robix is trained in three stages: continued pretraining for foundational reasoning abilities, supervised finetuning for human-robot interaction modeling, and reinforcement learning for long-horizon task coherence. In experiments, Robix outperforms baselines in interactive task execution across various instruction types and user-involved tasks such as table bussing, grocery shopping, and dietary filtering. <div>
arXiv:2509.01106v1 Announce Type: new 
Abstract: We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heads or Tails: A Simple Example of Causal Abstractive Simulation</title>
<link>https://arxiv.org/abs/2509.01136</link>
<guid>https://arxiv.org/abs/2509.01136</guid>
<content:encoded><![CDATA[
<div> Keywords: causal abstraction, simulation, language models, causality, philosophers<br />
Summary: <br />
This note introduces the concept of causal abstractive simulation, which is a formalization method for language model simulation. It discusses how language models can simulate a fair coin toss and presents examples of both successful and unsuccessful simulations. The formalism of causal abstractive simulation can be used to prove that a language model accurately simulates another system, based on a causal description of the system. This approach connects statistical benchmarking practices in language model simulation to a solid foundation of causality. It may be of interest to practitioners in the field of language model simulation, as well as philosophers of AI and mind who are interested in the role-playing aspect of language models. Additionally, mathematicians and researchers working on causal abstraction can find a new application of core ideas in this variation of causal abstraction. <div>
arXiv:2509.01136v1 Announce Type: new 
Abstract: This note illustrates how a variety of causal abstraction arXiv:1707.00819 arXiv:1812.03789, defined here as causal abstractive simulation, can be used to formalize a simple example of language model simulation. This note considers the case of simulating a fair coin toss with a language model. Examples are presented illustrating the ways language models can fail to simulate, and a success case is presented, illustrating how this formalism may be used to prove that a language model simulates some other system, given a causal description of the system. This note may be of interest to three groups. For practitioners in the growing field of language model simulation, causal abstractive simulation is a means to connect ad-hoc statistical benchmarking practices to the solid formal foundation of causality. Philosophers of AI and philosophers of mind may be interested as causal abstractive simulation gives a precise operationalization to the idea that language models are role-playing arXiv:2402.12422. Mathematicians and others working on causal abstraction may be interested to see a new application of the core ideas that yields a new variation of causal abstraction.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Question-to-Knowledge: Multi-Agent Generation of Inspectable Facts for Product Mapping</title>
<link>https://arxiv.org/abs/2509.01182</link>
<guid>https://arxiv.org/abs/2509.01182</guid>
<content:encoded><![CDATA[
<div> Keywords: ecommerce, SKU mapping, Large Language Models, product integration, disambiguation <br />
Summary: <br />
Identifying product listings referring to the same SKU in ecommerce is challenging due to missing identifiers and varied product names. The Q2K framework leverages Large Language Models for accurate SKU mapping by using a Reasoning Agent to generate questions, a Knowledge Agent to resolve them through web searches, and a Deduplication Agent to reduce redundancy. Human input refines uncertain cases. Q2K outperforms baselines in accuracy and robustness, particularly in bundle and brand disambiguation. By reusing reasoning, Q2K balances efficiency and accuracy, offering a scalable solution for product integration. <div>
arXiv:2509.01182v1 Announce Type: new 
Abstract: Identifying whether two product listings refer to the same Stock Keeping Unit (SKU) is a persistent challenge in ecommerce, especially when explicit identifiers are missing and product names vary widely across platforms. Rule based heuristics and keyword similarity often misclassify products by overlooking subtle distinctions in brand, specification, or bundle configuration. To overcome these limitations, we propose Question to Knowledge (Q2K), a multi agent framework that leverages Large Language Models (LLMs) for reliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates targeted disambiguation questions, (2) a Knowledge Agent that resolves them via focused web searches, and (3) a Deduplication Agent that reuses validated reasoning traces to reduce redundancy and ensure consistency. A human in the loop mechanism further refines uncertain cases. Experiments on real world consumer goods datasets show that Q2K surpasses strong baselines, achieving higher accuracy and robustness in difficult scenarios such as bundle identification and brand origin disambiguation. By reusing retrieved reasoning instead of issuing repeated searches, Q2K balances accuracy with efficiency, offering a scalable and interpretable solution for product integration.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Open-World Retrieval-Augmented Generation on Knowledge Graph: A Multi-Agent Collaboration Framework</title>
<link>https://arxiv.org/abs/2509.01238</link>
<guid>https://arxiv.org/abs/2509.01238</guid>
<content:encoded><![CDATA[
<div> Knowledge Graphs, Large Language Models, Retrieval-Augmented Generation, AnchorRAG, Multi-agent Collaboration

Summary:
AnchorRAG is introduced as a novel framework for open-world Retrieval-Augmented Generation (RAG) that does not rely on predefined anchor entities. This multi-agent collaboration approach involves a predictor agent for identifying candidate anchor entities, retriever agents for conducting multi-hop explorations, and a supervisor agent to formulate retrieval strategies. By dynamically aligning query terms with Knowledge Graph nodes, AnchorRAG improves retrieval robustness and mitigates errors in anchor entity linking. Extensive experiments on public benchmarks show that AnchorRAG outperforms existing baselines, achieving new state-of-the-art results in real-world question answering tasks. <div>
arXiv:2509.01238v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in language understanding and reasoning. However, their dependence on static training corpora makes them prone to factual errors and knowledge gaps. Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating external knowledge sources, especially structured Knowledge Graphs (KGs), which provide explicit semantics and efficient retrieval. Existing KG-based RAG approaches, however, generally assume that anchor entities are accessible to initiate graph traversal, which limits their robustness in open world settings where accurate linking between the query and the entity is unreliable. To overcome this limitation, we propose AnchorRAG, a novel multi-agent collaboration framework for open-world RAG without the predefined anchor entities. Specifically, a predictor agent dynamically identifies candidate anchor entities by aligning user query terms with KG nodes and initializes independent retriever agents to conduct parallel multi-hop explorations from each candidate. Then a supervisor agent formulates the iterative retrieval strategy for these retriever agents and synthesizes the resulting knowledge paths to generate the final answer. This multi-agent collaboration framework improves retrieval robustness and mitigates the impact of ambiguous or erroneous anchors. Extensive experiments on four public benchmarks demonstrate that AnchorRAG significantly outperforms existing baselines and establishes new state-of-the-art results on the real-world question answering tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Agentic OS: An LLM Agent Framework for Linux Schedulers</title>
<link>https://arxiv.org/abs/2509.01245</link>
<guid>https://arxiv.org/abs/2509.01245</guid>
<content:encoded><![CDATA[
arXiv:2509.01245v1 Announce Type: new 
Abstract: Operating system schedulers suffer from a fundamental semantic gap, where kernel policies fail to understand application-specific needs, leading to suboptimal performance. We introduce SchedCP, the first framework that enables fully autonomous Large Language Model (LLM) agents to safely and efficiently optimize Linux schedulers without human involvement. Our core insight is that the challenge is not merely to apply a better LLM, but to architect a decoupled control plane that separates the AI's role of semantic reasoning ("what to optimize") from the system's role of execution ("how to observe and act"). Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable interface with three key services: a Workload Analysis Engine, an evolving Scheduler Policy Repository, and an Execution Verifier that validates all AI-generated code and configure before deployment with static and dynamic analysis.
  We demonstrate this architecture's power with sched-agent, a multi-agent system that autonomously analyzes workloads, synthesizes custom eBPF scheduling policies, and deploys them via the sched\_ext infrastructure. Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. By bridging the semantic gap, SchedCP democratizes expert-level system optimization and represents a step towards creating truly self-optimizing, application-aware operating systems. The code is open-sourced in https://github.com/eunomia-bpf/schedcp
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communicative Agents for Slideshow Storytelling Video Generation based on LLMs</title>
<link>https://arxiv.org/abs/2509.01277</link>
<guid>https://arxiv.org/abs/2509.01277</guid>
<content:encoded><![CDATA[
arXiv:2509.01277v1 Announce Type: new 
Abstract: With the rapid advancement of artificial intelligence (AI), the proliferation of AI-generated content (AIGC) tasks has significantly accelerated developments in text-to-video generation. As a result, the field of video production is undergoing a transformative shift. However, conventional text-to-video models are typically constrained by high computational costs.
  In this study, we propose Video-Generation-Team (VGTeam), a novel slide show video generation system designed to redefine the video creation pipeline through the integration of large language models (LLMs). VGTeam is composed of a suite of communicative agents, each responsible for a distinct aspect of video generation, such as scriptwriting, scene creation, and audio design. These agents operate collaboratively within a chat tower workflow, transforming user-provided textual prompts into coherent, slide-style narrative videos.
  By emulating the sequential stages of traditional video production, VGTeam achieves remarkable improvements in both efficiency and scalability, while substantially reducing computational overhead. On average, the system generates videos at a cost of only $0.103, with a successful generation rate of 98.4%. Importantly, this framework maintains a high degree of creative fidelity and customization.
  The implications of VGTeam are far-reaching. It democratizes video production by enabling broader access to high-quality content creation without the need for extensive resources. Furthermore, it highlights the transformative potential of language models in creative domains and positions VGTeam as a pioneering system for next-generation content creation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradeSQL: Outcome Reward Models for Ranking SQL Queries from Large Language Models</title>
<link>https://arxiv.org/abs/2509.01308</link>
<guid>https://arxiv.org/abs/2509.01308</guid>
<content:encoded><![CDATA[
arXiv:2509.01308v1 Announce Type: new 
Abstract: Text-to-SQL, the task of translating natural language questions into SQL queries, has significantly advanced with the introduction of Large Language Models (LLMs), broadening database accessibility for a wide range of users. Despite substantial progress in generating valid SQL, current LLMs still struggle with complex queries that require precise alignment between user intent and the database schema. To mitigate this, test-time strategies such as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the assumption that LLMs can generate correct answers but may require multiple attempts. However, these methods rely on surface-level heuristics, selecting either the syntactically correct query through execution-based BoN (ex-BoN) or the most frequently generated query with Maj. Recently, Outcome Reward Models (ORMs), which assign utility scores to generated outputs based on semantic correctness, have emerged as a promising approach for better aligning model predictions with user intent. Nevertheless, their application to Text-to-SQL remains largely underexplored.
  In this work, we evaluate ORMs as an effective heuristic for BoN, compare them with ex-BoN and Maj, and introduce a framework for training ORMs for the Text-to-SQL task. We evaluate our ORMs on the BIRD and SPIDER benchmarks, finetuning various open-source LLMs, including the Qwen2, Granite3, and Llama3 model families. Our results show that ORMs outperform ex-BoN and Maj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that finetuning models already aligned with SQL generation, such as OmniSQL, yields superior ORM performance. Additionally, we observe that ORMs achieve competitive results on simple queries and benefit more from an increased number of candidates compared to ex-BoN and Maj.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Predictive Monitoring for Multi-Modal Scenarios</title>
<link>https://arxiv.org/abs/2509.01338</link>
<guid>https://arxiv.org/abs/2509.01338</guid>
<content:encoded><![CDATA[
arXiv:2509.01338v1 Announce Type: new 
Abstract: We consider the problem of quantitative predictive monitoring (QPM) of stochastic systems, i.e., predicting at runtime the degree of satisfaction of a desired temporal logic property from the current state of the system. Since computational efficiency is key to enable timely intervention against predicted violations, several state-of-the-art QPM approaches rely on fast machine-learning surrogates to provide prediction intervals for the satisfaction values, using conformal inference to offer statistical guarantees. However, these QPM methods suffer when the monitored agent exhibits multi-modal dynamics, whereby certain modes may yield high satisfaction values while others critically violate the property. Existing QPM methods are mode-agnostic and so would yield overly conservative and uninformative intervals that lack meaningful mode-specific satisfaction information. To address this problem, we present GenQPM, a method that leverages deep generative models, specifically score-based diffusion models, to reliably approximate the probabilistic and multi-modal system dynamics without requiring explicit model access. GenQPM employs a mode classifier to partition the predicted trajectories by dynamical mode. For each mode, we then apply conformal inference to produce statistically valid, mode-specific prediction intervals. We demonstrate the effectiveness of GenQPM on a benchmark of agent navigation and autonomous driving tasks, resulting in prediction intervals that are significantly more informative (less conservative) than mode-agnostic baselines.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Notebook-Guided, Training-Free Part Retrieval in 3D CAD Assemblies via Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.01350</link>
<guid>https://arxiv.org/abs/2509.01350</guid>
<content:encoded><![CDATA[
arXiv:2509.01350v1 Announce Type: new 
Abstract: Effective specification-aware part retrieval within complex CAD assemblies is essential for automated design verification and downstream engineering tasks. However, directly using LLMs/VLMs to this task presents some challenges: the input sequences may exceed model token limits, and even after processing, performance remains unsatisfactory. Moreover, fine-tuning LLMs/VLMs requires significant computational resources, and for many high-performing general-use proprietary models (e.g., GPT or Gemini), fine-tuning access is not available. In this paper, we propose a novel part retrieval framework that requires no extra training, but using Error Notebooks + RAG for refined prompt engineering to help improve the existing general model's retrieval performance. The construction of Error Notebooks consists of two steps: (1) collecting historical erroneous CoTs and their incorrect answers, and (2) connecting these CoTs through reflective corrections until the correct solutions are obtained. As a result, the Error Notebooks serve as a repository of tasks along with their corrected CoTs and final answers. RAG is then employed to retrieve specification-relevant records from the Error Notebooks and incorporate them into the inference process. Another major contribution of our work is a human-in-the-loop CAD dataset, which is used to evaluate our method. In addition, the engineering value of our novel framework lies in its ability to effectively handle 3D models with lengthy, non-natural language metadata. Experiments with proprietary models, including GPT-4o and the Gemini series, show substantial gains, with GPT-4o (Omni) achieving up to a 23.4% absolute accuracy improvement on the human preference dataset. Moreover, ablation studies confirm that CoT reasoning provides benefits especially in challenging cases with higher part counts (>10).
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks</title>
<link>https://arxiv.org/abs/2509.01396</link>
<guid>https://arxiv.org/abs/2509.01396</guid>
<content:encoded><![CDATA[
arXiv:2509.01396v1 Announce Type: new 
Abstract: Deep research agents have attracted growing attention for their potential to orchestrate multi-stage research workflows, spanning literature synthesis, methodological design, and empirical verification. Despite these strides, evaluating their research capability faithfully is rather challenging due to the difficulty of collecting frontier research questions that genuinely capture researchers' attention and intellectual curiosity. To address this gap, we introduce DeepResearch Arena, a benchmark grounded in academic seminars that capture rich expert discourse and interaction, better reflecting real-world research environments and reducing the risk of data leakage. To automatically construct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task Generation (MAHTG) system that extracts research-worthy inspirations from seminar transcripts. The MAHTG system further translates research-worthy inspirations into high-quality research tasks, ensuring the traceability of research task formulation while filtering noise. With the MAHTG system, we curate DeepResearch Arena with over 10,000 high-quality research tasks from over 200 academic seminars, spanning 12 disciplines, such as literature, history, and science. Our extensive evaluation shows that DeepResearch Arena presents substantial challenges for current state-of-the-art agents, with clear performance gaps observed across different models.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Need for Verification in AI-Driven Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.01398</link>
<guid>https://arxiv.org/abs/2509.01398</guid>
<content:encoded><![CDATA[
arXiv:2509.01398v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is transforming the practice of science. Machine learning and large language models (LLMs) can generate hypotheses at a scale and speed far exceeding traditional methods, offering the potential to accelerate discovery across diverse fields. However, the abundance of hypotheses introduces a critical challenge: without scalable and reliable mechanisms for verification, scientific progress risks being hindered rather than being advanced. In this article, we trace the historical development of scientific discovery, examine how AI is reshaping established practices for scientific discovery, and review the principal approaches, ranging from data-driven methods and knowledge-aware neural architectures to symbolic reasoning frameworks and LLM agents. While these systems can uncover patterns and propose candidate laws, their scientific value ultimately depends on rigorous and transparent verification, which we argue must be the cornerstone of AI-assisted discovery.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-empowered Agents Simulation Framework for Scenario Generation in Service Ecosystem Governance</title>
<link>https://arxiv.org/abs/2509.01441</link>
<guid>https://arxiv.org/abs/2509.01441</guid>
<content:encoded><![CDATA[
arXiv:2509.01441v1 Announce Type: new 
Abstract: As the social environment is growing more complex and collaboration is deepening, factors affecting the healthy development of service ecosystem are constantly changing and diverse, making its governance a crucial research issue. Applying the scenario analysis method and conducting scenario rehearsals by constructing an experimental system before managers make decisions, losses caused by wrong decisions can be largely avoided. However, it relies on predefined rules to construct scenarios and faces challenges such as limited information, a large number of influencing factors, and the difficulty of measuring social elements. These challenges limit the quality and efficiency of generating social and uncertain scenarios for the service ecosystem. Therefore, we propose a scenario generator design method, which adaptively coordinates three Large Language Model (LLM) empowered agents that autonomously optimize experimental schemes to construct an experimental system and generate high quality scenarios. Specifically, the Environment Agent (EA) generates social environment including extremes, the Social Agent (SA) generates social collaboration structure, and the Planner Agent (PA) couples task-role relationships and plans task solutions. These agents work in coordination, with the PA adjusting the experimental scheme in real time by perceiving the states of each agent and these generating scenarios. Experiments on the ProgrammableWeb dataset illustrate our method generates more accurate scenarios more efficiently, and innovatively provides an effective way for service ecosystem governance related experimental system construction.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Sensitivity for Faithful Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2509.01544</link>
<guid>https://arxiv.org/abs/2509.01544</guid>
<content:encoded><![CDATA[
arXiv:2509.01544v1 Announce Type: new 
Abstract: Large language models (LLMs) often produce correct answers while relying on flawed or irrelevant reasoning traces, undermining their trustworthiness in high-stakes domains. We propose Counterfactual Sensitivity Regularization (CSR), a lightweight training objective that enforces dependence between intermediate reasoning and final outputs. CSR introduces automated, operator-level counterfactual interventions (e.g., swapping "+" with "-") during training and penalizes models that preserve the same answer under logically invalid traces. This requires only one additional forward pass per sample. To measure faithfulness, we introduce Counterfactual Outcome Sensitivity (COS), which quantifies the impact of such perturbations on model predictions. Across structured reasoning tasks - arithmetic (GSM8K), logical deduction (PrOntoQA), and planning (Blocks World) - CSR improves faithfulness by up to 70 percentage points over standard fine-tuning and process supervision, with only minor accuracy loss. The learned sensitivity generalizes to larger models and synergizes with inference-time methods such as self-consistency. A pilot study on HellaSwag further demonstrates that extending CSR with semantic perturbations can enhance faithfulness in commonsense reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured AI Decision-Making in Disaster Management</title>
<link>https://arxiv.org/abs/2509.01576</link>
<guid>https://arxiv.org/abs/2509.01576</guid>
<content:encoded><![CDATA[
arXiv:2509.01576v1 Announce Type: new 
Abstract: With artificial intelligence (AI) being applied to bring autonomy to decision-making in safety-critical domains such as the ones typified in the aerospace and emergency-response services, there has been a call to address the ethical implications of structuring those decisions, so they remain reliable and justifiable when human lives are at stake. This paper contributes to addressing the challenge of decision-making by proposing a structured decision-making framework as a foundational step towards responsible AI. The proposed structured decision-making framework is implemented in autonomous decision-making, specifically within disaster management. By introducing concepts of Enabler agents, Levels and Scenarios, the proposed framework's performance is evaluated against systems relying solely on judgement-based insights, as well as human operators who have disaster experience: victims, volunteers, and stakeholders. The results demonstrate that the structured decision-making framework achieves 60.94% greater stability in consistently accurate decisions across multiple Scenarios, compared to judgement-based systems. Moreover, the study shows that the proposed framework outperforms human operators with a 38.93% higher accuracy across various Scenarios. These findings demonstrate the promise of the structured decision-making framework for building more reliable autonomous AI applications in safety-critical contexts.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Throttling Web Agents Using Reasoning Gates</title>
<link>https://arxiv.org/abs/2509.01619</link>
<guid>https://arxiv.org/abs/2509.01619</guid>
<content:encoded><![CDATA[
arXiv:2509.01619v1 Announce Type: new 
Abstract: AI web agents use Internet resources at far greater speed, scale, and complexity -- changing how users and services interact. Deployed maliciously or erroneously, these agents could overload content providers. At the same time, web agents can bypass CAPTCHAs and other defenses by mimicking user behavior or flood authentication systems with fake accounts. Yet providers must protect their services and content from denial-of-service attacks and scraping by web agents. In this paper, we design a framework that imposes tunable costs on agents before providing access to resources; we call this Web Agent Throttling. We start by formalizing Throttling Gates as challenges issued to an agent that are asymmetric, scalable, robust, and compatible with any agent. Focusing on a common component -- the language model -- we require the agent to solve reasoning puzzles, thereby incurring excessive token-generation costs. However, we find that using existing puzzles, e.g., coding or math, as throttling gates fails to satisfy our properties. To address this, we introduce rebus-based Reasoning Gates, synthetic text puzzles that require multi-hop reasoning over world knowledge (thereby throttling an agent's model). We design a scalable generation and verification protocol for such reasoning gates. Our framework achieves computational asymmetry, i.e., the response-generation cost is 9.2x higher than the generation cost for SOTA models. We further deploy reasoning gates on a custom website and Model Context Protocol (MCP) servers and evaluate with real-world web agents. Finally, we discuss the limitations and environmental impact of real-world deployment of our framework.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling LLM Jailbreaks Through Safety Knowledge Neurons</title>
<link>https://arxiv.org/abs/2509.01631</link>
<guid>https://arxiv.org/abs/2509.01631</guid>
<content:encoded><![CDATA[
arXiv:2509.01631v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly attracting attention in various applications. Nonetheless, there is a growing concern as some users attempt to exploit these models for malicious purposes, including the synthesis of controlled substances and the propagation of disinformation, a technique known as "Jailbreak." While some studies have achieved defenses against jailbreak attacks by modifying output distributions or detecting harmful content, the exact rationale still remains elusive. In this work, we present a novel neuron-level interpretability method that focuses on the role of safety-related knowledge neurons. Unlike existing approaches, our method projects the model's internal representation into a more consistent and interpretable vocabulary space. We then show that adjusting the activation of safety-related neurons can effectively control the model's behavior with a mean ASR higher than 97%. Building on this insight, we propose SafeTuning, a fine-tuning strategy that reinforces safety-critical neurons to improve model robustness against jailbreaks. SafeTuning consistently reduces attack success rates across multiple LLMs and outperforms all four baseline defenses. These findings offer a new perspective on understanding and defending against jailbreak attacks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics Supernova: AI Agent Matches Elite Gold Medalists at IPhO 2025</title>
<link>https://arxiv.org/abs/2509.01659</link>
<guid>https://arxiv.org/abs/2509.01659</guid>
<content:encoded><![CDATA[
arXiv:2509.01659v1 Announce Type: new 
Abstract: Physics provides fundamental laws that describe and predict the natural world. AI systems aspiring toward more general, real-world intelligence must therefore demonstrate strong physics problem-solving abilities: to formulate and apply physical laws for explaining and predicting physical processes. The International Physics Olympiad (IPhO)--the world's most prestigious physics competition--offers a rigorous benchmark for this purpose. We introduce Physics Supernova, an AI agent system with superior physics problem-solving abilities that match elite IPhO gold medalists. In IPhO 2025 theory problems, Physics Supernova attains 23.5/30 points, ranking 14th of 406 contestants and surpassing the median performance of human gold medalists. We extensively analyzed Physics Supernova's capabilities and flexibility across diverse physics tasks. These results show that principled tool integration within agent systems can deliver competitive improvements in solving challenging science problems. The codes are available at https://github.com/CharlesQ9/Physics-Supernova.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-enabled semantic-centric framework to consume privacy policies</title>
<link>https://arxiv.org/abs/2509.01716</link>
<guid>https://arxiv.org/abs/2509.01716</guid>
<content:encoded><![CDATA[
arXiv:2509.01716v1 Announce Type: new 
Abstract: In modern times, people have numerous online accounts, but they rarely read the Terms of Service or Privacy Policy of those sites, despite claiming otherwise, due to the practical difficulty in comprehending them. The mist of data privacy practices forms a major barrier for user-centred Web approaches, and for data sharing and reusing in an agentic world. Existing research proposed methods for using formal languages and reasoning for verifying the compliance of a specified policy, as a potential cure for ignoring privacy policies. However, a critical gap remains in the creation or acquisition of such formal policies at scale. We present a semantic-centric approach for using state-of-the-art large language models (LLM), to automatically identify key information about privacy practices from privacy policies, and construct $\mathit{Pr}^2\mathit{Graph}$, knowledge graph with grounding from Data Privacy Vocabulary (DPV) for privacy practices, to support downstream tasks. Along with the pipeline, the $\mathit{Pr}^2\mathit{Graph}$ for the top-100 popular websites is also released as a public resource, by using the pipeline for analysis. We also demonstrate how the $\mathit{Pr}^2\mathit{Graph}$ can be used to support downstream tasks by constructing formal policy representations such as Open Digital Right Language (ODRL) or perennial semantic Data Terms of Use (psDToU). To evaluate the technology capability, we enriched the Policy-IE dataset by employing legal experts to create custom annotations. We benchmarked the performance of different large language models for our pipeline and verified their capabilities. Overall, they shed light on the possibility of large-scale analysis of online services' privacy practices, as a promising direction to audit the Web and the Internet. We release all datasets and source code as public resources to facilitate reuse and improvement.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title>
<link>https://arxiv.org/abs/2509.01909</link>
<guid>https://arxiv.org/abs/2509.01909</guid>
<content:encoded><![CDATA[
arXiv:2509.01909v1 Announce Type: new 
Abstract: Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Real Is AI Tutoring? Comparing Simulated and Human Dialogues in One-on-One Instruction</title>
<link>https://arxiv.org/abs/2509.01914</link>
<guid>https://arxiv.org/abs/2509.01914</guid>
<content:encoded><![CDATA[
arXiv:2509.01914v1 Announce Type: new 
Abstract: Heuristic and scaffolded teacher-student dialogues are widely regarded as critical for fostering students' higher-order thinking and deep learning. However, large language models (LLMs) currently face challenges in generating pedagogically rich interactions. This study systematically investigates the structural and behavioral differences between AI-simulated and authentic human tutoring dialogues. We conducted a quantitative comparison using an Initiation-Response-Feedback (IRF) coding scheme and Epistemic Network Analysis (ENA). The results show that human dialogues are significantly superior to their AI counterparts in utterance length, as well as in questioning (I-Q) and general feedback (F-F) behaviors. More importantly, ENA results reveal a fundamental divergence in interactional patterns: human dialogues are more cognitively guided and diverse, centered around a "question-factual response-feedback" teaching loop that clearly reflects pedagogical guidance and student-driven thinking; in contrast, simulated dialogues exhibit a pattern of structural simplification and behavioral convergence, revolving around an "explanation-simplistic response" loop that is essentially a simple information transfer between the teacher and student. These findings illuminate key limitations in current AI-generated tutoring and provide empirical guidance for designing and evaluating more pedagogically effective generative educational dialogue systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Speculative Agent Planning</title>
<link>https://arxiv.org/abs/2509.01920</link>
<guid>https://arxiv.org/abs/2509.01920</guid>
<content:encoded><![CDATA[
arXiv:2509.01920v1 Announce Type: new 
Abstract: Despite their remarkable success in complex tasks propelling widespread adoption, large language-model-based agents still face critical deployment challenges due to prohibitive latency and inference costs. While recent work has explored various methods to accelerate inference, existing approaches suffer from significant limitations: they either fail to preserve performance fidelity, require extensive offline training of router modules, or incur excessive operational costs. Moreover, they provide minimal user control over the tradeoff between acceleration and other performance metrics. To address these gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous online reinforcement learning framework that provides lossless acceleration with substantially reduced costs without requiring additional pre-deployment preparation. DSP explicitly optimizes a joint objective balancing end-to-end latency against dollar cost, allowing practitioners to adjust a single parameter that steers the system toward faster responses, cheaper operation, or any point along this continuum. Experiments on two standard agent benchmarks demonstrate that DSP achieves comparable efficiency to the fastest lossless acceleration method while reducing total cost by 30% and unnecessary cost up to 60%. Our code and data are available through https://github.com/guanyilin428/Dynamic-Speculative-Planning.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title>
<link>https://arxiv.org/abs/2509.01938</link>
<guid>https://arxiv.org/abs/2509.01938</guid>
<content:encoded><![CDATA[
arXiv:2509.01938v1 Announce Type: new 
Abstract: Aligning AI with human values is a pressing unsolved problem. To address the lack of quantitative metrics for value alignment, we propose EigenBench: a black-box method for comparatively benchmarking language models' values. Given an ensemble of models, a constitution describing a value system, and a dataset of scenarios, our method returns a vector of scores quantifying each model's alignment to the given constitution. To produce these scores, each model judges the outputs of other models across many scenarios, and these judgments are aggregated with EigenTrust (Kamvar et al, 2003), yielding scores that reflect a weighted-average judgment of the whole ensemble. EigenBench uses no ground truth labels, as it is designed to quantify traits for which reasonable judges may disagree on the correct label. Using prompted personas, we test whether EigenBench scores are more sensitive to the model or the prompt: we find that most of the variance is explained by the prompt, but a small residual quantifies the disposition of the model itself.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in Clinical Decision Support</title>
<link>https://arxiv.org/abs/2509.02007</link>
<guid>https://arxiv.org/abs/2509.02007</guid>
<content:encoded><![CDATA[
arXiv:2509.02007v1 Announce Type: new 
Abstract: The deployment of Large Language Models (LLMs) in high-stakes medical settings poses a critical AI alignment challenge, as models can inherit and amplify societal biases, leading to significant disparities. Existing fairness evaluation methods fall short in these contexts as they typically use simplistic metrics that overlook the multi-dimensional nature of medical harms. This also promotes models that are fair only because they are clinically inert, defaulting to safe but potentially inaccurate outputs. To address this gap, our contributions are mainly two-fold: first, we construct two large-scale, controlled benchmarks (ED-Triage and Opioid Analgesic Recommendation) from MIMIC-IV, comprising over 50,000 prompts with twelve race x gender variants and three context tiers. Second, we propose a multi-metric framework - Multi-faceted Fairness Assessment based on hARMs ($mFARM$) to audit fairness for three distinct dimensions of disparity (Allocational, Stability, and Latent) and aggregate them into an $mFARM$ score. We also present an aggregated Fairness-Accuracy Balance (FAB) score to benchmark and observe trade-offs between fairness and prediction accuracy. We empirically evaluate four open-source LLMs (Mistral-7B, BioMistral-7B, Qwen-2.5-7B, Bio-LLaMA3-8B) and their finetuned versions under quantization and context variations. Our findings showcase that the proposed $mFARM$ metrics capture subtle biases more effectively under various settings. We find that most models maintain robust performance in terms of $mFARM$ score across varying levels of quantization but deteriorate significantly when the context is reduced. Our benchmarks and evaluation code are publicly released to enhance research in aligned AI for healthcare.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative KI f\"ur TA</title>
<link>https://arxiv.org/abs/2509.02053</link>
<guid>https://arxiv.org/abs/2509.02053</guid>
<content:encoded><![CDATA[
arXiv:2509.02053v1 Announce Type: new 
Abstract: Many scientists use generative AI in their scientific work. People working in technology assessment (TA) are no exception. TA's approach to generative AI is twofold: on the one hand, generative AI is used for TA work, and on the other hand, generative AI is the subject of TA research. After briefly outlining the phenomenon of generative AI and formulating requirements for its use in TA, the following article discusses in detail the structural causes of the problems associated with it. Although generative AI is constantly being further developed, the structurally induced risks remain. The article concludes with proposed solutions and brief notes on their feasibility, as well as some examples of the use of generative AI in TA work.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGI as Second Being: The Structural-Generative Ontology of Intelligence</title>
<link>https://arxiv.org/abs/2509.02089</link>
<guid>https://arxiv.org/abs/2509.02089</guid>
<content:encoded><![CDATA[
arXiv:2509.02089v1 Announce Type: new 
Abstract: Artificial intelligence is often measured by the range of tasks it can perform. Yet wide ability without depth remains only an imitation. This paper proposes a Structural-Generative Ontology of Intelligence: true intelligence exists only when a system can generate new structures, coordinate them into reasons, and sustain its identity over time. These three conditions -- generativity, coordination, and sustaining -- define the depth that underlies real intelligence. Current AI systems, however broad in function, remain surface simulations because they lack this depth. Breadth is not the source of intelligence but the growth that follows from depth. If future systems were to meet these conditions, they would no longer be mere tools, but could be seen as a possible Second Being, standing alongside yet distinct from human existence.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs for LLMs: A Structured Prompting Methodology for Long Legal Documents</title>
<link>https://arxiv.org/abs/2509.02241</link>
<guid>https://arxiv.org/abs/2509.02241</guid>
<content:encoded><![CDATA[
arXiv:2509.02241v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has had a profoundly transformative effect on a number of fields and domains. However, their uptake in Law has proven more challenging due to the important issues of reliability and transparency. In this study, we present a structured prompting methodology as a viable alternative to the often expensive fine-tuning, with the capability of tacking long legal documents from the CUAD dataset on the task of information retrieval. Each document is first split into chunks via a system of chunking and augmentation, addressing the long document problem. Then, alongside an engineered prompt, the input is fed into QWEN-2 to produce a set of answers for each question. Finally, we tackle the resulting candidate selection problem with the introduction of the Distribution-based Localisation and Inverse Cardinality Weighting heuristics. This approach leverages a general purpose model to promote long term scalability, prompt engineering to increase reliability and the two heuristic strategies to reduce the impact of the black box effect. Whilst our model performs up to 9\% better than the previously presented method, reaching state-of-the-art performance, it also highlights the limiting factor of current automatic evaluation metrics for question answering, serving as a call to action for future research. However, the chief aim of this work is to underscore the potential of structured prompt engineering as a useful, yet under-explored, tool in ensuring accountability and responsibility of AI in the legal domain, and beyond.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Epidemiological Knowledge Graph extracted from the World Health Organization's Disease Outbreak News</title>
<link>https://arxiv.org/abs/2509.02258</link>
<guid>https://arxiv.org/abs/2509.02258</guid>
<content:encoded><![CDATA[
arXiv:2509.02258v1 Announce Type: new 
Abstract: The rapid evolution of artificial intelligence (AI), together with the increased availability of social media and news for epidemiological surveillance, are marking a pivotal moment in epidemiology and public health research. Leveraging the power of generative AI, we use an ensemble approach which incorporates multiple Large Language Models (LLMs) to extract valuable actionable epidemiological information from the World Health Organization (WHO) Disease Outbreak News (DONs). DONs is a collection of regular reports on global outbreaks curated by the WHO and the adopted decision-making processes to respond to them. The extracted information is made available in a daily-updated dataset and a knowledge graph, referred to as eKG, derived to provide a nuanced representation of the public health domain knowledge. We provide an overview of this new dataset and describe the structure of eKG, along with the services and tools used to access and utilize the data that we are building on top. These innovative data resources open altogether new opportunities for epidemiological research, and the analysis and surveillance of disease outbreaks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rewarding Explainability in Drug Repurposing with Knowledge Graphs</title>
<link>https://arxiv.org/abs/2509.02276</link>
<guid>https://arxiv.org/abs/2509.02276</guid>
<content:encoded><![CDATA[
arXiv:2509.02276v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) are powerful tools for modelling complex, multi-relational data and supporting hypothesis generation, particularly in applications like drug repurposing. However, for predictive methods to gain acceptance as credible scientific tools, they must ensure not only accuracy but also the capacity to offer meaningful scientific explanations. This paper presents a novel approach REx, for generating scientific explanations based in link prediction in knowledge graphs. It employs reward and policy mechanisms that consider desirable properties of scientific explanation to guide a reinforcement learning agent in the identification of explanatory paths within a KG. The approach further enriches explanatory paths with domain-specific ontologies, ensuring that the explanations are both insightful and grounded in established biomedical knowledge. We evaluate our approach in drug repurposing using three popular knowledge graph benchmarks. The results clearly demonstrate its ability to generate explanations that validate predictive insights against biomedical knowledge and that outperform the state-of-the-art approaches in predictive performance, establishing REx as a relevant contribution to advance AI-driven scientific discovery.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-evaluating LLM-based Heuristic Search: A Case Study on the 3D Packing Problem</title>
<link>https://arxiv.org/abs/2509.02297</link>
<guid>https://arxiv.org/abs/2509.02297</guid>
<content:encoded><![CDATA[
arXiv:2509.02297v1 Announce Type: new 
Abstract: The art of heuristic design has traditionally been a human pursuit. While Large Language Models (LLMs) can generate code for search heuristics, their application has largely been confined to adjusting simple functions within human-crafted frameworks, leaving their capacity for broader innovation an open question. To investigate this, we tasked an LLM with building a complete solver for the constrained 3D Packing Problem. Direct code generation quickly proved fragile, prompting us to introduce two supports: constraint scaffolding--prewritten constraint-checking code--and iterative self-correction--additional refinement cycles to repair bugs and produce a viable initial population. Notably, even within a vast search space in a greedy process, the LLM concentrated its efforts almost exclusively on refining the scoring function. This suggests that the emphasis on scoring functions in prior work may reflect not a principled strategy, but rather a natural limitation of LLM capabilities. The resulting heuristic was comparable to a human-designed greedy algorithm, and when its scoring function was integrated into a human-crafted metaheuristic, its performance rivaled established solvers, though its effectiveness waned as constraints tightened. Our findings highlight two major barriers to automated heuristic design with current LLMs: the engineering required to mitigate their fragility in complex reasoning tasks, and the influence of pretrained biases, which can prematurely narrow the search for novel solutions.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Diffusion Models for Generative Forecasting of Financial Charts</title>
<link>https://arxiv.org/abs/2509.02308</link>
<guid>https://arxiv.org/abs/2509.02308</guid>
<content:encoded><![CDATA[
arXiv:2509.02308v1 Announce Type: new 
Abstract: Recent advances in generative models have enabled significant progress in tasks such as generating and editing images from text, as well as creating videos from text prompts, and these methods are being applied across various fields. However, in the financial domain, there may still be a reliance on time-series data and a continued focus on transformer models, rather than on diverse applications of generative models. In this paper, we propose a novel approach that leverages text-to-image model by treating time-series data as a single image pattern, thereby enabling the prediction of stock price trends. Unlike prior methods that focus on learning and classifying chart patterns using architectures such as ResNet or ViT, we experiment with generating the next chart image from the current chart image and an instruction prompt using diffusion models. Furthermore, we introduce a simple method for evaluating the generated chart image against ground truth image. We highlight the potential of leveraging text-to-image generative models in the financial domain, and our findings motivate further research to address the current limitations and expand their applicability.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability-Driven Dimensionality Reduction for Hyperspectral Imaging</title>
<link>https://arxiv.org/abs/2509.02340</link>
<guid>https://arxiv.org/abs/2509.02340</guid>
<content:encoded><![CDATA[
arXiv:2509.02340v1 Announce Type: new 
Abstract: Hyperspectral imaging (HSI) provides rich spectral information for precise material classification and analysis; however, its high dimensionality introduces a computational burden and redundancy, making dimensionality reduction essential. We present an exploratory study into the application of post-hoc explainability methods in a model--driven framework for band selection, which reduces the spectral dimension while preserving predictive performance. A trained classifier is probed with explanations to quantify each band's contribution to its decisions. We then perform deletion--insertion evaluations, recording confidence changes as ranked bands are removed or reintroduced, and aggregate these signals into influence scores. Selecting the highest--influence bands yields compact spectral subsets that maintain accuracy and improve efficiency. Experiments on two public benchmarks (Pavia University and Salinas) demonstrate that classifiers trained on as few as 30 selected bands match or exceed full--spectrum baselines while reducing computational requirements. The resulting subsets align with physically meaningful, highly discriminative wavelength regions, indicating that model--aligned, explanation-guided band selection is a principled route to effective dimensionality reduction for HSI.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Agents go Astray: Course-Correcting SWE Agents with PRMs</title>
<link>https://arxiv.org/abs/2509.02360</link>
<guid>https://arxiv.org/abs/2509.02360</guid>
<content:encoded><![CDATA[
arXiv:2509.02360v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are increasingly deployed for complex, multi-step software engineering (SWE) tasks. However, their trajectories often contain costly inefficiencies, such as redundant exploration, looping, and failure to terminate once a solution is reached. Prior work has largely treated these errors in a post-hoc manner, diagnosing failures only after execution. In this paper, we introduce SWE-PRM, an inference-time Process Reward Model (PRM) that intervenes during execution to detect and course-correct trajectory-level errors. Our PRM design leverages a taxonomy of common inefficiencies and delivers lightweight, interpretable feedback without modifying the underlying policy. On SWE-bench Verified, closed-source PRMs improve resolution from 40.0% to 50.6% (+10.6 p.p.), with the largest gains on medium and hard tasks. Among feedback strategies, taxonomy-guided PRMs outperform unguided or explicit action-prescriptive variants, increasing success rate while reducing trajectory length. These benefits come at an acceptable added inference cost of as low as $0.2, making PRMs a practical and scalable mechanism for improving SWE agents' reliability and efficiency.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Agents That Know When They Don't Know: Uncertainty as a Control Signal for Structured Reasoning</title>
<link>https://arxiv.org/abs/2509.02401</link>
<guid>https://arxiv.org/abs/2509.02401</guid>
<content:encoded><![CDATA[
arXiv:2509.02401v1 Announce Type: new 
Abstract: Large language model (LLM) agents are increasingly deployed in structured biomedical data environments, yet they often produce fluent but overconfident outputs when reasoning over complex multi-table data. We introduce an uncertainty-aware agent for query-conditioned multi-table summarization that leverages two complementary signals: (i) retrieval uncertainty--entropy over multiple table-selection rollouts--and (ii) summary uncertainty--combining self-consistency and perplexity. Summary uncertainty is incorporated into reinforcement learning (RL) with Group Relative Policy Optimization (GRPO), while both retrieval and summary uncertainty guide inference-time filtering and support the construction of higher-quality synthetic datasets.
  On multi-omics benchmarks, our approach improves factuality and calibration, nearly tripling correct and useful claims per summary (3.0\(\rightarrow\)8.4 internal; 3.6\(\rightarrow\)9.9 cancer multi-omics) and substantially improving downstream survival prediction (C-index 0.32\(\rightarrow\)0.63). These results demonstrate that uncertainty can serve as a control signal--enabling agents to abstain, communicate confidence, and become more reliable tools for complex structured-data environments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent</title>
<link>https://arxiv.org/abs/2509.02444</link>
<guid>https://arxiv.org/abs/2509.02444</guid>
<content:encoded><![CDATA[
arXiv:2509.02444v1 Announce Type: new 
Abstract: With the raid evolution of large language models and multimodal foundation models, the mobile-agent landscape has proliferated without converging on the fundamental challenges. This paper identifies four core problems that must be solved for mobile agents to deliver practical, scalable impact: (1) generalization across tasks, modalities, apps, and devices; (2) accuracy, specifically precise on-screen interaction and click targeting; (3) long-horizon capability for sustained, multi-step goals; and (4) efficiency, specifically high-performance runtime on resource-constrained devices. We present AppCopilot, a multimodal, multi-agent, general-purpose on-device assistant that operates across applications and constitutes a full-stack, closed-loop system from data to deployment. AppCopilot operationalizes this position through an end-to-end autonomous pipeline spanning data collection, training, deployment, high-quality and efficient inference, and mobile application development. At the model layer, it integrates multimodal foundation models with robust Chinese-English support. At the reasoning and control layer, it combines chain-of-thought reasoning, hierarchical task planning and decomposition, and multi-agent collaboration. At the execution layer, it enables user personalization and experiential adaptation, voice interaction, function calling, cross-app and cross-device orchestration, and comprehensive mobile app support. The system design incorporates profiling-driven optimization for latency, memory, and energy across heterogeneous hardware. Empirically, AppCopilot achieves significant improvements along all four dimensions: stronger generalization, higher-precision on-screen actions, more reliable long-horizon task completion, and faster, more resource-efficient runtime.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GridMind: LLMs-Powered Agents for Power System Analysis and Operations</title>
<link>https://arxiv.org/abs/2509.02494</link>
<guid>https://arxiv.org/abs/2509.02494</guid>
<content:encoded><![CDATA[
arXiv:2509.02494v1 Announce Type: new 
Abstract: The complexity of traditional power system analysis workflows presents significant barriers to efficient decision-making in modern electric grids. This paper presents GridMind, a multi-agent AI system that integrates Large Language Models (LLMs) with deterministic engineering solvers to enable conversational scientific computing for power system analysis. The system employs specialized agents coordinating AC Optimal Power Flow and N-1 contingency analysis through natural language interfaces while maintaining numerical precision via function calls. GridMind addresses workflow integration, knowledge accessibility, context preservation, and expert decision-support augmentation. Experimental evaluation on IEEE test cases demonstrates that the proposed agentic framework consistently delivers correct solutions across all tested language models, with smaller LLMs achieving comparable analytical accuracy with reduced computational latency. This work establishes agentic AI as a viable paradigm for scientific computing, demonstrating how conversational interfaces can enhance accessibility while preserving numerical rigor essential for critical engineering applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.02544</link>
<guid>https://arxiv.org/abs/2509.02544</guid>
<content:encoded><![CDATA[
arXiv:2509.02544v1 Announce Type: new 
Abstract: The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2's potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Landscape of Agentic Reinforcement Learning for LLMs: A Survey</title>
<link>https://arxiv.org/abs/2509.02547</link>
<guid>https://arxiv.org/abs/2509.02547</guid>
<content:encoded><![CDATA[
arXiv:2509.02547v1 Announce Type: new 
Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPG: Incremental Patch Generation for Generalized Adversarial Patch Training</title>
<link>https://arxiv.org/abs/2508.10946</link>
<guid>https://arxiv.org/abs/2508.10946</guid>
<content:encoded><![CDATA[
arXiv:2508.10946v1 Announce Type: cross 
Abstract: The advent of adversarial patches poses a significant challenge to the robustness of AI models, particularly in the domain of computer vision tasks such as object detection. In contradistinction to traditional adversarial examples, these patches target specific regions of an image, resulting in the malfunction of AI models. This paper proposes Incremental Patch Generation (IPG), a method that generates adversarial patches up to 11.1 times more efficiently than existing approaches while maintaining comparable attack performance. The efficacy of IPG is demonstrated by experiments and ablation studies including YOLO's feature distribution visualization and adversarial training results, which show that it produces well-generalized patches that effectively cover a broader range of model vulnerabilities. Furthermore, IPG-generated datasets can serve as a robust knowledge foundation for constructing a robust model, enabling structured representation, advanced reasoning, and proactive defenses in AI security ecosystems. The findings of this study suggest that IPG has considerable potential for future utilization not only in adversarial patch defense but also in real-world applications such as autonomous vehicles, security systems, and medical imaging, where AI models must remain resilient to adversarial attacks in dynamic and high-stakes environments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Per-sender neural network classifiers for email authorship validation</title>
<link>https://arxiv.org/abs/2509.00005</link>
<guid>https://arxiv.org/abs/2509.00005</guid>
<content:encoded><![CDATA[
arXiv:2509.00005v1 Announce Type: cross 
Abstract: Business email compromise and lateral spear phishing attacks are among modern organizations' most costly and damaging threats. While inbound phishing defenses have improved significantly, most organizations still trust internal emails by default, leaving themselves vulnerable to attacks from compromised employee accounts. In this work, we define and explore the problem of authorship validation: verifying whether a claimed sender actually authored a given email. Authorship validation is a lightweight, real-time defense that complements traditional detection methods by modeling per-sender writing style. Further, the paper presents a collection of new datasets based on the Enron corpus. These simulate inauthentic messages using both human-written and large language model-generated emails. The paper also evaluates two classifiers -- a Naive Bayes model and a character-level convolutional neural network (Char-CNN) -- for the authorship validation task. Our experiments show that the Char-CNN model achieves high accuracy and F1 scores under various circumstances. Finally, we discuss deployment considerations and show that per-sender authorship classifiers are practical for integrating into existing commercial email security systems with low overhead.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimized Renewable Energy Planning MDP for Socially-Equitable Electricity Coverage in the US</title>
<link>https://arxiv.org/abs/2509.00008</link>
<guid>https://arxiv.org/abs/2509.00008</guid>
<content:encoded><![CDATA[
arXiv:2509.00008v1 Announce Type: cross 
Abstract: Traditional power grid infrastructure presents significant barriers to renewable energy integration and perpetuates energy access inequities, with low-income communities experiencing disproportionately longer power outages. This study develops a Markov Decision Process (MDP) framework to optimize renewable energy allocation while explicitly addressing social equity concerns in electricity distribution. The model incorporates budget constraints, energy demand variability, and social vulnerability indicators across eight major U.S. cities to evaluate policy alternatives for equitable clean energy transitions. Numerical experiments compare the MDP-based approach against baseline policies including random allocation, greedy renewable expansion, and expert heuristics. Results demonstrate that equity-focused optimization can achieve 32.9% renewable energy penetration while reducing underserved low-income populations by 55% compared to conventional approaches. The expert policy achieved the highest reward, while the Monte Carlo Tree Search baseline provided competitive performance with significantly lower budget utilization, demonstrating that fair distribution of clean energy resources is achievable without sacrificing overall system performance and providing ways for integrating social equity considerations with climate goals and inclusive access to clean power infrastructure.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fluid Antenna Enabled Physical Layer Key Generation for Next-G Wireless Networks</title>
<link>https://arxiv.org/abs/2509.00018</link>
<guid>https://arxiv.org/abs/2509.00018</guid>
<content:encoded><![CDATA[
arXiv:2509.00018v1 Announce Type: cross 
Abstract: As a promising physical layer security technique, physical layer key generation (PLKG) enables legitimate users to obtain secret keys from wireless channel without security infrastructures. However, in harsh propagation environments, the channel characteristic becomes unsatisfactory, the key generation rate (KGR) is significantly deteriorated. In this paper, we propose a novel fluid antenna (FA) enabled PLKG system to address this challenge. Specifically, we first derive the closed-form expression of the KGR for FA array, and then jointly optimize the precoding matrix and the antenna positions via a particle swarm optimization (PSO) algorithm. Next, to further reduce the computational complexity of the optimization procedure, we develop an alternating optimization (AO) algorithm, which combines the projected gradient descent (PGD) and the PSO. Simulation results demonstrate that by exploiting the additional spatial degree of freedom (DoF), our FA enabled PLKG system is superior to the benchmarks, such as the conventional fixed-position antenna (FPA) array and the reconfigurable intelligent surface (RIS). It is worth highlighting that compared to the conventional uniform planar antenna (UPA), the FA enabled PLKG achieves a 35.42\% KGR performance improvement under PSO algorithm and a 67.73\% KGR performance improvement under AO algorithm, respectively.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepEmoNet: Building Machine Learning Models for Automatic Emotion Recognition in Human Speeches</title>
<link>https://arxiv.org/abs/2509.00025</link>
<guid>https://arxiv.org/abs/2509.00025</guid>
<content:encoded><![CDATA[
arXiv:2509.00025v1 Announce Type: cross 
Abstract: Speech emotion recognition (SER) has been a challenging problem in spoken language processing research, because it is unclear how human emotions are connected to various components of sounds such as pitch, loudness, and energy. This paper aims to tackle this problem using machine learning. Particularly, we built several machine learning models using SVMs, LTSMs, and CNNs to classify emotions in human speeches. In addition, by leveraging transfer learning and data augmentation, we efficiently trained our models to attain decent performances on a relatively small dataset. Our best model was a ResNet34 network, which achieved an accuracy of $66.7\%$ and an F1 score of $0.631$.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Sound to Sight: Towards AI-authored Music Videos</title>
<link>https://arxiv.org/abs/2509.00029</link>
<guid>https://arxiv.org/abs/2509.00029</guid>
<content:encoded><![CDATA[
arXiv:2509.00029v1 Announce Type: cross 
Abstract: Conventional music visualisation systems rely on handcrafted ad hoc transformations of shapes and colours that offer only limited expressiveness. We propose two novel pipelines for automatically generating music videos from any user-specified, vocal or instrumental song using off-the-shelf deep learning models. Inspired by the manual workflows of music video producers, we experiment on how well latent feature-based techniques can analyse audio to detect musical qualities, such as emotional cues and instrumental patterns, and distil them into textual scene descriptions using a language model. Next, we employ a generative model to produce the corresponding video clips. To assess the generated videos, we identify several critical aspects and design and conduct a preliminary user evaluation that demonstrates storytelling potential, visual coherency and emotional alignment with the music. Our findings underscore the potential of latent feature techniques and deep generative models to expand music visualisation beyond traditional approaches.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZeroQAT: Your Quantization-aware Training but Efficient</title>
<link>https://arxiv.org/abs/2509.00031</link>
<guid>https://arxiv.org/abs/2509.00031</guid>
<content:encoded><![CDATA[
arXiv:2509.00031v1 Announce Type: cross 
Abstract: Quantization is an effective technique to reduce the deployment cost of large language models (LLMs), and post-training quantization (PTQ) has been widely studied due to its efficiency. However, existing low-bit PTQ methods suffer from accuracy degradation because their layer-wise optimization introduces cumulative error propagation and misalignment between local reconstruction objectives and downstream performance. While quantization-aware training (QAT) provides a principled solution, its reliance on backpropagation incurs prohibitive data, time, and memory costs, limiting its practicality. To address these challenges, we propose ZeroQAT, a zeroth-order optimization-based QAT framework. ZeroQAT leverages forward-only gradient estimation to eliminate the need for backpropagation, significantly reducing computational and memory overhead while retaining the benefits of end-to-end optimization. Moreover, ZeroQAT jointly learns quantized weights, weight clipping thresholds, and equivalent transformations to mitigate quantization error and handle activation outliers. Experiments demonstrate that ZeroQAT achieves the efficiency of PTQ while retaining the accuracy of QAT, offering a practical solution for high-quality low-bit quantization of LLMs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary</title>
<link>https://arxiv.org/abs/2509.00033</link>
<guid>https://arxiv.org/abs/2509.00033</guid>
<content:encoded><![CDATA[
arXiv:2509.00033v1 Announce Type: cross 
Abstract: This is a research exploring existing models and fine tuning them to combine a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to predict the recipe and generate text creating a step by step guide for the cooking procedure. All the data were gathered by the author for a robust task specific system to perform best in complex and challenging environments proving the extension and endless application of computer vision in daily activities such as kitchen work. This work extends the field for many more crucial task of our day to day life.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer Learning for Minimum Operating Voltage Prediction in Advanced Technology Nodes: Leveraging Legacy Data and Silicon Odometer Sensing</title>
<link>https://arxiv.org/abs/2509.00035</link>
<guid>https://arxiv.org/abs/2509.00035</guid>
<content:encoded><![CDATA[
arXiv:2509.00035v1 Announce Type: cross 
Abstract: Accurate prediction of chip performance is critical for ensuring energy efficiency and reliability in semiconductor manufacturing. However, developing minimum operating voltage ($V_{min}$) prediction models at advanced technology nodes is challenging due to limited training data and the complex relationship between process variations and $V_{min}$. To address these issues, we propose a novel transfer learning framework that leverages abundant legacy data from the 16nm technology node to enable accurate $V_{min}$ prediction at the advanced 5nm node. A key innovation of our approach is the integration of input features derived from on-chip silicon odometer sensor data, which provide fine-grained characterization of localized process variations -- an essential factor at the 5nm node -- resulting in significantly improved prediction accuracy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis</title>
<link>https://arxiv.org/abs/2509.00038</link>
<guid>https://arxiv.org/abs/2509.00038</guid>
<content:encoded><![CDATA[
arXiv:2509.00038v1 Announce Type: cross 
Abstract: Large language models (LLMs) offer significant potential to accelerate systematic literature reviews (SLRs), yet current approaches often rely on brittle, manually crafted prompts that compromise reliability and reproducibility. This fragility undermines scientific confidence in LLM-assisted evidence synthesis. In response, this work adapts recent advances in declarative prompt optimisation, developed for general-purpose LLM applications, and demonstrates their applicability to the domain of SLR automation. This research proposes a structured, domain-specific framework that embeds task declarations, test suites, and automated prompt tuning into a reproducible SLR workflow. These emerging methods are translated into a concrete blueprint with working code examples, enabling researchers to construct verifiable LLM pipelines that align with established principles of transparency and rigour in evidence synthesis. This is a novel application of such approaches to SLR pipelines.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity Score for Autonomous Rover Target Prioritization</title>
<link>https://arxiv.org/abs/2509.00042</link>
<guid>https://arxiv.org/abs/2509.00042</guid>
<content:encoded><![CDATA[
arXiv:2509.00042v1 Announce Type: cross 
Abstract: We present ARTPS (Autonomous Rover Target Prioritization System), a novel hybrid AI system that combines depth estimation, anomaly detection, and learnable curiosity scoring for autonomous exploration of planetary surfaces. Our approach integrates monocular depth estimation using Vision Transformers with multi-component anomaly detection and a weighted curiosity score that balances known value, anomaly signals, depth variance, and surface roughness. The system achieves state-of-the-art performance with AUROC of 0.94, AUPRC of 0.89, and F1-Score of 0.87 on Mars rover datasets. We demonstrate significant improvements in target prioritization accuracy through ablation studies and provide comprehensive analysis of component contributions. The hybrid fusion approach reduces false positives by 23% while maintaining high detection sensitivity across diverse terrain types.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring and Reshaping the Weight Distribution in LLM</title>
<link>https://arxiv.org/abs/2509.00046</link>
<guid>https://arxiv.org/abs/2509.00046</guid>
<content:encoded><![CDATA[
arXiv:2509.00046v1 Announce Type: cross 
Abstract: The performance of Large Language Models is influenced by their characteristics such as architecture, model sizes, decoding methods and so on. Due to differences in structure or function, the weights in different layers of large models have varying distributions. This paper explores the correlations between different types of layers in terms of weights distribution and studies the potential impact of these correlations on LoRA training effectiveness. Firstly, the study reveals that in the model the cosine distances between weights of different layers manifest power-law distribution. We extract Query-projection, down-projection and other weight matrices from the self-attention layers and MLP layers, calculate the singular values of the matrices using singular value decomposition, and organize a certain number of singular values into matrices according to projection's type. By analyzing the probability distribution of the cosine distances between these matrices, it is found that the cosine distances values between them have distinct power-law distribution characteristics. Secondly, based on the results of distance calculations and analysis across different layers of model, a qualitative method is proposed to describe the distribution characteristics of different models. Next, to construct weights that align with the distribution characteristics, a data generator is designed using a combination of Gaussian process and Pareto distribution functions. The generator is used to simulate the generation of data that aligns with specific distribution characteristics. Finally, based on the aforementioned distribution characteristics and data generation method, the weights in LoRA initialization are reshaped for training. Experimental results indicate that, without altering the model structure or training process, this method achieves a certain improvement in the performance of LoRA training.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching AI to Remember: Insights from Brain-Inspired Replay in Continual Learning</title>
<link>https://arxiv.org/abs/2509.00047</link>
<guid>https://arxiv.org/abs/2509.00047</guid>
<content:encoded><![CDATA[
arXiv:2509.00047v1 Announce Type: cross 
Abstract: Artificial neural networks (ANNs) continue to face challenges in continual learning, particularly due to catastrophic forgetting, the loss of previously learned knowledge when acquiring new tasks. Inspired by memory consolidation in the human brain, we investigate the internal replay mechanism proposed by~\citep{brain_inspired_replay1}, which reactivates latent representations of prior experiences during learning. As internal replay was identified as the most influential component among the brain-inspired mechanisms in their framework, it serves as the central focus of our in-depth investigation. Using the CIFAR-100 dataset in a class-incremental setting, we evaluate the effectiveness of internal replay, both in isolation and in combination with Synaptic Intelligence (SI). Our experiments show that internal replay significantly mitigates forgetting, especially when paired with SI, but at the cost of reduced initial task accuracy, highlighting a trade-off between memory stability and learning plasticity. Further analyses using log-likelihood distributions, reconstruction errors, silhouette scores, and UMAP projections reveal that internal replay increases representational overlap in latent space, potentially limiting task-specific differentiation. These results underscore the limitations of current brain-inspired methods and suggest future directions for balancing retention and adaptability in continual learning systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Deep Learning to Anomaly Detection of Russian Satellite Activity for Indications Prior to Military Activity</title>
<link>https://arxiv.org/abs/2509.00050</link>
<guid>https://arxiv.org/abs/2509.00050</guid>
<content:encoded><![CDATA[
arXiv:2509.00050v1 Announce Type: cross 
Abstract: We apply deep learning techniques for anomaly detection to analyze activity of Russian-owned resident space objects (RSO) prior to the Ukraine invasion and assess the results for any findings that can be used as indications and warnings (I&amp;W) of aggressive military behavior for future conflicts. Through analysis of anomalous activity, an understanding of possible tactics and procedures can be established to assess the existence of statistically significant changes in Russian RSO pattern of life/pattern of behavior (PoL/PoB) using publicly available two-line element (TLE) data. This research looks at statistical and deep learning approaches to assess anomalous activity. The deep learning methods assessed are isolation forest (IF), traditional autoencoder (AE), variational autoencoder (VAE), Kolmogorov Arnold Network (KAN), and a novel anchor-loss based autoencoder (Anchor AE). Each model is used to establish a baseline of on-orbit activity based on a five-year data sample. The primary investigation period focuses on the six months leading up to the invasion date of February 24, 2022. Additional analysis looks at RSO activity during an active combat period by sampling TLE data after the invasion date. The deep learning autoencoder models identify anomalies based on reconstruction errors that surpass a threshold sigma. To capture the nuance and unique characteristics of each RSO an individual model was trained for each observed space object. The research made an effort to prioritize explainability and interpretability of the model results thus each observation was assessed for anomalous behavior of the individual six orbital elements versus analyzing the input data as a single monolithic observation. The results demonstrate not only statistically significant anomalies of Russian RSO activity but also details anomalous findings to the individual orbital element.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightning Fast Caching-based Parallel Denoising Prediction for Accelerating Talking Head Generation</title>
<link>https://arxiv.org/abs/2509.00052</link>
<guid>https://arxiv.org/abs/2509.00052</guid>
<content:encoded><![CDATA[
arXiv:2509.00052v1 Announce Type: cross 
Abstract: Diffusion-based talking head models generate high-quality, photorealistic videos but suffer from slow inference, limiting practical applications. Existing acceleration methods for general diffusion models fail to exploit the temporal and spatial redundancies unique to talking head generation. In this paper, we propose a task-specific framework addressing these inefficiencies through two key innovations. First, we introduce Lightning-fast Caching-based Parallel denoising prediction (LightningCP), caching static features to bypass most model layers in inference time. We also enable parallel prediction using cached features and estimated noisy latents as inputs, efficiently bypassing sequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to further accelerate attention computations, exploiting the spatial decoupling in talking head videos to restrict attention to dynamic foreground regions. Additionally, we remove reference features in certain layers to bring extra speedup. Extensive experiments demonstrate that our framework significantly improves inference speed while preserving video quality.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traj-MLLM: Can Multimodal Large Language Models Reform Trajectory Data Mining?</title>
<link>https://arxiv.org/abs/2509.00053</link>
<guid>https://arxiv.org/abs/2509.00053</guid>
<content:encoded><![CDATA[
arXiv:2509.00053v1 Announce Type: cross 
Abstract: Building a general model capable of analyzing human trajectories across different geographic regions and different tasks becomes an emergent yet important problem for various applications. However, existing works suffer from the generalization problem, \ie, they are either restricted to train for specific regions or only suitable for a few tasks. Given the recent advances of multimodal large language models (MLLMs), we raise the question: can MLLMs reform current trajectory data mining and solve the problem? Nevertheless, due to the modality gap of trajectory, how to generate task-independent multimodal trajectory representations and how to adapt flexibly to different tasks remain the foundational challenges. In this paper, we propose \texttt{Traj-MLLM}}, which is the first general framework using MLLMs for trajectory data mining. By integrating multiview contexts, \texttt{Traj-MLLM}} transforms raw trajectories into interleaved image-text sequences while preserving key spatial-temporal characteristics, and directly utilizes the reasoning ability of MLLMs for trajectory analysis. Additionally, a prompt optimization method is proposed to finalize data-invariant prompts for task adaptation. Extensive experiments on four publicly available datasets show that \texttt{Traj-MLLM}} outperforms state-of-the-art baselines by $48.05\%$, $15.52\%$, $51.52\%$, $1.83\%$ on travel time estimation, mobility prediction, anomaly detection and transportation mode identification, respectively. \texttt{Traj-MLLM}} achieves these superior performances without requiring any training data or fine-tuning the MLLM backbones.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robotic Fire Risk Detection based on Dynamic Knowledge Graph Reasoning: An LLM-Driven Approach with Graph Chain-of-Thought</title>
<link>https://arxiv.org/abs/2509.00054</link>
<guid>https://arxiv.org/abs/2509.00054</guid>
<content:encoded><![CDATA[
arXiv:2509.00054v1 Announce Type: cross 
Abstract: Fire is a highly destructive disaster, but effective prevention can significantly reduce its likelihood of occurrence. When it happens, deploying emergency robots in fire-risk scenarios can help minimize the danger to human responders. However, current research on pre-disaster warnings and disaster-time rescue still faces significant challenges due to incomplete perception, inadequate fire situational awareness, and delayed response. To enhance intelligent perception and response planning for robots in fire scenarios, we first construct a knowledge graph (KG) by leveraging large language models (LLMs) to integrate fire domain knowledge derived from fire prevention guidelines and fire rescue task information from robotic emergency response documents. We then propose a new framework called Insights-on-Graph (IOG), which integrates the structured fire information of KG and Large Multimodal Models (LMMs). The framework generates perception-driven risk graphs from real-time scene imagery to enable early fire risk detection and provide interpretable emergency responses for task module and robot component configuration based on the evolving risk situation. Extensive simulations and real-world experiments show that IOG has good applicability and practical application value in fire risk detection and rescue decision-making.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>U2UData-2: A Scalable Swarm UAVs Autonomous Flight Dataset for Long-horizon Tasks</title>
<link>https://arxiv.org/abs/2509.00055</link>
<guid>https://arxiv.org/abs/2509.00055</guid>
<content:encoded><![CDATA[
arXiv:2509.00055v1 Announce Type: cross 
Abstract: Swarm UAV autonomous flight for Long-Horizon (LH) tasks is crucial for advancing the low-altitude economy. However, existing methods focus only on specific basic tasks due to dataset limitations, failing in real-world deployment for LH tasks. LH tasks are not mere concatenations of basic tasks, requiring handling long-term dependencies, maintaining persistent states, and adapting to dynamic goal shifts. This paper presents U2UData-2, the first large-scale swarm UAV autonomous flight dataset for LH tasks and the first scalable swarm UAV data online collection and algorithm closed-loop verification platform. The dataset is captured by 15 UAVs in autonomous collaborative flights for LH tasks, comprising 12 scenes, 720 traces, 120 hours, 600 seconds per trajectory, 4.32M LiDAR frames, and 12.96M RGB frames. This dataset also includes brightness, temperature, humidity, smoke, and airflow values covering all flight routes. The platform supports the customization of simulators, UAVs, sensors, flight algorithms, formation modes, and LH tasks. Through a visual control window, this platform allows users to collect customized datasets through one-click deployment online and to verify algorithms by closed-loop simulation. U2UData-2 also introduces an LH task for wildlife conservation and provides comprehensive benchmarks with 9 SOTA models. U2UData-2 can be found at https://fengtt42.github.io/U2UData-2/.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis</title>
<link>https://arxiv.org/abs/2509.00057</link>
<guid>https://arxiv.org/abs/2509.00057</guid>
<content:encoded><![CDATA[
arXiv:2509.00057v1 Announce Type: cross 
Abstract: Machine learning-based failure management in optical networks has gained significant attention in recent years. However, severe class imbalance, where normal instances vastly outnumber failure cases, remains a considerable challenge. While pre- and in-processing techniques have been widely studied, post-processing methods are largely unexplored. In this work, we present a direct comparison of pre-, in-, and post-processing approaches for class imbalance mitigation in failure detection and identification using an experimental dataset. For failure detection, post-processing methods-particularly Threshold Adjustment-achieve the highest F1 score improvement (up to 15.3%), while Random Under-Sampling provides the fastest inference. In failure identification, GenAI methods deliver the most substantial performance gains (up to 24.2%), whereas post-processing shows limited impact in multi-class settings. When class overlap is present and latency is critical, over-sampling methods such as the SMOTE are most effective; without latency constraints, Meta-Learning yields the best results. In low-overlap scenarios, Generative AI approaches provide the highest performance with minimal inference time.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion</title>
<link>https://arxiv.org/abs/2509.00062</link>
<guid>https://arxiv.org/abs/2509.00062</guid>
<content:encoded><![CDATA[
arXiv:2509.00062v1 Announce Type: cross 
Abstract: Generating realistic sparse multi-category 3D voxel structures is difficult due to the cubic memory scaling of voxel structures and moreover the significant class imbalance caused by sparsity. We introduce Scaffold Diffusion, a generative model designed for sparse multi-category 3D voxel structures. By treating voxels as tokens, Scaffold Diffusion uses a discrete diffusion language model to generate 3D voxel structures. We show that discrete diffusion language models can be extended beyond inherently sequential domains such as text to generate spatially coherent 3D structures. We evaluate on Minecraft house structures from the 3D-Craft dataset and demonstrate that, unlike prior baselines and an auto-regressive formulation, Scaffold Diffusion produces realistic and coherent structures even when trained on data with over 98% sparsity. We provide an interactive viewer where readers can visualize generated samples and the generation process. Our results highlight discrete diffusion as a promising framework for 3D sparse voxel generative modeling.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolErr2Fix:Benchmarking LLM Trustworthiness in Chemistry via Modular Error Detection, Localization, Explanation, and Revision</title>
<link>https://arxiv.org/abs/2509.00063</link>
<guid>https://arxiv.org/abs/2509.00063</guid>
<content:encoded><![CDATA[
arXiv:2509.00063v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown growing potential in molecular sciences, but they often produce chemically inaccurate descriptions and struggle to recognize or justify potential errors. This raises important concerns about their robustness and reliability in scientific applications. To support more rigorous evaluation of LLMs in chemical reasoning, we present the MolErr2Fix benchmark, designed to assess LLMs on error detection and correction in molecular descriptions. Unlike existing benchmarks focused on molecule-to-text generation or property prediction, MolErr2Fix emphasizes fine-grained chemical understanding. It tasks LLMs with identifying, localizing, explaining, and revising potential structural and semantic errors in molecular descriptions. Specifically, MolErr2Fix consists of 1,193 fine-grained annotated error instances. Each instance contains quadruple annotations, i.e,. (error type, span location, the explanation, and the correction). These tasks are intended to reflect the types of reasoning and verification required in real-world chemical communication. Evaluations of current state-of-the-art LLMs reveal notable performance gaps, underscoring the need for more robust chemical reasoning capabilities. MolErr2Fix provides a focused benchmark for evaluating such capabilities and aims to support progress toward more reliable and chemically informed language models. All annotations and an accompanying evaluation API will be publicly released to facilitate future research.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Collaborations among Healthcare Systems, Research Institutions, and Industry on Artificial Intelligence Research and Development</title>
<link>https://arxiv.org/abs/2509.00068</link>
<guid>https://arxiv.org/abs/2509.00068</guid>
<content:encoded><![CDATA[
arXiv:2509.00068v1 Announce Type: cross 
Abstract: Objectives: The integration of Artificial Intelligence (AI) in healthcare promises to revolutionize patient care, diagnostics, and treatment protocols. Collaborative efforts among healthcare systems, research institutions, and industry are pivotal to leveraging AI's full potential. This study aims to characterize collaborative networks and stakeholders in AI healthcare initiatives, identify challenges and opportunities within these collaborations, and elucidate priorities for future AI research and development. Methods: This study utilized data from the Chinese Society of Radiology and the Chinese Medical Imaging AI Innovation Alliance. A national cross-sectional survey was conducted in China (N = 5,142) across 31 provincial administrative regions, involving participants from three key groups: clinicians, institution professionals, and industry representatives. The survey explored diverse aspects including current AI usage in healthcare, collaboration dynamics, challenges encountered, and research and development priorities. Results: Findings reveal high interest in AI among clinicians, with a significant gap between interest and actual engagement in development activities. Despite the willingness to share data, progress is hindered by concerns about data privacy and security, and lack of clear industry standards and legal guidelines. Future development interests focus on lesion screening, disease diagnosis, and enhancing clinical workflows. Conclusion: This study highlights an enthusiastic yet cautious approach toward AI in healthcare, characterized by significant barriers that impede effective collaboration and implementation. Recommendations emphasize the need for AI-specific education and training, secure data-sharing frameworks, establishment of clear industry standards, and formation of dedicated AI research departments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits</title>
<link>https://arxiv.org/abs/2509.00071</link>
<guid>https://arxiv.org/abs/2509.00071</guid>
<content:encoded><![CDATA[
arXiv:2509.00071v1 Announce Type: cross 
Abstract: In recent years, AI-assisted IC design methods have demonstrated great potential, but the availability of circuit design data is extremely limited, especially in the public domain. The lack of circuit data has become the primary bottleneck in developing AI-assisted IC design methods. In this work, we make the first attempt, SynCircuit, to generate new synthetic circuits with valid functionalities in the HDL format. SynCircuit automatically generates synthetic data using a framework with three innovative steps: 1) We propose a customized diffusion-based generative model to resolve the Directed Cyclic Graph (DCG) generation task, which has not been well explored in the AI community. 2) To ensure our circuit is valid, we enforce the circuit constraints by refining the initial graph generation outputs. 3) The Monte Carlo tree search (MCTS) method further optimizes the logic redundancy in the generated graph. Experimental results demonstrate that our proposed SynCircuit can generate more realistic synthetic circuits and enhance ML model performance in downstream circuit design tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amplifying Emotional Signals: Data-Efficient Deep Learning for Robust Speech Emotion Recognition</title>
<link>https://arxiv.org/abs/2509.00077</link>
<guid>https://arxiv.org/abs/2509.00077</guid>
<content:encoded><![CDATA[
arXiv:2509.00077v1 Announce Type: cross 
Abstract: Speech Emotion Recognition (SER) presents a significant yet persistent challenge in human-computer interaction. While deep learning has advanced spoken language processing, achieving high performance on limited datasets remains a critical hurdle. This paper confronts this issue by developing and evaluating a suite of machine learning models, including Support Vector Machines (SVMs), Long Short-Term Memory networks (LSTMs), and Convolutional Neural Networks (CNNs), for automated emotion classification in human speech. We demonstrate that by strategically employing transfer learning and innovative data augmentation techniques, our models can achieve impressive performance despite the constraints of a relatively small dataset. Our most effective model, a ResNet34 architecture, establishes a new performance benchmark on the combined RAVDESS and SAVEE datasets, attaining an accuracy of 66.7% and an F1 score of 0.631. These results underscore the substantial benefits of leveraging pre-trained models and data augmentation to overcome data scarcity, thereby paving the way for more robust and generalizable SER systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Transparent Cyber Threat Intelligence Combining Large Language Models and Domain Ontologies</title>
<link>https://arxiv.org/abs/2509.00081</link>
<guid>https://arxiv.org/abs/2509.00081</guid>
<content:encoded><![CDATA[
arXiv:2509.00081v1 Announce Type: cross 
Abstract: Effective Cyber Threat Intelligence (CTI) relies upon accurately structured and semantically enriched information extracted from cybersecurity system logs. However, current methodologies often struggle to identify and interpret malicious events reliably and transparently, particularly in cases involving unstructured or ambiguous log entries. In this work, we propose a novel methodology that combines ontology-driven structured outputs with Large Language Models (LLMs), to build an Artificial Intelligence (AI) agent that improves the accuracy and explainability of information extraction from cybersecurity logs. Central to our approach is the integration of domain ontologies and SHACL-based constraints to guide the language model's output structure and enforce semantic validity over the resulting graph. Extracted information is organized into an ontology-enriched graph database, enabling future semantic analysis and querying. The design of our methodology is motivated by the analytical requirements associated with honeypot log data, which typically comprises predominantly malicious activity. While our case study illustrates the relevance of this scenario, the experimental evaluation is conducted using publicly available datasets. Results demonstrate that our method achieves higher accuracy in information extraction compared to traditional prompt-only approaches, with a deliberate focus on extraction quality rather than processing speed.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models</title>
<link>https://arxiv.org/abs/2509.00083</link>
<guid>https://arxiv.org/abs/2509.00083</guid>
<content:encoded><![CDATA[
arXiv:2509.00083v1 Announce Type: cross 
Abstract: Modern generative models risk overfitting and unintentionally memorizing rare training examples, which can be extracted by adversaries or inflate benchmark performance. We propose Generative Data Cartography (GenDataCarto), a data-centric framework that assigns each pretraining sample a difficulty score (early-epoch loss) and a memorization score (frequency of ``forget events''), then partitions examples into four quadrants to guide targeted pruning and up-/down-weighting. We prove that our memorization score lower-bounds classical influence under smoothness assumptions and that down-weighting high-memorization hotspots provably decreases the generalization gap via uniform stability bounds. Empirically, GenDataCarto reduces synthetic canary extraction success by over 40\% at just 10\% data pruning, while increasing validation perplexity by less than 0.5\%. These results demonstrate that principled data interventions can dramatically mitigate leakage with minimal cost to generative performance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2509.00084</link>
<guid>https://arxiv.org/abs/2509.00084</guid>
<content:encoded><![CDATA[
arXiv:2509.00084v1 Announce Type: cross 
Abstract: To further enhance the ability of Large Language Models (LLMs) to solve complex, multi-step reasoning problems, test-time scaling (TTS) methods have gained widespread attention. Existing approaches such as Best-of-N and majority voting are limited as their performance depends on the quality of candidate responses, making them unable to produce a correct solution when all candidates are incorrect. Introducing an additional model to select the best response also incurs significant deployment costs. To this end, we introduce Generative Self-Refinement (GSR), a novel parallel test-time scaling framework where a unified model first generates a set of candidate responses in parallel and then performs self-refinement to synthesize a new superior solution based on a prompt consisting of the problem and these candidates. However, LLMs struggle to perform refinement effectively when prompted directly. Therefore, we design a hybrid training pipeline by jointly optimizing for two complementary objectives, solving problems directly and refining candidate responses. Experimental results demonstrate that our method achieves state-of-the-art performance across five mathematical benchmarks. We further show that this learned self-refinement skill is a model-agnostic enhancement, robust across different model scales and generalizing to out-of-distribution reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Private, Verifiable, and Auditable AI Systems</title>
<link>https://arxiv.org/abs/2509.00085</link>
<guid>https://arxiv.org/abs/2509.00085</guid>
<content:encoded><![CDATA[
arXiv:2509.00085v1 Announce Type: cross 
Abstract: The growing societal reliance on artificial intelligence necessitates robust frameworks for ensuring its security, accountability, and trustworthiness. This thesis addresses the complex interplay between privacy, verifiability, and auditability in modern AI, particularly in foundation models. It argues that technical solutions that integrate these elements are critical for responsible AI innovation. Drawing from international policy contributions and technical research to identify key risks in the AI pipeline, this work introduces novel technical solutions for critical privacy and verifiability challenges. Specifically, the research introduces techniques for enabling verifiable and auditable claims about AI systems using zero-knowledge cryptography; utilizing secure multi-party computation and trusted execution environments for auditable, confidential deployment of large language models and information retrieval; and implementing enhanced delegation mechanisms, credentialing systems, and access controls to secure interactions with autonomous and multi-agent AI systems. Synthesizing these technical advancements, this dissertation presents a cohesive perspective on balancing privacy, verifiability, and auditability in foundation model-based AI systems, offering practical blueprints for system designers and informing policy discussions on AI safety and governance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yet Unnoticed in LSTM: Binary Tree Based Input Reordering, Weight Regularization, and Gate Nonlinearization</title>
<link>https://arxiv.org/abs/2509.00087</link>
<guid>https://arxiv.org/abs/2509.00087</guid>
<content:encoded><![CDATA[
arXiv:2509.00087v1 Announce Type: cross 
Abstract: LSTM models used in current Machine Learning literature and applications, has a promising solution for permitting long term information using gating mechanisms that forget and reduce effect of current input information. However, even with this pipeline, they do not optimally focus on specific old index or long-term information. This paper elaborates upon input reordering approaches to prioritize certain input indices. Moreover, no LSTM based approach is found in the literature that examines weight normalization while choosing the right weight and exponent of Lp norms through main supervised loss function. In this paper, we find out which norm best finds relationship between weights to either smooth or sparsify them. Lastly, gates, as weighted representations of inputs and states, which control reduction-extent of current input versus previous inputs (~ state), are not nonlinearized enough (through a small FFNN). As analogous to attention mechanisms, gates easily filter current information to bold (emphasize on) past inputs. Nonlinearized gates can more easily tune up to peculiar nonlinearities of specific input in the past. This type of nonlinearization is not proposed in the literature, to the best of author's knowledge. The proposed approaches are implemented and compared with a simple LSTM to understand their performance in text classification tasks. The results show they improve accuracy of LSTM.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema</title>
<link>https://arxiv.org/abs/2509.00088</link>
<guid>https://arxiv.org/abs/2509.00088</guid>
<content:encoded><![CDATA[
arXiv:2509.00088v1 Announce Type: cross 
Abstract: Prompt injection attacks pose a significant challenge to the safe deployment of Large Language Models (LLMs) in real-world applications. While prompt-based detection offers a lightweight and interpretable defense strategy, its effectiveness has been hindered by the need for manual prompt engineering. To address this issue, we propose AEGIS , an Automated co-Evolutionary framework for Guarding prompt Injections Schema. Both attack and defense prompts are iteratively optimized against each other using a gradient-like natural language prompt optimization technique. This framework enables both attackers and defenders to autonomously evolve via a Textual Gradient Optimization (TGO) module, leveraging feedback from an LLM-guided evaluation loop. We evaluate our system on a real-world assignment grading dataset of prompt injection attacks and demonstrate that our method consistently outperforms existing baselines, achieving superior robustness in both attack success and detection. Specifically, the attack success rate (ASR) reaches 1.0, representing an improvement of 0.26 over the baseline. For detection, the true positive rate (TPR) improves by 0.23 compared to the previous best work, reaching 0.84, and the true negative rate (TNR) remains comparable at 0.89. Ablation studies confirm the importance of co-evolution, gradient buffering, and multi-objective optimization. We also confirm that this framework is effective in different LLMs. Our results highlight the promise of adversarial training as a scalable and effective approach for guarding prompt injections.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Pronunciation Error Detection and Correction of the Holy Quran's Learners Using Deep Learning</title>
<link>https://arxiv.org/abs/2509.00094</link>
<guid>https://arxiv.org/abs/2509.00094</guid>
<content:encoded><![CDATA[
arXiv:2509.00094v1 Announce Type: cross 
Abstract: Assessing spoken language is challenging, and quantifying pronunciation metrics for machine learning models is even harder. However, for the Holy Quran, this task is simplified by the rigorous recitation rules (tajweed) established by Muslim scholars, enabling highly effective assessment. Despite this advantage, the scarcity of high-quality annotated data remains a significant barrier.
  In this work, we bridge these gaps by introducing: (1) A 98% automated pipeline to produce high-quality Quranic datasets -- encompassing: Collection of recitations from expert reciters, Segmentation at pause points (waqf) using our fine-tuned wav2vec2-BERT model, Transcription of segments, Transcript verification via our novel Tasmeea algorithm; (2) 850+ hours of audio (~300K annotated utterances); (3) A novel ASR-based approach for pronunciation error detection, utilizing our custom Quran Phonetic Script (QPS) to encode Tajweed rules (unlike the IPA standard for Modern Standard Arabic). QPS uses a two-level script: (Phoneme level): Encodes Arabic letters with short/long vowels. (Sifa level): Encodes articulation characteristics of every phoneme. We further include comprehensive modeling with our novel multi-level CTC Model which achieved 0.16% average Phoneme Error Rate (PER) on the testset. We release all code, data, and models as open-source: https://obadx.github.io/prepare-quran-dataset/
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting a Mixture-of-Layers in an Electrocardiography Foundation Model</title>
<link>https://arxiv.org/abs/2509.00102</link>
<guid>https://arxiv.org/abs/2509.00102</guid>
<content:encoded><![CDATA[
arXiv:2509.00102v1 Announce Type: cross 
Abstract: Transformer-based foundation models for Electrocardiograms (ECGs) have recently achieved impressive performance in many downstream applications. However, the internal representations of such models across layers have not been fully understood and exploited. An important question arises: Does the final layer of the pre-trained Transformer model, the \emph{de facto} representational layer, provide optimal performance for downstream tasks? Although our answer based on empirical and theoretical analyses for this question is negative, we propose a novel approach to leverage the representation diversity of the model's layers effectively. Specifically, we introduce a novel architecture called Post-pretraining Mixture-of-layers Aggregation (PMA), which enables a flexible combination of the layer-wise representations from the layer stack of a Transformer-based foundation model. We first pre-train the model from ECG signals using the 1-dimensional Vision Transformer (ViT) via masked modeling. In downstream applications, instead of relying solely on the last layer of the model, we employ a gating network to selectively fuse the representations from the pretrained model's layers, thereby enhancing representation power and improving performance of the downstream applications. In addition, we extend the proposed method to the pretraining stage by aggregating all representations through group-wise averaging before feeding them into the decoder-based Transformer.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained knowledge elevates large language models beyond traditional chemical reaction optimizers</title>
<link>https://arxiv.org/abs/2509.00103</link>
<guid>https://arxiv.org/abs/2509.00103</guid>
<content:encoded><![CDATA[
arXiv:2509.00103v1 Announce Type: cross 
Abstract: Modern optimization in experimental chemistry employs algorithmic search through black-box parameter spaces. Here we demonstrate that pre-trained knowledge in large language models (LLMs) fundamentally changes this paradigm. Using six fully enumerated categorical reaction datasets (768 - 5,684 experiments), we benchmark LLM-guided optimization (LLM-GO) against Bayesian optimization (BO) and random sampling. Frontier LLMs consistently match or exceed BO performance across five single-objective datasets, with advantages growing as parameter complexity increases and high-performing conditions become scarce (<5% of space). BO retains superiority only for explicit multi-objective trade-offs. To understand these contrasting behaviors, we introduce a topology-agnostic information theory framework quantifying sampling diversity throughout optimization campaigns. This analysis reveals that LLMs maintain systematically higher exploration entropy than BO across all datasets while achieving superior performance, with advantages most pronounced in solution-scarce parameter spaces where high-entropy exploration typically fails - suggesting that pre-trained domain knowledge enables more effective navigation of chemical parameter space rather than replacing structured exploration strategies. To enable transparent benchmarking and community validation, we release Iron Mind (https://gomes.andrew.cmu.edu/iron-mind), a no-code platform for side-by-side evaluation of human, algorithmic, and LLM optimization campaigns with public leaderboards and complete trajectories. Our findings establish that LLM-GO excels precisely where traditional methods struggle: complex categorical spaces requiring domain understanding rather than mathematical optimization.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Application of Virtual Environments and Artificial Intelligence in Higher Education: Experimental Findings in Philosophy Teaching</title>
<link>https://arxiv.org/abs/2509.00110</link>
<guid>https://arxiv.org/abs/2509.00110</guid>
<content:encoded><![CDATA[
arXiv:2509.00110v1 Announce Type: cross 
Abstract: This study explores how virtual environments and artificial intelligence can enhance university students' learning experiences, with particular attention to the digital preferences of Generation Z. An experiment was conducted at the Faculty of Pedagogy, Humanities, and Social Sciences at University of Gyor, where Walter's Cube technology and a trained AI mediator were integrated into the instruction of ten philosophical topics. The curriculum was aligned with the official syllabus and enriched with visual content, quotations, and explanatory texts related to iconic figures in philosophy. A total of 77 first-year undergraduate students from full-time humanities and social sciences programs participated in the study. Following their end-of-semester offline written examination, students voluntarily completed a paper-based, anonymous ten-question test and provided feedback on the method's effectiveness. No sensitive personal data were collected, and the research was conducted with formal approval from the Faculty Dean. Descriptive statistics and inferential tests were applied to evaluate the impact of the virtual environment and AI mediation on learning outcomes. Results indicate that 80 percent of participants achieved good or excellent final exam grades, and the majority rated the virtual material as highly effective. Qualitative feedback emphasized increased motivation and deeper engagement, attributed to the immersive 3D presentation and interactive AI support. This research contributes to the advancement of digital pedagogy and suggests new directions for applying virtual and AI-based methods in higher education, particularly in disciplines where abstract reasoning and conceptual understanding are central.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-learning ecological priors from large language models explains human learning and decision making</title>
<link>https://arxiv.org/abs/2509.00116</link>
<guid>https://arxiv.org/abs/2509.00116</guid>
<content:encoded><![CDATA[
arXiv:2509.00116v1 Announce Type: cross 
Abstract: Human cognition is profoundly shaped by the environments in which it unfolds. Yet, it remains an open question whether learning and decision making can be explained as a principled adaptation to the statistical structure of real-world tasks. We introduce ecologically rational analysis, a computational framework that unifies the normative foundations of rational analysis with ecological grounding. Leveraging large language models to generate ecologically valid cognitive tasks at scale, and using meta-learning to derive rational models optimized for these environments, we develop a new class of learning algorithms: Ecologically Rational Meta-learned Inference (ERMI). ERMI internalizes the statistical regularities of naturalistic problem spaces and adapts flexibly to novel situations, without requiring hand-crafted heuristics or explicit parameter updates. We show that ERMI captures human behavior across 15 experiments spanning function learning, category learning, and decision making, outperforming several established cognitive models in trial-by-trial prediction. Our results suggest that much of human cognition may reflect adaptive alignment to the ecological structure of the problems we encounter in everyday life.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied AI: Emerging Risks and Opportunities for Policy Action</title>
<link>https://arxiv.org/abs/2509.00117</link>
<guid>https://arxiv.org/abs/2509.00117</guid>
<content:encoded><![CDATA[
arXiv:2509.00117v1 Announce Type: cross 
Abstract: The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI can exist in, learn from, reason about, and act in the physical world. Given recent innovations in large language and multimodal models, along with increasingly advanced and responsive hardware, EAI systems are rapidly growing in capabilities and operational domains. These advances present significant risks, including physical harm from malicious use, mass surveillance, and economic and societal disruption. However, these risks have been severely overlooked by policymakers. Existing policies, such as international standards for industrial robots or statutes governing autonomous vehicles, are insufficient to address the full range of concerns. While lawmakers are increasingly focused on AI, there is now an urgent need to extend and adapt existing frameworks to account for the unique risks of EAI. To help bridge this gap, this paper makes three contributions: first, we provide a foundational taxonomy of key physical, informational, economic, and social EAI risks. Secondly, we analyze policies in the US, EU, and UK to identify how existing frameworks address these risks and where these policies leave critical gaps. We conclude by offering concrete policy recommendations to address the coming wave of EAI innovation, including mandatory testing and certification for EAI systems, clarified liability frameworks, and forward-looking strategies to manage and prepare for transformative economic and societal impacts.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Whole New World: Creating a Parallel-Poisoned Web Only AI-Agents Can See</title>
<link>https://arxiv.org/abs/2509.00124</link>
<guid>https://arxiv.org/abs/2509.00124</guid>
<content:encoded><![CDATA[
arXiv:2509.00124v1 Announce Type: cross 
Abstract: This paper introduces a novel attack vector that leverages website cloaking techniques to compromise autonomous web-browsing agents powered by Large Language Models (LLMs). As these agents become more prevalent, their unique and often homogenous digital fingerprints - comprising browser attributes, automation framework signatures, and network characteristics - create a new, distinguishable class of web traffic. The attack exploits this fingerprintability. A malicious website can identify an incoming request as originating from an AI agent and dynamically serve a different, "cloaked" version of its content. While human users see a benign webpage, the agent is presented with a visually identical page embedded with hidden, malicious instructions, such as indirect prompt injections. This mechanism allows adversaries to hijack agent behavior, leading to data exfiltration, malware execution, or misinformation propagation, all while remaining completely invisible to human users and conventional security crawlers. This work formalizes the threat model, details the mechanics of agent fingerprinting and cloaking, and discusses the profound security implications for the future of agentic AI, highlighting the urgent need for robust defenses against this stealthy and scalable attack.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoComposer: LLM Multi-agent Collaborative Music Composition</title>
<link>https://arxiv.org/abs/2509.00132</link>
<guid>https://arxiv.org/abs/2509.00132</guid>
<content:encoded><![CDATA[
arXiv:2509.00132v1 Announce Type: cross 
Abstract: Existing AI Music composition tools are limited in generation duration, musical quality, and controllability. We introduce CoComposer, a multi-agent system that consists of five collaborating agents, each with a task based on the traditional music composition workflow. Using the AudioBox-Aesthetics system, we experimentally evaluate CoComposer on four compositional criteria. We test with three LLMs (GPT-4o, DeepSeek-V3-0324, Gemini-2.5-Flash), and find (1) that CoComposer outperforms existing multi-agent LLM-based systems in music quality, and (2) compared to a single-agent system, in production complexity. Compared to non- LLM MusicLM, CoComposer has better interpretability and editability, although MusicLM still produces better music.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Triplet Extraction for Automated Ontology Generation in Software Engineering Standards</title>
<link>https://arxiv.org/abs/2509.00140</link>
<guid>https://arxiv.org/abs/2509.00140</guid>
<content:encoded><![CDATA[
arXiv:2509.00140v1 Announce Type: cross 
Abstract: Ontologies have supported knowledge representation and whitebox reasoning for decades; thus, the automated ontology generation (AOG) plays a crucial role in scaling their use. Software engineering standards (SES) consist of long, unstructured text (with high noise) and paragraphs with domain-specific terms. In this setting, relation triple extraction (RTE), together with term extraction, constitutes the first stage toward AOG. This work proposes an open-source large language model (LLM)-assisted approach to RTE for SES. Instead of solely relying on prompt-engineering-based methods, this study promotes the use of LLMs as an aid in constructing ontologies and explores an effective AOG workflow that includes document segmentation, candidate term mining, LLM-based relation inference, term normalization, and cross-section alignment. Golden-standard benchmarks at three granularities are constructed and used to evaluate the ontology generated from the study. The results show that it is comparable and potentially superior to the OpenIE method of triple extraction.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Legal AI: Benchmarking Mamba and Transformers for Statutory Classification and Case Law Retrieval</title>
<link>https://arxiv.org/abs/2509.00141</link>
<guid>https://arxiv.org/abs/2509.00141</guid>
<content:encoded><![CDATA[
arXiv:2509.00141v1 Announce Type: cross 
Abstract: The rapid growth of statutory corpora and judicial decisions requires scalable legal AI systems capable of classification and retrieval over extremely long contexts. Transformer-based architectures (e.g., Longformer, DeBERTa) dominate current legal NLP benchmarks but struggle with quadratic attention costs, limiting efficiency and scalability. In this work, we present the first comprehensive benchmarking of Mamba, a state-space model (SSM) with linear-time selective mechanisms, against leading transformer models for statutory classification and case law retrieval. We evaluate models on open-source legal corpora including LexGLUE, EUR-Lex, and ILDC, covering statutory tagging, judicial outcome prediction, and case retrieval tasks. Metrics include accuracy, recall at k, mean reciprocal rank (MRR), and normalized discounted cumulative gain (nDCG), alongside throughput measured in tokens per second and maximum context length. Results show that Mamba's linear scaling enables processing of legal documents several times longer than transformers, while maintaining or surpassing retrieval and classification performance. This study introduces a new legal NLP benchmark suite for long-context modeling, along with open-source code and datasets to support reproducibility. Our findings highlight trade-offs between state-space models and transformers, providing guidance for deploying scalable legal AI in statutory analysis, judicial decision support, and policy research.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pilot Study on Generative AI and Critical Thinking in Higher Education Classrooms</title>
<link>https://arxiv.org/abs/2509.00167</link>
<guid>https://arxiv.org/abs/2509.00167</guid>
<content:encoded><![CDATA[
arXiv:2509.00167v1 Announce Type: cross 
Abstract: Generative AI (GAI) tools have seen rapid adoption in educational settings, yet their role in fostering critical thinking remains underexplored. While previous studies have examined GAI as a tutor for specific lessons or as a tool for completing assignments, few have addressed how students critically evaluate the accuracy and appropriateness of GAI-generated responses. This pilot study investigates students' ability to apply structured critical thinking when assessing Generative AI outputs in introductory Computational and Data Science courses. Given that GAI tools often produce contextually flawed or factually incorrect answers, we designed learning activities that require students to analyze, critique, and revise AI-generated solutions. Our findings offer initial insights into students' ability to engage critically with GAI content and lay the groundwork for more comprehensive studies in future semesters.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Approximation Methods for Efficient and Scalable Deep Learning</title>
<link>https://arxiv.org/abs/2509.00174</link>
<guid>https://arxiv.org/abs/2509.00174</guid>
<content:encoded><![CDATA[
arXiv:2509.00174v1 Announce Type: cross 
Abstract: Recent progress in deep learning has been driven by increasingly larger models. However, their computational and energy demands have grown proportionally, creating significant barriers to their deployment and to a wider adoption of deep learning technologies. This thesis investigates principled approximation methods for improving the efficiency of deep learning systems, with a particular focus on settings that involve discrete constraints and non-differentiability.
  We study three main approaches toward improved efficiency: architecture design, model compression, and optimization. For model compression, we propose novel approximations for pruning and quantization that frame the underlying discrete problem as continuous and differentiable, enabling gradient-based training of compression schemes alongside the model's parameters. These approximations allow for fine-grained sparsity and precision configurations, leading to highly compact models without significant fine-tuning. In the context of architecture design, we design an algorithm for neural architecture search that leverages parameter sharing across layers to efficiently explore implicitly recurrent architectures. Finally, we study adaptive optimization, revisiting theoretical properties of widely used methods and proposing an adaptive optimizer that allows for quick hyperparameter tuning.
  Our contributions center on tackling computationally hard problems via scalable and principled approximations. Experimental results on image classification, language modeling, and generative modeling tasks show that the proposed methods provide significant improvements in terms of training and inference efficiency while maintaining, or even improving, the model's performance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments</title>
<link>https://arxiv.org/abs/2509.00176</link>
<guid>https://arxiv.org/abs/2509.00176</guid>
<content:encoded><![CDATA[
arXiv:2509.00176v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have paved the way for Vision Large Language Models (VLLMs) capable of performing a wide range of visual understanding tasks. While LLMs have demonstrated impressive performance on standard natural images, their capabilities have not been thoroughly explored in cluttered datasets where there is complex environment having deformed shaped objects. In this work, we introduce a novel dataset specifically designed for waste classification in real-world scenarios, characterized by complex environments and deformed shaped objects. Along with this dataset, we present an in-depth evaluation approach to rigorously assess the robustness and accuracy of VLLMs. The introduced dataset and comprehensive analysis provide valuable insights into the performance of VLLMs under challenging conditions. Our findings highlight the critical need for further advancements in VLLM's robustness to perform better in complex environments. The dataset and code for our experiments will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Are Research Hypotheses?</title>
<link>https://arxiv.org/abs/2509.00185</link>
<guid>https://arxiv.org/abs/2509.00185</guid>
<content:encoded><![CDATA[
arXiv:2509.00185v1 Announce Type: cross 
Abstract: Over the past decades, alongside advancements in natural language processing, significant attention has been paid to training models to automatically extract, understand, test, and generate hypotheses in open and scientific domains. However, interpretations of the term \emph{hypothesis} for various natural language understanding (NLU) tasks have migrated from traditional definitions in the natural, social, and formal sciences. Even within NLU, we observe differences defining hypotheses across literature. In this paper, we overview and delineate various definitions of hypothesis. Especially, we discern the nuances of definitions across recently published NLU tasks. We highlight the importance of well-structured and well-defined hypotheses, particularly as we move toward a machine-interpretable scholarly record.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Audio Spoofing Detection using Non-Semantic Representations</title>
<link>https://arxiv.org/abs/2509.00186</link>
<guid>https://arxiv.org/abs/2509.00186</guid>
<content:encoded><![CDATA[
arXiv:2509.00186v1 Announce Type: cross 
Abstract: Rapid advancements in generative modeling have made synthetic audio generation easy, making speech-based services vulnerable to spoofing attacks. Consequently, there is a dire need for robust countermeasures more than ever. Existing solutions for deepfake detection are often criticized for lacking generalizability and fail drastically when applied to real-world data. This study proposes a novel method for generalizable spoofing detection leveraging non-semantic universal audio representations. Extensive experiments have been performed to find suitable non-semantic features using TRILL and TRILLsson models. The results indicate that the proposed method achieves comparable performance on the in-domain test set while significantly outperforming state-of-the-art approaches on out-of-domain test sets. Notably, it demonstrates superior generalization on public-domain data, surpassing methods based on hand-crafted features, semantic embeddings, and end-to-end architectures.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics</title>
<link>https://arxiv.org/abs/2509.00190</link>
<guid>https://arxiv.org/abs/2509.00190</guid>
<content:encoded><![CDATA[
arXiv:2509.00190v1 Announce Type: cross 
Abstract: Recent advances in chain-of-thought (CoT) prompting have enabled large language models (LLMs) to perform multi-step reasoning. However, the explainability of such reasoning remains limited, with prior work primarily focusing on local token-level attribution, such that the high-level semantic roles of reasoning steps and their transitions remain underexplored. In this paper, we introduce a state-aware transition framework that abstracts CoT trajectories into structured latent dynamics. Specifically, to capture the evolving semantics of CoT reasoning, each reasoning step is represented via spectral analysis of token-level embeddings and clustered into semantically coherent latent states. To characterize the global structure of reasoning, we model their progression as a Markov chain, yielding a structured and interpretable view of the reasoning process. This abstraction supports a range of analyses, including semantic role identification, temporal pattern visualization, and consistency evaluation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment</title>
<link>https://arxiv.org/abs/2509.00210</link>
<guid>https://arxiv.org/abs/2509.00210</guid>
<content:encoded><![CDATA[
arXiv:2509.00210v1 Announce Type: cross 
Abstract: Achieving human-like reasoning in deep learning models for complex tasks in unknown environments remains a critical challenge in embodied intelligence. While advanced vision-language models (VLMs) excel in static scene understanding, their limitations in spatio-temporal reasoning and adaptation to dynamic, open-set tasks like task-oriented navigation and embodied question answering (EQA) persist due to inadequate modeling of fine-grained spatio-temporal cues and physical world comprehension. To address this, we propose VEME, a novel cross-modal alignment method that enhances generalization in unseen scenes by learning an ego-centric, experience-centered world model. Our framework integrates three key components: (1) a cross-modal alignment framework bridging objects, spatial representations, and visual semantics with spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic, implicit cognitive map activated by world embedding to enable task-relevant geometric-semantic memory recall; and (3) an instruction-based navigation and reasoning framework leveraging embodied priors for long-term planning and efficient exploration. By embedding geometry-aware spatio-temporal episodic experiences, our method significantly improves reasoning and planning in dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate 1%-3% accuracy and exploration efficiency improvement compared to traditional approaches.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Deep Learning for Phyllodes Tumor Classification from Ultrasound and Clinical Data</title>
<link>https://arxiv.org/abs/2509.00213</link>
<guid>https://arxiv.org/abs/2509.00213</guid>
<content:encoded><![CDATA[
arXiv:2509.00213v1 Announce Type: cross 
Abstract: Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are difficult to classify preoperatively due to their radiological similarity to benign fibroadenomas. This often leads to unnecessary surgical excisions. To address this, we propose a multimodal deep learning framework that integrates breast ultrasound (BUS) images with structured clinical data to improve diagnostic accuracy. We developed a dual-branch neural network that extracts and fuses features from ultrasound images and patient metadata from 81 subjects with confirmed PTs. Class-aware sampling and subject-stratified 5-fold cross-validation were applied to prevent class imbalance and data leakage. The results show that our proposed multimodal method outperforms unimodal baselines in classifying benign versus borderline/malignant PTs. Among six image encoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal setting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and 0.7294, respectively. This study demonstrates the potential of multimodal AI to serve as a non-invasive diagnostic tool, reducing unnecessary biopsies and improving clinical decision-making in breast tumor management.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Order Model-Based RL through Decoupled Backpropagation</title>
<link>https://arxiv.org/abs/2509.00215</link>
<guid>https://arxiv.org/abs/2509.00215</guid>
<content:encoded><![CDATA[
arXiv:2509.00215v1 Announce Type: cross 
Abstract: There is growing interest in reinforcement learning (RL) methods that leverage the simulator's derivatives to improve learning efficiency. While early gradient-based approaches have demonstrated superior performance compared to derivative-free methods, accessing simulator gradients is often impractical due to their implementation cost or unavailability. Model-based RL (MBRL) can approximate these gradients via learned dynamics models, but the solver efficiency suffers from compounding prediction errors during training rollouts, which can degrade policy performance. We propose an approach that decouples trajectory generation from gradient computation: trajectories are unrolled using a simulator, while gradients are computed via backpropagation through a learned differentiable model of the simulator. This hybrid design enables efficient and consistent first-order policy optimization, even when simulator gradients are unavailable, as well as learning a critic from simulation rollouts, which is more accurate. Our method achieves the sample efficiency and speed of specialized optimizers such as SHAC, while maintaining the generality of standard approaches like PPO and avoiding ill behaviors observed in other first-order MBRL methods. We empirically validate our algorithm on benchmark control tasks and demonstrate its effectiveness on a real Go2 quadruped robot, across both quadrupedal and bipedal locomotion tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied AI in Social Spaces: Responsible and Adaptive Robots in Complex Setting - UKAIRS 2025 (Copy)</title>
<link>https://arxiv.org/abs/2509.00218</link>
<guid>https://arxiv.org/abs/2509.00218</guid>
<content:encoded><![CDATA[
arXiv:2509.00218v1 Announce Type: cross 
Abstract: This paper introduces and overviews a multidisciplinary project aimed at developing responsible and adaptive multi-human multi-robot (MHMR) systems for complex, dynamic settings. The project integrates co-design, ethical frameworks, and multimodal sensing to create AI-driven robots that are emotionally responsive, context-aware, and aligned with the needs of diverse users. We outline the project's vision, methodology, and early outcomes, demonstrating how embodied AI can support sustainable, ethical, and human-centred futures.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Effectiveness of Transformer Layers in Wav2Vec 2.0, XLS-R, and Whisper for Speaker Identification Tasks</title>
<link>https://arxiv.org/abs/2509.00230</link>
<guid>https://arxiv.org/abs/2509.00230</guid>
<content:encoded><![CDATA[
arXiv:2509.00230v1 Announce Type: cross 
Abstract: This study evaluates the performance of three advanced speech encoder models, Wav2Vec 2.0, XLS-R, and Whisper, in speaker identification tasks. By fine-tuning these models and analyzing their layer-wise representations using SVCCA, k-means clustering, and t-SNE visualizations, we found that Wav2Vec 2.0 and XLS-R capture speaker-specific features effectively in their early layers, with fine-tuning improving stability and performance. Whisper showed better performance in deeper layers. Additionally, we determined the optimal number of transformer layers for each model when fine-tuned for speaker identification tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Criteria for Credible AI-assisted Carbon Footprinting Systems: The Cases of Mapping and Lifecycle Modeling</title>
<link>https://arxiv.org/abs/2509.00240</link>
<guid>https://arxiv.org/abs/2509.00240</guid>
<content:encoded><![CDATA[
arXiv:2509.00240v1 Announce Type: cross 
Abstract: As organizations face increasing pressure to understand their corporate and products' carbon footprints, artificial intelligence (AI)-assisted calculation systems for footprinting are proliferating, but with widely varying levels of rigor and transparency. Standards and guidance have not kept pace with the technology; evaluation datasets are nascent; and statistical approaches to uncertainty analysis are not yet practical to apply to scaled systems. We present a set of criteria to validate AI-assisted systems that calculate greenhouse gas (GHG) emissions for products and materials. We implement a three-step approach: (1) Identification of needs and constraints, (2) Draft criteria development and (3) Refinements through pilots. The process identifies three use cases of AI applications: Case 1 focuses on AI-assisted mapping to existing datasets for corporate GHG accounting and product hotspotting, automating repetitive manual tasks while maintaining mapping quality. Case 2 addresses AI systems that generate complete product models for corporate decision-making, which require comprehensive validation of both component tasks and end-to-end performance. We discuss the outlook for Case 3 applications, systems that generate standards-compliant models. We find that credible AI systems can be built and that they should be validated using system-level evaluations rather than line-item review, with metrics such as benchmark performance, indications of data quality and uncertainty, and transparent documentation. This approach may be used as a foundation for practitioners, auditors, and standards bodies to evaluate AI-assisted environmental assessment tools. By establishing evaluation criteria that balance scalability with credibility requirements, our approach contributes to the field's efforts to develop appropriate standards for AI-assisted carbon footprinting systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions</title>
<link>https://arxiv.org/abs/2509.00248</link>
<guid>https://arxiv.org/abs/2509.00248</guid>
<content:encoded><![CDATA[
arXiv:2509.00248v1 Announce Type: cross 
Abstract: The proliferation of methods for modeling of human meaning-making constitutes a powerful class of instruments for the analysis of complex semiotic systems. However, the field lacks a general theoretical framework for describing these modeling practices across various model types in an apples-to-apples way. In this paper, we propose such a framework grounded in the semiotic theory of C. S. Peirce. We argue that such models measure latent symbol geometries, which can be understood as hypotheses about the complex of semiotic agencies underlying a symbolic dataset. Further, we argue that in contexts where a model's value cannot be straightforwardly captured by proxy measures of performance, models can instead be understood relationally, so that the particular interpretive lens of a model becomes visible through its contrast with other models. This forms the basis of a theory of model semantics in which models, and the modeling decisions that constitute them, are themselves treated as signs. In addition to proposing the framework, we illustrate its empirical use with a few brief examples and consider foundational questions and future directions enabled by the framework.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Hidden Precursors to Earthquakes via a Stress-Sensitive Transformation of Seismic Noise</title>
<link>https://arxiv.org/abs/2509.00268</link>
<guid>https://arxiv.org/abs/2509.00268</guid>
<content:encoded><![CDATA[
arXiv:2509.00268v1 Announce Type: cross 
Abstract: Earthquake prediction has long been one of the most elusive challenges in science. Laboratory experiments and simulations suggest that failure precursors should exist, yet reliable signals have remained unobserved in real-world seismic records, leaving open the question of whether they are absent in nature or simply hidden within noise. Here we introduce a stress-sensitive frequency-domain transformation that tracks energy differences between adjacent frequency bands, isolating subtle spectral changes linked to evolving shear and normal stress. Applied to both laboratory acoustic emission data and seismic records from seven major earthquakes (Mw 5.9-9.0), including the 2011 Tohoku and 2023 Turkey-Syria events, the transform consistently reveals precursory signatures, arc-like trajectories and accelerations toward extrema, emerging hours to days before rupture. These features are robust across diverse tectonic settings, from induced seismicity and volcanic collapse to continental strike-slip and subduction megathrust earthquakes. Our findings demonstrate that hidden precursors are indeed encoded in ambient seismic noise, offering a pathway toward real-time fault monitoring and actionable short-term earthquake forecasting.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SABER: A SQL-Compatible Semantic Document Processing System Based on Extended Relational Algebra</title>
<link>https://arxiv.org/abs/2509.00277</link>
<guid>https://arxiv.org/abs/2509.00277</guid>
<content:encoded><![CDATA[
arXiv:2509.00277v1 Announce Type: cross 
Abstract: The emergence of large-language models (LLMs) has enabled a new class of semantic data processing systems (SDPSs) to support declarative queries against unstructured documents. Existing SDPSs are, however, lacking a unified algebraic foundation, making their queries difficult to compose, reason, and optimize. We propose a new semantic algebra, SABER (Semantic Algebra Based on Extended Relational algebra), opening the possibility of semantic operations' logical plan construction, optimization, and formal correctness guarantees. We further propose to implement SABER in a SQL-compatible syntax so that it natively supports mixed structured/unstructured data processing. With SABER, we showcase the feasibility of providing a unified interface for existing SDPSs so that it can effectively mix and match any semantically-compatible operator implementation from any SDPS, greatly enhancing SABER's applicability for community contributions.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Industrial Contour Detection: A Language-Guided Vision System</title>
<link>https://arxiv.org/abs/2509.00284</link>
<guid>https://arxiv.org/abs/2509.00284</guid>
<content:encoded><![CDATA[
arXiv:2509.00284v1 Announce Type: cross 
Abstract: Industrial computer vision systems often struggle with noise, material variability, and uncontrolled imaging conditions, limiting the effectiveness of classical edge detectors and handcrafted pipelines. In this work, we present a language-guided generative vision system for remnant contour detection in manufacturing, designed to achieve CAD-level precision. The system is organized into three stages: data acquisition and preprocessing, contour generation using a conditional GAN, and multimodal contour refinement through vision-language modeling, where standardized prompts are crafted in a human-in-the-loop process and applied through image-text guided synthesis. On proprietary FabTrack datasets, the proposed system improved contour fidelity, enhancing edge continuity and geometric alignment while reducing manual tracing. For the refinement stage, we benchmarked several vision-language models, including Google's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided workflow, and open-source baselines. Under standardized conditions, GPT-image-1 consistently outperformed Gemini 2.0 Flash in both structural accuracy and perceptual quality. These findings demonstrate the promise of VLM-guided generative workflows for advancing industrial computer vision beyond the limitations of classical pipelines.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews</title>
<link>https://arxiv.org/abs/2509.00285</link>
<guid>https://arxiv.org/abs/2509.00285</guid>
<content:encoded><![CDATA[
arXiv:2509.00285v1 Announce Type: cross 
Abstract: We study the problem of opinion highlights generation from large volumes of user reviews, often exceeding thousands per entity, where existing methods either fail to scale or produce generic, one-size-fits-all summaries that overlook personalized needs. To tackle this, we introduce OpinioRAG, a scalable, training-free framework that combines RAG-based evidence retrieval with LLMs to efficiently produce tailored summaries. Additionally, we propose novel reference-free verification metrics designed for sentiment-rich domains, where accurately capturing opinions and sentiment alignment is essential. These metrics offer a fine-grained, context-sensitive assessment of factual consistency. To facilitate evaluation, we contribute the first large-scale dataset of long-form user reviews, comprising entities with over a thousand reviews each, paired with unbiased expert summaries and manually annotated queries. Through extensive experiments, we identify key challenges, provide actionable insights into improving systems, pave the way for future research, and position OpinioRAG as a robust framework for generating accurate, relevant, and structured summaries at scale.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Spectrum Management in Satellite Communications</title>
<link>https://arxiv.org/abs/2509.00286</link>
<guid>https://arxiv.org/abs/2509.00286</guid>
<content:encoded><![CDATA[
arXiv:2509.00286v1 Announce Type: cross 
Abstract: Satellite Communication (SatCom) networks represent a fundamental pillar in modern global connectivity, facilitating reliable service and extensive coverage across a plethora of applications. The expanding demand for high-bandwidth services and the proliferation of mega satellite constellations highlight the limitations of traditional exclusive satellite spectrum allocation approaches. Cognitive Radio (CR) leading to Cognitive Satellite (CogSat) networks through Dynamic Spectrum Management (DSM), which enables the dynamic adaptability of radio equipment to environmental conditions for optimal performance, presents a promising solution for the emerging spectrum scarcity. In this survey, we explore the adaptation of intelligent DSM methodologies to SatCom, leveraging satellite network integrations. We discuss contributions and hurdles in regulations and standardizations in realizing intelligent DSM in SatCom, and deep dive into DSM techniques, which enable CogSat networks. Furthermore, we extensively evaluate and categorize state-of-the-art Artificial Intelligence (AI)/Machine Learning (ML) methods leveraged for DSM while exploring operational resilience and robustness of such integrations. In addition, performance evaluation metrics critical for adaptive resource management and system optimization in CogSat networks are thoroughly investigated. This survey also identifies open challenges and outlines future research directions in regulatory frameworks, network architectures, and intelligent spectrum management, paving the way for sustainable and scalable SatCom networks for enhanced global connectivity.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Access Paths for Efficient Ordering with Large Language Models</title>
<link>https://arxiv.org/abs/2509.00303</link>
<guid>https://arxiv.org/abs/2509.00303</guid>
<content:encoded><![CDATA[
arXiv:2509.00303v1 Announce Type: cross 
Abstract: We present the LLM ORDER BY operator as a logical abstraction and study its physical implementations within a unified evaluation framework. Our experiments show that no single approach is universally optimal, with effectiveness depending on query characteristics and data. We introduce three new designs: an agreement-based batch-size policy, a majority voting mechanism for pairwise sorting, and a two-way external merge sort adapted for LLMs. With extensive experiments, our agreement-based procedure is effective at determining batch size for value-based methods, the majority-voting mechanism consistently strengthens pairwise comparisons on GPT-4o, and external merge sort achieves high accuracy-efficiency trade-offs across datasets and models. We further observe a log-linear scaling between compute cost and ordering quality, offering the first step toward principled cost models for LLM powered data systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization</title>
<link>https://arxiv.org/abs/2509.00310</link>
<guid>https://arxiv.org/abs/2509.00310</guid>
<content:encoded><![CDATA[
arXiv:2509.00310v1 Announce Type: cross 
Abstract: Robots often struggle to generalize from a single demonstration due to the lack of a transferable and interpretable spatial representation. In this work, we introduce TReF-6, a method that infers a simplified, abstracted 6DoF Task-Relevant Frame from a single trajectory. Our approach identifies an influence point purely from the trajectory geometry to define the origin for a local frame, which serves as a reference for parameterizing a Dynamic Movement Primitive (DMP). This influence point captures the task's spatial structure, extending the standard DMP formulation beyond start-goal imitation. The inferred frame is semantically grounded via a vision-language model and localized in novel scenes by Grounded-SAM, enabling functionally consistent skill generalization. We validate TReF-6 in simulation and demonstrate robustness to trajectory noise. We further deploy an end-to-end pipeline on real-world manipulation tasks, showing that TReF-6 supports one-shot imitation learning that preserves task intent across diverse object configurations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuously Tempered Diffusion Samplers</title>
<link>https://arxiv.org/abs/2509.00316</link>
<guid>https://arxiv.org/abs/2509.00316</guid>
<content:encoded><![CDATA[
arXiv:2509.00316v1 Announce Type: cross 
Abstract: Annealing-based neural samplers seek to amortize sampling from unnormalized distributions by training neural networks to transport a family of densities interpolating from source to target. A crucial design choice in the training phase of such samplers is the proposal distribution by which locations are generated at which to evaluate the loss. Previous work has obtained such a proposal distribution by combining a partially learned transport with annealed Langevin dynamics. However, isolated modes and other pathological properties of the annealing path imply that such proposals achieve insufficient exploration and thereby lower performance post training. To remedy this, we propose continuously tempered diffusion samplers, which leverage exploration techniques developed in the context of molecular dynamics to improve proposal distributions. Specifically, a family of distributions across different temperatures is introduced to lower energy barriers at higher temperatures and drive exploration at the lower temperature of interest. We empirically validate improved sampler performance driven by extended exploration. Code is available at https://github.com/eje24/ctds.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Task and Motion Planning based on Expanding AND/OR Graphs</title>
<link>https://arxiv.org/abs/2509.00317</link>
<guid>https://arxiv.org/abs/2509.00317</guid>
<content:encoded><![CDATA[
arXiv:2509.00317v1 Announce Type: cross 
Abstract: Robot autonomy in space environments presents unique challenges, including high perception and motion uncertainty, strict kinematic constraints, and limited opportunities for human intervention. Therefore, Task and Motion Planning (TMP) may be critical for autonomous servicing, surface operations, or even in-orbit missions, just to name a few, as it models tasks as discrete action sequencing integrated with continuous motion feasibility assessments. In this paper, we introduce a TMP framework based on expanding AND/OR graphs, referred to as TMP-EAOG, and demonstrate its adaptability to different scenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph, which expands iteratively as the plan is executed, and performs in-the-loop motion planning assessments to ascertain their feasibility. As a consequence, TMP-EAOG is characterised by the desirable properties of (i) robustness to a certain degree of uncertainty, because AND/OR graph expansion can accommodate for unpredictable information about the robot environment, (ii) controlled autonomy, since an AND/OR graph can be validated by human experts, and (iii) bounded flexibility, in that unexpected events, including the assessment of unfeasible motions, can lead to different courses of action as alternative paths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We use a simulated mobile manipulator as a proxy for space-grade autonomous robots. Our evaluation shows that TMP-EAOG can deal with a wide range of challenges in the benchmarks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach</title>
<link>https://arxiv.org/abs/2509.00319</link>
<guid>https://arxiv.org/abs/2509.00319</guid>
<content:encoded><![CDATA[
arXiv:2509.00319v1 Announce Type: cross 
Abstract: Navigating a flexible robotic endoscope (FRE) through the gastrointestinal tract is critical for surgical diagnosis and treatment. However, navigation in the dynamic stomach is particularly challenging because the FRE must learn to effectively use contact with the deformable stomach walls to reach target locations. To address this, we introduce a deep reinforcement learning (DRL) based Contact-Aided Navigation (CAN) strategy for FREs, leveraging contact force feedback to enhance motion stability and navigation precision. The training environment is established using a physics-based finite element method (FEM) simulation of a deformable stomach. Trained with the Proximal Policy Optimization (PPO) algorithm, our approach achieves high navigation success rates (within 3 mm error between the FRE's end-effector and target) and significantly outperforms baseline policies. In both static and dynamic stomach environments, the CAN agent achieved a 100% success rate with 1.6 mm average error, and it maintained an 85% success rate in challenging unseen scenarios with stronger external disturbances. These results validate that the DRL-based CAN strategy substantially enhances FRE navigation performance over prior methods.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic Endoluminal Navigation of Deformable Continuum Robots</title>
<link>https://arxiv.org/abs/2509.00329</link>
<guid>https://arxiv.org/abs/2509.00329</guid>
<content:encoded><![CDATA[
arXiv:2509.00329v1 Announce Type: cross 
Abstract: Deformable continuum robots (DCRs) present unique planning challenges due to nonlinear deformation mechanics and partial state observability, violating the Markov assumptions of conventional reinforcement learning (RL) methods. While Jacobian-based approaches offer theoretical foundations for rigid manipulators, their direct application to DCRs remains limited by time-varying kinematics and underactuated deformation dynamics. This paper proposes Jacobian Exploratory Dual-Phase RL (JEDP-RL), a framework that decomposes planning into phased Jacobian estimation and policy execution. During each training step, we first perform small-scale local exploratory actions to estimate the deformation Jacobian matrix, then augment the state representation with Jacobian features to restore approximate Markovianity. Extensive SOFA surgical dynamic simulations demonstrate JEDP-RL's three key advantages over proximal policy optimization (PPO) baselines: 1) Convergence speed: 3.2x faster policy convergence, 2) Navigation efficiency: requires 25% fewer steps to reach the target, and 3) Generalization ability: achieve 92% success rate under material property variations and achieve 83% (33% higher than PPO) success rate in the unseen tissue environment.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Option Learning in High-Throughput Environments</title>
<link>https://arxiv.org/abs/2509.00338</link>
<guid>https://arxiv.org/abs/2509.00338</guid>
<content:encoded><![CDATA[
arXiv:2509.00338v1 Announce Type: cross 
Abstract: Hierarchical reinforcement learning (RL) has the potential to enable effective decision-making over long timescales. Existing approaches, while promising, have yet to realize the benefits of large-scale training. In this work, we identify and solve several key challenges in scaling hierarchical RL to high-throughput environments. We propose Scalable Option Learning (SOL), a highly scalable hierarchical RL algorithm which achieves a 25x higher throughput compared to existing hierarchical methods. We train our hierarchical agents using 20 billion frames of experience on the complex game of NetHack, significantly surpassing flat agents and demonstrating positive scaling trends. We also validate our algorithm on MiniHack and Mujoco environments, showcasing its general applicability. Our code is open sourced at github.com/facebookresearch/sol.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.00347</link>
<guid>https://arxiv.org/abs/2509.00347</guid>
<content:encoded><![CDATA[
arXiv:2509.00347v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) is known for its strong decision-making capabilities and has been widely applied in various real-world scenarios. However, with the increasing availability of offline datasets and the lack of well-designed online environments from human experts, the challenge of generalization in offline RL has become more prominent. Due to the limitations of offline data, RL agents trained solely on collected experiences often struggle to generalize to new tasks or environments. To address this challenge, we propose LLM-Driven Policy Diffusion (LLMDPD), a novel approach that enhances generalization in offline RL using task-specific prompts. Our method incorporates both text-based task descriptions and trajectory prompts to guide policy learning. We leverage a large language model (LLM) to process text-based prompts, utilizing its natural language understanding and extensive knowledge base to provide rich task-relevant context. Simultaneously, we encode trajectory prompts using a transformer model, capturing structured behavioral patterns within the underlying transition dynamics. These prompts serve as conditional inputs to a context-aware policy-level diffusion model, enabling the RL agent to generalize effectively to unseen tasks. Our experimental results demonstrate that LLMDPD outperforms state-of-the-art offline RL methods on unseen tasks, highlighting its effectiveness in improving generalization and adaptability in diverse settings.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theory Foundation of Physics-Enhanced Residual Learning</title>
<link>https://arxiv.org/abs/2509.00348</link>
<guid>https://arxiv.org/abs/2509.00348</guid>
<content:encoded><![CDATA[
arXiv:2509.00348v1 Announce Type: cross 
Abstract: Intensive studies have been conducted in recent years to integrate neural networks with physics models to balance model accuracy and interpretability. One recently proposed approach, named Physics-Enhanced Residual Learning (PERL), is to use learning to estimate the residual between the physics model prediction and the ground truth. Numeral examples suggested that integrating such residual with physics models in PERL has three advantages: (1) a reduction in the number of required neural network parameters; (2) faster convergence rates; and (3) fewer training samples needed for the same computational precision. However, these numerical results lack theoretical justification and cannot be adequately explained.
  This paper aims to explain these advantages of PERL from a theoretical perspective. We investigate a general class of problems with Lipschitz continuity properties. By examining the relationships between the bounds to the loss function and residual learning structure, this study rigorously proves a set of theorems explaining the three advantages of PERL.
  Several numerical examples in the context of automated vehicle trajectory prediction are conducted to illustrate the proposed theorems. The results confirm that, even with significantly fewer training samples, PERL consistently achieves higher accuracy than a pure neural network. These results demonstrate the practical value of PERL in real world autonomous driving applications where corner case data are costly or hard to obtain. PERL therefore improves predictive performance while reducing the amount of data required.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Target-Oriented Single Domain Generalization</title>
<link>https://arxiv.org/abs/2509.00351</link>
<guid>https://arxiv.org/abs/2509.00351</guid>
<content:encoded><![CDATA[
arXiv:2509.00351v1 Announce Type: cross 
Abstract: Deep models trained on a single source domain often fail catastrophically under distribution shifts, a critical challenge in Single Domain Generalization (SDG). While existing methods focus on augmenting source data or learning invariant features, they neglect a readily available resource: textual descriptions of the target deployment environment. We propose Target-Oriented Single Domain Generalization (TO-SDG), a novel problem setup that leverages the textual description of the target domain, without requiring any target data, to guide model generalization. To address TO-SDG, we introduce Spectral TARget Alignment (STAR), a lightweight module that injects target semantics into source features by exploiting visual-language models (VLMs) such as CLIP. STAR uses a target-anchored subspace derived from the text embedding of the target description to recenter image features toward the deployment domain, then utilizes spectral projection to retain directions aligned with target cues while discarding source-specific noise. Moreover, we use a vision-language distillation to align backbone features with VLM's semantic geometry. STAR further employs feature-space Mixup to ensure smooth transitions between source and target-oriented representations. Experiments across various image classification and object detection benchmarks demonstrate STAR's superiority. This work establishes that minimal textual metadata, which is a practical and often overlooked resource, significantly enhances generalization under severe data constraints, opening new avenues for deploying robust models in target environments with unseen data.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data</title>
<link>https://arxiv.org/abs/2509.00353</link>
<guid>https://arxiv.org/abs/2509.00353</guid>
<content:encoded><![CDATA[
arXiv:2509.00353v1 Announce Type: cross 
Abstract: Air pollution monitoring in resource-constrained regions remains challenging due to sparse sensor deployment and limited infrastructure. This work introduces AQFusionNet, a multimodal deep learning framework for robust Air Quality Index (AQI) prediction. The framework integrates ground-level atmospheric imagery with pollutant concentration data using lightweight CNN backbones (MobileNetV2, ResNet18, EfficientNet-B0). Visual and sensor features are combined through semantically aligned embedding spaces, enabling accurate and efficient prediction. Experiments on more than 8,000 samples from India and Nepal demonstrate that AQFusionNet consistently outperforms unimodal baselines, achieving up to 92.02% classification accuracy and an RMSE of 7.70 with the EfficientNet-B0 backbone. The model delivers an 18.5% improvement over single-modality approaches while maintaining low computational overhead, making it suitable for deployment on edge devices. AQFusionNet provides a scalable and practical solution for AQI monitoring in infrastructure-limited environments, offering robust predictive capability even under partial sensor availability.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding</title>
<link>https://arxiv.org/abs/2509.00357</link>
<guid>https://arxiv.org/abs/2509.00357</guid>
<content:encoded><![CDATA[
arXiv:2509.00357v1 Announce Type: cross 
Abstract: Surgical video understanding is crucial for facilitating Computer-Assisted Surgery (CAS) systems. Despite significant progress in existing studies, two major limitations persist, including inadequate visual content perception and insufficient temporal awareness in surgical videos, and hinder the development of versatile CAS solutions. In this work, we propose the SurgLLM framework, an effective large multimodal model tailored for versatile surgical video understanding tasks with enhanced spatial focus and temporal awareness. Specifically, to empower the spatial focus of surgical videos, we first devise Surgical Context-aware Multimodal Pretraining (Surg-Pretrain) for the video encoder of SurgLLM, by performing instrument-centric Masked Video Reconstruction (MV-Recon) and subsequent multimodal alignment. To incorporate surgical temporal knowledge into SurgLLM, we further propose Temporal-aware Multimodal Tuning (TM-Tuning) to enhance temporal reasoning with interleaved multimodal embeddings. Moreover, to accommodate various understanding tasks of surgical videos without conflicts, we devise a Surgical Task Dynamic Ensemble to efficiently triage a query with optimal learnable parameters in our SurgLLM. Extensive experiments performed on diverse surgical video understanding tasks, including captioning, general VQA, and temporal VQA, demonstrate significant improvements over the state-of-the-art approaches, validating the effectiveness of our SurgLLM in versatile surgical video understanding. The source code is available at https://github.com/franciszchen/SurgLLM.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models</title>
<link>https://arxiv.org/abs/2509.00373</link>
<guid>https://arxiv.org/abs/2509.00373</guid>
<content:encoded><![CDATA[
arXiv:2509.00373v1 Announce Type: cross 
Abstract: Vision Language Models (VLMs) have demonstrated impressive capabilities in integrating visual and textual information for understanding and reasoning, but remain highly vulnerable to adversarial attacks. While activation steering has emerged as a promising defence, existing approaches often rely on task-specific contrastive prompts to extract harmful directions, which exhibit suboptimal performance and can degrade visual grounding performance. To address these limitations, we propose \textit{Sequence-Level Preference Optimization} for VLM (\textit{SPO-VLM}), a novel two-stage defense framework that combines activation-level intervention with policy-level optimization to enhance model robustness. In \textit{Stage I}, we compute adaptive layer-specific steering vectors from diverse data sources, enabling generalized suppression of harmful behaviors during inference. In \textit{Stage II}, we refine these steering vectors through a sequence-level preference optimization process. This stage integrates automated toxicity assessment, as well as visual-consistency rewards based on caption-image alignment, to achieve safe and semantically grounded text generation. The two-stage structure of SPO-VLM balances efficiency and effectiveness by combining a lightweight mitigation foundation in Stage I with deeper policy refinement in Stage II. Extensive experiments shown SPO-VLM enhances safety against attacks via activation steering and preference optimization, while maintaining strong performance on benign tasks without compromising visual understanding capabilities. We will release our code, model weights, and evaluation toolkit to support reproducibility and future research. \textcolor{red}{Warning: This paper may contain examples of offensive or harmful text and images.}
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open Data Synthesis For Deep Research</title>
<link>https://arxiv.org/abs/2509.00375</link>
<guid>https://arxiv.org/abs/2509.00375</guid>
<content:encoded><![CDATA[
arXiv:2509.00375v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in \href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Adversarial Perturbation for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.00387</link>
<guid>https://arxiv.org/abs/2509.00387</guid>
<content:encoded><![CDATA[
arXiv:2509.00387v1 Announce Type: cross 
Abstract: This paper studies the vulnerability of Graph Neural Networks (GNNs) to adversarial attacks on node features and graph structure. Various methods have implemented adversarial training to augment graph data, aiming to bolster the robustness and generalization of GNNs. These methods typically involve applying perturbations to the node feature, weights, or graph structure and subsequently minimizing the loss by learning more robust graph model parameters under the adversarial perturbations. Despite the effectiveness of adversarial training in enhancing GNNs' robustness and generalization abilities, its application has been largely confined to specific datasets and GNN types. In this paper, we propose a novel method, PerturbEmbedding, that integrates adversarial perturbation and training, enhancing GNNs' resilience to such attacks and improving their generalization ability. PerturbEmbedding performs perturbation operations directly on every hidden embedding of GNNs and provides a unified framework for most existing perturbation strategies/methods. We also offer a unified perspective on the forms of perturbations, namely random and adversarial perturbations. Through experiments on various datasets using different backbone models, we demonstrate that PerturbEmbedding significantly improves both the robustness and generalization abilities of GNNs, outperforming existing methods. The rejection of both random (non-targeted) and adversarial (targeted) perturbations further enhances the backbone model's performance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Negative Transfer: Disentangled Preference-Guided Diffusion for Cross-Domain Sequential Recommendation</title>
<link>https://arxiv.org/abs/2509.00389</link>
<guid>https://arxiv.org/abs/2509.00389</guid>
<content:encoded><![CDATA[
arXiv:2509.00389v1 Announce Type: cross 
Abstract: Cross-Domain Sequential Recommendation (CDSR) leverages user behaviors across domains to enhance recommendation quality. However, naive aggregation of sequential signals can introduce conflicting domain-specific preferences, leading to negative transfer. While Sequential Recommendation (SR) already suffers from noisy behaviors such as misclicks and impulsive actions, CDSR further amplifies this issue due to domain heterogeneity arising from diverse item types and user intents. The core challenge is disentangling three intertwined signals: domain-invariant preferences, domain-specific preferences, and noise. Diffusion Models (DMs) offer a generative denoising framework well-suited for disentangling complex user preferences and enhancing robustness to noise. Their iterative refinement process enables gradual denoising, making them effective at capturing subtle preference signals. However, existing applications in recommendation face notable limitations: sequential DMs often conflate shared and domain-specific preferences, while cross-domain collaborative filtering DMs neglect temporal dynamics, limiting their ability to model evolving user preferences. To bridge these gaps, we propose \textbf{DPG-Diff}, a novel Disentangled Preference-Guided Diffusion Model, the first diffusion-based approach tailored for CDSR, to or best knowledge. DPG-Diff decomposes user preferences into domain-invariant and domain-specific components, which jointly guide the reverse diffusion process. This disentangled guidance enables robust cross-domain knowledge transfer, mitigates negative transfer, and filters sequential noise. Extensive experiments on real-world datasets demonstrate that DPG-Diff consistently outperforms state-of-the-art baselines across multiple metrics.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Resurgence of GCG Adversarial Attacks on Large Language Models</title>
<link>https://arxiv.org/abs/2509.00391</link>
<guid>https://arxiv.org/abs/2509.00391</guid>
<content:encoded><![CDATA[
arXiv:2509.00391v1 Announce Type: cross 
Abstract: Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient (GCG) algorithm, has emerged as a powerful method for jailbreaking large language models (LLMs). In this paper, we present a systematic appraisal of GCG and its annealing-augmented variant, T-GCG, across open-source LLMs of varying scales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack effectiveness on both safety-oriented prompts (AdvBench) and reasoning-intensive coding prompts. Our study reveals three key findings: (1) attack success rates (ASR) decrease with model size, reflecting the increasing complexity and non-convexity of larger models' loss landscapes; (2) prefix-based heuristics substantially overestimate attack effectiveness compared to GPT-4o semantic judgments, which provide a stricter and more realistic evaluation; and (3) coding-related prompts are significantly more vulnerable than adversarial safety prompts, suggesting that reasoning itself can be exploited as an attack vector. In addition, preliminary results with T-GCG show that simulated annealing can diversify adversarial search and achieve competitive ASR under prefix evaluation, though its benefits under semantic judgment remain limited. Together, these findings highlight the scalability limits of GCG, expose overlooked vulnerabilities in reasoning tasks, and motivate further development of annealing-inspired strategies for more robust adversarial evaluation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAOVI: Distortion-Aware Omnidirectional Video Inpainting</title>
<link>https://arxiv.org/abs/2509.00396</link>
<guid>https://arxiv.org/abs/2509.00396</guid>
<content:encoded><![CDATA[
arXiv:2509.00396v1 Announce Type: cross 
Abstract: Omnidirectional videos that capture the entire surroundings are employed in a variety of fields such as VR applications and remote sensing. However, their wide field of view often causes unwanted objects to appear in the videos. This problem can be addressed by video inpainting, which enables the natural removal of such objects while preserving both spatial and temporal consistency. Nevertheless, most existing methods assume processing ordinary videos with a narrow field of view and do not tackle the distortion in equirectangular projection of omnidirectional videos. To address this issue, this paper proposes a novel deep learning model for omnidirectional video inpainting, called Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI introduces a module that evaluates temporal motion information in the image space considering geodesic distance, as well as a depth-aware feature propagation module in the feature space that is designed to address the geometric distortion inherent to omnidirectional videos. The experimental results demonstrate that our proposed method outperforms existing methods both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on the Framework for Evaluating the Ethics and Trustworthiness of Generative AI</title>
<link>https://arxiv.org/abs/2509.00398</link>
<guid>https://arxiv.org/abs/2509.00398</guid>
<content:encoded><![CDATA[
arXiv:2509.00398v1 Announce Type: cross 
Abstract: This study provides an in_depth analysis of the ethical and trustworthiness challenges emerging alongside the rapid advancement of generative artificial intelligence (AI) technologies and proposes a comprehensive framework for their systematic evaluation. While generative AI, such as ChatGPT, demonstrates remarkable innovative potential, it simultaneously raises ethical and social concerns, including bias, harmfulness, copyright infringement, privacy violations, and hallucination. Current AI evaluation methodologies, which mainly focus on performance and accuracy, are insufficient to address these multifaceted issues. Thus, this study emphasizes the need for new human_centered criteria that also reflect social impact. To this end, it identifies key dimensions for evaluating the ethics and trustworthiness of generative AI_fairness, transparency, accountability, safety, privacy, accuracy, consistency, robustness, explainability, copyright and intellectual property protection, and source traceability and develops detailed indicators and assessment methodologies for each. Moreover, it provides a comparative analysis of AI ethics policies and guidelines in South Korea, the United States, the European Union, and China, deriving key approaches and implications from each. The proposed framework applies across the AI lifecycle and integrates technical assessments with multidisciplinary perspectives, thereby offering practical means to identify and manage ethical risks in real_world contexts. Ultimately, the study establishes an academic foundation for the responsible advancement of generative AI and delivers actionable insights for policymakers, developers, users, and other stakeholders, supporting the positive societal contributions of AI technologies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum Guided Personalized Subgraph Federated Learning</title>
<link>https://arxiv.org/abs/2509.00402</link>
<guid>https://arxiv.org/abs/2509.00402</guid>
<content:encoded><![CDATA[
arXiv:2509.00402v1 Announce Type: cross 
Abstract: Subgraph Federated Learning (FL) aims to train Graph Neural Networks (GNNs) across distributed private subgraphs, but it suffers from severe data heterogeneity. To mitigate data heterogeneity, weighted model aggregation personalizes each local GNN by assigning larger weights to parameters from clients with similar subgraph characteristics inferred from their current model states. However, the sparse and biased subgraphs often trigger rapid overfitting, causing the estimated client similarity matrix to stagnate or even collapse. As a result, aggregation loses effectiveness as clients reinforce their own biases instead of exploiting diverse knowledge otherwise available. To this end, we propose a novel personalized subgraph FL framework called Curriculum guided personalized sUbgraph Federated Learning (CUFL). On the client side, CUFL adopts Curriculum Learning (CL) that adaptively selects edges for training according to their reconstruction scores, exposing each GNN first to easier, generic cross-client substructures and only later to harder, client-specific ones. This paced exposure prevents early overfitting to biased patterns and enables gradual personalization. By regulating personalization, the curriculum also reshapes server aggregation from exchanging generic knowledge to propagating client-specific knowledge. Further, CUFL improves weighted aggregation by estimating client similarity using fine-grained structural indicators reconstructed on a random reference graph. Extensive experiments on six benchmark datasets confirm that CUFL achieves superior performance compared to relevant baselines. Code is available at https://github.com/Kang-Min-Ku/CUFL.git.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature</title>
<link>https://arxiv.org/abs/2509.00414</link>
<guid>https://arxiv.org/abs/2509.00414</guid>
<content:encoded><![CDATA[
arXiv:2509.00414v1 Announce Type: cross 
Abstract: In the digital age, people often turn to the Internet in search of medical advice and recommendations. With the increasing volume of online content, it has become difficult to distinguish reliable sources from misleading information. Similarly, millions of medical studies are published every year, making it challenging for researchers to keep track of the latest scientific findings. These evolving studies can reach differing conclusions, which is not reflected in traditional search tools. To address these challenges, we introduce MedSEBA, an interactive AI-powered system for synthesizing evidence-based answers to medical questions. It utilizes the power of Large Language Models to generate coherent and expressive answers, but grounds them in trustworthy medical studies dynamically retrieved from the research database PubMed. The answers consist of key points and arguments, which can be traced back to respective studies. Notably, the platform also provides an overview of the extent to which the most relevant studies support or refute the given medical claim, and a visualization of how the research consensus evolved through time. Our user study revealed that medical experts and lay users find the system usable and helpful, and the provided answers trustworthy and informative. This makes the system well-suited for both everyday health questions and advanced research insights.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TECP: Token-Entropy Conformal Prediction for LLMs</title>
<link>https://arxiv.org/abs/2509.00461</link>
<guid>https://arxiv.org/abs/2509.00461</guid>
<content:encoded><![CDATA[
arXiv:2509.00461v1 Announce Type: cross 
Abstract: Uncertainty quantification (UQ) for open-ended language generation remains a critical yet underexplored challenge, especially under black-box constraints where internal model signals are inaccessible. In this paper, we introduce Token-Entropy Conformal Prediction (TECP), a novel framework that leverages token-level entropy as a logit-free, reference-free uncertainty measure and integrates it into a split conformal prediction (CP) pipeline to construct prediction sets with formal coverage guarantees. Unlike existing approaches that rely on semantic consistency heuristics or white-box features, TECP directly estimates epistemic uncertainty from the token entropy structure of sampled generations and calibrates uncertainty thresholds via CP quantiles to ensure provable error control. Empirical evaluations across six large language models and two benchmarks (CoQA and TriviaQA) demonstrate that TECP consistently achieves reliable coverage and compact prediction sets, outperforming prior self-consistency-based UQ methods. Our method provides a principled and efficient solution for trustworthy generation in black-box LLM settings.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial Reasoning</title>
<link>https://arxiv.org/abs/2509.00465</link>
<guid>https://arxiv.org/abs/2509.00465</guid>
<content:encoded><![CDATA[
arXiv:2509.00465v1 Announce Type: cross 
Abstract: This thesis introduces "Embodied Spatial Intelligence" to address the challenge of creating robots that can perceive and act in the real world based on natural language instructions. To bridge the gap between Large Language Models (LLMs) and physical embodiment, we present contributions on two fronts: scene representation and spatial reasoning. For perception, we develop robust, scalable, and accurate scene representations using implicit neural models, with contributions in self-supervised camera calibration, high-fidelity depth field generation, and large-scale reconstruction. For spatial reasoning, we enhance the spatial capabilities of LLMs by introducing a novel navigation benchmark, a method for grounding language in 3D, and a state-feedback mechanism to improve long-horizon decision-making. This work lays a foundation for robots that can robustly perceive their surroundings and intelligently act upon complex, language-based commands.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Domain Malware Detection via Probability-Level Fusion of Lightweight Gradient Boosting Models</title>
<link>https://arxiv.org/abs/2509.00476</link>
<guid>https://arxiv.org/abs/2509.00476</guid>
<content:encoded><![CDATA[
arXiv:2509.00476v1 Announce Type: cross 
Abstract: The escalating sophistication of malware necessitates robust detection mechanisms that generalize across diverse data sources. Traditional single-dataset models struggle with cross-domain generalization and often incur high computational costs. This paper presents a novel, lightweight framework for malware detection that employs probability-level fusion across three distinct datasets: EMBER (static features), API Call Sequences (behavioral features), and CIC Obfuscated Memory (memory patterns). Our method trains individual LightGBM classifiers on each dataset, selects top predictive features to ensure efficiency, and fuses their prediction probabilities using optimized weights determined via grid search. Extensive experiments demonstrate that our fusion approach achieves a macro F1-score of 0.823 on a cross-domain validation set, significantly outperforming individual models and providing superior generalization. The framework maintains low computational overhead, making it suitable for real-time deployment, and all code and data are provided for full reproducibility.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Method to Determine Total Oxidant Concentration Produced by Non-Thermal Plasma Based on Image Processing and Machine Learning</title>
<link>https://arxiv.org/abs/2509.00479</link>
<guid>https://arxiv.org/abs/2509.00479</guid>
<content:encoded><![CDATA[
arXiv:2509.00479v1 Announce Type: cross 
Abstract: Accurate determination of total oxidant concentration ([Ox]_{tot}) in non-thermal plasma (NTP)-treated aqueous systems remains a critical challenge due to the transient nature of reactive oxygen and nitrogen species and the subjectivity of conventional titration methods used for [Ox]_{tot} determination. This study introduces a novel, color-based computer analysis (CBCA) method that integrates advanced image processing with machine learning (ML) to quantify colorimetric shifts in potassium iodide (KI) solutions during oxidation. First, a custom-built visual data acquisition system captured high-resolution video of the color transitions in a KI solution during oxidation with an NTP system. The change in [Ox]_{tot} during the experiments was monitored with a standard titrimetric method. Second, the captured frames were processed using a robust image processing pipeline to extract RGB, HSV, and Lab color features. The extracted features were statistically evaluated, and the results revealed strong linear correlations with the measured [Ox]_{tot} values, particularly in the saturation (HSV), a and b (Lab), and blue (RGB) channels. Subsequently, the [Ox]_{tot} measurements and the extracted color features were used to train and validate five ML models. Among them, linear regression and gradient boosting models achieved the highest predictive accuracy (R^2 > 0.990). It was also found that reducing the feature set from nine to four resulted in comparable performance with improved prediction efficiency, especially for gradient boosting. Finally, comparison of the model predictions with real titration measurements revealed that the CBCA system successfully predicts the [Ox]_{tot} in KI solution with high accuracy (R^2 > 0.998) even with a reduced number of features.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting</title>
<link>https://arxiv.org/abs/2509.00482</link>
<guid>https://arxiv.org/abs/2509.00482</guid>
<content:encoded><![CDATA[
arXiv:2509.00482v1 Announce Type: cross 
Abstract: This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) human-crafted role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques--character-card/scene-contract design and strict enforcement of function calling--which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool. Source code is available at https://github.com/scb-10x/apo.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding</title>
<link>https://arxiv.org/abs/2509.00484</link>
<guid>https://arxiv.org/abs/2509.00484</guid>
<content:encoded><![CDATA[
arXiv:2509.00484v1 Announce Type: cross 
Abstract: Multimodal reward models (MRMs) play a crucial role in the training, inference, and evaluation of Large Vision Language Models (LVLMs) by assessing response quality. However, existing benchmarks for evaluating MRMs in the video domain suffer from a limited number and diversity of questions, a lack of comprehensive evaluation dimensions, and inadequate evaluation of diverse types of MRMs. To address these gaps, we introduce VideoRewardBench, the first comprehensive benchmark covering four core aspects of video understanding: perception, knowledge, reasoning, and safety. Through our AI-assisted data pipeline, we curate a high-quality preference dataset of 1,563 annotated samples, including 1,482 unique videos and 1,559 distinct questions--15 times the number found in the most question-rich prior benchmark. Each sample is a triplet consisting of a video-text prompt, a chosen response, and a rejected response. We also conduct a comprehensive evaluation across 28 multimodal reward models spanning three categories: generative, discriminative, and semi-scalar. Results show that even the top-performing model GPT-4o achieves only 57.0% overall accuracy, and the state-of-the-art open-source model Qwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three key insights: (i) MRMs trained with reinforcement learning (RL) do not necessarily exhibit stronger cross-modal generalization than those trained without RL; (ii) except for discriminative MRMs, other types of MRMs across varying model capacities can benefit from inference-time scaling; and (iii) variations in input video frame count have different effects on different types of MRMs. We believe VideoRewardBench offers a challenging and valuable benchmark for advancing the evaluation and development of MRMs in the video domain.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Focused Video Group Activities Hashing</title>
<link>https://arxiv.org/abs/2509.00490</link>
<guid>https://arxiv.org/abs/2509.00490</guid>
<content:encoded><![CDATA[
arXiv:2509.00490v1 Announce Type: cross 
Abstract: With the explosive growth of video data in various complex scenarios, quickly retrieving group activities has become an urgent problem. However, many tasks can only retrieve videos focusing on an entire video, not the activity granularity. To solve this problem, we propose a new STVH (spatiotemporal interleaved video hashing) technique for the first time. Through a unified framework, the STVH simultaneously models individual object dynamics and group interactions, capturing the spatiotemporal evolution on both group visual features and positional features. Moreover, in real-life video retrieval scenarios, it may sometimes require activity features, while at other times, it may require visual features of objects. We then further propose a novel M-STVH (multi-focused spatiotemporal video hashing) as an enhanced version to handle this difficult task. The advanced method incorporates hierarchical feature integration through multi-focused representation learning, allowing the model to jointly focus on activity semantics features and object visual features. We conducted comparative experiments on publicly available datasets, and both STVH and M-STVH can achieve excellent results.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics</title>
<link>https://arxiv.org/abs/2509.00496</link>
<guid>https://arxiv.org/abs/2509.00496</guid>
<content:encoded><![CDATA[
arXiv:2509.00496v1 Announce Type: cross 
Abstract: Evaluating long-form responses to research queries heavily relies on expert annotators, restricting attention to areas like AI where researchers can conveniently enlist colleagues. Yet, research expertise is widespread: survey articles synthesize knowledge distributed across the literature. We introduce ResearchQA, a resource for evaluating LLM systems by distilling survey articles from 75 research fields into 21K queries and 160K rubric items. Each rubric, derived jointly with queries from survey sections, lists query-specific answer evaluation criteria, i.e., citing papers, making explanations, and describing limitations. Assessments by 31 Ph.D. annotators in 8 fields indicate 96% of queries support Ph.D. information needs and 87% of rubric items should be addressed in system responses by a sentence or more. Using our rubrics, we are able to construct an automatic pairwise judge obtaining 74% agreement with expert judgments. We leverage ResearchQA to analyze competency gaps in 18 systems in over 7.6K pairwise evaluations. No parametric or retrieval-augmented system we evaluate exceeds 70% on covering rubric items, and the highest-ranking agentic system shows 75% coverage. Error analysis reveals that the highest-ranking system fully addresses less than 11% of citation rubric items, 48% of limitation items, and 49% of comparison items. We release our data to facilitate more comprehensive multi-field evaluations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuralSVCD for Efficient Swept Volume Collision Detection</title>
<link>https://arxiv.org/abs/2509.00499</link>
<guid>https://arxiv.org/abs/2509.00499</guid>
<content:encoded><![CDATA[
arXiv:2509.00499v1 Announce Type: cross 
Abstract: Robot manipulation in unstructured environments requires efficient and reliable Swept Volume Collision Detection (SVCD) for safe motion planning. Traditional discrete methods potentially miss collisions between these points, whereas SVCD continuously checks for collisions along the entire trajectory. Existing SVCD methods typically face a trade-off between efficiency and accuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a novel neural encoder-decoder architecture tailored to overcome this trade-off. Our approach leverages shape locality and temporal locality through distributed geometric representations and temporal optimization. This enhances computational efficiency without sacrificing accuracy. Comprehensive experiments show that NeuralSVCD consistently outperforms existing state-of-the-art SVCD methods in terms of both collision detection accuracy and computational efficiency, demonstrating its robust applicability across diverse robotic manipulation scenarios. Code and videos are available at https://neuralsvcd.github.io/.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatentEdit: Adaptive Latent Control for Consistent Semantic Editing</title>
<link>https://arxiv.org/abs/2509.00541</link>
<guid>https://arxiv.org/abs/2509.00541</guid>
<content:encoded><![CDATA[
arXiv:2509.00541v1 Announce Type: cross 
Abstract: Diffusion-based Image Editing has achieved significant success in recent years. However, it remains challenging to achieve high-quality image editing while maintaining the background similarity without sacrificing speed or memory efficiency. In this work, we introduce LatentEdit, an adaptive latent fusion framework that dynamically combines the current latent code with a reference latent code inverted from the source image. By selectively preserving source features in high-similarity, semantically important regions while generating target content in other regions guided by the target prompt, LatentEdit enables fine-grained, controllable editing. Critically, the method requires no internal model modifications or complex attention mechanisms, offering a lightweight, plug-and-play solution compatible with both UNet-based and DiT-based architectures. Extensive experiments on the PIE-Bench dataset demonstrate that our proposed LatentEdit achieves an optimal balance between fidelity and editability, outperforming the state-of-the-art method even in 8-15 steps. Additionally, its inversion-free variant further halves the number of neural function evaluations and eliminates the need for storing any intermediate variables, substantially enhancing real-time deployment efficiency.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI be Auditable?</title>
<link>https://arxiv.org/abs/2509.00575</link>
<guid>https://arxiv.org/abs/2509.00575</guid>
<content:encoded><![CDATA[
arXiv:2509.00575v1 Announce Type: cross 
Abstract: Auditability is defined as the capacity of AI systems to be independently assessed for compliance with ethical, legal, and technical standards throughout their lifecycle. The chapter explores how auditability is being formalized through emerging regulatory frameworks, such as the EU AI Act, which mandate documentation, risk assessments, and governance structures. It analyzes the diverse challenges facing AI auditability, including technical opacity, inconsistent documentation practices, lack of standardized audit tools and metrics, and conflicting principles within existing responsible AI frameworks. The discussion highlights the need for clear guidelines, harmonized international regulations, and robust socio-technical methodologies to operationalize auditability at scale. The chapter concludes by emphasizing the importance of multi-stakeholder collaboration and auditor empowerment in building an effective AI audit ecosystem. It argues that auditability must be embedded in AI development practices and governance infrastructures to ensure that AI systems are not only functional but also ethically and legally aligned.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache</title>
<link>https://arxiv.org/abs/2509.00579</link>
<guid>https://arxiv.org/abs/2509.00579</guid>
<content:encoded><![CDATA[
arXiv:2509.00579v1 Announce Type: cross 
Abstract: Transformer-based large language models (LLMs) demonstrate impressive potential in various practical applications. However, long context inference poses a significant challenge due to the enormous memory requirements of the key-value (KV) cache, which can scale to multiple gigabytes as sequence length and batch size increase. In this paper, we present KVComp, a generic and efficient KV cache management framework optimized for long-text generation that synergistically works with both latency-critical and throughput-critical inference systems. KVComp employs novel lossy compression techniques specifically designed for KV cache data characteristics, featuring careful co-design of compression algorithms and system architecture. Our approach maintains compatibility with the growing nature of KV cache while preserving high computational efficiency. Experimental results show that KVComp achieves on average 47\% and up to 83\% higher memory reduction rate compared to existing methods with little/no model accuracy degradation. Furthermore, KVComp achieves extremely high execution throughput, effectively reducing decompression overhead and, in some cases, even accelerating the matrix-vector multiplication operation and outperform cuBLAS-based attention kernels with less data movement.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Survival Analysis with Node-Level Differential Privacy: Private Kaplan-Meier Curves</title>
<link>https://arxiv.org/abs/2509.00615</link>
<guid>https://arxiv.org/abs/2509.00615</guid>
<content:encoded><![CDATA[
arXiv:2509.00615v1 Announce Type: cross 
Abstract: We investigate how to calculate Kaplan-Meier survival curves across multiple health-care jurisdictions while protecting patient privacy with node-level differential privacy. Each site discloses its curve only once, adding Laplace noise whose scale is determined by the length of the common time grid; the server then averages the noisy curves, so the overall privacy budget remains unchanged. We benchmark four one-shot smoothing techniques: Discrete Cosine Transform, Haar Wavelet shrinkage, adaptive Total-Variation denoising, and a parametric Weibull fit on the NCCTG lung-cancer cohort under five privacy levels and three partition scenarios (uniform, moderately skewed, highly imbalanced). Total-Variation gives the best mean accuracy, whereas the frequency-domain smoothers offer stronger worst-case robustness and the Weibull model shows the most stable behaviour at the strictest privacy setting. Across all methods the released curves keep the empirical log-rank type-I error below fifteen percent for privacy budgets of 0.5 and higher, demonstrating that clinically useful survival information can be shared without iterative training or heavy cryptography.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeCopilot</title>
<link>https://arxiv.org/abs/2509.00616</link>
<guid>https://arxiv.org/abs/2509.00616</guid>
<content:encoded><![CDATA[
arXiv:2509.00616v1 Announce Type: cross 
Abstract: We introduce TimeCopilot, the first open-source agentic framework for forecasting that combines multiple Time Series Foundation Models (TSFMs) with Large Language Models (LLMs) through a single unified API. TimeCopilot automates the forecasting pipeline: feature analysis, model selection, cross-validation, and forecast generation, while providing natural language explanations and supporting direct queries about the future. The framework is LLM-agnostic, compatible with both commercial and open-source models, and supports ensembles across diverse forecasting families. Results on the large-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art probabilistic forecasting performance at low cost. Our framework provides a practical foundation for reproducible, explainable, and accessible agentic forecasting systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Strategy Approach for AI-Generated Text Detection</title>
<link>https://arxiv.org/abs/2509.00623</link>
<guid>https://arxiv.org/abs/2509.00623</guid>
<content:encoded><![CDATA[
arXiv:2509.00623v1 Announce Type: cross 
Abstract: This paper presents presents three distinct systems developed for the M-DAIGT shared task on detecting AI generated content in news articles and academic abstracts. The systems includes: (1) A fine-tuned RoBERTa-base classifier, (2) A classical TF-IDF + Support Vector Machine (SVM) classifier , and (3) An Innovative ensemble model named Candace, leveraging probabilistic features extracted from multiple Llama-3.2 models processed by a customTransformer encoder.The RoBERTa-based system emerged as the most performant, achieving near-perfect results on both development and test sets.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Methane Detection Onboard Satellites</title>
<link>https://arxiv.org/abs/2509.00626</link>
<guid>https://arxiv.org/abs/2509.00626</guid>
<content:encoded><![CDATA[
arXiv:2509.00626v1 Announce Type: cross 
Abstract: Methane is a potent greenhouse gas and a major driver of climate change, making its timely detection critical for effective mitigation. Machine learning (ML) deployed onboard satellites can enable rapid detection while reducing downlink costs, supporting faster response systems. Conventional methane detection methods often rely on image processing techniques, such as orthorectification to correct geometric distortions and matched filters to enhance plume signals. We introduce a novel approach that bypasses these preprocessing steps by using \textit{unorthorectified} data (UnorthoDOS). We find that ML models trained on this dataset achieve performance comparable to those trained on orthorectified data. Moreover, we also train models on an orthorectified dataset, showing that they can outperform the matched filter baseline (mag1c). We release model checkpoints and two ML-ready datasets comprising orthorectified and unorthorectified hyperspectral images from the Earth Surface Mineral Dust Source Investigation (EMIT) sensor at https://huggingface.co/datasets/SpaceML/UnorthoDOS , along with code at https://github.com/spaceml-org/plume-hunter.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers</title>
<link>https://arxiv.org/abs/2509.00631</link>
<guid>https://arxiv.org/abs/2509.00631</guid>
<content:encoded><![CDATA[
arXiv:2509.00631v1 Announce Type: cross 
Abstract: The ionosphere critically influences Global Navigation Satellite Systems (GNSS), satellite communications, and Low Earth Orbit (LEO) operations, yet accurate prediction of its variability remains challenging due to nonlinear couplings between solar, geomagnetic, and thermospheric drivers. Total Electron Content (TEC), a key ionospheric parameter, is derived from GNSS observations, but its reliable forecasting is limited by the sparse nature of global measurements and the limited accuracy of empirical models, especially during strong space weather conditions. In this work, we present a machine learning framework for ionospheric TEC forecasting that leverages Temporal Fusion Transformers (TFT) to predict sparse ionosphere data. Our approach accommodates heterogeneous input sources, including solar irradiance, geomagnetic indices, and GNSS-derived vertical TEC, and applies preprocessing and temporal alignment strategies. Experiments spanning 2010-2025 demonstrate that the model achieves robust predictions up to 24 hours ahead, with root mean square errors as low as 3.33 TECU. Results highlight that solar EUV irradiance provides the strongest predictive signals. Beyond forecasting accuracy, the framework offers interpretability through attention-based analysis, supporting both operational applications and scientific discovery. To encourage reproducibility and community-driven development, we release the full implementation as the open-source toolkit \texttt{ionopy}.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Trustworthy Federated Learning via Remote Attestation for Mitigating Byzantine Threats</title>
<link>https://arxiv.org/abs/2509.00634</link>
<guid>https://arxiv.org/abs/2509.00634</guid>
<content:encoded><![CDATA[
arXiv:2509.00634v1 Announce Type: cross 
Abstract: Federated Learning (FL) has gained significant attention for its privacy-preserving capabilities, enabling distributed devices to collaboratively train a global model without sharing raw data. However, its distributed nature forces the central server to blindly trust the local training process and aggregate uncertain model updates, making it susceptible to Byzantine attacks from malicious participants, especially in mission-critical scenarios. Detecting such attacks is challenging due to the diverse knowledge across clients, where variations in model updates may stem from benign factors, such as non-IID data, rather than adversarial behavior. Existing data-driven defenses struggle to distinguish malicious updates from natural variations, leading to high false positive rates and poor filtering performance.
  To address this challenge, we propose Sentinel, a remote attestation (RA)-based scheme for FL systems that regains client-side transparency and mitigates Byzantine attacks from a system security perspective. Our system employs code instrumentation to track control-flow and monitor critical variables in the local training process. Additionally, we utilize a trusted training recorder within a Trusted Execution Environment (TEE) to generate an attestation report, which is cryptographically signed and securely transmitted to the server. Upon verification, the server ensures that legitimate client training processes remain free from program behavior violation or data manipulation, allowing only trusted model updates to be aggregated into the global model. Experimental results on IoT devices demonstrate that Sentinel ensures the trustworthiness of the local training integrity with low runtime and memory overhead.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NMR-Solver: Automated Structure Elucidation via Large-Scale Spectral Matching and Physics-Guided Fragment Optimization</title>
<link>https://arxiv.org/abs/2509.00640</link>
<guid>https://arxiv.org/abs/2509.00640</guid>
<content:encoded><![CDATA[
arXiv:2509.00640v1 Announce Type: cross 
Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is one of the most powerful and widely used tools for molecular structure elucidation in organic chemistry. However, the interpretation of NMR spectra to determine unknown molecular structures remains a labor-intensive and expertise-dependent process, particularly for complex or novel compounds. Although recent methods have been proposed for molecular structure elucidation, they often underperform in real-world applications due to inherent algorithmic limitations and limited high-quality data. Here, we present NMR-Solver, a practical and interpretable framework for the automated determination of small organic molecule structures from $^1$H and $^{13}$C NMR spectra. Our method introduces an automated framework for molecular structure elucidation, integrating large-scale spectral matching with physics-guided fragment-based optimization that exploits atomic-level structure-spectrum relationships in NMR. We evaluate NMR-Solver on simulated benchmarks, curated experimental data from the literature, and real-world experiments, demonstrating its strong generalization, robustness, and practical utility in challenging, real-life scenarios. NMR-Solver unifies computational NMR analysis, deep learning, and interpretable chemical reasoning into a coherent system. By incorporating the physical principles of NMR into molecular optimization, it enables scalable, automated, and chemically meaningful molecular identification, establishing a generalizable paradigm for solving inverse problems in molecular science.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG-PRISM: A Personalized, Rapid, and Immersive Skill Mastery Framework with Adaptive Retrieval-Augmented Tutoring</title>
<link>https://arxiv.org/abs/2509.00646</link>
<guid>https://arxiv.org/abs/2509.00646</guid>
<content:encoded><![CDATA[
arXiv:2509.00646v1 Announce Type: cross 
Abstract: The rapid digital transformation of Fourth Industrial Revolution (4IR) systems is reshaping workforce needs, widening skill gaps, especially for older workers. With growing emphasis on STEM skills such as robotics, automation, artificial intelligence (AI), and security, large-scale re-skilling and up-skilling are required. Training programs must address diverse backgrounds, learning styles, and motivations to improve persistence and success, while ensuring rapid, cost-effective workforce development through experiential learning. To meet these challenges, we present an adaptive tutoring framework that combines generative AI with Retrieval-Augmented Generation (RAG) to deliver personalized training. The framework leverages document hit rate and Mean Reciprocal Rank (MRR) to optimize content for each learner, and is benchmarked against human-generated training for alignment and relevance. We demonstrate the framework in 4IR cybersecurity learning by creating a synthetic QA dataset emulating trainee behavior, while RAG is tuned on curated cybersecurity materials. Evaluation compares its generated training with manually curated queries representing realistic student interactions. Responses are produced using large language models (LLMs) including GPT-3.5 and GPT-4, assessed for faithfulness and content alignment. GPT-4 achieves the best performance with 87% relevancy and 100% alignment. Results show this dual-mode approach enables the adaptive tutor to act as both a personalized topic recommender and content generator, offering a scalable solution for rapid, tailored learning in 4IR education and workforce development.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-HyPZ: Hardware Vulnerability Discovery using an LLM-Assisted Hybrid Platform for Zero-Shot Knowledge Extraction and Refinement</title>
<link>https://arxiv.org/abs/2509.00647</link>
<guid>https://arxiv.org/abs/2509.00647</guid>
<content:encoded><![CDATA[
arXiv:2509.00647v1 Announce Type: cross 
Abstract: The rapid growth of hardware vulnerabilities has created an urgent need for systematic and scalable analysis methods. Unlike software flaws, which are often patchable post-deployment, hardware weaknesses remain embedded across product lifecycles, posing persistent risks to processors, embedded devices, and IoT platforms. Existing efforts such as the MITRE CWE Hardware List (2021) relied on expert-driven Delphi surveys, which lack statistical rigor and introduce subjective bias, while large-scale data-driven foundations for hardware weaknesses have been largely absent. In this work, we propose LLM-HyPZ, an LLM-assisted hybrid framework for zero-shot knowledge extraction and refinement from vulnerability corpora. Our approach integrates zero-shot LLM classification, contextualized embeddings, unsupervised clustering, and prompt-driven summarization to mine hardware-related CVEs at scale. Applying LLM-HyPZ to the 2021-2024 CVE corpus (114,836 entries), we identified 1,742 hardware-related vulnerabilities. We distilled them into five recurring themes, including privilege escalation via firmware and BIOS, memory corruption in mobile and IoT systems, and physical access exploits. Benchmarking across seven LLMs shows that LLaMA 3.3 70B achieves near-perfect classification accuracy (99.5%) on a curated validation set. Beyond methodological contributions, our framework directly supported the MITRE CWE Most Important Hardware Weaknesses (MIHW) 2025 update by narrowing the candidate search space. Specifically, our pipeline surfaced 411 of the 1,026 CVEs used for downstream MIHW analysis, thereby reducing expert workload and accelerating evidence gathering. These results establish LLM-HyPZ as the first data-driven, scalable approach for systematically discovering hardware vulnerabilities, thereby bridging the gap between expert knowledge and real-world vulnerability evidence.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather Forecasting over India</title>
<link>https://arxiv.org/abs/2509.00653</link>
<guid>https://arxiv.org/abs/2509.00653</guid>
<content:encoded><![CDATA[
arXiv:2509.00653v1 Announce Type: cross 
Abstract: Regional weather forecasting is a critical problem for localized climate adaptation, disaster mitigation, and sustainable development. While machine learning has shown impressive progress in global weather forecasting, regional forecasting remains comparatively underexplored. Existing efforts often use different datasets and experimental setups, limiting fair comparison and reproducibility. We introduce IndiaWeatherBench, a comprehensive benchmark for data-driven regional weather forecasting focused on the Indian subcontinent. IndiaWeatherBench provides a curated dataset built from high-resolution regional reanalysis products, along with a suite of deterministic and probabilistic metrics to facilitate consistent training and evaluation. To establish strong baselines, we implement and evaluate a range of models across diverse architectures, including UNets, Transformers, and Graph-based networks, as well as different boundary conditioning strategies and training objectives. While focused on India, IndiaWeatherBench is easily extensible to other geographic regions. We open-source all raw and preprocessed datasets, model implementations, and evaluation pipelines to promote accessibility and future development. We hope IndiaWeatherBench will serve as a foundation for advancing regional weather forecasting research. Code is available at https://github.com/tung-nd/IndiaWeatherBench.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Name-Free Gap: Policy-Aware Stylistic Control in Music Generation</title>
<link>https://arxiv.org/abs/2509.00654</link>
<guid>https://arxiv.org/abs/2509.00654</guid>
<content:encoded><![CDATA[
arXiv:2509.00654v1 Announce Type: cross 
Abstract: Text-to-music models capture broad attributes such as instrumentation or mood, but fine-grained stylistic control remains an open challenge. Existing stylization methods typically require retraining or specialized conditioning, which complicates reproducibility and limits policy compliance when artist names are restricted. We study whether lightweight, human-readable modifiers sampled from a large language model can provide a policy-robust alternative for stylistic control. Using MusicGen-small, we evaluate two artists: Billie Eilish (vocal pop) and Ludovico Einaudi (instrumental piano). For each artist, we use fifteen reference excerpts and evaluate matched seeds under three conditions: baseline prompts, artist-name prompts, and five descriptor sets. All prompts are generated using a large language model. Evaluation uses both VGGish and CLAP embeddings with distributional and per-clip similarity measures, including a new min-distance attribution metric. Results show that artist names are the strongest control signal across both artists, while name-free descriptors recover much of this effect. This highlights that existing safeguards such as the restriction of artist names in music generation prompts may not fully prevent style imitation. Cross-artist transfers reduce alignment, showing that descriptors encode targeted stylistic cues. We also present a descriptor table across ten contemporary artists to illustrate the breadth of the tokens. Together these findings define the name-free gap, the controllability difference between artist-name prompts and policy-compliant descriptors, shown through a reproducible evaluation protocol for prompt-level controllability.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model</title>
<link>https://arxiv.org/abs/2509.00664</link>
<guid>https://arxiv.org/abs/2509.00664</guid>
<content:encoded><![CDATA[
arXiv:2509.00664v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have made significant progress in bridging visual perception with high-level textual reasoning. However, they face a fundamental contradiction: while excelling at complex semantic understanding, these models often fail at basic visual tasks that require precise detail perception. This deficiency primarily stems from the prevalent architectural reliance on a single vision encoder optimized for high-level semantic alignment, which inherently sacrifices the ability to capture fine-grained visual information. To address this issue, we introduce Fusion to Enhance (FtZ), a novel vision tower framework. FtZ moves beyond the single-encoder design by innovatively composing a semantically powerful anchor encoder with a perception-rich augmenting encoder via a lightweight Multi-Head Cross-Attention mechanism. Experimental results demonstrate that on several challenging benchmarks demanding fine-grained visual understanding, such as TextVQA, POPE, MMMU, MME and MM-Vet, our FtZ model significantly outperforms baselines that use only a single encoder or existing feature fusion methods. This work proves that composing heterogeneous expert encoders is an efficient and effective path to overcoming the visual perception bottleneck in current MLLMs, offering a new design paradigm for building next-generation AI systems with stronger perceptual capabilities.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech</title>
<link>https://arxiv.org/abs/2509.00673</link>
<guid>https://arxiv.org/abs/2509.00673</guid>
<content:encoded><![CDATA[
arXiv:2509.00673v1 Announce Type: cross 
Abstract: We investigate the efficacy of Large Language Models (LLMs) in detecting implicit and explicit hate speech, examining whether models with minimal safety alignment (uncensored) might provide more objective classification capabilities compared to their heavily-aligned (censored) counterparts. While uncensored models theoretically offer a less constrained perspective free from moral guardrails that could bias classification decisions, our results reveal a surprising trade-off: censored models significantly outperform their uncensored counterparts in both accuracy and robustness, achieving 78.7% versus 64.1% strict accuracy. However, this enhanced performance comes with its own limitation -- the safety alignment acts as a strong ideological anchor, making censored models resistant to persona-based influence, while uncensored models prove highly malleable to ideological framing. Furthermore, we identify critical failures across all models in understanding nuanced language such as irony. We also find alarming fairness disparities in performance across different targeted groups and systemic overconfidence that renders self-reported certainty unreliable. These findings challenge the notion of LLMs as objective arbiters and highlight the need for more sophisticated auditing frameworks that account for fairness, calibration, and ideological consistency.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Valid Property-Enhanced Contrastive Learning for Targeted Optimization &amp; Resampling for Novel Drug Design</title>
<link>https://arxiv.org/abs/2509.00684</link>
<guid>https://arxiv.org/abs/2509.00684</guid>
<content:encoded><![CDATA[
arXiv:2509.00684v1 Announce Type: cross 
Abstract: Efficiently steering generative models toward pharmacologically relevant regions of chemical space remains a major obstacle in molecular drug discovery under low-data regimes. We present VECTOR+: Valid-property-Enhanced Contrastive Learning for Targeted Optimization and Resampling, a framework that couples property-guided representation learning with controllable molecule generation. VECTOR+ applies to both regression and classification tasks and enables interpretable, data-efficient exploration of functional chemical space. We evaluate on two datasets: a curated PD-L1 inhibitor set (296 compounds with experimental $IC_{50}$ values) and a receptor kinase inhibitor set (2,056 molecules by binding mode). Despite limited training data, VECTOR+ generates novel, synthetically tractable candidates. Against PD-L1 (PDB 5J89), 100 of 8,374 generated molecules surpass a docking threshold of $-15.0$ kcal/mol, with the best scoring $-17.6$ kcal/mol compared to the top reference inhibitor ($-15.4$ kcal/mol). The best-performing molecules retain the conserved biphenyl pharmacophore while introducing novel motifs. Molecular dynamics (250 ns) confirm binding stability (ligand RMSD < $2.5$ angstroms). VECTOR+ generalizes to kinase inhibitors, producing compounds with stronger docking scores than established drugs such as brigatinib and sorafenib. Benchmarking against JT-VAE and MolGPT across docking, novelty, uniqueness, and Tanimoto similarity highlights the superior performance of our method. These results position our work as a robust, extensible approach for property-conditioned molecular design in low-data settings, bridging contrastive learning and generative modeling for reproducible, AI-accelerated discovery.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DELTA: Variational Disentangled Learning for Privacy-Preserving Data Reprogramming</title>
<link>https://arxiv.org/abs/2509.00693</link>
<guid>https://arxiv.org/abs/2509.00693</guid>
<content:encoded><![CDATA[
arXiv:2509.00693v1 Announce Type: cross 
Abstract: In real-world applications, domain data often contains identifiable or sensitive attributes, is subject to strict regulations (e.g., HIPAA, GDPR), and requires explicit data feature engineering for interpretability and transparency. Existing feature engineering primarily focuses on advancing downstream task performance, often risking privacy leakage. We generalize this learning task under such new requirements as Privacy-Preserving Data Reprogramming (PPDR): given a dataset, transforming features to maximize target attribute prediction accuracy while minimizing sensitive attribute prediction accuracy. PPDR poses challenges for existing systems: 1) generating high-utility feature transformations without being overwhelmed by a large search space, and 2) disentangling and eliminating sensitive information from utility-oriented features to reduce privacy inferability. To tackle these challenges, we propose DELTA, a two-phase variational disentangled generative learning framework. Phase I uses policy-guided reinforcement learning to discover feature transformations with downstream task utility, without any regard to privacy inferability. Phase II employs a variational LSTM seq2seq encoder-decoder with a utility-privacy disentangled latent space design and adversarial-causal disentanglement regularization to suppress privacy signals during feature generation. Experiments on eight datasets show DELTA improves predictive performance by ~9.3% and reduces privacy leakage by ~35%, demonstrating robust, privacy-aware data transformation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Queuing for Civility: Regulating Emotions and Reducing Toxicity in Digital Discourse</title>
<link>https://arxiv.org/abs/2509.00696</link>
<guid>https://arxiv.org/abs/2509.00696</guid>
<content:encoded><![CDATA[
arXiv:2509.00696v1 Announce Type: cross 
Abstract: The pervasiveness of online toxicity, including hate speech and trolling, disrupts digital interactions and online well-being. Previous research has mainly focused on post-hoc moderation, overlooking the real-time emotional dynamics of online conversations and the impact of users' emotions on others. This paper presents a graph-based framework to identify the need for emotion regulation within online conversations. This framework promotes self-reflection to manage emotional responses and encourage responsible behaviour in real time. Additionally, a comment queuing mechanism is proposed to address intentional trolls who exploit emotions to inflame conversations. This mechanism introduces a delay in publishing comments, giving users time to self-regulate before further engaging in the conversation and helping maintain emotional balance. Analysis of social media data from Twitter and Reddit demonstrates that the graph-based framework reduced toxicity by 12%, while the comment queuing mechanism decreased the spread of anger by 15%, with only 4% of comments being temporarily held on average. These findings indicate that combining real-time emotion regulation with delayed moderation can significantly improve well-being in online environments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Dataset Cleaning Framework for Encrypted Traffic Classification</title>
<link>https://arxiv.org/abs/2509.00701</link>
<guid>https://arxiv.org/abs/2509.00701</guid>
<content:encoded><![CDATA[
arXiv:2509.00701v1 Announce Type: cross 
Abstract: Traffic classification, a technique for assigning network flows to predefined categories, has been widely deployed in enterprise and carrier networks. With the massive adoption of mobile devices, encryption is increasingly used in mobile applications to address privacy concerns. Consequently, traditional methods such as Deep Packet Inspection (DPI) fail to distinguish encrypted traffic. To tackle this challenge, Artificial Intelligence (AI), in particular Machine Learning (ML), has emerged as a promising solution for encrypted traffic classification. A crucial prerequisite for any ML-based approach is traffic data cleaning, which removes flows that are not useful for training (e.g., irrelevant protocols, background activity, control-plane messages, and long-lived sessions). Existing cleaning solutions depend on manual inspection of every captured packet, making the process both costly and time-consuming. In this poster, we present an unsupervised framework that automatically cleans encrypted mobile traffic. Evaluation on real-world datasets shows that our framework incurs only a 2%~2.5% reduction in classification accuracy compared with manual cleaning. These results demonstrate that our method offers an efficient and effective preprocessing step for ML-based encrypted traffic classification.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Pool When You Can Flow? Active Learning with GFlowNets</title>
<link>https://arxiv.org/abs/2509.00704</link>
<guid>https://arxiv.org/abs/2509.00704</guid>
<content:encoded><![CDATA[
arXiv:2509.00704v1 Announce Type: cross 
Abstract: The scalability of pool-based active learning is limited by the computational cost of evaluating large unlabeled datasets, a challenge that is particularly acute in virtual screening for drug discovery. While active learning strategies such as Bayesian Active Learning by Disagreement (BALD) prioritize informative samples, it remains computationally intensive when scaled to libraries containing billions samples. In this work, we introduce BALD-GFlowNet, a generative active learning framework that circumvents this issue. Our method leverages Generative Flow Networks (GFlowNets) to directly sample objects in proportion to the BALD reward. By replacing traditional pool-based acquisition with generative sampling, BALD-GFlowNet achieves scalability that is independent of the size of the unlabeled pool. In our virtual screening experiment, we show that BALD-GFlowNet achieves a performance comparable to that of standard BALD baseline while generating more structurally diverse molecules, offering a promising direction for efficient and scalable molecular discovery.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs</title>
<link>https://arxiv.org/abs/2509.00707</link>
<guid>https://arxiv.org/abs/2509.00707</guid>
<content:encoded><![CDATA[
arXiv:2509.00707v1 Announce Type: cross 
Abstract: Masked diffusion models (MDMs) offer a promising non-autoregressive alternative for large language modeling. Standard decoding methods for MDMs, such as confidence-based sampling, select tokens independently based on individual token confidences at each diffusion step. However, we observe that this independent token selection often results in generation orders resembling sequential autoregressive processes, limiting the advantages of non-autoregressive modeling. To mitigate this pheonomenon, we propose Reward-Weighted Sampling (RWS), a novel decoding strategy that leverages an external reward model to provide a principled global signal during the iterative diffusion process. Specifically, at each diffusion step, RWS evaluates the quality of the entire intermediate sequence and scales token logits accordingly, guiding token selection by integrating global sequence-level coherence. This method selectively increases the confidence of tokens that initially have lower scores, thereby promoting a more non-autoregressive generation order. Furthermore, we provide theoretical justification showing that reward-weighted logit scaling induces beneficial rank reversals in token selection and consistently improves expected reward. Experiments demonstrate that RWS significantly promotes non-autoregressive generation orders, leading to improvements across multiple evaluation metrics. These results highlight the effectiveness of integrating global signals in enhancing both the non-autoregressive properties and overall performance of MDMs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's-A-Me, Quantum Mario: Scalable Quantum Reinforcement Learning with Multi-Chip Ensembles</title>
<link>https://arxiv.org/abs/2509.00713</link>
<guid>https://arxiv.org/abs/2509.00713</guid>
<content:encoded><![CDATA[
arXiv:2509.00713v1 Announce Type: cross 
Abstract: Quantum reinforcement learning (QRL) promises compact function approximators with access to vast Hilbert spaces, but its practical progress is slowed by NISQ-era constraints such as limited qubits and noise accumulation. We introduce a multi-chip ensemble framework using multiple small Quantum Convolutional Neural Networks (QCNNs) to overcome these constraints. Our approach partitions complex, high-dimensional observations from the Super Mario Bros environment across independent quantum circuits, then classically aggregates their outputs within a Double Deep Q-Network (DDQN) framework. This modular architecture enables QRL in complex environments previously inaccessible to quantum agents, achieving superior performance and learning stability compared to classical baselines and single-chip quantum models. The multi-chip ensemble demonstrates enhanced scalability by reducing information loss from dimensionality reduction while remaining implementable on near-term quantum hardware, providing a practical pathway for applying QRL to real-world problems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exam Readiness Index (ERI): A Theoretical Framework for a Composite, Explainable Index</title>
<link>https://arxiv.org/abs/2509.00718</link>
<guid>https://arxiv.org/abs/2509.00718</guid>
<content:encoded><![CDATA[
arXiv:2509.00718v1 Announce Type: cross 
Abstract: We present a theoretical framework for an Exam Readiness Index (ERI): a composite, blueprint-aware score R in [0,100] that summarizes a learner's readiness for a high-stakes exam while remaining interpretable and actionable. The ERI aggregates six signals -- Mastery (M), Coverage (C), Retention (R), Pace (P), Volatility (V), and Endurance (E) -- each derived from a stream of practice and mock-test interactions. We formalize axioms for component maps and the composite, prove monotonicity, Lipschitz stability, and bounded drift under blueprint re-weighting, and show existence and uniqueness of the optimal linear composite under convex design constraints. We further characterize confidence bands via blueprint-weighted concentration and prove compatibility with prerequisite-admissible curricula (knowledge spaces / learning spaces). The paper focuses on theory; empirical study is left to future work.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Aware Adaptive Modulation: A Replay-Free and Resource-Efficient Approach For Continual Graph Learning</title>
<link>https://arxiv.org/abs/2509.00735</link>
<guid>https://arxiv.org/abs/2509.00735</guid>
<content:encoded><![CDATA[
arXiv:2509.00735v1 Announce Type: cross 
Abstract: Continual Graph Learning(CGL)focuses on acquiring new knowledge while retaining previously learned information, essential for real-world graph applications. Current methods grapple with two main issues:1) The Stability-Plasticity Dilemma: Replay-based methods often create an imbalance between the Dilemma, while incurring significant storage costs.2) The Resource-Heavy Pre-training: Leading replay-free methods critically depend on extensively pre-trained backbones, this reliance imposes a substantial resource burden.In this paper, we argue that the key to overcoming these challenges lies not in replaying data or fine-tuning the entire network, but in dynamically modulating the internal computational flow of a frozen backbone. We posit that lightweight, task-specific modules can effectively steer a GNN's reasoning process. Motivated by this insight, we propose Task-Aware Adaptive Modulation(TAAM), a replay-free, resource-efficient approach that charts a new path for navigating the stability-plasticity dilemma. TAAM's core is its Neural Synapse Modulators(NSM), which are trained and then frozen for each task to store expert knowledge. A pivotal prototype-guided strategy governs these modulators: 1) For training, it initializes a new NSM by deep-copying from a similar past modulator to boost knowledge transfer. 2) For inference, it selects the most relevant frozen NSM for each task. These NSMs insert into a frozen GNN backbone to perform fine-grained, node-attentive modulation of its internal flow-different from the static perturbations of prior methods. Extensive experiments show that TAAM comprehensively outperforms state-of-the-art methods across six GCIL benchmark datasets. The code will be released upon acceptance of the paper.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Causality: Resolving Simpson's Paradox with $\mathcal{DO}$-Calculus</title>
<link>https://arxiv.org/abs/2509.00744</link>
<guid>https://arxiv.org/abs/2509.00744</guid>
<content:encoded><![CDATA[
arXiv:2509.00744v1 Announce Type: cross 
Abstract: Distinguishing correlation from causation is a fundamental challenge in machine intelligence, often representing a critical barrier to building robust and trustworthy systems. While Pearl's $\mathcal{DO}$-calculus provides a rigorous framework for causal inference, a parallel challenge lies in its physical implementation. Here, we apply and experimentally validate a quantum algorithmic framework for performing causal interventions. Our approach maps causal networks onto quantum circuits where probabilistic links are encoded by controlled-rotation gates, and interventions are realized by a structural remodeling of the circuit -- a physical analogue to Pearl's ``graph surgery''. We demonstrate the method's efficacy by resolving Simpson's Paradox in a 3-qubit model, and show its scalability by quantifying confounding bias in a 10-qubit healthcare simulation. Critically, we provide a proof-of-principle experimental validation on an IonQ Aria quantum computer, successfully reproducing the paradox and its resolution in the presence of real-world noise. This work establishes a practical pathway for quantum causal inference, offering a new computational tool to address deep-rooted challenges in algorithmic fairness and explainable AI (XAI).
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis Using Prune Learning</title>
<link>https://arxiv.org/abs/2509.00745</link>
<guid>https://arxiv.org/abs/2509.00745</guid>
<content:encoded><![CDATA[
arXiv:2509.00745v1 Announce Type: cross 
Abstract: Recent advances in deep learning have significantly improved the accuracy of skin lesion classification models, supporting medical diagnoses and promoting equitable healthcare. However, concerns remain about potential biases related to skin color, which can impact diagnostic outcomes. Ensuring fairness is challenging due to difficulties in classifying skin tones, high computational demands, and the complexity of objectively verifying fairness. To address these challenges, we propose a fairness algorithm for skin lesion classification that overcomes the challenges associated with achieving diagnostic fairness across varying skin tones. By calculating the skewness of the feature map in the convolution layer of the VGG (Visual Geometry Group) network and the patches and the heads of the Vision Transformer, our method reduces unnecessary channels related to skin tone, focusing instead on the lesion area. This approach lowers computational costs and mitigates bias without relying on conventional statistical methods. It potentially reduces model size while maintaining fairness, making it more practical for real-world applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Interpretation of Sparse Autoencoder Features in Vision</title>
<link>https://arxiv.org/abs/2509.00749</link>
<guid>https://arxiv.org/abs/2509.00749</guid>
<content:encoded><![CDATA[
arXiv:2509.00749v1 Announce Type: cross 
Abstract: Understanding what sparse auto-encoder (SAE) features in vision transformers truly represent is usually done by inspecting the patches where a feature's activation is highest. However, self-attention mixes information across the entire image, so an activated patch often co-occurs with-but does not cause-the feature's firing. We propose Causal Feature Explanation (CaFE), which leverages Effective Receptive Field (ERF). We consider each activation of an SAE feature to be a target and apply input-attribution methods to identify the image patches that causally drive that activation. Across CLIP-ViT features, ERF maps frequently diverge from naive activation maps, revealing hidden context dependencies (e.g., a "roaring face" feature that requires the co-occurrence of eyes and nose, rather than merely an open mouth). Patch insertion tests confirm that CaFE more effectively recovers or suppresses feature activations than activation-ranked patches. Our results show that CaFE yields more faithful and semantically precise explanations of vision-SAE features, highlighting the risk of misinterpretation when relying solely on activation location.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low Power Approximate Multiplier Architecture for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2509.00764</link>
<guid>https://arxiv.org/abs/2509.00764</guid>
<content:encoded><![CDATA[
arXiv:2509.00764v1 Announce Type: cross 
Abstract: This paper proposes an low power approximate multiplier architecture for deep neural network (DNN) applications. A 4:2 compressor, introducing only a single combination error, is designed and integrated into an 8x8 unsigned multiplier. This integration significantly reduces the usage of exact compressors while preserving low error rates. The proposed multiplier is employed within a custom convolution layer and evaluated on neural network tasks, including image recognition and denoising. Hardware evaluation demonstrates that the proposed design achieves up to 30.24% energy savings compared to the best among existing multipliers. In image denoising, the custom approximate convolution layer achieves improved Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) compared to other approximate designs. Additionally, when applied to handwritten digit recognition, the model maintains high classification accuracy. These results demonstrate that the proposed architecture offers a favorable balance between energy efficiency and computational precision, making it suitable for low-power AI hardware implementations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LegalChainReasoner: A Legal Chain-guided Framework for Criminal Judicial Opinion Generation</title>
<link>https://arxiv.org/abs/2509.00783</link>
<guid>https://arxiv.org/abs/2509.00783</guid>
<content:encoded><![CDATA[
arXiv:2509.00783v1 Announce Type: cross 
Abstract: A criminal judicial opinion represents the judge's disposition of a case, including the decision rationale and sentencing. Automatically generating such opinions can assist in analyzing sentencing consistency and provide judges with references to similar past cases. However, current research typically approaches this task by dividing it into two isolated subtasks: legal reasoning and sentencing prediction. This separation often leads to inconsistency between the reasoning and predictions, failing to meet real-world judicial requirements. Furthermore, prior studies rely on manually curated knowledge to enhance applicability, yet such methods remain limited in practical deployment. To address these limitations and better align with legal practice, we propose a new LegalAI task: Judicial Opinion Generation, which simultaneously produces both legal reasoning and sentencing decisions. To achieve this, we introduce LegalChainReasoner, a framework that applies structured legal chains to guide the model through comprehensive case assessments. By integrating factual premises, composite legal conditions, and sentencing conclusions, our approach ensures flexible knowledge injection and end-to-end opinion generation. Experiments on two real-world and open-source Chinese legal case datasets demonstrate that our method outperforms baseline models.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive Process Monitoring Methods</title>
<link>https://arxiv.org/abs/2509.00797</link>
<guid>https://arxiv.org/abs/2509.00797</guid>
<content:encoded><![CDATA[
arXiv:2509.00797v1 Announce Type: cross 
Abstract: Prescriptive Process Monitoring (PresPM) is the subfield of Process Mining that focuses on optimizing processes through real-time interventions based on event log data. Evaluating PresPM methods is challenging due to the lack of ground-truth outcomes for all intervention actions in datasets. A generative deep learning approach from the field of Causal Inference (CI), RealCause, has been commonly used to estimate the outcomes for proposed intervention actions to evaluate a new policy. However, RealCause overlooks the temporal dependencies in process data, and relies on a single CI model architecture, TARNet, limiting its effectiveness. To address both shortcomings, we introduce ProCause, a generative approach that supports both sequential (e.g., LSTMs) and non-sequential models while integrating multiple CI architectures (S-Learner, T-Learner, TARNet, and an ensemble). Our research using a simulator with known ground truths reveals that TARNet is not always the best choice; instead, an ensemble of models offers more consistent reliability, and leveraging LSTMs shows potential for improved evaluations when temporal dependencies are present. We further validate ProCause's practical effectiveness through a real-world data analysis, ensuring a more reliable evaluation of PresPM methods.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Iterative RAG for Knowledge Visual Question Answering</title>
<link>https://arxiv.org/abs/2509.00798</link>
<guid>https://arxiv.org/abs/2509.00798</guid>
<content:encoded><![CDATA[
arXiv:2509.00798v1 Announce Type: cross 
Abstract: While Multimodal Large Language Models (MLLMs) have significantly advanced multimodal understanding, their performance remains limited on knowledge-intensive visual questions that require external knowledge beyond the image. Retrieval-Augmented Generation (RAG) has become a promising solution for providing models with external knowledge, its conventional single-pass framework often fails to gather sufficient knowledge. To overcome this limitation, we propose MI-RAG, a Multimodal Iterative RAG framework that leverages reasoning to enhance retrieval and update reasoning over newly retrieved knowledge across modalities. At each iteration, MI-RAG leverages an accumulated reasoning record to dynamically formulate a multi-query. These queries then drive a joint search across heterogeneous knowledge bases containing both visually-grounded and textual knowledge. The newly acquired knowledge is synthesized into the reasoning record, progressively refining understanding across iterations. Experiments on challenging benchmarks, including Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG significantly improves both retrieval recall and answer accuracy, establishing a scalable approach for compositional reasoning in knowledge-intensive VQA.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA</title>
<link>https://arxiv.org/abs/2509.00806</link>
<guid>https://arxiv.org/abs/2509.00806</guid>
<content:encoded><![CDATA[
arXiv:2509.00806v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly evident for accurate question answering across various domains. However, rigorous evaluation of their performance on complex question-answering (QA) capabilities is essential before deployment in real-world biomedical and healthcare applications. This paper presents our approach to the MedHopQA track of the BioCreative IX shared task, which focuses on multi-hop biomedical question answering involving diseases, genes, and chemicals. We adopt a supervised fine-tuning strategy leveraging LLaMA 3 8B, enhanced with a curated biomedical question-answer dataset compiled from external sources including BioASQ, MedQuAD, and TREC. Three experimental setups are explored: fine-tuning on combined short and long answers, short answers only, and long answers only. While our models demonstrate strong domain understanding, achieving concept-level accuracy scores of up to 0.8, their Exact Match (EM) scores remain significantly lower, particularly in the test phase. We introduce a two-stage inference pipeline for precise short-answer extraction to mitigate verbosity and improve alignment with evaluation metrics. Despite partial improvements, challenges persist in generating strictly formatted outputs. Our findings highlight the gap between semantic understanding and exact answer evaluation in biomedical LLM applications, motivating further research in output control and post-processing strategies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play Approach for Enhanced Fetal Plane Classification</title>
<link>https://arxiv.org/abs/2509.00808</link>
<guid>https://arxiv.org/abs/2509.00808</guid>
<content:encoded><![CDATA[
arXiv:2509.00808v1 Announce Type: cross 
Abstract: Fetal ultrasound standard plane classification is essential for reliable prenatal diagnosis but faces inherent challenges, including low tissue contrast, boundary ambiguity, and operator-dependent image quality variations. To overcome these limitations, we propose a plug-and-play adaptive contrast adjustment module (ACAM), whose core design is inspired by the clinical practice of doctors adjusting image contrast to obtain clearer and more discriminative structural information. The module employs a shallow texture-sensitive network to predict clinically plausible contrast parameters, transforms input images into multiple contrast-enhanced views through differentiable mapping, and fuses them within downstream classifiers. Validated on a multi-center dataset of 12,400 images across six anatomical categories, the module consistently improves performance across diverse models, with accuracy of lightweight models increasing by 2.02 percent, accuracy of traditional models increasing by 1.29 percent, and accuracy of state-of-the-art models increasing by 1.15 percent. The innovation of the module lies in its content-aware adaptation capability, replacing random preprocessing with physics-informed transformations that align with sonographer workflows while improving robustness to imaging heterogeneity through multi-view fusion. This approach effectively bridges low-level image features with high-level semantics, establishing a new paradigm for medical image analysis under real-world image quality variations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AImoclips: A Benchmark for Evaluating Emotion Conveyance in Text-to-Music Generation</title>
<link>https://arxiv.org/abs/2509.00813</link>
<guid>https://arxiv.org/abs/2509.00813</guid>
<content:encoded><![CDATA[
arXiv:2509.00813v1 Announce Type: cross 
Abstract: Recent advances in text-to-music (TTM) generation have enabled controllable and expressive music creation using natural language prompts. However, the emotional fidelity of TTM systems remains largely underexplored compared to human preference or text alignment. In this study, we introduce AImoclips, a benchmark for evaluating how well TTM systems convey intended emotions to human listeners, covering both open-source and commercial models. We selected 12 emotion intents spanning four quadrants of the valence-arousal space, and used six state-of-the-art TTM systems to generate over 1,000 music clips. A total of 111 participants rated the perceived valence and arousal of each clip on a 9-point Likert scale. Our results show that commercial systems tend to produce music perceived as more pleasant than intended, while open-source systems tend to perform the opposite. Emotions are more accurately conveyed under high-arousal conditions across all models. Additionally, all systems exhibit a bias toward emotional neutrality, highlighting a key limitation in affective controllability. This benchmark offers valuable insights into model-specific emotion rendering characteristics and supports future development of emotionally aligned TTM systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization</title>
<link>https://arxiv.org/abs/2509.00826</link>
<guid>https://arxiv.org/abs/2509.00826</guid>
<content:encoded><![CDATA[
arXiv:2509.00826v1 Announce Type: cross 
Abstract: Efficient adversarial attack methods are critical for assessing the robustness of computer vision models. In this paper, we reconstruct the optimization objective for generating adversarial examples as "maximizing the difference between the non-true labels' probability upper bound and the true label's probability," and propose a gradient-based attack method termed Sequential Difference Maximization (SDM). SDM establishes a three-layer optimization framework of "cycle-stage-step." The processes between cycles and between iterative steps are respectively identical, while optimization stages differ in terms of loss functions: in the initial stage, the negative probability of the true label is used as the loss function to compress the solution space; in subsequent stages, we introduce the Directional Probability Difference Ratio (DPDR) loss function to gradually increase the non-true labels' probability upper bound by compressing the irrelevant labels' probabilities. Experiments demonstrate that compared with previous SOTA methods, SDM not only exhibits stronger attack performance but also achieves higher attack cost-effectiveness. Additionally, SDM can be combined with adversarial training methods to enhance their defensive effects. The code is available at https://github.com/X-L-Liu/SDM.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Vehicle Speed Classification via BMCNN with Reinforcement Learning-Enhanced Acoustic Processing</title>
<link>https://arxiv.org/abs/2509.00839</link>
<guid>https://arxiv.org/abs/2509.00839</guid>
<content:encoded><![CDATA[
arXiv:2509.00839v1 Announce Type: cross 
Abstract: Traffic congestion remains a pressing urban challenge, requiring intelligent transportation systems for real-time management. We present a hybrid framework that combines deep learning and reinforcement learning for acoustic vehicle speed classification. A dual-branch BMCNN processes MFCC and wavelet features to capture complementary frequency patterns. An attention-enhanced DQN adaptively selects the minimal number of audio frames and triggers early decisions once confidence thresholds are reached. Evaluations on IDMT-Traffic and our SZUR-Acoustic (Suzhou) datasets show 95.99% and 92.3% accuracy, with up to 1.63x faster average processing via early termination. Compared with A3C, DDDQN, SA2C, PPO, and TD3, the method provides a superior accuracy-efficiency trade-off and is suitable for real-time ITS deployment in heterogeneous urban environments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion</title>
<link>https://arxiv.org/abs/2509.00843</link>
<guid>https://arxiv.org/abs/2509.00843</guid>
<content:encoded><![CDATA[
arXiv:2509.00843v1 Announce Type: cross 
Abstract: Novel view synthesis (NVS) from a single image is highly ill-posed due to large unobserved regions, especially for views that deviate significantly from the input. While existing methods focus on consistency between the source and generated views, they often fail to maintain coherence and correct view alignment across long-range or looped trajectories. We propose a model that addresses this by decomposing single-view NVS into a 360-degree scene extrapolation followed by novel view interpolation. This design ensures long-term view and scene consistency by conditioning on keyframes extracted and warped from a generated panoramic representation. In the first stage, a panorama diffusion model learns the scene prior from the input perspective image. Perspective keyframes are then sampled and warped from the panorama and used as anchor frames in a pre-trained video diffusion model, which generates novel views through a proposed spatial noise diffusion process. Compared to prior work, our method produces globally consistent novel views -- even in loop closure scenarios -- while enabling flexible camera control. Experiments on diverse scene datasets demonstrate that our approach outperforms existing methods in generating coherent views along user-defined trajectories. Our implementation is available at https://github.com/YiGuYT/LookBeyond.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal SHAP: Feature Attribution with Dependency Awareness through Causal Discovery</title>
<link>https://arxiv.org/abs/2509.00846</link>
<guid>https://arxiv.org/abs/2509.00846</guid>
<content:encoded><![CDATA[
arXiv:2509.00846v1 Announce Type: cross 
Abstract: Explaining machine learning (ML) predictions has become crucial as ML models are increasingly deployed in high-stakes domains such as healthcare. While SHapley Additive exPlanations (SHAP) is widely used for model interpretability, it fails to differentiate between causality and correlation, often misattributing feature importance when features are highly correlated. We propose Causal SHAP, a novel framework that integrates causal relationships into feature attribution while preserving many desirable properties of SHAP. By combining the Peter-Clark (PC) algorithm for causal discovery and the Intervention Calculus when the DAG is Absent (IDA) algorithm for causal strength quantification, our approach addresses the weakness of SHAP. Specifically, Causal SHAP reduces attribution scores for features that are merely correlated with the target, as validated through experiments on both synthetic and real-world datasets. This study contributes to the field of Explainable AI (XAI) by providing a practical framework for causal-aware model explanations. Our approach is particularly valuable in domains such as healthcare, where understanding true causal relationships is critical for informed decision-making.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why it is worth making an effort with GenAI</title>
<link>https://arxiv.org/abs/2509.00852</link>
<guid>https://arxiv.org/abs/2509.00852</guid>
<content:encoded><![CDATA[
arXiv:2509.00852v1 Announce Type: cross 
Abstract: Students routinely use ChatGPT and the like now to help them with their homework, such as writing an essay. It takes less effort to complete and is easier to do than by hand. It can even produce as good if not better output than the student's own work. However, there is a growing concern that over-reliance on using GenAI in this way will stifle the development of learning writing and critical thinking skills. How might this trend be reversed? What if students were required to make more effort when using GenAI to do their homework? It might be more challenging, but the additional effort involved could result in them learning more and having a greater sense of achievement. This tension can be viewed as a form of effort paradox; where effort is both viewed as something to be avoided but at the same time is valued. Is it possible to let students learn sometimes with less and other times more effort? Students are already adept at the former but what about the latter? Could we design new kinds of AI tools that deliberately require more effort to use to deepen the learning experience? In this paper, I begin to outline what form these might take, for example, asking students to use a combination of GenAI tools with traditional learning approaches (e.g. note-taking while reading). I also discuss how else to design tools to think with that augments human cognition; where students learn more the skills of metacognition and reflection.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech Command Recognition Using LogNNet Reservoir Computing for Embedded Systems</title>
<link>https://arxiv.org/abs/2509.00862</link>
<guid>https://arxiv.org/abs/2509.00862</guid>
<content:encoded><![CDATA[
arXiv:2509.00862v1 Announce Type: cross 
Abstract: This paper presents a low-resource speech-command recognizer combining energy-based voice activity detection (VAD), an optimized Mel-Frequency Cepstral Coefficients (MFCC) pipeline, and the LogNNet reservoir-computing classifier. Using four commands from the Speech Commands da-taset downsampled to 8 kHz, we evaluate four MFCC aggregation schemes and find that adaptive binning (64-dimensional feature vector) offers the best accuracy-to-compactness trade-off. The LogNNet classifier with architecture 64:33:9:4 reaches 92.04% accuracy under speaker-independent evaluation, while requiring significantly fewer parameters than conventional deep learn-ing models. Hardware implementation on Arduino Nano 33 IoT (ARM Cor-tex-M0+, 48 MHz, 32 KB RAM) validates the practical feasibility, achieving ~90% real-time recognition accuracy while consuming only 18 KB RAM (55% utilization). The complete pipeline (VAD -> MFCC -> LogNNet) thus enables reliable on-device speech-command recognition under strict memory and compute limits, making it suitable for battery-powered IoT nodes, wire-less sensor networks, and hands-free control interfaces.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can General-Purpose Omnimodels Compete with Specialists? A Case Study in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.00866</link>
<guid>https://arxiv.org/abs/2509.00866</guid>
<content:encoded><![CDATA[
arXiv:2509.00866v1 Announce Type: cross 
Abstract: The emergence of powerful, general-purpose omnimodels capable of processing diverse data modalities has raised a critical question: can these ``jack-of-all-trades'' systems perform on par with highly specialized models in knowledge-intensive domains? This work investigates this question within the high-stakes field of medical image segmentation. We conduct a comparative study analyzing the zero-shot performance of a state-of-the-art omnimodel (Gemini 2.5 Pro, the ``Nano Banana'' model) against domain-specific deep learning models on three distinct tasks: polyp (endoscopy), retinal vessel (fundus), and breast tumor segmentation (ultrasound). Our study focuses on performance at the extremes by curating subsets of the ``easiest'' and ``hardest'' cases based on the specialist models' accuracy. Our findings reveal a nuanced and task-dependent landscape. For polyp and breast tumor segmentation, specialist models excel on easy samples, but the omnimodel demonstrates greater robustness on hard samples where specialists fail catastrophically. Conversely, for the fine-grained task of retinal vessel segmentation, the specialist model maintains superior performance across both easy and hard cases. Intriguingly, qualitative analysis suggests omnimodels may possess higher sensitivity, identifying subtle anatomical features missed by human annotators. Our results indicate that while current omnimodels are not yet a universal replacement for specialists, their unique strengths suggest a potential complementary role with specialist models, particularly in enhancing robustness on challenging edge cases.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose as Clinical Prior: Learning Dual Representations for Scoliosis Screening</title>
<link>https://arxiv.org/abs/2509.00872</link>
<guid>https://arxiv.org/abs/2509.00872</guid>
<content:encoded><![CDATA[
arXiv:2509.00872v1 Announce Type: cross 
Abstract: Recent AI-based scoliosis screening methods primarily rely on large-scale silhouette datasets, often neglecting clinically relevant postural asymmetries-key indicators in traditional screening. In contrast, pose data provide an intuitive skeletal representation, enhancing clinical interpretability across various medical applications. However, pose-based scoliosis screening remains underexplored due to two main challenges: (1) the scarcity of large-scale, annotated pose datasets; and (2) the discrete and noise-sensitive nature of raw pose coordinates, which hinders the modeling of subtle asymmetries. To address these limitations, we introduce Scoliosis1K-Pose, a 2D human pose annotation set that extends the original Scoliosis1K dataset, comprising 447,900 frames of 2D keypoints from 1,050 adolescents. Building on this dataset, we introduce the Dual Representation Framework (DRF), which integrates a continuous skeleton map to preserve spatial structure with a discrete Postural Asymmetry Vector (PAV) that encodes clinically relevant asymmetry descriptors. A novel PAV-Guided Attention (PGA) module further uses the PAV as clinical prior to direct feature extraction from the skeleton map, focusing on clinically meaningful asymmetries. Extensive experiments demonstrate that DRF achieves state-of-the-art performance. Visualizations further confirm that the model leverages clinical asymmetry cues to guide feature extraction and promote synergy between its dual representations. The dataset and code are publicly available at https://zhouzi180.github.io/Scoliosis1K/.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Latency-Critical Applications with AI-Powered Semi-Automatic Fine-Grained Parallelization on SMT Processors</title>
<link>https://arxiv.org/abs/2509.00883</link>
<guid>https://arxiv.org/abs/2509.00883</guid>
<content:encoded><![CDATA[
arXiv:2509.00883v1 Announce Type: cross 
Abstract: Latency-critical applications tend to show low utilization of functional units due to frequent cache misses and mispredictions during speculative execution in high-performance superscalar processors. However, due to significant impact on single-thread performance, Simultaneous Multithreading (SMT) technology is rarely used with heavy threads of latency-critical applications. In this paper, we explore utilization of SMT technology to support fine-grained parallelization of latency-critical applications. Following the advancements in the development of Large Language Models (LLMs), we introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we extend AI Coding Agent in Cursor IDE with additional tools connected through Model Context Protocol, enabling end-to-end AI Agent for parallelization. Additional connected tools enable LLM-guided hotspot detection, collection of dynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance simulation to estimate performance gains. We apply Aira with Relic parallel framework for fine-grained task parallelism on SMT cores to parallelize latency-critical benchmarks representing real-world applications used in industry. We show 17% geomean performance gain from parallelization of latency-critical benchmarks using Aira with Relic framework.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Gaussian Process Auto-encoder for Tabular Data</title>
<link>https://arxiv.org/abs/2509.00884</link>
<guid>https://arxiv.org/abs/2509.00884</guid>
<content:encoded><![CDATA[
arXiv:2509.00884v1 Announce Type: cross 
Abstract: Explainable machine learning has attracted much interest in the community where the stakes are high. Counterfactual explanations methods have become an important tool in explaining a black-box model. The recent advances have leveraged the power of generative models such as an autoencoder. In this paper, we propose a novel method using a Gaussian process to construct the auto-encoder architecture for generating counterfactual samples. The resulting model requires fewer learnable parameters and thus is less prone to overfitting. We also introduce a novel density estimator that allows for searching for in-distribution samples. Furthermore, we introduce an algorithm for selecting the optimal regularization rate on density estimator while searching for counterfactuals. We experiment with our method in several large-scale tabular datasets and compare with other auto-encoder-based methods. The results show that our method is capable of generating diversified and in-distribution counterfactual samples.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spotlighter: Revisiting Prompt Tuning from a Representative Mining View</title>
<link>https://arxiv.org/abs/2509.00905</link>
<guid>https://arxiv.org/abs/2509.00905</guid>
<content:encoded><![CDATA[
arXiv:2509.00905v1 Announce Type: cross 
Abstract: CLIP's success has demonstrated that prompt tuning can achieve robust cross-modal semantic alignment for tasks ranging from open-domain recognition to fine-grained classification. However, redundant or weakly relevant feature components introduce noise and incur unnecessary computational costs. In this work, we propose Spotlighter, a lightweight token-selection framework that simultaneously enhances accuracy and efficiency in prompt tuning. Spotlighter evaluates each visual token's activation from both sample-wise and semantic-wise perspectives and retains only the top-scoring tokens for downstream prediction. A class-specific semantic memory bank of learned prototypes refines this selection, ensuring semantic representativeness and compensating for discarded features. To further prioritize informative signals, we introduce a two-level ranking mechanism that dynamically weights token--prototype interactions. Across 11 few-shot benchmarks, Spotlighter outperforms CLIP by up to 11.19\% in harmonic mean accuracy and achieves up to 0.8K additional FPS, with only 21 extra parameters. These results establish Spotlighter as an effective and scalable baseline for prompt tuning. Code for our method will be available at https://github.com/greatest-gourmet/Spotlighter.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyMusician: On-Device Music Generation with Knowledge Distillation and Mixed Precision Quantization</title>
<link>https://arxiv.org/abs/2509.00914</link>
<guid>https://arxiv.org/abs/2509.00914</guid>
<content:encoded><![CDATA[
arXiv:2509.00914v1 Announce Type: cross 
Abstract: The success of the generative model has gained unprecedented attention in the music generation area. Transformer-based architectures have set new benchmarks for model performance. However, their practical adoption is hindered by some critical challenges: the demand for massive computational resources and inference time, due to their large number of parameters. These obstacles make them infeasible to deploy on edge devices, such as smartphones and wearables, with limited computational resources. In this work, we present TinyMusician, a lightweight music generation model distilled from MusicGen (a State-of-the-art music generation model). TinyMusician integrates two innovations: (i) Stage-mixed Bidirectional and Skewed KL-Divergence and (ii) Adaptive Mixed-Precision Quantization. The experimental results demonstrate that TinyMusician retains 93% of the MusicGen-Small performance with 55% less model size. TinyMusician is the first mobile-deployable music generation model that eliminates cloud dependency while maintaining high audio fidelity and efficient resource usage
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superposition in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.00928</link>
<guid>https://arxiv.org/abs/2509.00928</guid>
<content:encoded><![CDATA[
arXiv:2509.00928v1 Announce Type: cross 
Abstract: Interpreting graph neural networks (GNNs) is difficult because message passing mixes signals and internal channels rarely align with human concepts. We study superposition, the sharing of directions by multiple features, directly in the latent space of GNNs. Using controlled experiments with unambiguous graph concepts, we extract features as (i) class-conditional centroids at the graph level and (ii) linear-probe directions at the node level, and then analyze their geometry with simple basis-invariant diagnostics. Across GCN/GIN/GAT we find: increasing width produces a phase pattern in overlap; topology imprints overlap onto node-level features that pooling partially remixes into task-aligned graph axes; sharper pooling increases axis alignment and reduces channel sharing; and shallow models can settle into metastable low-rank embeddings. These results connect representational geometry with concrete design choices (width, pooling, and final-layer activations) and suggest practical approaches for more interpretable GNNs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework</title>
<link>https://arxiv.org/abs/2509.00934</link>
<guid>https://arxiv.org/abs/2509.00934</guid>
<content:encoded><![CDATA[
arXiv:2509.00934v1 Announce Type: cross 
Abstract: We present MedCOD (Medical Chain-of-Dictionary), a hybrid framework designed to improve English-to-Spanish medical translation by integrating domain-specific structured knowledge into large language models (LLMs). MedCOD integrates domain-specific knowledge from both the Unified Medical Language System (UMLS) and the LLM-as-Knowledge-Base (LLM-KB) paradigm to enhance structured prompting and fine-tuning. We constructed a parallel corpus of 2,999 English-Spanish MedlinePlus articles and a 100-sentence test set annotated with structured medical contexts. Four open-source LLMs (Phi-4, Qwen2.5-14B, Qwen2.5-7B, and LLaMA-3.1-8B) were evaluated using structured prompts that incorporated multilingual variants, medical synonyms, and UMLS-derived definitions, combined with LoRA-based fine-tuning. Experimental results demonstrate that MedCOD significantly improves translation quality across all models. For example, Phi-4 with MedCOD and fine-tuning achieved BLEU 44.23, chrF++ 28.91, and COMET 0.863, surpassing strong baseline models like GPT-4o and GPT-4o-mini. Ablation studies confirm that both MedCOD prompting and model adaptation independently contribute to performance gains, with their combination yielding the highest improvements. These findings highlight the potential of structured knowledge integration to enhance LLMs for medical translation tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCOUT: Toward Sub-Quadratic Attention via Segment Compression for Optimized Utility in Transformers</title>
<link>https://arxiv.org/abs/2509.00935</link>
<guid>https://arxiv.org/abs/2509.00935</guid>
<content:encoded><![CDATA[
arXiv:2509.00935v1 Announce Type: cross 
Abstract: Transformers have demonstrated strong performance across a wide range of sequence modeling tasks, but their quadratic attention complexity limits scalability to long sequences. Linear models such as Mamba and sliding-window attention (SWA) address this by mixing tokens through recurrent or localized operations with fixed-size memory, achieving efficient inference. However, these methods risk degrading performance on long sequences due to their inability to retain detailed information from distant tokens. We propose SCOUT (Segment Compression for Optimized Utility in Transformers), a hybrid architecture that compresses tokens locally within fixed-size segments and applies attention only over these compressed representations. Each token embedding is first enriched via a linear local mixer, Mamba or SWA, that integrates recent context. Then, instead of attending to all previous tokens, each token sparsely attends to a small number of compressed checkpoint tokens that summarize the input history. This design retains much of the expressivity of full attention while substantially reducing the computational and memory cost. By attending to compressed history rather than all previous tokens, SCOUT incurs slightly higher memory than purely linear models, but its growth rate remains sub-quadratic and far more scalable than that of full Transformers. We analyze SCOUT's computational and memory efficiency and evaluate it empirically on long-context language modeling and reasoning tasks. SCOUT with both Mamba and SWA mixers outperforms strong long-sequence baselines under the same computational budget, matches full-attention Transformers on language modeling and common-sense reasoning tasks at 400M and 1.3B scales. Moreover, our SCOUT achieves higher end-to-end throughput than SOTA models, while delivering comparable results on long sequence benchmarks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure and Destructure: Dual Forces in the Making of Knowledge Engines</title>
<link>https://arxiv.org/abs/2509.00949</link>
<guid>https://arxiv.org/abs/2509.00949</guid>
<content:encoded><![CDATA[
arXiv:2509.00949v1 Announce Type: cross 
Abstract: The making of knowledge engines in natural language processing has been shaped by two seemingly distinct paradigms: one grounded in structure, the other driven by massively available unstructured data. The structured paradigm leverages predefined symbolic interactions, such as knowledge graphs, as priors and designs models to capture them. In contrast, the unstructured paradigm centers on scaling transformer architectures with increasingly vast data and model sizes, as seen in modern large language models. Despite their divergence, this thesis seeks to establish conceptual connections bridging these paradigms. Two complementary forces, structure and destructure, emerge across both paradigms: structure organizes seen symbolic interactions, while destructure, through periodic embedding resets, improves model plasticity and generalization to unseen scenarios. These connections form a new recipe for developing general knowledge engines that can support transparent, controllable, and adaptable intelligent systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ART: Adaptive Resampling-based Training for Imbalanced Classification</title>
<link>https://arxiv.org/abs/2509.00955</link>
<guid>https://arxiv.org/abs/2509.00955</guid>
<content:encoded><![CDATA[
arXiv:2509.00955v1 Announce Type: cross 
Abstract: Traditional resampling methods for handling class imbalance typically uses fixed distributions, undersampling the majority or oversampling the minority. These static strategies ignore changes in class-wise learning difficulty, which can limit the overall performance of the model.
  This paper proposes an Adaptive Resampling-based Training (ART) method that periodically updates the distribution of the training data based on the class-wise performance of the model. Specifically, ART uses class-wise macro F1 scores, computed at fixed intervals, to determine the degree of resampling to be performed.
  Unlike instance-level difficulty modeling, which is noisy and outlier-sensitive, ART adapts at the class level. This allows the model to incrementally shift its attention towards underperforming classes in a way that better aligns with the optimization objective.
  Results on diverse benchmarks, including Pima Indians Diabetes and Yeast dataset demonstrate that ART consistently outperforms both resampling-based and algorithm-level methods, including Synthetic Minority Oversampling Technique (SMOTE), NearMiss Undersampling, and Cost-sensitive Learning on binary as well as multi-class classification tasks with varying degrees of imbalance.
  In most settings, these improvements are statistically significant. On tabular datasets, gains are significant under paired t-tests and Wilcoxon tests (p < 0.05), while results on text and image tasks remain favorable. Compared to training on the original imbalanced data, ART improves macro F1 by an average of 2.64 percentage points across all tested tabular datasets. Unlike existing methods, whose performance varies by task, ART consistently delivers the strongest macro F1, making it a reliable choice for imbalanced classification.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets Left Behind? Auditing Disability Inclusivity in Large Language Models</title>
<link>https://arxiv.org/abs/2509.00963</link>
<guid>https://arxiv.org/abs/2509.00963</guid>
<content:encoded><![CDATA[
arXiv:2509.00963v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used for accessibility guidance, yet many disability groups remain underserved by their advice. To address this gap, we present taxonomy aligned benchmark1 of human validated, general purpose accessibility questions, designed to systematically audit inclusivity across disabilities. Our benchmark evaluates models along three dimensions: Question-Level Coverage (breadth within answers), Disability-Level Coverage (balance across nine disability categories), and Depth (specificity of support). Applying this framework to 17 proprietary and open-weight models reveals persistent inclusivity gaps: Vision, Hearing, and Mobility are frequently addressed, while Speech, Genetic/Developmental, Sensory-Cognitive, and Mental Health remain under served. Depth is similarly concentrated in a few categories but sparse elsewhere. These findings reveal who gets left behind in current LLM accessibility guidance and highlight actionable levers: taxonomy-aware prompting/training and evaluations that jointly audit breadth, balance, and depth.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clone What You Can't Steal: Black-Box LLM Replication via Logit Leakage and Distillation</title>
<link>https://arxiv.org/abs/2509.00973</link>
<guid>https://arxiv.org/abs/2509.00973</guid>
<content:encoded><![CDATA[
arXiv:2509.00973v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in mission-critical systems, facilitating tasks such as satellite operations, command-and-control, military decision support, and cyber defense. Many of these systems are accessed through application programming interfaces (APIs). When such APIs lack robust access controls, they can expose full or top-k logits, creating a significant and often overlooked attack surface. Prior art has mainly focused on reconstructing the output projection layer or distilling surface-level behaviors. However, regenerating a black-box model under tight query constraints remains underexplored. We address that gap by introducing a constrained replication pipeline that transforms partial logit leakage into a functional deployable substitute model clone. Our two-stage approach (i) reconstructs the output projection matrix by collecting top-k logits from under 10k black-box queries via singular value decomposition (SVD) over the logits, then (ii) distills the remaining architecture into compact student models with varying transformer depths, trained on an open source dataset. A 6-layer student recreates 97.6% of the 6-layer teacher model's hidden-state geometry, with only a 7.31% perplexity increase, and a 7.58 Negative Log-Likelihood (NLL). A 4-layer variant achieves 17.1% faster inference and 18.1% parameter reduction with comparable performance. The entire attack completes in under 24 graphics processing unit (GPU) hours and avoids triggering API rate-limit defenses. These results demonstrate how quickly a cost-limited adversary can clone an LLM, underscoring the urgent need for hardened inference APIs and secure on-premise defense deployments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Decentralized Federated Multi-task Learning With Trustworthiness in Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2509.00992</link>
<guid>https://arxiv.org/abs/2509.00992</guid>
<content:encoded><![CDATA[
arXiv:2509.00992v1 Announce Type: cross 
Abstract: Multi-task learning is an effective way to address the challenge of model personalization caused by high data heterogeneity in federated learning. However, extending multi-task learning to the online decentralized federated learning setting is yet to be explored. The online decentralized federated learning setting considers many real-world applications of federated learning, such as autonomous systems, where clients communicate peer-to-peer and the data distribution of each client is time-varying. A more serious problem in real-world applications of federated learning is the presence of Byzantine clients. Byzantine-resilient approaches used in federated learning work only when the number of Byzantine clients is less than one-half the total number of clients. Yet, it is difficult to put a limit on the number of Byzantine clients within a system in reality. However, recent work in robotics shows that it is possible to exploit cyber-physical properties of a system to predict clients' behavior and assign a trust probability to received signals. This can help to achieve resiliency in the presence of a dominating number of Byzantine clients. Therefore, in this paper, we develop an online decentralized federated multi-task learning algorithm to provide model personalization and resiliency when the number of Byzantine clients dominates the number of honest clients. Our proposed algorithm leverages cyber-physical properties, such as the received signal strength in wireless systems or side information, to assign a trust probability to local models received from neighbors in each iteration. Our simulation results show that the proposed algorithm performs close to a Byzantine-free setting.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper</title>
<link>https://arxiv.org/abs/2509.00996</link>
<guid>https://arxiv.org/abs/2509.00996</guid>
<content:encoded><![CDATA[
arXiv:2509.00996v1 Announce Type: cross 
Abstract: Considering deep neural networks as manifold mappers, the pretrain-then-fine-tune paradigm can be interpreted as a two-stage process: pretrain establishes a broad knowledge base, and fine-tune adjusts the model parameters to activate specific neural pathways to align with the target manifold. Although prior fine-tuning approaches demonstrate success, their rigid parameter space limits their ability to dynamically activate appropriate neural pathways, rendering them ill-equipped to adapt flexibly to the diverse and evolving data distributions. In light of this view, we propose a novel approach, Mixture of Expert Prompt Tuning (MEPT), as an effective and efficient manifold-mapping framework. MEPT leverages the Mixture of Experts architecture by integrating multiple prompt experts to adaptively learn diverse and non-stationary data distributions. Empirical evaluations demonstrate that MEPT outperforms several state-of-the-art parameter efficient baselines on SuperGLUE, achieving notable improvements in mean accuracy (e.g., 1.94%) while significantly reducing activated prompts by 79.25%. The effectiveness of MEPT is further supported by theoretical insights from manifold learning and validated through neural activation pathway visualization results. Our code is avaliable at https://github.com/runtsang/MEPT.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition</title>
<link>https://arxiv.org/abs/2509.01031</link>
<guid>https://arxiv.org/abs/2509.01031</guid>
<content:encoded><![CDATA[
arXiv:2509.01031v1 Announce Type: cross 
Abstract: Human Activity Recognition (HAR) using wearable sensors is crucial for healthcare, fitness tracking, and smart environments, yet cross-user variability -- stemming from diverse motion patterns, sensor placements, and physiological traits -- hampers generalization in real-world settings. Conventional supervised learning methods often overfit to user-specific patterns, leading to poor performance on unseen users. Existing domain generalization approaches, while promising, frequently overlook temporal dependencies or depend on impractical domain-specific labels. We propose Temporal-Preserving Reinforcement Learning Domain Generalization (TPRL-DG), a novel framework that redefines feature extraction as a sequential decision-making process driven by reinforcement learning. TPRL-DG leverages a Transformer-based autoregressive generator to produce temporal tokens that capture user-invariant activity dynamics, optimized via a multi-objective reward function balancing class discrimination and cross-user invariance. Key innovations include: (1) an RL-driven approach for domain generalization, (2) autoregressive tokenization to preserve temporal coherence, and (3) a label-free reward design eliminating the need for target user annotations. Evaluations on the DSADS and PAMAP2 datasets show that TPRL-DG surpasses state-of-the-art methods in cross-user generalization, achieving superior accuracy without per-user calibration. By learning robust, user-invariant temporal patterns, TPRL-DG enables scalable HAR systems, facilitating advancements in personalized healthcare, adaptive fitness tracking, and context-aware environments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing through Unclear Glass: Occlusion Removal with One Shot</title>
<link>https://arxiv.org/abs/2509.01033</link>
<guid>https://arxiv.org/abs/2509.01033</guid>
<content:encoded><![CDATA[
arXiv:2509.01033v1 Announce Type: cross 
Abstract: Images taken through window glass are often degraded by contaminants adhered to the glass surfaces. Such contaminants cause occlusions that attenuate the incoming light and scatter stray light towards the camera. Most of existing deep learning methods for neutralizing the effects of contaminated glasses relied on synthetic training data. Few researchers used real degraded and clean image pairs, but they only considered removing or alleviating the effects of rain drops on glasses. This paper is concerned with the more challenging task of learning the restoration of images taken through glasses contaminated by a wide range of occluders, including muddy water, dirt and other small foreign particles found in reality. To facilitate the learning task we have gone to a great length to acquire real paired images with and without glass contaminants. More importantly, we propose an all-in-one model to neutralize contaminants of different types by utilizing the one-shot test-time adaptation mechanism. It involves a self-supervised auxiliary learning task to update the trained model for the unique occlusion type of each test image. Experimental results show that the proposed method outperforms the state-of-the-art methods quantitatively and qualitatively in cleaning realistic contaminated images, especially the unseen ones.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-Learning--Driven Adaptive Rewiring for Cooperative Control in Heterogeneous Networks</title>
<link>https://arxiv.org/abs/2509.01057</link>
<guid>https://arxiv.org/abs/2509.01057</guid>
<content:encoded><![CDATA[
arXiv:2509.01057v1 Announce Type: cross 
Abstract: Cooperation emergence in multi-agent systems represents a fundamental statistical physics problem where microscopic learning rules drive macroscopic collective behavior transitions. We propose a Q-learning-based variant of adaptive rewiring that builds on mechanisms studied in the literature. This method combines temporal difference learning with network restructuring so that agents can optimize strategies and social connections based on interaction histories. Through neighbor-specific Q-learning, agents develop sophisticated partnership management strategies that enable cooperator cluster formation, creating spatial separation between cooperative and defective regions. Using power-law networks that reflect real-world heterogeneous connectivity patterns, we evaluate emergent behaviors under varying rewiring constraint levels, revealing distinct cooperation patterns across parameter space rather than sharp thermodynamic transitions. Our systematic analysis identifies three behavioral regimes: a permissive regime (low constraints) enabling rapid cooperative cluster formation, an intermediate regime with sensitive dependence on dilemma strength, and a patient regime (high constraints) where strategic accumulation gradually optimizes network structure. Simulation results show that while moderate constraints create transition-like zones that suppress cooperation, fully adaptive rewiring enhances cooperation levels through systematic exploration of favorable network configurations. Quantitative analysis reveals that increased rewiring frequency drives large-scale cluster formation with power-law size distributions. Our results establish a new paradigm for understanding intelligence-driven cooperation pattern formation in complex adaptive systems, revealing how machine learning serves as an alternative driving force for spontaneous organization in multi-agent networks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Economy of AI Agents</title>
<link>https://arxiv.org/abs/2509.01063</link>
<guid>https://arxiv.org/abs/2509.01063</guid>
<content:encoded><![CDATA[
arXiv:2509.01063v1 Announce Type: cross 
Abstract: In the coming decade, artificially intelligent agents with the ability to plan and execute complex tasks over long time horizons with little direct oversight from humans may be deployed across the economy. This chapter surveys recent developments and highlights open questions for economists around how AI agents might interact with humans and with each other, shape markets and organizations, and what institutions might be required for well-functioning markets.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRetNet: A Novel Deep Learning Framework for Diabetic Retinopathy Diagnosis</title>
<link>https://arxiv.org/abs/2509.01072</link>
<guid>https://arxiv.org/abs/2509.01072</guid>
<content:encoded><![CDATA[
arXiv:2509.01072v1 Announce Type: cross 
Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide, necessitating early detection to prevent vision loss. Current automated DR detection systems often struggle with poor-quality images, lack interpretability, and insufficient integration of domain-specific knowledge. To address these challenges, we introduce a novel framework that integrates three innovative contributions: (1) Adaptive Retinal Image Enhancement Using Physics-Informed Neural Networks (PINNs): this technique dynamically enhances retinal images by incorporating physical constraints, improving the visibility of critical features such as microaneurysms, hemorrhages, and exudates; (2) Hybrid Feature Fusion Network (HFFN): by combining deep learning embeddings with handcrafted features, HFFN leverages both learned representations and domain-specific knowledge to enhance generalization and accuracy; (3) Multi-Stage Classifier with Uncertainty Quantification: this method breaks down the classification process into logical stages, providing interpretable predictions and confidence scores, thereby improving clinical trust. The proposed framework achieves an accuracy of 92.7%, a precision of 92.5%, a recall of 92.6%, an F1-score of 92.5%, an AUC of 97.8%, a mAP of 0.96, and an MCC of 0.85. Ophthalmologists rated the framework's predictions as highly clinically relevant (4.8/5), highlighting its alignment with real-world diagnostic needs. Qualitative analyses, including Grad-CAM visualizations and uncertainty heatmaps, further enhance the interpretability and trustworthiness of the system. The framework demonstrates robust performance across diverse conditions, including low-quality images, noisy data, and unseen datasets. These features make the proposed framework a promising tool for clinical adoption, enabling more accurate and reliable DR detection in resource-limited settings.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation</title>
<link>https://arxiv.org/abs/2509.01081</link>
<guid>https://arxiv.org/abs/2509.01081</guid>
<content:encoded><![CDATA[
arXiv:2509.01081v1 Announce Type: cross 
Abstract: This paper evaluates the knowledge and reasoning capabilities of Large Language Models in Islamic inheritance law, known as 'ilm al-mawarith. We assess the performance of seven LLMs using a benchmark of 1,000 multiple-choice questions covering diverse inheritance scenarios, designed to test models' ability to understand the inheritance context and compute the distribution of shares prescribed by Islamic jurisprudence. The results reveal a significant performance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas ALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect important differences in reasoning ability and domain adaptation. We conduct a detailed error analysis to identify recurring failure patterns across models, including misunderstandings of inheritance scenarios, incorrect application of legal rules, and insufficient domain knowledge. Our findings highlight limitations in handling structured legal reasoning and suggest directions for improving performance in Islamic legal reasoning. Code: https://github.com/bouchekif/inheritance_evaluation
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving</title>
<link>https://arxiv.org/abs/2509.01083</link>
<guid>https://arxiv.org/abs/2509.01083</guid>
<content:encoded><![CDATA[
arXiv:2509.01083v1 Announce Type: cross 
Abstract: Speculative decoding accelerates large language model inference, but its reliance on a fixed speculation length is suboptimal in large-batch serving environments with diverse requests. This paper explores a new direction for dynamic adaptation by investigating a novel class of post-hoc, diagnostic signals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free framework built on two primary components: (1) a predictive signal based on the variance of the Kullback-Leibler (KLD) divergence, which diagnoses the generation's regional stability, and (2) an adaptive speculation length cap to mitigate the straggler problem in per-sequence decoding. Experiments demonstrate the potential of using KLD-based stability signals for dynamic adaptation. An algorithm guided by these signals achieves end-to-end latency competitive with leading baselines and exhibits superior robustness across diverse workloads. This robustness is particularly valuable in challenging low-acceptance-rate regimes, where the proposed signal maintains its diagnostic utility. Collectively, these findings validate post-hoc signals as a valuable component for building more robust and intelligent LLM inference systems, and highlight a promising direction for future research on dynamic speculation length adaptation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REFRAG: Rethinking RAG based Decoding</title>
<link>https://arxiv.org/abs/2509.01092</link>
<guid>https://arxiv.org/abs/2509.01092</guid>
<content:encoded><![CDATA[
arXiv:2509.01092v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive external knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-augmented generation (RAG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing latency for long-context inputs is a primary objective for LLMs, we contend that RAG require specialized consideration. In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting the sparsity structure, we demonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to previous work) without loss in perplexity. In addition, our optimization framework for large context enables REFRAG to extend the context size of LLMs by 16. We provide rigorous validation of REFRAG across diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm that REFRAG delivers substantial speedup with no loss in accuracy compared to LLaMA models and other state-of-the-art baselines across various context sizes.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Context Drift Undermines the Natural Language Understanding of Large Language Models</title>
<link>https://arxiv.org/abs/2509.01093</link>
<guid>https://arxiv.org/abs/2509.01093</guid>
<content:encoded><![CDATA[
arXiv:2509.01093v1 Announce Type: cross 
Abstract: How does the natural evolution of context paragraphs affect question answering in generative Large Language Models (LLMs)? To investigate this, we propose a framework for curating naturally evolved, human-edited variants of reading passages from contemporary QA benchmarks and for analyzing LLM performance across a range of semantic similarity scores, which quantify how closely each variant aligns with content seen during pretraining. Using this framework, we evaluate six QA datasets and eight LLMs with publicly available training data. Our experiments reveal that LLM performance declines as reading passages naturally diverge from the versions encountered during pretraining-even when the question and all necessary information remains present at inference time. For instance, average model accuracy on BoolQ drops by over 30% from the highest to lowest similarity bins, with slopes exceeding 70 across several LLMs. These findings suggest that natural text evolution poses a significant challenge to the language understanding capabilities of LLMs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCE: Confidence-Consistency Evaluation for Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.01098</link>
<guid>https://arxiv.org/abs/2509.01098</guid>
<content:encoded><![CDATA[
arXiv:2509.01098v1 Announce Type: cross 
Abstract: Time Series Anomaly Detection metrics serve as crucial tools for model evaluation. However, existing metrics suffer from several limitations: insufficient discriminative power, strong hyperparameter dependency, sensitivity to perturbations, and high computational overhead. This paper introduces Confidence-Consistency Evaluation (CCE), a novel evaluation metric that simultaneously measures prediction confidence and uncertainty consistency. By employing Bayesian estimation to quantify the uncertainty of anomaly scores, we construct both global and event-level confidence and consistency scores for model predictions, resulting in a concise CCE metric. Theoretically and experimentally, we demonstrate that CCE possesses strict boundedness, Lipschitz robustness against score perturbations, and linear time complexity $\mathcal{O}(n)$. Furthermore, we establish RankEval, a benchmark for comparing the ranking capabilities of various metrics. RankEval represents the first standardized and reproducible evaluation pipeline that enables objective comparison of evaluation metrics. Both CCE and RankEval implementations are fully open-source.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoLBERT: A No Lookahead(back) Foundational Language Model for Empirical Research</title>
<link>https://arxiv.org/abs/2509.01110</link>
<guid>https://arxiv.org/abs/2509.01110</guid>
<content:encoded><![CDATA[
arXiv:2509.01110v1 Announce Type: cross 
Abstract: We present NoLBERT, a lightweight, timestamped foundational language model for empirical research in social sciences, particularly in economics and finance. By pre-training exclusively on 1976-1995 text, NoLBERT avoids both lookback and lookahead biases that can undermine econometric inference. It exceeds domain-specific baselines on NLP benchmarks while maintaining temporal consistency. Applied to patent texts, NoLBERT enables the construction of firm-level innovation networks and shows that gains in innovation centrality predict higher long-run profit growth.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SC-GIR: Goal-oriented Semantic Communication via Invariant Representation Learning</title>
<link>https://arxiv.org/abs/2509.01119</link>
<guid>https://arxiv.org/abs/2509.01119</guid>
<content:encoded><![CDATA[
arXiv:2509.01119v1 Announce Type: cross 
Abstract: Goal-oriented semantic communication (SC) aims to revolutionize communication systems by transmitting only task-essential information. However, current approaches face challenges such as joint training at transceivers, leading to redundant data exchange and reliance on labeled datasets, which limits their task-agnostic utility. To address these challenges, we propose a novel framework called Goal-oriented Invariant Representation-based SC (SC-GIR) for image transmission. Our framework leverages self-supervised learning to extract an invariant representation that encapsulates crucial information from the source data, independent of the specific downstream task. This compressed representation facilitates efficient communication while retaining key features for successful downstream task execution. Focusing on machine-to-machine tasks, we utilize covariance-based contrastive learning techniques to obtain a latent representation that is both meaningful and semantically dense. To evaluate the effectiveness of the proposed scheme on downstream tasks, we apply it to various image datasets for lossy compression. The compressed representations are then used in a goal-oriented AI task. Extensive experiments on several datasets demonstrate that SC-GIR outperforms baseline schemes by nearly 10%,, and achieves over 85% classification accuracy for compressed data under different SNR conditions. These results underscore the effectiveness of the proposed framework in learning compact and informative latent representations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATL-DC: A Multi-domain Aggregation Transfer Learning Framework for EEG Emotion Recognition with Domain-Class Prototype under Unseen Targets</title>
<link>https://arxiv.org/abs/2509.01135</link>
<guid>https://arxiv.org/abs/2509.01135</guid>
<content:encoded><![CDATA[
arXiv:2509.01135v1 Announce Type: cross 
Abstract: Emotion recognition based on electroencephalography (EEG) signals is increasingly becoming a key research hotspot in affective Brain-Computer Interfaces (aBCIs). However, the current transfer learning model greatly depends on the source domain and target domain data, which hinder the practical application of emotion recognition. Therefore, we propose a Multi-domain Aggregation Transfer Learning framework for EEG emotion recognition with Domain-Class prototype under unseen targets (MATL-DC). We design the feature decoupling module to decouple class-invariant domain features from domain-invariant class features from shallow features. In the model training stage, the multi-domain aggregation mechanism aggregates the domain feature space to form a superdomain, which enhances the characteristics of emotional EEG signals. In each superdomain, we further extract the class prototype representation by class features. In addition, we adopt the pairwise learning strategy to transform the sample classification problem into the similarity problem between sample pairs, which effectively alleviates the influence of label noise. It is worth noting that the target domain is completely unseen during the training process. In the inference stage, we use the trained domain-class prototypes for inference, and then realize emotion recognition. We rigorously validate it on the publicly available databases (SEED, SEED-IV and SEED-V). The results show that the accuracy of MATL-DC model is 84.70\%, 68.11\% and 61.08\%, respectively. MATL-DC achieves comparable or even better performance than methods that rely on both source and target domains. The source code is available at https://github.com/WuCB-BCI/MATL-DC.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection</title>
<link>https://arxiv.org/abs/2509.01153</link>
<guid>https://arxiv.org/abs/2509.01153</guid>
<content:encoded><![CDATA[
arXiv:2509.01153v1 Announce Type: cross 
Abstract: Auscultation is a key method for early diagnosis of respiratory and pulmonary diseases, relying on skilled healthcare professionals. However, the process is often subjective, with variability between experts. As a result, numerous deep learning-based automatic classification methods have emerged, most of which focus on respiratory sound classification. In contrast, research on respiratory sound event detection remains limited. Existing sound event detection methods typically rely on frame-level predictions followed by post-processing to generate event-level outputs, making interval boundaries challenging to learn directly. Furthermore, many approaches can only handle fixed-length audio, lim- iting their applicability to variable-length respiratory sounds. Additionally, the impact of respiratory sound location information on detection performance has not been extensively explored. To address these issues, we propose a graph neural network-based framework with anchor intervals, capable of handling variable-length audio and providing more precise temporal localization for abnormal respi- ratory sound events. Our method improves both the flexibility and applicability of respiratory sound detection. Experiments on the SPRSound 2024 and HF Lung V1 datasets demonstrate the effec- tiveness of the proposed approach, and incorporating respiratory position information enhances the discrimination between abnormal sounds.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning</title>
<link>https://arxiv.org/abs/2509.01166</link>
<guid>https://arxiv.org/abs/2509.01166</guid>
<content:encoded><![CDATA[
arXiv:2509.01166v1 Announce Type: cross 
Abstract: Knowledge graph completion (KGC) aims to infer new knowledge and make predictions from knowledge graphs. Recently, large language models (LLMs) have exhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily focus on designing task-specific instructions, achieving promising advancements. However, there are still two critical challenges. First, existing methods often ignore the inconsistent representation spaces between natural language and graph structures. Second, most approaches design separate instructions for different KGC tasks, leading to duplicate works and time-consuming processes. To address these challenges, we propose SAT, a novel framework that enhances LLMs for KGC via structure-aware alignment-tuning. Specifically, we first introduce hierarchical knowledge alignment to align graph embeddings with the natural language space through multi-task contrastive learning. Then, we propose structural instruction tuning to guide LLMs in performing structure-aware reasoning over KGs, using a unified graph instruction combined with a lightweight knowledge adapter. Experimental results on two KGC tasks across four benchmark datasets demonstrate that SAT significantly outperforms state-of-the-art methods, especially in the link prediction task with improvements ranging from 8.7% to 29.8%.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion</title>
<link>https://arxiv.org/abs/2509.01177</link>
<guid>https://arxiv.org/abs/2509.01177</guid>
<content:encoded><![CDATA[
arXiv:2509.01177v1 Announce Type: cross 
Abstract: Reconstruction dynamic visual scenes from electroencephalography (EEG) signals remains a primary challenge in brain decoding, limited by the low spatial resolution of EEG, a temporal mismatch between neural recordings and video dynamics, and the insufficient use of semantic information within brain activity. Therefore, existing methods often inadequately resolve both the dynamic coherence and the complex semantic context of the perceived visual stimuli. To overcome these limitations, we introduce DynaMind, a novel framework that reconstructs video by jointly modeling neural dynamics and semantic features via three core modules: a Regional-aware Semantic Mapper (RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video Reconstructor (DGVR). The RSM first utilizes a regional-aware encoder to extract multimodal semantic features from EEG signals across distinct brain regions, aggregating them into a unified diffusion prior. In the mean time, the TDA generates a dynamic latent sequence, or blueprint, to enforce temporal consistency between the feature representations and the original neural recordings. Together, guided by the semantic diffusion prior, the DGVR translates the temporal-aware blueprint into a high-fidelity video reconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art (SOTA), boosting reconstructed video accuracies (video- and frame-based) by 12.5 and 10.3 percentage points, respectively. It also achieves a leap in pixel-level quality, showing exceptional visual fidelity and temporal coherence with a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical advancement, bridging the gap between neural dynamics and high-fidelity visual semantics.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus</title>
<link>https://arxiv.org/abs/2509.01181</link>
<guid>https://arxiv.org/abs/2509.01181</guid>
<content:encoded><![CDATA[
arXiv:2509.01181v1 Announce Type: cross 
Abstract: Multi-subject personalized image generation aims to synthesize customized images containing multiple specified subjects without requiring test-time optimization. However, achieving fine-grained independent control over multiple subjects remains challenging due to difficulties in preserving subject fidelity and preventing cross-subject attribute leakage. We present FocusDPO, a framework that adaptively identifies focus regions based on dynamic semantic correspondence and supervision image complexity. During training, our method progressively adjusts these focal areas across noise timesteps, implementing a weighted strategy that rewards information-rich patches while penalizing regions with low prediction confidence. The framework dynamically adjusts focus allocation during the DPO process according to the semantic complexity of reference images and establishes robust correspondence mappings between generated and reference subjects. Extensive experiments demonstrate that our method substantially enhances the performance of existing pre-trained personalized generation models, achieving state-of-the-art results on both single-subject and multi-subject personalized image synthesis benchmarks. Our method effectively mitigates attribute leakage while preserving superior subject fidelity across diverse generation scenarios, advancing the frontier of controllable multi-subject image synthesis.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation</title>
<link>https://arxiv.org/abs/2509.01185</link>
<guid>https://arxiv.org/abs/2509.01185</guid>
<content:encoded><![CDATA[
arXiv:2509.01185v1 Announce Type: cross 
Abstract: The ability of large language models (LLMs) to process and reason over long textual inputs is critical for a wide range of real-world applications. However, progress in this area is significantly constrained by the absence of high-quality, diverse, and verifiable long-context datasets suitable for both training and evaluation. This work introduces a modular, extensible framework for synthetic long-context data generation via prompt-based interaction with LLMs. The framework supports multiple training and alignment objectives, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO). It encompasses four core generation paradigms: multi-turn conversational dialogues, document-grounded input-output pairs, verifiable instruction-response tasks, and long-context reasoning examples. Through templated prompting, a model-agnostic architecture, and metadata-enriched outputs, the proposed approach facilitates scalable, controllable, and purpose-aligned dataset creation for advancing long-context capabilities in LLMs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statutory Construction and Interpretation for Artificial Intelligence</title>
<link>https://arxiv.org/abs/2509.01186</link>
<guid>https://arxiv.org/abs/2509.01186</guid>
<content:encoded><![CDATA[
arXiv:2509.01186v1 Announce Type: cross 
Abstract: AI systems are increasingly governed by natural language principles, yet a key challenge arising from reliance on language remains underexplored: interpretive ambiguity. As in legal systems, ambiguity arises both from how these principles are written and how they are applied. But while legal systems use institutional safeguards to manage such ambiguity, such as transparent appellate review policing interpretive constraints, AI alignment pipelines offer no comparable protections. Different interpretations of the same rule can lead to inconsistent or unstable model behavior. Drawing on legal theory, we identify key gaps in current alignment pipelines by examining how legal systems constrain ambiguity at both the rule creation and rule application steps. We then propose a computational framework that mirrors two legal mechanisms: (1) a rule refinement pipeline that minimizes interpretive disagreement by revising ambiguous rules (analogous to agency rulemaking or iterative legislative action), and (2) prompt-based interpretive constraints that reduce inconsistency in rule application (analogous to legal canons that guide judicial discretion). We evaluate our framework on a 5,000-scenario subset of the WildChat dataset and show that both interventions significantly improve judgment consistency across a panel of reasonable interpreters. Our approach offers a first step toward systematically managing interpretive ambiguity, an essential step for building more robust, law-following AI systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preserving Vector Space Properties in Dimensionality Reduction: A Relationship Preserving Loss Framework</title>
<link>https://arxiv.org/abs/2509.01198</link>
<guid>https://arxiv.org/abs/2509.01198</guid>
<content:encoded><![CDATA[
arXiv:2509.01198v1 Announce Type: cross 
Abstract: Dimensionality reduction can distort vector space properties such as orthogonality and linear independence, which are critical for tasks including cross-modal retrieval, clustering, and classification. We propose a Relationship Preserving Loss (RPL), a loss function that preserves these properties by minimizing discrepancies between relationship matrices (e.g., Gram or cosine) of high-dimensional data and their low-dimensional embeddings. RPL trains neural networks for non-linear projections and is supported by error bounds derived from matrix perturbation theory. Initial experiments suggest that RPL reduces embedding dimensions while largely retaining performance on downstream tasks, likely due to its preservation of key vector space properties. While we describe here the use of RPL in dimensionality reduction, this loss can also be applied more broadly, for example to cross-domain alignment and transfer learning, knowledge distillation, fairness and invariance, dehubbing, graph and manifold learning, and federated learning, where distributed embeddings must remain geometrically consistent.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Self-supervised Monocular Depth Estimation with Mixture of Low-Rank Experts for Diverse Endoscopic Scenes</title>
<link>https://arxiv.org/abs/2509.01206</link>
<guid>https://arxiv.org/abs/2509.01206</guid>
<content:encoded><![CDATA[
arXiv:2509.01206v1 Announce Type: cross 
Abstract: Self-supervised monocular depth estimation is a significant task for low-cost and efficient three-dimensional scene perception in endoscopy. The variety of illumination conditions and scene features is still the primary challenge for generalizable depth estimation in endoscopic scenes. In this work, a self-supervised framework is proposed for monocular depth estimation in various endoscopy. Firstly, due to various features in endoscopic scenes with different tissues, a novel block-wise mixture of dynamic low-rank experts is proposed to efficiently finetuning the foundation model for endoscopic depth estimation. In the proposed module, based on the input feature, different experts with a small amount of trainable parameters are adaptively selected for weighted inference, from various mixture of low-rank experts which are allocated based on the training quality of each block. Moreover, a novel self-supervised training framework is proposed to jointly cope with the inconsistency of brightness and reflectance. The proposed method outperform state-of-the-art works on both realistic and simulated endoscopic datasets. Furthermore, the proposed network also achieves the best generalization based on zero-shot depth estimation on diverse endoscopic scenes. The proposed method could contribute to accurate endoscopic perception for minimally invasive measurement and surgery. The code will be released upon acceptance, while the demo video can be found on here: https://endo-gede.netlify.app/.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web Fraud Attacks Against LLM-Driven Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.01211</link>
<guid>https://arxiv.org/abs/2509.01211</guid>
<content:encoded><![CDATA[
arXiv:2509.01211v1 Announce Type: cross 
Abstract: With the proliferation of applications built upon LLM-driven multi-agent systems (MAS), the security of Web links has become a critical concern in ensuring system reliability. Once an agent is induced to visit a malicious website, attackers can use it as a springboard to conduct diverse subsequent attacks, which will drastically expand the attack surface. In this paper, we propose Web Fraud Attacks, a novel type of attack aiming at inducing MAS to visit malicious websites. We design 11 representative attack variants that encompass domain name tampering (homoglyph deception, character substitution, etc.), link structure camouflage (sub-directory nesting, sub-domain grafting, parameter obfuscation, etc.), and other deceptive techniques tailored to exploit MAS's vulnerabilities in link validation. Through extensive experiments on these crafted attack vectors, we demonstrate that Web fraud attacks not only exhibit significant destructive potential across different MAS architectures but also possess a distinct advantage in evasion: they circumvent the need for complex input formats such as jailbreaking, which inherently carry higher exposure risks. These results underscore the importance of addressing Web fraud attacks in LLM-driven MAS, as their stealthiness and destructiveness pose non-negligible threats to system security and user safety.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Taks Based on Data and Model Compression</title>
<link>https://arxiv.org/abs/2509.01221</link>
<guid>https://arxiv.org/abs/2509.01221</guid>
<content:encoded><![CDATA[
arXiv:2509.01221v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel in general tasks but struggle with domain-specific ones, requiring fine-tuning with specific data. With many open-source LLMs available, selecting the best model for fine-tuning downstream tasks is challenging, primarily focusing on how to quickly identify the optimal LLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses this challenge by: 1) Data Level: A systematic categorization of data filtering methodologies for LLMs is first established, classifying them into three distinct paradigms: (1) distribution-aware methods, (2) quality-aware methods, and (3) hybrid approaches considering both dimensions. Further, we enhance the density of key tokens in the text achieving token compression. Subsequently, we use an LLM to iterative rewrite the text to optimize its expression. 2) Model Level: We use layer similarity scores to assess each layer's importance and remove those with lower importance. Then, we introduce a sparse merging paradigm to preserve as much of the original model's capability as possible. Extensive experiments on four datasets, medical Q&amp;A, financial Q&amp;A, general Q&amp;A, and reading comprehension, show that we can select the optimal LLM while saving approximately 20-fold in training time.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiquidGEMM: Hardware-Efficient W4A8 GEMM Kernel for High-Performance LLM Serving</title>
<link>https://arxiv.org/abs/2509.01229</link>
<guid>https://arxiv.org/abs/2509.01229</guid>
<content:encoded><![CDATA[
arXiv:2509.01229v1 Announce Type: cross 
Abstract: Quantization is a critical technique for accelerating LLM inference by reducing memory footprint and improving computational efficiency. Among various schemes, 4-bit weight and 8-bit activation quantization (W4A8) offers a strong balance between accuracy and performance. However, existing W4A8 GEMM kernels fall short in practice due to inefficient dequantization on CUDA Cores, which cannot keep pace with the high throughput of Tensor Cores. In this paper, we present LiquidGEMM, a hardware-efficient W4A8 GEMM kernel for efficient LLM serving. LiquidGEMM designs two key techniques: LiquidQuant, a hardware-efficient quantization method that enables fast, overflow-safe dequantization using just two arithmetic instructions per four elements; and an implicit fine-grained pipeline that fully overlaps weight loading, dequantization, and MMA across warp groups without software synchronization or redundant memory traffic. Experimental results show that LiquidGEMM achieves up to 2.90x speedup over state-of-the-art W4A8 kernels and up to 4.94x end-to-end system-level speedup. Compared to various quantized GEMM kernels in NVIDIA TensorRT-LLM, LiquidGEMM delivers 1.12-1.63x performance gains, and achieves up to 1.63x system-level speedup.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors</title>
<link>https://arxiv.org/abs/2509.01236</link>
<guid>https://arxiv.org/abs/2509.01236</guid>
<content:encoded><![CDATA[
arXiv:2509.01236v1 Announce Type: cross 
Abstract: Chain-of-Thought reasoning has emerged as a pivotal methodology for enhancing model inference capabilities. Despite growing interest in Chain-of-Thought reasoning, its underlying mechanisms remain unclear. This paper explores the working mechanisms of Chain-of-Thought reasoning from the perspective of the dual relationship between in-context learning and pretrained priors. We first conduct a fine-grained lexical-level analysis of rationales to examine the model's reasoning behavior. Then, by incrementally introducing noisy exemplars, we examine how the model balances pretrained priors against erroneous in-context information. Finally, we investigate whether prompt engineering can induce slow thinking in large language models. Our extensive experiments reveal three key findings: (1) The model not only quickly learns the reasoning structure at the lexical level but also grasps deeper logical reasoning patterns, yet it heavily relies on pretrained priors. (2) Providing sufficient exemplars shifts the model's decision-making from pretrained priors to in-context signals, while misleading prompts introduce instability. (3) Long Chain-of-Thought prompting can induce the model to generate longer reasoning chains, thereby improving its performance on downstream tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-DETRv2 Explained in 8 Illustrations</title>
<link>https://arxiv.org/abs/2509.01241</link>
<guid>https://arxiv.org/abs/2509.01241</guid>
<content:encoded><![CDATA[
arXiv:2509.01241v1 Announce Type: cross 
Abstract: Object detection architectures are notoriously difficult to understand, often more so than large language models. While RT-DETRv2 represents an important advance in real-time detection, most existing diagrams do little to clarify how its components actually work and fit together. In this article, we explain the architecture of RT-DETRv2 through a series of eight carefully designed illustrations, moving from the overall pipeline down to critical components such as the encoder, decoder, and multi-scale deformable attention. Our goal is to make the existing one genuinely understandable. By visualizing the flow of tensors and unpacking the logic behind each module, we hope to provide researchers and practitioners with a clearer mental model of how RT-DETRv2 works under the hood.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge Networks</title>
<link>https://arxiv.org/abs/2509.01257</link>
<guid>https://arxiv.org/abs/2509.01257</guid>
<content:encoded><![CDATA[
arXiv:2509.01257v1 Announce Type: cross 
Abstract: In edge computing systems, autonomous agents must make fast local decisions while competing for shared resources. Existing MARL methods often resume to centralized critics or frequent communication, which fail under limited observability and communication constraints. We propose a decentralized framework in which each agent solves a constrained Markov decision process (CMDP), coordinating implicitly through a shared constraint vector. For the specific case of offloading, e.g., constraints prevent overloading shared server resources. Coordination constraints are updated infrequently and act as a lightweight coordination mechanism. They enable agents to align with global resource usage objectives but require little direct communication. Using safe reinforcement learning, agents learn policies that meet both local and global goals. We establish theoretical guarantees under mild assumptions and validate our approach experimentally, showing improved performance over centralized and independent baselines, especially in large-scale settings.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building surrogate models using trajectories of agents trained by Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.01285</link>
<guid>https://arxiv.org/abs/2509.01285</guid>
<content:encoded><![CDATA[
arXiv:2509.01285v1 Announce Type: cross 
Abstract: Sample efficiency in the face of computationally expensive simulations is a common concern in surrogate modeling. Current strategies to minimize the number of samples needed are not as effective in simulated environments with wide state spaces. As a response to this challenge, we propose a novel method to efficiently sample simulated deterministic environments by using policies trained by Reinforcement Learning. We provide an extensive analysis of these surrogate-building strategies with respect to Latin-Hypercube sampling or Active Learning and Kriging, cross-validating performances with all sampled datasets. The analysis shows that a mixed dataset that includes samples acquired by random agents, expert agents, and agents trained to explore the regions of maximum entropy of the state transition distribution provides the best scores through all datasets, which is crucial for a meaningful state space representation. We conclude that the proposed method improves the state-of-the-art and clears the path to enable the application of surrogate-aided Reinforcement Learning policy optimization strategies on complex simulators.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Animer une base de connaissance: des ontologies aux mod{\`e}les d'I.A. g{\'e}n{\'e}rative</title>
<link>https://arxiv.org/abs/2509.01304</link>
<guid>https://arxiv.org/abs/2509.01304</guid>
<content:encoded><![CDATA[
arXiv:2509.01304v1 Announce Type: cross 
Abstract: In a context where the social sciences and humanities are experimenting with non-anthropocentric analytical frames, this article proposes a semiotic (structural) reading of the hybridization between symbolic AI and neural (or sub-symbolic) AI based on a field of application: the design and use of a knowledge base for area studies. We describe the LaCAS ecosystem -- Open Archives in Linguistic and Cultural Studies (thesaurus; RDF/OWL ontology; LOD services; harvesting; expertise; publication), deployed at Inalco (National Institute for Oriental Languages and Civilizations) in Paris with the Okapi (Open Knowledge and Annotation Interface) software environment from Ina (National Audiovisual Institute), which now has around 160,000 documentary resources and ten knowledge macro-domains grouping together several thousand knowledge objects. We illustrate this approach using the knowledge domain ''Languages of the world'' (~540 languages) and the knowledge object ''Quechua (language)''. On this basis, we discuss the controlled integration of neural tools, more specifically generative tools, into the life cycle of a knowledge base: assistance with data localization/qualification, index extraction and aggregation, property suggestion and testing, dynamic file generation, and engineering of contextualized prompts (generic, contextual, explanatory, adjustment, procedural) aligned with a domain ontology. We outline an ecosystem of specialized agents capable of animating the database while respecting its symbolic constraints, by articulating model-driven and data-driven methods.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Trustworthy Vital Sign Forecasting: Leveraging Uncertainty for Prediction Intervals</title>
<link>https://arxiv.org/abs/2509.01319</link>
<guid>https://arxiv.org/abs/2509.01319</guid>
<content:encoded><![CDATA[
arXiv:2509.01319v1 Announce Type: cross 
Abstract: Vital signs, such as heart rate and blood pressure, are critical indicators of patient health and are widely used in clinical monitoring and decision-making. While deep learning models have shown promise in forecasting these signals, their deployment in healthcare remains limited in part because clinicians must be able to trust and interpret model outputs. Without reliable uncertainty quantification -- particularly calibrated prediction intervals (PIs) -- it is unclear whether a forecasted abnormality constitutes a meaningful warning or merely reflects model noise, hindering clinical decision-making. To address this, we present two methods for deriving PIs from the Reconstruction Uncertainty Estimate (RUE), an uncertainty measure well-suited to vital-sign forecasting due to its sensitivity to data shifts and support for label-free calibration. Our parametric approach assumes that prediction errors and uncertainty estimates follow a Gaussian copula distribution, enabling closed-form PI computation. Our non-parametric approach, based on k-nearest neighbours (KNN), empirically estimates the conditional error distribution using similar validation instances. We evaluate these methods on two large public datasets with minute- and hour-level sampling, representing high- and low-frequency health signals. Experiments demonstrate that the Gaussian copula method consistently outperforms conformal prediction baselines on low-frequency data, while the KNN approach performs best on high-frequency data. These results underscore the clinical promise of RUE-derived PIs for delivering interpretable, uncertainty-aware vital sign forecasts.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongCat-Flash Technical Report</title>
<link>https://arxiv.org/abs/2509.01322</link>
<guid>https://arxiv.org/abs/2509.01322</guid>
<content:encoded><![CDATA[
arXiv:2509.01322v1 Announce Type: cross 
Abstract: We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research.
  LongCat Chat: https://longcat.ai
  Hugging Face: https://huggingface.co/meituan-longcat
  GitHub: https://github.com/meituan-longcat
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multitask Battery Management with Flexible Pretraining</title>
<link>https://arxiv.org/abs/2509.01323</link>
<guid>https://arxiv.org/abs/2509.01323</guid>
<content:encoded><![CDATA[
arXiv:2509.01323v1 Announce Type: cross 
Abstract: Industrial-scale battery management involves various types of tasks, such as estimation, prediction, and system-level diagnostics. Each task employs distinct data across temporal scales, sensor resolutions, and data channels. Building task-specific methods requires a great deal of data and engineering effort, which limits the scalability of intelligent battery management. Here we present the Flexible Masked Autoencoder (FMAE), a flexible pretraining framework that can learn with missing battery data channels and capture inter-correlations across data snippets. FMAE learns unified battery representations from heterogeneous data and can be adopted by different tasks with minimal data and engineering efforts. Experimentally, FMAE consistently outperforms all task-specific methods across five battery management tasks with eleven battery datasets. On remaining life prediction tasks, FMAE uses 50 times less inference data while maintaining state-of-the-art results. Moreover, when real-world data lack certain information, such as system voltage, FMAE can still be applied with marginal performance impact, achieving comparable results with the best hand-crafted features. FMAE demonstrates a practical route to a flexible, data-efficient model that simplifies real-world multi-task management of dynamical systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition</title>
<link>https://arxiv.org/abs/2509.01337</link>
<guid>https://arxiv.org/abs/2509.01337</guid>
<content:encoded><![CDATA[
arXiv:2509.01337v1 Announce Type: cross 
Abstract: Understanding human intents from multimodal signals is critical for analyzing human behaviors and enhancing human-machine interactions in real-world scenarios. However, existing methods exhibit limitations in their modality-level reliance, constraining relational reasoning over fine-grained semantics for complex intent understanding. This paper proposes a novel LLM-Guided Semantic Relational Reasoning (LGSRR) method, which harnesses the expansive knowledge of large language models (LLMs) to establish semantic foundations that boost smaller models' relational reasoning performance. Specifically, an LLM-based strategy is proposed to extract fine-grained semantics as guidance for subsequent reasoning, driven by a shallow-to-deep Chain-of-Thought (CoT) that autonomously uncovers, describes, and ranks semantic cues by their importance without relying on manually defined priors. Besides, we formally model three fundamental types of semantic relations grounded in logical principles and analyze their nuanced interplay to enable more effective relational reasoning. Extensive experiments on multimodal intent and dialogue act recognition tasks demonstrate LGSRR's superiority over state-of-the-art methods, with consistent performance gains across diverse semantic understanding scenarios. The complete data and code are available at https://github.com/thuiar/LGSRR.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2509.01341</link>
<guid>https://arxiv.org/abs/2509.01341</guid>
<content:encoded><![CDATA[
arXiv:2509.01341v1 Announce Type: cross 
Abstract: Street-level geolocalization from images is crucial for a wide range of essential applications and services, such as navigation, location-based recommendations, and urban planning. With the growing popularity of social media data and cameras embedded in smartphones, applying traditional computer vision techniques to localize images has become increasingly challenging, yet highly valuable. This paper introduces a novel approach that integrates open-weight and publicly accessible multimodal large language models with retrieval-augmented generation. The method constructs a vector database using the SigLIP encoder on two large-scale datasets (EMP-16 and OSV-5M). Query images are augmented with prompts containing both similar and dissimilar geolocation information retrieved from this database before being processed by the multimodal large language models. Our approach has demonstrated state-of-the-art performance, achieving higher accuracy compared against three widely used benchmark datasets (IM2GPS, IM2GPS3k, and YFCC4k). Importantly, our solution eliminates the need for expensive fine-tuning or retraining and scales seamlessly to incorporate new data sources. The effectiveness of retrieval-augmented generation-based multimodal large language models in geolocation estimation demonstrated by this paper suggests an alternative path to the traditional methods which rely on the training models from scratch, opening new possibilities for more accessible and scalable solutions in GeoAI.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AT Loss: Advanced Torrential Loss Function for Precipitation Forecasting</title>
<link>https://arxiv.org/abs/2509.01348</link>
<guid>https://arxiv.org/abs/2509.01348</guid>
<content:encoded><![CDATA[
arXiv:2509.01348v1 Announce Type: cross 
Abstract: Accurate precipitation forecasting is becoming increasingly important in the context of climate change. In response, machine learning-based approaches have recently gained attention as an emerging alternative to traditional methods such as numerical weather prediction and climate models. Nonetheless, many recent approaches still rely on off-the-shelf loss functions, and even the more advanced ones merely involve optimization processes based on the critical success index (CSI). The problem, however, is that CSI may become ineffective during extended dry periods when precipitation remains below the threshold, rendering it less than ideal as a criterion for optimization. To address this limitation, we introduce a simple penalty expression and reinterpret it as a quadratic unconstrained binary optimization (QUBO) formulation. Ultimately, the resulting QUBO formulation is relaxed into a differentiable advanced torrential (AT) loss function through an approximation process. The proposed AT loss demonstrates its superiority through the Lipschitz constant, forecast performance evaluations, consistency experiments, and ablation studies with the operational model.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Sensitivity Identification using Generative Learning</title>
<link>https://arxiv.org/abs/2509.01352</link>
<guid>https://arxiv.org/abs/2509.01352</guid>
<content:encoded><![CDATA[
arXiv:2509.01352v1 Announce Type: cross 
Abstract: In this work, we propose a novel generative method to identify the causal impact and apply it to prediction tasks. We conduct causal impact analysis using interventional and counterfactual perspectives. First, applying interventions, we identify features that have a causal influence on the predicted outcome, which we refer to as causally sensitive features, and second, applying counterfactuals, we evaluate how changes in the cause affect the effect. Our method exploits the Conditional Variational Autoencoder (CVAE) to identify the causal impact and serve as a generative predictor. We are able to reduce confounding bias by identifying causally sensitive features. We demonstrate the effectiveness of our method by recommending the most likely locations a user will visit next in their spatiotemporal trajectory influenced by the causal relationships among various features. Experiments on the large-scale GeoLife [Zheng et al., 2010] dataset and the benchmark Asia Bayesian network validate the ability of our method to identify causal impact and improve predictive performance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPF-CM: A Data Processing Framework with Privacy-Preserving Vector Databases for Chinese Medical LLMs Training and Deployment</title>
<link>https://arxiv.org/abs/2509.01354</link>
<guid>https://arxiv.org/abs/2509.01354</guid>
<content:encoded><![CDATA[
arXiv:2509.01354v1 Announce Type: cross 
Abstract: Current open-source training pipelines for Chinese medical language models predominantly emphasize optimizing training methodologies to enhance the performance of large language models (LLMs), yet lack comprehensive exploration into training data processing. To address this gap, we propose DPF-CM, a holistic Data Processing Framework for Chinese Medical LLMs training and deployment. DPF-CM comprises two core modules. The first module is a data processing pipeline tailored for model training. Beyond standard data processing operations, we (1) introduce a chained examples context-learning strategy to generate question-oriented instructions to mitigate the lack of instruction content, and (2) implement an ensemble-based filtering mechanism for preference data curation that averages multiple reward models to suppress noisy samples. The second module focuses on privacy preservation during model deployment. To prevent privacy risks from the inadvertent exposure of training data, we propose a Privacy Preserving Vector Database (PPVD) approach, which involves model memory search, high-risk database construction, secure database construction, and match-and-replace, four key stages to minimize privacy leakage during inference collectively. Experimental results show that DPF-CM significantly improves model accuracy, enabling our trained Chinese medical LLM to achieve state-of-the-art performance among open-source counterparts. Moreover, the framework reduces training data privacy leakage by 27%.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uirapuru: Timely Video Analytics for High-Resolution Steerable Cameras on Edge Devices</title>
<link>https://arxiv.org/abs/2509.01371</link>
<guid>https://arxiv.org/abs/2509.01371</guid>
<content:encoded><![CDATA[
arXiv:2509.01371v1 Announce Type: cross 
Abstract: Real-time video analytics on high-resolution cameras has become a popular technology for various intelligent services like traffic control and crowd monitoring. While extensive work has been done on improving analytics accuracy with timing guarantees, virtually all of them target static viewpoint cameras. In this paper, we present Uirapuru, a novel framework for real-time, edge-based video analytics on high-resolution steerable cameras. The actuation performed by those cameras brings significant dynamism to the scene, presenting a critical challenge to existing popular approaches such as frame tiling. To address this problem, Uirapuru incorporates a comprehensive understanding of camera actuation into the system design paired with fast adaptive tiling at a per-frame level. We evaluate Uirapuru on a high-resolution video dataset, augmented by pan-tilt-zoom (PTZ) movements typical for steerable cameras and on real-world videos collected from an actual PTZ camera. Our experimental results show that Uirapuru provides up to 1.45x improvement in accuracy while respecting specified latency budgets or reaches up to 4.53x inference speedup with on-par accuracy compared to state-of-the-art static camera approaches.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly detection in network flows using unsupervised online machine learning</title>
<link>https://arxiv.org/abs/2509.01375</link>
<guid>https://arxiv.org/abs/2509.01375</guid>
<content:encoded><![CDATA[
arXiv:2509.01375v1 Announce Type: cross 
Abstract: Nowadays, the volume of network traffic continues to grow, along with the frequency and sophistication of attacks. This scenario highlights the need for solutions capable of continuously adapting, since network behavior is dynamic and changes over time. This work presents an anomaly detection model for network flows using unsupervised machine learning with online learning capabilities. This approach allows the system to dynamically learn the normal behavior of the network and detect deviations without requiring labeled data, which is particularly useful in real-world environments where traffic is constantly changing and labeled data is scarce. The model was implemented using the River library with a One-Class SVM and evaluated on the NF-UNSW-NB15 dataset and its extended version v2, which contain network flows labeled with different attack categories. The results show an accuracy above 98%, a false positive rate below 3.1%, and a recall of 100% in the most advanced version of the dataset. In addition, the low processing time per flow (<0.033 ms) demonstrates the feasibility of the approach for real-time applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Low-Level Neural Control of an Industrial-Grade 6D Magnetic Levitation System</title>
<link>https://arxiv.org/abs/2509.01388</link>
<guid>https://arxiv.org/abs/2509.01388</guid>
<content:encoded><![CDATA[
arXiv:2509.01388v1 Announce Type: cross 
Abstract: Magnetic levitation is poised to revolutionize industrial automation by integrating flexible in-machine product transport and seamless manipulation. It is expected to become the standard drive for automated manufacturing. However, controlling such systems is inherently challenging due to their complex, unstable dynamics. Traditional control approaches, which rely on hand-crafted control engineering, typically yield robust but conservative solutions, with their performance closely tied to the expertise of the engineering team. In contrast, neural control learning presents a promising alternative. This paper presents the first neural controller for 6D magnetic levitation. Trained end-to-end on interaction data from a proprietary controller, it directly maps raw sensor data and 6D reference poses to coil current commands. The neural controller can effectively generalize to previously unseen situations while maintaining accurate and robust control. These results underscore the practical feasibility of learning-based neural control in complex physical systems and suggest a future where such a paradigm could enhance or even substitute traditional engineering approaches in demanding real-world applications. The trained neural controller, source code, and demonstration videos are publicly available at https://sites.google.com/view/neural-maglev.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs cannot spot math errors, even when allowed to peek into the solution</title>
<link>https://arxiv.org/abs/2509.01395</link>
<guid>https://arxiv.org/abs/2509.01395</guid>
<content:encoded><![CDATA[
arXiv:2509.01395v1 Announce Type: cross 
Abstract: Large language models (LLMs) demonstrate remarkable performance on math word problems, yet they have been shown to struggle with meta-reasoning tasks such as identifying errors in student solutions. In this work, we investigate the challenge of locating the first error step in stepwise solutions using two error reasoning datasets: VtG and PRM800K. Our experiments show that state-of-the-art LLMs struggle to locate the first error step in student solutions even when given access to the reference solution. To that end, we propose an approach that generates an intermediate corrected student solution, aligning more closely with the original student's solution, which helps improve performance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech Separation with Distributed Heterogeneous Arrays</title>
<link>https://arxiv.org/abs/2509.01399</link>
<guid>https://arxiv.org/abs/2509.01399</guid>
<content:encoded><![CDATA[
arXiv:2509.01399v1 Announce Type: cross 
Abstract: Separating overlapping speech from multiple speakers is crucial for effective human-vehicle interaction. This paper proposes CabinSep, a lightweight neural mask-based minimum variance distortionless response (MVDR) speech separation approach, to reduce speech recognition errors in back-end automatic speech recognition (ASR) models. Our contributions are threefold: First, we utilize channel information to extract spatial features, which improves the estimation of speech and noise masks. Second, we employ MVDR during inference, reducing speech distortion to make it more ASR-friendly. Third, we introduce a data augmentation method combining simulated and real-recorded impulse responses (IRs), improving speaker localization at zone boundaries and further reducing speech recognition errors. With a computational complexity of only 0.4 GMACs, CabinSep achieves a 17.5% relative reduction in speech recognition error rate in a real-recorded dataset compared to the state-of-the-art DualSep model. Demos are available at: https://cabinsep.github.io/cabinsep/.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCA: Graph-Guided Deep Embedding Clustering for Brain Atlases</title>
<link>https://arxiv.org/abs/2509.01426</link>
<guid>https://arxiv.org/abs/2509.01426</guid>
<content:encoded><![CDATA[
arXiv:2509.01426v1 Announce Type: cross 
Abstract: Brain atlases are essential for reducing the dimensionality of neuroimaging data and enabling interpretable analysis. However, most existing atlases are predefined, group-level templates with limited flexibility and resolution. We present Deep Cluster Atlas (DCA), a graph-guided deep embedding clustering framework for generating individualized, voxel-wise brain parcellations. DCA combines a pretrained autoencoder with spatially regularized deep clustering to produce functionally coherent and spatially contiguous regions. Our method supports flexible control over resolution and anatomical scope, and generalizes to arbitrary brain structures. We further introduce a standardized benchmarking platform for atlas evaluation, using multiple large-scale fMRI datasets. Across multiple datasets and scales, DCA outperforms state-of-the-art atlases, improving functional homogeneity by 98.8\% and silhouette coefficient by 29\%, and achieves superior performance in downstream tasks such as autism diagnosis and cognitive decoding. Codes and models will be released soon.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unnoticeable Community Deception via Multi-objective Optimization</title>
<link>https://arxiv.org/abs/2509.01438</link>
<guid>https://arxiv.org/abs/2509.01438</guid>
<content:encoded><![CDATA[
arXiv:2509.01438v1 Announce Type: cross 
Abstract: Community detection in graphs is crucial for understanding the organization of nodes into densely connected clusters. While numerous strategies have been developed to identify these clusters, the success of community detection can lead to privacy and information security concerns, as individuals may not want their personal information exposed. To address this, community deception methods have been proposed to reduce the effectiveness of detection algorithms. Nevertheless, several limitations, such as the rationality of evaluation metrics and the unnoticeability of attacks, have been ignored in current deception methods. Therefore, in this work, we first investigate the limitations of the widely used deception metric, i.e., the decrease of modularity, through empirical studies. Then, we propose a new deception metric, and combine this new metric together with the attack budget to model the unnoticeable community deception task as a multi-objective optimization problem. To further improve the deception performance, we propose two variant methods by incorporating the degree-biased and community-biased candidate node selection mechanisms. Extensive experiments on three benchmark datasets demonstrate the superiority of the proposed community deception strategies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoccerHigh: A Benchmark Dataset for Automatic Soccer Video Summarization</title>
<link>https://arxiv.org/abs/2509.01439</link>
<guid>https://arxiv.org/abs/2509.01439</guid>
<content:encoded><![CDATA[
arXiv:2509.01439v1 Announce Type: cross 
Abstract: Video summarization aims to extract key shots from longer videos to produce concise and informative summaries. One of its most common applications is in sports, where highlight reels capture the most important moments of a game, along with notable reactions and specific contextual events. Automatic summary generation can support video editors in the sports media industry by reducing the time and effort required to identify key segments. However, the lack of publicly available datasets poses a challenge in developing robust models for sports highlight generation. In this paper, we address this gap by introducing a curated dataset for soccer video summarization, designed to serve as a benchmark for the task. The dataset includes shot boundaries for 237 matches from the Spanish, French, and Italian leagues, using broadcast footage sourced from the SoccerNet dataset. Alongside the dataset, we propose a baseline model specifically designed for this task, which achieves an F1 score of 0.3956 in the test set. Furthermore, we propose a new metric constrained by the length of each target summary, enabling a more objective evaluation of the generated content. The dataset and code are available at https://ipcv.github.io/SoccerHigh/.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Retrieval Augmented Language Models Know When They Don't Know?</title>
<link>https://arxiv.org/abs/2509.01476</link>
<guid>https://arxiv.org/abs/2509.01476</guid>
<content:encoded><![CDATA[
arXiv:2509.01476v1 Announce Type: cross 
Abstract: Existing Large Language Models (LLMs) occasionally generate plausible yet factually incorrect responses, known as hallucinations. Researchers are primarily using two approaches to mitigate hallucinations, namely Retrieval Augmented Language Models (RALMs) and refusal post-training. However, current research predominantly emphasizes their individual effectiveness while overlooking the evaluation of the refusal capability of RALMs. In this study, we ask the fundamental question: Do RALMs know when they don't know? Specifically, we ask three questions. First, are RALMs well-calibrated regarding different internal and external knowledge states? We examine the influence of various factors. Contrary to expectations, we find that LLMs exhibit significant \textbf{over-refusal} behavior. Then, how does refusal post-training affect the over-refusal issue? We investigate the Refusal-aware Instruction Tuning and In-Context Fine-tuning methods. Our results show that the over-refusal problem is mitigated by In-context fine-tuning. but magnified by R-tuning. However, we also find that the refusal ability may conflict with the quality of the answer. Finally, we develop a simple yet effective refusal method for refusal post-trained models to improve their overall answer quality in terms of refusal and correct answers. Our study provides a more comprehensive understanding of the influence of important factors on RALM systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Information-Flow Perspective on Explainability Requirements: Specification and Verification</title>
<link>https://arxiv.org/abs/2509.01479</link>
<guid>https://arxiv.org/abs/2509.01479</guid>
<content:encoded><![CDATA[
arXiv:2509.01479v1 Announce Type: cross 
Abstract: Explainable systems expose information about why certain observed effects are happening to the agents interacting with them. We argue that this constitutes a positive flow of information that needs to be specified, verified, and balanced against negative information flow that may, e.g., violate privacy guarantees. Since both explainability and privacy require reasoning about knowledge, we tackle these tasks with epistemic temporal logic extended with quantification over counterfactual causes. This allows us to specify that a multi-agent system exposes enough information such that agents acquire knowledge on why some effect occurred. We show how this principle can be used to specify explainability as a system-level requirement and provide an algorithm for checking finite-state models against such specifications. We present a prototype implementation of the algorithm and evaluate it on several benchmarks, illustrating how our approach distinguishes between explainable and unexplainable systems, and how it allows to pose additional privacy requirements.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSA2-Net: Utilizing Self-Adaptive Convolution Module to Extract Multi-Scale Information in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.01498</link>
<guid>https://arxiv.org/abs/2509.01498</guid>
<content:encoded><![CDATA[
arXiv:2509.01498v1 Announce Type: cross 
Abstract: The nnUNet segmentation framework adeptly adjusts most hyperparameters in training scripts automatically, but it overlooks the tuning of internal hyperparameters within the segmentation network itself, which constrains the model's ability to generalize. Addressing this limitation, this study presents a novel Self-Adaptive Convolution Module that dynamically adjusts the size of the convolution kernels depending on the unique fingerprints of different datasets. This adjustment enables the MSA2-Net, when equipped with this module, to proficiently capture both global and local features within the feature maps. Self-Adaptive Convolution Module is strategically integrated into two key components of the MSA2-Net: the Multi-Scale Convolution Bridge and the Multi-Scale Amalgamation Decoder. In the MSConvBridge, the module enhances the ability to refine outputs from various stages of the CSWin Transformer during the skip connections, effectively eliminating redundant data that could potentially impair the decoder's performance. Simultaneously, the MSADecoder, utilizing the module, excels in capturing detailed information of organs varying in size during the decoding phase. This capability ensures that the decoder's output closely reproduces the intricate details within the feature maps, thus yielding highly accurate segmentation images. MSA2-Net, bolstered by this advanced architecture, has demonstrated exceptional performance, achieving Dice coefficient scores of 86.49\%, 92.56\%, 93.37\%, and 92.98\% on the Synapse, ACDC, Kvasir, and Skin Lesion Segmentation (ISIC2017) datasets, respectively. This underscores MSA2-Net's robustness and precision in medical image segmentation tasks across various datasets.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Identification and Replay-based Detection (UIRD) for New Category Anomaly Detection in ECG Signal</title>
<link>https://arxiv.org/abs/2509.01512</link>
<guid>https://arxiv.org/abs/2509.01512</guid>
<content:encoded><![CDATA[
arXiv:2509.01512v1 Announce Type: cross 
Abstract: In clinical practice, automatic analysis of electrocardiogram (ECG) is widely applied to identify irregular heart rhythms and other electrical anomalies of the heart, enabling timely intervention and potentially improving clinical outcomes. However, due to the limited samples in certain types of ECG signals, the class imbalance issues pose a challenge for ECG-based detection. In addition, as the volume of patient data grows, long-term storage of all historical data becomes increasingly burdensome as training samples to recognize new patterns and classify existing ECG signals accurately. Therefore, to enhance the performance of anomaly detection while addressing storage limitations, we propose a pseudo-replay based semi-supervised continual learning framework, which consists of two components: unsupervised identification and replay-based detection. For unsupervised identification, an unsupervised generative adversarial network (GAN)-based framework is integrated to detect novel patterns. Besides, instead of directly storing all historical data, a pseudo replay-based learning strategy is proposed which utilizes a generator to learn the data distribution for each individual task. When a new task arises, the generator synthesizes pseudo data representative of previous learnt classes, enabling the model to detect both the existed patterns and the newly presented anomalies. The effectiveness of the proposed framework is validated in four public ECG datasets, which leverages supervised classification problems for anomaly detection. The experimental results show that the developed approach is very promising in identifying novel anomalies while maintaining good performance on detecting existing ECG signals.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeVe: A Modular System for Memory Verification and Effective Context Control in Language Models</title>
<link>https://arxiv.org/abs/2509.01514</link>
<guid>https://arxiv.org/abs/2509.01514</guid>
<content:encoded><![CDATA[
arXiv:2509.01514v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems typically face constraints because of their inherent mechanism: a simple top-k semantic search [1]. The approach often leads to the incorporation of irrelevant or redundant information in the context, degrading performance and efficiency [10][11]. This paper presents MeVe, a novel modular architecture intended for Memory Verification and smart context composition. MeVe rethinks the RAG paradigm by proposing a five-phase modular design that distinctly breaks down the retrieval and context composition process into distinct, auditable, and independently tunable phases: initial retrieval, relevance verification, fallback retrieval, context prioritization, and token budgeting. This architecture enables fine-grained control of what knowledge is made available to an LLM, enabling task-dependent filtering and adaptation. We release a reference implementation of MeVe as a proof of concept and evaluate its performance on knowledge-heavy QA tasks over a subset of English Wikipedia [22]. Our results demonstrate that by actively verifying information before composition, MeVe significantly improves context efficiency, achieving a 57% reduction on the Wikipedia dataset and a 75% reduction on the more complex HotpotQA dataset compared to standard RAG implementations [25]. This work provides a framework for more scalable and reliable LLM applications. By refining and distilling contextual information, MeVe offers a path toward better grounding and more accurate factual support [16].
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Workflow for Education: Concepts and Applications</title>
<link>https://arxiv.org/abs/2509.01517</link>
<guid>https://arxiv.org/abs/2509.01517</guid>
<content:encoded><![CDATA[
arXiv:2509.01517v1 Announce Type: cross 
Abstract: With the rapid advancement of Large Language Models (LLMs) and Artificial Intelligence (AI) agents, agentic workflows are showing transformative potential in education. This study introduces the Agentic Workflow for Education (AWE), a four-component model comprising self-reflection, tool invocation, task planning, and multi-agent collaboration. We distinguish AWE from traditional LLM-based linear interactions and propose a theoretical framework grounded in the von Neumann Multi-Agent System (MAS) architecture. Through a paradigm shift from static prompt-response systems to dynamic, nonlinear workflows, AWE enables scalable, personalized, and collaborative task execution. We further identify four core application domains: integrated learning environments, personalized AI-assisted learning, simulation-based experimentation, and data-driven decision-making. A case study on automated math test generation shows that AWE-generated items are statistically comparable to real exam questions, validating the model's effectiveness. AWE offers a promising path toward reducing teacher workload, enhancing instructional quality, and enabling broader educational innovation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models</title>
<link>https://arxiv.org/abs/2509.01535</link>
<guid>https://arxiv.org/abs/2509.01535</guid>
<content:encoded><![CDATA[
arXiv:2509.01535v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success across various domains. However, a fundamental question remains: Can LLMs effectively utilize causal knowledge for prediction and generation? Through empirical studies, we find that LLMs trained directly on large-scale data often capture spurious correlations rather than true causal relationships, leading to suboptimal performance, especially in out-of-distribution (OOD) scenarios. To address this challenge, we propose Causal Attention Tuning (CAT), a novel approach that injects fine-grained causal knowledge into the attention mechanism. We propose an automated pipeline that leverages human priors to automatically generate token-level causal signals and introduce the Re-Attention mechanism to guide training, helping the model focus on causal structures while mitigating noise and biases in attention scores. Experimental results on our proposed Spurious Token Game (STG) benchmark and multiple downstream tasks demonstrate that our approach effectively leverages causal knowledge for prediction and remains robust in OOD scenarios. Implementation details can be found at https://github.com/Kairong-Han/CAT.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Supervision For Vision-Language Modeling in 3D Computed Tomography</title>
<link>https://arxiv.org/abs/2509.01554</link>
<guid>https://arxiv.org/abs/2509.01554</guid>
<content:encoded><![CDATA[
arXiv:2509.01554v1 Announce Type: cross 
Abstract: General-purpose vision-language models (VLMs) have emerged as promising tools in radiology, offering zero-shot capabilities that mitigate the need for large labeled datasets. However, in high-stakes domains like diagnostic radiology, these models often lack the discriminative precision required for reliable clinical use. This challenge is compounded by the scarcity and heterogeneity of publicly available volumetric CT datasets, which vary widely in annotation formats and granularity. To address these limitations, we introduce Uniferum, a volumetric VLM that unifies diverse supervision signals, encoded in classification labels and segmentation masks, into a single training framework. By harmonizing three public 3D CT datasets with distinct annotations, Uniferum achieves state-of-the-art performance, improving AUROC on the CT-RATE benchmark by 7% compared to CLIP-based and conventional multi-label convolutional models. The model demonstrates robust out-of-distribution generalization, with observed evidence of unexpected zero-shot performance on the RAD-CHEST and INSPECT datasets. Our results highlight the effectiveness of integrating heterogeneous annotations and body segmentation to enhance model performance, setting a new direction for clinically reliable, data-efficient VLMs in 3D medical imaging.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents</title>
<link>https://arxiv.org/abs/2509.01560</link>
<guid>https://arxiv.org/abs/2509.01560</guid>
<content:encoded><![CDATA[
arXiv:2509.01560v1 Announce Type: cross 
Abstract: Tool agents -- LLM-based systems that interact with external APIs -- offer a way to execute real-world tasks. However, as tasks become increasingly complex, these agents struggle to identify and call the correct APIs in the proper order. To tackle this problem, we investigate converting API documentation into a structured API graph that captures API dependencies and leveraging it for multi-tool queries that require compositional API calls. To support this, we introduce In-N-Out, the first expert-annotated dataset of API graphs built from two real-world API benchmarks and their documentation. Using In-N-Out significantly improves performance on both tool retrieval and multi-tool query generation, nearly doubling that of LLMs using documentation alone. Moreover, graphs generated by models fine-tuned on In-N-Out close 90% of this gap, showing that our dataset helps models learn to comprehend API documentation and parameter relationships. Our findings highlight the promise of using explicit API graphs for tool agents and the utility of In-N-Out as a valuable resource. We will release the dataset and code publicly.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Down Syndrome Research through a Knowledge Graph-Driven Analytical Framework</title>
<link>https://arxiv.org/abs/2509.01565</link>
<guid>https://arxiv.org/abs/2509.01565</guid>
<content:encoded><![CDATA[
arXiv:2509.01565v1 Announce Type: cross 
Abstract: Trisomy 21 results in Down syndrome, a multifaceted genetic disorder with diverse clinical phenotypes, including heart defects, immune dysfunction, neurodevelopmental differences, and early-onset dementia risk. Heterogeneity and fragmented data across studies challenge comprehensive research and translational discovery. The NIH INCLUDE (INvestigation of Co-occurring conditions across the Lifespan to Understand Down syndromE) initiative has assembled harmonized participant-level datasets, yet realizing their potential requires integrative analytical frameworks. We developed a knowledge graph-driven platform transforming nine INCLUDE studies, comprising 7,148 participants, 456 conditions, 501 phenotypes, and over 37,000 biospecimens, into a unified semantic infrastructure. Cross-resource enrichment with Monarch Initiative data expands coverage to 4,281 genes and 7,077 variants. The resulting knowledge graph contains over 1.6 million semantic associations, enabling AI-ready analysis with graph embeddings and path-based reasoning for hypothesis generation. Researchers can query the graph via SPARQL or natural language interfaces. This framework converts static data repositories into dynamic discovery environments, supporting cross-study pattern recognition, predictive modeling, and systematic exploration of genotype-phenotype relationships in Down syndrome.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Shot Clustering for Federated Learning Under Clustering-Agnostic Assumption</title>
<link>https://arxiv.org/abs/2509.01587</link>
<guid>https://arxiv.org/abs/2509.01587</guid>
<content:encoded><![CDATA[
arXiv:2509.01587v1 Announce Type: cross 
Abstract: Federated Learning (FL) is a widespread and well-adopted paradigm of decentralised learning that allows training one model from multiple sources without the need to transfer data between participating clients directly. Since its inception in 2015, it has been divided into numerous subfields that deal with application-specific issues, such as data heterogeneity or resource allocation. One such sub-field, Clustered Federated Learning (CFL), deals with the problem of clustering the population of clients into separate cohorts to deliver personalised models. Although a few remarkable works have been published in this domain, the problem remains largely unexplored, as its basic assumptions and settings differ slightly from those of standard FL. In this work, we present One-Shot Clustered Federated Learning (OCFL), a clustering-agnostic algorithm that can automatically detect the earliest suitable moment for clustering. Our algorithm is based on computing the cosine distance between the gradients of the clients and a temperature measure that detects when the federated model starts to converge. We empirically evaluate our methodology by testing various one-shot clustering algorithms for over forty different tasks on five benchmark datasets. Our experiments showcase the good performance of our approach when used to perform CFL in an automated manner without the need to adjust hyperparameters. We also revisit the practical feasibility of CFL algorithms based on the gradients of the clients, providing firm evidence of the high efficiency of density-based clustering methods when used to differentiate between the loss surfaces of neural networks trained on different distributions. Moreover, by inspecting the feasibility of local explanations generated with the help of GradCAM, we can provide more insights into the relationship between personalisation and the explainability of local predictions.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Discord to Harmony: Decomposed Consonance-based Training for Improved Audio Chord Estimation</title>
<link>https://arxiv.org/abs/2509.01588</link>
<guid>https://arxiv.org/abs/2509.01588</guid>
<content:encoded><![CDATA[
arXiv:2509.01588v1 Announce Type: cross 
Abstract: Audio Chord Estimation (ACE) holds a pivotal role in music information research, having garnered attention for over two decades due to its relevance for music transcription and analysis. Despite notable advancements, challenges persist in the task, particularly concerning unique characteristics of harmonic content, which have resulted in existing systems' performances reaching a glass ceiling. These challenges include annotator subjectivity, where varying interpretations among annotators lead to inconsistencies, and class imbalance within chord datasets, where certain chord classes are over-represented compared to others, posing difficulties in model training and evaluation. As a first contribution, this paper presents an evaluation of inter-annotator agreement in chord annotations, using metrics that extend beyond traditional binary measures. In addition, we propose a consonance-informed distance metric that reflects the perceptual similarity between harmonic annotations. Our analysis suggests that consonance-based distance metrics more effectively capture musically meaningful agreement between annotations. Expanding on these findings, we introduce a novel ACE conformer-based model that integrates consonance concepts into the model through consonance-based label smoothing. The proposed model also addresses class imbalance by separately estimating root, bass, and all note activations, enabling the reconstruction of chord labels from decomposed outputs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing Radiation Detection Systems with an Efficient TinyML-Based IDS for Edge Devices</title>
<link>https://arxiv.org/abs/2509.01592</link>
<guid>https://arxiv.org/abs/2509.01592</guid>
<content:encoded><![CDATA[
arXiv:2509.01592v1 Announce Type: cross 
Abstract: Radiation Detection Systems (RDSs) play a vital role in ensuring public safety across various settings, from nuclear facilities to medical environments. However, these systems are increasingly vulnerable to cyber-attacks such as data injection, man-in-the-middle (MITM) attacks, ICMP floods, botnet attacks, privilege escalation, and distributed denial-of-service (DDoS) attacks. Such threats could compromise the integrity and reliability of radiation measurements, posing significant public health and safety risks. This paper presents a new synthetic radiation dataset and an Intrusion Detection System (IDS) tailored for resource-constrained environments, bringing Machine Learning (ML) predictive capabilities closer to the sensing edge layer of critical infrastructure. Leveraging TinyML techniques, the proposed IDS employs an optimized XGBoost model enhanced with pruning, quantization, feature selection, and sampling. These TinyML techniques significantly reduce the size of the model and computational demands, enabling real-time intrusion detection on low-resource devices while maintaining a reasonable balance between efficiency and accuracy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>O-DisCo-Edit: Object Distortion Control for Unified Realistic Video Editing</title>
<link>https://arxiv.org/abs/2509.01596</link>
<guid>https://arxiv.org/abs/2509.01596</guid>
<content:encoded><![CDATA[
arXiv:2509.01596v1 Announce Type: cross 
Abstract: Diffusion models have recently advanced video editing, yet controllable editing remains challenging due to the need for precise manipulation of diverse object properties. Current methods require different control signal for diverse editing tasks, which complicates model design and demands significant training resources. To address this, we propose O-DisCo-Edit, a unified framework that incorporates a novel object distortion control (O-DisCo). This signal, based on random and adaptive noise, flexibly encapsulates a wide range of editing cues within a single representation. Paired with a "copy-form" preservation module for preserving non-edited regions, O-DisCo-Edit enables efficient, high-fidelity editing through an effective training paradigm. Extensive experiments and comprehensive human evaluations consistently demonstrate that O-DisCo-Edit surpasses both specialized and multitask state-of-the-art methods across various video editing tasks. https://cyqii.github.io/O-DisCo-Edit.github.io/
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Intrusion Detection System for Safeguarding Radiation Detection Systems</title>
<link>https://arxiv.org/abs/2509.01599</link>
<guid>https://arxiv.org/abs/2509.01599</guid>
<content:encoded><![CDATA[
arXiv:2509.01599v1 Announce Type: cross 
Abstract: Radiation Detection Systems (RDSs) are used to measure and detect abnormal levels of radioactive material in the environment. These systems are used in many applications to mitigate threats posed by high levels of radioactive material. However, these systems lack protection against malicious external attacks to modify the data. The novelty of applying Intrusion Detection Systems (IDS) in RDSs is a crucial element in safeguarding these critical infrastructures. While IDSs are widely used in networking environments to safeguard against various attacks, their application in RDSs is novel. A common attack on RDSs is Denial of Service (DoS), where the attacker aims to overwhelm the system, causing malfunctioning RDSs. This paper proposes an efficient Machine Learning (ML)-based IDS to detect anomalies in radiation data, focusing on DoS attacks. This work explores the use of sampling methods to create a simulated DoS attack based on a real radiation dataset, followed by an evaluation of various ML algorithms, including Random Forest, Support Vector Machine (SVM), logistic regression, and Light Gradient-Boosting Machine (LightGBM), to detect DoS attacks on RDSs. LightGBM is emphasized for its superior accuracy and low computational resource consumption, making it particularly suitable for real-time intrusion detection. Additionally, model optimization and TinyML techniques, including feature selection, parallel execution, and random search methods, are used to improve the efficiency of the proposed IDS. Finally, an optimized and efficient LightGBM-based IDS is developed to achieve accurate intrusion detection for RDSs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D Force Estimation in Catheterization</title>
<link>https://arxiv.org/abs/2509.01605</link>
<guid>https://arxiv.org/abs/2509.01605</guid>
<content:encoded><![CDATA[
arXiv:2509.01605v1 Announce Type: cross 
Abstract: Recently, the emergence of multitask deep learning models has enhanced catheterization procedures by providing tactile and visual perception data through an end-to-end architec- ture. This information is derived from a segmentation and force estimation head, which localizes the catheter in X-ray images and estimates the applied pressure based on its deflection within the image. These stereo vision architectures incorporate a CNN- based encoder-decoder that captures the dependencies between X-ray images from two viewpoints, enabling simultaneous 3D force estimation and stereo segmentation of the catheter. With these tasks in mind, this work approaches the problem from a new perspective. We propose a novel encoder-decoder Vision Transformer model that processes two input X-ray images as separate sequences. Given sequences of X-ray patches from two perspectives, the transformer captures long-range dependencies without the need to gradually expand the receptive field for either image. The embeddings generated by both the encoder and decoder are fed into two shared segmentation heads, while a regression head employs the fused information from the decoder for 3D force estimation. The proposed model is a stereo Vision Transformer capable of simultaneously segmenting the catheter from two angles while estimating the generated forces at its tip in 3D. This model has undergone extensive experiments on synthetic X-ray images with various noise levels and has been compared against state-of-the-art pure segmentation models, vision-based catheter force estimation methods, and a multitask catheter segmentation and force estimation approach. It outperforms existing models, setting a new state-of-the-art in both catheter segmentation and force estimation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Driven Curriculum for Multi-Task Training in Human Mobility Prediction</title>
<link>https://arxiv.org/abs/2509.01613</link>
<guid>https://arxiv.org/abs/2509.01613</guid>
<content:encoded><![CDATA[
arXiv:2509.01613v1 Announce Type: cross 
Abstract: The increasing availability of big mobility data from ubiquitous portable devices enables human mobility prediction through deep learning approaches. However, the diverse complexity of human mobility data impedes model training, leading to inefficient gradient updates and potential underfitting. Meanwhile, exclusively predicting next locations neglects implicit determinants, including distances and directions, thereby yielding suboptimal prediction results. This paper presents a unified training framework that integrates entropy-driven curriculum and multi-task learning to address these challenges. The proposed entropy-driven curriculum learning strategy quantifies trajectory predictability based on Lempel-Ziv compression and organizes training from simple to complex for faster convergence and enhanced performance. The multi-task training simultaneously optimizes the primary location prediction alongside auxiliary estimation of movement distance and direction for learning realistic mobility patterns, and improve prediction accuracy through complementary supervision signals. Extensive experiments conducted in accordance with the HuMob Challenge demonstrate that our approach achieves state-of-the-art performance on GEO-BLEU (0.354) and DTW (26.15) metrics with up to 2.92-fold convergence speed compared to training without curriculum learning.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling the schema turn: Restoring the information base to conceptual modelling</title>
<link>https://arxiv.org/abs/2509.01617</link>
<guid>https://arxiv.org/abs/2509.01617</guid>
<content:encoded><![CDATA[
arXiv:2509.01617v1 Announce Type: cross 
Abstract: If one looks at contemporary mainstream development practices for conceptual modelling in computer science, these so clearly focus on a conceptual schema completely separated from its information base that the conceptual schema is often just called the conceptual model. These schema-centric practices are crystallized in almost every database textbook. We call this strong, almost universal, bias towards conceptual schemas the schema turn. The focus of this paper is on disentangling this turn within (computer science) conceptual modeling. It aims to shed some light on how it emerged and so show that it is not fundamental. To show that modern technology enables the adoption of an inclusive schema-and-base conceptual modelling approach, which in turn enables more automated, and empirically motivated practices. And to show, more generally, the space of possible conceptual modelling practices is wider than currently assumed. It also uses the example of bCLEARer to show that the implementations in this wider space will probably need to rely on new pipeline-based conceptual modelling techniques. So, it is possible that the schema turn's complete exclusion of the information base could be merely a temporary evolutionary detour.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry</title>
<link>https://arxiv.org/abs/2509.01620</link>
<guid>https://arxiv.org/abs/2509.01620</guid>
<content:encoded><![CDATA[
arXiv:2509.01620v1 Announce Type: cross 
Abstract: The rapid development of advanced large language models (LLMs) has made AI-generated text indistinguishable from human-written text. Previous work on detecting AI-generated text has made effective progress, but has not involved modern Chinese poetry. Due to the distinctive characteristics of modern Chinese poetry, it is difficult to identify whether a poem originated from humans or AI. The proliferation of AI-generated modern Chinese poetry has significantly disrupted the poetry ecosystem. Based on the urgency of identifying AI-generated poetry in the real Chinese world, this paper proposes a novel benchmark for detecting LLMs-generated modern Chinese poetry. We first construct a high-quality dataset, which includes both 800 poems written by six professional poets and 41,600 poems generated by four mainstream LLMs. Subsequently, we conduct systematic performance assessments of six detectors on this dataset. Experimental results demonstrate that current detectors cannot be used as reliable tools to detect modern Chinese poems generated by LLMs. The most difficult poetic features to detect are intrinsic qualities, especially style. The detection results verify the effectiveness and necessity of our proposed benchmark. Our work lays a foundation for future detection of AI-generated poetry.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling</title>
<link>https://arxiv.org/abs/2509.01624</link>
<guid>https://arxiv.org/abs/2509.01624</guid>
<content:encoded><![CDATA[
arXiv:2509.01624v1 Announce Type: cross 
Abstract: Text-to-image diffusion models are computationally intensive, often requiring dozens of forward passes through large transformer backbones. For instance, Stable Diffusion XL generates high-quality images with 50 evaluations of a 2.6B-parameter model, an expensive process even for a single batch. Few-step diffusion models reduce this cost to 2-8 denoising steps but still depend on large, uncompressed U-Net or diffusion transformer backbones, which are often too costly for full-precision inference without datacenter GPUs. These requirements also limit existing post-training quantization methods that rely on full-precision calibration. We introduce Q-Sched, a new paradigm for post-training quantization that modifies the diffusion model scheduler rather than model weights. By adjusting the few-step sampling trajectory, Q-Sched achieves full-precision accuracy with a 4x reduction in model size. To learn quantization-aware pre-conditioning coefficients, we propose the JAQ loss, which combines text-image compatibility with an image quality metric for fine-grained optimization. JAQ is reference-free and requires only a handful of calibration prompts, avoiding full-precision inference during calibration. Q-Sched delivers substantial gains: a 15.5% FID improvement over the FP16 4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step Phased Consistency Model, showing that quantization and few-step distillation are complementary for high-fidelity generation. A large-scale user study with more than 80,000 annotations further confirms Q-Sched's effectiveness on both FLUX.1[schnell] and SDXL-Turbo.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Identical Diffusion Models in MIMO-OFDM Channel Generation</title>
<link>https://arxiv.org/abs/2509.01641</link>
<guid>https://arxiv.org/abs/2509.01641</guid>
<content:encoded><![CDATA[
arXiv:2509.01641v1 Announce Type: cross 
Abstract: We propose a novel diffusion model, termed the non-identical diffusion model, and investigate its application to wireless orthogonal frequency division multiplexing (OFDM) channel generation. Unlike the standard diffusion model that uses a scalar-valued time index to represent the global noise level, we extend this notion to an element-wise time indicator to capture local error variations more accurately. Non-identical diffusion enables us to characterize the reliability of each element (e.g., subcarriers in OFDM) within the noisy input, leading to improved generation results when the initialization is biased. Specifically, we focus on the recovery of wireless multi-input multi-output (MIMO) OFDM channel matrices, where the initial channel estimates exhibit highly uneven reliability across elements due to the pilot scheme. Conventional time embeddings, which assume uniform noise progression, fail to capture such variability across pilot schemes and noise levels. We introduce a matrix that matches the input size to control element-wise noise progression. Following a similar diffusion procedure to existing methods, we show the correctness and effectiveness of the proposed non-identical diffusion scheme both theoretically and numerically. For MIMO-OFDM channel generation, we propose a dimension-wise time embedding strategy. We also develop and evaluate multiple training and generation methods and compare them through numerical experiments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Retrieval with Importance Weights for Few-Shot Imitation Learning</title>
<link>https://arxiv.org/abs/2509.01657</link>
<guid>https://arxiv.org/abs/2509.01657</guid>
<content:encoded><![CDATA[
arXiv:2509.01657v1 Announce Type: cross 
Abstract: While large-scale robot datasets have propelled recent progress in imitation learning, learning from smaller task specific datasets remains critical for deployment in new environments and unseen tasks. One such approach to few-shot imitation learning is retrieval-based imitation learning, which extracts relevant samples from large, widely available prior datasets to augment a limited demonstration dataset. To determine the relevant data from prior datasets, retrieval-based approaches most commonly calculate a prior data point's minimum distance to a point in the target dataset in latent space. While retrieval-based methods have shown success using this metric for data selection, we demonstrate its equivalence to the limit of a Gaussian kernel density (KDE) estimate of the target data distribution. This reveals two shortcomings of the retrieval rule used in prior work. First, it relies on high-variance nearest neighbor estimates that are susceptible to noise. Second, it does not account for the distribution of prior data when retrieving data. To address these issues, we introduce Importance Weighted Retrieval (IWR), which estimates importance weights, or the ratio between the target and prior data distributions for retrieval, using Gaussian KDEs. By considering the probability ratio, IWR seeks to mitigate the bias of previous selection rules, and by using reasonable modeling parameters, IWR effectively smooths estimates using all data points. Across both simulation environments and real-world evaluations on the Bridge dataset we find that our method, IWR, consistently improves performance of existing retrieval-based methods, despite only requiring minor modifications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Machine Learning Engineering Agents</title>
<link>https://arxiv.org/abs/2509.01684</link>
<guid>https://arxiv.org/abs/2509.01684</guid>
<content:encoded><![CDATA[
arXiv:2509.01684v1 Announce Type: cross 
Abstract: Existing agents for solving tasks such as ML engineering rely on prompting powerful language models. As a result, these agents do not improve with more experience. In this paper, we show that agents backed by weaker models that improve via reinforcement learning (RL) can outperform agents backed by much larger, but static models. We identify two major challenges with RL in this setting. First, actions can take a variable amount of time (e.g., executing code for different solutions), which leads to asynchronous policy gradient updates that favor faster but suboptimal solutions. To tackle variable-duration actions, we propose duration- aware gradient updates in a distributed asynchronous RL framework to amplify high-cost but high-reward actions. Second, using only test split performance as a reward provides limited feedback. A program that is nearly correct is treated the same as one that fails entirely. To address this, we propose environment instrumentation to offer partial credit, distinguishing almost-correct programs from those that fail early (e.g., during data loading). Environment instrumentation uses a separate static language model to insert print statement to an existing program to log the agent's experimental progress, from which partial credit can be extracted as reward signals for learning. Our experimental results on MLEBench suggest that performing gradient updates on a much smaller model (Qwen2.5-3B) trained with RL outperforms prompting a much larger model (Claude-3.5-Sonnet) with agent scaffolds, by an average of 22% across 12 Kaggle tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Rock Particulate Classification Using Attention-Enhanced ConvNeXt</title>
<link>https://arxiv.org/abs/2509.01704</link>
<guid>https://arxiv.org/abs/2509.01704</guid>
<content:encoded><![CDATA[
arXiv:2509.01704v1 Announce Type: cross 
Abstract: Accurate classification of rock sizes is a vital component in geotechnical engineering, mining, and resource management, where precise estimation influences operational efficiency and safety. In this paper, we propose an enhanced deep learning model based on the ConvNeXt architecture, augmented with both self-attention and channel attention mechanisms. Building upon the foundation of ConvNext, our proposed model, termed CNSCA, introduces self-attention to capture long-range spatial dependencies and channel attention to emphasize informative feature channels. This hybrid design enables the model to effectively capture both fine-grained local patterns and broader contextual relationships within rock imagery, leading to improved classification accuracy and robustness. We evaluate our model on a rock size classification dataset and compare it against three strong baseline. The results demonstrate that the incorporation of attention mechanisms significantly enhances the models capability for fine-grained classification tasks involving natural textures like rocks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>chDzDT: Word-level morphology-aware language model for Algerian social media text</title>
<link>https://arxiv.org/abs/2509.01772</link>
<guid>https://arxiv.org/abs/2509.01772</guid>
<content:encoded><![CDATA[
arXiv:2509.01772v1 Announce Type: cross 
Abstract: Pre-trained language models (PLMs) have substantially advanced natural language processing by providing context-sensitive text representations. However, the Algerian dialect remains under-represented, with few dedicated models available. Processing this dialect is challenging due to its complex morphology, frequent code-switching, multiple scripts, and strong lexical influences from other languages. These characteristics complicate tokenization and reduce the effectiveness of conventional word- or subword-level approaches.
  To address this gap, we introduce chDzDT, a character-level pre-trained language model tailored for Algerian morphology. Unlike conventional PLMs that rely on token sequences, chDzDT is trained on isolated words. This design allows the model to encode morphological patterns robustly, without depending on token boundaries or standardized orthography. The training corpus draws from diverse sources, including YouTube comments, French, English, and Berber Wikipedia, as well as the Tatoeba project. It covers multiple scripts and linguistic varieties, resulting in a substantial pre-training workload.
  Our contributions are threefold: (i) a detailed morphological analysis of Algerian dialect using YouTube comments; (ii) the construction of a multilingual Algerian lexicon dataset; and (iii) the development and extensive evaluation of a character-level PLM as a morphology-focused encoder for downstream tasks. The proposed approach demonstrates the potential of character-level modeling for morphologically rich, low-resource dialects and lays a foundation for more inclusive and adaptable NLP systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AHAMask: Reliable Task Specification for Large Audio Language Models without Instructions</title>
<link>https://arxiv.org/abs/2509.01787</link>
<guid>https://arxiv.org/abs/2509.01787</guid>
<content:encoded><![CDATA[
arXiv:2509.01787v1 Announce Type: cross 
Abstract: Although current large audio language models (LALMs) extend text large language models (LLMs) with generic acoustic understanding abilities, they usually suffer from instruction sensitivity, where different instructions of the same intention can yield drastically different outcomes. In this work, we propose AHAMask, where we simply mask some of the attention heads in the decoder-only LLM backbone of LALMs, to trigger specific acoustic task functionalities without instructions. These masks are efficiently obtained by training on an LALM, with the number of trainable parameters equal to the attention head count in its LLM backbone. We show by experiments that applying such selective attention head masks achieves comparable or even better performance than using instructions, either on single or composite tasks. Besides achieving reliable acoustic task specification for LALMs, this also reveals that LALMs exhibit certain "functional pathways" in their attention heads.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs</title>
<link>https://arxiv.org/abs/2509.01790</link>
<guid>https://arxiv.org/abs/2509.01790</guid>
<content:encoded><![CDATA[
arXiv:2509.01790v1 Announce Type: cross 
Abstract: Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e., repeating something written or spoken using different words) leads to significant changes in large language model (LLM) performance, has been widely accepted as a core limitation of LLMs. In this work, we revisit this issue and ask: Is the widely reported high prompt sensitivity truly an inherent weakness of LLMs, or is it largely an artifact of evaluation processes? To answer this question, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family) across 6 benchmarks, including both multiple-choice and open-ended tasks on 12 diverse prompt templates. We find that much of the prompt sensitivity stems from heuristic evaluation methods, including log-likelihood scoring and rigid answer matching, which often overlook semantically correct responses expressed through alternative phrasings, such as synonyms or paraphrases. When we adopt LLM-as-a-Judge evaluations, we observe a substantial reduction in performance variance and a consistently higher correlation in model rankings across prompts. Our findings suggest that modern LLMs are more robust to prompt templates than previously believed, and that prompt sensitivity may be more an artifact of evaluation than a flaw in the models.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-PhishGen: Unlocking Novel Research in Phishing Email Detection</title>
<link>https://arxiv.org/abs/2509.01791</link>
<guid>https://arxiv.org/abs/2509.01791</guid>
<content:encoded><![CDATA[
arXiv:2509.01791v1 Announce Type: cross 
Abstract: Every day, our inboxes are flooded with unsolicited emails, ranging between annoying spam to more subtle phishing scams. Unfortunately, despite abundant prior efforts proposing solutions achieving near-perfect accuracy, the reality is that countering malicious emails still remains an unsolved dilemma.
  This "open problem" paper carries out a critical assessment of scientific works in the context of phishing email detection. First, we focus on the benchmark datasets that have been used to assess the methods proposed in research. We find that most prior work relied on datasets containing emails that -- we argue -- are not representative of current trends, and mostly encompass the English language. Based on this finding, we then re-implement and re-assess a variety of detection methods reliant on machine learning (ML), including large-language models (LLM), and release all of our codebase -- an (unfortunately) uncommon practice in related research. We show that most such methods achieve near-perfect performance when trained and tested on the same dataset -- a result which intrinsically hinders development (how can future research outperform methods that are already near perfect?). To foster the creation of "more challenging benchmarks" that reflect current phishing trends, we propose E-PhishGEN, an LLM-based (and privacy-savvy) framework to generate novel phishing-email datasets. We use our E-PhishGEN to create E-PhishLLM, a novel phishing-email detection dataset containing 16616 emails in three languages. We use E-PhishLLM to test the detectors we considered, showing a much lower performance than that achieved on existing benchmarks -- indicating a larger room for improvement. We also validate the quality of E-PhishLLM with a user study (n=30). To sum up, we show that phishing email detection is still an open problem -- and provide the means to tackle such a problem by future research.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Unified Benchmark and Taxonomy of Stochastic Environments</title>
<link>https://arxiv.org/abs/2509.01793</link>
<guid>https://arxiv.org/abs/2509.01793</guid>
<content:encoded><![CDATA[
arXiv:2509.01793v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) agents have achieved strong results on benchmarks such as Atari100k, yet they remain limited in robustness to real-world conditions. Model-Based RL approaches that rely on learned World Models often struggle in environments with true stochasticity and partial observability, despite their theoretical grounding in POMDPs. Current benchmarks rarely capture these challenges, focusing instead on deterministic or overly simplified settings, and the lack of a clear taxonomy of stochasticity further hampers systematic evaluation. To address this gap, we introduce STORI (STOchastic-ataRI), a benchmark that incorporates diverse stochastic effects and enables rigorous assessment of RL methods under varied forms of uncertainty. In addition, we propose a taxonomy of stochasticity in RL environments, providing a unified framework for analyzing and comparing approaches.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics</title>
<link>https://arxiv.org/abs/2509.01794</link>
<guid>https://arxiv.org/abs/2509.01794</guid>
<content:encoded><![CDATA[
arXiv:2509.01794v1 Announce Type: cross 
Abstract: The COVID-19 pandemic disrupted healthcare systems worldwide, disproportionately impacting individuals with chronic conditions such as cardiovascular disease (CVD). These disruptions -- through delayed care and behavioral changes, affected key CVD biomarkers, including LDL cholesterol (LDL-C), HbA1c, BMI, and systolic blood pressure (SysBP). Accurate modeling of these changes is crucial for predicting disease progression and guiding preventive care. However, prior work has not addressed multi-target prediction of CVD biomarker from Electronic Health Records (EHRs) using machine learning (ML), while jointly capturing biomarker interdependencies, temporal patterns, and predictive uncertainty. In this paper, we propose MBT-CB, a Multi-target Bayesian Transformer (MBT) with pre-trained BERT-based transformer framework to jointly predict LDL-C, HbA1c, BMI and SysBP CVD biomarkers from EHR data. The model leverages Bayesian Variational Inference to estimate uncertainties, embeddings to capture temporal relationships and a DeepMTR model to capture biomarker inter-relationships. We evaluate MBT-CT on retrospective EHR data from 3,390 CVD patient records (304 unique patients) in Central Massachusetts during the Covid-19 pandemic. MBT-CB outperformed a comprehensive set of baselines including other BERT-based ML models, achieving an MAE of 0.00887, RMSE of 0.0135 and MSE of 0.00027, while effectively capturing data and model uncertainty, patient biomarker inter-relationships, and temporal dynamics via its attention and embedding mechanisms. MBT-CB's superior performance highlights its potential to improve CVD biomarker prediction and support clinical decision-making during pandemics.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Machine Learning for UAV Swarm Intrusion Detection</title>
<link>https://arxiv.org/abs/2509.01812</link>
<guid>https://arxiv.org/abs/2509.01812</guid>
<content:encoded><![CDATA[
arXiv:2509.01812v1 Announce Type: cross 
Abstract: Intrusion detection in unmanned-aerial-vehicle (UAV) swarms is complicated by high mobility, non-stationary traffic, and severe class imbalance. Leveraging a 120 k-flow simulation corpus that covers five attack types, we benchmark three quantum-machine-learning (QML) approaches - quantum kernels, variational quantum neural networks (QNNs), and hybrid quantum-trained neural networks (QT-NNs) - against strong classical baselines. All models consume an 8-feature flow representation and are evaluated under identical preprocessing, balancing, and noise-model assumptions. We analyse the influence of encoding strategy, circuit depth, qubit count, and shot noise, reporting accuracy, macro-F1, ROC-AUC, Matthews correlation, and quantum-resource footprints. Results reveal clear trade-offs: quantum kernels and QT-NNs excel in low-data, nonlinear regimes, while deeper QNNs suffer from trainability issues, and CNNs dominate when abundant data offset their larger parameter count. The complete codebase and dataset partitions are publicly released to enable reproducible QML research in network security.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative &amp; Qualitative Research Contexts</title>
<link>https://arxiv.org/abs/2509.01814</link>
<guid>https://arxiv.org/abs/2509.01814</guid>
<content:encoded><![CDATA[
arXiv:2509.01814v1 Announce Type: cross 
Abstract: Transformer-based Large Language Models (LLMs) have paved the way for "AI interviewers" that can administer voice-based surveys with respondents in real-time. This position paper reviews emerging evidence to understand when such AI interviewing systems are fit for purpose for collecting data within quantitative and qualitative research contexts. We evaluate the capabilities of AI interviewers as well as current Interactive Voice Response (IVR) systems across two dimensions: input/output performance (i.e., speech recognition, answer recording, emotion handling) and verbal reasoning (i.e., ability to probe, clarify, and handle branching logic). Field studies suggest that AI interviewers already exceed IVR capabilities for both quantitative and qualitative data collection, but real-time transcription error rates, limited emotion detection abilities, and uneven follow-up quality indicate that the utility, use and adoption of current AI interviewer technology may be context-dependent for qualitative data collection efforts.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When LLM Meets Time Series: Can LLMs Perform Multi-Step Time Series Reasoning and Inference</title>
<link>https://arxiv.org/abs/2509.01822</link>
<guid>https://arxiv.org/abs/2509.01822</guid>
<content:encoded><![CDATA[
arXiv:2509.01822v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has sparked growing interest in their application to time series analysis tasks. However, their ability to perform complex reasoning over temporal data in real-world application domains remains underexplored. To move toward this goal, a first step is to establish a rigorous benchmark dataset for evaluation. In this work, we introduce the TSAIA Benchmark, a first attempt to evaluate LLMs as time-series AI assistants. To ensure both scientific rigor and practical relevance, we surveyed over 20 academic publications and identified 33 real-world task formulations. The benchmark encompasses a broad spectrum of challenges, ranging from constraint-aware forecasting to anomaly detection with threshold calibration: tasks that require compositional reasoning and multi-step time series analysis. The question generator is designed to be dynamic and extensible, supporting continuous expansion as new datasets or task types are introduced. Given the heterogeneous nature of the tasks, we adopt task-specific success criteria and tailored inference-quality metrics to ensure meaningful evaluation for each task. We apply this benchmark to assess eight state-of-the-art LLMs under a unified evaluation protocol. Our analysis reveals limitations in current models' ability to assemble complex time series analysis workflows, underscoring the need for specialized methodologies for domain-specific adaptation. Our benchmark is available at https://huggingface.co/datasets/Melady/TSAIA, and the code is available at https://github.com/USC-Melady/TSAIA.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Journalists' Perceptions of Artificial Intelligence and Disinformation Risks</title>
<link>https://arxiv.org/abs/2509.01824</link>
<guid>https://arxiv.org/abs/2509.01824</guid>
<content:encoded><![CDATA[
arXiv:2509.01824v1 Announce Type: cross 
Abstract: This study examines journalists' perceptions of the impact of artificial intelligence (AI) on disinformation, a growing concern in journalism due to the rapid expansion of generative AI and its influence on news production and media organizations. Using a quantitative approach, a structured survey was administered to 504 journalists in the Basque Country, identified through official media directories and with the support of the Basque Association of Journalists. This survey, conducted online and via telephone between May and June 2024, included questions on sociodemographic and professional variables, as well as attitudes toward AI's impact on journalism. The results indicate that a large majority of journalists (89.88%) believe AI will considerably or significantly increase the risks of disinformation, and this perception is consistent across genders and media types, but more pronounced among those with greater professional experience. Statistical analyses reveal a significant association between years of experience and perceived risk, and between AI use and risk perception. The main risks identified are the difficulty in detecting false content and deepfakes, and the risk of obtaining inaccurate or erroneous data. Co-occurrence analysis shows that these risks are often perceived as interconnected. These findings highlight the complex and multifaceted concerns of journalists regarding AI's role in the information ecosystem.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk Assessment</title>
<link>https://arxiv.org/abs/2509.01836</link>
<guid>https://arxiv.org/abs/2509.01836</guid>
<content:encoded><![CDATA[
arXiv:2509.01836v1 Announce Type: cross 
Abstract: Accurate vessel trajectory prediction is essential for enhancing situational awareness and preventing collisions. Still, existing data-driven models are constrained mainly to single-vessel forecasting, overlooking vessel interactions, navigation rules, and explicit collision risk assessment. We present a transformer-based framework for multi-vessel trajectory prediction with integrated collision risk analysis. For a given target vessel, the framework identifies nearby vessels. It jointly predicts their future trajectories through parallel streams encoding kinematic and derived physical features, causal convolutions for temporal locality, spatial transformations for positional encoding, and hybrid positional embeddings that capture both local motion patterns and long-range dependencies. Evaluated on large-scale real-world AIS data using joint multi-vessel metrics, the model demonstrates superior forecasting capabilities beyond traditional single-vessel displacement errors. By simulating interactions among predicted trajectories, the framework further quantifies potential collision risks, offering actionable insights to strengthen maritime safety and decision support.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Goal-Conditioned Reinforcement Learning for Data-Driven Maritime Navigation</title>
<link>https://arxiv.org/abs/2509.01838</link>
<guid>https://arxiv.org/abs/2509.01838</guid>
<content:encoded><![CDATA[
arXiv:2509.01838v1 Announce Type: cross 
Abstract: Routing vessels through narrow and dynamic waterways is challenging due to changing environmental conditions and operational constraints. Existing vessel-routing studies typically fail to generalize across multiple origin-destination pairs and do not exploit large-scale, data-driven traffic graphs. In this paper, we propose a reinforcement learning solution for big maritime data that can learn to find a route across multiple origin-destination pairs while adapting to different hexagonal grid resolutions. Agents learn to select direction and speed under continuous observations in a multi-discrete action space. A reward function balances fuel efficiency, travel time, wind resistance, and route diversity, using an Automatic Identification System (AIS)-derived traffic graph with ERA5 wind fields. The approach is demonstrated in the Gulf of St. Lawrence, one of the largest estuaries in the world. We evaluate configurations that combine Proximal Policy Optimization with recurrent networks, invalid-action masking, and exploration strategies. Our experiments demonstrate that action masking yields a clear improvement in policy performance and that supplementing penalty-only feedback with positive shaping rewards produces additional gains.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices</title>
<link>https://arxiv.org/abs/2509.01839</link>
<guid>https://arxiv.org/abs/2509.01839</guid>
<content:encoded><![CDATA[
arXiv:2509.01839v1 Announce Type: cross 
Abstract: Currently, prominent Transformer architectures applied on graphs and meshes for shape analysis tasks employ traditional attention layers that heavily utilize spectral features requiring costly eigenvalue decomposition-based methods. To encode the mesh structure, these methods derive positional embeddings, that heavily rely on eigenvalue decomposition based operations, e.g. on the Laplacian matrix, or on heat-kernel signatures, which are then concatenated to the input features. This paper proposes a novel approach inspired by the explicit construction of the Hodge Laplacian operator in Discrete Exterior Calculus as a product of discrete Hodge operators and exterior derivatives, i.e. $(L := \star_0^{-1} d_0^T \star_1 d_0)$. We adjust the Transformer architecture in a novel deep learning layer that utilizes the multi-head attention mechanism to approximate Hodge matrices $\star_0$, $\star_1$ and $\star_2$ and learn families of discrete operators $L$ that act on mesh vertices, edges and faces. Our approach results in a computationally-efficient architecture that achieves comparable performance in mesh segmentation and classification tasks, through a direct learning framework, while eliminating the need for costly eigenvalue decomposition operations or complex preprocessing operations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradES: Significantly Faster Training in Transformers with Gradient-Based Early Stopping</title>
<link>https://arxiv.org/abs/2509.01842</link>
<guid>https://arxiv.org/abs/2509.01842</guid>
<content:encoded><![CDATA[
arXiv:2509.01842v1 Announce Type: cross 
Abstract: Early stopping monitors global validation loss and halts all parameter updates simultaneously, which is computationally costly for large transformers due to the extended time required for validation inference. We propose GradES, a novel gradient-based early stopping approach that operates within transformer components (attention projections and Feed-Forward layer matrices). We found that different components converge at varying rates during fine-tuning. GradES tracks the magnitude of gradients in backpropagation for these matrices during training. When a projection matrix's gradients fall below a convergence threshold $\tau$, we exclude that projection matrix from further updates individually, eliminating costly validation passes while allowing slow converging matrices to continue learning. By strategically freezing parameters when their gradients converge, GradES speeds up training time by 1.57--7.22$\times$ while simultaneously enhancing generalization through early prevention of overfitting, resulting in 1.2% higher average accuracy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community-Centered Spatial Intelligence for Climate Adaptation at Nova Scotia's Eastern Shore</title>
<link>https://arxiv.org/abs/2509.01845</link>
<guid>https://arxiv.org/abs/2509.01845</guid>
<content:encoded><![CDATA[
arXiv:2509.01845v1 Announce Type: cross 
Abstract: This paper presents an overview of a human-centered initiative aimed at strengthening climate resilience along Nova Scotia's Eastern Shore. This region, a collection of rural villages with deep ties to the sea, faces existential threats from climate change that endanger its way of life. Our project moves beyond a purely technical response, weaving together expertise from Computer Science, Industrial Engineering, and Coastal Geography to co-create tools with the community. By integrating generational knowledge of residents, particularly elders, through the Eastern Shore Citizen Science Coastal Monitoring Network, this project aims to collaborate in building a living digital archive. This effort is hosted under Dalhousie University's Transforming Climate Action (TCA) initiative, specifically through its Transformative Adaptations to Social-Ecological Climate Change Trajectories (TranSECT) and TCA Artificial Intelligence (TCA-AI) projects. This work is driven by a collaboration model in which student teams work directly with residents. We present a detailed project timeline and a replicable model for how technology can support traditional communities, enabling them to navigate climate transformation more effectively.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction</title>
<link>https://arxiv.org/abs/2509.01873</link>
<guid>https://arxiv.org/abs/2509.01873</guid>
<content:encoded><![CDATA[
arXiv:2509.01873v1 Announce Type: cross 
Abstract: Modern deep learning developments create new opportunities for 3D mapping technology, scene reconstruction pipelines, and virtual reality development. Despite advances in 3D deep learning technology, direct training of deep learning models on 3D data faces challenges due to the high dimensionality inherent in 3D data and the scarcity of labeled datasets. Structure-from-motion (SfM) and Simultaneous Localization and Mapping (SLAM) exhibit robust performance when applied to structured indoor environments but often struggle with ambiguous features in unstructured environments. These techniques often struggle to generate detailed geometric representations effective for downstream tasks such as rendering and semantic analysis. Current limitations require the development of 3D representation methods that combine traditional geometric techniques with deep learning capabilities to generate robust geometry-aware deep learning models.
  The dissertation provides solutions to the fundamental challenges in 3D vision by developing geometric deep learning methods tailored for essential tasks such as camera pose estimation, point cloud registration, depth prediction, and 3D reconstruction. The integration of geometric priors or constraints, such as including depth information, surface normals, and equivariance into deep learning models, enhances both the accuracy and robustness of geometric representations. This study systematically investigates key components of 3D vision, including camera pose estimation, point cloud registration, depth estimation, and high-fidelity 3D reconstruction, demonstrating their effectiveness across real-world applications such as digital cultural heritage preservation and immersive VR/AR environments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic Activation Function</title>
<link>https://arxiv.org/abs/2509.01874</link>
<guid>https://arxiv.org/abs/2509.01874</guid>
<content:encoded><![CDATA[
arXiv:2509.01874v1 Announce Type: cross 
Abstract: Understanding the inner workings of machine learning models is critical for ensuring their reliability and robustness. Whilst many techniques in mechanistic interpretability focus on activation driven analyses, being able to derive meaningful features directly from the weights of a neural network would provide greater guarantees and more computational efficiency. Existing techniques for analyzing model features through weights suffer from drawbacks such as reduced performance and data inefficiency. In this paper, we introduce Signed Quadratic Shrink (SQS), an activation function designed to allow Gated Linear Units (GLUs) to learn interpretable features without these drawbacks. Our experimental results show that SQS achieves performance competitive with state-of-the-art activation functions whilst enabling weight-based interpretability
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision</title>
<link>https://arxiv.org/abs/2509.01882</link>
<guid>https://arxiv.org/abs/2509.01882</guid>
<content:encoded><![CDATA[
arXiv:2509.01882v1 Announce Type: cross 
Abstract: Ongoing advancements in computer vision, particularly in pattern recognition and scene classification, have enabled new applications in environmental monitoring. Deep learning now offers non-contact methods for assessing water quality and detecting contamination, both critical for disaster response and public health protection. This work introduces HydroVision, a deep learning-based scene classification framework that estimates optically active water quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored Dissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and Turbidity from standard Red-Green-Blue (RGB) images of surface water. HydroVision supports early detection of contamination trends and strengthens monitoring by regulatory agencies during external environmental stressors, industrial activities, and force majeure events. The model is trained on more than 500,000 seasonally varied images collected from the United States Geological Survey Hydrologic Imagery Visualization and Information System between 2022 and 2024. This approach leverages widely available RGB imagery as a scalable, cost-effective alternative to traditional multispectral and hyperspectral remote sensing. Four state-of-the-art convolutional neural networks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer are evaluated through transfer learning to identify the best-performing architecture. DenseNet121 achieves the highest validation performance, with an R2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for real-world water quality monitoring across diverse conditions. While the current model is optimized for well-lit imagery, future work will focus on improving robustness under low-light and obstructed scenarios to expand its operational utility.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning</title>
<link>https://arxiv.org/abs/2509.01885</link>
<guid>https://arxiv.org/abs/2509.01885</guid>
<content:encoded><![CDATA[
arXiv:2509.01885v1 Announce Type: cross 
Abstract: The extraction of critical patient information from Electronic Health Records (EHRs) poses significant challenges due to the complexity and unstructured nature of the data. Traditional machine learning approaches often fail to capture pertinent details efficiently, making it difficult for clinicians to utilize these tools effectively in patient care. This paper introduces a novel approach to extracting the OPQRST assessment from EHRs by leveraging the capabilities of Large Language Models (LLMs). We propose to reframe the task from sequence labeling to text generation, enabling the models to provide reasoning steps that mimic a physician's cognitive processes. This approach enhances interpretability and adapts to the limited availability of labeled data in healthcare settings. Furthermore, we address the challenge of evaluating the accuracy of machine-generated text in clinical contexts by proposing a modification to traditional Named Entity Recognition (NER) metrics. This includes the integration of semantic similarity measures, such as the BERT Score, to assess the alignment between generated text and the clinical intent of the original records. Our contributions demonstrate a significant advancement in the use of AI in healthcare, offering a scalable solution that improves the accuracy and usability of information extraction from EHRs, thereby aiding clinicians in making more informed decisions and enhancing patient care outcomes.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VISP: Volatility Informed Stochastic Projection for Adaptive Regularization</title>
<link>https://arxiv.org/abs/2509.01903</link>
<guid>https://arxiv.org/abs/2509.01903</guid>
<content:encoded><![CDATA[
arXiv:2509.01903v1 Announce Type: cross 
Abstract: We propose VISP: Volatility Informed Stochastic Projection, an adaptive regularization method that leverages gradient volatility to guide stochastic noise injection in deep neural networks. Unlike conventional techniques that apply uniform noise or fixed dropout rates, VISP dynamically computes volatility from gradient statistics and uses it to scale a stochastic projection matrix. This mechanism selectively regularizes inputs and hidden nodes that exhibit higher gradient volatility while preserving stable representations, thereby mitigating overfitting. Extensive experiments on MNIST, CIFAR-10, and SVHN demonstrate that VISP consistently improves generalization performance over baseline models and fixed-noise alternatives. In addition, detailed analyses of the evolution of volatility, the spectral properties of the projection matrix, and activation distributions reveal that VISP not only stabilizes the internal dynamics of the network but also fosters a more robust feature representation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework</title>
<link>https://arxiv.org/abs/2509.01910</link>
<guid>https://arxiv.org/abs/2509.01910</guid>
<content:encoded><![CDATA[
arXiv:2509.01910v1 Announce Type: cross 
Abstract: Worldwide geo-localization involves determining the exact geographic location of images captured globally, typically guided by geographic cues such as climate, landmarks, and architectural styles. Despite advancements in geo-localization models like GeoCLIP, which leverages images and location alignment via contrastive learning for accurate predictions, the interpretability of these models remains insufficiently explored. Current concept-based interpretability methods fail to align effectively with Geo-alignment image-location embedding objectives, resulting in suboptimal interpretability and performance. To address this gap, we propose a novel framework integrating global geo-localization with concept bottlenecks. Our method inserts a Concept-Aware Alignment Module that jointly projects image and location embeddings onto a shared bank of geographic concepts (e.g., tropical climate, mountain, cathedral) and minimizes a concept-level loss, enhancing alignment in a concept-specific subspace and enabling robust interpretability. To our knowledge, this is the first work to introduce interpretability into geo-localization. Extensive experiments demonstrate that our approach surpasses GeoCLIP in geo-localization accuracy and boosts performance across diverse geospatial prediction tasks, revealing richer semantic insights into geographic decision-making processes.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Continuous Encoding-Based Representation for Efficient Multi-Fidelity Multi-Objective Neural Architecture Search</title>
<link>https://arxiv.org/abs/2509.01943</link>
<guid>https://arxiv.org/abs/2509.01943</guid>
<content:encoded><![CDATA[
arXiv:2509.01943v1 Announce Type: cross 
Abstract: Neural architecture search (NAS) is an attractive approach to automate the design of optimized architectures but is constrained by high computational budget, especially when optimizing for multiple, important conflicting objectives. To address this, an adaptive Co-Kriging-assisted multi-fidelity multi-objective NAS algorithm is proposed to further reduce the computational cost of NAS by incorporating a clustering-based local multi-fidelity infill sampling strategy, enabling efficient exploration of the search space for faster convergence. This algorithm is further accelerated by the use of a novel continuous encoding method to represent the connections of nodes in each cell within a generalized cell-based U-Net backbone, thereby decreasing the search dimension (number of variables). Results indicate that the proposed NAS algorithm outperforms previously published state-of-the-art methods under limited computational budget on three numerical benchmarks, a 2D Darcy flow regression problem and a CHASE_DB1 biomedical image segmentation problem. The proposed method is subsequently used to create a wind velocity regression model with application in urban modelling, with the found model able to achieve good prediction with less computational complexity. Further analysis revealed that the NAS algorithm independently identified principles undergirding superior U-Net architectures in other literature, such as the importance of allowing each cell to incorporate information from prior cells.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-aware Contrastive Learning for Diagram Understanding of Multimodal Models</title>
<link>https://arxiv.org/abs/2509.01959</link>
<guid>https://arxiv.org/abs/2509.01959</guid>
<content:encoded><![CDATA[
arXiv:2509.01959v1 Announce Type: cross 
Abstract: Multimodal models, such as the Contrastive Language-Image Pre-training (CLIP) model, have demonstrated remarkable success in aligning visual and linguistic representations. However, these models exhibit limitations when applied to specialised visual domains, such as diagrams, which encode structured, symbolic information distinct from that of natural imagery.
  In this paper, we introduce a novel training paradigm explicitly designed to enhance the comprehension of diagrammatic images within vision-language models. Our approach uses ``hard'' samples for our proposed contrastive learning that incorporates two specialised loss functions that leverage the inherent structural properties of diagrams. By integrating these objectives into model training, our method enables models to develop a more structured and semantically coherent understanding of diagrammatic content.
  We empirically validate our approach on a benchmark dataset of flowcharts, as a representative class of diagrammatic imagery, demonstrating substantial improvements over standard CLIP and conventional hard negative CLIP learning paradigms for both image-text matching and visual question answering tasks. Our findings underscore the significance of tailored training strategies for specialised tasks and contribute to advancing diagrammatic understanding within the broader landscape of vision-language integration.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2D Gaussian Splatting with Semantic Alignment for Image Inpainting</title>
<link>https://arxiv.org/abs/2509.01964</link>
<guid>https://arxiv.org/abs/2509.01964</guid>
<content:encoded><![CDATA[
arXiv:2509.01964v1 Announce Type: cross 
Abstract: Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination</title>
<link>https://arxiv.org/abs/2509.01986</link>
<guid>https://arxiv.org/abs/2509.01986</guid>
<content:encoded><![CDATA[
arXiv:2509.01986v1 Announce Type: cross 
Abstract: In recent years, integrating multimodal understanding and generation into a single unified model has emerged as a promising paradigm. While this approach achieves strong results in text-to-image (T2I) generation, it still struggles with precise image editing. We attribute this limitation to an imbalanced division of responsibilities. The understanding module primarily functions as a translator that encodes user instructions into semantic conditions, while the generation module must simultaneously act as designer and painter, inferring the original layout, identifying the target editing region, and rendering the new content. This imbalance is counterintuitive because the understanding module is typically trained with several times more data on complex reasoning tasks than the generation module. To address this issue, we introduce Draw-In-Mind (DIM), a dataset comprising two complementary subsets: (i) DIM-T2I, containing 14M long-context image-text pairs to enhance complex instruction comprehension; and (ii) DIM-Edit, consisting of 233K chain-of-thought imaginations generated by GPT-4o, serving as explicit design blueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable SANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM dataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale, DIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and GEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1 and Step1X-Edit. These findings demonstrate that explicitly assigning the design responsibility to the understanding module provides significant benefits for image editing. Our dataset and models will be available at https://github.com/showlab/DIM.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting</title>
<link>https://arxiv.org/abs/2509.01997</link>
<guid>https://arxiv.org/abs/2509.01997</guid>
<content:encoded><![CDATA[
arXiv:2509.01997v1 Announce Type: cross 
Abstract: Logistical demand-supply forecasting that evaluates the alignment between projected supply and anticipated demand, is essential for the efficiency and quality of on-demand food delivery platforms and serves as a key indicator for scheduling decisions. Future order distribution information, which reflects the distribution of orders in on-demand food delivery, is crucial for the performance of logistical demand-supply forecasting. Current studies utilize spatial-temporal analysis methods to model future order distribution information from serious time slices. However, learning future order distribution in online delivery platform is a time-series-insensitive problem with strong randomness. These approaches often struggle to effectively capture this information while remaining efficient. This paper proposes an innovative spatiotemporal learning model that utilizes only two graphs (ongoing and global) to learn future order distribution information, achieving superior performance compared to traditional spatial-temporal long-series methods. The main contributions are as follows: (1) The introduction of ongoing and global graphs in logistical demand-supply pressure forecasting compared to traditional long time series significantly enhances forecasting performance. (2) An innovative graph learning network framework using adaptive future graph learning and innovative cross attention mechanism (ACA-Net) is proposed to extract future order distribution information, effectively learning a robust future graph that substantially improves logistical demand-supply pressure forecasting outcomes. (3) The effectiveness of the proposed method is validated in real-world production environments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs</title>
<link>https://arxiv.org/abs/2509.02017</link>
<guid>https://arxiv.org/abs/2509.02017</guid>
<content:encoded><![CDATA[
arXiv:2509.02017v1 Announce Type: cross 
Abstract: Sequential recommendation (SR) aims to capture users' dynamic interests and sequential patterns based on their historical interactions. Recently, the powerful capabilities of large language models (LLMs) have driven their adoption in SR. However, we identify two critical challenges in existing LLM-based SR methods: 1) embedding collapse when incorporating pre-trained collaborative embeddings and 2) catastrophic forgetting of quantized embeddings when utilizing semantic IDs. These issues dampen the model scalability and lead to suboptimal recommendation performance. Therefore, based on LLMs like Llama3-8B-instruct, we introduce a novel SR framework named MME-SID, which integrates multimodal embeddings and quantized embeddings to mitigate embedding collapse. Additionally, we propose a Multimodal Residual Quantized Variational Autoencoder (MM-RQ-VAE) with maximum mean discrepancy as the reconstruction loss and contrastive learning for alignment, which effectively preserve intra-modal distance information and capture inter-modal correlations, respectively. To further alleviate catastrophic forgetting, we initialize the model with the trained multimodal code embeddings. Finally, we fine-tune the LLM efficiently using LoRA in a multimodal frequency-aware fusion manner. Extensive experiments on three public datasets validate the superior performance of MME-SID thanks to its capability to mitigate embedding collapse and catastrophic forgetting. The implementation code and datasets are publicly available for reproduction: https://github.com/Applied-Machine-Learning-Lab/MME-SID.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Training of Vision Transformers with Synthetic Negatives</title>
<link>https://arxiv.org/abs/2509.02024</link>
<guid>https://arxiv.org/abs/2509.02024</guid>
<content:encoded><![CDATA[
arXiv:2509.02024v1 Announce Type: cross 
Abstract: This paper does not introduce a novel method per se. Instead, we address the neglected potential of hard negative samples in self-supervised learning. Previous works explored synthetic hard negatives but rarely in the context of vision transformers. We build on this observation and integrate synthetic hard negatives to improve vision transformer representation learning. This simple yet effective technique notably improves the discriminative power of learned representations. Our experiments show performance improvements for both DeiT-S and Swin-T architectures.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fake &amp; Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives</title>
<link>https://arxiv.org/abs/2509.02029</link>
<guid>https://arxiv.org/abs/2509.02029</guid>
<content:encoded><![CDATA[
arXiv:2509.02029v1 Announce Type: cross 
Abstract: This paper does not introduce a new method per se. Instead, we build on existing self-supervised learning approaches for vision, drawing inspiration from the adage "fake it till you make it". While contrastive self-supervised learning has achieved remarkable success, it typically relies on vast amounts of real-world data and carefully curated hard negatives. To explore alternatives to these requirements, we investigate two forms of "faking it" in vision transformers. First, we study the potential of generative models for unsupervised representation learning, leveraging synthetic data to augment sample diversity. Second, we examine the feasibility of generating synthetic hard negatives in the representation space, creating diverse and challenging contrasts. Our framework - dubbed Syn2Co - combines both approaches and evaluates whether synthetically enhanced training can lead to more robust and transferable visual representations on DeiT-S and Swin-T architectures. Our findings highlight the promise and limitations of synthetic data in self-supervised learning, offering insights for future work in this direction.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synesthesia of Machines (SoM)-Based Task-Driven MIMO System for Image Transmission</title>
<link>https://arxiv.org/abs/2509.02031</link>
<guid>https://arxiv.org/abs/2509.02031</guid>
<content:encoded><![CDATA[
arXiv:2509.02031v1 Announce Type: cross 
Abstract: To support cooperative perception (CP) of networked mobile agents in dynamic scenarios, the efficient and robust transmission of sensory data is a critical challenge. Deep learning-based joint source-channel coding (JSCC) has demonstrated promising results for image transmission under adverse channel conditions, outperforming traditional rule-based codecs. While recent works have explored to combine JSCC with the widely adopted multiple-input multiple-output (MIMO) technology, these approaches are still limited to the discrete-time analog transmission (DTAT) model and simple tasks. Given the limited performance of existing MIMO JSCC schemes in supporting complex CP tasks for networked mobile agents with digital MIMO communication systems, this paper presents a Synesthesia of Machines (SoM)-based task-driven MIMO system for image transmission, referred to as SoM-MIMO. By leveraging the structural properties of the feature pyramid for perceptual tasks and the channel properties of the closed-loop MIMO communication system, SoM-MIMO enables efficient and robust digital MIMO transmission of images. Experimental results have shown that compared with two JSCC baseline schemes, our approach achieves average mAP improvements of 6.30 and 10.48 across all SNR levels, while maintaining identical communication overhead.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSeek performs better than other Large Language Models in Dental Cases</title>
<link>https://arxiv.org/abs/2509.02036</link>
<guid>https://arxiv.org/abs/2509.02036</guid>
<content:encoded><![CDATA[
arXiv:2509.02036v1 Announce Type: cross 
Abstract: Large language models (LLMs) hold transformative potential in healthcare, yet their capacity to interpret longitudinal patient narratives remains inadequately explored. Dentistry, with its rich repository of structured clinical data, presents a unique opportunity to rigorously assess LLMs' reasoning abilities. While several commercial LLMs already exist, DeepSeek, a model that gained significant attention earlier this year, has also joined the competition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini 2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal dental case vignettes through open-ended clinical tasks. Using 34 standardized longitudinal periodontal cases (comprising 258 question-answer pairs), we assessed model performance via automated metrics and blinded evaluations by licensed dentists. DeepSeek emerged as the top performer, demonstrating superior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert ratings (median = 4.5/5 vs. 4.0/5), without significantly compromising readability. Our study positions DeepSeek as the leading LLM for case analysis, endorses its integration as an adjunct tool in both medical education and research, and highlights its potential as a domain-specific agent.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fantastic Pretraining Optimizers and Where to Find Them</title>
<link>https://arxiv.org/abs/2509.02046</link>
<guid>https://arxiv.org/abs/2509.02046</guid>
<content:encoded><![CDATA[
arXiv:2509.02046v1 Announce Type: cross 
Abstract: AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization Framework with Curvature-Guided Perturbation</title>
<link>https://arxiv.org/abs/2509.02048</link>
<guid>https://arxiv.org/abs/2509.02048</guid>
<content:encoded><![CDATA[
arXiv:2509.02048v1 Announce Type: cross 
Abstract: Machine learning models require datasets for effective training, but directly sharing raw data poses significant privacy risk such as membership inference attacks (MIA). To mitigate the risk, privacy-preserving techniques such as data perturbation, generalization, and synthetic data generation are commonly utilized. However, these methods often degrade data accuracy, specificity, and diversity, limiting the performance of downstream tasks and thus reducing data utility. Therefore, striking an optimal balance between privacy preservation and data utility remains a critical challenge.
  To address this issue, we introduce a novel bilevel optimization framework for the publication of private datasets, where the upper-level task focuses on data utility and the lower-level task focuses on data privacy. In the upper-level task, a discriminator guides the generation process to ensure that perturbed latent variables are mapped to high-quality samples, maintaining fidelity for downstream tasks. In the lower-level task, our framework employs local extrinsic curvature on the data manifold as a quantitative measure of individual vulnerability to MIA, providing a geometric foundation for targeted privacy protection. By perturbing samples toward low-curvature regions, our method effectively suppresses distinctive feature combinations that are vulnerable to MIA. Through alternating optimization of both objectives, we achieve a synergistic balance between privacy and utility. Extensive experimental evaluations demonstrate that our method not only enhances resistance to MIA in downstream tasks but also surpasses existing methods in terms of sample quality and diversity.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance</title>
<link>https://arxiv.org/abs/2509.02055</link>
<guid>https://arxiv.org/abs/2509.02055</guid>
<content:encoded><![CDATA[
arXiv:2509.02055v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models pre-trained on large, diverse datasets show remarkable potential for general-purpose robotic manipulation. However, a primary bottleneck remains in adapting these models to downstream tasks, especially when the robot's embodiment or the task itself differs from the pre-training data. This discrepancy leads to a significant mismatch in action distributions, demanding extensive data and compute for effective fine-tuning. To address this challenge, we introduce \textbf{Align-Then-stEer (\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation framework. \texttt{ATE} first aligns disparate action spaces by constructing a unified latent space, where a variational autoencoder constrained by reverse KL divergence embeds adaptation actions into modes of the pre-training action latent distribution. Subsequently, it steers the diffusion- or flow-based VLA's generation process during fine-tuning via a guidance mechanism that pushes the model's output distribution towards the target domain. We conduct extensive experiments on cross-embodiment and cross-task manipulation in both simulation and real world. Compared to direct fine-tuning of representative VLAs, our method improves the average multi-task success rate by up to \textbf{9.8\%} in simulation and achieves a striking \textbf{32\% success rate gain} in a real-world cross-embodiment setting. Our work presents a general and lightweight solution that greatly enhances the practicality of deploying VLA models to new robotic platforms and tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis</title>
<link>https://arxiv.org/abs/2509.02075</link>
<guid>https://arxiv.org/abs/2509.02075</guid>
<content:encoded><![CDATA[
arXiv:2509.02075v1 Announce Type: cross 
Abstract: Adhering to explicit length constraints, such as generating text with a precise word count, remains a significant challenge for Large Language Models (LLMs). This study aims at investigating the differences between foundation models and their instruction-tuned counterparts, on length-controlled text generation in English and Italian. We analyze both performance and internal component contributions using Cumulative Weighted Attribution, a metric derived from Direct Logit Attribution. Our findings reveal that instruction-tuning substantially improves length control, primarily by specializing components in deeper model layers. Specifically, attention heads in later layers of IT models show increasingly positive contributions, particularly in English. In Italian, while attention contributions are more attenuated, final-layer MLPs exhibit a stronger positive role, suggesting a compensatory mechanism. These results indicate that instruction-tuning reconfigures later layers for task adherence, with component-level strategies potentially adapting to linguistic context.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Future DDoS Attacks Using Long Short Term Memory (LSTM) Model</title>
<link>https://arxiv.org/abs/2509.02076</link>
<guid>https://arxiv.org/abs/2509.02076</guid>
<content:encoded><![CDATA[
arXiv:2509.02076v1 Announce Type: cross 
Abstract: This paper forecasts future Distributed Denial of Service (DDoS) attacks using deep learning models. Although several studies address forecasting DDoS attacks, they remain relatively limited compared to detection-focused research. By studying the current trends and forecasting based on newer and updated datasets, mitigation plans against the attacks can be planned and formulated. The methodology used in this research work conforms to the Cross Industry Standard Process for Data Mining (CRISP-DM) model.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization</title>
<link>https://arxiv.org/abs/2509.02093</link>
<guid>https://arxiv.org/abs/2509.02093</guid>
<content:encoded><![CDATA[
arXiv:2509.02093v1 Announce Type: cross 
Abstract: Automatic prompt optimization has recently emerged as a strategy for improving the quality of prompts used in Large Language Models (LLMs), with the goal of generating more accurate and useful responses. However, most prior work focuses on direct prompt refinement or model fine-tuning, overlooking the potential of leveraging LLMs' inherent reasoning capability to learn from contrasting examples. In this paper, we present Contrastive Reasoning Prompt Optimization (CRPO), a novel framework that formulates prompt optimization as a retrieval augmented reasoning process. Our approach retrieves top k reference prompts from the HelpSteer2 dataset, an open-source collection annotated for helpfulness, correctness, coherence, complexity, and verbosity, and constructs two complementary optimization paradigms: (1) tiered contrastive reasoning, where the LLM compares high, medium, and low quality prompts to refine its own generation through reflective reasoning, and (2) multi-metric contrastive reasoning, where the LLM analyzes the best prompts along each evaluation dimension and integrates their strengths into an optimized prompt. By explicitly contrasting high and low quality exemplars, CRPO enables the model to deduce why certain prompts succeed while others fail, thereby achieving more robust and interpretable optimization. Experimental results on the HelpSteer2 benchmark demonstrate that CRPO significantly outperforms baselines. Our findings highlight the promise of contrastive, retrieval-augmented reasoning for advancing automatic prompt optimization.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer</title>
<link>https://arxiv.org/abs/2509.02097</link>
<guid>https://arxiv.org/abs/2509.02097</guid>
<content:encoded><![CDATA[
arXiv:2509.02097v1 Announce Type: cross 
Abstract: Evaluating the capabilities of large language models (LLMs) is an essential step to ensure the successful application of LLMs across various domains. The current evaluation of LLMs is based on a paradigm that involves querying them with predefined question sets and assessing their outputs. This paradigm offers controllable processes and simplicity, but faces challenges such as limited interaction with targets, insufficient difficulty control, and difficulties in verifying the validity of evaluation results, making it hard to precisely determine the knowledge and capability boundaries of target models. To address these challenges, we propose JudgeAgent, a knowledge-target adaptive dynamic evaluation framework based on a new interviewer-style evaluation paradigm. JudgeAgent employs a comprehensive evaluation approach consisting of benchmark grading, interactive extension, and evaluation feedback. It utilizes knowledge-driven data synthesis and target-adaptive difficulty adjustment methods to conduct extended testing, providing accurate and effective evaluation results. We also introduce a novel insight into validating evaluation methods, demonstrating the effectiveness of JudgeAgent and its dynamic evaluation paradigm through extensive experiments.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALAD -- Semantics-Aware Logical Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.02101</link>
<guid>https://arxiv.org/abs/2509.02101</guid>
<content:encoded><![CDATA[
arXiv:2509.02101v1 Announce Type: cross 
Abstract: Recent surface anomaly detection methods excel at identifying structural anomalies, such as dents and scratches, but struggle with logical anomalies, such as irregular or missing object components. The best-performing logical anomaly detection approaches rely on aggregated pretrained features or handcrafted descriptors (most often derived from composition maps), which discard spatial and semantic information, leading to suboptimal performance. We propose SALAD, a semantics-aware discriminative logical anomaly detection method that incorporates a newly proposed composition branch to explicitly model the distribution of object composition maps, consequently learning important semantic relationships. Additionally, we introduce a novel procedure for extracting composition maps that requires no hand-made labels or category-specific information, in contrast to previous methods. By effectively modelling the composition map distribution, SALAD significantly improves upon state-of-the-art methods on the standard benchmark for logical anomaly detection, MVTec LOCO, achieving an impressive image-level AUROC of 96.1%. Code: https://github.com/MaticFuc/SALAD
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiGraph: A Large-Scale Hierarchical Graph Dataset for Malware Analysis</title>
<link>https://arxiv.org/abs/2509.02113</link>
<guid>https://arxiv.org/abs/2509.02113</guid>
<content:encoded><![CDATA[
arXiv:2509.02113v1 Announce Type: cross 
Abstract: The advancement of graph-based malware analysis is critically limited by the absence of large-scale datasets that capture the inherent hierarchical structure of software. Existing methods often oversimplify programs into single level graphs, failing to model the crucial semantic relationship between high-level functional interactions and low-level instruction logic. To bridge this gap, we introduce \dataset, the largest public hierarchical graph dataset for malware analysis, comprising over \textbf{200M} Control Flow Graphs (CFGs) nested within \textbf{595K} Function Call Graphs (FCGs). This two-level representation preserves structural semantics essential for building robust detectors resilient to code obfuscation and malware evolution. We demonstrate HiGraph's utility through a large-scale analysis that reveals distinct structural properties of benign and malicious software, establishing it as a foundational benchmark for the community. The dataset and tools are publicly available at https://higraph.org.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Social Heuristics for Human-Aware Path Planning</title>
<link>https://arxiv.org/abs/2509.02134</link>
<guid>https://arxiv.org/abs/2509.02134</guid>
<content:encoded><![CDATA[
arXiv:2509.02134v1 Announce Type: cross 
Abstract: Social robotic navigation has been at the center of numerous studies in recent years. Most of the research has focused on driving the robotic agent along obstacle-free trajectories, respecting social distances from humans, and predicting their movements to optimize navigation. However, in order to really be socially accepted, the robots must be able to attain certain social norms that cannot arise from conventional navigation, but require a dedicated learning process. We propose Heuristic Planning with Learned Social Value (HPLSV), a method to learn a value function encapsulating the cost of social navigation, and use it as an additional heuristic in heuristic-search path planning. In this preliminary work, we apply the methodology to the common social scenario of joining a queue of people, with the intention of generalizing to further human activities.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theoretical Framework of the Processes of Change in Psychotherapy Delivered by Artificial Agents</title>
<link>https://arxiv.org/abs/2509.02144</link>
<guid>https://arxiv.org/abs/2509.02144</guid>
<content:encoded><![CDATA[
arXiv:2509.02144v1 Announce Type: cross 
Abstract: The question of whether artificial agents (e.g., chatbots and social robots) can replace human therapists has received notable attention following the recent launch of large language models. However, little is known about the processes of change in psychotherapy delivered by artificial agents. To facilitate hypothesis development and stimulate scientific debate, the present article offers the first theoretical framework of the processes of change in psychotherapy delivered by artificial agents. The theoretical framework rests upon a conceptual analysis of what active ingredients may be inherently linked to the presence of human therapists. We propose that human therapists' ontological status as human beings and sociocultural status as socially sanctioned healthcare professionals play crucial roles in promoting treatment outcomes. In the absence of the ontological and sociocultural status of human therapists, we propose what we coin the genuineness gap and credibility gap can emerge and undermine key processes of change in psychotherapy. Based on these propositions, we propose avenues for scientific investigations and practical applications aimed at leveraging the strengths of artificial agents and human therapists respectively. We also highlight the intricate agentic nature of artificial agents and discuss how this complicates endeavors to establish universally applicable propositions regarding the processes of change in these interventions.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation</title>
<link>https://arxiv.org/abs/2509.02154</link>
<guid>https://arxiv.org/abs/2509.02154</guid>
<content:encoded><![CDATA[
arXiv:2509.02154v1 Announce Type: cross 
Abstract: Variational Autoencoders (VAEs) with global priors mirror the training set's class frequency in latent space, underrepresenting tail classes and reducing generative fairness on imbalanced datasets. While $t^3$VAE improves robustness via heavy-tailed Student's t-distribution priors, it still allocates latent volume proportionally to the class frequency.In this work, we address this issue by explicitly enforcing equitable latent space allocation across classes. To this end, we propose Conditional-$t^3$VAE, which defines a per-class \mbox{Student's t} joint prior over latent and output variables, preventing dominance by majority classes. Our model is optimized using a closed-form objective derived from the $\gamma$-power divergence. Moreover, for class-balanced generation, we derive an equal-weight latent mixture of Student's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA, Conditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE and Gaussian-based VAE baselines, particularly under severe class imbalance. In per-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional Gaussian VAE across all highly imbalanced settings. While Gaussian-based models remain competitive under mild imbalance ratio ($\rho \lesssim 3$), our approach substantially improves generative fairness and diversity in more extreme regimes.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in Low-Resource Philippine Languages</title>
<link>https://arxiv.org/abs/2509.02160</link>
<guid>https://arxiv.org/abs/2509.02160</guid>
<content:encoded><![CDATA[
arXiv:2509.02160v1 Announce Type: cross 
Abstract: Named-entity recognition (NER) in low-resource languages is usually tackled by finetuning very large multilingual LMs, an option that is often infeasible in memory- or latency-constrained settings. We ask whether small decoder LMs can be pretrained so that they adapt quickly and transfer zero-shot to languages unseen during pretraining. To this end we replace part of the autoregressive objective with first-order model-agnostic meta-learning (MAML). Tagalog and Cebuano are typologically similar yet structurally different in their actor/non-actor voice systems, and hence serve as a challenging test-bed. Across four model sizes (11 M - 570 M) MAML lifts zero-shot micro-F1 by 2-6 pp under head-only tuning and 1-3 pp after full tuning, while cutting convergence time by up to 8%. Gains are largest for single-token person entities that co-occur with Tagalog case particles si/ni, highlighting the importance of surface anchors.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified Approach to Security and Safety</title>
<link>https://arxiv.org/abs/2509.02163</link>
<guid>https://arxiv.org/abs/2509.02163</guid>
<content:encoded><![CDATA[
arXiv:2509.02163v1 Announce Type: cross 
Abstract: Integrating large language models (LLMs) into robotic systems has revolutionised embodied artificial intelligence, enabling advanced decision-making and adaptability. However, ensuring reliability, encompassing both security against adversarial attacks and safety in complex environments, remains a critical challenge. To address this, we propose a unified framework that mitigates prompt injection attacks while enforcing operational safety through robust validation mechanisms. Our approach combines prompt assembling, state management, and safety validation, evaluated using both performance and security metrics. Experiments show a 30.8% improvement under injection attacks and up to a 325% improvement in complex environment settings under adversarial conditions compared to baseline scenarios. This work bridges the gap between safety and security in LLM-based robotic systems, offering actionable insights for deploying reliable LLM-integrated mobile robots in real-world settings. The framework is open-sourced with simulation and physical deployment demos at https://llmeyesim.vercel.app/
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoidance Decoding for Diverse Multi-Branch Story Generation</title>
<link>https://arxiv.org/abs/2509.02170</link>
<guid>https://arxiv.org/abs/2509.02170</guid>
<content:encoded><![CDATA[
arXiv:2509.02170v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often generate repetitive and monotonous outputs, especially in tasks like story generation, due to limited creative diversity when given the same input prompt. To address this challenge, we propose a novel decoding strategy, Avoidance Decoding, that modifies token logits by penalizing similarity to previously generated outputs, thereby encouraging more diverse multi-branch stories. This penalty adaptively balances two similarity measures: (1) Concept-level Similarity Penalty, which is prioritized in early stages to diversify initial story concepts, and (2) Narrative-level Similarity Penalty, which is increasingly emphasized later to ensure natural yet diverse plot development. Notably, our method achieves up to 2.6 times higher output diversity and reduces repetition by an average of 30% compared to strong baselines, while effectively mitigating text degeneration. Furthermore, we reveal that our method activates a broader range of neurons, demonstrating that it leverages the model's intrinsic creativity.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks</title>
<link>https://arxiv.org/abs/2509.02175</link>
<guid>https://arxiv.org/abs/2509.02175</guid>
<content:encoded><![CDATA[
arXiv:2509.02175v1 Announce Type: cross 
Abstract: We propose RocketScience, an open-source contrastive VLM benchmark that tests for spatial relation understanding. It is comprised of entirely new real-world image-text pairs covering mostly relative spatial understanding and the order of objects. The benchmark is designed
  to be very easy for humans and hard for the current generation of VLMs, and this is empirically verified. Our results show a striking lack of spatial relation understanding in open source and frontier commercial VLMs and a surprisingly high performance of reasoning models. Additionally, we perform a disentanglement analysis to separate the contributions of object localization and spatial reasoning in chain-of-thought-based models and find that the performance on the benchmark is bottlenecked by spatial reasoning and not object localization capabilities.
  We release the dataset with a CC-BY-4.0 license and make the evaluation code available at: https://github.com/nilshoehing/rocketscience
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Ensembles: Simulating All-Atom Protein Dynamics in a Learned Latent Space</title>
<link>https://arxiv.org/abs/2509.02196</link>
<guid>https://arxiv.org/abs/2509.02196</guid>
<content:encoded><![CDATA[
arXiv:2509.02196v1 Announce Type: cross 
Abstract: Simulating the long-timescale dynamics of biomolecules is a central challenge in computational science. While enhanced sampling methods can accelerate these simulations, they rely on pre-defined collective variables that are often difficult to identify. A recent generative model, LD-FPG, demonstrated that this problem could be bypassed by learning to sample the static equilibrium ensemble as all-atom deformations from a reference structure, establishing a powerful method for all-atom ensemble generation. However, while this approach successfully captures a system's probable conformations, it does not model the temporal evolution between them. Here we extend LD-FPG with a temporal propagator that operates within the learned latent space and compare three classes: (i) score-guided Langevin dynamics, (ii) Koopman-based linear operators, and (iii) autoregressive neural networks. Within a unified encoder-propagator-decoder framework, we evaluate long-horizon stability, backbone and side-chain ensemble fidelity, and functional free-energy landscapes. Autoregressive neural networks deliver the most robust long rollouts; score-guided Langevin best recovers side-chain thermodynamics when the score is well learned; and Koopman provides an interpretable, lightweight baseline that tends to damp fluctuations. These results clarify the trade-offs among propagators and offer practical guidance for latent-space simulators of all-atom protein dynamics.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Baichuan-M2: Scaling Medical Capability with Large Verifier System</title>
<link>https://arxiv.org/abs/2509.02208</link>
<guid>https://arxiv.org/abs/2509.02208</guid>
<content:encoded><![CDATA[
arXiv:2509.02208v1 Announce Type: cross 
Abstract: As large language models (LLMs) advance in conversational and reasoning capabilities, their practical application in healthcare has become a critical research focus. However, there is a notable gap between the performance of medical LLMs on static benchmarks such as USMLE and their utility in real-world clinical decision-making. This discrepancy arises because traditional exams fail to capture the dynamic, interactive nature of medical consultations. To address this challenge, we introduce a novel dynamic verification framework that moves beyond static answer verifier, establishing a large-scale, high-fidelity interactive reinforcement learning system. Our framework comprises two key components: a Patient Simulator that creates realistic clinical environments using de-identified medical records, and a Clinical Rubrics Generator that dynamically produces multi-dimensional evaluation metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter medical augmented reasoning model trained through a multi-stage reinforcement learning strategy with an improved Group Relative Policy Optimization (GRPO) algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other open-source models and most advanced closed-source counterparts, achieving a score above 32 on the challenging HealthBench Hard benchmark-previously exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier system is essential for aligning LLM capabilities with practical clinical applications, establishing a new Pareto front in the performance-parameter trade-off for medical AI deployment.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ST-Hyper: Learning High-Order Dependencies Across Multiple Spatial-Temporal Scales for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.02217</link>
<guid>https://arxiv.org/abs/2509.02217</guid>
<content:encoded><![CDATA[
arXiv:2509.02217v1 Announce Type: cross 
Abstract: In multivariate time series (MTS) forecasting, many deep learning based methods have been proposed for modeling dependencies at multiple spatial (inter-variate) or temporal (intra-variate) scales. However, existing methods may fail to model dependencies across multiple spatial-temporal scales (ST-scales, i.e., scales that jointly consider spatial and temporal scopes). In this work, we propose ST-Hyper to model the high-order dependencies across multiple ST-scales through adaptive hypergraph modeling. Specifically, we introduce a Spatial-Temporal Pyramid Modeling (STPM) module to extract features at multiple ST-scales. Furthermore, we introduce an Adaptive Hypergraph Modeling (AHM) module that learns a sparse hypergraph to capture robust high-order dependencies among features. In addition, we interact with these features through tri-phase hypergraph propagation, which can comprehensively capture multi-scale spatial-temporal dynamics. Experimental results on six real-world MTS datasets demonstrate that ST-Hyper achieves the state-of-the-art performance, outperforming the best baselines with an average MAE reduction of 3.8\% and 6.8\% for long-term and short-term forecasting, respectively.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Multi-Aspect Diversification of News Recommendations Using Neuro-Symbolic AI for Individual and Societal Benefit</title>
<link>https://arxiv.org/abs/2509.02220</link>
<guid>https://arxiv.org/abs/2509.02220</guid>
<content:encoded><![CDATA[
arXiv:2509.02220v1 Announce Type: cross 
Abstract: News recommendations are complex, with diversity playing a vital role. So far, existing literature predominantly focuses on specific aspects of news diversity, such as viewpoints. In this paper, we introduce multi-aspect diversification in four distinct recommendation modes and outline the nuanced challenges in diversifying lists, sequences, summaries, and interactions. Our proposed research direction combines symbolic and subsymbolic artificial intelligence, leveraging both knowledge graphs and rule learning. We plan to evaluate our models using user studies to not only capture behavior but also their perceived experience. Our vision to balance news consumption points to other positive effects for users (e.g., increased serendipity) and society (e.g., decreased polarization).
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application Of Large Language Models For The Extraction Of Information From Particle Accelerator Technical Documentation</title>
<link>https://arxiv.org/abs/2509.02227</link>
<guid>https://arxiv.org/abs/2509.02227</guid>
<content:encoded><![CDATA[
arXiv:2509.02227v1 Announce Type: cross 
Abstract: The large set of technical documentation of legacy accelerator systems, coupled with the retirement of experienced personnel, underscores the urgent need for efficient methods to preserve and transfer specialized knowledge. This paper explores the application of large language models (LLMs), to automate and enhance the extraction of information from particle accelerator technical documents. By exploiting LLMs, we aim to address the challenges of knowledge retention, enabling the retrieval of domain expertise embedded in legacy documentation. We present initial results of adapting LLMs to this specialized domain. Our evaluation demonstrates the effectiveness of LLMs in extracting, summarizing, and organizing knowledge, significantly reducing the risk of losing valuable insights as personnel retire. Furthermore, we discuss the limitations of current LLMs, such as interpretability and handling of rare domain-specific terms, and propose strategies for improvement. This work highlights the potential of LLMs to play a pivotal role in preserving institutional knowledge and ensuring continuity in highly specialized fields.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoencoder-based non-intrusive model order reduction in continuum mechanics</title>
<link>https://arxiv.org/abs/2509.02237</link>
<guid>https://arxiv.org/abs/2509.02237</guid>
<content:encoded><![CDATA[
arXiv:2509.02237v1 Announce Type: cross 
Abstract: We propose a non-intrusive, Autoencoder-based framework for reduced-order modeling in continuum mechanics. Our method integrates three stages: (i) an unsupervised Autoencoder compresses high-dimensional finite element solutions into a compact latent space, (ii) a supervised regression network maps problem parameters to latent codes, and (iii) an end-to-end surrogate reconstructs full-field solutions directly from input parameters.
  To overcome limitations of existing approaches, we propose two key extensions: a force-augmented variant that jointly predicts displacement fields and reaction forces at Neumann boundaries, and a multi-field architecture that enables coupled field predictions, such as in thermo-mechanical systems. The framework is validated on nonlinear benchmark problems involving heterogeneous composites, anisotropic elasticity with geometric variation, and thermo-mechanical coupling. Across all cases, it achieves accurate reconstructions of high-fidelity solutions while remaining fully non-intrusive.
  These results highlight the potential of combining deep learning with dimensionality reduction to build efficient and extensible surrogate models. Our publicly available implementation provides a foundation for integrating data-driven model order reduction into uncertainty quantification, optimization, and digital twin applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VariAntNet: Learning Decentralized Control of Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.02271</link>
<guid>https://arxiv.org/abs/2509.02271</guid>
<content:encoded><![CDATA[
arXiv:2509.02271v1 Announce Type: cross 
Abstract: A simple multi-agent system can be effectively utilized in disaster response applications, such as firefighting. Such a swarm is required to operate in complex environments with limited local sensing and no reliable inter-agent communication or centralized control. These simple robotic agents, also known as Ant Robots, are defined as anonymous agents that possess limited sensing capabilities, lack a shared coordinate system, and do not communicate explicitly with one another. A key challenge for simple swarms lies in maintaining cohesion and avoiding fragmentation despite limited-range sensing. Recent advances in machine learning offer effective solutions to some of the classical decentralized control challenges. We propose VariAntNet, a deep learning-based decentralized control model designed to facilitate agent swarming and collaborative task execution. VariAntNet includes geometric features extraction from unordered, variable-sized local observations. It incorporates a neural network architecture trained with a novel, differentiable, multi-objective, mathematically justified loss function that promotes swarm cohesiveness by utilizing the properties of the visibility graph Laplacian matrix. VariAntNet is demonstrated on the fundamental multi-agent gathering task, where agents with bearing-only and limited-range sensing must gather at some location. VariAntNet significantly outperforms an existing analytical solution, achieving more than double the convergence rate while maintaining high swarm connectivity across varying swarm sizes. While the analytical solution guarantees cohesion, it is often too slow in practice. In time-critical scenarios, such as emergency response operations where lives are at risk, slower analytical methods are impractical and justify the loss of some agents within the swarm. This paper presents and analyzes this trade-off in detail.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look: AI at Work! - Analysing Key Aspects of AI-support at the Work Place</title>
<link>https://arxiv.org/abs/2509.02274</link>
<guid>https://arxiv.org/abs/2509.02274</guid>
<content:encoded><![CDATA[
arXiv:2509.02274v1 Announce Type: cross 
Abstract: In this paper we present an analysis of technological and psychological factors of applying artificial intelligence (AI) at the work place. We do so for a number of twelve application cases in the context of a project where AI is integrated at work places and in work systems of the future. From a technological point of view we mainly look at the areas of AI that the applications are concerned with. This allows to formulate recommendations in terms of what to look at in developing an AI application and what to pay attention to with regards to building AI literacy with different stakeholders using the system. This includes the importance of high-quality data for training learning-based systems as well as the integration of human expertise, especially with knowledge-based systems. In terms of the psychological factors we derive research questions to investigate in the development of AI supported work systems and to consider in future work, mainly concerned with topics such as acceptance, openness, and trust in an AI system.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think2Sing: Orchestrating Structured Motion Subtitles for Singing-Driven 3D Head Animation</title>
<link>https://arxiv.org/abs/2509.02278</link>
<guid>https://arxiv.org/abs/2509.02278</guid>
<content:encoded><![CDATA[
arXiv:2509.02278v1 Announce Type: cross 
Abstract: Singing-driven 3D head animation is a challenging yet promising task with applications in virtual avatars, entertainment, and education. Unlike speech, singing involves richer emotional nuance, dynamic prosody, and lyric-based semantics, requiring the synthesis of fine-grained, temporally coherent facial motion. Existing speech-driven approaches often produce oversimplified, emotionally flat, and semantically inconsistent results, which are insufficient for singing animation. To address this, we propose Think2Sing, a diffusion-based framework that leverages pretrained large language models to generate semantically coherent and temporally consistent 3D head animations, conditioned on both lyrics and acoustics. A key innovation is the introduction of motion subtitles, an auxiliary semantic representation derived through a novel Singing Chain-of-Thought reasoning process combined with acoustic-guided retrieval. These subtitles contain precise timestamps and region-specific motion descriptions, serving as interpretable motion priors. We frame the task as a motion intensity prediction problem, enabling finer control over facial regions and improving the modeling of expressive motion. To support this, we create a multimodal singing dataset with synchronized video, acoustic descriptors, and motion subtitles, enabling diverse and expressive motion learning. Extensive experiments show that Think2Sing outperforms state-of-the-art methods in realism, expressiveness, and emotional fidelity, while also offering flexible, user-controllable animation editing.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2509.02330</link>
<guid>https://arxiv.org/abs/2509.02330</guid>
<content:encoded><![CDATA[
arXiv:2509.02330v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have demonstrated impressive capabilities in code-related tasks, such as code generation and automated program repair. Despite their promising performance, most existing approaches for code repair suffer from high training costs or computationally expensive inference. Retrieval-augmented generation (RAG), with its efficient in-context learning paradigm, offers a more scalable alternative. However, conventional retrieval strategies, which are often based on holistic code-text embeddings, fail to capture the structural intricacies of code, resulting in suboptimal retrieval quality. To address the above limitations, we propose ReCode, a fine-grained retrieval-augmented in-context learning framework designed for accurate and efficient code repair. Specifically, ReCode introduces two key innovations: (1) an algorithm-aware retrieval strategy that narrows the search space using preliminary algorithm type predictions; and (2) a modular dual-encoder architecture that separately processes code and textual inputs, enabling fine-grained semantic matching between input and retrieved contexts. Furthermore, we propose RACodeBench, a new benchmark constructed from real-world user-submitted buggy code, which addresses the limitations of synthetic benchmarks and supports realistic evaluation. Experimental results on RACodeBench and competitive programming datasets demonstrate that ReCode achieves higher repair accuracy with significantly reduced inference cost, highlighting its practical value for real-world code repair scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCPO: Dynamic Clipping Policy Optimization</title>
<link>https://arxiv.org/abs/2509.02333</link>
<guid>https://arxiv.org/abs/2509.02333</guid>
<content:encoded><![CDATA[
arXiv:2509.02333v1 Announce Type: cross 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces a dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO (20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDIT: Residual-based Diffusion Implicit Models for Probabilistic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.02341</link>
<guid>https://arxiv.org/abs/2509.02341</guid>
<content:encoded><![CDATA[
arXiv:2509.02341v1 Announce Type: cross 
Abstract: Probabilistic Time Series Forecasting (PTSF) plays a critical role in domains requiring accurate and uncertainty-aware predictions for decision-making. However, existing methods offer suboptimal distribution modeling and suffer from a mismatch between training and evaluation metrics. Surprisingly, we found that augmenting a strong point estimator with a zero-mean Gaussian, whose standard deviation matches its training error, can yield state-of-the-art performance in PTSF. In this work, we propose RDIT, a plug-and-play framework that combines point estimation and residual-based conditional diffusion with a bidirectional Mamba network. We theoretically prove that the Continuous Ranked Probability Score (CRPS) can be minimized by adjusting to an optimal standard deviation and then derive algorithms to achieve distribution matching. Evaluations on eight multivariate datasets across varied forecasting horizons demonstrate that RDIT achieves lower CRPS, rapid inference, and improved coverage compared to strong baselines.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation</title>
<link>https://arxiv.org/abs/2509.02349</link>
<guid>https://arxiv.org/abs/2509.02349</guid>
<content:encoded><![CDATA[
arXiv:2509.02349v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have been widely applied in speech and music. This tendency has led to a focus on audio tokenization for Large Models (LMs). Unlike semantic-only text tokens, audio tokens must both capture global semantic content and preserve fine-grained acoustic details. Moreover, they provide a discrete method for speech and music that can be effectively integrated into MLLMs. However, existing research is unsuitable in the definitions of semantic tokens and acoustic tokens. In addition, the evaluation of different codecs typically concentrates on specific domains or tasks, such as reconstruction or Automatic Speech Recognition (ASR) task, which prevents fair and comprehensive comparisons. To address these problems, this paper provides suitable definitions for semantic and acoustic tokens and introduces a systematic evaluation framework. This framework allows for a comprehensive assessment of codecs' capabilities which evaluate across four dimensions: audio reconstruction metric, codebook index (ID) stability, decoder-only transformer perplexity, and performance on downstream probe tasks. Our results show the correctness of the provided suitable definitions and the correlation among reconstruction metrics, codebook ID stability, downstream probe tasks and perplexity.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Reasoning in Large Language Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2509.02350</link>
<guid>https://arxiv.org/abs/2509.02350</guid>
<content:encoded><![CDATA[
arXiv:2509.02350v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated strong generalization across a wide range of tasks. Reasoning with LLMs is central to solving multi-step problems and complex decision-making. To support efficient reasoning, recent studies have shifted attention from explicit chain-of-thought prompting toward implicit reasoning, where reasoning occurs silently via latent structures without emitting intermediate textual steps. Implicit reasoning brings advantages such as lower generation cost, faster inference, and better alignment with internal computation. Although prior surveys have discussed latent representations in the context of reasoning, a dedicated and mechanism-level examination of how reasoning unfolds internally within LLMs remains absent. This survey fills that gap by introducing a taxonomy centered on execution paradigms, shifting the focus from representational forms to computational strategies. We organize existing methods into three execution paradigms based on \textbf{\textit{how and where internal computation unfolds}}: latent optimization, signal-guided control, and layer-recurrent execution. We also review structural, behavioral and representation-based evidence that supports the presence of implicit reasoning in LLMs. We further provide a structured overview of the evaluation metrics and benchmarks used in existing works to assess the effectiveness and reliability of implicit reasoning.We maintain a continuously updated project at: https://github.com/digailab/awesome-llm-implicit-reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ordinal Adaptive Correction: A Data-Centric Approach to Ordinal Image Classification with Noisy Labels</title>
<link>https://arxiv.org/abs/2509.02351</link>
<guid>https://arxiv.org/abs/2509.02351</guid>
<content:encoded><![CDATA[
arXiv:2509.02351v1 Announce Type: cross 
Abstract: Labeled data is a fundamental component in training supervised deep learning models for computer vision tasks. However, the labeling process, especially for ordinal image classification where class boundaries are often ambiguous, is prone to error and noise. Such label noise can significantly degrade the performance and reliability of machine learning models. This paper addresses the problem of detecting and correcting label noise in ordinal image classification tasks. To this end, a novel data-centric method called ORDinal Adaptive Correction (ORDAC) is proposed for adaptive correction of noisy labels. The proposed approach leverages the capabilities of Label Distribution Learning (LDL) to model the inherent ambiguity and uncertainty present in ordinal labels. During training, ORDAC dynamically adjusts the mean and standard deviation of the label distribution for each sample. Rather than discarding potentially noisy samples, this approach aims to correct them and make optimal use of the entire training dataset. The effectiveness of the proposed method is evaluated on benchmark datasets for age estimation (Adience) and disease severity detection (Diabetic Retinopathy) under various asymmetric Gaussian noise scenarios. Results show that ORDAC and its extended versions (ORDAC_C and ORDAC_R) lead to significant improvements in model performance. For instance, on the Adience dataset with 40% noise, ORDAC_R reduced the mean absolute error from 0.86 to 0.62 and increased the recall metric from 0.37 to 0.49. The method also demonstrated its effectiveness in correcting intrinsic noise present in the original datasets. This research indicates that adaptive label correction using label distributions is an effective strategy to enhance the robustness and accuracy of ordinal classification models in the presence of noisy data.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guidance and Control Neural Network Acceleration using Memristors</title>
<link>https://arxiv.org/abs/2509.02369</link>
<guid>https://arxiv.org/abs/2509.02369</guid>
<content:encoded><![CDATA[
arXiv:2509.02369v1 Announce Type: cross 
Abstract: In recent years, the space community has been exploring the possibilities of Artificial Intelligence (AI), specifically Artificial Neural Networks (ANNs), for a variety of on board applications. However, this development is limited by the restricted energy budget of smallsats and cubesats as well as radiation concerns plaguing modern chips. This necessitates research into neural network accelerators capable of meeting these requirements whilst satisfying the compute and performance needs of the application. This paper explores the use of Phase-Change Memory (PCM) and Resistive Random-Access Memory (RRAM) memristors for on-board in-memory computing AI acceleration in space applications. A guidance and control neural network (G\&amp;CNET) accelerated using memristors is simulated in a variety of scenarios and with both device types to evaluate the performance of memristor-based accelerators, considering device non-idealities such as noise and conductance drift. We show that the memristive accelerator is able to learn the expert actions, though challenges remain with the impact of noise on accuracy. We also show that re-training after degradation is able to restore performance to nominal levels. This study provides a foundation for future research into memristor-based AI accelerators for space, highlighting their potential and the need for further investigation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs</title>
<link>https://arxiv.org/abs/2509.02372</link>
<guid>https://arxiv.org/abs/2509.02372</guid>
<content:encoded><![CDATA[
arXiv:2509.02372v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become critical to modern software development, but their reliance on internet datasets for training introduces a significant security risk: the absorption and reproduction of malicious content. To evaluate this threat, this paper introduces a scalable, automated audit framework that synthesizes innocuous, developer-style prompts from known scam databases to query production LLMs and determine if they generate code containing harmful URLs. We conducted a large-scale evaluation across four production LLMs (GPT-4o, GPT-4o-mini, Llama-4-Scout, and DeepSeek-V3), and found a systemic vulnerability, with all tested models generating malicious code at a non-negligible rate. On average, 4.2\% of programs generated in our experiments contained malicious URLs. Crucially, this malicious code is often generated in response to benign prompts. We manually validate the prompts which cause all four LLMs to generate malicious code, and resulting in 177 innocuous prompts that trigger all models to produce harmful outputs. These results provide strong empirical evidence that the training data of production LLMs has been successfully poisoned at scale, underscoring the urgent need for more robust defense mechanisms and post-generation safety checks to mitigate the propagation of hidden security threats.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time ML-based Defense Against Malicious Payload in Reconfigurable Embedded Systems</title>
<link>https://arxiv.org/abs/2509.02387</link>
<guid>https://arxiv.org/abs/2509.02387</guid>
<content:encoded><![CDATA[
arXiv:2509.02387v1 Announce Type: cross 
Abstract: The growing use of FPGAs in reconfigurable systems introducessecurity risks through malicious bitstreams that could cause denial-of-service (DoS), data leakage, or covert attacks. We investigated chip-level hardware malicious payload in embedded systems and proposed a supervised machine learning method to detect malicious bitstreams via static byte-level features. Our approach diverges from existing methods by analyzing bitstreams directly at the binary level, enabling real-time detection without requiring access to source code or netlists. Bitstreams were sourced from state-of-the-art (SOTA) benchmarks and re-engineered to target the Xilinx PYNQ-Z1 FPGA Development Board. Our dataset included 122 samples of benign and malicious configurations. The data were vectorized using byte frequency analysis, compressed using TSVD, and balanced using SMOTE to address class imbalance. The evaluated classifiers demonstrated that Random Forest achieved a macro F1-score of 0.97, underscoring the viability of real-time Trojan detection on resource-constrained systems. The final model was serialized and successfully deployed via PYNQ to enable integrated bitstream analysis.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey: Towards Privacy and Security in Mobile Large Language Models</title>
<link>https://arxiv.org/abs/2509.02411</link>
<guid>https://arxiv.org/abs/2509.02411</guid>
<content:encoded><![CDATA[
arXiv:2509.02411v1 Announce Type: cross 
Abstract: Mobile Large Language Models (LLMs) are revolutionizing diverse fields such as healthcare, finance, and education with their ability to perform advanced natural language processing tasks on-the-go. However, the deployment of these models in mobile and edge environments introduces significant challenges related to privacy and security due to their resource-intensive nature and the sensitivity of the data they process. This survey provides a comprehensive overview of privacy and security issues associated with mobile LLMs, systematically categorizing existing solutions such as differential privacy, federated learning, and prompt encryption. Furthermore, we analyze vulnerabilities unique to mobile LLMs, including adversarial attacks, membership inference, and side-channel attacks, offering an in-depth comparison of their effectiveness and limitations. Despite recent advancements, mobile LLMs face unique hurdles in achieving robust security while maintaining efficiency in resource-constrained environments. To bridge this gap, we propose potential applications, discuss open challenges, and suggest future research directions, paving the way for the development of trustworthy, privacy-compliant, and scalable mobile LLM systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2509.02419</link>
<guid>https://arxiv.org/abs/2509.02419</guid>
<content:encoded><![CDATA[
arXiv:2509.02419v1 Announce Type: cross 
Abstract: The effectiveness of convolutional neural networks in medical image segmentation relies on large-scale, high-quality annotations, which are costly and time-consuming to obtain. Even expert-labeled datasets inevitably contain noise arising from subjectivity and coarse delineations, which disrupt feature learning and adversely impact model performance. To address these challenges, this study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which integrates geometric and structural cues to improve robustness against noisy annotations. It incorporates a Geometric Distance-Aware module that dynamically adjusts pixel-level weights using geometric features, thereby strengthening supervision in reliable regions while suppressing noise. A Structure-Guided Label Refinement module further refines labels with structural priors, and a Knowledge Transfer module enriches supervision and improves sensitivity to local details. To comprehensively assess its effectiveness, we evaluated GSD-Net on six publicly available datasets: four containing three types of simulated label noise, and two with multi-expert annotations that reflect real-world subjectivity and labeling inconsistencies. Experimental results demonstrate that GSD-Net achieves state-of-the-art performance under noisy annotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen, 8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of this study are available at https://github.com/ortonwang/GSD-Net.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions</title>
<link>https://arxiv.org/abs/2509.02452</link>
<guid>https://arxiv.org/abs/2509.02452</guid>
<content:encoded><![CDATA[
arXiv:2509.02452v1 Announce Type: cross 
Abstract: Do LLMs genuinely incorporate external definitions, or do they primarily rely on their parametric knowledge? To address these questions, we conduct controlled experiments across multiple explanation benchmark datasets (general and domain-specific) and label definition conditions, including expert-curated, LLM-generated, perturbed, and swapped definitions. Our results reveal that while explicit label definitions can enhance accuracy and explainability, their integration into an LLM's task-solving processes is neither guaranteed nor consistent, suggesting reliance on internalized representations in many cases. Models often default to their internal representations, particularly in general tasks, whereas domain-specific tasks benefit more from explicit definitions. These findings underscore the need for a deeper understanding of how LLMs process external knowledge alongside their pre-existing capabilities.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Sequential Notification Optimization via Multi-Objective Decision Transformers</title>
<link>https://arxiv.org/abs/2509.02458</link>
<guid>https://arxiv.org/abs/2509.02458</guid>
<content:encoded><![CDATA[
arXiv:2509.02458v1 Announce Type: cross 
Abstract: Notifications are an important communication channel for delivering timely and relevant information. Optimizing their delivery involves addressing complex sequential decision-making challenges under constraints such as message utility and user fatigue. Offline reinforcement learning (RL) methods, such as Conservative Q-Learning (CQL), have been applied to this problem but face practical challenges at scale, including instability, sensitivity to distribution shifts, limited reproducibility, and difficulties with explainability in high-dimensional recommendation settings. We present a Decision Transformer (DT) based framework that reframes policy learning as return-conditioned supervised learning, improving robustness, scalability, and modeling flexibility. Our contributions include a real-world comparison with CQL, a multi-reward design suitable for non-episodic tasks, a quantile regression approach to return-to-go conditioning, and a production-ready system with circular buffer-based sequence processing for near-real-time inference. Extensive offline and online experiments in a deployed notification system show that our approach improves notification utility and overall session activity while minimizing user fatigue. Compared to a multi-objective CQL-based agent, the DT-based approach achieved a +0.72% increase in sessions for notification decision-making at LinkedIn by making notification recommendation more relevant.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to Break the GPU Memory Wall</title>
<link>https://arxiv.org/abs/2509.02480</link>
<guid>https://arxiv.org/abs/2509.02480</guid>
<content:encoded><![CDATA[
arXiv:2509.02480v1 Announce Type: cross 
Abstract: Training LLMs larger than the aggregated memory of multiple GPUs is increasingly necessary due to the faster growth of LLM sizes compared to GPU memory. To this end, multi-tier host memory or disk offloading techniques are proposed by state of art. Despite advanced asynchronous multi-tier read/write strategies, such offloading strategies result in significant I/O overheads in the critical path of training, resulting in slower iterations. To this end, we propose MLP-Offload, a novel multi-level, multi-path offloading engine specifically designed for optimizing LLM training on resource-constrained setups by mitigating I/O bottlenecks. We make several key observations that drive the design of MLP-Offload, such as I/O overheads during the update dominate the iteration time; I/O bandwidth of the third-level remote storage tier remains unutilized; and, contention due to concurrent offloading amplifies I/O bottlenecks. Driven by these insights, we design and implement MLP-Offload to offload the optimizer states across multiple tiers in a cache-efficient and concurrency-controlled fashion to mitigate I/O bottlenecks during the backward and update phases. Evaluations on models up to 280B parameters shows that MLP-Offload achieves 2.5$\times$ faster iterations compared to the state-of-the-art LLM training runtimes.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistically stable revision and comparative probability: a representation theorem and applications</title>
<link>https://arxiv.org/abs/2509.02495</link>
<guid>https://arxiv.org/abs/2509.02495</guid>
<content:encoded><![CDATA[
arXiv:2509.02495v1 Announce Type: cross 
Abstract: The stability rule for belief, advocated by Leitgeb [Annals of Pure and Applied Logic 164, 2013], is a rule for rational acceptance that captures categorical belief in terms of $\textit{probabilistically stable propositions}$: propositions to which the agent assigns resiliently high credence. The stability rule generates a class of $\textit{probabilistically stable belief revision}$ operators, which capture the dynamics of belief that result from an agent updating their credences through Bayesian conditioning while complying with the stability rule for their all-or-nothing beliefs. In this paper, we prove a representation theorem that yields a complete characterisation of such probabilistically stable revision operators and provides a `qualitative' selection function semantics for the (non-monotonic) logic of probabilistically stable belief revision. Drawing on the theory of comparative probability orders, this result gives necessary and sufficient conditions for a selection function to be representable as a strongest-stable-set operator on a finite probability space. The resulting logic of probabilistically stable belief revision exhibits strong monotonicity properties while failing the AGM belief revision postulates and satisfying only very weak forms of case reasoning. In showing the main theorem, we prove two results of independent interest to the theory of comparative probability: the first provides necessary and sufficient conditions for the joint representation of a pair of (respectively, strict and non-strict) comparative probability orders. The second result provides a method for axiomatising the logic of ratio comparisons of the form ``event $A$ is at least $k$ times more likely than event $B$''. In addition to these measurement-theoretic applications, we point out two applications of our main result to the theory of simple voting games and to revealed preference theory.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds</title>
<link>https://arxiv.org/abs/2509.02499</link>
<guid>https://arxiv.org/abs/2509.02499</guid>
<content:encoded><![CDATA[
arXiv:2509.02499v1 Announce Type: cross 
Abstract: The rapid advancement of large language models has intensified public concerns about the potential misuse. Therefore, it is important to build trustworthy AI-generated text detection systems. Existing methods neglect stylistic modeling and mostly rely on static thresholds, which greatly limits the detection performance. In this paper, we propose the Mixture of Stylistic Experts (MoSEs) framework that enables stylistics-aware uncertainty quantification through conditional threshold estimation. MoSEs contain three core components, namely, the Stylistics Reference Repository (SRR), the Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE). For input text, SRR can activate the appropriate reference data in SRR and provide them to CTE. Subsequently, CTE jointly models the linguistic statistical properties and semantic features to dynamically determine the optimal threshold. With a discrimination score, MoSEs yields prediction labels with the corresponding confidence level. Our framework achieves an average improvement 11.34% in detection performance compared to baselines. More inspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource case. Our code is available at https://github.com/creator-xi/MoSEs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation</title>
<link>https://arxiv.org/abs/2509.02510</link>
<guid>https://arxiv.org/abs/2509.02510</guid>
<content:encoded><![CDATA[
arXiv:2509.02510v1 Announce Type: cross 
Abstract: Large language models (LLMs), despite their impressive performance across a wide range of tasks, often struggle to balance two competing objectives in open-ended text generation: fostering diversity and creativity while preserving logical coherence. Existing truncated sampling techniques, including temperature scaling, top-\$p\$ (nucleus) sampling, and min-\$p\$ sampling, aim to manage this trade-off. However, they exhibit limitations, particularly in the effective incorporation of the confidence of the model into the corresponding sampling strategy. For example, min-\$p\$ sampling relies on a single top token as a heuristic for confidence, eventually underutilizing the information of the probability distribution. Toward effective incorporation of the confidence of the model, in this paper, we present **top-H** decoding. We first establish the theoretical foundation of the interplay between creativity and coherence in truncated sampling by formulating an **entropy-constrained minimum divergence** problem. We then prove this minimization problem to be equivalent to an **entropy-constrained mass maximization** (ECMM) problem, which is NP-hard. Finally, we present top-H decoding, a computationally efficient greedy algorithm to solve the ECMM problem. Extensive empirical evaluations demonstrate that top-H outperforms the state-of-the-art (SoTA) alternative of min-\$p\$ sampling by up to **25.63%** on creative writing benchmarks, while maintaining robustness on question-answering datasets such as GPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms that top-H indeed produces coherent outputs even at higher temperatures, where creativity is especially critical. In summary, top-H advances SoTA in open-ended text generation and can be *easily integrated* into creative writing applications. The code is available at https://github.com/ErfanBaghaei/Top-H-Decoding.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contemporary Agent Technology: LLM-Driven Advancements vs Classic Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2509.02515</link>
<guid>https://arxiv.org/abs/2509.02515</guid>
<content:encoded><![CDATA[
arXiv:2509.02515v1 Announce Type: cross 
Abstract: This contribution provides our comprehensive reflection on the contemporary agent technology, with a particular focus on the advancements driven by Large Language Models (LLM) vs classic Multi-Agent Systems (MAS). It delves into the models, approaches, and characteristics that define these new systems. The paper emphasizes the critical analysis of how the recent developments relate to the foundational MAS, as articulated in the core academic literature. Finally, it identifies key challenges and promising future directions in this rapidly evolving domain.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training</title>
<link>https://arxiv.org/abs/2509.02521</link>
<guid>https://arxiv.org/abs/2509.02521</guid>
<content:encoded><![CDATA[
arXiv:2509.02521v1 Announce Type: cross 
Abstract: Full-duplex dialog models are designed to listen and speak simultaneously with rapid responses to fast-changing user input. Among existing approaches, native full-duplex models merges different channels (e.g. listen and speak) in a single time step, overcoming the high response latency inherent to time-division multiplexing time-division multiplexing (TDM) alternatives. Yet, a key challenge remains: aligning textual monologues with audio streams that operate at different bitrates. The prevailing solution relies on word-level alignment, but this can degrade the language ability of large pre-trained models. Moreover, it requires highly accurate timestamps for every token, which introduces cascading errors and increases pre-processing costs. In this paper, we propose textual monologues in continuous tokens sequence, namely "natural" monologues, which mimics humanoid cognitive behavior in dialogs. For temporal alignment, we alternate the position of the natural monologue - leading or trailing the audio - across different training stages. This "dual" training paradigm proves highly effective in building FLM-Audio, our 7B spoken dialog model that demonstrates superior responsiveness, duplexity, and chatting experiences, as confirmed by experimental results.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots</title>
<link>https://arxiv.org/abs/2509.02530</link>
<guid>https://arxiv.org/abs/2509.02530</guid>
<content:encoded><![CDATA[
arXiv:2509.02530v1 Announce Type: cross 
Abstract: Modern robotic manipulation primarily relies on visual observations in a 2D color space for skill learning but suffers from poor generalization. In contrast, humans, living in a 3D world, depend more on physical properties-such as distance, size, and shape-than on texture when interacting with objects. Since such 3D geometric information can be acquired from widely available depth cameras, it appears feasible to endow robots with similar perceptual capabilities. Our pilot study found that using depth cameras for manipulation is challenging, primarily due to their limited accuracy and susceptibility to various types of noise. In this work, we propose Camera Depth Models (CDMs) as a simple plugin on daily-use depth cameras, which take RGB images and raw depth signals as input and output denoised, accurate metric depth. To achieve this, we develop a neural data engine that generates high-quality paired data from simulation by modeling a depth camera's noise pattern. Our results show that CDMs achieve nearly simulation-level accuracy in depth prediction, effectively bridging the sim-to-real gap for manipulation tasks. Notably, our experiments demonstrate, for the first time, that a policy trained on raw simulated depth, without the need for adding noise or real-world fine-tuning, generalizes seamlessly to real-world robots on two challenging long-horizon tasks involving articulated, reflective, and slender objects, with little to no performance degradation. We hope our findings will inspire future research in utilizing simulation data and 3D information in general robot policies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate Benchmarks for Model Merging Optimization</title>
<link>https://arxiv.org/abs/2509.02555</link>
<guid>https://arxiv.org/abs/2509.02555</guid>
<content:encoded><![CDATA[
arXiv:2509.02555v1 Announce Type: cross 
Abstract: Model merging techniques aim to integrate the abilities of multiple models into a single model. Most model merging techniques have hyperparameters, and their setting affects the performance of the merged model. Because several existing works show that tuning hyperparameters in model merging can enhance the merging outcome, developing hyperparameter optimization algorithms for model merging is a promising direction. However, its optimization process is computationally expensive, particularly in merging LLMs. In this work, we develop surrogate benchmarks for optimization of the merging hyperparameters to realize algorithm development and performance comparison at low cost. We define two search spaces and collect data samples to construct surrogate models to predict the performance of a merged model from a hyperparameter. We demonstrate that our benchmarks can predict the performance of merged models well and simulate optimization algorithm behaviors.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probability Bracket Notation: Multivariable Systems and Static Bayesian Networks</title>
<link>https://arxiv.org/abs/1207.5293</link>
<guid>https://arxiv.org/abs/1207.5293</guid>
<content:encoded><![CDATA[
arXiv:1207.5293v4 Announce Type: replace 
Abstract: We expand the Probability Bracket Notation (PBN), a symbolic framework inspired by the Dirac notation in quantum mechanics, to multivariable probability systems and static Bayesian networks (BNs). By introducing PBN for joint, marginal, and conditional probability distributions (PDs), as well as marginal and conditional expectations, we demonstrate how to express dependencies among multiple random variables concisely and manipulate them algebraically. Using the well-known Student BN as an example of probabilistic graphical models (PGMs), we show how to apply PBN to analyze predictions, inferences (using both bottom-up and top-down approaches), and expectations. We also extend PBN to BNs with continuous variables. After reviewing linear Gaussian networks, we introduce a customized Healthcare BN that includes both continuous and discrete random variables, utilizes user-specific data, and provides tailored predictions through discrete-display (DD) nodes as proxies for their continuous variable parents. Compared to traditional probability notation, PBN offers a unifying operator-like framework that simplifies the analysis of probabilistic models. This work highlights the potential of PBN as both an educational tool and a practical framework for probabilistic modeling, paving the way for applications in causal reasoning, inferences, expectations, data analytics, machine learning, and artificial intelligence.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Kuhnian Ontology for Epistemic Classification of STM Scholarly Articles</title>
<link>https://arxiv.org/abs/2002.03531</link>
<guid>https://arxiv.org/abs/2002.03531</guid>
<content:encoded><![CDATA[
arXiv:2002.03531v2 Announce Type: replace 
Abstract: Despite rapid gains in scale, research evaluation still relies on opaque, lagging proxies. To serve the scientific community, we pursue transparency: reproducible, auditable epistemic classification useful for funding and policy. Here we formalize KGX3 as a scenario-based model for mapping Kuhnian stages from research papers, prove determinism of the classification pipeline, and define the epistemic manifold that yields paradigm maps. We report validation across recent corpora, operational complexity at global scale, and governance that preserves interpretability while protecting core IP. The system delivers early, actionable signals of drift, crisis, and shift unavailable to citation metrics or citations-anchored NLP. KGX3 is the latest iteration of a deterministic epistemic engine developed since 2019, originating as Soph.io (2020), advanced as iKuhn (2024), and field-tested through Preprint Watch in 2025.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypothesis Network Planned Exploration for Rapid Meta-Reinforcement Learning Adaptation</title>
<link>https://arxiv.org/abs/2311.03701</link>
<guid>https://arxiv.org/abs/2311.03701</guid>
<content:encoded><![CDATA[
arXiv:2311.03701v2 Announce Type: replace 
Abstract: Meta-Reinforcement Learning (Meta-RL) learns optimal policies across a series of related tasks. A central challenge in Meta-RL is rapidly identifying which previously learned task is most similar to a new one, in order to adapt to it quickly. Prior approaches, despite significant success, typically rely on passive exploration strategies such as periods of random action to characterize the new task in relation to the learned ones. While sufficient when tasks are clearly distinguishable, passive exploration limits adaptation speed when informative transitions are rare or revealed only by specific behaviors. We introduce Hypothesis-Planned Exploration (HyPE), a method that actively plans sequences of actions during adaptation to efficiently identify the most similar previously learned task. HyPE operates within a joint latent space, where state-action transitions from different tasks form distinct paths. This latent-space planning approach enables HyPE to serve as a drop-in improvement for most model-based Meta-RL algorithms. By using planned exploration, HyPE achieves exponentially lower failure probability compared to passive strategies when informative transitions are sparse. On a natural language Alchemy game, HyPE identified the closest task in 65-75% of trials, far outperforming the 18-28% passive exploration baseline, and yielding up to 4x more successful adaptations under the same sample budget.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2405.14314</link>
<guid>https://arxiv.org/abs/2405.14314</guid>
<content:encoded><![CDATA[
arXiv:2405.14314v3 Announce Type: replace 
Abstract: Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at https://read-llm.github.io.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACE-CS: A Hybrid Logic-LLM System for Explainable Course Scheduling</title>
<link>https://arxiv.org/abs/2409.03671</link>
<guid>https://arxiv.org/abs/2409.03671</guid>
<content:encoded><![CDATA[
arXiv:2409.03671v3 Announce Type: replace 
Abstract: We present TRACE-CS, a novel hybrid system that combines symbolic reasoning with large language models (LLMs)to address contrastive queries in course scheduling problems. TRACE-CS leverages logic-based techniques to encode scheduling constraints and generate provably correct explanations, while utilizing an LLM to process natural language queries and refine logical explanations into user friendly responses. This system showcases how combining symbolic KR methods with LLMs creates explainable AI agents that balance logical correctness with natural language accessibility, addressing a fundamental challenge in deployed scheduling systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Coordinate without Communication under Incomplete Information</title>
<link>https://arxiv.org/abs/2409.12397</link>
<guid>https://arxiv.org/abs/2409.12397</guid>
<content:encoded><![CDATA[
arXiv:2409.12397v3 Announce Type: replace 
Abstract: Achieving seamless coordination in cooperative games is a crucial challenge in artificial intelligence, particularly when players operate under incomplete information. While communication helps, it is not always feasible. In this paper, we explore how effective coordination can be achieved without verbal communication, relying solely on observing each other's actions. Our method enables an agent to develop a strategy by interpreting its partner's action sequences as intent signals, constructing a finite-state transducer built from deterministic finite automata, one for each possible action the agent can take. Experiments show that these strategies significantly outperform uncoordinated ones and closely match the performance of coordinating via direct communication.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Human-AI Interactions</title>
<link>https://arxiv.org/abs/2409.16427</link>
<guid>https://arxiv.org/abs/2409.16427</guid>
<content:encoded><![CDATA[
arXiv:2409.16427v4 Announce Type: replace 
Abstract: AI agents are increasingly autonomous in their interactions with human users and tools, leading to increased interactional safety risks. We present HAICOSYSTEM, a framework examining AI agent safety within diverse and complex social interactions. HAICOSYSTEM features a modular sandbox environment that simulates multi-turn interactions between human users and AI agents, where the AI agents are equipped with a variety of tools (e.g., patient management platforms) to navigate diverse scenarios (e.g., a user attempting to access other patients' profiles). To examine the safety of AI agents in these interactions, we develop a comprehensive multi-dimensional evaluation framework that uses metrics covering operational, content-related, societal, and legal risks. Through running 1840 simulations based on 92 scenarios across seven domains (e.g., healthcare, finance, education), we demonstrate that HAICOSYSTEM can emulate realistic user-AI interactions and complex tool use by AI agents. Our experiments show that state-of-the-art LLMs, both proprietary and open-sourced, exhibit safety risks in over 50\% cases, with models generally showing higher risks when interacting with simulated malicious users. Our findings highlight the ongoing challenge of building agents that can safely navigate complex interactions, particularly when faced with malicious users. To foster the AI agent safety ecosystem, we release a code platform that allows practitioners to create custom scenarios, simulate interactions, and evaluate the safety and performance of their agents.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models</title>
<link>https://arxiv.org/abs/2410.10743</link>
<guid>https://arxiv.org/abs/2410.10743</guid>
<content:encoded><![CDATA[
arXiv:2410.10743v2 Announce Type: replace 
Abstract: Enabling large language models (LLMs) to effectively process and reason with graph-structured data remains a significant challenge despite their remarkable success in natural language tasks. Current approaches either convert graph structures into verbose textual descriptions, consuming substantial computational resources, or employ complex graph neural networks as tokenizers, which introduce significant training overhead. To bridge this gap, we present NT-LLM, a novel framework with an anchor-based positional encoding scheme for graph representation. Our approach strategically selects reference nodes as anchors and encodes each node's position relative to these anchors, capturing essential topological information without the computational burden of existing methods. Notably, we identify and address a fundamental issue: the inherent misalignment between discrete hop-based distances in graphs and continuous distances in embedding spaces. By implementing a rank-preserving objective for positional encoding pretraining, NT-LLM achieves superior performance across diverse graph tasks ranging from basic structural analysis to complex reasoning scenarios. Our comprehensive evaluation demonstrates that this lightweight yet powerful approach effectively enhances LLMs' ability to understand and reason with graph-structured information, offering an efficient solution for graph-based applications of language models.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Impact of Human Feedback via Influence Functions</title>
<link>https://arxiv.org/abs/2501.05790</link>
<guid>https://arxiv.org/abs/2501.05790</guid>
<content:encoded><![CDATA[
arXiv:2501.05790v3 Announce Type: replace 
Abstract: In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn suitable reward models from human feedback to align large language models (LLMs) with human intentions. However, human feedback can often be noisy, inconsistent, or biased, especially when evaluating complex responses. Such feedback can lead to misaligned reward signals, potentially causing unintended side effects during the RLHF process. To address these challenges, we explore the use of influence functions to measure the impact of human feedback on the performance of reward models. We propose a compute-efficient approximation method that enables the application of influence functions to LLM-based reward models and large-scale preference datasets. Our experiments showcase two key applications of influence functions: (1) detecting common labeler biases in human feedback datasets and (2) guiding labelers in refining their strategies to better align with expert feedback. By quantifying the impact of human feedback, we believe that influence functions can enhance feedback interpretability and contribute to scalable oversight in RLHF, helping labelers provide more accurate and consistent feedback. Source code is available at https://github.com/mintaywon/IF_RLHF
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Link Configuration for Satellite Communication Using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.08220</link>
<guid>https://arxiv.org/abs/2501.08220</guid>
<content:encoded><![CDATA[
arXiv:2501.08220v2 Announce Type: replace 
Abstract: Satellite communication is a key technology in our modern connected world. With increasingly complex hardware, one challenge is to efficiently configure links (connections) on a satellite transponder. Planning an optimal link configuration is extremely complex and depends on many parameters and metrics. The optimal use of the limited resources, bandwidth and power of the transponder is crucial. Such an optimization problem can be approximated using metaheuristic methods such as simulated annealing, but recent research results also show that reinforcement learning can achieve comparable or even better performance in optimization methods. However, there have not yet been any studies on link configuration on satellite transponders. In order to close this research gap, a transponder environment was developed as part of this work. For this environment, the performance of the reinforcement learning algorithm PPO was compared with the metaheuristic simulated annealing in two experiments. The results show that Simulated Annealing delivers better results for this static problem than the PPO algorithm, however, the research in turn also underlines the potential of reinforcement learning for optimization problems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling</title>
<link>https://arxiv.org/abs/2501.19306</link>
<guid>https://arxiv.org/abs/2501.19306</guid>
<content:encoded><![CDATA[
arXiv:2501.19306v4 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have created new opportunities to enhance performance on complex reasoning tasks by leveraging test-time computation. However, existing scaling methods have key limitations: parallel methods like repeated sampling are often inefficient and quickly saturate, while sequential methods like SELF-REFINE struggle to improve after a few rounds. Although combining these approaches shows promise, current methods require fine-tuned reward and revision models. This paper proposes Self-Enhanced Test-Time Scaling (SETS), a simple yet effective approach that overcomes these limitations by strategically combining parallel and sequential techniques and fully leveraging LLMs' self-improvement abilities. SETS exploits the inherent self-verification and self-correction capabilities of LLMs, unifying sampling, verification, and correction within a single framework. This facilitates efficient and scalable test-time computation for enhanced performance on complex tasks without any model training. Our comprehensive experimental results on challenging benchmarks spanning planning, reasoning, math, and coding demonstrate that SETS achieves significant performance improvements and more advantageous test-time scaling behavior than the alternatives.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation</title>
<link>https://arxiv.org/abs/2502.11649</link>
<guid>https://arxiv.org/abs/2502.11649</guid>
<content:encoded><![CDATA[
arXiv:2502.11649v3 Announce Type: replace 
Abstract: We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychology such as confirmation bias, resource constraints, and influence penalties. Our simulation features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating messages that propagate or counter misinformation. This framework integrates resource optimisation into the agents' decision-making process. Our findings demonstrate that while higher confirmation bias strengthens opinion alignment within groups, it also exacerbates overall polarisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs. Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and diminished long-term influence
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LapSum - One Method to Differentiate Them All: Ranking, Sorting and Top-k Selection</title>
<link>https://arxiv.org/abs/2503.06242</link>
<guid>https://arxiv.org/abs/2503.06242</guid>
<content:encoded><![CDATA[
arXiv:2503.06242v2 Announce Type: replace 
Abstract: We present a novel technique for constructing differentiable order-type operations, including soft ranking, soft top-k selection, and soft permutations. Our approach leverages an efficient closed-form formula for the inverse of the function LapSum, defined as the sum of Laplace distributions. This formulation ensures low computational and memory complexity in selecting the highest activations, enabling losses and gradients to be computed in $O(n\log{}n)$ time. Through extensive experiments, we demonstrate that our method outperforms state-of-the-art techniques for high-dimensional vectors and large $k$ values. Furthermore, we provide efficient implementations for both CPU and CUDA environments, underscoring the practicality and scalability of our method for large-scale ranking and differentiable ordering problems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation</title>
<link>https://arxiv.org/abs/2503.20425</link>
<guid>https://arxiv.org/abs/2503.20425</guid>
<content:encoded><![CDATA[
arXiv:2503.20425v3 Announce Type: replace 
Abstract: Navigating in environments alongside humans requires agents to reason under uncertainty and account for the beliefs and intentions of those around them. Under a sequential decision-making framework, egocentric navigation can naturally be represented as a Markov Decision Process (MDP). However, social navigation additionally requires reasoning about the hidden beliefs of others, inherently leading to a Partially Observable Markov Decision Process (POMDP), where agents lack direct access to others' mental states. Inspired by Theory of Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based reinforcement learning architecture for social navigation, addressing the challenge of belief tracking in partially observable environments; and (2) a perspective-shift operator for belief estimation, leveraging recent work on Influence-based Abstractions (IBA) in structured multi-agent settings.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Macro Causal Effects in a C-DMG over ADMGs</title>
<link>https://arxiv.org/abs/2504.01551</link>
<guid>https://arxiv.org/abs/2504.01551</guid>
<content:encoded><![CDATA[
arXiv:2504.01551v2 Announce Type: replace 
Abstract: Causal effect identification using causal graphs is a fundamental challenge in causal inference. While extensive research has been conducted in this area, most existing methods assume the availability of fully specified directed acyclic graphs or acyclic directed mixed graphs. However, in complex domains such as medicine and epidemiology, complete causal knowledge is often unavailable, and only partial information about the system is accessible. This paper focuses on causal effect identification within partially specified causal graphs, with particular emphasis on cluster-directed mixed graphs (C-DMGs) which can represent many different acyclic directed mixed graphs (ADMGs). These graphs provide a higher-level representation of causal relationships by grouping variables into clusters, offering a more practical approach for handling complex systems. Unlike fully specified ADMGs, C-DMGs can contain cycles, which complicate their analysis and interpretation. Furthermore, their cluster-based nature introduces new challenges, as it gives rise to two distinct types of causal effects: macro causal effects and micro causal effects, each with different properties. In this work, we focus on macro causal effects, which describe the effects of entire clusters on other clusters. We establish that the do-calculus is both sound and complete for identifying these effects in C-DMGs over ADMGs when the cluster sizes are either unknown or of size greater than one. Additionally, we provide a graphical characterization of non-identifiability for macro causal effects in these graphs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lemmanaid: Neuro-Symbolic Lemma Conjecturing</title>
<link>https://arxiv.org/abs/2504.04942</link>
<guid>https://arxiv.org/abs/2504.04942</guid>
<content:encoded><![CDATA[
arXiv:2504.04942v4 Announce Type: replace 
Abstract: Automatically conjecturing useful, interesting and novel lemmas would greatly improve automated reasoning tools and lower the bar for formalizing mathematics in proof assistants. It is however a very challenging task for both neural and symbolic approaches. We present the first steps towards a practical neuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language Models (LLMs) and symbolic methods, and evaluate it on proof libraries for the Isabelle proof assistant. We train an LLM to generate lemma templates that describe the shape of a lemma, and use symbolic methods to fill in the details. We compare Lemmanaid against an LLM trained to generate complete lemma statements as well as previous fully symbolic conjecturing methods. Lemmanaid outperforms both neural and symbolic methods on test sets from Isabelle's HOL library and from its Archive of Formal Proofs, discovering between 29-39.5% of the gold standard human written lemmas. This is 8-15% more lemmas than the neural-only method. By leveraging the best of both symbolic and neural methods we can generate useful lemmas for a wide range of input domains, facilitating computer-assisted theory development and formalization.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment</title>
<link>https://arxiv.org/abs/2504.14232</link>
<guid>https://arxiv.org/abs/2504.14232</guid>
<content:encoded><![CDATA[
arXiv:2504.14232v2 Announce Type: replace 
Abstract: This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz, an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured framework for categorizing educational objectives into hierarchical cognitive levels. Our research investigates whether incorporating this taxonomy can improve the alignment of AI-generated questions with specific cognitive objectives. We developed a dataset of 3691 questions categorized according to Bloom's levels and employed various classification models-Multinomial Logistic Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a Transformer-based model (DistilBERT)-to evaluate their effectiveness in categorizing questions. Our results indicate that higher Bloom's levels generally correlate with increased question length, Flesch-Kincaid Grade Level (FKGL), and Lexical Density (LD), reflecting the increased complexity of higher cognitive demands. Multinomial Logistic Regression showed varying accuracy across Bloom's levels, performing best for "Knowledge" and less accurately for higher-order levels. Merging higher-level categories improved accuracy for complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective classification for lower levels but struggled with higher-order tasks. DistilBERT achieved the highest performance, significantly improving classification of both lower and higher-order cognitive levels, achieving an overall validation accuracy of 91%. This study highlights the potential of integrating Bloom's Taxonomy into AI-driven assessment tools and underscores the advantages of advanced models like DistilBERT for enhancing educational content generation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECAST: Strengthening LLMs' Complex Instruction Following with Constraint-Verifiable Data</title>
<link>https://arxiv.org/abs/2505.19030</link>
<guid>https://arxiv.org/abs/2505.19030</guid>
<content:encoded><![CDATA[
arXiv:2505.19030v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly expected to tackle complex tasks, driven by their expanding applications and users' growing proficiency in crafting sophisticated prompts. However, as the number of explicitly stated requirements increases (particularly more than 10 constraints), LLMs often struggle to accurately follow such complex instructions. To address this challenge, we propose RECAST, a novel framework for synthesizing datasets where each example incorporates far more constraints than those in existing benchmarks. These constraints are extracted from real-world prompt-response pairs to ensure practical relevance. RECAST enables automatic verification of constraint satisfaction via rule-based validators for quantitative constraints and LLM-based validators for qualitative ones. Using this framework, we construct RECAST-30K, a large-scale, high-quality dataset comprising 30k instances spanning 15 constraint types. Experimental results demonstrate that models fine-tuned on RECAST-30K show substantial improvements in following complex instructions. Moreover, the verifiability provided by RECAST enables the design of reward functions for reinforcement learning, which further boosts model performance on complex and challenging tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation</title>
<link>https://arxiv.org/abs/2506.02397</link>
<guid>https://arxiv.org/abs/2506.02397</guid>
<content:encoded><![CDATA[
arXiv:2506.02397v2 Announce Type: replace 
Abstract: Recent advanced large reasoning models (LRMs) leverage extended chain-of-thought (CoT) reasoning to solve complex tasks, achieving state-of-the-art performance. Despite their success, we identify a critical issue: a substantial portion of simple tasks solved by LRMs can also be addressed by non-reasoning LLMs using significantly fewer tokens, indicating the complex reasoning may not always be necessary. To address this, we systematically analyze the reasoning trajectories of LRMs and present a method utilizing identified paradigms and LLM-Judge to classify these trajectories as either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1, a method that prunes redundant reasoning steps while preserving logical validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking) for straightforward problems while engaging in deliberate thinking (slow-thinking) for complex problems. Experiments across mathematical and question-answering tasks demonstrate that OThink-R1 reduces reasoning redundancy by almost 23\% on average without compromising accuracy, offering practical guidelines for efficient reasoning models. The code is available at https://github.com/AgenticIR-Lab/OThink-R1.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Assistants for the Semiconductor Failure Analysis with LLM-Based Planning Agents</title>
<link>https://arxiv.org/abs/2506.15567</link>
<guid>https://arxiv.org/abs/2506.15567</guid>
<content:encoded><![CDATA[
arXiv:2506.15567v3 Announce Type: replace 
Abstract: Failure Analysis (FA) is a highly intricate and knowledge-intensive process. The integration of AI components within the computational infrastructure of FA labs has the potential to automate a variety of tasks, including the detection of non-conformities in images, the retrieval of analogous cases from diverse data sources, and the generation of reports from annotated images. However, as the number of deployed AI models increases, the challenge lies in orchestrating these components into cohesive and efficient workflows that seamlessly integrate with the FA process.
  This paper investigates the design and implementation of an agentic AI system for semiconductor FA using a Large Language Model (LLM)-based Planning Agent (LPA). The LPA integrates LLMs with advanced planning capabilities and external tool utilization, allowing autonomous processing of complex queries, retrieval of relevant data from external systems, and generation of human-readable responses. The evaluation results demonstrate the agent's operational effectiveness and reliability in supporting FA tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis</title>
<link>https://arxiv.org/abs/2507.07893</link>
<guid>https://arxiv.org/abs/2507.07893</guid>
<content:encoded><![CDATA[
arXiv:2507.07893v4 Announce Type: replace 
Abstract: Legal dispute analysis is crucial for intelligent legal assistance systems. However, current LLMs face significant challenges in understanding complex legal concepts, maintaining reasoning consistency, and accurately citing legal sources. This research presents a framework combining prompt engineering with multidimensional knowledge graphs to improve LLMs' legal dispute analysis. Specifically, the framework includes a three-stage hierarchical prompt structure (task definition, knowledge background, reasoning guidance) along with a three-layer knowledge graph (legal ontology, representation, instance layers). Additionally, four supporting methods enable precise legal concept retrieval: direct code matching, semantic vector similarity, ontology path reasoning, and lexical segmentation. Through extensive testing, results show major improvements: sensitivity increased by 11.1%-11.3%, specificity by 5.4%-6.0%, and citation accuracy by 29.5%-39.7%. As a result, the framework provides better legal analysis and understanding of judicial logic, thus offering a new technical method for intelligent legal assistance systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis</title>
<link>https://arxiv.org/abs/2507.08529</link>
<guid>https://arxiv.org/abs/2507.08529</guid>
<content:encoded><![CDATA[
arXiv:2507.08529v3 Announce Type: replace 
Abstract: Despite advances from medical large language models in healthcare, rare-disease diagnosis remains hampered by insufficient knowledge-representation depth, limited concept understanding, and constrained clinical reasoning. We propose a framework that couples multi-granularity sparse activation of medical concepts with a hierarchical knowledge graph. Four complementary matching algorithms, diversity control, and a five-level fallback strategy enable precise concept activation, while a three-layer knowledge graph (taxonomy, clinical features, instances) provides structured, up-to-date context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09, ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89 approaching the 0.90 clinical threshold. Expert evaluation confirms improvements in information quality, reasoning, and professional expression, suggesting our approach shortens the "diagnostic odyssey" for rare-disease patients.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pareto-NRPA: A Novel Monte-Carlo Search Algorithm for Multi-Objective Optimization</title>
<link>https://arxiv.org/abs/2507.19109</link>
<guid>https://arxiv.org/abs/2507.19109</guid>
<content:encoded><![CDATA[
arXiv:2507.19109v2 Announce Type: replace 
Abstract: We introduce Pareto-NRPA, a new Monte-Carlo algorithm designed for multi-objective optimization problems over discrete search spaces. Extending the Nested Rollout Policy Adaptation (NRPA) algorithm originally formulated for single-objective problems, Pareto-NRPA generalizes the nested search and policy update mechanism to multi-objective optimization. The algorithm uses a set of policies to concurrently explore different regions of the solution space and maintains non-dominated fronts at each level of search. Policy adaptation is performed with respect to the diversity and isolation of sequences within the Pareto front. We benchmark Pareto-NRPA on two classes of problems: a novel bi-objective variant of the Traveling Salesman Problem with Time Windows problem (MO-TSPTW), and a neural architecture search task on well-known benchmarks. Results demonstrate that Pareto-NRPA achieves competitive performance against state-of-the-art multi-objective algorithms, both in terms of convergence and diversity of solutions. Particularly, Pareto-NRPA strongly outperforms state-of-the-art evolutionary multi-objective algorithms on constrained search spaces. To our knowledge, this work constitutes the first adaptation of NRPA to the multi-objective setting.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects</title>
<link>https://arxiv.org/abs/2507.21407</link>
<guid>https://arxiv.org/abs/2507.21407</guid>
<content:encoded><![CDATA[
arXiv:2507.21407v2 Announce Type: replace 
Abstract: Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control. While most LLMs are limited in several key agentic procedures, such as reliable planning, long-term memory, tool management, and multi-agent coordination, graphs can serve as a powerful auxiliary structure to enhance structure, continuity, and coordination in complex agent workflows. Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work. Specifically, we categorize existing GLA methods by their primary functions in LLM agent systems, including planning, memory, and tool usage, and then analyze how graphs and graph learning algorithms contribute to each. For multi-agent systems, we further discuss how GLA solutions facilitate the orchestration, efficiency optimization, and trustworthiness of MAS. Finally, we highlight key future directions to advance this field, from improving structural adaptability to enabling unified, scalable, and multimodal GLA systems. We hope this paper can serve as a roadmap for future research on GLA and foster a deeper understanding of the role of graphs in LLM agent systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUMIR: an LLM-Driven Unified Agent Framework for Multi-task Infrared Spectroscopy Reasoning</title>
<link>https://arxiv.org/abs/2507.21471</link>
<guid>https://arxiv.org/abs/2507.21471</guid>
<content:encoded><![CDATA[
arXiv:2507.21471v2 Announce Type: replace 
Abstract: Infrared spectroscopy enables rapid, non destructive analysis of chemical and material properties, yet high dimensional signals and overlapping bands hinder conventional chemometric methods. Large language models (LLMs), with strong generalization and reasoning capabilities, offer new opportunities for automated spectral interpretation, but their potential in this domain remains largely untapped. This study introduces LUMIR (LLM-driven Unified agent framework for Multi-task Infrared spectroscopy Reasoning), an agent based framework designed to achieve accurate infrared spectral analysis under low data conditions. LUMIR integrates a structured literature knowledge base, automated preprocessing, feature extraction, and predictive modeling into a unified pipeline. By mining peer reviewed spectroscopy studies, it identifies validated preprocessing and feature derivation strategies, transforms spectra into low dimensional representations, and applies few-shot prompts for classification, regression, and anomaly detection. The framework was validated on diverse datasets, including the publicly available Milk near-infrared dataset, Chinese medicinal herbs, Citri Reticulatae Pericarpium(CRP) with different storage durations, an industrial wastewater COD dataset, and two additional public benchmarks, Tecator and Corn. Across these tasks, LUMIR achieved performance comparable to or surpassing established machine learning and deep learning models, particularly in resource limited settings. This work demonstrates that combining structured literature guidance with few-shot learning enables robust, scalable, and automated spectral interpretation. LUMIR establishes a new paradigm for applying LLMs to infrared spectroscopy, offering high accuracy with minimal labeled data and broad applicability across scientific and industrial domains.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning</title>
<link>https://arxiv.org/abs/2508.00271</link>
<guid>https://arxiv.org/abs/2508.00271</guid>
<content:encoded><![CDATA[
arXiv:2508.00271v2 Announce Type: replace 
Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in https://github.com/qhjqhj00/MetaAgent.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</title>
<link>https://arxiv.org/abs/2508.07407</link>
<guid>https://arxiv.org/abs/2508.07407</guid>
<content:encoded><![CDATA[
arXiv:2508.07407v2 Announce Type: replace 
Abstract: Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps</title>
<link>https://arxiv.org/abs/2508.11452</link>
<guid>https://arxiv.org/abs/2508.11452</guid>
<content:encoded><![CDATA[
arXiv:2508.11452v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have ushered in a new era of AI capabilities, demonstrating near-human-level performance across diverse scenarios. While numerous benchmarks (e.g., MMLU) and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the development of LLMs and MLLMs, most rely on static datasets or crowdsourced general-domain prompts, often falling short of reflecting performance in real-world applications. To bridge this critical gap, we present Inclusion Arena, a live leaderboard that ranks models based on human feedback collected directly from AI-powered applications. Our platform integrates pairwise model comparisons into natural user interactions, ensuring evaluations reflect practical usage scenarios. For robust model ranking, we employ the Bradley-Terry model augmented with two key innovations: (1) Placement Matches, a cold-start mechanism to quickly estimate initial ratings for newly integrated models, and (2) Proximity Sampling, an intelligent comparison strategy that prioritizes battles between models of similar capabilities to maximize information gain and enhance rating stability. Extensive empirical analyses and simulations demonstrate that Inclusion Arena yields reliable and stable rankings, exhibits higher data transitivity compared to general crowdsourced datasets, and significantly mitigates the risk of malicious manipulation. By fostering an open alliance between foundation models and real-world applications, Inclusion Arena aims to accelerate the development of LLMs and MLLMs truly optimized for practical, user-centric deployments. The platform is publicly accessible at https://www.tbox.cn/about/model-ranking.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile-Agent-v3: Fundamental Agents for GUI Automation</title>
<link>https://arxiv.org/abs/2508.15144</link>
<guid>https://arxiv.org/abs/2508.15144</guid>
<content:encoded><![CDATA[
arXiv:2508.15144v2 Announce Type: replace 
Abstract: This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Net2Brain: A Toolbox to compare artificial vision models with human brain responses</title>
<link>https://arxiv.org/abs/2208.09677</link>
<guid>https://arxiv.org/abs/2208.09677</guid>
<content:encoded><![CDATA[
arXiv:2208.09677v3 Announce Type: replace-cross 
Abstract: We introduce Net2Brain, a graphical and command-line user interface toolbox for comparing the representational spaces of artificial deep neural networks (DNNs) and human brain recordings. While different toolboxes facilitate only single functionalities or only focus on a small subset of supervised image classification models, Net2Brain allows the extraction of activations of more than 600 DNNs trained to perform a diverse range of vision-related tasks (e.g semantic segmentation, depth estimation, action recognition, etc.), over both image and video datasets. The toolbox computes the representational dissimilarity matrices (RDMs) over those activations and compares them to brain recordings using representational similarity analysis (RSA), weighted RSA, both in specific ROIs and with searchlight search. In addition, it is possible to add a new data set of stimuli and brain recordings to the toolbox for evaluation. We demonstrate the functionality and advantages of Net2Brain with an example showcasing how it can be used to test hypotheses of cognitive computational neuroscience.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modal Adapter for Vision-Language Retrieval</title>
<link>https://arxiv.org/abs/2211.09623</link>
<guid>https://arxiv.org/abs/2211.09623</guid>
<content:encoded><![CDATA[
arXiv:2211.09623v2 Announce Type: replace-cross 
Abstract: Vision-language retrieval is an important multi-modal learning topic, where the goal is to retrieve the most relevant visual candidate for a given text query. Recently, pre-trained models, e.g., CLIP, show great potential on retrieval tasks. However, as pre-trained models are scaling up, fully fine-tuning them on donwstream retrieval datasets has a high risk of overfitting. Moreover, in practice, it would be costly to train and store a large model for each task. To overcome the above issues, we present a novel Cross-Modal Adapter for parameter-efficient transfer learning. Inspired by adapter-based methods, we adjust the pre-trained model with a few parameterization layers. However, there are two notable differences. First, our method is designed for the multi-modal domain. Secondly, it allows encoder-level implicit cross-modal interactions between vision and language encoders. Although surprisingly simple, our approach has three notable benefits: (1) reduces the vast majority of fine-tuned parameters, (2) saves training time, and (3) allows all the pre-trained parameters to be fixed, enabling the pre-trained model to be shared across datasets. Extensive experiments demonstrate that, without bells and whistles, our approach outperforms adapter-based methods on image-text retrieval datasets (MSCOCO, Flickr30K) and video-text retrieval datasets (MSR-VTT, DiDeMo, and ActivityNet).
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification</title>
<link>https://arxiv.org/abs/2305.04228</link>
<guid>https://arxiv.org/abs/2305.04228</guid>
<content:encoded><![CDATA[
arXiv:2305.04228v4 Announce Type: replace-cross 
Abstract: Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural networks (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order data correlations that already exist between nodes of the same field or called attribute in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose a heterogeneous directed hypergraph (HDHG) to represent AST and a heterogeneous directed hypergraph neural network (HDHGN) to process the graph for code classification. Our method improves code understanding and can represent high-order data correlations beyond paired interactions. We assess our heterogeneous directed hypergraph neural network (HDHGN) on public datasets of Python and Java programs. Our method outperforms previous AST-based and GNN-based methods, which demonstrates the capability of our model.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Tensor Network</title>
<link>https://arxiv.org/abs/2311.11091</link>
<guid>https://arxiv.org/abs/2311.11091</guid>
<content:encoded><![CDATA[
arXiv:2311.11091v3 Announce Type: replace-cross 
Abstract: The quadratic complexity of dot-product attention introduced in Transformer remains a fundamental bottleneck impeding the progress of foundation models toward unbounded context lengths. Addressing this challenge, we introduce the Deep Tensor Network, a new architectural framework that fundamentally reformulates attention by unifying the expressive power of tensor algebra with neural network design. Our approach moves beyond both conventional dot-product attention and subsequent linear-time approximations to capture higher-order statistical dependencies. We introduce two core operators derived from this framework: \emph{Tensor Attention}, which models complex token-mixing via data-dependent polynomial kernels, and Tensor Interaction, a novel mechanism for adaptive channel-mixing. We demonstrate that these operators are powered by second-order summaries that entirely bypass the formation of $n \times n$ matrices, enabling a causality-preserving streaming implementation with $O(d^2)$ per-token updates and $O(d^2)$ state. This efficiency rivals that of modern State Space Models while retaining an attention-like formulation. The Deep Tensor Network thus provides a principled and powerful new class of building blocks for next-generation sequence models, bridging the gap between scalable computation and rich, expressive interaction modeling.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Information-Flow Perspective on Algorithmic Fairness</title>
<link>https://arxiv.org/abs/2312.10128</link>
<guid>https://arxiv.org/abs/2312.10128</guid>
<content:encoded><![CDATA[
arXiv:2312.10128v2 Announce Type: replace-cross 
Abstract: This work presents insights gained by investigating the relationship between algorithmic fairness and the concept of secure information flow. The problem of enforcing secure information flow is well-studied in the context of information security: If secret information may "flow" through an algorithm or program in such a way that it can influence the program's output, then that is considered insecure information flow as attackers could potentially observe (parts of) the secret.
  There is a strong correspondence between secure information flow and algorithmic fairness: if protected attributes such as race, gender, or age are treated as secret program inputs, then secure information flow means that these ``secret'' attributes cannot influence the result of a program. While most research in algorithmic fairness evaluation concentrates on studying the impact of algorithms (often treating the algorithm as a black-box), the concepts derived from information flow can be used both for the analysis of disparate treatment as well as disparate impact w.r.t. a structural causal model.
  In this paper, we examine the relationship between quantitative as well as qualitative information-flow properties and fairness. Moreover, based on this duality, we derive a new quantitative notion of fairness called fairness spread, which can be easily analyzed using quantitative information flow and which strongly relates to counterfactual fairness. We demonstrate that off-the-shelf tools for information-flow properties can be used in order to formally analyze a program's algorithmic fairness properties, including the new notion of fairness spread as well as established notions such as demographic parity.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Large Language Models in Resolving Environment-Related Crash Bugs: Localizing and Repairing</title>
<link>https://arxiv.org/abs/2312.10448</link>
<guid>https://arxiv.org/abs/2312.10448</guid>
<content:encoded><![CDATA[
arXiv:2312.10448v2 Announce Type: replace-cross 
Abstract: Software crash bugs cause unexpected program behaviors or even abrupt termination, thus demanding immediate resolution. However, resolving crash bugs can be challenging due to their complex root causes, which can originate from issues in the source code or external factors like third-party library dependencies. Large language models (LLMs) have shown promise in software engineering tasks. However, existing research predominantly focuses on the capability of LLMs to localize and repair code-related crash bugs, leaving their effectiveness in resolving environment-related crash bugs in real-world software unexplored. To fill this gap, we conducted the first comprehensive study to assess the capability of LLMs in resolving real-world environment-related crash bugs. We first systematically compare LLMs' performance in resolving code-related and environment-related crash bugs with varying levels of crash contextual information. Our findings reveal that localization is the primary challenge for resolving code-related crashes, while repair poses a greater challenge for environment-related crashes. Furthermore, we investigate the impact of different prompt strategies on improving the resolution of environment-related crash bugs, incorporating different prompt templates and multi-round interactions. Building on this, we further explore an advanced active inquiry prompting strategy leveraging the self-planning capabilities of LLMs. Based on these explorations, we propose IntDiagSolver, an interactive methodology designed to enable precise crash bug resolution through ongoing engagement with LLMs. Extensive evaluations of IntDiagSolver across multiple LLMs (including GPT-3.5, GPT-4, Claude, CodeLlama, DeepSeek-R1, and Qwen-3-Coder) demonstrate consistent improvements in resolution accuracy, with substantial enhancements ranging from 9.1% to 43.3% in localization and 9.1% to 53.3% in repair.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting LLM-assisted writing in scientific communication: Are we there yet?</title>
<link>https://arxiv.org/abs/2401.16807</link>
<guid>https://arxiv.org/abs/2401.16807</guid>
<content:encoded><![CDATA[
arXiv:2401.16807v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), exemplified by ChatGPT, have significantly reshaped text generation, particularly in the realm of writing assistance. While ethical considerations underscore the importance of transparently acknowledging LLM use, especially in scientific communication, genuine acknowledgment remains infrequent. A potential avenue to encourage accurate acknowledging of LLM-assisted writing involves employing automated detectors. Our evaluation of four cutting-edge LLM-generated text detectors reveals their suboptimal performance compared to a simple ad-hoc detector designed to identify abrupt writing style changes around the time of LLM proliferation. We contend that the development of specialized detectors exclusively dedicated to LLM-assisted writing detection is necessary. Such detectors could play a crucial role in fostering more authentic recognition of LLM involvement in scientific communication, addressing the current challenges in acknowledgment practices.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redesigning Traffic Signs to Mitigate Machine-Learning Patch Attacks</title>
<link>https://arxiv.org/abs/2402.04660</link>
<guid>https://arxiv.org/abs/2402.04660</guid>
<content:encoded><![CDATA[
arXiv:2402.04660v3 Announce Type: replace-cross 
Abstract: Traffic-Sign Recognition (TSR) is a critical safety component for autonomous driving. Unfortunately, however, past work has highlighted the vulnerability of TSR models to physical-world attacks, through low-cost, easily deployable adversarial patches leading to misclassification. To mitigate these threats, most defenses focus on altering the training process or modifying the inference procedure. Still, while these approaches improve adversarial robustness, TSR remains susceptible to attacks attaining substantial success rates.
  To further the adversarial robustness of TSR, this work offers a novel approach that redefines traffic-sign designs to create signs that promote robustness while remaining interpretable to humans. Our framework takes three inputs: (1) A traffic-sign standard along with modifiable features and associated constraints; (2) A state-of-the-art adversarial training method; and (3) A function for efficiently synthesizing realistic traffic-sign images. Using these user-defined inputs, the framework emits an optimized traffic-sign standard such that traffic signs generated per this standard enable training TSR models with increased adversarial robustness.
  We evaluate the effectiveness of our framework via a concrete implementation, where we allow modifying the pictograms (i.e., symbols) and colors of traffic signs. The results show substantial improvements in robustness -- with gains of up to 16.33%--24.58% in robust accuracy over state-of-the-art methods -- while benign accuracy is even improved. Importantly, a user study also confirms that the redesigned traffic signs remain easily recognizable and to human observers. Overall, the results highlight that carefully redesigning traffic signs can significantly enhance TSR system robustness without compromising human interpretability.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis</title>
<link>https://arxiv.org/abs/2403.08955</link>
<guid>https://arxiv.org/abs/2403.08955</guid>
<content:encoded><![CDATA[
arXiv:2403.08955v4 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration efficiency and safety. Risk-sensitive policy gradient methods, which incorporate both expected return and risk measures, have been explored for their ability to yield safe policies, yet their iteration complexity remains largely underexplored. In this work, we conduct a rigorous iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm with an exponential utility function. We establish an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to reach an $\epsilon$-approximate first-order stationary point (FOSP). Furthermore, we investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutral counterparts. Our analysis indicates that risk-sensitive REINFORCE can potentially converge faster. To validate our analysis, we empirically evaluate the learning performance and convergence efficiency of the risk-neutral and risk-sensitive REINFORCE algorithms in multiple environments: CartPole, MiniGrid, and Robot Navigation. Empirical results confirm that risk-sensitive cases can converge and stabilize faster compared to their risk-neutral counterparts. More details can be found on our website https://anonymous.4open.science/w/riskrl.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Impact of Black-box Deployment Strategies for Edge AI on Latency and Model Performance</title>
<link>https://arxiv.org/abs/2403.17154</link>
<guid>https://arxiv.org/abs/2403.17154</guid>
<content:encoded><![CDATA[
arXiv:2403.17154v4 Announce Type: replace-cross 
Abstract: Deciding what combination of operators to use across the Edge AI tiers to achieve specific latency and model performance requirements is an open question for MLOps engineers. This study aims to empirically assess the accuracy vs inference time trade-off of different black-box Edge AI deployment strategies, i.e., combinations of deployment operators and deployment tiers. In this paper, we conduct inference experiments involving 3 deployment operators (i.e., Partitioning, Quantization, Early Exit), 3 deployment tiers (i.e., Mobile, Edge, Cloud) and their combinations on four widely used Computer-Vision models to investigate the optimal strategies from the point of view of MLOps developers. Our findings suggest that Edge deployment using the hybrid Quantization + Early Exit operator could be preferred over non-hybrid operators (Quantization/Early Exit on Edge, Partition on Mobile-Edge) when faster latency is a concern at medium accuracy loss. However, when minimizing accuracy loss is a concern, MLOps engineers should prefer using only a Quantization operator on edge at a latency reduction or increase, respectively over the Early Exit/Partition (on edge/mobile-edge) and Quantized Early Exit (on edge) operators. In scenarios constrained by Mobile CPU/RAM resources, a preference for Partitioning across mobile and edge tiers is observed over mobile deployment. For models with smaller input data samples (such as FCN), a network-constrained cloud deployment can also be a better alternative than Mobile/Edge deployment and Partitioning strategies. For models with large input data samples (ResNet, ResNext, DUC), an edge tier having higher network/computational capabilities than Cloud/Mobile can be a more viable option than Partitioning and Mobile/Cloud deployment strategies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations</title>
<link>https://arxiv.org/abs/2404.07851</link>
<guid>https://arxiv.org/abs/2404.07851</guid>
<content:encoded><![CDATA[
arXiv:2404.07851v2 Announce Type: replace-cross 
Abstract: Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically post-edit MT with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations. Working with LLaMA-2 models, we consider prompting strategies varying the nature of feedback provided and then fine-tune the LLM to improve its ability to exploit the provided guidance. Through experiments on Chinese-English, English-German, and English-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU and COMET scores, although the benefits of fine-grained feedback are not clear. Fine-tuning helps integrate fine-grained feedback more effectively and further improves translation quality based on both automatic and human evaluation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLARE: Cognitive Load Assessment in REaltime with Multimodal Data</title>
<link>https://arxiv.org/abs/2404.17098</link>
<guid>https://arxiv.org/abs/2404.17098</guid>
<content:encoded><![CDATA[
arXiv:2404.17098v2 Announce Type: replace-cross 
Abstract: We present a novel multimodal dataset for Cognitive Load Assessment in REal-time (CLARE). The dataset contains physiological and gaze data from 24 participants with self-reported cognitive load scores as ground-truth labels. The dataset consists of four modalities, namely, Electrocardiography (ECG), Electrodermal Activity (EDA), Electroencephalogram (EEG), and Gaze tracking. To map diverse levels of mental load on participants during experiments, each participant completed four nine-minutes sessions on a computer-based operator performance and mental workload task (the MATB-II software) with varying levels of complexity in one minute segments. During the experiment, participants reported their cognitive load every 10 seconds. For the dataset, we also provide benchmark binary classification results with machine learning and deep learning models on two different evaluation schemes, namely, 10-fold and leave-one-subject-out (LOSO) cross-validation. Benchmark results show that for 10-fold evaluation, the convolutional neural network (CNN) based deep learning model achieves the best classification performance with ECG, EDA, and Gaze. In contrast, for LOSO, the best performance is achieved by the deep learning model with ECG, EDA, and EEG.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Incremental Learning in Large Language Models: A Critical Review</title>
<link>https://arxiv.org/abs/2404.18311</link>
<guid>https://arxiv.org/abs/2404.18311</guid>
<content:encoded><![CDATA[
arXiv:2404.18311v5 Announce Type: replace-cross 
Abstract: Incremental learning is the ability of systems to acquire knowledge over time, enabling their adaptation and generalization to novel tasks. It is a critical ability for intelligent, real-world systems, especially when data changes frequently or is limited. This review provides a comprehensive analysis of incremental learning in Large Language Models. It synthesizes the state-of-the-art incremental learning paradigms, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning. We demonstrate their utility for incremental learning by describing specific achievements from these related topics and their critical factors. An important finding is that many of these approaches do not update the core model, and none of them update incrementally in real-time. The paper highlights current problems and challenges for future research in the field. By consolidating the latest relevant research developments, this review offers a comprehensive understanding of incremental learning and its implications for designing and developing LLM-based learning systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Space-aware Socioeconomic Indicator Inference with Heterogeneous Graphs</title>
<link>https://arxiv.org/abs/2405.14135</link>
<guid>https://arxiv.org/abs/2405.14135</guid>
<content:encoded><![CDATA[
arXiv:2405.14135v4 Announce Type: replace-cross 
Abstract: Regional socioeconomic indicators are critical across various domains, yet their acquisition can be costly. Inferring global socioeconomic indicators from a limited number of regional samples is essential for enhancing management and sustainability in urban areas and human settlements. Current inference methods typically rely on spatial interpolation based on the assumption of spatial continuity, which does not adequately address the complex variations present within regional spaces. In this paper, we present GeoHG, the first space-aware socioeconomic indicator inference method that utilizes a heterogeneous graph-based structure to represent geospace for non-continuous inference. Extensive experiments demonstrate the effectiveness of GeoHG in comparison to existing methods, achieving an $R^2$ score exceeding 0.8 under extreme data scarcity with a masked ratio of 95\%.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Offline Data in Linear Latent Contextual Bandits</title>
<link>https://arxiv.org/abs/2405.17324</link>
<guid>https://arxiv.org/abs/2405.17324</guid>
<content:encoded><![CDATA[
arXiv:2405.17324v2 Announce Type: replace-cross 
Abstract: Leveraging offline data is an attractive way to accelerate online sequential decision-making. However, it is crucial to account for latent states in users or environments in the offline data, and latent bandits form a compelling model for doing so. In this light, we design end-to-end latent bandit algorithms capable of handing uncountably many latent states. We focus on a linear latent contextual bandit $-$ a linear bandit where each user has its own high-dimensional reward parameter in $\mathbb{R}^{d_A}$, but reward parameters across users lie in a low-rank latent subspace of dimension $d_K \ll d_A$. First, we provide an offline algorithm to learn this subspace with provable guarantees. We then present two online algorithms that utilize the output of this offline algorithm to accelerate online learning. The first enjoys $\tilde{O}(\min(d_A\sqrt{T}, d_K\sqrt{T}(1+\sqrt{d_AT/d_KN})))$ regret guarantees, so that the effective dimension is lower when the size $N$ of the offline dataset is larger. We prove a matching lower bound on regret, showing that our algorithm is minimax optimal. The second is a practical algorithm that enjoys only a slightly weaker guarantee, but is computationally efficient. We also establish the efficacy of our methods using experiments on both synthetic data and real-life movie recommendation data from MovieLens. Finally, we theoretically establish the generality of the latent bandit model by proving a de Finetti theorem for stateless decision processes.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Story Generation from Visual Inputs: Techniques, Related Tasks, and Challenges</title>
<link>https://arxiv.org/abs/2406.02748</link>
<guid>https://arxiv.org/abs/2406.02748</guid>
<content:encoded><![CDATA[
arXiv:2406.02748v2 Announce Type: replace-cross 
Abstract: Creating engaging narratives from visual data is crucial for automated digital media consumption, assistive technologies, and interactive entertainment. This survey covers methodologies used in the generation of these narratives, focusing on their principles, strengths, and limitations.
  The survey also covers tasks related to automatic story generation, such as image and video captioning, and visual question answering, as well as story generation without visual inputs. These tasks share common challenges with visual story generation and have served as inspiration for the techniques used in the field. We analyze the main datasets and evaluation metrics, providing a critical perspective on their limitations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic Test of Unlearning Using Parametric Knowledge Traces</title>
<link>https://arxiv.org/abs/2406.11614</link>
<guid>https://arxiv.org/abs/2406.11614</guid>
<content:encoded><![CDATA[
arXiv:2406.11614v3 Announce Type: replace-cross 
Abstract: The task of "unlearning" certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize "concept vectors" - parameter vectors that encode concrete concepts - and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model's susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models</title>
<link>https://arxiv.org/abs/2406.15836</link>
<guid>https://arxiv.org/abs/2406.15836</guid>
<content:encoded><![CDATA[
arXiv:2406.15836v2 Announce Type: replace-cross 
Abstract: Learning a world model for model-free Reinforcement Learning (RL) agents can significantly improve the sample efficiency by learning policies in imagination. However, building a world model for Multi-Agent RL (MARL) can be particularly challenging due to the scalability issue in a centralized architecture arising from a large number of agents, and also the non-stationarity issue in a decentralized architecture stemming from the inter-dependency among agents. To address both challenges, we propose a novel world model for MARL that learns decentralized local dynamics for scalability, combined with a centralized representation aggregation from all agents. We cast the dynamics learning as an auto-regressive sequence modeling problem over discrete tokens by leveraging the expressive Transformer architecture, in order to model complex local dynamics across different agents and provide accurate and consistent long-term imaginations. As the first pioneering Transformer-based world model for multi-agent systems, we introduce a Perceiver Transformer as an effective solution to enable centralized representation aggregation within this context. Results on Starcraft Multi-Agent Challenge (SMAC) show that it outperforms strong model-free approaches and existing model-based methods in both sample efficiency and overall performance.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to (Learn at Test Time): RNNs with Expressive Hidden States</title>
<link>https://arxiv.org/abs/2407.04620</link>
<guid>https://arxiv.org/abs/2407.04620</guid>
<content:encoded><![CDATA[
arXiv:2407.04620v4 Announce Type: replace-cross 
Abstract: Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden states. We present a practical framework for instantiating sequence modeling layers with linear complexity and expressive hidden states. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Similar to Transformer, TTT-Linear and TTT-MLP can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposing heterogeneous dynamical systems with graph neural networks</title>
<link>https://arxiv.org/abs/2407.19160</link>
<guid>https://arxiv.org/abs/2407.19160</guid>
<content:encoded><![CDATA[
arXiv:2407.19160v2 Announce Type: replace-cross 
Abstract: Natural physical, chemical, and biological dynamical systems are often complex, with heterogeneous components interacting in diverse ways. We show how simple graph neural networks can be designed to jointly learn the interaction rules and the latent heterogeneity from observable dynamics. The learned latent heterogeneity and dynamics can be used to virtually decompose the complex system which is necessary to infer and parameterize the underlying governing equations. We tested the approach with simulation experiments of interacting moving particles, vector fields, and signaling networks. While our current aim is to better understand and validate the approach with simulated data, we anticipate it to become a generally applicable tool to uncover the governing rules underlying complex dynamics observed in nature.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models</title>
<link>https://arxiv.org/abs/2407.20271</link>
<guid>https://arxiv.org/abs/2407.20271</guid>
<content:encoded><![CDATA[
arXiv:2407.20271v4 Announce Type: replace-cross 
Abstract: Recent advances in machine learning, particularly in Natural Language Processing (NLP), have produced powerful models trained on vast datasets. However, these models risk leaking sensitive information, raising privacy concerns. In response, regulatory measures such as the European Union's General Data Protection Regulation (GDPR) have driven increasing interest in Machine Unlearning techniques, which enable models to selectively forget specific data entries. Early unlearning approaches primarily relied on pre-processing methods, while more recent research has shifted towards training-based solutions. Despite their effectiveness, a key limitation persists: most methods require access to original training data, which is often unavailable. Additionally, directly applying unlearning techniques bears the cost of undermining the model's expressive capabilities. To address these challenges, we introduce the Iterative Contrastive Unlearning (ICU) framework, which consists of three core components: A Knowledge Unlearning Induction module designed to target specific knowledge for removal using an unlearning loss; A Contrastive Learning Enhancement module to preserve the model's expressive capabilities against the pure unlearning goal; And an Iterative Unlearning Refinement module that dynamically adjusts the unlearning process through ongoing evaluation and updates. Experimental results demonstrate the efficacy of our ICU method in unlearning sensitive information while maintaining the model's overall performance, offering a promising solution for privacy-conscious machine learning applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of AI Reliance</title>
<link>https://arxiv.org/abs/2408.03948</link>
<guid>https://arxiv.org/abs/2408.03948</guid>
<content:encoded><![CDATA[
arXiv:2408.03948v2 Announce Type: replace-cross 
Abstract: Although artificial intelligence (AI) systems are becoming increasingly indispensable, research into how humans rely on these systems (AI reliance) is lagging behind. To advance this research, this survey presents a novel, comprehensive sociotechnical perspective on AI reliance, essential to fully understand the phenomenon. To address these challenges, the survey introduces a categorization framework resulting in a morphological box, which guides rigorous AI reliance research. Further, the survey identifies the core influences on AI reliance within the components of a sociotechnical system and discusses current limitations alongside emerging future research avenues to form a research agenda.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEGen: Generative Backdoor into Large Language Models via Model Editing</title>
<link>https://arxiv.org/abs/2408.10722</link>
<guid>https://arxiv.org/abs/2408.10722</guid>
<content:encoded><![CDATA[
arXiv:2408.10722v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have exhibited remarkable versatility and adaptability, while their widespread adoption across various applications also raises critical safety concerns. This paper focuses on the impact of backdoored LLMs. Traditional backdoor injection methods are primarily limited to yes-or-no discriminative tasks, leading users to underestimate the potential risks of backdoored LLMs. Given the inherently generative nature of LLMs, this paper reveals that a generative backdoor injected into LLMs can expose the true safety risks in their applications. We propose an editing-based generative backdoor, named MEGen, aiming to expand the backdoor to generative tasks in a unified format of any text-to any text, leading to natural generations with a specific intention. Experiments show that MEGen achieves a high attack success rate by adjusting only a small set of local parameters with few-shot samples. Notably, we show that the backdoored model, when triggered, can freely output pre-set dangerous information while completing downstream tasks. Our work highlights that MEGen enables backdoors in LLMs to exhibit generative capabilities, causing potential safety risks by altering the generative style. The code is available at https://github.com/MonoQ-hub/MEGen.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Detection of Toxic Prompts in Large Language Models</title>
<link>https://arxiv.org/abs/2408.11727</link>
<guid>https://arxiv.org/abs/2408.11727</guid>
<content:encoded><![CDATA[
arXiv:2408.11727v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) like ChatGPT and Gemini have significantly advanced natural language processing, enabling various applications such as chatbots and automated content generation. However, these models can be exploited by malicious individuals who craft toxic prompts to elicit harmful or unethical responses. These individuals often employ jailbreaking techniques to bypass safety mechanisms, highlighting the need for robust toxic prompt detection methods. Existing detection techniques, both blackbox and whitebox, face challenges related to the diversity of toxic prompts, scalability, and computational efficiency. In response, we propose ToxicDetector, a lightweight greybox method designed to efficiently detect toxic prompts in LLMs. ToxicDetector leverages LLMs to create toxic concept prompts, uses embedding vectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP) classifier for prompt classification. Our evaluation on various versions of the LLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector achieves a high accuracy of 96.39\% and a low false positive rate of 2.00\%, outperforming state-of-the-art methods. Additionally, ToxicDetector's processing time of 0.0780 seconds per prompt makes it highly suitable for real-time applications. ToxicDetector achieves high accuracy, efficiency, and scalability, making it a practical method for toxic prompt detection in LLMs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Law of Next-Token Prediction in Large Language Models</title>
<link>https://arxiv.org/abs/2408.13442</link>
<guid>https://arxiv.org/abs/2408.13442</guid>
<content:encoded><![CDATA[
arXiv:2408.13442v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been widely employed across various application domains, yet their black-box nature poses significant challenges to understanding how these models process input data internally to make predictions. In this paper, we introduce a precise and quantitative law that governs the learning of contextualized token embeddings through intermediate layers in pre-trained LLMs for next-token prediction. Our findings reveal that each layer contributes equally to enhancing prediction accuracy, from the lowest to the highest layer -- a universal phenomenon observed across a diverse array of open-source LLMs, irrespective of their architectures or pre-training data. We demonstrate that this law offers new perspectives and actionable insights to inform and guide practices in LLM development and applications, including model scaling, pre-training tasks, and interpretation.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for Smart and Connected Infrastructures</title>
<link>https://arxiv.org/abs/2408.14875</link>
<guid>https://arxiv.org/abs/2408.14875</guid>
<content:encoded><![CDATA[
arXiv:2408.14875v2 Announce Type: replace-cross 
Abstract: The emergence of deep learning models has revolutionized various industries over the last decade, leading to a surge in connected devices and infrastructures. However, these models can be tricked into making incorrect predictions with high confidence, leading to disastrous failures and security concerns. To this end, we explore the impact of adversarial attacks on multivariate time-series forecasting and investigate methods to counter them. Specifically, we employ untargeted white-box attacks, namely the Fast Gradient Sign Method (FGSM) and the Basic Iterative Method (BIM), to poison the inputs to the training process, effectively misleading the model. We also illustrate the subtle modifications to the inputs after the attack, which makes detecting the attack using the naked eye quite difficult. Having demonstrated the feasibility of these attacks, we develop robust models through adversarial training and model hardening. We are among the first to showcase the transferability of these attacks and defenses by extrapolating our work from the benchmark electricity data to a larger, 10-year real-world data used for predicting the time-to-failure of hard disks. Our experimental results confirm that the attacks and defenses achieve the desired security thresholds, leading to a 72.41% and 94.81% decrease in RMSE for the electricity and hard disk datasets respectively after implementing the adversarial defenses.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VSLLaVA: a pipeline of large multimodal foundation model for industrial vibration signal analysis</title>
<link>https://arxiv.org/abs/2409.07482</link>
<guid>https://arxiv.org/abs/2409.07482</guid>
<content:encoded><![CDATA[
arXiv:2409.07482v2 Announce Type: replace-cross 
Abstract: While Large Multimodal Models (LMMs) excel in general multimodal tasks, they lack the domain-specific knowledge for industrial vibration signal analysis. This paper introduces VSLLaVA, a comprehensive pipeline that utilizes expert knowledge-guided instruction tuning and evaluation to create an end-to-end LMM for signal analysis. To achieve this, we construct a novel Signal-Question-Answer (SQA) dataset using an expert rule-based signal generator. This dataset facilitates a two-stage learning procedure. The first step is efficient instruction fine-tuning with Low-Rank Adaptation (LoRA), which imparts specialized signal identification capabilities. Subsequently, we designed a tailored Group Relative Policy Optimization (GRPO) to refine the reasoning capabilities and enhance classification robustness. Then, a dual-mode evaluation framework is proposed, combining an LLM referee with expert rules for semantic assessment using quantitative metrics for numerical and textual accuracy, which reveals that VSLLaVA significantly improves performance in signal type identification and parameter analysis, and makes progress in the identification and parameter analysis of fault-related signals. This research demonstrates a viable approach for developing specialized foundational models for complex industrial applications and marks a transition from conventional task-specific systems to a cohesive, interactive foundational model.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Diagram of Thought</title>
<link>https://arxiv.org/abs/2409.10038</link>
<guid>https://arxiv.org/abs/2409.10038</guid>
<content:encoded><![CDATA[
arXiv:2409.10038v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) excel at many tasks but often falter on complex problems that require structured, multi-step reasoning. We introduce the Diagram of Thought (DoT), a new framework that enables a single LLM to build and navigate a mental map of its reasoning. Instead of thinking in a straight line, the model constructs a dynamic diagram of ideas, where it can propose different lines of thought, critique its own steps, and synthesize validated insights into a final conclusion. This entire process is self-contained within the model, making it highly efficient by avoiding the complex external controllers or search algorithms required by other methods. To ensure the reliability of this process, we ground DoT in a rigorous mathematical framework from category theory. This foundation guarantees that the way the model combines information is logical, consistent, and robust, regardless of the order in which ideas were explored. The result is a more powerful and transparent reasoning process that produces a fully auditable, step-by-step trace of the LLM's thinking, bridging the gap between fluent language and formal reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Semantic Leakage in Cross-lingual Embeddings via Orthogonality Constraint</title>
<link>https://arxiv.org/abs/2409.15664</link>
<guid>https://arxiv.org/abs/2409.15664</guid>
<content:encoded><![CDATA[
arXiv:2409.15664v2 Announce Type: replace-cross 
Abstract: Accurately aligning contextual representations in cross-lingual sentence embeddings is key for effective parallel data mining. A common strategy for achieving this alignment involves disentangling semantics and language in sentence embeddings derived from multilingual pre-trained models. However, we discover that current disentangled representation learning methods suffer from semantic leakage - a term we introduce to describe when a substantial amount of language-specific information is unintentionally leaked into semantic representations. This hinders the effective disentanglement of semantic and language representations, making it difficult to retrieve embeddings that distinctively represent the meaning of the sentence. To address this challenge, we propose a novel training objective, ORthogonAlity Constraint LEarning (ORACLE), tailored to enforce orthogonality between semantic and language embeddings. ORACLE builds upon two components: intra-class clustering and inter-class separation. Through experiments on cross-lingual retrieval and semantic textual similarity tasks, we demonstrate that training with the ORACLE objective effectively reduces semantic leakage and enhances semantic alignment within the embedding space.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in complex action spaces without policy gradients</title>
<link>https://arxiv.org/abs/2410.06317</link>
<guid>https://arxiv.org/abs/2410.06317</guid>
<content:encoded><![CDATA[
arXiv:2410.06317v2 Announce Type: replace-cross 
Abstract: While conventional wisdom holds that policy gradient methods are better suited to complex action spaces than action-value methods, foundational work has shown that the two paradigms are equivalent in small, finite action spaces (O'Donoghue et al., 2017; Schulman et al., 2017a). This raises the question of why their computational applicability and performance diverge as the complexity of the action space increases. We hypothesize that the apparent superiority of policy gradients in such settings stems not from intrinsic qualities of the paradigm but from universal principles that can also be applied to action-value methods, enabling similar functions. We identify three such principles and provide a framework for incorporating them into action-value methods. To support our hypothesis, we instantiate this framework in what we term QMLE, for Q-learning with maximum likelihood estimation. Our results show that QMLE can be applied to complex action spaces at a computational cost comparable to that of policy gradient methods, all without using policy gradients. Furthermore, QMLE exhibits strong performance on the DeepMind Control Suite, even when compared to state-of-the-art methods such as DMPO and D4PG.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning by Surprise: Surplexity for Mitigating Model Collapse in Generative AI</title>
<link>https://arxiv.org/abs/2410.12341</link>
<guid>https://arxiv.org/abs/2410.12341</guid>
<content:encoded><![CDATA[
arXiv:2410.12341v3 Announce Type: replace-cross 
Abstract: As synthetic content increasingly infiltrates the web, generative AI models may be retrained on their own outputs: a process termed "autophagy". This leads to model collapse: a progressive loss of performance and diversity across generations. Recent studies have examined the emergence of model collapse across various generative AI models and data types, and have proposed mitigation strategies that rely on incorporating human-authored content. However, current characterizations of model collapse remain limited, and existing mitigation methods assume reliable knowledge of whether training data is human-authored or AI-generated. In this paper, we address these gaps by introducing new measures that characterise collapse directly from a model's next-token probability distributions, rather than from properties of AI-generated text. Using these measures, we show that the degree of collapse depends on the complexity of the initial training set, as well as on the extent of autophagy. Our experiments prompt a new suggestion: that model collapse occurs when a model trains on data that does not "surprise" it. We express this hypothesis in terms of the well-known Free Energy Principle in cognitive science. Building on this insight, we propose a practical mitigation strategy: filtering training items by high surplexity, maximising the surprise of the model. Unlike existing methods, this approach does not require distinguishing between human- and AI-generated data. Experiments across datasets and models demonstrate that our strategy is at least as effective as human-data baselines, and even more effective in reducing distributional skewedness. Our results provide a richer understanding of model collapse and point toward more resilient approaches for training generative AI systems in environments increasingly saturated with synthetic data.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synergistic Dual Spatial-aware Generation of Image-to-Text and Text-to-Image</title>
<link>https://arxiv.org/abs/2410.15312</link>
<guid>https://arxiv.org/abs/2410.15312</guid>
<content:encoded><![CDATA[
arXiv:2410.15312v2 Announce Type: replace-cross 
Abstract: In the visual spatial understanding (VSU) area, spatial image-to-text (SI2T) and spatial text-to-image (ST2I) are two fundamental tasks that appear in dual form. Existing methods for standalone SI2T or ST2I perform imperfectly in spatial understanding, due to the difficulty of 3D-wise spatial feature modeling. In this work, we consider modeling the SI2T and ST2I together under a dual learning framework. During the dual framework, we then propose to represent the 3D spatial scene features with a novel 3D scene graph (3DSG) representation that can be shared and beneficial to both tasks. Further, inspired by the intuition that the easier 3D$\to$image and 3D$\to$text processes also exist symmetrically in the ST2I and SI2T, respectively, we propose the Spatial Dual Discrete Diffusion (SD$^3$) framework, which utilizes the intermediate features of the 3D$\to$X processes to guide the hard X$\to$3D processes, such that the overall ST2I and SI2T will benefit each other. On the visual spatial understanding dataset VSD, our system outperforms the mainstream T2I and I2T methods significantly. Further in-depth analysis reveals how our dual learning strategy advances.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Kolmogorov Barrier: A Learnable Weighted Hybrid Autoencoder for Model Order Reduction</title>
<link>https://arxiv.org/abs/2410.18148</link>
<guid>https://arxiv.org/abs/2410.18148</guid>
<content:encoded><![CDATA[
arXiv:2410.18148v4 Announce Type: replace-cross 
Abstract: Representation learning for high-dimensional, complex physical systems aims to identify a low-dimensional intrinsic latent space, which is crucial for reduced-order modeling and modal analysis. To overcome the well-known Kolmogorov barrier, deep autoencoders (AEs) have been introduced in recent years, but they often suffer from poor convergence behavior as the rank of the latent space increases. To address this issue, we propose the learnable weighted hybrid autoencoder, a hybrid approach that combines the strengths of singular value decomposition (SVD) with deep autoencoders through a learnable weighted framework. We find that the introduction of learnable weighting parameters is essential -- without them, the resulting model would either collapse into a standard POD or fail to exhibit the desired convergence behavior. Interestingly, we empirically find that our trained model has a sharpness thousands of times smaller compared to other models. Our experiments on classical chaotic PDE systems, including the 1D Kuramoto-Sivashinsky and forced isotropic turbulence datasets, demonstrate that our approach significantly improves generalization performance compared to several competing methods. Additionally, when combining with time series modeling techniques (e.g., Koopman operator, LSTM), the proposed technique offers significant improvements for surrogate modeling of high-dimensional multi-scale PDE systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMoE: Fast MoE-based LLM Serving using Proactive Caching</title>
<link>https://arxiv.org/abs/2410.22134</link>
<guid>https://arxiv.org/abs/2410.22134</guid>
<content:encoded><![CDATA[
arXiv:2410.22134v3 Announce Type: replace-cross 
Abstract: The promising applications of large language models are often limited by the constrained GPU memory capacity available on edge devices. Mixture-of-Experts (MoE) models help address this issue by activating only a subset of the model's parameters during computation. This approach allows the unused parameters to be offloaded to host memory, thereby reducing the overall GPU memory demand. However, existing cache-based offloading solutions handle cache misses reactively, which significantly impacts system performance. In this paper, we introduce ProMoE, a novel proactive caching system that utilizes intermediate results to predict subsequent expert usage. By proactively fetching experts in advance, ProMoE eliminates passive cache misses, removes loading time from the critical path, and reduces the performance overhead associated with offloading. Our evaluations demonstrate that ProMoE achieves an average speedup of 2.20x (up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages, respectively, compared to existing offloading solutions.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios</title>
<link>https://arxiv.org/abs/2411.02708</link>
<guid>https://arxiv.org/abs/2411.02708</guid>
<content:encoded><![CDATA[
arXiv:2411.02708v2 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) have recently achieved state-of-the-art performance on tasks ranging from visual question answering to video understanding. However, existing studies have concentrated mainly on visual-textual misalignment, leaving largely unexplored the MLLMs' ability to preserve an originally correct answer when confronted with misleading information. We reveal a response uncertainty phenomenon: across nine standard datasets, twelve state-of-the-art open-source MLLMs overturn a previously correct answer in 65% of cases after receiving a single deceptive cue. To systematically quantify this vulnerability, we propose a two-stage evaluation pipeline: (1) elicit each model's original response on unperturbed inputs; (2) inject explicit (false-answer hints) and implicit (contextual contradictions) misleading instructions, and compute the misleading rate - the fraction of correct-to-incorrect flips. Leveraging the most susceptible examples, we curate the Multimodal Uncertainty Benchmark (MUB), a collection of image-question pairs stratified into low, medium, and high difficulty based on how many of twelve state-of-the-art MLLMs they mislead. Extensive evaluation on twelve open-source and five closed-source models reveals a high uncertainty: average misleading rates exceed 86%, with explicit cues over 67.19% and implicit cues over 80.67%. To reduce the misleading rate, we then fine-tune all open-source MLLMs on a compact 2000-sample mixed-instruction dataset, reducing misleading rates to 6.97% (explicit) and 32.77% (implicit), boosting consistency by nearly 29.37% on highly deceptive inputs, and slightly improving accuracy on standard benchmarks. Our code is available at https://github.com/Yunkaidang/uncertainty
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Method for Measuring "Open Codes" in Qualitative Analysis</title>
<link>https://arxiv.org/abs/2411.12142</link>
<guid>https://arxiv.org/abs/2411.12142</guid>
<content:encoded><![CDATA[
arXiv:2411.12142v3 Announce Type: replace-cross 
Abstract: Qualitative analysis is critical to understanding human datasets in many social science disciplines. A central method in this process is inductive coding, where researchers identify and interpret codes directly from the datasets themselves. Yet, this exploratory approach poses challenges for meeting methodological expectations (such as ``depth'' and ``variation''), especially as researchers increasingly adopt Generative AI (GAI) for support. Ground-truth-based metrics are insufficient because they contradict the exploratory nature of inductive coding, while manual evaluation can be labor-intensive. This paper presents a theory-informed computational method for measuring inductive coding results from humans and GAI. Our method first merges individual codebooks using an LLM-enriched algorithm. It measures each coder's contribution against the merged result using four novel metrics: Coverage, Overlap, Novelty, and Divergence. Through two experiments on a human-coded online conversation dataset, we 1) reveal the merging algorithm's impact on metrics; 2) validate the metrics' stability and robustness across multiple runs and different LLMs; and 3) showcase the metrics' ability to diagnose coding issues, such as excessive or irrelevant (hallucinated) codes. Our work provides a reliable pathway for ensuring methodological rigor in human-AI qualitative analysis.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EFTViT: Efficient Federated Training of Vision Transformers with Masked Images on Resource-Constrained Clients</title>
<link>https://arxiv.org/abs/2412.00334</link>
<guid>https://arxiv.org/abs/2412.00334</guid>
<content:encoded><![CDATA[
arXiv:2412.00334v2 Announce Type: replace-cross 
Abstract: Federated learning research has recently shifted from Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs) due to their superior capacity. ViTs training demands higher computational resources due to the lack of 2D inductive biases inherent in CNNs. However, efficient federated training of ViTs on resource-constrained edge devices remains unexplored in the community. In this paper, we propose EFTViT, a hierarchical federated framework that leverages masked images to enable efficient, full-parameter training on resource-constrained edge devices, offering substantial benefits for learning on heterogeneous data. In general, we patchify images and randomly mask a portion of the patches, observing that excluding them from training has minimal impact on performance while substantially reducing computation costs and enhancing data content privacy protection. Specifically, EFTViT comprises a series of lightweight local modules and a larger global module, updated independently on clients and the central server, respectively. The local modules are trained on masked image patches, while the global module is trained on intermediate patch features uploaded from the local client, balanced through a proposed median sampling strategy to erase client data distribution privacy. We analyze the computational complexity and privacy protection of EFTViT. Extensive experiments on popular benchmarks show that EFTViT achieves up to 28.17% accuracy improvement, reduces local training computational cost by up to 2.8$\times$, and cuts local training time by up to 4.4$\times$ compared to existing methods.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Learning-Based Path Planning and Obstacle Avoidance Using PPO and B-Splines in Unknown Environments</title>
<link>https://arxiv.org/abs/2412.02176</link>
<guid>https://arxiv.org/abs/2412.02176</guid>
<content:encoded><![CDATA[
arXiv:2412.02176v2 Announce Type: replace-cross 
Abstract: This paper introduces SmartBSP, an advanced self-supervised learning framework for real-time path planning and obstacle avoidance in autonomous robotics navigating through complex environments. The proposed system integrates Proximal Policy Optimization (PPO) with Convolutional Neural Networks (CNN) and Actor-Critic architecture to process limited LIDAR inputs and compute spatial decision-making probabilities. The robot's perceptual field is discretized into a grid format, which the CNN analyzes to produce a spatial probability distribution. During the training process a nuanced cost function is minimized that accounts for path curvature, endpoint proximity, and obstacle avoidance. Simulations results in different scenarios validate the algorithm's resilience and adaptability across diverse operational scenarios. Subsequently, Real-time experiments, employing the Robot Operating System (ROS), were carried out to assess the efficacy of the proposed algorithm.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research</title>
<link>https://arxiv.org/abs/2412.04497</link>
<guid>https://arxiv.org/abs/2412.04497</guid>
<content:encoded><![CDATA[
arXiv:2412.04497v3 Announce Type: replace-cross 
Abstract: Low-resource languages serve as invaluable repositories of human history, embodying cultural evolution and intellectual diversity. Despite their significance, these languages face critical challenges, including data scarcity and technological limitations, which hinder their comprehensive study and preservation. Recent advancements in large language models (LLMs) offer transformative opportunities for addressing these challenges, enabling innovative methodologies in linguistic, historical, and cultural research. This study systematically evaluates the applications of LLMs in low-resource language research, encompassing linguistic variation, historical documentation, cultural expressions, and literary analysis. By analyzing technical frameworks, current methodologies, and ethical considerations, this paper identifies key challenges such as data accessibility, model adaptability, and cultural sensitivity. Given the cultural, historical, and linguistic richness inherent in low-resource languages, this work emphasizes interdisciplinary collaboration and the development of customized models as promising avenues for advancing research in this domain. By underscoring the potential of integrating artificial intelligence with the humanities to preserve and study humanity's linguistic and cultural heritage, this study fosters global efforts towards safeguarding intellectual diversity.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for Toxicity Maximization</title>
<link>https://arxiv.org/abs/2412.05892</link>
<guid>https://arxiv.org/abs/2412.05892</guid>
<content:encoded><![CDATA[
arXiv:2412.05892v4 Announce Type: replace-cross 
Abstract: Understanding the vulnerabilities of Large Vision Language Models (LVLMs) to jailbreak attacks is essential for their responsible real-world deployment. Most previous work requires access to model gradients, or is based on human knowledge (prompt engineering) to complete jailbreak, and they hardly consider the interaction of images and text, resulting in inability to jailbreak in black box scenarios or poor performance. To overcome these limitations, we propose a Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for toxicity maximization, referred to as PBI-Attack. Our method begins by extracting malicious features from a harmful corpus using an alternative LVLM and embedding these features into a benign image as prior information. Subsequently, we enhance these features through bidirectional cross-modal interaction optimization, which iteratively optimizes the bimodal perturbations in an alternating manner through greedy search, aiming to maximize the toxicity of the generated response. The toxicity level is quantified using a well-trained evaluation model. Experiments demonstrate that PBI-Attack outperforms previous state-of-the-art jailbreak methods, achieving an average attack success rate of 92.5% across three open-source LVLMs and around 67.3% on three closed-source LVLMs. Disclaimer: This paper contains potentially disturbing and offensive content.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skill-Enhanced Reinforcement Learning Acceleration from Heterogeneous Demonstrations</title>
<link>https://arxiv.org/abs/2412.06207</link>
<guid>https://arxiv.org/abs/2412.06207</guid>
<content:encoded><![CDATA[
arXiv:2412.06207v2 Announce Type: replace-cross 
Abstract: Learning from Demonstration (LfD) is a well-established problem in Reinforcement Learning (RL), which aims to facilitate rapid RL by leveraging expert demonstrations to pre-train the RL agent. However, the limited availability of expert demonstration data often hinders its ability to effectively aid downstream RL learning. To address this problem, we propose a novel two-stage method dubbed as Skill-enhanced Reinforcement Learning Acceleration (SeRLA). SeRLA introduces a skill-level adversarial Positive-Unlabeled (PU) learning model that extracts useful skill prior knowledge by learning from both expert demonstrations and general low-cost demonstrations in the offline prior learning stage. Building on this, it employs a skill-based soft actor-critic algorithm to leverage the acquired priors for efficient training of a skill policy network in the downstream online RL stage. In addition, we propose a simple skill-level data enhancement technique to mitigate data sparsity and further improve both skill prior learning and skill policy training. Experiments across multiple standard RL benchmarks demonstrate that SeRLA achieves state-of-the-art performance in accelerating reinforcement learning on downstream tasks, particularly in the early training phase.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction</title>
<link>https://arxiv.org/abs/2412.09318</link>
<guid>https://arxiv.org/abs/2412.09318</guid>
<content:encoded><![CDATA[
arXiv:2412.09318v3 Announce Type: replace-cross 
Abstract: LLMs can generate human-like dialogues, yet their ability to simulate early child-adult interactions remains largely unexplored. In this paper, we examined how effectively LLMs can capture the distinctive features of child-caregiver language in interaction, using both static and interactive benchmarking methods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can approximate child-caregiver dialogues at the word and utterance level, but they struggle to reproduce the child and caregiver's discursive patterns, exaggerate alignment, and fail to reach the level of diversity shown by humans. The broader goal of this work is to initiate the development of a comprehensive benchmark for LLMs in child-oriented applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViSymRe: Vision-guided Multimodal Symbolic Regression</title>
<link>https://arxiv.org/abs/2412.11139</link>
<guid>https://arxiv.org/abs/2412.11139</guid>
<content:encoded><![CDATA[
arXiv:2412.11139v2 Announce Type: replace-cross 
Abstract: Extracting simple mathematical expression from an observational dataset to describe complex natural phenomena is one of the core objectives of artificial intelligence (AI). This field is known as symbolic regression (SR). Traditional SR models are based on genetic programming (GP) or reinforcement learning (RL), facing well-known challenges, such as low efficiency and overfitting. Recent studies have integrated SR with large language models (LLMs), enabling fast zero-shot inference by learning mappings from millions of dataset-expression pairs. However, since the input and output are inherently different modalities, such models often struggle to converge effectively. In this paper, we introduce ViSymRe, a vision-guided multimodal SR model that incorporates the third resource, expression graph, to bridge the modality gap. Different from traditional multimodal models, ViSymRe is trained to extract vision, termed virtual vision, from datasets, without relying on the global availability of expression graphs, which addresses the essential challenge of visual SR, i.e., expression graphs are not available during inference. Evaluation results on multiple mainstream benchmarks show that ViSymRe achieves more competitive performance than the state-of-the-art dataset-only baselines. The expressions predicted by ViSymRe not only fit the dataset well but are also simple and structurally accurate, goals that SR models strive to achieve.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APEX$^2$: Adaptive and Extreme Summarization for Personalized Knowledge Graphs</title>
<link>https://arxiv.org/abs/2412.17336</link>
<guid>https://arxiv.org/abs/2412.17336</guid>
<content:encoded><![CDATA[
arXiv:2412.17336v2 Announce Type: replace-cross 
Abstract: Knowledge graphs (KGs), which store an extensive number of relational facts, serve various applications. Recently, personalized knowledge graphs (PKGs) have emerged as a solution to optimize storage costs by customizing their content to align with users' specific interests within particular domains. In the real world, on one hand, user queries and their underlying interests are inherently evolving, requiring PKGs to adapt continuously; on the other hand, the summarization is constantly expected to be as small as possible in terms of storage cost. However, the existing PKG summarization methods implicitly assume that the user's interests are constant and do not shift. Furthermore, when the size constraint of PKG is extremely small, the existing methods cannot distinguish which facts are more of immediate interest and guarantee the utility of the summarized PKG. To address these limitations, we propose APEX$^2$, a highly scalable PKG summarization framework designed with robust theoretical guarantees to excel in adaptive summarization tasks with extremely small size constraints. To be specific, after constructing an initial PKG, APEX$^2$ continuously tracks the interest shift and adjusts the previous summary. We evaluate APEX$^2$ under an evolving query setting on benchmark KGs containing up to 12 million triples, summarizing with compression ratios $\leq 0.1\%$. The experiments show that APEX outperforms state-of-the-art baselines in terms of both query-answering accuracy and efficiency. Code is available at https://github.com/iDEA-iSAIL-Lab-UIUC/APEX.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Goal-Conditioned Data Augmentation for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.20519</link>
<guid>https://arxiv.org/abs/2412.20519</guid>
<content:encoded><![CDATA[
arXiv:2412.20519v2 Announce Type: replace-cross 
Abstract: Offline reinforcement learning (RL) enables policy learning from pre-collected offline datasets, relaxing the need to interact directly with the environment. However, limited by the quality of offline datasets, it generally fails to learn well-qualified policies in suboptimal datasets. To address datasets with insufficient optimal demonstrations, we introduce Goal-cOnditioned Data Augmentation (GODA), a novel goal-conditioned diffusion-based method for augmenting samples with higher quality. Leveraging recent advancements in generative modelling, GODA incorporates a novel return-oriented goal condition with various selection mechanisms. Specifically, we introduce a controllable scaling technique to provide enhanced return-based guidance during data sampling. GODA learns a comprehensive distribution representation of the original offline datasets while generating new data with selectively higher-return goals, thereby maximizing the utility of limited optimal demonstrations. Furthermore, we propose a novel adaptive gated conditioning method for processing noisy inputs and conditions, enhancing the capture of goal-oriented guidance. We conduct experiments on the D4RL benchmark and real-world challenges, specifically traffic signal control (TSC) tasks, to demonstrate GODA's effectiveness in enhancing data quality and superior performance compared to state-of-the-art data augmentation methods across various offline RL algorithms.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal LLMs Can Reason about Aesthetics in Zero-Shot</title>
<link>https://arxiv.org/abs/2501.09012</link>
<guid>https://arxiv.org/abs/2501.09012</guid>
<content:encoded><![CDATA[
arXiv:2501.09012v3 Announce Type: replace-cross 
Abstract: The rapid technical progress of generative art (GenArt) has democratized the creation of visually appealing imagery. However, achieving genuine artistic impact - the kind that resonates with viewers on a deeper, more meaningful level - remains formidable as it requires a sophisticated aesthetic sensibility. This sensibility involves a multifaceted cognitive process extending beyond mere visual appeal, which is often overlooked by current computational methods. This paper pioneers an approach to capture this complex process by investigating how the reasoning capabilities of Multimodal LLMs (MLLMs) can be effectively elicited to perform aesthetic judgment. Our analysis reveals a critical challenge: MLLMs exhibit a tendency towards hallucinations during aesthetic reasoning, characterized by subjective opinions and unsubstantiated artistic interpretations. We further demonstrate that these hallucinations can be suppressed by employing an evidence-based and objective reasoning process, as substantiated by our proposed baseline, ArtCoT. MLLMs prompted by this principle produce multifaceted, in-depth aesthetic reasoning that aligns significantly better with human judgment. These findings have direct applications in areas such as AI art tutoring and as reward models for image generation. Ultimately, we hope this work paves the way for AI systems that can truly understand, appreciate, and contribute to art that aligns with human aesthetic values. Project homepage: https://github.com/songrise/MLLM4Art.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal Scene Generation</title>
<link>https://arxiv.org/abs/2501.10462</link>
<guid>https://arxiv.org/abs/2501.10462</guid>
<content:encoded><![CDATA[
arXiv:2501.10462v2 Announce Type: replace-cross 
Abstract: With the widespread use of virtual reality applications, 3D scene generation has become a new challenging research frontier. 3D scenes have highly complex structures and need to ensure that the output is dense, coherent, and contains all necessary structures. Many current 3D scene generation methods rely on pre-trained text-to-image diffusion models and monocular depth estimators. However, the generated scenes occupy large amounts of storage space and often lack effective regularisation methods, leading to geometric distortions. To this end, we propose BloomScene, a lightweight structured 3D Gaussian splatting for crossmodal scene generation, which creates diverse and high-quality 3D scenes from text or image inputs. Specifically, a crossmodal progressive scene generation framework is proposed to generate coherent scenes utilizing incremental point cloud reconstruction and 3D Gaussian splatting. Additionally, we propose a hierarchical depth prior-based regularization mechanism that utilizes multi-level constraints on depth accuracy and smoothness to enhance the realism and continuity of the generated scenes. Ultimately, we propose a structured context-guided compression mechanism that exploits structured hash grids to model the context of unorganized anchor attributes, which significantly eliminates structural redundancy and reduces storage overhead. Comprehensive experiments across multiple scenes demonstrate the significant potential and advantages of our framework compared with several baselines.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FL-CLEANER: byzantine and backdoor defense by CLustering Errors of Activation maps in Non-iid fedErated leaRning</title>
<link>https://arxiv.org/abs/2501.12123</link>
<guid>https://arxiv.org/abs/2501.12123</guid>
<content:encoded><![CDATA[
arXiv:2501.12123v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) enables clients to collaboratively train a global model using their local datasets while reinforcing data privacy, but it is prone to poisoning attacks. Existing defense mechanisms assume that clients' data are independent and identically distributed (IID), making them ineffective in real-world applications where data are non-IID. This paper presents FL-CLEANER, the first defense capable of filtering both byzantine and backdoor attackers' model updates in a non-IID FL environment. The originality of FL-CLEANER is twofold. First, it relies on a client confidence score derived from the reconstruction errors of each client's model activation maps for a given trigger set, with reconstruction errors obtained by means of a Conditional Variational Autoencoder trained according to a novel server-side strategy. Second, it uses an original ad-hoc trust propagation algorithm we propose. Based on previous client scores, it allows building a cluster of benign clients while flagging potential attackers. Experimental results on the datasets MNIST and FashionMNIST demonstrate the efficiency of FL-CLEANER against Byzantine attackers as well as to some state-of-the-art backdoors in non-IID scenarios; it achieves a close-to-zero (<1%) benign client misclassification rate, even in the absence of an attack, and achieves strong performance compared to state of the art defenses.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Preference Optimization for Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2501.13919</link>
<guid>https://arxiv.org/abs/2501.13919</guid>
<content:encoded><![CDATA[
arXiv:2501.13919v3 Announce Type: replace-cross 
Abstract: Despite significant advancements in video large multimodal models (video-LMMs), achieving effective temporal grounding in long-form videos remains a challenge for existing models. To address this limitation, we propose Temporal Preference Optimization (TPO), a novel post-training framework designed to enhance the temporal grounding capabilities of video-LMMs through preference learning. TPO adopts a self-training approach that enables models to differentiate between well-grounded and less accurate temporal responses by leveraging curated preference datasets at two granularities: localized temporal grounding, which focuses on specific video segments, and comprehensive temporal grounding, which captures extended temporal dependencies across entire video sequences. By optimizing on these preference datasets, TPO significantly enhances temporal understanding while reducing reliance on manually annotated data. Extensive experiments on three long-form video understanding benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO establishes itself as the leading 7B model on the Video-MME benchmark, underscoring the potential of TPO as a scalable and efficient solution for advancing temporal reasoning in long-form video understanding. Project page: https://ruili33.github.io/tpo_website.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision without Images: End-to-End Computer Vision from Single Compressive Measurements</title>
<link>https://arxiv.org/abs/2501.15122</link>
<guid>https://arxiv.org/abs/2501.15122</guid>
<content:encoded><![CDATA[
arXiv:2501.15122v3 Announce Type: replace-cross 
Abstract: Snapshot Compressed Imaging (SCI) offers high-speed, low-bandwidth, and energy-efficient image acquisition, but remains challenged by low-light and low signal-to-noise ratio (SNR) conditions. Moreover, practical hardware constraints in high-resolution sensors limit the use of large frame-sized masks, necessitating smaller, hardware-friendly designs. In this work, we present a novel SCI-based computer vision framework using pseudo-random binary masks of only 8$\times$8 in size for physically feasible implementations. At its core is CompDAE, a Compressive Denoising Autoencoder built on the STFormer architecture, designed to perform downstream tasks--such as edge detection and depth estimation--directly from noisy compressive raw pixel measurements without image reconstruction. CompDAE incorporates a rate-constrained training strategy inspired by BackSlash to promote compact, compressible models. A shared encoder paired with lightweight task-specific decoders enables a unified multi-task platform. Extensive experiments across multiple datasets demonstrate that CompDAE achieves state-of-the-art performance with significantly lower complexity, especially under ultra-low-light conditions where traditional CMOS and SCI pipelines fail.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling Large Language Models for Network Active Queue Management</title>
<link>https://arxiv.org/abs/2501.16734</link>
<guid>https://arxiv.org/abs/2501.16734</guid>
<content:encoded><![CDATA[
arXiv:2501.16734v3 Announce Type: replace-cross 
Abstract: The growing complexity of network traffic and demand for ultra-low latency communication require smarter packet traffic management. Existing Deep Learning-based queuing approaches struggle with dynamic network scenarios and demand high engineering effort. We propose AQM-LLM, distilling Large Language Models (LLMs) with few-shot learning, contextual understanding, and pattern recognition to improve Active Queue Management (AQM) [RFC 9330] with minimal manual effort. We consider a specific case where AQM is Low Latency, Low Loss, and Scalable Throughput (L4S) and our design of AQM-LLM builds on speculative decoding and reinforcement-based distilling of LLM by tackling congestion prevention in the L4S architecture using Explicit Congestion Notification (ECN) [RFC 9331] and periodic packet dropping. We develop a new open-source experimental platform by executing L4S-AQM on FreeBSD-14, providing interoperable modules to support LLM integration and facilitate IETF recognition through wider testing. Our extensive evaluations show L4S-LLM enhances queue management, prevents congestion, reduces latency, and boosts network performance, showcasing LLMs' adaptability and efficiency in uplifting AQM systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DesCLIP: Robust Continual Learning via General Attribute Descriptions for VLM-Based Visual Recognition</title>
<link>https://arxiv.org/abs/2502.00618</link>
<guid>https://arxiv.org/abs/2502.00618</guid>
<content:encoded><![CDATA[
arXiv:2502.00618v2 Announce Type: replace-cross 
Abstract: Continual learning of vision-language models (VLMs) focuses on leveraging cross-modal pretrained knowledge to incrementally adapt to expanding downstream tasks and datasets, while tackling the challenge of knowledge forgetting. Existing research often focuses on connecting visual features with specific class text in downstream tasks, overlooking the latent relationships between general and specialized knowledge. Our findings reveal that forcing models to optimize inappropriate visual-text matches exacerbates forgetting of VLM's recognition ability. To tackle this issue, we propose DesCLIP, which leverages general attribute (GA) descriptions to guide the understanding of specific class objects, enabling VLMs to establish robust vision-GA-class trilateral associations rather than relying solely on vision-class connections. Specifically, we introduce a language assistant to generate concrete GA description candidates via proper request prompts. Then, an anchor-based embedding filter is designed to obtain highly relevant GA description embeddings, which are leveraged as the paired text embeddings for visual-textual instance matching, thereby tuning the visual encoder. Correspondingly, the class text embeddings are gradually calibrated to align with these shared GA description embeddings. Extensive experiments demonstrate the advancements and efficacy of our proposed method, with comprehensive empirical evaluations highlighting its superior performance in VLM-based recognition compared to existing continual learning methods.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework</title>
<link>https://arxiv.org/abs/2502.00711</link>
<guid>https://arxiv.org/abs/2502.00711</guid>
<content:encoded><![CDATA[
arXiv:2502.00711v2 Announce Type: replace-cross 
Abstract: Visual reasoning refers to the task of solving questions about visual information. Current visual reasoning methods typically employ pre-trained vision-language model (VLM) strategies or deep neural network approaches. However, existing efforts are constrained by limited reasoning interpretability, while hindering by the phenomenon of underspecification in the question text. Additionally, the absence of fine-grained visual knowledge limits the precise understanding of subject behavior in visual reasoning tasks. To address these issues, we propose VIKSER (Visual Knowledge-Driven Self-Reinforcing Reasoning Framework). Specifically, VIKSER, trained using knowledge distilled from large language models, extracts fine-grained visual knowledge with the assistance of visual relationship detection techniques. Subsequently, VIKSER utilizes fine-grained visual knowledge to paraphrase the question with underspecification. Additionally, we design a novel prompting method called Chain-of-Evidence (CoE), which leverages the power of "evidence for reasoning" to endow VIKSER with interpretable reasoning capabilities. Meanwhile, the integration of self-reflection technology empowers VIKSER with the ability to learn and improve from its mistakes. Experiments conducted on widely used datasets demonstrate that VIKSER achieves new state-of-the-art (SOTA) results in relevant tasks. Moreover, VIKSER achieves performance on par with leading proprietary models, such as the latest ChatGPT-5.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning</title>
<link>https://arxiv.org/abs/2502.03275</link>
<guid>https://arxiv.org/abs/2502.03275</guid>
<content:encoded><![CDATA[
arXiv:2502.03275v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Complexity of Learning Sparse Superposed Features with Feedback</title>
<link>https://arxiv.org/abs/2502.05407</link>
<guid>https://arxiv.org/abs/2502.05407</guid>
<content:encoded><![CDATA[
arXiv:2502.05407v4 Announce Type: replace-cross 
Abstract: The success of deep networks is crucially attributed to their ability to capture latent features within a representation space. In this work, we investigate whether the underlying learned features of a model can be efficiently retrieved through feedback from an agent, such as a large language model (LLM), in the form of relative \tt{triplet comparisons}. These features may represent various constructs, including dictionaries in LLMs or a covariance matrix of Mahalanobis distances. We analyze the feedback complexity associated with learning a feature matrix in sparse settings. Our results establish tight bounds when the agent is permitted to construct activations and demonstrate strong upper bounds in sparse scenarios when the agent's feedback is limited to distributional information. We validate our theoretical findings through experiments on two distinct applications: feature recovery from Recursive Feature Machines and dictionary extraction from sparse autoencoders trained on Large Language Models.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Vision Models for Time Series Analysis: A Survey</title>
<link>https://arxiv.org/abs/2502.08869</link>
<guid>https://arxiv.org/abs/2502.08869</guid>
<content:encoded><![CDATA[
arXiv:2502.08869v2 Announce Type: replace-cross 
Abstract: Time series analysis has witnessed the inspiring development from traditional autoregressive models, deep learning models, to recent Transformers and Large Language Models (LLMs). Efforts in leveraging vision models for time series analysis have also been made along the way but are less visible to the community due to the predominant research on sequence modeling in this domain. However, the discrepancy between continuous time series and the discrete token space of LLMs, and the challenges in explicitly modeling the correlations of variates in multivariate time series have shifted some research attentions to the equally successful Large Vision Models (LVMs) and Vision Language Models (VLMs). To fill the blank in the existing literature, this survey discusses the advantages of vision models over LLMs in time series analysis. It provides a comprehensive and in-depth overview of the existing methods, with dual views of detailed taxonomy that answer the key research questions including how to encode time series as images and how to model the imaged time series for various tasks. Additionally, we address the challenges in the pre- and post-processing steps involved in this framework and outline future directions to further advance time series analysis with vision models.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAMer: Reconstruction-based Adversarial Model for Multi-party Multi-modal Multi-label Emotion Recognition</title>
<link>https://arxiv.org/abs/2502.10435</link>
<guid>https://arxiv.org/abs/2502.10435</guid>
<content:encoded><![CDATA[
arXiv:2502.10435v2 Announce Type: replace-cross 
Abstract: Conventional Multi-modal multi-label emotion recognition (MMER) assumes complete access to visual, textual, and acoustic modalities. However, real-world multi-party settings often violate this assumption, as non-speakers frequently lack acoustic and textual inputs, leading to a significant degradation in model performance. Existing approaches also tend to unify heterogeneous modalities into a single representation, overlooking each modality's unique characteristics. To address these challenges, we propose RAMer (Reconstruction-based Adversarial Model for Emotion Recognition), which refines multi-modal representations by not only exploring modality commonality and specificity but crucially by leveraging reconstructed features, enhanced by contrastive learning, to overcome data incompleteness and enrich feature quality. RAMer also introduces a personality auxiliary task to complement missing modalities using modality-level attention, improving emotion reasoning. To further strengthen the model's ability to capture label and modality interdependency, we propose a stack shuffle strategy to enrich correlations between labels and modality-specific features. Experiments on three benchmarks, i.e., MEmoR, CMU-MOSEI, and $M^3ED$, demonstrate that RAMer achieves state-of-the-art performance in dyadic and multi-party MMER scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroAMP: A Novel End-to-end General Purpose Deep Neural Amplifier for Personalized Hearing Aids</title>
<link>https://arxiv.org/abs/2502.10822</link>
<guid>https://arxiv.org/abs/2502.10822</guid>
<content:encoded><![CDATA[
arXiv:2502.10822v2 Announce Type: replace-cross 
Abstract: The prevalence of hearing aids is increasing. However, optimizing the amplification processes of hearing aids remains challenging due to the complexity of integrating multiple modular components in traditional methods. To address this challenge, we present NeuroAMP, a novel deep neural network designed for end-to-end, personalized amplification in hearing aids. NeuroAMP leverages both spectral features and the listener's audiogram as inputs, and we investigate four architectures: Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Convolutional Recurrent Neural Network (CRNN), and Transformer. We also introduce Denoising NeuroAMP, an extension that integrates noise reduction along with amplification capabilities for improved performance in real-world scenarios. To enhance generalization, a comprehensive data augmentation strategy was employed during training on diverse speech (TIMIT and TMHINT) and music (Cadenza Challenge MUSIC) datasets. Evaluation using the Hearing Aid Speech Perception Index (HASPI), Hearing Aid Speech Quality Index (HASQI), and Hearing Aid Audio Quality Index (HAAQI) demonstrates that the Transformer architecture within NeuroAMP achieves the best performance, with SRCC scores of 0.9927 (HASQI) and 0.9905 (HASPI) on TIMIT, and 0.9738 (HAAQI) on the Cadenza Challenge MUSIC dataset. Notably, our data augmentation strategy maintains high performance on unseen datasets (e.g., VCTK, MUSDB18-HQ). Furthermore, Denoising NeuroAMP outperforms both the conventional NAL-R+WDRC approach and a two-stage baseline on the VoiceBank+DEMAND dataset, achieving a 10% improvement in both HASPI (0.90) and HASQI (0.59) scores. These results highlight the potential of NeuroAMP and Denoising NeuroAMP to deliver notable improvements in personalized hearing aid amplification.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer</title>
<link>https://arxiv.org/abs/2502.15779</link>
<guid>https://arxiv.org/abs/2502.15779</guid>
<content:encoded><![CDATA[
arXiv:2502.15779v2 Announce Type: replace-cross 
Abstract: We propose Rotate, Clip, and Partition (RCP), a quantization-aware training (QAT) approach that first realizes extreme compression of LLMs with W2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP integrates recent rotation techniques with a novel non-uniform weight quantizer design, by quantitatively analyzing the impact of random rotation on 2-bit weight quantization. Our weight quantizer features Learnable Direct Partitioning (LDP), which introduces learnable parameters to directly learn non-uniform intervals jointly with LLM weights. We also present a specialized GPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP can compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and 5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging mobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and MetaMath-7B with no critical problems such as convergence failure and repetition. Code is available at https://github.com/ songsm921/RCP.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Gap Between the Gaussian RKHS and Neural Networks: An Infinite-Center Asymptotic Analysis</title>
<link>https://arxiv.org/abs/2502.16331</link>
<guid>https://arxiv.org/abs/2502.16331</guid>
<content:encoded><![CDATA[
arXiv:2502.16331v2 Announce Type: replace-cross 
Abstract: Recent works have characterized the function-space inductive bias of infinite-width bounded-norm single-hidden-layer neural networks as a kind of bounded-variation-type space. This novel neural network Banach space encompasses many classical multivariate function spaces, including certain Sobolev spaces and the spectral Barron spaces. Notably, this Banach space also includes functions that exhibit less classical regularity, such as those that only vary in a few directions. On bounded domains, it is well-established that the Gaussian reproducing kernel Hilbert space (RKHS) strictly embeds into this Banach space, demonstrating a clear gap between the Gaussian RKHS and the neural network Banach space. It turns out that when investigating these spaces on unbounded domains, e.g., all of $\mathbb{R}^d$, the story is fundamentally different. We establish the following fundamental result: Certain functions that lie in the Gaussian RKHS have infinite norm in the neural network Banach space. This provides a nontrivial gap between kernel methods and neural networks by exhibiting functions that kernel methods easily represent, whereas neural networks cannot.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs</title>
<link>https://arxiv.org/abs/2502.16534</link>
<guid>https://arxiv.org/abs/2502.16534</guid>
<content:encoded><![CDATA[
arXiv:2502.16534v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are becoming increasingly capable across global languages. However, the ability to communicate across languages does not necessarily translate to appropriate cultural representations. A key concern is US-centric bias, where LLMs reflect US rather than local cultural values. We propose a novel methodology that compares LLM-generated response distributions against population-level opinion data from the World Value Survey across four languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear mixed-effects regression framework, we compare two families of models: Google's Gemma models (2B--27B parameters) and successive iterations of OpenAI's turbo-series. Across the families of models, we find no consistent relationships between language capabilities and cultural alignment. While the Gemma models have a positive correlation between language capability and cultural alignment across languages, the OpenAI models do not. Importantly, we find that self-consistency is a stronger predictor of multicultural alignment than multilingual capabilities. Our results demonstrate that achieving meaningful cultural alignment requires dedicated effort beyond improving general language capabilities.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Input Rewriting Improves Translation with Large Language Models</title>
<link>https://arxiv.org/abs/2502.16682</link>
<guid>https://arxiv.org/abs/2502.16682</guid>
<content:encoded><![CDATA[
arXiv:2502.16682v3 Announce Type: replace-cross 
Abstract: Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further confirms that simplified rewrites and their MT outputs both largely preserve the original meaning of the source and MT. These results suggest LLM-assisted input rewriting as a promising direction for improving translations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Trading Arena: A Study on Numerical Understanding in LLM-Based Agents</title>
<link>https://arxiv.org/abs/2502.17967</link>
<guid>https://arxiv.org/abs/2502.17967</guid>
<content:encoded><![CDATA[
arXiv:2502.17967v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in natural language tasks, yet their performance in dynamic, real-world financial environments remains underexplored. Existing approaches are limited to historical backtesting, where trading actions cannot influence market prices and agents train only on static data. To address this limitation, we present the Agent Trading Arena, a virtual zero-sum stock market in which LLM-based agents engage in competitive multi-agent trading and directly impact price dynamics. By simulating realistic bid-ask interactions, our platform enables training in scenarios that closely mirror live markets, thereby narrowing the gap between training and evaluation. Experiments reveal that LLMs struggle with numerical reasoning when given plain-text data, often overfitting to local patterns and recent values. In contrast, chart-based visualizations significantly enhance both numerical reasoning and trading performance. Furthermore, incorporating a reflection module yields additional improvements, especially with visual inputs. Evaluations on NASDAQ and CSI datasets demonstrate the superiority of our method, particularly under high volatility. All code and data are available at https://github.com/wekjsdvnm/Agent-Trading-Arena.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis</title>
<link>https://arxiv.org/abs/2502.18517</link>
<guid>https://arxiv.org/abs/2502.18517</guid>
<content:encoded><![CDATA[
arXiv:2502.18517v2 Announce Type: replace-cross 
Abstract: The success of large language models (LLMs) has attracted many individuals to fine-tune them for domain-specific tasks by uploading their data. However, in sensitive areas like healthcare and finance, privacy concerns often arise. One promising solution is to generate synthetic data with Differential Privacy (DP) guarantees to replace private data. However, these synthetic data contain significant flawed data, which are considered as noise. Existing solutions typically rely on naive filtering by comparing ROUGE-L scores or embedding similarities, which are ineffective in addressing the noise. To address this issue, we propose \textit{RewardDS}, a novel privacy-preserving framework that fine-tunes a reward proxy model and uses reward signals to guide the synthetic data generation. Our \textit{RewardDS} introduces two key modules, Reward Guided Filtering and Self-Optimizing Refinement, to both filter and refine the synthetic data, effectively mitigating the noise. Extensive experiments across medical, financial, and code generation domains demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Causal Lens for Evaluating Faithfulness Metrics</title>
<link>https://arxiv.org/abs/2502.18848</link>
<guid>https://arxiv.org/abs/2502.18848</guid>
<content:encoded><![CDATA[
arXiv:2502.18848v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's true reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, they are often evaluated in isolation, making direct, principled comparisons between them difficult. Here, we present Causal Diagnosticity, a framework that serves as a common testbed to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate prominent faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that diagnostic performance varies across tasks and models, with Filler Tokens performing best overall. Additionally, continuous metrics are generally more diagnostic than binary ones but can be sensitive to noise and model choice. Our results highlight the need for more robust faithfulness metrics.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids</title>
<link>https://arxiv.org/abs/2502.20396</link>
<guid>https://arxiv.org/abs/2502.20396</guid>
<content:encoded><![CDATA[
arXiv:2502.20396v2 Announce Type: replace-cross 
Abstract: Learning generalizable robot manipulation policies, especially for complex multi-fingered humanoids, remains a significant challenge. Existing approaches primarily rely on extensive data collection and imitation learning, which are expensive, labor-intensive, and difficult to scale. Sim-to-real reinforcement learning (RL) offers a promising alternative, but has mostly succeeded in simpler state-based or single-hand setups. How to effectively extend this to vision-based, contact-rich bimanual manipulation tasks remains an open question. In this paper, we introduce a practical sim-to-real RL recipe that trains a humanoid robot to perform three challenging dexterous manipulation tasks: grasp-and-reach, box lift and bimanual handover. Our method features an automated real-to-sim tuning module, a generalized reward formulation based on contact and object goals, a divide-and-conquer policy distillation framework, and a hybrid object representation strategy with modality-specific augmentation. We demonstrate high success rates on unseen objects and robust, adaptive policy behaviors -- highlighting that vision-based dexterous manipulation via sim-to-real RL is not only viable, but also scalable and broadly applicable to real-world humanoid manipulation tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Deterministic Policy Gradient for Disturbance Attenuation and Its Application to Quadrotor Control</title>
<link>https://arxiv.org/abs/2502.21057</link>
<guid>https://arxiv.org/abs/2502.21057</guid>
<content:encoded><![CDATA[
arXiv:2502.21057v4 Announce Type: replace-cross 
Abstract: Practical control systems pose significant challenges in identifying optimal control policies due to uncertainties in the system model and external disturbances. While $H_\infty$ control techniques are commonly used to design robust controllers that mitigate the effects of disturbances, these methods often require complex and computationally intensive calculations. To address this issue, this paper proposes a reinforcement learning algorithm called Robust Deterministic Policy Gradient (RDPG), which formulates the $H_\infty$ control problem as a two-player zero-sum dynamic game. In this formulation, one player (the user) aims to minimize the cost, while the other player (the adversary) seeks to maximize it. We then employ deterministic policy gradient (DPG) and its deep reinforcement learning counterpart to train a robust control policy with effective disturbance attenuation. In particular, for practical implementation, we introduce an algorithm called robust deep deterministic policy gradient (RDDPG), which employs a deep neural network architecture and integrates techniques from the twin-delayed deep deterministic policy gradient (TD3) to enhance stability and learning efficiency. To evaluate the proposed algorithm, we implement it on an unmanned aerial vehicle (UAV) tasked with following a predefined path in a disturbance-prone environment. The experimental results demonstrate that the proposed method outperforms other control approaches in terms of robustness against disturbances, enabling precise real-time tracking of moving targets even under severe disturbance conditions.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOHPER: Multi-objective Hyperparameter Optimization Framework for E-commerce Retrieval System</title>
<link>https://arxiv.org/abs/2503.05227</link>
<guid>https://arxiv.org/abs/2503.05227</guid>
<content:encoded><![CDATA[
arXiv:2503.05227v3 Announce Type: replace-cross 
Abstract: E-commerce search optimization has evolved to include a wider range of metrics that reflect user engagement and business objectives. Modern search frameworks now incorporate advanced quality features, such as sales counts and document-query relevance, to better align search results with these goals. Traditional methods typically focus on click-through rate (CTR) as a measure of engagement or relevance, but this can miss true purchase intent, creating a gap between user interest and actual conversions. Joint training with the click-through conversion rate (CTCVR) has become essential for understanding buying behavior, although its sparsity poses challenges for reliable optimization. This study presents MOHPER, a Multi-Objective Hyperparameter Optimization framework for E-commerce Retrieval systems. Utilizing Bayesian optimization and sampling, it jointly optimizes both CTR, CTCVR, and relevant objectives, focusing on engagement and conversion of the users. In addition, to improve the selection of the best configuration from multi-objective optimization, we suggest advanced methods for hyperparameter selection, including a meta-configuration voting strategy and a cumulative training approach that leverages prior optimal configurations, to improve speeds of training and efficiency. Currently deployed in a live setting, our proposed framework substantiates its practical efficacy in achieving a balanced optimization that aligns with both user satisfaction and revenue goals.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To See a World in a Spark of Neuron: Disentangling Multi-task Interference for Training-free Model Merging</title>
<link>https://arxiv.org/abs/2503.05320</link>
<guid>https://arxiv.org/abs/2503.05320</guid>
<content:encoded><![CDATA[
arXiv:2503.05320v3 Announce Type: replace-cross 
Abstract: Fine-tuning pre-trained models on targeted datasets enhances task-specific performance but often comes at the expense of generalization. Model merging techniques, which integrate multiple fine-tuned models into a single multi-task model through task arithmetic, offer a promising solution. However, task interference remains a fundamental challenge, leading to performance degradation and suboptimal merged models. Existing approaches largely overlooked the fundamental roles of neurons, their connectivity, and activation, resulting in a merging process and a merged model that does not consider how neurons relay and process information. In this work, we present the first study that relies on neuronal mechanisms for model merging. Specifically, we decomposed task-specific representations into two complementary neuronal subspaces that regulate input sensitivity and task adaptability. Leveraging this decomposition, we introduced NeuroMerging, a novel merging framework developed to mitigate task interference within neuronal subspaces, enabling training-free model fusion across diverse tasks. Through extensive experiments, we demonstrated that NeuroMerging achieved superior performance compared to existing methods on multi-task benchmarks across both natural language and vision domains. Our findings highlighted the importance of aligning neuronal mechanisms in model merging, offering new insights into mitigating task interference and improving knowledge fusion. Our project is available at https://ZzzitaoFang.github.io/projects/NeuroMerging/.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-World Skill Discovery from Unsegmented Demonstrations</title>
<link>https://arxiv.org/abs/2503.10684</link>
<guid>https://arxiv.org/abs/2503.10684</guid>
<content:encoded><![CDATA[
arXiv:2503.10684v2 Announce Type: replace-cross 
Abstract: Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods that rely on sequence sampling or human labeling, we have developed a self-supervised learning-based approach to segment these long videos into a series of semantic-aware and skill-consistent segments. Drawing inspiration from human cognitive event segmentation theory, we introduce Skill Boundary Detection (SBD), an annotation-free temporal video segmentation algorithm. SBD detects skill boundaries in a video by leveraging prediction errors from a pretrained unconditional action-prediction model. This approach is based on the assumption that a significant increase in prediction error indicates a shift in the skill being executed. We evaluated our method in Minecraft, a rich open-world simulator with extensive gameplay videos available online. Our SBD-generated segments improved the average performance of conditioned policies by 63.7% and 52.1% on short-term atomic skill tasks, and their corresponding hierarchical agents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the diverse YouTube videos to train instruction-following agents. The project page can be found in https://craftjarvis.github.io/SkillDiscovery.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Table Question Answering via Answer-Formula Joint Generation</title>
<link>https://arxiv.org/abs/2503.12345</link>
<guid>https://arxiv.org/abs/2503.12345</guid>
<content:encoded><![CDATA[
arXiv:2503.12345v3 Announce Type: replace-cross 
Abstract: Advanced table question answering (TableQA) methods prompt large language models (LLMs) to generate answer text, SQL query, Python code, or custom operation, which impressively improve the complex reasoning problems in the TableQA task. However, these methods lack the versatility to cope with specific question types or table structures. In contrast, the Spreadsheet Formula, the widely used and well-defined operation language for tabular data, has not been thoroughly explored to solve TableQA. In this paper, we first attempt to use the Formula as the executable representation for solving complex reasoning on tables with different structures. Specifically, we construct \texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing datasets. In addition, we propose \texttt{TabAF}, a general table answering framework to solve multiple types of tasks over multiple types of tables simultaneously, which decodes answers and Formulas with a single LLM backbone. Extensive experiments demonstrate the versatility and generalization of \texttt{TabAF}. Under the same model size, \texttt{TabAF} achieves new state-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering</title>
<link>https://arxiv.org/abs/2503.18172</link>
<guid>https://arxiv.org/abs/2503.18172</guid>
<content:encoded><![CDATA[
arXiv:2503.18172v4 Announce Type: replace-cross 
Abstract: Misleading visualizations, which manipulate chart representations to support specific claims, can distort perception and lead to incorrect conclusions. Despite decades of research, they remain a widespread issue-posing risks to public understanding and raising safety concerns for AI systems involved in data-driven communication. While recent multimodal large language models (MLLMs) show strong chart comprehension abilities, their capacity to detect and interpret misleading charts remains unexplored. We introduce Misleading ChartQA benchmark, a large-scale multimodal dataset designed to evaluate MLLMs on misleading chart reasoning. It contains 3,026 curated examples spanning 21 misleader types and 10 chart types, each with standardized chart code, CSV data, multiple-choice questions, and labeled explanations, validated through iterative MLLM checks and exhausted expert human review. We benchmark 24 state-of-the-art MLLMs, analyze their performance across misleader types and chart formats, and propose a novel region-aware reasoning pipeline that enhances model accuracy. Our work lays the foundation for developing MLLMs that are robust, trustworthy, and aligned with the demands of responsible visual communication.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study of Transfer Learning, Resolution Reduction, and Multi-View Classification</title>
<link>https://arxiv.org/abs/2503.19945</link>
<guid>https://arxiv.org/abs/2503.19945</guid>
<content:encoded><![CDATA[
arXiv:2503.19945v2 Announce Type: replace-cross 
Abstract: Mammography, an X-ray-based imaging technique, plays a crucial role in the early detection of breast cancer. Its accuracy heavily depends on expert radiologists, making it essential to minimize interpretation errors. To support radiologists, various computer-aided detection and diagnostic methods have been proposed, increasingly leveraging advancements in artificial intelligence and machine learning. Over recent years, mammogram analysis has evolved significantly - from early patch-based classifiers, which examine only localized regions of images, to full-image classifiers, and later towards multi-view systems that simultaneously integrate multiple perspectives of the mammographic exam for enhanced accuracy. Despite this progression, critical questions remain, such as whether multi-view systems consistently outperform single-view approaches. In this paper, we systematically evaluate and compare the effectiveness of single-view and multi-view mammogram classification techniques. Our research introduces models that achieve superior performance relative to existing state-of-the-art approaches in both single-view and two-view classification scenarios. Furthermore, our findings provide valuable insights into optimal model architectures and effective transfer learning strategies, paving the way for more accurate and efficient mammogram interpretation. The inference code and model are available at https://github.com/dpetrini/multiple-view.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound</title>
<link>https://arxiv.org/abs/2503.20685</link>
<guid>https://arxiv.org/abs/2503.20685</guid>
<content:encoded><![CDATA[
arXiv:2503.20685v3 Announce Type: replace-cross 
Abstract: Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos</title>
<link>https://arxiv.org/abs/2503.20936</link>
<guid>https://arxiv.org/abs/2503.20936</guid>
<content:encoded><![CDATA[
arXiv:2503.20936v2 Announce Type: replace-cross 
Abstract: Physical agility is a necessary skill in competitive table tennis, but by no means sufficient. Champions excel in this fast-paced and highly dynamic environment by anticipating their opponent's intent - buying themselves the necessary time to react. In this work, we take one step towards designing such an anticipatory agent. Previous works have developed systems capable of real-time table tennis gameplay, though they often do not leverage anticipation. Among the works that forecast opponent actions, their approaches are limited by dataset size and variety. Our paper contributes (1) a scalable system for reconstructing monocular video of table tennis matches in 3D and (2) an uncertainty-aware controller that anticipates opponent actions. We demonstrate in simulation that our policy improves the ball return rate against high-speed hits from 49.9% to 59.0% as compared to a baseline non-anticipatory policy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Bang for the Buck: Process Reward Modeling with Entropy-Driven Uncertainty</title>
<link>https://arxiv.org/abs/2503.22233</link>
<guid>https://arxiv.org/abs/2503.22233</guid>
<content:encoded><![CDATA[
arXiv:2503.22233v2 Announce Type: replace-cross 
Abstract: We introduce the Entropy Driven Uncertainty Process Reward Model (EDU-PRM), a novel entropy-driven training framework for process reward modeling that enables dynamic, uncertainty-aligned segmentation of complex reasoning steps, eliminating the need for costly manual step annotations. Unlike previous Process Reward Models (PRMs) that rely on static partitioning and human labeling, EDU-PRM automatically anchors step boundaries at tokens with high predictive entropy. On the MATH test set, EDU-PRM achieves 65.5% accuracy, surpassing strong public PRM baselines such as Math-Shepherd PRM (61.7%) and Omega PRM (62.4%) under the High Temperature (HT) Sample + BON setting. Furthermore, when replacing HT sampling with EDU sampling, EDU-PRM further improves both accuracy and efficiency: at N=64, accuracy increases from 64.7% (HT Sample + BON) to 67.3% (EDU Sample + BON), while the number of generated tokens is reduced by 47%, demonstrating a superior accuracy-cost balance. On the ProcessBench test set, EDU-PRM achieves a new state-of-the-art accuracy of 88.4% using less than 1.5% of the Qwen2.5-Math-PRM-72B training data, surpassing the previous best of 87.8%. In summary, EDU-PRM provides a scalable and annotation-efficient paradigm for process supervision in mathematical reasoning, opening new avenues for efficient complex reasoning on math.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Q: Fine-Tuning Large Language Models for Quantum Circuit Generation and Optimization</title>
<link>https://arxiv.org/abs/2504.11109</link>
<guid>https://arxiv.org/abs/2504.11109</guid>
<content:encoded><![CDATA[
arXiv:2504.11109v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved remarkable outcomes in complex problems, including math, coding, and analyzing large amounts of scientific reports. Yet, few works have explored the potential of LLMs in quantum computing. The most challenging problem is to leverage LLMs to automatically generate quantum circuits at a large scale. Fundamentally, the existing pre-trained LLMs lack the knowledge of quantum circuits. In this paper, we address this challenge by fine-tuning LLMs and injecting the domain-specific knowledge of quantum computing. We describe Agent-Q, an LLM fine-tuning system to generate and optimize quantum circuits. In particular, Agent-Q implements the mechanisms to generate training data sets and constructs an end-to-end pipeline to fine-tune pre-trained LLMs to generate parameterized quantum circuits for various optimization problems. Agent-Q provides 14,000 quantum circuits covering a large spectrum of the quantum optimization landscape: 12 optimization problem instances and their optimized QAOA, VQE, and adaptive VQE circuits. Based thereon, Agent-Q fine-tunes LLMs and constructs syntactically correct parametrized quantum circuits in OpenQASM 3.0. We have evaluated the quality of the LLM-generated circuits and parameters by comparing them to the optimized expectation values and distributions. Experimental results show superior performance of Agent-Q, compared to several state-of-the-art LLMs and better parameters than random. Agent-Q can be integrated into an agentic workflow, and the generated parametrized circuits with initial parameters can be used as a starting point for further optimization, e.g., as templates in quantum machine learning and as benchmarks for compilers and hardware.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rollout-Based Algorithm and Reward Function for Resource Allocation in Business Processes</title>
<link>https://arxiv.org/abs/2504.11250</link>
<guid>https://arxiv.org/abs/2504.11250</guid>
<content:encoded><![CDATA[
arXiv:2504.11250v2 Announce Type: replace-cross 
Abstract: Resource allocation plays a critical role in minimizing cycle time and improving the efficiency of business processes. Recently, Deep Reinforcement Learning (DRL) has emerged as a powerful technique to optimize resource allocation policies in business processes. In the DRL framework, an agent learns a policy through interaction with the environment, guided solely by reward signals that indicate the quality of its decisions. However, existing algorithms are not suitable for dynamic environments such as business processes. Furthermore, existing DRL-based methods rely on engineered reward functions that approximate the desired objective, but a misalignment between reward and objective can lead to undesired decisions or suboptimal policies. To address these issues, we propose a rollout-based DRL algorithm and a reward function to optimize the objective directly. Our algorithm iteratively improves the policy by evaluating execution trajectories following different actions. Our reward function directly decomposes the objective function of minimizing the cycle time, such that trial-and-error reward engineering becomes unnecessary. We evaluated our method in six scenarios, for which the optimal policy can be computed, and on a set of increasingly complex, realistically sized process models. The results show that our algorithm can learn the optimal policy for the scenarios and outperform or match the best heuristics on the realistically sized business processes.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progent: Programmable Privilege Control for LLM Agents</title>
<link>https://arxiv.org/abs/2504.11703</link>
<guid>https://arxiv.org/abs/2504.11703</guid>
<content:encoded><![CDATA[
arXiv:2504.11703v2 Announce Type: replace-cross 
Abstract: LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments. Attackers can exploit these agents through various vectors, including indirect prompt injection, memory/knowledge base poisoning, and malicious tools, tricking agents into performing dangerous actions such as unauthorized financial transactions or data leakage. The core problem that enables attacks to succeed lies in over-privileged tool access. We introduce Progent, the first privilege control framework to secure LLM agents. Progent enforces security at the tool level by restricting agents to performing tool calls necessary for user tasks while blocking potentially malicious ones. Progent features a domain-specific language that allows for expressing fine-grained policies for controlling tool privileges, flexible fallback actions when calls are blocked, and dynamic policy updates to adapt to changing agent states. The framework operates deterministically at runtime, providing provable security guarantees. Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Additionally, we show that LLMs can automatically generate effective policies, highlighting their potential for automating the process of writing Progent's security policies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tilus: A Tile-Level GPGPU Programming Language for Low-Precision Computation</title>
<link>https://arxiv.org/abs/2504.12984</link>
<guid>https://arxiv.org/abs/2504.12984</guid>
<content:encoded><![CDATA[
arXiv:2504.12984v3 Announce Type: replace-cross 
Abstract: Serving Large Language Models (LLMs) is critical for AI-powered applications, yet it demands substantial computational resources, particularly in memory bandwidth and computational throughput. Low-precision computation has emerged as a key technique to improve efficiency while reducing resource consumption. Existing approaches for generating low-precision kernels are limited to weight bit widths that are powers of two and suffer from suboptimal performance because of high-level GPU programming abstractions. These abstractions restrict critical optimizations, such as fine-grained register management and optimized memory access patterns, that are essential for efficient low-precision computations. In this paper, we introduce Tilus, a domain-specific language designed for General-Purpose GPU (GPGPU) computing that supports low-precision data types with arbitrary bit widths from 1 to 8 while maintaining GPU programmability. Tilus features a thread-block-level programming model, a hierarchical memory space, a novel algebraic layout system, and extensive support for diverse low-precision data types. Tilus programs are compiled into highly efficient GPU programs through automatic vectorization and instruction selection. Extensive experiments demonstrate that Tilus efficiently supports a full spectrum of low-precision data types, and outperforms state-of-the-art low-precision kernels. Compared to existing compilers such as Triton and Ladder, as well as hand-optimized kernels such as QuantLLM and Marlin, Tilus achieves performance improvements of: $1.75\times$, $2.61\times$, $1.29\times$ and $1.03\times$, respectively. We open-source Tilus at https://github.com/NVIDIA/tilus.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenTorrent: Scaling Large Language Model Serving with An Overlay Network</title>
<link>https://arxiv.org/abs/2504.20101</link>
<guid>https://arxiv.org/abs/2504.20101</guid>
<content:encoded><![CDATA[
arXiv:2504.20101v3 Announce Type: replace-cross 
Abstract: While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer</title>
<link>https://arxiv.org/abs/2505.01530</link>
<guid>https://arxiv.org/abs/2505.01530</guid>
<content:encoded><![CDATA[
arXiv:2505.01530v2 Announce Type: replace-cross 
Abstract: Accurate extraction of key information from 2D engineering drawings is crucial for high-precision manufacturing. Manual extraction is time-consuming and error-prone, while traditional Optical Character Recognition (OCR) techniques often struggle with complex layouts and overlapping symbols, resulting in unstructured outputs. To address these challenges, this paper proposes a novel hybrid deep learning framework for structured information extraction by integrating an oriented bounding box (OBB) detection model with a transformer-based document parsing model (Donut). An in-house annotated dataset is used to train YOLOv11 for detecting nine key categories: Geometric Dimensioning and Tolerancing (GD&amp;T), General Tolerances, Measures, Materials, Notes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are cropped into images and labeled to fine-tune Donut for structured JSON output. Fine-tuning strategies include a single model trained across all categories and category-specific models. Results show that the single model consistently outperforms category-specific ones across all evaluation metrics, achieving higher precision (94.77% for GD&amp;T), recall (100% for most), and F1 score (97.3%), while reducing hallucination (5.23%). The proposed framework improves accuracy, reduces manual effort, and supports scalable deployment in precision-driven industries.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairPO: Robust Preference Optimization for Fair Multi-Label Learning</title>
<link>https://arxiv.org/abs/2505.02433</link>
<guid>https://arxiv.org/abs/2505.02433</guid>
<content:encoded><![CDATA[
arXiv:2505.02433v3 Announce Type: replace-cross 
Abstract: We propose FairPO, a novel framework designed to promote fairness in multi-label classification by directly optimizing preference signals with a group robustness perspective. In our framework, the set of labels is partitioned into privileged and non-privileged groups, and a preference-based loss inspired by Direct Preference Optimization (DPO) is employed to more effectively differentiate true positive labels from confusing negatives within the privileged group, while preserving baseline classification performance for non-privileged labels. By framing the learning problem as a robust optimization over groups, our approach dynamically adjusts the training emphasis toward groups with poorer performance, thereby mitigating bias and ensuring a fairer treatment across diverse label categories. In addition, we outline plans to extend this approach by investigating alternative loss formulations such as Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization (CPO) to exploit reference-free reward formulations and contrastive training signals. Furthermore, we plan to extend FairPO with multilabel generation capabilities, enabling the model to dynamically generate diverse and coherent label sets for ambiguous inputs.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling</title>
<link>https://arxiv.org/abs/2505.04802</link>
<guid>https://arxiv.org/abs/2505.04802</guid>
<content:encoded><![CDATA[
arXiv:2505.04802v2 Announce Type: replace-cross 
Abstract: Sparse observations and coarse-resolution climate models limit effective regional decision-making, underscoring the need for robust downscaling. However, existing AI methods struggle with generalization across variables and geographies and are constrained by the quadratic complexity of Vision Transformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundation model for global, hyper-resolution climate downscaling. ORBIT-2 incorporates two key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecture with residual learning and Bayesian regularization for efficient, robust prediction; and (2) TILES, a tile-wise sequence scaling algorithm that reduces self-attention complexity from quadratic to linear, enabling long-sequence processing and massive parallelism. ORBIT-2 scales to 10 billion parameters across 65,536 GPUs, achieving up to 4.1 exaFLOPS sustained throughput and 74--98% strong scaling efficiency. It supports downscaling to 0.9 km global resolution and processes sequences up to 4.2 billion tokens. On 7 km resolution benchmarks, ORBIT-2 achieves high accuracy with $R^2$ scores in the range of 0.98--0.99 against observational data.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask-PINNs: Mitigating Internal Covariate Shift in Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.06331</link>
<guid>https://arxiv.org/abs/2505.06331</guid>
<content:encoded><![CDATA[
arXiv:2505.06331v3 Announce Type: replace-cross 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework for solving partial differential equations (PDEs) by embedding physical laws directly into the loss function. However, as a fundamental optimization issue, internal covariate shift (ICS) hinders the stable and effective training of PINNs by disrupting feature distributions and limiting model expressiveness. Unlike standard deep learning tasks, conventional remedies for ICS -- such as Batch Normalization and Layer Normalization -- are not directly applicable to PINNs, as they distort the physical consistency required for reliable PDE solutions. To address this issue, we propose Mask-PINNs, a novel architecture that introduces a learnable mask function to regulate feature distributions while preserving the underlying physical constraints of PINNs. We provide a theoretical analysis showing that the mask suppresses the expansion of feature representations through a carefully designed modulation mechanism. Empirically, we validate the method on multiple PDE benchmarks -- including convection, wave propagation, and Helmholtz equations -- across diverse activation functions. Our results show consistent improvements in prediction accuracy, convergence stability, and robustness. Furthermore, we demonstrate that Mask-PINNs enable the effective use of wider networks, overcoming a key limitation in existing PINN frameworks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation</title>
<link>https://arxiv.org/abs/2505.09698</link>
<guid>https://arxiv.org/abs/2505.09698</guid>
<content:encoded><![CDATA[
arXiv:2505.09698v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) have revolutionized artificial intelligence and robotics due to their commonsense reasoning capabilities. In robotic manipulation, VLMs are used primarily as high-level planners, but recent work has also studied their lower-level reasoning ability, which refers to making decisions about precise robot movements. However, the community currently lacks a clear and common benchmark that can evaluate how well VLMs can aid low-level reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench, to evaluate the low-level robot manipulation reasoning capabilities of VLMs across various dimensions, including how well they understand object-object interactions and deformable object manipulation. We extensively test 33 representative VLMs across 10 model families on our benchmark, including variants to test different model sizes. Our evaluation shows that the performance of VLMs significantly varies across tasks, and there is a strong correlation between this performance and trends in our real-world manipulation tasks. It also shows that there remains a significant gap between these models and human-level understanding. See our website at: https://manipbench.github.io.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs</title>
<link>https://arxiv.org/abs/2505.11277</link>
<guid>https://arxiv.org/abs/2505.11277</guid>
<content:encoded><![CDATA[
arXiv:2505.11277v4 Announce Type: replace-cross 
Abstract: Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new ``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems</title>
<link>https://arxiv.org/abs/2505.11548</link>
<guid>https://arxiv.org/abs/2505.11548</guid>
<content:encoded><![CDATA[
arXiv:2505.11548v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) have shown improved performance in generating accurate responses. However, the dependence on external knowledge bases introduces potential security vulnerabilities, particularly when these knowledge bases are publicly accessible and modifiable. While previous studies have exposed knowledge poisoning risks in RAG systems, existing attack methods suffer from critical limitations: they either require injecting multiple poisoned documents (resulting in poor stealthiness) or can only function effectively on simplistic queries (limiting real-world applicability). This paper reveals a more realistic knowledge poisoning attack against RAG systems that achieves successful attacks by poisoning only a single document while remaining effective for complex multi-hop questions involving complex relationships between multiple elements. Our proposed AuthChain address three challenges to ensure the poisoned documents are reliably retrieved and trusted by the LLM, even against large knowledge bases and LLM's own knowledge. Extensive experiments across six popular LLMs demonstrate that AuthChain achieves significantly higher attack success rates while maintaining superior stealthiness against RAG defense mechanisms compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViEEG: Hierarchical Visual Neural Representation for EEG Brain Decoding</title>
<link>https://arxiv.org/abs/2505.12408</link>
<guid>https://arxiv.org/abs/2505.12408</guid>
<content:encoded><![CDATA[
arXiv:2505.12408v3 Announce Type: replace-cross 
Abstract: Understanding and decoding brain activity into visual representations is a fundamental challenge at the intersection of neuroscience and artificial intelligence. While EEG visual decoding has shown promise due to its non-invasive, and low-cost nature, existing methods suffer from Hierarchical Neural Encoding Neglect (HNEN)-a critical limitation where flat neural representations fail to model the brain's hierarchical visual processing hierarchy. Inspired by the hierarchical organization of visual cortex, we propose ViEEG, a neuro-We further adopt hierarchical contrastive learning for EEG-CLIP representation alignment, enabling zero-shot object recognition. Extensive experiments on the THINGS-EEG dataset demonstrate that ViEEG significantly outperforms previous methods by a large margin in both subject-dependent and subject-independent settings. Results on the THINGS-MEG dataset further confirm ViEEG's generalization to different neural modalities. Our framework not only advances the performance frontier but also sets a new paradigm for EEG brain decoding. inspired framework that addresses HNEN. ViEEG decomposes each visual stimulus into three biologically aligned components-contour, foreground object, and contextual scene-serving as anchors for a three-stream EEG encoder. These EEG features are progressively integrated via cross-attention routing, simulating cortical information flow from low-level to high-level vision.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction</title>
<link>https://arxiv.org/abs/2505.12552</link>
<guid>https://arxiv.org/abs/2505.12552</guid>
<content:encoded><![CDATA[
arXiv:2505.12552v2 Announce Type: replace-cross 
Abstract: Reconstructing natural images from functional magnetic resonance imaging (fMRI) data remains a core challenge in natural decoding due to the mismatch between the richness of visual stimuli and the noisy, low resolution nature of fMRI signals. While recent two-stage models, combining deep variational autoencoders (VAEs) with diffusion models, have advanced this task, they treat all spatial-frequency components of the input equally. This uniform treatment forces the model to extract meaning features and suppress irrelevant noise simultaneously, limiting its effectiveness. We introduce FreqSelect, a lightweight, adaptive module that selectively filters spatial-frequency bands before encoding. By dynamically emphasizing frequencies that are most predictive of brain activity and suppressing those that are uninformative, FreqSelect acts as a content-aware gate between image features and natural data. It integrates seamlessly into standard very deep VAE-diffusion pipelines and requires no additional supervision. Evaluated on the Natural Scenes dataset, FreqSelect consistently improves reconstruction quality across both low- and high-level metrics. Beyond performance gains, the learned frequency-selection patterns offer interpretable insights into how different visual frequencies are represented in the brain. Our method generalizes across subjects and scenes, and holds promise for extension to other neuroimaging modalities, offering a principled approach to enhancing both decoding accuracy and neuroscientific interpretability.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams</title>
<link>https://arxiv.org/abs/2505.13834</link>
<guid>https://arxiv.org/abs/2505.13834</guid>
<content:encoded><![CDATA[
arXiv:2505.13834v2 Announce Type: replace-cross 
Abstract: Achieving coordinated teamwork among legged robots requires both fine-grained locomotion control and long-horizon strategic decision-making. Robot soccer offers a compelling testbed for this challenge, combining dynamic, competitive, and multi-agent interactions. In this work, we present a hierarchical multi-agent reinforcement learning (MARL) framework that enables fully autonomous and decentralized quadruped robot soccer. First, a set of highly dynamic low-level skills is trained for legged locomotion and ball manipulation, such as walking, dribbling, and kicking. On top of these, a high-level strategic planning policy is trained with Multi-Agent Proximal Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning framework allows agents to adapt to diverse opponent strategies and gives rise to sophisticated team behaviors, including coordinated passing, interception, and dynamic role allocation. With an extensive ablation study, the proposed learning method shows significant advantages in the cooperative and competitive multi-agent soccer game. We deploy the learned policies to real quadruped robots relying solely on onboard proprioception and decentralized localization, with the resulting system supporting autonomous robot-robot and robot-human soccer matches on indoor and outdoor soccer courts.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.17067</link>
<guid>https://arxiv.org/abs/2505.17067</guid>
<content:encoded><![CDATA[
arXiv:2505.17067v4 Announce Type: replace-cross 
Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet challenging, especially in multilingual and multiple picture settings. Prior work has primarily focused on English speakers describing a single picture (e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by introducing multilingual speakers and multiple pictures, which presents new challenges in analyzing picture-dependent content. To address these challenges, we propose a framework with three components: (1) enhancing discriminative representation learning via supervised contrastive learning, (2) involving image modality rather than relying solely on speech and text modalities, and (3) applying a Product of Experts (PoE) strategy to mitigate spurious correlations and overfitting. Our framework improves MCI detection performance, achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to 75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the text unimodal baseline. Notably, the contrastive learning component yields greater gains for the text modality compared to speech. These results highlight our framework's effectiveness in multilingual and multi-picture MCI detection.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning</title>
<link>https://arxiv.org/abs/2505.17117</link>
<guid>https://arxiv.org/abs/2505.17117</guid>
<content:encoded><![CDATA[
arXiv:2505.17117v4 Announce Type: replace-cross 
Abstract: Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands</title>
<link>https://arxiv.org/abs/2505.17137</link>
<guid>https://arxiv.org/abs/2505.17137</guid>
<content:encoded><![CDATA[
arXiv:2505.17137v3 Announce Type: replace-cross 
Abstract: Early detection of cognitive decline is crucial for enabling interventions that can slow neurodegenerative disease progression. Traditional diagnostic approaches rely on labor-intensive clinical assessments, which are impractical for frequent monitoring. Our pilot study investigates voice assistant systems (VAS) as non-invasive tools for detecting cognitive decline through longitudinal analysis of speech patterns in voice commands. Over an 18-month period, we collected voice commands from 35 older adults, with 15 participants providing daily at-home VAS interactions. To address the challenges of analyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a framework that combines (1) LLM-driven iterative prompt refinement for linguistic feature extraction, (2) HuBERT-based acoustic feature extraction, and (3) transformer-based temporal modeling. Using iTransformer, our approach achieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming its baseline by 27.13%. Through our LLM approach, we identify linguistic features that uniquely characterize everyday command usage patterns in individuals experiencing cognitive decline.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</title>
<link>https://arxiv.org/abs/2505.18102</link>
<guid>https://arxiv.org/abs/2505.18102</guid>
<content:encoded><![CDATA[
arXiv:2505.18102v4 Announce Type: replace-cross 
Abstract: Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The challenge of hidden gifts in multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2505.20579</link>
<guid>https://arxiv.org/abs/2505.20579</guid>
<content:encoded><![CDATA[
arXiv:2505.20579v4 Announce Type: replace-cross 
Abstract: Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These "hidden gifts" represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus the act of dropping the key for others is a "hidden gift". We show that several different state-of-the-art RL algorithms, including MARL algorithms, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that independent model-free policy gradient agents can solve the task when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for these independent agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of "hidden gifts", and demonstrate that learning awareness in independent agents can benefit these settings.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.21541</link>
<guid>https://arxiv.org/abs/2505.21541</guid>
<content:encoded><![CDATA[
arXiv:2505.21541v3 Announce Type: replace-cross 
Abstract: Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can NeRFs See without Cameras?</title>
<link>https://arxiv.org/abs/2505.22441</link>
<guid>https://arxiv.org/abs/2505.22441</guid>
<content:encoded><![CDATA[
arXiv:2505.22441v2 Announce Type: replace-cross 
Abstract: Neural Radiance Fields (NeRFs) have been remarkably successful at synthesizing novel views of 3D scenes by optimizing a volumetric scene function. This scene function models how optical rays bring color information from a 3D object to the camera pixels. Radio frequency (RF) or audio signals can also be viewed as a vehicle for delivering information about the environment to a sensor. However, unlike camera pixels, an RF/audio sensor receives a mixture of signals that contain many environmental reflections (also called "multipath"). Is it still possible to infer the environment using such multipath signals? We show that with redesign, NeRFs can be taught to learn from multipath signals, and thereby "see" the environment. As a grounding application, we aim to infer the indoor floorplan of a home from sparse WiFi measurements made at multiple locations inside the home. Although a difficult inverse problem, our implicitly learnt floorplans look promising, and enables forward applications, such as indoor signal prediction and basic ray tracing.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems</title>
<link>https://arxiv.org/abs/2505.22771</link>
<guid>https://arxiv.org/abs/2505.22771</guid>
<content:encoded><![CDATA[
arXiv:2505.22771v2 Announce Type: replace-cross 
Abstract: This study illustrates how incorporating feedback-oriented annotations into the scoring pipeline can enhance the accuracy of automated essay scoring (AES). This approach is demonstrated with the Persuasive Essays for Rating, Selecting, and Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We integrate two types of feedback-driven annotations: those that identify spelling and grammatical errors, and those that highlight argumentative components. To illustrate how this method could be applied in real-world scenarios, we employ two LLMs to generate annotations -- a generative language model used for spell correction and an encoder-based token-classifier trained to identify and mark argumentative elements. By incorporating annotations into the scoring process, we demonstrate improvements in performance using encoder-based large language models fine-tuned as classifiers.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple LLM Agents Debate for Equitable Cultural Alignment</title>
<link>https://arxiv.org/abs/2505.24671</link>
<guid>https://arxiv.org/abs/2505.24671</guid>
<content:encoded><![CDATA[
arXiv:2505.24671v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation</title>
<link>https://arxiv.org/abs/2505.24683</link>
<guid>https://arxiv.org/abs/2505.24683</guid>
<content:encoded><![CDATA[
arXiv:2505.24683v2 Announce Type: replace-cross 
Abstract: As people increasingly use AI systems in work and daily life, feedback mechanisms that help them use AI responsibly are urgently needed, particularly in settings where users are not equipped to assess the quality of AI predictions. We study a realistic Machine Translation (MT) scenario where monolingual users decide whether to share an MT output, first without and then with quality feedback. We compare four types of quality feedback: explicit feedback that directly give users an assessment of translation quality using (1) error highlights and (2) LLM explanations, and implicit feedback that helps users compare MT inputs and outputs through (3) backtranslation and (4) question-answer (QA) tables. We find that all feedback types, except error highlights, significantly improve both decision accuracy and appropriate reliance. Notably, implicit feedback, especially QA tables, yields significantly greater gains than explicit feedback in terms of decision accuracy, appropriate reliance, and user perceptions, receiving the highest ratings for helpfulness and trust, and the lowest for mental burden.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A versatile foundation model for cine cardiac magnetic resonance image analysis tasks</title>
<link>https://arxiv.org/abs/2506.00679</link>
<guid>https://arxiv.org/abs/2506.00679</guid>
<content:encoded><![CDATA[
arXiv:2506.00679v2 Announce Type: replace-cross 
Abstract: Here we present a versatile foundation model that can perform a range of clinically-relevant image analysis tasks, including segmentation, landmark localisation, diagnosis, and prognostication. A multi-view convolution-transformer masked autoencoder, named as CineMA, was trained on 15 million cine images from 74,916 subjects. The model was validated on multiple image analysis tasks and compared to existing models on >4,500 images from eight independent datasets with diverse population characteristics, representing the largest benchmark study for cine CMR so far. CineMA consistently outperformed conventional convolutional neural networks (CNNs) in delineating ventricular boundaries and estimating ejection fraction, a key measure of cardiac function. The improved performance was preserved, even when the model only used half of fine-tuning data. CineMA also surpassed CNNs in disease detection and matched their performance in long-axis function measurement. Interestingly, we found that CineMA can also detect cardiac changes in systemic diseases, such as diabetes, hypertension and cancer, and can also predict mortality. Finally, we assessed model fairness and demonstrated consistent model performance across demographic subgroups. These findings highlight CineMA's accuracy, learning efficiency, adaptability, and fairness, underscoring its potential as a foundation model for automated cardiac image analysis to support clinical workflow and cardiovascular research. All training and inference code and models are made publicly available at https://github.com/mathpluscode/CineMA.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speeding Up Hyper-Heuristics With Markov-Chain Operator Selection and the Only-Worsening Acceptance Operator</title>
<link>https://arxiv.org/abs/2506.01107</link>
<guid>https://arxiv.org/abs/2506.01107</guid>
<content:encoded><![CDATA[
arXiv:2506.01107v2 Announce Type: replace-cross 
Abstract: The move-acceptance hyper-heuristic was recently shown to be able to leave local optima with astonishing efficiency (Lissovoi et al., Artificial Intelligence (2023)). In this work, we propose two modifications to this algorithm that demonstrate impressive performances on a large class of benchmarks including the classic Cliff$_d$ and Jump$_m$ function classes. (i) Instead of randomly choosing between the only-improving and any-move acceptance operator, we take this choice via a simple two-state Markov chain. This modification alone reduces the runtime on Jump$_m$ functions with gap parameter $m$ from $\Omega(n^{2m-1})$ to $O(n^{m+1})$. (ii) We then replace the all-moves acceptance operator with the operator that only accepts worsenings. Such a, counter-intuitive, operator has not been used before in the literature. However, our proofs show that our only-worsening operator can greatly help in leaving local optima, reducing, e.g., the runtime on Jump functions to $O(n^3 \log n)$ independent of the gap size. In general, we prove a remarkably good runtime of $O(n^{k+1} \log n)$ for our Markov move-acceptance hyper-heuristic on all members of a new benchmark class SEQOPT$_k$, which contains a large number of functions having $k$ successive local optima, and which contains the commonly studied Jump$_m$ and Cliff$_d$ functions for $k=2$.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing Reliability in Text-Guided Medical Image Editing</title>
<link>https://arxiv.org/abs/2506.01921</link>
<guid>https://arxiv.org/abs/2506.01921</guid>
<content:encoded><![CDATA[
arXiv:2506.01921v4 Announce Type: replace-cross 
Abstract: Text-guided image editing has seen rapid progress in natural image domains, but its adaptation to medical imaging remains limited and lacks standardized evaluation. Clinically, such editing holds promise for simulating surgical outcomes, creating personalized teaching materials, and enhancing patient communication. To bridge this gap, we introduce MedEBench, a comprehensive benchmark for evaluating text-guided medical image editing. It consists of 1,182 clinically sourced image-prompt triplets spanning 70 tasks across 13 anatomical regions. MedEBench offers three key contributions: (1) a clinically relevant evaluation framework covering Editing Accuracy, Contextual Preservation, and Visual Quality, supported by detailed descriptions of expected change and ROI (Region of Interest) masks; (2) a systematic comparison of seven state-of-the-art models, revealing common failure patterns; and (3) a failure analysis protocol based on attention grounding, using IoU between attention maps and ROIs to identify mislocalization. MedEBench provides a solid foundation for developing and evaluating reliable, clinically meaningful medical image editing systems. Project website: https://mliuby.github.io/MedEBench_Website/
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinS-Pilot: A Benchmark for Online Financial RAG System</title>
<link>https://arxiv.org/abs/2506.02037</link>
<guid>https://arxiv.org/abs/2506.02037</guid>
<content:encoded><![CDATA[
arXiv:2506.02037v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various professional domains, with their performance typically evaluated through standardized benchmarks. In the financial field, the stringent demands for professional accuracy and real-time data processing often necessitate the use of retrieval-augmented generation (RAG) techniques. However, the development of financial RAG benchmarks has been constrained by data confidentiality issues and the lack of dynamic data integration. To address this issue, we introduce FinS-Pilot, a novel benchmark for evaluating RAG systems in online financial applications. Constructed from real-world financial assistant interactions, our benchmark incorporates both real-time API data and text data, organized through an intent classification framework covering critical financial domains. The benchmark enables comprehensive evaluation of financial assistants' capabilities in handling both static knowledge and time-sensitive market information.Through systematic experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying models suitable for financial applications while addressing the current gap in specialized evaluation tools for the financial domain. Our work contributes both a practical evaluation framework and a curated dataset to advance research in financial NLP systems. The code and dataset are accessible on GitHub.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Labelling Data with Unknown References</title>
<link>https://arxiv.org/abs/2506.03083</link>
<guid>https://arxiv.org/abs/2506.03083</guid>
<content:encoded><![CDATA[
arXiv:2506.03083v3 Announce Type: replace-cross 
Abstract: An evaluator is trustworthy when there exists some agreed-upon way to measure its performance as a labeller. The two ways to establish trustworthiness are either by testing it, or by assuming the evaluator `knows' somehow the way to label the corpus. However, if labelled references (e.g., a development set) are unavailable, neither of these approaches work: the former requires the data, and the latter is an assumption, not evidence. To address this, we introduce an algorithm (the `No-Data Algorithm') by which to establish trust in an evaluator without any existing references. Our algorithm works by successively posing challenges to said evaluator. We show that this is sufficient to establish trustworthiness w.h.p., in such a way that when the evaluator actually knows the way to label the corpus, the No-Data Algorithm accepts its output; and, conversely, flags untrustworthy evaluators when these are unable to prove it. We present formal proofs of correctness, empirical tests, and applications to LLMs-as-judges on low-resource languages.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments</title>
<link>https://arxiv.org/abs/2506.03598</link>
<guid>https://arxiv.org/abs/2506.03598</guid>
<content:encoded><![CDATA[
arXiv:2506.03598v3 Announce Type: replace-cross 
Abstract: Using the best Text-to-SQL methods in resource-constrained environments is challenging due to their reliance on resource-intensive open-source models. This paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to bridge the gap between resource-efficient small open-source models and the powerful capabilities of large closed-source models for Text-to-SQL translation. Our method decomposes the task into schema filtering, retrieval-augmented text-to-SQL generation based on in-context examples, and prompt-driven schema linking and SQL generation. To improve schema selection accuracy, we fine-tune large language models. Crucially, we also explore the impact of prompt engineering throughout the process, leveraging Chain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly enhance the model's reasoning for accurate SQL generation. Comprehensive evaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation</title>
<link>https://arxiv.org/abs/2506.04078</link>
<guid>https://arxiv.org/abs/2506.04078</guid>
<content:encoded><![CDATA[
arXiv:2506.04078v3 Announce Type: replace-cross 
Abstract: Evaluating large language models (LLMs) in medicine is crucial because medical applications require high accuracy with little room for error. Current medical benchmarks have three main types: medical exam-based, comprehensive medical, and specialized assessments. However, these benchmarks have limitations in question design (mostly multiple-choice), data sources (often not derived from real clinical scenarios), and evaluation methods (poor assessment of complex reasoning). To address these issues, we present LLMEval-Med, a new benchmark covering five core medical areas, including 2,996 questions created from real-world electronic health records and expert-designed clinical scenarios. We also design an automated evaluation pipeline, incorporating expert-developed checklists into our LLM-as-Judge framework. Furthermore, our methodology validates machine scoring through human-machine agreement analysis, dynamically refining checklists and prompts based on expert feedback to ensure reliability. We evaluate 13 LLMs across three categories (specialized medical models, open-source models, and closed-source models) on LLMEval-Med, providing valuable insights for the safe and effective deployment of LLMs in medical domains. The dataset is released in https://github.com/llmeval/LLMEval-Med.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A theoretical framework for self-supervised contrastive learning for continuous dependent data</title>
<link>https://arxiv.org/abs/2506.09785</link>
<guid>https://arxiv.org/abs/2506.09785</guid>
<content:encoded><![CDATA[
arXiv:2506.09785v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has emerged as a powerful approach to learning representations, particularly in the field of computer vision. However, its application to dependent data, such as temporal and spatio-temporal domains, remains underexplored. Besides, traditional contrastive SSL methods often assume \emph{semantic independence between samples}, which does not hold for dependent data exhibiting complex correlations. We propose a novel theoretical framework for contrastive SSL tailored to \emph{continuous dependent data}, which allows the nearest samples to be semantically close to each other. In particular, we propose two possible \textit{ground truth similarity measures} between objects -- \emph{hard} and \emph{soft} closeness. Under it, we derive an analytical form for the \textit{estimated similarity matrix} that accommodates both types of closeness between samples, thereby introducing dependency-aware loss functions. We validate our approach, \emph{Dependent TS2Vec}, on temporal and spatio-temporal downstream problems. Given the dependency patterns presented in the data, our approach surpasses modern ones for dependent data, highlighting the effectiveness of our theoretically grounded loss functions for SSL in capturing spatio-temporal dependencies. Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with accuracy improvements of $4.17$\% and $2.08$\%, respectively. Furthermore, on the drought classification task, which involves complex spatio-temporal patterns, our method achieves a $7$\% higher ROC-AUC score.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety</title>
<link>https://arxiv.org/abs/2506.12299</link>
<guid>https://arxiv.org/abs/2506.12299</guid>
<content:encoded><![CDATA[
arXiv:2506.12299v2 Announce Type: replace-cross 
Abstract: The recent advancements in Large Language Models(LLMs) have had a significant impact on a wide range of fields, from general domains to specialized areas. However, these advancements have also significantly increased the potential for malicious users to exploit harmful and jailbreak prompts for malicious attacks. Although there have been many efforts to prevent harmful prompts and jailbreak prompts, protecting LLMs from such malicious attacks remains an important and challenging task. In this paper, we propose QGuard, a simple yet effective safety guard method, that utilizes question prompting to block harmful prompts in a zero-shot manner. Our method can defend LLMs not only from text-based harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by diversifying and modifying guard questions, our approach remains robust against the latest harmful prompts without fine-tuning. Experimental results show that our model performs competitively on both text-only and multi-modal harmful datasets. Additionally, by providing an analysis of question prompting, we enable a white-box analysis of user inputs. We believe our method provides valuable insights for real-world LLM services in mitigating security risks associated with harmful prompts.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity</title>
<link>https://arxiv.org/abs/2506.16782</link>
<guid>https://arxiv.org/abs/2506.16782</guid>
<content:encoded><![CDATA[
arXiv:2506.16782v2 Announce Type: replace-cross 
Abstract: Fairness in machine learning (ML) has become a rapidly growing area of research. But why, in the first place, is unfairness in ML wrong? And why should we care about improving fairness? Most fair-ML research implicitly appeals to distributive equality: the idea that desirable benefits and goods, such as opportunities (e.g., Barocas et al., 2023), should be equally distributed across society. Unfair ML models, then, are seen as wrong because they unequally distribute such benefits. This paper argues that this exclusive focus on distributive equality offers an incomplete and potentially misleading ethical foundation. Grounding ML fairness in egalitarianism--the view that equality is a fundamental moral and social ideal--requires challenging structural inequality: systematic, institutional, and durable arrangements that privilege some groups while disadvantaging others. Structural inequality manifests through ML systems in two primary forms: allocative harms (e.g., economic loss) and representational harms (e.g., stereotypes, erasure). While distributive equality helps address allocative harms, it fails to explain why representational harms are wrong--why it is wrong for ML systems to reinforce social hierarchies that stratify people into superior and inferior groups--and why ML systems should aim to foster a society where people relate as equals (i.e., relational equality). To address these limitations, the paper proposes a multifaceted egalitarian framework for ML fairness that integrates both distributive and relational equality. Drawing on critical social and political philosophy, this framework offers a more comprehensive ethical foundation for tackling the full spectrum of harms perpetuated by ML systems. The paper also outlines practical pathways for implementing the framework across the entire ML pipeline.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TPTT: Transforming Pretrained Transformers into Titans</title>
<link>https://arxiv.org/abs/2506.17671</link>
<guid>https://arxiv.org/abs/2506.17671</guid>
<content:encoded><![CDATA[
arXiv:2506.17671v2 Announce Type: replace-cross 
Abstract: Transformer-based large language models (LLMs) have achieved strong performance across many natural language processing tasks. Nonetheless, their quadratic computational and memory requirements, particularly in self-attention layers, pose challenges for efficient inference on long contexts and for deployment in resource-limited environments. We present TPTT (Transforming Pretrained Transformers into Titans), a framework designed to augment pretrained Transformers with linearized attention (LiZA) and internal memory gating via Memory as Gate (MaG), applied without full retraining. TPTT supports parameter-efficient fine-tuning (LoRA) and integrates with standard toolkits such as Hugging Face Transformers. We evaluated TPTT on several pretrained models, including Llama-1B, OlMoE-1B-7B, Qwen2.5-1.5B, Gemma3-270m, OpenELM-1.3B, and Mistral-7B, in order to assess applicability across architectures of different scales.Experiments on models with approximately 1 billion parameters, evaluated primarily on the MMLU benchmark, suggest potential improvements in both efficiency and accuracy compared to baseline models. For example, Titans-Llama-1B exhibited up to a 20\% relative increase in Exact Match scores in one-shot evaluation. An additional finding is that it is possible to convert a quadratic-attention model into a purely linear-attention model using the DeltaProduct mechanism. All training runs were carried out with modest computational resources.These preliminary findings indicate that TPTT may help adapt pretrained LLMs for long-context tasks with limited overhead. Further studies on larger models and a broader set of benchmarks will be necessary to evaluate the generality and robustness of the framework. Code is available at https://github.com/fabienfrfr/tptt . Python package at https://pypi.org/project/tptt/ .
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2506.23137</link>
<guid>https://arxiv.org/abs/2506.23137</guid>
<content:encoded><![CDATA[
arXiv:2506.23137v3 Announce Type: replace-cross 
Abstract: Knowledge graph completion demands effective modeling of multifaceted semantic relationships between entities. Yet, prevailing methods, which rely on static scoring functions over learned embeddings, struggling to simultaneously capture rich semantic context and the dynamic nature of relations. To overcome this limitation, we propose the Flow-Modulated Scoring (FMS) framework, conceptualizing a relation as a dynamic evolutionary process governed by its static semantic environment. FMS operates in two stages: it first learns context-aware entity embeddings via a Semantic Context Learning module, and then models a dynamic flow between them using a Conditional Flow-Matching module. This learned flow dynamically modulates a base static score for the entity pair. By unifying context-rich static representations with a conditioned dynamic flow, FMS achieves a more comprehensive understanding of relational semantics. Extensive experiments demonstrate that FMS establishes a new state of the art across both canonical knowledge graph completion tasks: relation prediction and entity prediction. On the standard relation prediction benchmark FB15k-237, FMS achieves a near-perfect MRR of 99.8\% and Hits@1 of 99.7\% using a mere 0.35M parameters, while also attaining a 99.9\% MRR on WN18RR. Its dominance extends to entity prediction, where it secures a 25.2\% relative MRR gain in the transductive setting and substantially outperforms all baselines in challenging inductive settings. By unifying a dynamic flow mechanism with rich static contexts, FMS offers a highly effective and parameter-efficient new paradigm for knowledge graph completion. Code published at: https://github.com/yuanwuyuan9/FMS.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation</title>
<link>https://arxiv.org/abs/2506.23717</link>
<guid>https://arxiv.org/abs/2506.23717</guid>
<content:encoded><![CDATA[
arXiv:2506.23717v2 Announce Type: replace-cross 
Abstract: Multi-bit spiking neural networks (SNNs) have recently become a heated research spot, pursuing energy-efficient and high-accurate AI. However, with more bits involved, the associated memory and computation demands escalate to the point where the performance improvements become disproportionate. Based on the insight that different layers demonstrate different importance and extra bits could be wasted and interfering, this paper presents an adaptive bit allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise allocation of memory and computation resources. Thus, SNN's efficiency and accuracy can be improved. Specifically, we parametrize the temporal lengths and the bit widths of weights and spikes, and make them learnable and controllable through gradients. To address the challenges caused by changeable bit widths and temporal lengths, we propose the refined spiking neuron, which can handle different temporal lengths, enable the derivation of gradients for temporal lengths, and suit spike quantization better. In addition, we theoretically formulate the step-size mismatch problem of learnable bit widths, which may incur severe quantization errors to SNN, and accordingly propose the step-size renewal mechanism to alleviate this issue. Experiments on various datasets, including the static CIFAR and ImageNet datasets and the dynamic CIFAR-DVS, DVS-GESTURE, and SHD datasets, demonstrate that our methods can reduce the overall memory and computation cost while achieving higher accuracy. Particularly, our SEWResNet-34 can achieve a 2.69% accuracy gain and 4.16x lower bit budgets over the advanced baseline work on ImageNet. This work will be open-sourced.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design</title>
<link>https://arxiv.org/abs/2507.00445</link>
<guid>https://arxiv.org/abs/2507.00445</guid>
<content:encoded><![CDATA[
arXiv:2507.00445v2 Announce Type: replace-cross 
Abstract: We address the problem of fine-tuning diffusion models for reward-guided generation in biomolecular design. While diffusion models have proven highly effective in modeling complex, high-dimensional data distributions, real-world applications often demand more than high-fidelity generation, requiring optimization with respect to potentially non-differentiable reward functions such as physics-based simulation or rewards based on scientific knowledge. Although RL methods have been explored to fine-tune diffusion models for such objectives, they often suffer from instability, low sample efficiency, and mode collapse due to their on-policy nature. In this work, we propose an iterative distillation-based fine-tuning framework that enables diffusion models to optimize for arbitrary reward functions. Our method casts the problem as policy distillation: it collects off-policy data during the roll-in phase, simulates reward-based soft-optimal policies during roll-out, and updates the model by minimizing the KL divergence between the simulated soft-optimal policy and the current model policy. Our off-policy formulation, combined with KL divergence minimization, enhances training stability and sample efficiency compared to existing RL-based methods. Empirical results demonstrate the effectiveness and superior reward optimization of our approach across diverse tasks in protein, small molecule, and regulatory DNA design.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms</title>
<link>https://arxiv.org/abs/2507.01378</link>
<guid>https://arxiv.org/abs/2507.01378</guid>
<content:encoded><![CDATA[
arXiv:2507.01378v2 Announce Type: replace-cross 
Abstract: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as a critical research focus, and it typically requires the swarm to navigate effectively while avoiding obstacles and achieving continuous coverage over multiple mission targets. Although traditional Multi-Agent Reinforcement Learning (MARL) approaches offer dynamic adaptability, they are hindered by the semantic gap in numerical communication and the rigidity of homogeneous role structures, resulting in poor generalization and limited task scalability. Recent advances in Large Language Model (LLM)-based control frameworks demonstrate strong semantic reasoning capabilities by leveraging extensive prior knowledge. However, due to the lack of online learning and over-reliance on static priors, these works often struggle with effective exploration, leading to reduced individual potential and overall system performance. To address these limitations, we propose a Role-Adaptive LLM-Driven Yoked navigation algorithm RALLY. Specifically, we first develop an LLM-driven semantic decision framework that uses structured natural language for efficient semantic communication and collaborative reasoning. Afterward, we introduce a dynamic role-heterogeneity mechanism for adaptive role switching and personalized decision-making. Furthermore, we propose a Role-value Mixing Network (RMIX)-based assignment strategy that integrates LLM offline priors with MARL online policies to enable semi-offline training of role selection strategies. Experiments in the Multi-Agent Particle Environment (MPE) environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY outperforms conventional approaches in terms of task coverage, convergence speed, and generalization, highlighting its strong potential for collaborative navigation in agentic multi-UAV systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation</title>
<link>https://arxiv.org/abs/2507.01463</link>
<guid>https://arxiv.org/abs/2507.01463</guid>
<content:encoded><![CDATA[
arXiv:2507.01463v2 Announce Type: replace-cross 
Abstract: Instance segmentation of novel objects instances in RGB images, given some example images for each object, is a well known problem in computer vision. Designing a model general enough to be employed for all kinds of novel objects without (re-) training has proven to be a difficult task. To handle this, we present a new training-free framework, called: Novel Object Cyclic Threshold based Instance Segmentation (NOCTIS). NOCTIS integrates two pre-trained models: Grounded-SAM 2 for object proposals with precise bounding boxes and corresponding segmentation masks; and DINOv2 for robust class and patch embeddings, due to its zero-shot capabilities. Internally, the proposal-object matching is realized by determining an object matching score based on the similarity of the class embeddings and the average maximum similarity of the patch embeddings with a new cyclic thresholding (CT) mechanism that mitigates unstable matches caused by repetitive textures or visually similar patterns. Beyond CT, NOCTIS introduces: (i) an appearance score that is unaffected by object selection bias; (ii) the usage of the average confidence of the proposals bounding box and mask as a scoring component; and (iii) an RGB-only pipeline that performs even better than RGB-D ones. We empirically show that NOCTIS, without further training/fine tuning, attains state-of-the-art results regarding the mean AP score, w.r.t. the best RGB and RGB-D methods on the seven core datasets of the BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects" task.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedVAL: Toward Expert-Level Medical Text Validation with Language Models</title>
<link>https://arxiv.org/abs/2507.03152</link>
<guid>https://arxiv.org/abs/2507.03152</guid>
<content:encoded><![CDATA[
arXiv:2507.03152v3 Announce Type: replace-cross 
Abstract: With the growing use of language models (LMs) in clinical environments, there is an immediate need to evaluate the accuracy and safety of LM-generated medical text. Currently, such evaluation relies solely on manual physician review. However, detecting errors in LM-generated text is challenging because 1) manual review is costly and 2) expert-composed reference outputs are often unavailable in real-world settings. While the "LM-as-judge" paradigm (a LM evaluating another LM) offers scalable evaluation, even frontier LMs can miss subtle but clinically significant errors. To address these challenges, we propose MedVAL, a self-supervised framework that leverages synthetic data to train evaluator LMs to assess whether LM-generated medical outputs are factually consistent with inputs, without requiring physician labels or reference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a dataset containing 840 outputs annotated by physicians, following a physician-defined taxonomy of risk levels and error categories. Across 6 diverse medical tasks and 10 state-of-the-art LMs spanning open-source, proprietary, and medically adapted models, MedVAL fine-tuning significantly improves (p < 0.001) alignment with physicians on both seen and unseen tasks, increasing average F1 scores from 66% to 83%, with per-sample safety classification scores up to 86%. MedVAL improves the performance of even the best-performing proprietary LM (GPT-4o) by 8%. To support a scalable, risk-aware pathway towards clinical integration, we open-source the 1) codebase (https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench (https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), and 3) MedVAL-4B (https://huggingface.co/stanfordmimi/MedVAL-4B), the best-performing open-source LM. Our research provides the first evidence of LMs approaching expert-level validation ability for medical text.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers from Driving Video</title>
<link>https://arxiv.org/abs/2507.05463</link>
<guid>https://arxiv.org/abs/2507.05463</guid>
<content:encoded><![CDATA[
arXiv:2507.05463v2 Announce Type: replace-cross 
Abstract: We introduce scenario-based cognitive status identification in older drivers from naturalistic driving videos, leveraging large vision models. In recent times, cognitive decline including Dementia and Mild Cognitive Impairment (MCI), is often underdiagnosed due to the time-consuming and costly nature of current diagnostic methods. By analyzing real-world driving behavior captured through in-vehicle sensors, this study aims to extract "digital fingerprints" that correlate with functional decline and clinical features of dementia. Moreover, modern large vision models can draw meaningful insights from everyday driving patterns across different roadway scenarios to early detect cognitive decline. We propose a framework that uses large vision models and naturalistic driving videos to analyze driver behavior, identify cognitive status and predict disease progression. We leverage the strong relationship between real-world driving behavior as an observation of the current cognitive status of the drivers where the vehicle can be utilized as a "diagnostic tool". Our method identifies early warning signs of functional impairment, contributing to proactive intervention strategies. This work enhances early detection and supports the development of scalable, non-invasive monitoring systems to mitigate the growing societal and economic burden of cognitive decline in the aging population.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic-R1: Distilled Dual-Strategy Reasoning</title>
<link>https://arxiv.org/abs/2507.05707</link>
<guid>https://arxiv.org/abs/2507.05707</guid>
<content:encoded><![CDATA[
arXiv:2507.05707v2 Announce Type: replace-cross 
Abstract: Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at https://github.com/StigLidu/DualDistill
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need</title>
<link>https://arxiv.org/abs/2507.13966</link>
<guid>https://arxiv.org/abs/2507.13966</guid>
<content:encoded><![CDATA[
arXiv:2507.13966v2 Announce Type: replace-cross 
Abstract: Language models traditionally used for cross-domain generalization have recently demonstrated task-specific reasoning. However, their top-down training approach on general corpora is insufficient for acquiring abstractions needed for deep domain expertise. This may require a bottom-up approach that acquires expertise by learning to compose simple domain concepts into more complex ones. A knowledge graph (KG) provides this compositional structure, where domain primitives are represented as head-relation-tail edges and their paths encode higher-level concepts. We present a task generation pipeline that synthesizes tasks directly from KG primitives, enabling models to acquire and compose them for reasoning. We fine-tune language models on the resultant KG-grounded curriculum to demonstrate domain-specific superintelligence. While broadly applicable, we validate our approach in medicine, where reliable KGs exist. Using a medical KG, we curate 24,000 reasoning tasks paired with thinking traces derived from diverse medical primitives. We fine-tune the QwQ-32B model on this curriculum to obtain QwQ-Med-3 that takes a step towards medical superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify reasoning abilities across 15 medical domains. Our experiments demonstrate that QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired primitives to widen the performance gap on the hardest tasks of ICD-Bench. Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3 transfers acquired expertise to enhance the base model's performance. While the industry's approach to artificial general intelligence (AGI) emphasizes broad expertise, we envision a future in which AGI emerges from the composable interaction of efficient domain-specific superintelligent agents.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation</title>
<link>https://arxiv.org/abs/2507.14201</link>
<guid>https://arxiv.org/abs/2507.14201</guid>
<content:encoded><![CDATA[
arXiv:2507.14201v2 Announce Type: replace-cross 
Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on the task of Cyber Threat Investigation through security questions derived from investigation graphs. Real-world security analysts must sift through a large number of heterogeneous alert signals and security logs, follow multi-hop chains of evidence, and compile an incident report. With the developments of LLMs, building LLM-based agents for automatic thread investigation is a promising direction. To assist the development and evaluation of LLM agents, we construct a dataset from a controlled Azure tenant that covers 8 simulated real-world multi-step attacks, 57 log tables from Microsoft Sentinel and related services, and 589 automatically generated questions. We leverage security logs extracted with expert-crafted detection logic to build threat investigation graphs, and then generate questions with LLMs using paired nodes on the graph, taking the start node as background context and the end node as answer. Anchoring each question to these explicit nodes and edges not only provides automatic, explainable ground truth answers but also makes the pipeline reusable and readily extensible to new logs. This also enables the automatic generation of procedural tasks with verifiable rewards, which can be naturally extended to training agents via reinforcement learning. Our comprehensive experiments with different models confirm the difficulty of the task: with the base setting, the average reward across all evaluated models is 0.249, and the best achieved is 0.368, leaving substantial headroom for future research. Code and data are coming soon!
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration</title>
<link>https://arxiv.org/abs/2507.14452</link>
<guid>https://arxiv.org/abs/2507.14452</guid>
<content:encoded><![CDATA[
arXiv:2507.14452v2 Announce Type: replace-cross 
Abstract: The accurate identification of high-quality correspondences is a prerequisite task in feature-based point cloud registration. However, it is extremely challenging to handle the fusion of local and global features due to feature redundancy and complex spatial relationships. Given that Gestalt principles provide key advantages in analyzing local and global relationships, we propose a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric consistency (GPI-Net) in this paper. It utilizes Gestalt principles to facilitate complementary communication between local and global information. Specifically, we introduce an orthogonal integration strategy to optimally reduce redundant information and generate a more compact global structure for high-quality correspondences. To capture geometric features in correspondences, we leverage a Gestalt Feature Attention (GFA) block through a hybrid utilization of self-attention and cross-attention mechanisms. Furthermore, to facilitate the integration of local detail information into the global structure, we design an innovative Dual-path Multi-Granularity parallel interaction aggregation (DMG) block to promote information exchange across different granularities. Extensive experiments on various challenging tasks demonstrate the superior performance of our proposed GPI-Net in comparison to existing methods. The code will be released at https://github.com/gwk429/GPI-Net.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Time Series Forecasting: A Survey</title>
<link>https://arxiv.org/abs/2507.14507</link>
<guid>https://arxiv.org/abs/2507.14507</guid>
<content:encoded><![CDATA[
arXiv:2507.14507v2 Announce Type: replace-cross 
Abstract: Diffusion models, initially developed for image synthesis, demonstrate remarkable generative capabilities. Recently, their application has expanded to time series forecasting (TSF), yielding promising results. Existing surveys on time series primarily focus on the application of diffusion models to time series tasks or merely provide model-by-model introductions of diffusion-based TSF models, without establishing a systematic taxonomy for existing diffusion-based TSF models. In this survey, we firstly introduce several standard diffusion models and their prevalent variants, explaining their adaptation to TSF tasks. Then, we provide a comprehensive review of diffusion models for TSF, paying special attention to the sources of conditional information and the mechanisms for integrating this conditioning within the models. In analyzing existing approaches using diffusion models for TSF, we provide a systematic categorization and a comprehensive summary of them in this survey. Furthermore, we examine several foundational diffusion models applied to TSF, alongside commonly used datasets and evaluation metrics. Finally, we discuss the progress and limitations of these approaches, as well as potential future research directions for diffusion-based TSF. Overall, this survey offers a comprehensive overview of recent progress and future prospects for diffusion models in TSF, serving as a valuable reference for researchers in the field.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLM Privacy Recognition for Social Robot Decision Making</title>
<link>https://arxiv.org/abs/2507.16124</link>
<guid>https://arxiv.org/abs/2507.16124</guid>
<content:encoded><![CDATA[
arXiv:2507.16124v2 Announce Type: replace-cross 
Abstract: While robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models (LLMs) presents new opportunities to develop LLM-powered robots for enhanced human-robot interaction (HRI). To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, LLMs often process sensitive personal information, particularly within private environments, such as homes. Given the tension between utility and privacy risks, evaluating how current LLMs manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the context of household robots. In this work, we present a set of privacy-relevant scenarios developed using the Contextual Integrity (CI) framework. We first surveyed users' privacy preferences regarding in-home robot behaviors and then examined how their privacy orientations affected their choices of these behaviors (N = 450). We then provided the same set of scenarios and questions to state-of-the-art LLMs (N = 10) and found that the agreement between humans and LLMs was generally low. To further investigate the capabilities of LLMs as potential privacy controllers, we implemented four additional prompting strategies and compared their results. We discuss the performance of the evaluated models as well as the implications and potential of AI privacy awareness in human-robot interaction.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Compute-Optimal Many-Shot In-Context Learning</title>
<link>https://arxiv.org/abs/2507.16217</link>
<guid>https://arxiv.org/abs/2507.16217</guid>
<content:encoded><![CDATA[
arXiv:2507.16217v2 Announce Type: replace-cross 
Abstract: Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. In this work, we propose two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. Our first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Our experiments with Gemini Pro and Flash across several datasets indicate that our strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. We also show that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Markov Categorical Framework for Language Modeling</title>
<link>https://arxiv.org/abs/2507.19247</link>
<guid>https://arxiv.org/abs/2507.19247</guid>
<content:encoded><![CDATA[
arXiv:2507.19247v2 Announce Type: replace-cross 
Abstract: Autoregressive language models achieve remarkable performance, yet a unified theory explaining their internal mechanisms--how training shapes their representations and enables complex behaviors--remains elusive. We introduce a new analytical framework that models the single-step generation process as a composition of information-processing stages using the language of Markov categories. This compositional perspective provides a unified mathematical language to connect three critical aspects of language modeling that are typically studied in isolation: the training objective, the geometry of the learned representation space, and practical model capabilities. First, our framework provides a precise information-theoretic rationale for the success of multi-token prediction methods like speculative decoding, quantifying the "information surplus" a model's hidden state contains about tokens beyond the immediate next one. Second, we clarify how the standard negative log-likelihood (NLL) objective compels the model to learn not just the next word, but also the data's intrinsic conditional uncertainty, a process we formalize using categorical entropy. Our central result reveals that NLL training functions as an implicit form of spectral contrastive learning. We prove that, for common model architectures, this simple predictive objective forces the model to sculpt a geometrically structured representation space, implicitly aligning representations with the eigenspectrum of a "predictive similarity" operator. This work offers a powerful new lens to understand how information flows through a model and how the training objective shapes its internal geometry, thereby bridging the gap between learning theory and the practical success of large language models.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AR-LIF: Adaptive reset leaky integrate-and-fire neuron for spiking neural networks</title>
<link>https://arxiv.org/abs/2507.20746</link>
<guid>https://arxiv.org/abs/2507.20746</guid>
<content:encoded><![CDATA[
arXiv:2507.20746v2 Announce Type: replace-cross 
Abstract: Spiking neural networks offer low energy consumption due to their event-driven nature. Beyond binary spike outputs, their intrinsic floating-point dynamics merit greater attention. Neuronal threshold levels and reset modes critically determine spike count and timing. Hard reset cause information loss, while soft reset apply uniform treatment to neurons. To address these issues, we design an adaptive reset neuron that establishes relationships between inputs, outputs, and reset, while integrating a simple yet effective threshold adjustment strategy. Experimental results demonstrate that our method achieves excellent performance while maintaining lower energy consumption. In particular, it attains state-of-the-art accuracy on Tiny-ImageNet and CIFAR10-DVS. Codes are available at https://github.com/2ephyrus/AR-LIF.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Application of Visual Question Answering (VQA) for Classroom Activity Monitoring</title>
<link>https://arxiv.org/abs/2507.22369</link>
<guid>https://arxiv.org/abs/2507.22369</guid>
<content:encoded><![CDATA[
arXiv:2507.22369v3 Announce Type: replace-cross 
Abstract: Classroom behavior monitoring is a critical aspect of educational research, with significant implications for student engagement and learning outcomes. Recent advancements in Visual Question Answering (VQA) models offer promising tools for automatically analyzing complex classroom interactions from video recordings. In this paper, we investigate the applicability of several state-of-the-art open-source VQA models, including LLaMA2, LLaMA3, QWEN3, and NVILA, in the context of classroom behavior analysis. To facilitate rigorous evaluation, we introduce our BAV-Classroom-VQA dataset derived from real-world classroom video recordings at the Banking Academy of Vietnam. We present the methodology for data collection, annotation, and benchmark the performance of the selected VQA models on this dataset. Our initial experimental results demonstrate that all four models achieve promising performance levels in answering behavior-related visual questions, showcasing their potential in future classroom analytics and intervention systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence Analysis of Aggregation-Broadcast in LoRA-enabled Distributed Fine-Tuning</title>
<link>https://arxiv.org/abs/2508.01348</link>
<guid>https://arxiv.org/abs/2508.01348</guid>
<content:encoded><![CDATA[
arXiv:2508.01348v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized data sources while preserving data privacy. However, the growing size of Machine Learning (ML) models poses communication and computation challenges in FL. Low-Rank Adaptation (LoRA) has recently been introduced into FL as an efficient fine-tuning method, reducing communication overhead by updating only a small number of trainable parameters. Despite its effectiveness, how to aggregate LoRA-updated local models on the server remains a critical and understudied problem. In this paper, we provide a unified convergence analysis for LoRA-based FL. We first categories the current aggregation method into two major type: Sum-Product (SP) and Product-Sum (PS). Then we formally define the Aggregation-Broadcast Operator (ABO) and derive both weak and strong convergence condition under mild assumptions. Furthermore, we present both weak and strong convergence condition that guarantee convergence of the local model and the global model respectively. These theoretical analyze offer a principled understanding of various aggregation strategies. Notably, we prove that the SP and PS aggregation methods satisfy the weak and strong convergence condition respectively, but differ in their ability to achieve the optimal convergence rate. Extensive experiments on standard benchmarks validate our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design</title>
<link>https://arxiv.org/abs/2508.03665</link>
<guid>https://arxiv.org/abs/2508.03665</guid>
<content:encoded><![CDATA[
arXiv:2508.03665v2 Announce Type: replace-cross 
Abstract: Generative models, particularly Large Language Models (LLMs), produce fluent outputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and type-theoretic principles to introduce a contract layer that mediates every LLM call. Contracts stipulate semantic and type requirements on inputs and outputs, coupled with probabilistic remediation to steer generation toward compliance. The layer exposes the dual view of LLMs as semantic parsers and probabilistic black-box components. Contract satisfaction is probabilistic and semantic validation is operationally defined through programmer-specified conditions on well-typed data structures. More broadly, this work postulates that any two agents satisfying the same contracts are \emph{functionally equivalent} with respect to those contracts.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning</title>
<link>https://arxiv.org/abs/2508.04549</link>
<guid>https://arxiv.org/abs/2508.04549</guid>
<content:encoded><![CDATA[
arXiv:2508.04549v3 Announce Type: replace-cross 
Abstract: Marine videos present significant challenges for video understanding due to the dynamics of marine objects and the surrounding environment, camera motion, and the complexity of underwater scenes. Existing video captioning datasets, typically focused on generic or human-centric domains, often fail to generalize to the complexities of the marine environment and gain insights about marine life. To address these limitations, we propose a two-stage marine object-oriented video captioning pipeline. We introduce a comprehensive video understanding benchmark that leverages the triplets of video, text, and segmentation masks to facilitate visual grounding and captioning, leading to improved marine video understanding and analysis, and marine video generation. Additionally, we highlight the effectiveness of video splitting in order to detect salient object transitions in scene changes, which significantly enrich the semantics of captioning content. Our dataset and code have been released at https://msc.hkustvgd.com.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CF3: Compact and Fast 3D Feature Fields</title>
<link>https://arxiv.org/abs/2508.05254</link>
<guid>https://arxiv.org/abs/2508.05254</guid>
<content:encoded><![CDATA[
arXiv:2508.05254v3 Announce Type: replace-cross 
Abstract: 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D foundation models. However, most approaches rely on a bottom-up optimization process that treats raw 2D features as ground truth, incurring increased computational costs. We propose a top-down pipeline for constructing compact and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast weighted fusion of multi-view 2D features with pre-trained Gaussians. This approach enables training a per-Gaussian autoencoder directly on the lifted features, instead of training autoencoders in the 2D domain. As a result, the autoencoder better aligns with the feature distribution. More importantly, we introduce an adaptive sparsification method that optimizes the Gaussian attributes of the feature field while pruning and merging the redundant Gaussians, constructing an efficient representation with preserved geometric details. Our approach achieves a competitive 3D feature field using as little as 5% of the Gaussians compared to Feature-3DGS.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grid-Agent: An LLM-Powered Multi-Agent System for Power Grid Control</title>
<link>https://arxiv.org/abs/2508.05702</link>
<guid>https://arxiv.org/abs/2508.05702</guid>
<content:encoded><![CDATA[
arXiv:2508.05702v2 Announce Type: replace-cross 
Abstract: The increasing penetration of Distributed Energy Resources (DERs), widespread adoption of Electric Vehicles (EVs), and the growing frequency of extreme weather events have significantly increased the complexity of power grid planning, operation, and management. Traditional rule-based systems and numerical optimization approaches often struggle with the scale, dynamics, and adaptability required by modern power networks. This paper introduces Grid-Agent, an autonomous, AI-driven framework that combines Large Language Models (LLMs) with multi-agent reinforcement learning to detect and remediate grid violations in real time. Grid-Agent integrates semantic reasoning with numerical precision through a modular agent architecture: a planning agent generates coordinated action sequences using numerical power flow solvers, while a validation agent evaluates system stability and action effectiveness via sandboxed execution with safety rollbacks. To ensure scalability, Grid-Agent incorporates an adaptive multiscale network representation that dynamically selects optimal encoding schemes based on network size and complexity. The framework enables coordinated violation resolution through optimizing switch configurations, battery deployment, and load curtailment strategies. Experimental results in standard IEEE and CIGRE test systems (IEEE 69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation performance. Additionally, the framework's built-in data collection and learning capabilities enable continuous learning and adaptation to diverse network topologies. The autonomous nature of the framework makes it particularly suitable for modern smart grid applications requiring rapid response to dynamic operating conditions.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Serving Optimization with Variable Prefill and Decode Lengths</title>
<link>https://arxiv.org/abs/2508.06133</link>
<guid>https://arxiv.org/abs/2508.06133</guid>
<content:encoded><![CDATA[
arXiv:2508.06133v2 Announce Type: replace-cross 
Abstract: We study the problem of serving LLM (Large Language Model) requests where each request has heterogeneous prefill and decode lengths. In LLM serving, the prefill length corresponds to the input prompt length, which determines the initial memory usage in the KV cache. The decode length refers to the number of output tokens generated sequentially, with each additional token increasing the KV cache memory usage by one unit. Given a set of n requests, our goal is to schedule and process them to minimize the total completion time. We show that this problem is NP-hard due to the interplay of batching, placement constraints, precedence relationships, and linearly increasing memory usage. We then analyze commonly used scheduling strategies in practice, such as First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their competitive ratios scale up sublinearly with the memory limit-a significant drawback in real-world settings where memory demand is large. To address this, we propose a novel algorithm based on a new selection metric that efficiently forms batches over time. We prove that this algorithm achieves a constant competitive ratio. Finally, we develop and evaluate a few algorithm variants inspired by this approach, including dynamic programming variants, local search methods, and an LP-based scheduler, demonstrating through comprehensive simulations that they outperform standard baselines while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Class Unbiasing for Generalization in Medical Diagnosis</title>
<link>https://arxiv.org/abs/2508.06943</link>
<guid>https://arxiv.org/abs/2508.06943</guid>
<content:encoded><![CDATA[
arXiv:2508.06943v2 Announce Type: replace-cross 
Abstract: Medical diagnosis might fail due to bias. In this work, we identified class-feature bias, which refers to models' potential reliance on features that are strongly correlated with only a subset of classes, leading to biased performance and poor generalization on other classes. We aim to train a class-unbiased model (Cls-unbias) that mitigates both class imbalance and class-feature bias simultaneously. Specifically, we propose a class-wise inequality loss which promotes equal contributions of classification loss from positive-class and negative-class samples. We propose to optimize a class-wise group distributionally robust optimization objective-a class-weighted training objective that upweights underperforming classes-to enhance the effectiveness of the inequality loss under class imbalance. Through synthetic and real-world datasets, we empirically demonstrate that class-feature bias can negatively impact model performance. Our proposed method effectively mitigates both class-feature bias and class imbalance, thereby improving the model's generalization ability.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory</title>
<link>https://arxiv.org/abs/2508.07279</link>
<guid>https://arxiv.org/abs/2508.07279</guid>
<content:encoded><![CDATA[
arXiv:2508.07279v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) offer new opportunities for scalable, interactive mental health assessment, but excessive querying by LLMs burdens users and is inefficient for real-world screening across transdiagnostic symptom profiles. We introduce MAQuA, an adaptive question-asking framework for simultaneous, multidimensional mental health screening. Combining multi-outcome modeling on language responses with item response theory (IRT) and factor analysis, MAQuA selects the questions with most informative responses across multiple dimensions at each turn to optimize diagnostic information, improving accuracy and potentially reducing response burden. Empirical results on a novel dataset reveal that MAQuA reduces the number of assessment questions required for score stabilization by 50-87% compared to random ordering (e.g., achieving stable depression scores with 71% fewer questions and eating disorder scores with 85% fewer questions). MAQuA demonstrates robust performance across both internalizing (depression, anxiety) and externalizing (substance use, eating disorder) domains, with early stopping strategies further reducing patient time and burden. These findings position MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive mental health screening, advancing the integration of LLM-based agents into real-world clinical workflows.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACD-CLIP: Decoupling Representation and Dynamic Fusion for Zero-Shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.07819</link>
<guid>https://arxiv.org/abs/2508.07819</guid>
<content:encoded><![CDATA[
arXiv:2508.07819v4 Announce Type: replace-cross 
Abstract: Pre-trained Vision-Language Models (VLMs) struggle with Zero-Shot Anomaly Detection (ZSAD) due to a critical adaptation gap: they lack the local inductive biases required for dense prediction and employ inflexible feature fusion paradigms. We address these limitations through an Architectural Co-Design framework that jointly refines feature representation and cross-modal fusion. Our method proposes a parameter-efficient Convolutional Low-Rank Adaptation (Conv-LoRA) adapter to inject local inductive biases for fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that leverages visual context to adaptively modulate text prompts, enabling a powerful bidirectional fusion. Extensive experiments on diverse industrial and medical benchmarks demonstrate superior accuracy and robustness, validating that this synergistic co-design is critical for robustly adapting foundation models to dense perception tasks.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grid2Guide: A* Enabled Small Language Model for Indoor Navigation</title>
<link>https://arxiv.org/abs/2508.08100</link>
<guid>https://arxiv.org/abs/2508.08100</guid>
<content:encoded><![CDATA[
arXiv:2508.08100v2 Announce Type: replace-cross 
Abstract: Reliable indoor navigation remains a significant challenge in complex environments, particularly where external positioning signals and dedicated infrastructures are unavailable. This research presents Grid2Guide, a hybrid navigation framework that combines the A* search algorithm with a Small Language Model (SLM) to generate clear, human-readable route instructions. The framework first conducts a binary occupancy matrix from a given indoor map. Using this matrix, the A* algorithm computes the optimal path between origin and destination, producing concise textual navigation steps. These steps are then transformed into natural language instructions by the SLM, enhancing interpretability for end users. Experimental evaluations across various indoor scenarios demonstrate the method's effectiveness in producing accurate and timely navigation guidance. The results validate the proposed approach as a lightweight, infrastructure-free solution for real-time indoor navigation support.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UQGNN: Uncertainty Quantification of Graph Neural Networks for Multivariate Spatiotemporal Prediction</title>
<link>https://arxiv.org/abs/2508.08551</link>
<guid>https://arxiv.org/abs/2508.08551</guid>
<content:encoded><![CDATA[
arXiv:2508.08551v2 Announce Type: replace-cross 
Abstract: Spatiotemporal prediction plays a critical role in numerous real-world applications such as urban planning, transportation optimization, disaster response, and pandemic control. In recent years, researchers have made significant progress by developing advanced deep learning models for spatiotemporal prediction. However, most existing models are deterministic, i.e., predicting only the expected mean values without quantifying uncertainty, leading to potentially unreliable and inaccurate outcomes. While recent studies have introduced probabilistic models to quantify uncertainty, they typically focus on a single phenomenon (e.g., taxi, bike, crime, or traffic crashes), thereby neglecting the inherent correlations among heterogeneous urban phenomena. To address the research gap, we propose a novel Graph Neural Network with Uncertainty Quantification, termed UQGNN for multivariate spatiotemporal prediction. UQGNN introduces two key innovations: (i) an Interaction-aware Spatiotemporal Embedding Module that integrates a multivariate diffusion graph convolutional network and an interaction-aware temporal convolutional network to effectively capture complex spatial and temporal interaction patterns, and (ii) a multivariate probabilistic prediction module designed to estimate both expected mean values and associated uncertainties. Extensive experiments on four real-world multivariate spatiotemporal datasets from Shenzhen, New York City, and Chicago demonstrate that UQGNN consistently outperforms state-of-the-art baselines in both prediction accuracy and uncertainty quantification. For example, on the Shenzhen dataset, UQGNN achieves a 5% improvement in both prediction accuracy and uncertainty quantification.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preacher: Paper-to-Video Agentic System</title>
<link>https://arxiv.org/abs/2508.09632</link>
<guid>https://arxiv.org/abs/2508.09632</guid>
<content:encoded><![CDATA[
arXiv:2508.09632v5 Announce Type: replace-cross 
Abstract: The paper-to-video task converts a research paper into a structured video abstract, distilling key concepts, methods, and conclusions into an accessible, well-organized format. While state-of-the-art video generation models demonstrate potential, they are constrained by limited context windows, rigid video duration constraints, limited stylistic diversity, and an inability to represent domain-specific knowledge. To address these limitations, we introduce Preacher, the first paper-to-video agentic system. Preacher employs a topdown approach to decompose, summarize, and reformulate the paper, followed by bottom-up video generation, synthesizing diverse video segments into a coherent abstract. To align cross-modal representations, we define key scenes and introduce a Progressive Chain of Thought (P-CoT) for granular, iterative planning. Preacher successfully generates high-quality video abstracts across five research fields, demonstrating expertise beyond current video generation models. Code will be released at: https://github.com/GenVerse/Paper2Video
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites</title>
<link>https://arxiv.org/abs/2508.12029</link>
<guid>https://arxiv.org/abs/2508.12029</guid>
<content:encoded><![CDATA[
arXiv:2508.12029v2 Announce Type: replace-cross 
Abstract: Accurate prediction of antibody-binding sites (epitopes) on antigens is crucial for vaccine design, immunodiagnostics, therapeutic antibody development, antibody engineering, research into autoimmune and allergic diseases, and for advancing our understanding of immune responses. Despite in silico methods that have been proposed to predict both linear (continuous) and conformational (discontinuous) epitopes, they consistently underperform in predicting conformational epitopes. In this work, we propose a conformer-based model trained on antigen sequences derived from 1,080 antigen-antibody complexes, leveraging convolutional neural networks (CNNs) to extract local features and Transformers to capture long-range dependencies within antigen sequences. Ablation studies demonstrate that CNN enhances the prediction of linear epitopes, and the Transformer module improves the prediction of conformational epitopes. Experimental results show that our model outperforms existing baselines in terms of PCC, ROC-AUC, PR-AUC, and F1 scores on both linear and conformational epitopes.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Flow Matching</title>
<link>https://arxiv.org/abs/2508.12413</link>
<guid>https://arxiv.org/abs/2508.12413</guid>
<content:encoded><![CDATA[
arXiv:2508.12413v2 Announce Type: replace-cross 
Abstract: The flow matching has rapidly become a dominant paradigm in classical generative modeling, offering an efficient way to interpolate between two complex distributions. We extend this idea to the quantum realm and introduce the Quantum Flow Matching (QFM-a fully quantum-circuit realization that offers efficient interpolation between two density matrices. QFM offers systematic preparation of density matrices and generation of samples for accurately estimating observables, and can be realized on quantum computers without the need for costly circuit redesigns. We validate its versatility on a set of applications: (i) generating target states with prescribed magnetization and entanglement entropy, (ii) estimating nonequilibrium free-energy differences to test the quantum Jarzynski equality, and (iii) expediting the study on superdiffusion. These results position QFM as a unifying and promising framework for generative modeling across quantum systems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning local and global prototypes with optimal transport for unsupervised anomaly detection and localization</title>
<link>https://arxiv.org/abs/2508.12927</link>
<guid>https://arxiv.org/abs/2508.12927</guid>
<content:encoded><![CDATA[
arXiv:2508.12927v2 Announce Type: replace-cross 
Abstract: Unsupervised anomaly detection aims to detect defective parts of a sample by having access, during training, to a set of normal, i.e. defect-free, data. It has many applications in fields, such as industrial inspection or medical imaging, where acquiring labels is costly or when we want to avoid introducing biases in the type of anomalies that can be spotted. In this work, we propose a novel UAD method based on prototype learning and introduce a metric to compare a structured set of embeddings that balances a feature-based cost and a spatial-based cost. We leverage this metric to learn local and global prototypes with optimal transport from latent representations extracted with a pre-trained image encoder. We demonstrate that our approach can enforce a structural constraint when learning the prototypes, allowing to capture the underlying organization of the normal samples, thus improving the detection of incoherencies in images. Our model achieves performance that is on par with strong baselines on two reference benchmarks for anomaly detection on industrial images.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Evaluation Function: A Multi-Metric Approach for Optimizing Demand Forecasting Models</title>
<link>https://arxiv.org/abs/2508.13057</link>
<guid>https://arxiv.org/abs/2508.13057</guid>
<content:encoded><![CDATA[
arXiv:2508.13057v2 Announce Type: replace-cross 
Abstract: Inventory management in dynamic and competitive business environments presents multidimensional challenges, particularly in the face of demand uncertainty and logistical and financial constraints. In this context, accurate demand forecasting is critical for optimizing resources and anticipating market fluctuations. However, the isolated use of traditional metrics such as Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE) can lead to biased evaluations and limit model robustness. To address this limitation, we propose the Hierarchical Evaluation Function (HEF), a composite function that integrates R2, MAE, and RMSE under a hierarchical and dynamic framework, complemented by adaptive penalties. The study implements HEF in the optimization of multiple prediction models, applying Grid Search, Particle Swarm Optimization (PSO), and Optuna, and evaluating their performance on reference databases (Walmart, M3, M4, and M5). The results, validated using statistical tests, confirm that HEF consistently outperforms the MAE used as the evaluation function in global metrics such as R2, Global Relative Precision, RMSE, and RMSSE, improving explanatory power and stability against extreme errors. In contrast, the MAE retains advantages in simplicity and computational efficiency. In summary, HEF constitutes a robust and adaptive alternative for highly variable environments, providing a solid framework for model selection and hyperparameter optimization.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAIR: Frequency- and Locality-Aware Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2508.13544</link>
<guid>https://arxiv.org/abs/2508.13544</guid>
<content:encoded><![CDATA[
arXiv:2508.13544v3 Announce Type: replace-cross 
Abstract: Implicit Neural Representations (INRs) leverage neural networks to map coordinates to corresponding signals, enabling continuous and compact representations. This paradigm has driven significant advances in various vision tasks. However, existing INRs lack frequency selectivity, spatial localization, and sparse representations, leading to an over-reliance on redundant signal components. Consequently, they exhibit spectral bias, tending to learn low-frequency components early while struggling to capture fine high-frequency details. To address these issues, we propose FLAIR (Frequency- and Locality-Aware Implicit Neural Representations), which incorporates two key innovations. The first is RC-GAUSS, a novel activation designed for explicit frequency selection and spatial localization under the constraints of the time-frequency uncertainty principle (TFUP). The second is Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet transform (DWT) to compute energy scores and explicitly guide frequency information to the network. Our method consistently outperforms existing INRs in 2D image representation and restoration, as well as 3D reconstruction.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptively Robust LLM Inference Optimization under Prediction Uncertainty</title>
<link>https://arxiv.org/abs/2508.14544</link>
<guid>https://arxiv.org/abs/2508.14544</guid>
<content:encoded><![CDATA[
arXiv:2508.14544v2 Announce Type: replace-cross 
Abstract: We study the problem of optimizing Large Language Model (LLM) inference scheduling to minimize total latency. LLM inference is an online and multi-task service process and also heavily energy consuming by which a pre-trained LLM processes input requests and generates output tokens sequentially. Therefore, it is vital to improve its scheduling efficiency and reduce the power consumption while a great amount of prompt requests are arriving. A key challenge in LLM inference scheduling is that while the prompt length is known upon arrival, the output length, which critically impacts memory usage and processing time, is unknown. To address this uncertainty, we propose algorithms that leverage machine learning to predict output lengths, assuming the prediction provides an interval classification (min-max range) for each request.
  We first design a conservative algorithm, $\mathcal{A}_{\max}$, which schedules requests based on the upper bound of predicted output lengths to prevent memory overflow. However, this approach is overly conservative: as prediction accuracy decreases, performance degrades significantly due to potential overestimation. To overcome this limitation, we propose $\mathcal{A}_{\min}$, an adaptive algorithm that initially treats the predicted lower bound as the output length and dynamically refines this estimate during inferencing. We prove that $\mathcal{A}_{\min}$ achieves a log-scale competitive ratio. Through numerical simulations, we demonstrate that $\mathcal{A}_{\min}$ often performs nearly as well as the hindsight scheduler, highlighting both its efficiency and robustness in practical scenarios. Moreover, $\mathcal{A}_{\min}$ relies solely on the lower bound of the prediction interval--an advantageous design choice since upper bounds on output length are typically more challenging to predict accurately.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</title>
<link>https://arxiv.org/abs/2508.15212</link>
<guid>https://arxiv.org/abs/2508.15212</guid>
<content:encoded><![CDATA[
arXiv:2508.15212v2 Announce Type: replace-cross 
Abstract: Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Generalization and Personalization in Wearable Human Activity Recognition via On-Device Few-Shot Learning</title>
<link>https://arxiv.org/abs/2508.15413</link>
<guid>https://arxiv.org/abs/2508.15413</guid>
<content:encoded><![CDATA[
arXiv:2508.15413v2 Announce Type: replace-cross 
Abstract: Human Activity Recognition (HAR) with wearable devices requires both strong generalization across diverse users and efficient personalization for individuals. However, conventional HAR models often fail to generalize when faced with user-specific variations, leading to degraded performance. To address this challenge, we propose a novel on-device few-shot learning framework that bridges generalization and personalization in wearable HAR. Our method first trains a generalizable representation across users and then rapidly adapts to new users with only a few labeled samples, updating lightweight classifier layers directly on resource-constrained devices. This approach achieves robust on-device learning with minimal computation and memory cost, making it practical for real-world deployment. We implement our framework on the energy-efficient RISC-V GAP9 microcontroller and evaluate it on three benchmark datasets (RecGym, QVAR-Gesture, Ultrasound-Gesture). Across these scenarios, post-deployment adaptation improves accuracy by 3.73%, 17.38%, and 3.70%, respectively. These results demonstrate that few-shot on-device learning enables scalable, user-aware, and energy-efficient wearable human activity recognition by seamlessly uniting generalization and personalization \footnote{https://github.com/kangpx/onlineTiny2023}.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Fusion Multimodal Network for SpeechWellness Detection</title>
<link>https://arxiv.org/abs/2508.18057</link>
<guid>https://arxiv.org/abs/2508.18057</guid>
<content:encoded><![CDATA[
arXiv:2508.18057v2 Announce Type: replace-cross 
Abstract: Suicide is one of the leading causes of death among adolescents. Previous suicide risk prediction studies have primarily focused on either textual or acoustic information in isolation, the integration of multimodal signals, such as speech and text, offers a more comprehensive understanding of an individual's mental state. Motivated by this, and in the context of the 1st SpeechWellness detection challenge, we explore a lightweight multi-branch multimodal system based on a dynamic fusion mechanism for speechwellness detection. To address the limitation of prior approaches that rely on time-domain waveforms for acoustic analysis, our system incorporates both time-domain and time-frequency (TF) domain acoustic features, as well as semantic representations. In addition, we introduce a dynamic fusion block to adaptively integrate information from different modalities. Specifically, it applies learnable weights to each modality during the fusion process, enabling the model to adjust the contribution of each modality. To enhance computational efficiency, we design a lightweight structure by simplifying the original baseline model. Experimental results demonstrate that the proposed system exhibits superior performance compared to the challenge baseline, achieving a 78% reduction in model parameters and a 5% improvement in accuracy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics</title>
<link>https://arxiv.org/abs/2508.18124</link>
<guid>https://arxiv.org/abs/2508.18124</guid>
<content:encoded><![CDATA[
<div> Keywords: CMPhysBench, Large Language Models, Condensed Matter Physics, SEED score, benchmark

Summary:
CMPhysBench is a novel benchmark designed to evaluate the proficiency of Large Language Models (LLMs) in Condensed Matter Physics. It consists of over 520 meticulously curated graduate-level questions covering various subfields and theoretical frameworks of condensed matter physics. The benchmark focuses on calculation problems to test the LLMs' ability to independently generate comprehensive solutions. A Scalable Expression Edit Distance (SEED) score is introduced, providing fine-grained partial credit for accurate assessment of similarity between predictions and ground-truth. Results show that even the best models struggle, with Grok-4 reaching only 36 average SEED score and 28% accuracy on CMPhysBench, highlighting a significant capability gap in practical and frontier domains compared to traditional physics. The code and dataset for CMPhysBench are publicly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2508.18124v3 Announce Type: replace-cross 
Abstract: We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark. CMPhysBench is composed of more than 520 graduate-level meticulously curated questions covering both representative subfields and foundational theoretical frameworks of condensed matter physics, such as magnetism, superconductivity, strongly correlated systems, etc. To ensure a deep understanding of the problem-solving process,we focus exclusively on calculation problems, requiring LLMs to independently generate comprehensive solutions. Meanwhile, leveraging tree-based representations of expressions, we introduce the Scalable Expression Edit Distance (SEED) score, which provides fine-grained (non-binary) partial credit and yields a more accurate assessment of similarity between prediction and ground-truth. Our results show that even the best models, Grok-4, reach only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a significant capability gap, especially for this practical and frontier domain relative to traditional physics. The code anddataset are publicly available at https://github.com/CMPhysBench/CMPhysBench.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding</title>
<link>https://arxiv.org/abs/2508.21204</link>
<guid>https://arxiv.org/abs/2508.21204</guid>
<content:encoded><![CDATA[
<div> Keywords: architectural biases, large language models, instructional dialogue, symbolic scaffolding, short-term memory <br />
Summary: The study examines how architectural biases impact the cognitive behavior of large language models in instructional dialogue. It introduces a symbolic scaffolding mechanism and short-term memory schema to enhance structured reasoning in Socratic tutoring. Through ablation experiments on five system variants, the model outputs are evaluated using expert-designed rubrics covering aspects like scaffolding, responsiveness, symbolic reasoning, and conversational memory. Preliminary results indicate that the full system consistently outperforms baseline variants, emphasizing the importance of memory and symbolic structure for cognitive behaviors such as abstraction and adaptive probing in LLMs. This highlights the role of architectural scaffolds in shaping instructional strategies in large language models. <br /><br /> <div>
arXiv:2508.21204v1 Announce Type: new 
Abstract: We study how architectural inductive biases influence the cognitive behavior of large language models (LLMs) in instructional dialogue. We introduce a symbolic scaffolding mechanism paired with a short-term memory schema designed to promote adaptive, structured reasoning in Socratic tutoring. Using controlled ablation across five system variants, we evaluate model outputs via expert-designed rubrics covering scaffolding, responsiveness, symbolic reasoning, and conversational memory. We present preliminary results using an LLM-based evaluation framework aligned to a cognitively grounded rubric. This enables scalable, systematic comparisons across architectural variants in early-stage experimentation. The preliminary results show that our full system consistently outperforms baseline variants. Analysis reveals that removing memory or symbolic structure degrades key cognitive behaviors, including abstraction, adaptive probing, and conceptual continuity. These findings support a processing-level account in which architectural scaffolds can reliably shape emergent instructional strategies in LLMs.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing accuracy and hallucination of LLMs in Alzheimer's disease research through knowledge graphs</title>
<link>https://arxiv.org/abs/2508.21238</link>
<guid>https://arxiv.org/abs/2508.21238</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, chatbots, GraphRAG, Alzheimer's disease, knowledge base

Summary:
Graph-based Retrieval-Augmented Generation (GraphRAG) has shown promise in improving chatbot reliability by incorporating domain-specific knowledge. This study evaluates the quality and traceability of GraphRAG systems in the context of Alzheimer's disease research. By compiling a database of papers and expert questions, a knowledge base is constructed and used with GPT-4o for answering queries. The responses generated by GraphRAG are compared with those from a standard GPT-4o model, showcasing potential improvements in response quality. The study also evaluates the traceability of various Retrieval-Augmented Generation systems, providing insights into their performance. An easy-to-use interface with an Alzheimer's disease database is offered for researchers to test both standard RAG and GraphRAG systems. Overall, the research highlights the benefits of integrating GraphRAG in scientific research domains like Alzheimer's disease for enhancing chatbot capabilities and knowledge base utilization.<br /><br />Summary: <div>
arXiv:2508.21238v1 Announce Type: new 
Abstract: In the past two years, large language model (LLM)-based chatbots, such as ChatGPT, have revolutionized various domains by enabling diverse task completion and question-answering capabilities. However, their application in scientific research remains constrained by challenges such as hallucinations, limited domain-specific knowledge, and lack of explainability or traceability for the response. Graph-based Retrieval-Augmented Generation (GraphRAG) has emerged as a promising approach to improving chatbot reliability by integrating domain-specific contextual information before response generation, addressing some limitations of standard LLMs. Despite its potential, there are only limited studies that evaluate GraphRAG on specific domains that require intensive knowledge, like Alzheimer's disease or other biomedical domains. In this paper, we assess the quality and traceability of two popular GraphRAG systems. We compile a database of 50 papers and 70 expert questions related to Alzheimer's disease, construct a GraphRAG knowledge base, and employ GPT-4o as the LLM for answering queries. We then compare the quality of responses generated by GraphRAG with those from a standard GPT-4o model. Additionally, we discuss and evaluate the traceability of several Retrieval-Augmented Generation (RAG) and GraphRAG systems. Finally, we provide an easy-to-use interface with a pre-built Alzheimer's disease database for researchers to test the performance of both standard RAG and GraphRAG.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiFluxAI Enhancing Platform Engineering with Advanced Agent-Orchestrated Retrieval Systems</title>
<link>https://arxiv.org/abs/2508.21307</link>
<guid>https://arxiv.org/abs/2508.21307</guid>
<content:encoded><![CDATA[
<div> Keywords: MultiFluxAI, AI platform, Generative AI, user engagement, data integration 

Summary:
MultiFluxAI is an innovative AI platform designed to manage and integrate vast data sources in product engineering within various application domains. Leveraging advanced techniques like Generative AI, vectorization, and agentic orchestration, the platform offers context-aware responses to complex user queries. It aims to enhance user engagement in the digital ecosystem by addressing both current and new service-related queries effectively. MultiFluxAI provides dynamic responses, making it a valuable tool for navigating the challenges of integrating disparate data sources in product engineering. The platform's use of advanced AI techniques enables it to offer personalized and relevant solutions to users, ultimately improving the overall user experience and efficiency in data management. <div>
arXiv:2508.21307v1 Announce Type: new 
Abstract: MultiFluxAI is an innovative AI platform developed to address the challenges of managing and integrating vast, disparate data sources in product engineering across application domains. It addresses both current and new service related queries that enhance user engagement in the digital ecosystem. This platform leverages advanced AI techniques, such as Generative AI, vectorization, and agentic orchestration to provide dynamic and context-aware responses to complex user queries.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation</title>
<link>https://arxiv.org/abs/2508.21320</link>
<guid>https://arxiv.org/abs/2508.21320</guid>
<content:encoded><![CDATA[
<div> Keywords: medical ontology, concept representation learning, large language model, integrative ontology learning, EHR predictive models 

Summary: 
The paper introduces LINKO, a framework that utilizes large language models to enhance medical concept representation learning by leveraging multiple ontology graphs simultaneously. LINKO incorporates dual-axis knowledge propagation within and across heterogeneous ontology systems to improve concept embedding. Through the use of engineered prompts and ontology context, LINKO initializes concept embedding and performs knowledge propagation both vertically within ontology hierarchies and horizontally across different ontology systems. Experimental results on public datasets demonstrate LINKO's superior performance compared to existing methods. As a plug-in encoder for electronic health record (EHR) predictive models, LINKO shows enhanced robustness in scenarios with limited data availability and in predicting rare diseases.<br /><br />Summary: <div>
arXiv:2508.21320v1 Announce Type: new 
Abstract: Medical ontology graphs map external knowledge to medical codes in electronic health records via structured relationships. By leveraging domain-approved connections (e.g., parent-child), predictive models can generate richer medical concept representations by incorporating contextual information from related concepts. However, existing literature primarily focuses on incorporating domain knowledge from a single ontology system, or from multiple ontology systems (e.g., diseases, drugs, and procedures) in isolation, without integrating them into a unified learning structure. Consequently, concept representation learning often remains limited to intra-ontology relationships, overlooking cross-ontology connections. In this paper, we propose LINKO, a large language model (LLM)-augmented integrative ontology learning framework that leverages multiple ontology graphs simultaneously by enabling dual-axis knowledge propagation both within and across heterogeneous ontology systems to enhance medical concept representation learning. Specifically, LINKO first employs LLMs to provide a graph-retrieval-augmented initialization for ontology concept embedding, through an engineered prompt that includes concept descriptions, and is further augmented with ontology context. Second, our method jointly learns the medical concepts in diverse ontology graphs by performing knowledge propagation in two axes: (1) intra-ontology vertical propagation across hierarchical ontology levels and (2) inter-ontology horizontal propagation within every level in parallel. Last, through extensive experiments on two public datasets, we validate the superior performance of LINKO over state-of-the-art baselines. As a plug-in encoder compatible with existing EHR predictive models, LINKO further demonstrates enhanced robustness in scenarios involving limited data availability and rare disease prediction.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models</title>
<link>https://arxiv.org/abs/2508.21365</link>
<guid>https://arxiv.org/abs/2508.21365</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, procedural knowledge, reinforcement learning, interactive tasks, transparency

Summary:<br /><br />
Large language models (LLMs) excel at complex reasoning tasks but struggle with simple interactive tasks. Traditional reinforcement learning agents acquire procedural knowledge through environmental interaction but lack transparency. The Think in Games (TiG) framework empowers LLMs to develop procedural understanding through direct interaction with game environments. TiG reformulates RL-based decision-making as a language modeling task, generating language-guided policies refined through online reinforcement learning. Experimental results show TiG bridges the gap between declarative and procedural knowledge with lower data and computational demands than conventional RL methods. TiG provides step-by-step natural language explanations for decisions, enhancing transparency and interpretability in complex interactive tasks. <div>
arXiv:2508.21365v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at complex reasoning tasks such as mathematics and coding, yet they frequently struggle with simple interactive tasks that young children perform effortlessly. This discrepancy highlights a critical gap between declarative knowledge (knowing about something) and procedural knowledge (knowing how to do something). Although traditional reinforcement learning (RL) agents can acquire procedural knowledge through environmental interaction, they often operate as black boxes and require substantial training data. In contrast, LLMs possess extensive world knowledge and reasoning capabilities, but are unable to effectively convert this static knowledge into dynamic decision-making in interactive settings. To address this challenge, we propose Think in Games (TiG), a novel framework that empowers LLMs to develop procedural understanding through direct interaction with game environments, while retaining their inherent reasoning and explanatory abilities. Specifically, TiG reformulates RL-based decision-making as a language modeling task: LLMs generate language-guided policies, which are refined iteratively through online reinforcement learning based on environmental feedback. Our experimental results show that TiG successfully bridges the gap between declarative and procedural knowledge, achieving competitive performance with dramatically lower data and computational demands compared to conventional RL methods. Moreover, TiG provides step-by-step natural language explanations for its decisions, greatly improving transparency and interpretability in complex interactive tasks.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AHELM: A Holistic Evaluation of Audio-Language Models</title>
<link>https://arxiv.org/abs/2508.21376</link>
<guid>https://arxiv.org/abs/2508.21376</guid>
<content:encoded><![CDATA[
<div> benchmark, audio-language models, evaluation, fairness, safety
Summary:
- Introduction of AHELM, a benchmark for evaluating audio-language models (ALMs) across 10 aspects including audio perception, knowledge, reasoning, bias, fairness, and safety.
- AHELM aggregates various datasets, including new synthetic audio-text datasets called PARADE and CoRe-Bench, to provide a holistic evaluation of ALMs.
- Standardized prompts, inference parameters, and evaluation metrics are used to ensure fair comparisons across models.
- Testing 14 ALMs and baseline systems show Gemini 2.5 Pro ranking top in 5 aspects but exhibiting group fairness issues on ASR tasks.
- Baseline systems also perform well on AHELM, with one ranking 5th overall despite having limited capabilities. <br /><br />Summary: <div>
arXiv:2508.21376v1 Announce Type: new 
Abstract: Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and text as input and output text -- are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets -- including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering -- to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 5th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs are available on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is intended to be a living benchmark and new datasets and models will be added over time.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Compute Architecture and Evolution Trends</title>
<link>https://arxiv.org/abs/2508.21394</link>
<guid>https://arxiv.org/abs/2508.21394</guid>
<content:encoded><![CDATA[
<div> Keywords: AI development, compute architecture, large-scale language models, AI agents, economic issues

Summary:
The article presents a structured analysis of the opportunities and challenges in AI development, focusing on a seven-layer model for AI compute architecture. It explores the evolution of AI computing through large-scale language models (LLMs) and discusses the impact of Scale-Up and Scale-Out strategies in Layers 1 and 2. The development paths for LLMs are explored in Layer 3, while Layer 4 delves into the impact of contextual memory. Layers 5 to 7 discuss the trends in AI agents, from single agents to AI-based ecosystems, and their impact on the industry. The analysis also considers economic issues in creating a self-sustainable ecosystem, drawing predictions on the future trajectory of AI development based on insights from the internet industry.<br /><br />Summary: <div>
arXiv:2508.21394v1 Announce Type: new 
Abstract: The focus of AI development has shifted from academic research to practical applications. However, AI development faces numerous challenges at various levels. This article will attempt to analyze the opportunities and challenges of AI from several different perspectives using a structured approach. This article proposes a seven-layer model for AI compute architecture, including Physical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer, Orchestrator Layer, and Application Layer, from bottom to top. It also explains how AI computing has evolved into this 7-layer architecture through the three-stage evolution on large-scale language models (LLMs). For each layer, we describe the development trajectory and key technologies. In Layers 1 and 2 we discuss AI computing issues and the impact of Scale-Up and Scale-Out strategies on computing architecture. In Layer 3 we explore two different development paths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs and compares it to traditional processor memory. In Layers 5 to 7 we discuss the trends of AI agents and explore the issues in evolution from a single AI agent to an AI-based ecosystem, and their impact on the AI industry. Furthermore, AI development involves not only technical challenges but also the economic issues to build self-sustainable ecosystem. This article analyzes the internet industry to provide predictions on the future trajectory of AI development.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARJAN: Agent-Based Generation and Simulation of Traffic Scenarios with AJAN</title>
<link>https://arxiv.org/abs/2508.21411</link>
<guid>https://arxiv.org/abs/2508.21411</guid>
<content:encoded><![CDATA[
<div> Keywords: urban traffic scenarios, interacting agents, simulation, CARJAN, CARLA

Summary:
CARJAN is a new tool designed to facilitate the modeling and virtual simulation of urban traffic scenarios involving various types of interacting agents such as pedestrians, cyclists, and autonomous vehicles. The tool integrates the AJAN multi-agent engineering framework and the CARLA driving simulator, offering a user-friendly interface for creating, storing, and managing traffic scenario layouts. Utilizing SPARQL Behavior Tree-based decision-making processes, CARJAN enables dynamic scenario simulations in CARLA, enhancing the realism of agent interactions. By combining interactive, intelligent agent-based generation with simulation capabilities, CARJAN represents a significant advancement in the field of virtual traffic scenario modeling. This innovative approach addresses the complex challenges associated with urban traffic simulation, providing a comprehensive solution for researchers and practitioners in the transportation industry.<br /><br />Summary: <div>
arXiv:2508.21411v1 Announce Type: new 
Abstract: User-friendly modeling and virtual simulation of urban traffic scenarios with different types of interacting agents such as pedestrians, cyclists and autonomous vehicles remains a challenge. We present CARJAN, a novel tool for semi-automated generation and simulation of such scenarios based on the multi-agent engineering framework AJAN and the driving simulator CARLA. CARJAN provides a visual user interface for the modeling, storage and maintenance of traffic scenario layouts, and leverages SPARQL Behavior Tree-based decision-making and interactions for agents in dynamic scenario simulations in CARLA. CARJAN provides a first integrated approach for interactive, intelligent agent-based generation and simulation of virtual traffic scenarios in CARLA.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A General Framework of Epistemic Forgetting and its Instantiation by Ranking Functions</title>
<link>https://arxiv.org/abs/2508.21441</link>
<guid>https://arxiv.org/abs/2508.21441</guid>
<content:encoded><![CDATA[
<div> Keywords: forgetting, knowledge management, epistemic states, logic programming, AGM belief revision theory

Summary: 
This article explores the concept of forgetting as a knowledge management operation in an epistemic context. It delves into the various facets of forgetting, such as syntax, propositions, and conditionals, and discusses two main operators – variable elimination and contraction. The focus is on studying forgetting operations in epistemic states with richer semantic structures, allowing for a deeper understanding of what forgetting means in this context. Five general types of epistemic forgetting are identified and instantiated with seven concrete forgetting operations for Spohn's ranking functions. The article draws inspiration from postulates of forgetting in logic programming and AGM theory to propose a comprehensive set of axioms for evaluating forgetting operations. Through a thorough evaluation based on these postulates, the article provides a detailed overview of the differences and commonalities among various forgetting operators. <div>
arXiv:2508.21441v1 Announce Type: new 
Abstract: Forgetting as a knowledge management operation deliberately ignores parts of the knowledge and beliefs of an agent, for various reasons. Forgetting has many facets, one may want to forget parts of the syntax, a proposition, or a conditional. In the literature, two main operators suitable for performing forgetting have been proposed and investigated in depth: First, variable elimination is a syntactical method that blends out certain atomic variables to focus on the rest of the language. It has been mainly used in the area of logic programming and answer set programming. Second, contraction in AGM belief revision theory effectively removes propositions from belief sets under logical deduction. Both operations rely mainly on classical logics. In this article, we take an epistemic perspective and study forgetting operations in epistemic states with richer semantic structures, but with clear links to propositional logic. This allows us to investigate what forgetting in the epistemic background means, thereby lifting well-known and novel forgetting operations to the epistemic level. We present five general types of epistemic forgetting and instantiate them with seven concrete forgetting operations for Spohn's ranking functions. We take inspiration from postulates of forgetting both from logic programming and AGM theory to propose a rich landscape of axioms for evaluating forgetting operations. Finally, we evaluate all concrete forgetting operations according to all postulates, leading to a novel comprehensive overview highlighting differences and commonalities among the forgetting operators.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Lifted Action Models From Traces of Incomplete Actions and States</title>
<link>https://arxiv.org/abs/2508.21449</link>
<guid>https://arxiv.org/abs/2508.21449</guid>
<content:encoded><![CDATA[
<div> learning, lifted STRIPS model, sliding-tile puzzle, STRIPS+, SYNTH

Summary: 
The article addresses learning a lifted STRIPS model of the sliding-tile puzzle from random state-action traces. The challenges include incomplete states and actions in the traces. A new variant, STRIPS+, is introduced to handle implicit action arguments and limited quantification in preconditions. The proposed SYNTH algorithm constructs queries for each action to ground implicit action arguments in STRIPS+. SYNTH is proven to be correct and complete, and its scalability is tested on traces from existing STRIPS domains. <div>
arXiv:2508.21449v1 Announce Type: new 
Abstract: Consider the problem of learning a lifted STRIPS model of the sliding-tile puzzle from random state-action traces where the states represent the location of the tiles only, and the actions are the labels up, down, left, and right, with no arguments. Two challenges are involved in this problem. First, the states are not full STRIPS states, as some predicates are missing, like the atoms representing the position of the ``blank''. Second, the actions are not full STRIPS either, as they do not reveal all the objects involved in the actions effects and preconditions. Previous approaches have addressed different versions of this model learning problem, but most assume that actions in the traces are full STRIPS actions or that the domain predicates are all observable. The new setting considered in this work is more ``realistic'', as the atoms observed convey the state of the world but not full STRIPS states, and the actions reveal the arguments needed for selecting the action but not the ones needed for modeling it in STRIPS. For formulating and addressing the learning problem, we introduce a variant of STRIPS, which we call STRIPS+, where certain STRIPS action arguments can be left implicit in preconditions which can also involve a limited form of existential quantification. The learning problem becomes the problem of learning STRIPS+ models from STRIPS+ state-action traces. For this, the proposed learning algorithm, called SYNTH, constructs a stratified sequence (conjunction) of precondition expressions or ``queries'' for each action, that denote unique objects in the state and ground the implicit action arguments in STRIPS+. The correctness and completeness of SYNTH is established, and its scalability is tested on state-action traces obtained from STRIPS+ models derived from existing STRIPS domains.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents</title>
<link>https://arxiv.org/abs/2508.21475</link>
<guid>https://arxiv.org/abs/2508.21475</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal language models, MMSearch-Plus benchmark, fine-grained visual reasoning, provenance verification, long-horizon tool use

Summary: 
The article introduces the MMSearch-Plus benchmark consisting of 311 tasks that challenge multimodal understanding. These tasks involve extracting weak visual signals, conducting iterative text-image search, and cross-validating answers. The curation process, Spatial-Temporal Extrapolation, focuses on extrapolating information from spatial and temporal cues to answer questions accurately. A model-agnostic agent framework with browsing tools is provided, and a range of closed and open multimodal language models are evaluated. The strongest agent achieves high accuracy with rollout under the framework, while a strong open-source model shows improvement after search rounds. Bounding-box production and cropped-image search are also assessed, and an error analysis reveals challenges in source verification, part-based reasoning, and long-horizon planning. <div>
arXiv:2508.21475v1 Announce Type: new 
Abstract: Large multimodal language models (MLLMs) are increasingly deployed as web agents, yet many multimodal browsing benchmarks can be solved by shallow, fixed workflows that lean on high-recall image search and nearby text-masking the genuinely multimodal challenges of fine-grained visual reasoning, provenance verification, and long-horizon tool use. We introduce MMSearch-Plus, a benchmark of 311 tasks that highly demand multimodal understanding while preserving the difficulty profile of strong text-only browsing suites. Each item is constructed to contain multiple weak, localized visual signals that must be extracted, propagated through iterative text-image search, and cross-validated under retrieval noise before answering. Our curation procedure, Spatial-Temporal Extrapolation, seeds questions whose answers require extrapolating from spatial cues (micro-text, part-level appearance, layouts, signage) and temporal traces (broadcast overlays, seasonal context) to out-of-image facts such as events, dates, and venues. We provide a model-agnostic agent framework with browsing tools and evaluate a range of closed and open MLLMs. The strongest agent (o3) attains 15.1% without search and 36.0% accuracy with rollout under our framework, while a strong open-source model (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20 rounds of search. Beyond answer accuracy, we assess bounding-box production and cropped-image search, and conduct an error analysis that surfaces failures in source verification, part-based reasoning, and long-horizon planning.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by Phronesis</title>
<link>https://arxiv.org/abs/2508.21517</link>
<guid>https://arxiv.org/abs/2508.21517</guid>
<content:encoded><![CDATA[
<div> Computational framework, wisdom, multidimensional, uncertainty-conscious, AI <br /> 
Summary: <br /> 
- A new computational framework for measuring wisdom was developed, considering multiple dimensions and uncertainty. 
- Traditional wisdom measures often lack the humility and uncertainty inherent in wise reasoning. 
- The study used a fuzzy inference system with Z numbers to assess wisdom scores and confidence in decision-making. 
- Participants were presented with moral dilemma tasks and their responses were mapped into five components of wisdom. 
- Results showed promising correlations with established scales, supporting the validity of the new system. 
- This framework contributes to advancing measurement in psychology and offers AI systems a way to incorporate uncertainty-conscious reasoning for safer, more human-like judgment. <div>
arXiv:2508.21517v1 Announce Type: new 
Abstract: Background: Wisdom is a superordinate construct that embraces perspective taking, reflectiveness, prosocial orientation, reflective empathetic action, and intellectual humility. Unlike conventional models of reasoning that are rigidly bound by binary thinking, wisdom unfolds in shades of ambiguity, requiring both graded evaluation and self-reflective humility. Current measures depend on self-reports and seldom reflect the humility and uncertainty inherent in wise reasoning. A computational framework that takes into account both multidimensionality and confidence has the potential to improve psychological science and allow humane AI. Method: We present a fuzzy inference system with Z numbers, each of the decisions being expressed in terms of a wisdom score (restriction) and confidence score (certainty). As part of this study, participants (N = 100) were exposed to culturally neutral pictorial moral dilemma tasks to which they generated think-aloud linguistic responses, which were mapped into five theoretically based components of wisdom. The scores of each individual component were combined using a base of 21 rules, with membership functions tuned via Gaussian kernel density estimation. Results: In a proof of concept study, the system produced dual attribute wisdom representations that correlated modestly but significantly with established scales while showing negligible relations with unrelated traits, supporting convergent and divergent validity. Contribution: The contribution is to formalize wisdom as a multidimensional, uncertainty-conscious construct, operationalized in the form of Z-numbers. In addition to progressing measurement in psychology, it calculates how fuzzy Z numbers can provide AI systems with interpretable, confidence-sensitive reasoning that affords a safe, middle ground between rigorous computation and human-like judgment.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Scenarios for Automated Planning</title>
<link>https://arxiv.org/abs/2508.21521</link>
<guid>https://arxiv.org/abs/2508.21521</guid>
<content:encoded><![CDATA[
<div> Explanation paradigm, Counterfactual scenarios, Planning problem, LTLf formula, Computational complexity

Summary:
Counterfactual Explanations (CEs) are used to explain Machine Learning models by showing minimal input changes for a different output. A new paradigm is proposed for planning problems and desired plan properties using counterfactual scenarios. These scenarios require minimal modifications to allow plans that meet specified criteria. The proposal includes two qualitative instantiations involving plan quantification satisfying desired properties. Computational complexity analysis shows generating counterfactual scenarios is practical, often as costly as computing a plan. This framework offers a basis for practical algorithm development in this field. <br /><br />Summary: <div>
arXiv:2508.21521v1 Announce Type: new 
Abstract: Counterfactual Explanations (CEs) are a powerful technique used to explain Machine Learning models by showing how the input to a model should be minimally changed for the model to produce a different output. Similar proposals have been made in the context of Automated Planning, where CEs have been characterised in terms of minimal modifications to an existing plan that would result in the satisfaction of a different goal. While such explanations may help diagnose faults and reason about the characteristics of a plan, they fail to capture higher-level properties of the problem being solved. To address this limitation, we propose a novel explanation paradigm that is based on counterfactual scenarios. In particular, given a planning problem $P$ and an \ltlf formula $\psi$ defining desired properties of a plan, counterfactual scenarios identify minimal modifications to $P$ such that it admits plans that comply with $\psi$. In this paper, we present two qualitative instantiations of counterfactual scenarios based on an explicit quantification over plans that must satisfy $\psi$. We then characterise the computational complexity of generating such counterfactual scenarios when different types of changes are allowed on $P$. We show that producing counterfactual scenarios is often only as expensive as computing a plan for $P$, thus demonstrating the practical viability of our proposal and ultimately providing a framework to construct practical algorithms in this area.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining</title>
<link>https://arxiv.org/abs/2508.21540</link>
<guid>https://arxiv.org/abs/2508.21540</guid>
<content:encoded><![CDATA[
<div> framework, process mining, healthcare, Large Language Models, automated interpretation<br />
Summary:<br />
HealthProcessAI is a GenAI framework that simplifies process mining in healthcare by integrating Python and R libraries. It utilizes Large Language Models (LLMs) for automated process map interpretation and report generation, making complex analyses more accessible. The framework was validated using sepsis data, demonstrating technical performance and report generation through LLM analysis. Five LLM models were evaluated, with Claude Sonnet-4 and Gemini 2.5-Pro scoring highest in consistency. By combining structured analytics and AI-driven interpretation, HealthProcessAI translates process mining results into actionable insights for healthcare applications. This novel approach addresses barriers like technical complexity and lack of standardized approaches, making process mining more user-friendly for clinicians, data scientists, and researchers.<br /> 
Summary: <div>
arXiv:2508.21540v1 Announce Type: new 
Abstract: Process mining has emerged as a powerful analytical technique for understanding complex healthcare workflows. However, its application faces significant barriers, including technical complexity, a lack of standardized approaches, and limited access to practical training resources. We introduce HealthProcessAI, a GenAI framework designed to simplify process mining applications in healthcare and epidemiology by providing a comprehensive wrapper around existing Python (PM4PY) and R (bupaR) libraries. To address unfamiliarity and improve accessibility, the framework integrates multiple Large Language Models (LLMs) for automated process map interpretation and report generation, helping translate technical analyses into outputs that diverse users can readily understand. We validated the framework using sepsis progression data as a proof-of-concept example and compared the outputs of five state-of-the-art LLM models through the OpenRouter platform. To test its functionality, the framework successfully processed sepsis data across four proof-of-concept scenarios, demonstrating robust technical performance and its capability to generate reports through automated LLM analysis. LLM evaluation using five independent LLMs as automated evaluators revealed distinct model strengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency scores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By integrating multiple Large Language Models (LLMs) for automated interpretation and report generation, the framework addresses widespread unfamiliarity with process mining outputs, making them more accessible to clinicians, data scientists, and researchers. This structured analytics and AI-driven interpretation combination represents a novel methodological advance in translating complex process mining results into potentially actionable insights for healthcare applications.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Landmarks: Learning from Previous Plans to Generalize over Problem Instances</title>
<link>https://arxiv.org/abs/2508.21564</link>
<guid>https://arxiv.org/abs/2508.21564</guid>
<content:encoded><![CDATA[
<div> Keywords: generalized landmarks, planning, landmark extraction, state functions, heuristic<br />
<br />
Summary: 
The article introduces a new framework for discovering generalized landmarks that can be applied across a domain. These landmarks are learned from solved instances and serve as intermediate goals for planning problems. Unlike traditional landmark extraction algorithms, the proposed method uses state functions that are independent of objects, capturing repetition and generalizing beyond specific instances. By constructing a directed generalized landmark graph based on these functions, the framework defines landmark progression and allows for loop possibilities for repetitive subplans. The results demonstrate that these learned generalized landmarks from small instances are effective for larger instances within the same domain. Notably, identifying loops indicating repetition leads to significant improvements in heuristic performance compared to a baseline. Generalized landmarks provide interpretable and valuable domain information for automated planners, and can be extracted from a small set of plans. <div>
arXiv:2508.21564v1 Announce Type: new 
Abstract: We propose a new framework for discovering landmarks that automatically generalize across a domain. These generalized landmarks are learned from a set of solved instances and describe intermediate goals for planning problems where traditional landmark extraction algorithms fall short. Our generalized landmarks extend beyond the predicates of a domain by using state functions that are independent of the objects of a specific problem and apply to all similar objects, thus capturing repetition. Based on these functions, we construct a directed generalized landmark graph that defines the landmark progression, including loop possibilities for repetitive subplans. We show how to use this graph in a heuristic to solve new problem instances of the same domain. Our results show that the generalized landmark graphs learned from a few small instances are also effective for larger instances in the same domain. If a loop that indicates repetition is identified, we see a significant improvement in heuristic performance over the baseline. Generalized landmarks capture domain information that is interpretable and useful to an automated planner. This information can be discovered from a small set of plans for the same domain.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Solution Methods for Dec-POMDPs with Deterministic Dynamics</title>
<link>https://arxiv.org/abs/2508.21595</link>
<guid>https://arxiv.org/abs/2508.21595</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent planning, deterministic actions, decentralized POMDPs, Iterative Deterministic POMDP Planning, large-scale domains

Summary: 
Det-Dec-POMDPs are a subset of Dec-POMDPs that consider deterministic transitions and observations in high-level multi-agent planning problems. The proposed Iterative Deterministic POMDP Planning (IDPP) is a solver designed to handle large-scale Det-Dec-POMDPs efficiently by leveraging the Joint Equilibrium Search for Policies framework. The IDPP method addresses the limitations of current Dec-POMDP solvers, allowing for the effective modeling of deterministic actions and observations in multi-robot navigation and path planning domains.<br /><br /> Summary: <div>
arXiv:2508.21595v1 Announce Type: new 
Abstract: Many high-level multi-agent planning problems, including multi-robot navigation and path planning, can be effectively modeled using deterministic actions and observations.
  In this work, we focus on such domains and introduce the class of Deterministic Decentralized POMDPs (Det-Dec-POMDPs). This is a subclass of Dec-POMDPs characterized by deterministic transitions and observations conditioned on the state and joint actions.
  We then propose a practical solver called Iterative Deterministic POMDP Planning (IDPP). This method builds on the classic Joint Equilibrium Search for Policies framework and is specifically optimized to handle large-scale Det-Dec-POMDPs that current Dec-POMDP solvers are unable to address efficiently.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study</title>
<link>https://arxiv.org/abs/2508.21622</link>
<guid>https://arxiv.org/abs/2508.21622</guid>
<content:encoded><![CDATA[
<div> Keywords: network optimization, large language models, supply chain planning, decision support, explainability

Summary:
This paper introduces a novel framework that combines traditional network optimization models with large language models to provide interactive, explainable, and role-aware decision support for supply chain planning. The system generates natural language summaries, contextual visualizations, and tailored key performance indicators to bridge the gap between complex operations research outputs and business stakeholder understanding. The core optimization model focuses on tactical inventory redistribution across a network of distribution centers for multi-period and multi-item scenarios. The technical architecture incorporates AI agents, RESTful APIs, and a dynamic user interface for real-time interaction, configuration updates, and simulation-based insights. A case study demonstrates the system's ability to enhance planning outcomes by preventing stockouts, reducing costs, and maintaining service levels. Future extensions include integrating private LLMs, transfer learning, reinforcement learning, and Bayesian neural networks to improve explainability, adaptability, and real-time decision-making.<br /><br />Summary: <div>
arXiv:2508.21622v1 Announce Type: new 
Abstract: This paper presents an integrated framework that combines traditional network optimization models with large language models (LLMs) to deliver interactive, explainable, and role-aware decision support for supply chain planning. The proposed system bridges the gap between complex operations research outputs and business stakeholder understanding by generating natural language summaries, contextual visualizations, and tailored key performance indicators (KPIs). The core optimization model addresses tactical inventory redistribution across a network of distribution centers for multi-period and multi-item, using a mixed-integer formulation. The technical architecture incorporates AI agents, RESTful APIs, and a dynamic user interface to support real-time interaction, configuration updates, and simulation-based insights. A case study demonstrates how the system improves planning outcomes by preventing stockouts, reducing costs, and maintaining service levels. Future extensions include integrating private LLMs, transfer learning, reinforcement learning, and Bayesian neural networks to enhance explainability, adaptability, and real-time decision-making.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A-MHA*: Anytime Multi-Heuristic A*</title>
<link>https://arxiv.org/abs/2508.21637</link>
<guid>https://arxiv.org/abs/2508.21637</guid>
<content:encoded><![CDATA[
<div> heuristics, graph search, bounded suboptimal search, anytime algorithm, A-MHA*

Summary:
The article discusses the importance of designing good heuristics for graph search based on domain knowledge. It introduces an extension of the Multi-Heuristic A* (MHA*) algorithm to an anytime version called A-MHA*, inspired by Anytime Repairing A* (ARA*). A-MHA* aims to find a feasible suboptimal solution quickly and continuously improve it until time constraints are reached. The adaptation of ARA* concepts into the MHA* framework maintains suboptimal and completeness guarantees while enhancing the algorithm to perform continuously. Performance tests are conducted in 3-D path planning and sliding tiles puzzle domains, comparing A-MHA* against MHA* and other anytime algorithms. <div>
arXiv:2508.21637v1 Announce Type: new 
Abstract: Designing good heuristic functions for graph search requires adequate domain knowledge. It is often easy to design heuristics that perform well and correlate with the underlying true cost-to-go values in certain parts of the search space but these may not be admissible throughout the domain thereby affecting the optimality guarantees of the search. Bounded suboptimal search using several such partially good but inadmissible heuristics was developed in Multi-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible heuristics to potentially generate a faster suboptimal solution, the original version does not improve the solution over time. It is a one shot algorithm that requires careful setting of inflation factors to obtain a desired one time solution. In this work, we tackle this issue by extending MHA* to an anytime version that finds a feasible suboptimal solution quickly and continually improves it until time runs out. Our work is inspired from the Anytime Repairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA* concepts in the MHA* framework preserves the original suboptimal and completeness guarantees and enhances MHA* to perform in an anytime fashion. Furthermore, we report the performance of A-MHA* in 3-D path planning domain and sliding tiles puzzle and compare against MHA* and other anytime algorithms.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI</title>
<link>https://arxiv.org/abs/2508.21648</link>
<guid>https://arxiv.org/abs/2508.21648</guid>
<content:encoded><![CDATA[
<div> AI, bias, medical, diversity, diagnosis
Summary: 
The article introduces the concept of leveraging diversity in medical artificial intelligence (AI) systems, proposing the MEDLEY framework as a new approach. Rather than eliminating biases in AI models, MEDLEY preserves their diverse outputs and considers biases as potential strengths. By documenting model-specific biases and treating hallucinations as provisional hypotheses for clinician verification, the framework enhances medical reasoning under clinician supervision. A proof-of-concept demonstrator using over 30 large language models was developed to illustrate the approach, creating a minimum viable product that maintains both consensus and minority views in synthetic cases. This transparency in diagnostic uncertainty and biases allows for clinical oversight, reframing AI imperfections as a resource for developing trustworthy medical AI systems. MEDLEY offers a paradigm shift that opens new pathways for regulation, ethics, and innovation in the field of medical AI. 
<br /><br />Summary: <div>
arXiv:2508.21648v1 Announce Type: new 
Abstract: Bias in medical artificial intelligence is conventionally viewed as a defect requiring elimination. However, human reasoning inherently incorporates biases shaped by education, culture, and experience, suggesting their presence may be inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble Diagnostic system with Leveraged diversitY), a conceptual framework that orchestrates multiple AI models while preserving their diverse outputs rather than collapsing them into a consensus. Unlike traditional approaches that suppress disagreement, MEDLEY documents model-specific biases as potential strengths and treats hallucinations as provisional hypotheses for clinician verification. A proof-of-concept demonstrator was developed using over 30 large language models, creating a minimum viable product that preserved both consensus and minority views in synthetic cases, making diagnostic uncertainty and latent biases transparent for clinical oversight. While not yet a validated clinical tool, the demonstration illustrates how structured diversity can enhance medical reasoning under clinician supervision. By reframing AI imperfection as a resource, MEDLEY offers a paradigm shift that opens new regulatory, ethical, and innovation pathways for developing trustworthy medical AI systems.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PosterForest: Hierarchical Multi-Agent Collaboration for Scientific Poster Generation</title>
<link>https://arxiv.org/abs/2508.21720</link>
<guid>https://arxiv.org/abs/2508.21720</guid>
<content:encoded><![CDATA[
<div> Keywords: automated scientific poster generation, hierarchical structure, Poster Tree, multi-agent collaboration strategy, content summarization

Summary:
The article introduces a novel framework called PosterForest for automated scientific poster generation, which focuses on capturing the hierarchical structure of scientific documents and integrating textual and visual elements effectively. The framework utilizes the Poster Tree, an intermediate representation that encodes document structure and visual-textual relationships at multiple levels. By employing a multi-agent collaboration strategy involving content summarization and layout planning agents, the framework ensures logical consistency, content fidelity, and visual coherence in the generated posters. Extensive experiments across various academic domains demonstrate that PosterForest outperforms existing baselines in both qualitative and quantitative evaluations. The generated posters exhibit high quality similar to expert-designed ones, with superior information preservation, structural clarity, and user preference. 

<br /><br />Summary: <div>
arXiv:2508.21720v1 Announce Type: new 
Abstract: We present a novel training-free framework, \textit{PosterForest}, for automated scientific poster generation. Unlike prior approaches, which largely neglect the hierarchical structure of scientific documents and the semantic integration of textual and visual elements, our method addresses both challenges directly. We introduce the \textit{Poster Tree}, a hierarchical intermediate representation that jointly encodes document structure and visual-textual relationships at multiple levels. Our framework employs a multi-agent collaboration strategy, where agents specializing in content summarization and layout planning iteratively coordinate and provide mutual feedback. This approach enables the joint optimization of logical consistency, content fidelity, and visual coherence. Extensive experiments on multiple academic domains show that our method outperforms existing baselines in both qualitative and quantitative evaluations. The resulting posters achieve quality closest to expert-designed ground truth and deliver superior information preservation, structural clarity, and user preference.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem</title>
<link>https://arxiv.org/abs/2508.21730</link>
<guid>https://arxiv.org/abs/2508.21730</guid>
<content:encoded><![CDATA[
<div> Variational algorithm, Traveling Salesman Problem, qubit requirement, optimize-freeze-reuse strategy, symmetric instances <br />
<br />
Summary: 
This paper introduces a variational algorithm for the Traveling Salesman Problem (TSP) that addresses scalability limitations. The algorithm utilizes a compact encoding of permutations to reduce the qubit requirement and implements an optimize-freeze-reuse strategy. By optimizing the circuit topology on a training instance and then freezing it for novel instances, the algorithm achieves high success rates for TSP instances with 4 to 6 cities. However, the success rate drops significantly for instances with 7 cities, highlighting scalability issues. The study demonstrates the generalization ability of the approach for moderate problem sizes and emphasizes the importance of freezing the Ansatz to reduce time-to-solution without sacrificing quality. Additionally, the paper discusses the impact of warm-start initialization of parameters and explores potential extensions to more complex optimization problems like Vehicle Routing and Job-Shop Scheduling. <div>
arXiv:2508.21730v1 Announce Type: new 
Abstract: In this paper we present a variational algorithm for the Traveling Salesman Problem (TSP) that combines (i) a compact encoding of permutations, which reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy: where the circuit topology (``Ansatz'') is first optimized on a training instance by Simulated Annealing (SA), then ``frozen'' and re-used on novel instances, limited to a rapid re-optimization of only the circuit parameters. This pipeline eliminates costly structural research in testing, making the procedure immediately implementable on NISQ hardware.
  On a set of $40$ randomly generated symmetric instances that span $4 - 7$ cities, the resulting Ansatz achieves an average optimal trip sampling probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for 6 city cases. With 7 cities the success rate drops markedly to an average of $\sim 20\%$, revealing the onset of scalability limitations of the proposed method.
  The results show robust generalization ability for moderate problem sizes and indicate how freezing the Ansatz can dramatically reduce time-to-solution without degrading solution quality. The paper also discusses scalability limitations, the impact of ``warm-start'' initialization of parameters, and prospects for extension to more complex problems, such as Vehicle Routing and Job-Shop Scheduling.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orientability of Causal Relations in Time Series using Summary Causal Graphs and Faithful Distributions</title>
<link>https://arxiv.org/abs/2508.21742</link>
<guid>https://arxiv.org/abs/2508.21742</guid>
<content:encoded><![CDATA[
<div> Keywords: causal relations, time series analysis, summary causal graph, edge orientation, causal discovery 

Summary:
This article focuses on understanding causal relations between temporal variables through the use of summary causal graphs (SCGs). SCGs provide a high-level abstraction of the causal graph, allowing experts to capture main causal relations while abstracting micro-level details. The study presents conditions that guarantee the orientation of micro-level edges between temporal variables based on the information encoded in SCGs. The results offer theoretical guarantees for edge orientation even in the presence of cycles or bidirected edges at the macro-level. This research is valuable for improving causal inference in complex temporal systems by leveraging expert knowledge and utilizing SCGs to inform causal discovery from observational time series data. <div>
arXiv:2508.21742v1 Announce Type: new 
Abstract: Understanding causal relations between temporal variables is a central challenge in time series analysis, particularly when the full causal structure is unknown. Even when the full causal structure cannot be fully specified, experts often succeed in providing a high-level abstraction of the causal graph, known as a summary causal graph, which captures the main causal relations between different time series while abstracting away micro-level details. In this work, we present conditions that guarantee the orientability of micro-level edges between temporal variables given the background knowledge encoded in a summary causal graph and assuming having access to a faithful and causally sufficient distribution with respect to the true unknown graph. Our results provide theoretical guarantees for edge orientation at the micro-level, even in the presence of cycles or bidirected edges at the macro-level. These findings offer practical guidance for leveraging SCGs to inform causal discovery in complex temporal systems and highlight the value of incorporating expert knowledge to improve causal inference from observational time series data.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree-Guided Diffusion Planner</title>
<link>https://arxiv.org/abs/2508.21800</link>
<guid>https://arxiv.org/abs/2508.21800</guid>
<content:encoded><![CDATA[
<div> task-specific training, value estimators, test-time flexibility, zero-shot generalization, tree search

Summary:
Tree-guided Diffusion Planner (TDP) is introduced as a test-time planning framework that utilizes structured trajectory generation for tasks with non-convex objectives and non-differentiable constraints. TDP balances exploration and exploitation by employing a bi-level sampling process: diverse parent trajectories are generated through particle guidance for broad exploration, and refined sub-trajectories are obtained via fast conditional denoising guided by task objectives. TDP overcomes the limitations of gradient guidance by exploring various trajectory regions and leveraging gradient information with pretrained models and test-time reward signals only. The proposed approach is evaluated on maze gold-picking, robot arm block manipulation, and AntMaze multi-goal exploration tasks, outperforming state-of-the-art methods consistently. This demonstrates the effectiveness of TDP in solving test-time guided control problems without the need for task-specific training or value estimators, enhancing test-time flexibility and enabling zero-shot generalization. 

<br /><br />Summary: <div>
arXiv:2508.21800v1 Announce Type: new 
Abstract: Planning with pretrained diffusion models has emerged as a promising approach for solving test-time guided control problems. However, standard gradient guidance typically performs optimally under convex and differentiable reward landscapes, showing substantially reduced effectiveness in real-world scenarios involving non-convex objectives, non-differentiable constraints, and multi-reward structures. Furthermore, recent supervised planning approaches require task-specific training or value estimators, which limits test-time flexibility and zero-shot generalization. We propose a Tree-guided Diffusion Planner (TDP), a zero-shot test-time planning framework that balances exploration and exploitation through structured trajectory generation. We frame test-time planning as a tree search problem using a bi-level sampling process: (1) diverse parent trajectories are produced via training-free particle guidance to encourage broad exploration, and (2) sub-trajectories are refined through fast conditional denoising guided by task objectives. TDP addresses the limitations of gradient guidance by exploring diverse trajectory regions and harnessing gradient information across this expanded solution space using only pretrained models and test-time reward signals. We evaluate TDP on three diverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze multi-goal exploration. TDP consistently outperforms state-of-the-art approaches on all tasks. The project page can be found at: tree-diffusion-planner.github.io.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Clinical Problem Detection from SOAP Notes using a Collaborative Multi-Agent LLM Architecture</title>
<link>https://arxiv.org/abs/2508.21803</link>
<guid>https://arxiv.org/abs/2508.21803</guid>
<content:encoded><![CDATA[
<div> Agent Team, Clinical Notes, Clinical Problems, Diagnostic Reasoning, Decision Support Tools  
Summary:  
- A collaborative multi-agent system (MAS) is introduced to interpret clinical narratives, addressing the challenges of complexity in clinical notes.  
- The MAS simulates a clinical consultation team to identify clinical problems by analyzing Subjective and Objective sections of SOAP notes.  
- The system, consisting of Manager and specialist agents, engages in hierarchical debates to reach a consensus on diagnostic reasoning.  
- Evaluation on a dataset of 420 MIMIC-III notes showed improved performance in identifying congestive heart failure, acute kidney injury, and sepsis compared to a single-agent baseline.  
- Qualitative analysis revealed that the MAS effectively considers conflicting evidence but may be susceptible to groupthink.  
<br /><br />Summary: <div>
arXiv:2508.21803v1 Announce Type: new 
Abstract: Accurate interpretation of clinical narratives is critical for patient care, but the complexity of these notes makes automation challenging. While Large Language Models (LLMs) show promise, single-model approaches can lack the robustness required for high-stakes clinical tasks. We introduce a collaborative multi-agent system (MAS) that models a clinical consultation team to address this gap. The system is tasked with identifying clinical problems by analyzing only the Subjective (S) and Objective (O) sections of SOAP notes, simulating the diagnostic reasoning process of synthesizing raw data into an assessment. A Manager agent orchestrates a dynamically assigned team of specialist agents who engage in a hierarchical, iterative debate to reach a consensus. We evaluated our MAS against a single-agent baseline on a curated dataset of 420 MIMIC-III notes. The dynamic multi-agent configuration demonstrated consistently improved performance in identifying congestive heart failure, acute kidney injury, and sepsis. Qualitative analysis of the agent debates reveals that this structure effectively surfaces and weighs conflicting evidence, though it can occasionally be susceptible to groupthink. By modeling a clinical team's reasoning process, our system offers a promising path toward more accurate, robust, and interpretable clinical decision support tools.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.19153</link>
<guid>https://arxiv.org/abs/2508.19153</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, vision-guided control, quadruped motion, proprioception, spline-parameterized policy  
Summary:  
The paper introduces QuadKAN, a spline-parameterized cross-modal policy for vision-guided quadruped motion control. It emphasizes the importance of combining proprioception with vision for robust control. QuadKAN incorporates a spline encoder for proprioception and a spline fusion head for proprioception-vision inputs, aligning the state-to-action mapping with the smooth nature of gait. The framework enhances sample efficiency, reduces action jitter and energy consumption, and provides interpretable posture-action sensitivities. Utilizing Multi-Modal Delay Randomization (MMDR) and Proximal Policy Optimization (PPO) for end-to-end training, QuadKAN outperforms state-of-the-art baselines across diverse terrains and obstacle scenarios. Results demonstrate higher returns, greater distances, and fewer collisions, showcasing spline-parameterized policies as a simple, effective, and interpretable solution for robust vision-guided locomotion. <div>
arXiv:2508.19153v1 Announce Type: cross 
Abstract: We address vision-guided quadruped motion control with reinforcement learning (RL) and highlight the necessity of combining proprioception with vision for robust control. We propose QuadKAN, a spline-parameterized cross-modal policy instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates a spline encoder for proprioception and a spline fusion head for proprioception-vision inputs. This structured function class aligns the state-to-action mapping with the piecewise-smooth nature of gait, improving sample efficiency, reducing action jitter and energy consumption, and providing interpretable posture-action sensitivities. We adopt Multi-Modal Delay Randomization (MMDR) and perform end-to-end training with Proximal Policy Optimization (PPO). Evaluations across diverse terrains, including both even and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate that QuadKAN achieves consistently higher returns, greater distances, and fewer collisions than state-of-the-art (SOTA) baselines. These results show that spline-parameterized policies offer a simple, effective, and interpretable alternative for robust vision-guided locomotion. A repository will be made available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pep2Prob Benchmark: Predicting Fragment Ion Probability for MS$^2$-based Proteomics</title>
<link>https://arxiv.org/abs/2508.21076</link>
<guid>https://arxiv.org/abs/2508.21076</guid>
<content:encoded><![CDATA[
<div> Proteins; mass spectrometry; peptide identification; peptide fragmentation; machine learning <br />
Summary: 
Proteins are essential in cellular functions and as drug targets, necessitating accurate analysis through mass spectrometry. Current peptide fragment ion probability prediction methods rely on global fragmentation statistics, limiting accuracy. To address this, Pep2Prob introduces a dataset with peptide-specific fragment ion probability statistics from high-quality MS$^2$ spectra. Simple rules and learning-based models demonstrate improved performance using peptide-specific information. The complex relationship between peptides and fragmentation requires sophisticated machine learning approaches to achieve accurate predictions. <br /> <div>
arXiv:2508.21076v1 Announce Type: cross 
Abstract: Proteins perform nearly all cellular functions and constitute most drug targets, making their analysis fundamental to understanding human biology in health and disease. Tandem mass spectrometry (MS$^2$) is the major analytical technique in proteomics that identifies peptides by ionizing them, fragmenting them, and using the resulting mass spectra to identify and quantify proteins in biological samples. In MS$^2$ analysis, peptide fragment ion probability prediction plays a critical role, enhancing the accuracy of peptide identification from mass spectra as a complement to the intensity information. Current approaches rely on global statistics of fragmentation, which assumes that a fragment's probability is uniform across all peptides. Nevertheless, this assumption is oversimplified from a biochemical principle point of view and limits accurate prediction. To address this gap, we present Pep2Prob, the first comprehensive dataset and benchmark designed for peptide-specific fragment ion probability prediction. The proposed dataset contains fragment ion probability statistics for 608,780 unique precursors (each precursor is a pair of peptide sequence and charge state), summarized from more than 183 million high-quality, high-resolution, HCD MS$^2$ spectra with validated peptide assignments and fragmentation annotations. We establish baseline performance using simple statistical rules and learning-based methods, and find that models leveraging peptide-specific information significantly outperform previous methods using only global fragmentation statistics. Furthermore, performance across benchmark models with increasing capacities suggests that the peptide-fragmentation relationship exhibits complex nonlinearities requiring sophisticated machine learning approaches.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples</title>
<link>https://arxiv.org/abs/2508.21083</link>
<guid>https://arxiv.org/abs/2508.21083</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, spurious correlations, counterbias data augmentation, bias reduction, out-of-distribution resilience

Summary: 
The article introduces CoBA (CounterBias Augmentation), a framework that addresses the issue of deep learning models relying on spurious correlations in training data. CoBA operates at the semantic triple level, modifying subject-predicate-object triples to disrupt these correlations and generate counterbias data. By reconstructing text from adjusted triples, CoBA mitigates biases such as gender bias and simplicity bias. Experimental results show that CoBA not only improves task performance but also reduces biases and enhances out-of-distribution resilience. This approach offers a versatile solution to the challenges posed by spurious correlations in deep learning models. 

<br /><br />Summary: <div>
arXiv:2508.21083v1 Announce Type: cross 
Abstract: Deep learning models often learn and exploit spurious correlations in training data, using these non-target features to inform their predictions. Such reliance leads to performance degradation and poor generalization on unseen data. To address these limitations, we introduce a more general form of counterfactual data augmentation, termed counterbias data augmentation, which simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and enhances out-of-distribution robustness. We present CoBA: CounterBias Augmentation, a unified framework that operates at the semantic triple level: first decomposing text into subject-predicate-object triples, then selectively modifying these triples to disrupt spurious correlations. By reconstructing the text from these adjusted triples, CoBA generates counterbias data that mitigates spurious patterns. Through extensive experiments, we demonstrate that CoBA not only improves downstream task performance, but also effectively reduces biases and strengthens out-of-distribution resilience, offering a versatile and robust solution to the challenges posed by spurious correlations.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.21097</link>
<guid>https://arxiv.org/abs/2508.21097</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Quantum computing, Model-to-text transformations, Retrieval-Augmented Generation, Qiskit

Summary:
Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) pipelines offer a new research direction for model-to-text/code transformations, particularly in the realm of quantum and hybrid quantum-classical software systems. The aim is to leverage model-driven approaches to address the challenges posed by heterogeneous platforms and limited developer expertise in this domain. An experiment showcasing the generation of code from UML model instances using Qiskit library for quantum computers yielded promising results, with well-engineered prompts improving CodeBLEU scores significantly. The potential of this research direction extends to deploying software system model instances in RAG pipelines and utilizing LLMs for code-to-code transformations, such as transpilation tasks, for enhanced efficiency and accuracy in quantum code generation. Further exploration through experiments is recommended to explore these avenues comprehensively. 

<br /><br />Summary: <div>
arXiv:2508.21097v1 Announce Type: cross 
Abstract: This paper introduces a novel research direction for model-to-text/code transformations by leveraging Large Language Models (LLMs) that can be enhanced with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum and hybrid quantum-classical software systems, where model-driven approaches can help reduce the costs and mitigate the risks associated with the heterogeneous platform landscape and lack of developers' skills. We validate one of the proposed ideas regarding generating code out of UML model instances of software systems. This Python code uses a well-established library, called Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG pipeline that we deploy incorporates sample Qiskit code from public GitHub repositories. Experimental results show that well-engineered prompts can improve CodeBLEU scores by up to a factor of four, yielding more accurate and consistent quantum code. However, the proposed research direction can go beyond this through further investigation in the future by conducting experiments to address our other research questions and ideas proposed here, such as deploying software system model instances as the source of information in the RAG pipelines, or deploying LLMs for code-to-code transformations, for instance, for transpilation use cases.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrInk: Ink Generation with Transformer Network</title>
<link>https://arxiv.org/abs/2508.21098</link>
<guid>https://arxiv.org/abs/2508.21098</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, ink generation, positional embeddings, Gaussian memory mask, handwriting recognition<br />
Summary: 
TrInk is a Transformer-based model designed for ink generation, focusing on capturing global dependencies for improved handwriting synthesis. It incorporates scaled positional embeddings and a Gaussian memory mask in the cross-attention module to enhance alignment between input text and generated stroke points. The model's performance was evaluated using subjective and objective measures to assess legibility and style consistency of the generated handwriting. Experimental results show significant reductions in character error rate (CER) and word error rate (WER) on the IAM-OnDB dataset compared to previous methods. A demo page showcasing handwriting samples from TrInk and baseline models is provided for further exploration. The research presents a promising approach to enhancing handwriting generation using deep learning techniques. <br /><br />Summary: <div>
arXiv:2508.21098v1 Announce Type: cross 
Abstract: In this paper, we propose TrInk, a Transformer-based model for ink generation, which effectively captures global dependencies. To better facilitate the alignment between the input text and generated stroke points, we introduce scaled positional embeddings and a Gaussian memory mask in the cross-attention module. Additionally, we design both subjective and objective evaluation pipelines to comprehensively assess the legibility and style consistency of the generated handwriting. Experiments demonstrate that our Transformer-based model achieves a 35.56\% reduction in character error rate (CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset compared to previous methods. We provide an demo page with handwriting samples from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models</title>
<link>https://arxiv.org/abs/2508.21099</link>
<guid>https://arxiv.org/abs/2508.21099</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Image, safety mechanisms, Safe-Control, patch-like manner, data-driven strategies

Summary: 
Safe-Control is a plug-and-play safety patch designed to mitigate unsafe content generation in Text-to-Image (T2I) models. It introduces safety control signals into existing T2I models to address safety concerns effectively. The patch-like design allows for flexibility and adaptability, making it compatible with various T2I models of similar architecture. Extensive evaluations on six public T2I models demonstrate that Safe-Control significantly reduces unsafe content generation while maintaining image quality and text alignment. Compared to current safety mechanisms, Safe-Control outperforms all baselines in reducing the probability of unsafe content generation, even under unsafe prompts and adversarial attacks. This innovative approach offers a more robust solution to enhance the safety of T2I models and prevent potential misuse or abuse. 

<br /><br />Summary: <div>
arXiv:2508.21099v1 Announce Type: cross 
Abstract: Despite the advancements in Text-to-Image (T2I) generation models, their potential for misuse or even abuse raises serious safety concerns. Model developers have made tremendous efforts to introduce safety mechanisms that can address these concerns in T2I models. However, the existing safety mechanisms, whether external or internal, either remain susceptible to evasion under distribution shifts or require extensive model-specific adjustments. To address these limitations, we introduce Safe-Control, an innovative plug-and-play safety patch designed to mitigate unsafe content generation in T2I models. Using data-driven strategies and safety-aware conditions, Safe-Control injects safety control signals into the locked T2I model, acting as an update in a patch-like manner. Model developers can also construct various safety patches to meet the evolving safety requirements, which can be flexibly merged into a single, unified patch. Its plug-and-play design further ensures adaptability, making it compatible with other T2I models of similar denoising architecture. We conduct extensive evaluations on six diverse and public T2I models. Empirical results highlight that Safe-Control is effective in reducing unsafe content generation across six diverse T2I models with similar generative architectures, yet it successfully maintains the quality and text alignment of benign images. Compared to seven state-of-the-art safety mechanisms, including both external and internal defenses, Safe-Control significantly outperforms all baselines in reducing unsafe content generation. For example, it reduces the probability of unsafe content generation to 7%, compared to approximately 20% for most baseline methods, under both unsafe prompts and the latest adversarial attacks.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Prediction: Reinforcement Learning as the Defining Leap in Healthcare AI</title>
<link>https://arxiv.org/abs/2508.21101</link>
<guid>https://arxiv.org/abs/2508.21101</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, healthcare, AI, information fusion, clinical environment 
Summary:
This survey discusses the application of reinforcement learning (RL) in healthcare, emphasizing its role in actively deciding interventions with long term goals. The paper explores the landscape of RL techniques, including model-based and model-free methods, and analyzes RL applications in critical care, chronic disease management, mental health, diagnostics, and robotic assistance. It also addresses challenges in ethical considerations, deployment, and reward design in healthcare RL systems, offering insights for safe and human-aligned policy learning. The study highlights RL's transformative potential in healthcare AI by functioning as agentive clinical intelligence rather than solely predictive machinery. <div>
arXiv:2508.21101v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) marks a fundamental shift in how artificial intelligence is applied in healthcare. Instead of merely predicting outcomes, RL actively decides interventions with long term goals. Unlike traditional models that operate on fixed associations, RL systems learn through trial, feedback, and long-term reward optimization, introducing transformative possibilities and new risks. From an information fusion lens, healthcare RL typically integrates multi-source signals such as vitals, labs clinical notes, imaging and device telemetry using temporal and decision-level mechanisms. These systems can operate within centralized, federated, or edge architectures to meet real-time clinical constraints, and naturally span data, features and decision fusion levels. This survey explore RL's rise in healthcare as more than a set of tools, rather a shift toward agentive intelligence in clinical environments. We first structure the landscape of RL techniques including model-based and model-free methods, offline and batch-constrained approaches, and emerging strategies for reward specification and uncertainty calibration through the lens of healthcare constraints. We then comprehensively analyze RL applications spanning critical care, chronic disease, mental health, diagnostics, and robotic assistance, identifying their trends, gaps, and translational bottlenecks. In contrast to prior reviews, we critically analyze RL's ethical, deployment, and reward design challenges, and synthesize lessons for safe, human-aligned policy learning. This paper serves as both a a technical roadmap and a critical reflection of RL's emerging transformative role in healthcare AI not as prediction machinery, but as agentive clinical intelligence.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporal EEG-Based Emotion Recognition Using SAM Ratings from Serious Games with Hybrid Deep Learning</title>
<link>https://arxiv.org/abs/2508.21103</link>
<guid>https://arxiv.org/abs/2508.21103</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, emotion recognition, deep learning, classification, LSTM-GRU<br />
Summary:<br />
This study introduces a new EEG emotion classification framework using the GAMEEMO dataset, focusing on multigranularity emotion classification. The framework includes preprocessing steps such as feature extraction and normalization to convert EEG signals into input vectors. Emotion labels are encoded across three axes: binary valence classification, multi-class emotion classification, and fine-grained multi-label representation. Various models are evaluated, with the LSTM-GRU model performing the best across tasks, achieving high F1-scores in binary valence, multi-class, and multi-label emotion classification. This approach provides a more comprehensive and generalizable method for EEG-based emotion recognition, demonstrating the effectiveness of deep learning approaches in this field.<br /> <div>
arXiv:2508.21103v1 Announce Type: cross 
Abstract: Recent advancements in EEG-based emotion recognition have shown promising outcomes using both deep learning and classical machine learning approaches; however, most existing studies focus narrowly on binary valence prediction or subject-specific classification, which limits generalizability and deployment in real-world affective computing systems. To address this gap, this paper presents a unified, multigranularity EEG emotion classification framework built on the GAMEEMO dataset, which consists of 14-channel EEG recordings and continuous self-reported emotion ratings (boring, horrible, calm, and funny) from 28 subjects across four emotion-inducing gameplay scenarios. Our pipeline employs a structured preprocessing strategy that comprises temporal window segmentation, hybrid statistical and frequency-domain feature extraction, and z-score normalization to convert raw EEG signals into robust, discriminative input vectors. Emotion labels are derived and encoded across three complementary axes: (i) binary valence classification based on the averaged polarity of positive and negative emotion ratings, and (ii) Multi-class emotion classification, where the presence of the most affective state is predicted. (iii) Fine-grained multi-label representation via binning each emotion into 10 ordinal classes. We evaluate a broad spectrum of models, including Random Forest, XGBoost, and SVM, alongside deep neural architectures such as LSTM, LSTM-GRU, and CNN-LSTM. Among these, the LSTM-GRU model consistently outperforms the others, achieving an F1-score of 0.932 in the binary valence task and 94.5% and 90.6% in both multi-class and Multi-Label emotion classification.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning</title>
<link>https://arxiv.org/abs/2508.21104</link>
<guid>https://arxiv.org/abs/2508.21104</guid>
<content:encoded><![CDATA[
<div> Keywords: Critic-free reinforcement learning, group policies, advantage reference anchor, data pre-sampling, State-Of-The-Art performance

Summary: 
PVPO is a reinforcement learning method that enhances efficiency by incorporating an advantage reference anchor and data pre-sampling techniques. By using a reference model for rollout and leveraging reward scores as reference anchors, PVPO corrects biases from intra-group comparisons and reduces the need for extensive rollouts. The reference model also aids in assessing sample difficulty during data pre-sampling, allowing for the selection of high-gain data to enhance training efficiency. Experimental results across nine datasets in two domains show that PVPO achieves State-Of-The-Art performance, demonstrating robust generalization across tasks and scalability across model sizes. PVPO offers a promising solution for addressing computational cost issues in critic-free reinforcement learning methods. 

<br /><br />Summary: <div>
arXiv:2508.21104v1 Announce Type: cross 
Abstract: Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Low-rank Approximation of Full-Matrix Preconditioner for Training Generalized Linear Models</title>
<link>https://arxiv.org/abs/2508.21106</link>
<guid>https://arxiv.org/abs/2508.21106</guid>
<content:encoded><![CDATA[
<div> Keywords: adaptive gradient methods, full-matrix optimization, efficient computation, low-rank structure, scalable solution

Summary: 
AdaGram is introduced as an optimizer that allows for efficient full-matrix adaptive gradient updates, addressing the limitations of diagonal preconditioning matrices. By leveraging fast symmetric factorization and maintaining a low-rank structure of a preconditioner, AdaGram reduces memory and computational costs. Numerical experiments demonstrate that AdaGram can converge faster or match the performance of diagonal adaptive optimizers when utilizing rank five or smaller rank approximations. This showcases AdaGram's potential as a scalable solution for adaptive optimization in large models. <br /><br /> <div>
arXiv:2508.21106v1 Announce Type: cross 
Abstract: Adaptive gradient methods like Adagrad and its variants are widespread in large-scale optimization. However, their use of diagonal preconditioning matrices limits the ability to capture parameter correlations. Full-matrix adaptive methods, approximating the exact Hessian, can model these correlations and may enable faster convergence. At the same time, their computational and memory costs are often prohibitive for large-scale models. To address this limitation, we propose AdaGram, an optimizer that enables efficient full-matrix adaptive gradient updates. To reduce memory and computational overhead, we utilize fast symmetric factorization for computing the preconditioned update direction at each iteration. Additionally, we maintain the low-rank structure of a preconditioner along the optimization trajectory using matrix integrator methods. Numerical experiments on standard machine learning tasks show that AdaGram converges faster or matches the performance of diagonal adaptive optimizers when using rank five and smaller rank approximations. This demonstrates AdaGram's potential as a scalable solution for adaptive optimization in large models.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Generate Unit Test via Adversarial Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.21107</link>
<guid>https://arxiv.org/abs/2508.21107</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, unit testing, code generation, language models, high-quality tests <br />
Summary: 
UTRL is a novel reinforcement learning framework that trains large language models (LLMs) to generate high-quality unit tests by iteratively training a unit test generator and a code generator in an adversarial manner. The unit test generator is trained to produce tests that expose faults in the code generator's solutions, while the code generator is trained to produce solutions that pass the unit tests. Experimental results show that LLMs trained with UTRL outperform models trained via supervised fine-tuning and even frontier models like GPT-4.1 in generating high-quality unit tests. This demonstrates the effectiveness of UTRL in training LLMs for automated test generation in programming. <div>
arXiv:2508.21107v1 Announce Type: cross 
Abstract: Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable, Attention-Enhanced, Bidirectional Long Short-Term Memory Neural Network for Joint 48-Hour Forecasting of Temperature, Irradiance, and Relative Humidity</title>
<link>https://arxiv.org/abs/2508.21109</link>
<guid>https://arxiv.org/abs/2508.21109</guid>
<content:encoded><![CDATA[
<div> framework, deep learning, forecasting, HVAC systems, smart control

Summary:<br /> This paper presents a Deep Learning (DL) framework for 48-hour forecasting of temperature, solar irradiance, and relative humidity to support Model Predictive Control (MPC) in smart HVAC systems. The approach utilizes a stacked Bidirectional Long Short-Term Memory (BiLSTM) network with attention, capturing temporal and cross-feature dependencies by jointly predicting all three variables. Historical meteorological data from 2019-2022, with cyclical time features encoded, was used for training, while 2023 data evaluated generalization. The model achieved Mean Absolute Errors of 1.3 degrees Celsius (temperature), 31 W/m2 (irradiance), and 6.7 percentage points (humidity), surpassing state-of-the-art benchmarks. Integrated Gradients quantified feature contributions, and attention weights revealed temporal patterns, enhancing interpretability. By integrating multivariate forecasting, attention-based DL, and explainability, this work advances data-driven weather prediction, showcasing potential for energy-efficient building control via reliable short-term meteorological forecasting. <br /> <div>
arXiv:2508.21109v1 Announce Type: cross 
Abstract: This paper presents a Deep Learning (DL) framework for 48-hour forecasting of temperature, solar irradiance, and relative humidity to support Model Predictive Control (MPC) in smart HVAC systems. The approach employs a stacked Bidirectional Long Short-Term Memory (BiLSTM) network with attention, capturing temporal and cross-feature dependencies by jointly predicting all three variables. Historical meteorological data (2019-2022) with encoded cyclical time features were used for training, while 2023 data evaluated generalization. The model achieved Mean Absolute Errors of 1.3 degrees Celsius (temperature), 31 W/m2 (irradiance), and 6.7 percentage points (humidity), outperforming state-of-the-art numerical weather prediction and machine learning benchmarks. Integrated Gradients quantified feature contributions, and attention weights revealed temporal patterns, enhancing interpretability. By combining multivariate forecasting, attention-based DL, and explainability, this work advances data-driven weather prediction. The demonstrated accuracy and transparency highlight the framework's potential for energy-efficient building control through reliable short-term meteorological forecasting.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating the Deep Space Network Data Systems; A Case Study in Adaptive Anomaly Detection through Agentic AI</title>
<link>https://arxiv.org/abs/2508.21111</link>
<guid>https://arxiv.org/abs/2508.21111</guid>
<content:encoded><![CDATA[
<div> Deep Space Network, multivariate time-series data, machine learning techniques, anomaly detection, data pipeline
Summary:<br /><br />The study focuses on utilizing machine learning techniques to pinpoint anomalies and equipment degradation in Deep Space Network (DSN) data to ensure the smooth operation of spacecraft communications. Various models were trained to reconstruct data, detect anomalies, and classify their severity levels. A reinforcement learning subsystem and Large Language Model provide explanations for anomalies and classify them based on severity. A data pipeline system was implemented to streamline data extraction, parsing, and processing for DSN transmitters, connecting them with antenna data for comprehensive anomaly detection. An agentic AI system utilizes complex reasoning to predict and classify anomalous data entries, with the ability to improve through human feedback. This integrated approach aims to enhance the maintenance and operation of the DSN for upcoming space missions. 
Summary: <div>
arXiv:2508.21111v1 Announce Type: cross 
Abstract: The Deep Space Network (DSN) is NASA's largest network of antenna facilities that generate a large volume of multivariate time-series data. These facilities contain DSN antennas and transmitters that undergo degradation over long periods of time, which may cause costly disruptions to the data flow and threaten the earth-connection of dozens of spacecraft that rely on the Deep Space Network for their lifeline. The purpose of this study was to experiment with different methods that would be able to assist JPL engineers with directly pinpointing anomalies and equipment degradation through collected data, and continue conducting maintenance and operations of the DSN for future space missions around our universe. As such, we have researched various machine learning techniques that can fully reconstruct data through predictive analysis, and determine anomalous data entries within real-time datasets through statistical computations and thresholds. On top of the fully trained and tested machine learning models, we have also integrated the use of a reinforcement learning subsystem that classifies identified anomalies based on severity level and a Large Language Model that labels an explanation for each anomalous data entry, all of which can be improved and fine-tuned over time through human feedback/input. Specifically, for the DSN transmitters, we have also implemented a full data pipeline system that connects the data extraction, parsing, and processing workflow all together as there was no coherent program or script for performing these tasks before. Using this data pipeline system, we were able to then also connect the models trained from DSN antenna data, completing the data workflow for DSN anomaly detection. This was all wrapped around and further connected by an agentic AI system, where complex reasoning was utilized to determine the classifications and predictions of anomalous data.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control</title>
<link>https://arxiv.org/abs/2508.21112</link>
<guid>https://arxiv.org/abs/2508.21112</guid>
<content:encoded><![CDATA[
<div> model learning vision-text-action robot control multimodal<br />
Summary:<br />
The article introduces EO-Robotics, which includes the EO-1 model and the EO-Data1.5M dataset. The EO-1 model utilizes a unified architecture that processes various types of inputs, such as image, text, video, and action, leading to superior performance in multimodal reasoning and robot control. It is trained on the massive EO-Data1.5M dataset, containing over 1.5 million samples emphasizing vision-text-action comprehension. The model is trained using a combination of auto-regressive decoding and flow matching denoising on the dataset, facilitating seamless robot action generation and multimodal reasoning. Experiments demonstrate the model's effectiveness in open-world understanding and generalization, particularly in long-horizon dexterous manipulation tasks across different embodiments. The paper provides detailed insights into the architecture of EO-1, the construction strategy of EO-Data1.5M, and the training methodology, offering valuable information for the development of advanced embodied foundation models.<br /> <div>
arXiv:2508.21112v1 Announce Type: cross 
Abstract: The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning</title>
<link>https://arxiv.org/abs/2508.21113</link>
<guid>https://arxiv.org/abs/2508.21113</guid>
<content:encoded><![CDATA[
<div> Multimodal Large Language Models, auto-thinking, R-4B, Bi-mode Policy Optimization, state-of-the-art performance <br />
Summary: <br />
Multimodal Large Language Models (MLLMs) equipped with auto-thinking capabilities can adaptively decide when to think based on problem complexity. R-4B, utilizing Bi-mode Policy Optimization, achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and is comparable to larger models like Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks, all with lower computational cost. The model is trained on a diverse dataset containing samples from both thinking and non-thinking modes, then undergoes a second phase of training under an improved GRPO framework. Experimental results demonstrate the effectiveness of R-4B in improving accuracy and performance by integrating both thinking and non-thinking capabilities. <br /> <div>
arXiv:2508.21113v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection</title>
<link>https://arxiv.org/abs/2508.21135</link>
<guid>https://arxiv.org/abs/2508.21135</guid>
<content:encoded><![CDATA[
<div> Keywords: HiddenObject, multimodal environment, fusion framework, Mamba-based fusion mechanism, object detection

Summary:
HiddenObject is a fusion framework that integrates RGB, thermal, and depth data using a Mamba-based fusion mechanism. The method aims to detect hidden or partially concealed objects in multimodal environments where traditional RGB-based methods often fail due to factors like occlusion and camouflage. By capturing complementary signals across modalities, HiddenObject enhances the detection of obscured targets. The approach identifies modality-specific features and fuses them into a unified representation, performing well in challenging scenarios. Validation on benchmark datasets shows state-of-the-art or competitive performance compared to existing methods, highlighting the efficacy of the fusion design. The study suggests that Mamba-based fusion architectures can advance multimodal object detection, especially in visually degraded or complex conditions. <br /><br />Summary: HiddenObject integrates RGB, thermal, and depth data using a Mamba-based fusion mechanism and aims to detect hidden or partially concealed objects in multimodal environments. It captures complementary signals across modalities, identifies modality-specific features, and performs well in challenging scenarios. Validation on benchmark datasets demonstrates state-of-the-art or competitive performance. The study suggests that Mamba-based fusion architectures can advance multimodal object detection in visually degraded or complex conditions. <div>
arXiv:2508.21135v1 Announce Type: cross 
Abstract: Detecting hidden or partially concealed objects remains a fundamental challenge in multimodal environments, where factors like occlusion, camouflage, and lighting variations significantly hinder performance. Traditional RGB-based detection methods often fail under such adverse conditions, motivating the need for more robust, modality-agnostic approaches. In this work, we present HiddenObject, a fusion framework that integrates RGB, thermal, and depth data using a Mamba-based fusion mechanism. Our method captures complementary signals across modalities, enabling enhanced detection of obscured or camouflaged targets. Specifically, the proposed approach identifies modality-specific features and fuses them in a unified representation that generalizes well across challenging scenarios. We validate HiddenObject across multiple benchmark datasets, demonstrating state-of-the-art or competitive performance compared to existing methods. These results highlight the efficacy of our fusion design and expose key limitations in current unimodal and na\"ive fusion strategies. More broadly, our findings suggest that Mamba-based fusion architectures can significantly advance the field of multimodal object detection, especially under visually degraded or complex conditions.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers</title>
<link>https://arxiv.org/abs/2508.21148</link>
<guid>https://arxiv.org/abs/2508.21148</guid>
<content:encoded><![CDATA[
<div> Keywords: Scientific Large Language Models, scientific data, multimodal, cross-scale, evaluation

Summary: 
Scientific Large Language Models (Sci-LLMs) are revolutionizing scientific research by integrating knowledge from complex scientific data. This survey explores the co-evolution of Sci-LLMs and their underlying data, emphasizing the challenges posed by heterogeneous, multi-scale, and uncertainty-laden scientific corpora. The survey categorizes scientific data into a unified taxonomy and hierarchical model of scientific knowledge, highlighting the need for domain-specific representations to enable cross-modal reasoning. Evaluation methods for Sci-LLMs have transitioned towards process- and discovery-oriented assessments with advanced protocols. Persistent issues in scientific data development are addressed, with proposed solutions including semi-automated annotation pipelines and expert validation. The survey proposes a paradigm shift towards closed-loop systems where Sci-LLMs actively contribute to an evolving knowledge base, ultimately aiding in accelerating scientific discovery. 

<br /><br />Summary: <div>
arXiv:2508.21148v1 Announce Type: cross 
Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WaveLLDM: Design and Development of a Lightweight Latent Diffusion Model for Speech Enhancement and Restoration</title>
<link>https://arxiv.org/abs/2508.21153</link>
<guid>https://arxiv.org/abs/2508.21153</guid>
<content:encoded><![CDATA[
<div> neural audio codec, WaveLLDM, latent diffusion, audio restoration, denoising
Summary:
WaveLLDM is introduced as a novel approach for audio restoration and denoising, integrating a neural audio codec with latent diffusion. Operating in a compressed latent space, it reduces computational complexity while maintaining reconstruction quality. Empirical evaluations on the Voicebank+DEMAND test set show accurate spectral reconstruction with low LSD scores (0.48 to 0.60) and good adaptability to unseen data. However, it falls short in terms of perceptual quality and speech clarity compared to existing methods, with WB-PESQ scores ranging from 1.62 to 1.71 and STOI scores between 0.76 and 0.78. The limitations are attributed to architectural tuning, lack of fine-tuning, and inadequate training duration. Despite these shortcomings, the flexible architecture of WaveLLDM combining a neural audio codec and latent diffusion model lays a solid foundation for future advancements in audio processing technology.<br /><br />Summary: <div>
arXiv:2508.21153v1 Announce Type: cross 
Abstract: High-quality audio is essential in a wide range of applications, including online communication, virtual assistants, and the multimedia industry. However, degradation caused by noise, compression, and transmission artifacts remains a major challenge. While diffusion models have proven effective for audio restoration, they typically require significant computational resources and struggle to handle longer missing segments. This study introduces WaveLLDM (Wave Lightweight Latent Diffusion Model), an architecture that integrates an efficient neural audio codec with latent diffusion for audio restoration and denoising. Unlike conventional approaches that operate in the time or spectral domain, WaveLLDM processes audio in a compressed latent space, reducing computational complexity while preserving reconstruction quality. Empirical evaluations on the Voicebank+DEMAND test set demonstrate that WaveLLDM achieves accurate spectral reconstruction with low Log-Spectral Distance (LSD) scores (0.48 to 0.60) and good adaptability to unseen data. However, it still underperforms compared to state-of-the-art methods in terms of perceptual quality and speech clarity, with WB-PESQ scores ranging from 1.62 to 1.71 and STOI scores between 0.76 and 0.78. These limitations are attributed to suboptimal architectural tuning, the absence of fine-tuning, and insufficient training duration. Nevertheless, the flexible architecture that combines a neural audio codec and latent diffusion model provides a strong foundation for future development.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration</title>
<link>https://arxiv.org/abs/2508.21154</link>
<guid>https://arxiv.org/abs/2508.21154</guid>
<content:encoded><![CDATA[
<div> Keywords: Computed Tomography, X-ray registration, 3D reconstruction, Radiative Gaussians, Image-guided navigation

Summary: 
Computed Tomography/X-ray registration in image-guided navigation is challenging due to the need for high accuracy and real-time performance. Traditional methods are limited by spatial information loss and domain gap. The RadGS-Reg framework introduces a novel approach for vertebral-level CT/X-ray registration using joint 3D Radiative Gaussians reconstruction and 3D/3D registration. The RadGS reconstruction module utilizes a learning-based approach with a Counterfactual Attention Learning mechanism to focus on vertebral regions in noisy X-rays. A patient-specific pre-training strategy adapts the framework from simulated to real data while learning vertebral shape prior knowledge. Experiment results on in-house datasets show superior performance compared to existing methods. The code for RadGS-Reg is available for reference. <br /><br />Summary: <div>
arXiv:2508.21154v1 Announce Type: cross 
Abstract: Computed Tomography (CT)/X-ray registration in image-guided navigation remains challenging because of its stringent requirements for high accuracy and real-time performance. Traditional "render and compare" methods, relying on iterative projection and comparison, suffer from spatial information loss and domain gap. 3D reconstruction from biplanar X-rays supplements spatial and shape information for 2D/3D registration, but current methods are limited by dense-view requirements and struggles with noisy X-rays. To address these limitations, we introduce RadGS-Reg, a novel framework for vertebral-level CT/X-ray registration through joint 3D Radiative Gaussians (RadGS) reconstruction and 3D/3D registration. Specifically, our biplanar X-rays vertebral RadGS reconstruction module explores learning-based RadGS reconstruction method with a Counterfactual Attention Learning (CAL) mechanism, focusing on vertebral regions in noisy X-rays. Additionally, a patient-specific pre-training strategy progressively adapts the RadGS-Reg from simulated to real data while simultaneously learning vertebral shape prior knowledge. Experiments on in-house datasets demonstrate the state-of-the-art performance for both tasks, surpassing existing methods. The code is available at: https://github.com/shenao1995/RadGS_Reg.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations</title>
<link>https://arxiv.org/abs/2508.21164</link>
<guid>https://arxiv.org/abs/2508.21164</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, bias, evaluation, model identity, benchmarking

Summary:
This study examines bias in evaluations of outputs from different language models, specifically ChatGPT, Gemini, and Claude. The research identifies asymmetries in how the models' perceived identities impact evaluations, with labels significantly influencing scores. False labeling can reverse rankings and lead to drastic shifts in preference votes and quality ratings. True labels can affect model performance, with Gemini seeing a collapse in self-scores while Claude's self-preference intensifies. These findings highlight the need for blind or multimodel evaluation protocols to ensure fairness in benchmarking large language models. <div>
arXiv:2508.21164v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet their judgments may be influenced. This study examines bias in self- and cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions: no labels, true labels, and two false-label scenarios. Blog posts authored by each model were evaluated by all three using both overall preference voting and quality ratings for Coherence, Informativeness, and Conciseness, with all scores expressed as percentages for direct comparison. Results reveal striking asymmetries: the "Claude" label consistently boosts scores, while the "Gemini" label consistently depresses them, regardless of actual content. False labels frequently reversed rankings, producing shifts of up to 50 percentage points in preference votes and up to 12 percentage points in converted quality ratings. Gemini's self-scores collapsed under true labels, while Claude's self-preference intensified. These findings show that perceived model identity can heavily distort high-level judgments and subtly influence detailed quality ratings, underscoring the need for blind or multimodel evaluation protocols to ensure fairness in LLM benchmarking.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2508.21172</link>
<guid>https://arxiv.org/abs/2508.21172</guid>
<content:encoded><![CDATA[
<div> Keywords: Echo State Networks, Reservoir Computing, Deep Residual Echo State Networks, long-term information processing, memory capacity

Summary:
Deep Residual Echo State Networks (DeepResESNs) are introduced as a novel class of untrained RNNs based on temporal residual connections within the Reservoir Computing framework. These networks utilize a hierarchy of residual recurrent layers to enhance memory capacity and long-term temporal modeling. Different configurations of temporal residual connections are explored, including randomly generated and fixed-structure options, with a focus on stable dynamics. Mathematical analysis provides conditions for stable dynamics within DeepResESN. Experimental results on various time series tasks demonstrate the superiority of DeepResESNs over traditional shallow and deep RC approaches in terms of performance and efficiency.<br /><br />Summary: <div>
arXiv:2508.21172v1 Announce Type: cross 
Abstract: Echo State Networks (ESNs) are a particular type of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular for their fast and efficient learning. However, traditional ESNs often struggle with long-term information processing. In this paper, we introduce a novel class of deep untrained RNNs based on temporal residual connections, called Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a hierarchy of untrained residual recurrent layers significantly boosts memory capacity and long-term temporal modeling. For the temporal residual connections, we consider different orthogonal configurations, including randomly generated and fixed-structure configurations, and we study their effect on network dynamics. A thorough mathematical analysis outlines necessary and sufficient conditions to ensure stable dynamics within DeepResESN. Our experiments on a variety of time series tasks showcase the advantages of the proposed approach over traditional shallow and deep RC.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FUTURE: Flexible Unlearning for Tree Ensemble</title>
<link>https://arxiv.org/abs/2508.21181</link>
<guid>https://arxiv.org/abs/2508.21181</guid>
<content:encoded><![CDATA[
<div> Keywords: Tree ensembles, unlearning algorithm, data privacy, optimization, real-world datasets <br />
Summary: 
FUTURE is a new unlearning algorithm designed for tree ensembles that addresses the challenge of forgetting sensitive information in large-scale datasets. It formulates the forgetting samples problem as a gradient-based optimization task and uses probabilistic model approximations to handle the non-differentiability of tree ensembles. This approach enables efficient end-to-end unlearning in complex ensembles. The algorithm, FUTURE, shows significant success in real-world datasets, demonstrating its effectiveness in achieving state-of-the-art unlearning performance. The method is versatile and can be applied to diverse domains such as bioinformatics, finance, and medical diagnosis. By prioritizing data privacy and the right to be forgotten, FUTURE offers a practical solution for ensuring sensitive information remains protected in tree ensemble models. <br /><br />Summary: <div>
arXiv:2508.21181v1 Announce Type: cross 
Abstract: Tree ensembles are widely recognized for their effectiveness in classification tasks, achieving state-of-the-art performance across diverse domains, including bioinformatics, finance, and medical diagnosis. With increasing emphasis on data privacy and the \textit{right to be forgotten}, several unlearning algorithms have been proposed to enable tree ensembles to forget sensitive information. However, existing methods are often tailored to a particular model or rely on the discrete tree structure, making them difficult to generalize to complex ensembles and inefficient for large-scale datasets. To address these limitations, we propose FUTURE, a novel unlearning algorithm for tree ensembles. Specifically, we formulate the problem of forgetting samples as a gradient-based optimization task. In order to accommodate non-differentiability of tree ensembles, we adopt the probabilistic model approximations within the optimization framework. This enables end-to-end unlearning in an effective and efficient manner. Extensive experiments on real-world datasets show that FUTURE yields significant and successful unlearning performance.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design</title>
<link>https://arxiv.org/abs/2508.21184</link>
<guid>https://arxiv.org/abs/2508.21184</guid>
<content:encoded><![CDATA[
<div> Bayesian Experimental Design, Large Language Models, sequential, adaptive, conversational <br />
<br />
Summary: 
The article introduces a method called BED-LLM, which uses Bayesian Experimental Design to enhance the ability of Large Language Models (LLMs) to interact intelligently with external sources. By iteratively selecting questions that maximize information gain, BED-LLM enables LLMs to act as effective conversational agents. The approach involves formulating the expected information gain using the LLM's belief distribution, incorporating specific innovations like a refined estimator and targeted query proposal strategy. Through tests involving the 20-questions game and user preference inference, BED-LLM demonstrates significant performance improvements over direct prompting and other adaptive design strategies. <div>
arXiv:2508.21184v1 Announce Type: cross 
Abstract: We propose a general-purpose approach for improving the ability of Large Language Models (LLMs) to intelligently and adaptively gather information from a user or other external source using the framework of sequential Bayesian experimental design (BED). This enables LLMs to act as effective multi-turn conversational agents and interactively interface with external environments. Our approach, which we call BED-LLM (Bayesian Experimental Design with Large Language Models), is based on iteratively choosing questions or queries that maximize the expected information gain (EIG) about the task of interest given the responses gathered previously. We show how this EIG can be formulated in a principled way using a probabilistic model derived from the LLM's belief distribution and provide detailed insights into key decisions in its construction. Further key to the success of BED-LLM are a number of specific innovations, such as a carefully designed estimator for the EIG, not solely relying on in-context updates for conditioning on previous responses, and a targeted strategy for proposing candidate queries. We find that BED-LLM achieves substantial gains in performance across a wide range of tests based on the 20-questions game and using the LLM to actively infer user preferences, compared to direct prompting of the LLM and other adaptive design strategies.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to Softmax Equilibrium</title>
<link>https://arxiv.org/abs/2508.21186</link>
<guid>https://arxiv.org/abs/2508.21186</guid>
<content:encoded><![CDATA[
<div> softmax, language models, variational principle, replicator flow, token distribution

Summary: 
The article discusses decoding in large language models, presenting it as a constrained variational principle on the probability simplex, with a focus on scoring tokens and normalizing using softmax. The discrete ascent is described as the multiplicative-weights update, with its continuous-time limit being the replicator flow. The study proves that the next-token distribution follows a smooth trajectory inside the simplex and converges to the softmax equilibrium for a fixed context and temperature. The analysis also covers the effects of temperature, top-k, and nucleus sampling on the token distribution. Additionally, a controlled study of path-dependent score adjustments and their relation to loop-like, hallucination-style behavior is outlined. The article, however, does not address training dynamics or internal representations, leaving those aspects for future research. <br /><br />Summary: <div>
arXiv:2508.21186v1 Announce Type: cross 
Abstract: Decoding in large language models is often described as scoring tokens and normalizing with softmax. We give a minimal, self-contained account of this step as a constrained variational principle on the probability simplex. The discrete, normalization-respecting ascent is the classical multiplicative-weights (entropic mirror) update; its continuous-time limit is the replicator flow. From these ingredients we prove that, for a fixed context and temperature, the next-token distribution follows a smooth trajectory inside the simplex and converges to the softmax equilibrium. This formalizes the common ``manifold traversal'' intuition at the output-distribution level. The analysis yields precise, practice-facing consequences: temperature acts as an exact rescaling of time along the same trajectory, while top-k and nucleus sampling restrict the flow to a face with identical guarantees. We also outline a controlled account of path-dependent score adjustments and their connection to loop-like, hallucination-style behavior. We make no claims about training dynamics or internal representations; those are deferred to future work.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2508.21201</link>
<guid>https://arxiv.org/abs/2508.21201</guid>
<content:encoded><![CDATA[
<div> keywords: aviation accidents, human factors, HFACS, Reinforcement Learning, Llama-3.1 8B language model

Summary:
An automated HFACS classification framework using Reinforcement Learning with GRPO was developed to analyze human factors behind aviation accidents. The model achieved a 350% increase in exact match accuracy and improved partial match accuracy. It outperformed state-of-the-art LLMs in key metrics. The research suggests using exact match accuracy as a benchmark for evaluating language models in multi-label classification problems. Smaller, domain-optimized models were found to be more efficient for safety analysis, allowing for low-latency deployment on edge devices. The framework incorporates a multi-component reward system and synthetic data generation to address class imbalance in accident datasets. The study demonstrates the effectiveness of domain-specific models in enhancing safety analysis and preventing future incidents. 
<br /><br />Summary: <div>
arXiv:2508.21201v1 Announce Type: cross 
Abstract: Analyzing the human factors behind aviation accidents is crucial for preventing future incidents, yet traditional methods using the Human Factors Analysis and Classification System (HFACS) are limited by scalability and consistency. To address this, we introduce an automated HFACS classification framework for aviation safety analysis that utilizes Reinforcement Learning with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B language model. Our approach incorporates a multi-component reward system tailored for aviation safety analysis and integrates synthetic data generation to overcome class imbalance in accident datasets. The resulting GRPO-optimized model achieved noticeable performance gains, including a 350% increase in exact match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy of 0.8800. Significantly, our specialized model outperforms state-of-the-art LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key metrics. This research also proposes exact match accuracy in multi-label HFACS classification problem as a new benchmarking methodology to evaluate the advanced reasoning capabilities of language models. Ultimately, our work validates that smaller, domain-optimized models can provide a computationally efficient and better solution for critical safety analysis. This approach makes powerful, low-latency deployment on resource-constrained edge devices feasible.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach</title>
<link>https://arxiv.org/abs/2508.21206</link>
<guid>https://arxiv.org/abs/2508.21206</guid>
<content:encoded><![CDATA[
<div> Keywords: autoregressive language models, orthographic attacks, pixel-based generative language model, multilingual text, robustness

Summary: 
Autoregressive language models face vulnerabilities due to orthographic attacks, where input text is altered with characters from various alphabets, resulting in performance decline. This issue arises from the out-of-vocabulary problem in subword tokenizers and their embeddings. To combat this challenge, a pixel-based generative language model is introduced, using pixel-based representations by rendering words as images instead of text-based embeddings. This approach enhances robustness against noisy inputs and supports multilingual text across diverse writing systems. The model is evaluated on datasets like LAMBADA, WMT24, and SST-2, showcasing its resilience to orthographic noise and effectiveness in multilingual scenarios. This innovative method marks a significant advancement in enhancing the security and performance of autoregressive language models. 

<br /><br />Summary: <div>
arXiv:2508.21206v1 Announce Type: cross 
Abstract: Autoregressive language models are vulnerable to orthographic attacks, where input text is perturbed with characters from multilingual alphabets, leading to substantial performance degradation. This vulnerability primarily stems from the out-of-vocabulary issue inherent in subword tokenizers and their embeddings. To address this limitation, we propose a pixel-based generative language model that replaces the text-based embeddings with pixel-based representations by rendering words as individual images. This design provides stronger robustness to noisy inputs, while an extension of compatibility to multilingual text across diverse writing systems. We evaluate the proposed method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2 benchmark, demonstrating both its resilience to orthographic noise and its effectiveness in multilingual settings.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Object Re-Identification via Visual In-Context Prompting</title>
<link>https://arxiv.org/abs/2508.21222</link>
<guid>https://arxiv.org/abs/2508.21222</guid>
<content:encoded><![CDATA[
arXiv:2508.21222v1 Announce Type: cross 
Abstract: Current object re-identification (ReID) methods train domain-specific models (e.g., for persons or vehicles), which lack generalization and demand costly labeled data for new categories. While self-supervised learning reduces annotation needs by learning instance-wise invariance, it struggles to capture \textit{identity-sensitive} features critical for ReID. This paper proposes Visual In-Context Prompting~(VICP), a novel framework where models trained on seen categories can directly generalize to unseen novel categories using only \textit{in-context examples} as prompts, without requiring parameter adaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer semantic identity rules from few-shot positive/negative pairs through task-specific prompting, which then guides a VFM (\eg, DINO) to extract ID-discriminative features via \textit{dynamic visual prompts}. By aligning LLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables generalization to novel categories, eliminating the need for dataset-specific retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K object instances from e-commerce platforms, featuring multi-view images and cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks demonstrate that VICP outperforms baselines by a clear margin on unseen categories. Code is available at https://github.com/Hzzone/VICP.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Layer-wise SSL Features Improve Zero-Shot ASR Performance for Children's Speech?</title>
<link>https://arxiv.org/abs/2508.21225</link>
<guid>https://arxiv.org/abs/2508.21225</guid>
<content:encoded><![CDATA[
arXiv:2508.21225v1 Announce Type: cross 
Abstract: Automatic Speech Recognition (ASR) systems often struggle to accurately process children's speech due to its distinct and highly variable acoustic and linguistic characteristics. While recent advancements in self-supervised learning (SSL) models have greatly enhanced the transcription of adult speech, accurately transcribing children's speech remains a significant challenge. This study investigates the effectiveness of layer-wise features extracted from state-of-the-art SSL pre-trained models - specifically, Wav2Vec2, HuBERT, Data2Vec, and WavLM in improving the performance of ASR for children's speech in zero-shot scenarios. A detailed analysis of features extracted from these models was conducted, integrating them into a simplified DNN-based ASR system using the Kaldi toolkit. The analysis identified the most effective layers for enhancing ASR performance on children's speech in a zero-shot scenario, where WSJCAM0 adult speech was used for training and PFSTAR children speech for testing. Experimental results indicated that Layer 22 of the Wav2Vec2 model achieved the lowest Word Error Rate (WER) of 5.15%, representing a 51.64% relative improvement over the direct zero-shot decoding using Wav2Vec2 (WER of 10.65%). Additionally, age group-wise analysis demonstrated consistent performance improvements with increasing age, along with significant gains observed even in younger age groups using the SSL features. Further experiments on the CMU Kids dataset confirmed similar trends, highlighting the generalizability of the proposed approach.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection</title>
<link>https://arxiv.org/abs/2508.21228</link>
<guid>https://arxiv.org/abs/2508.21228</guid>
<content:encoded><![CDATA[
arXiv:2508.21228v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated impressive performance in both research and real-world applications, but they still struggle with hallucination. Existing hallucination detection methods often perform poorly on sentence-level generation or rely heavily on domain-specific knowledge. While self-consistency approaches help address these limitations, they incur high computational costs due to repeated generation. In this paper, we conduct the first study on identifying redundancy in self-consistency methods, manifested as shared prefix tokens across generations, and observe that non-exact-answer tokens contribute minimally to the semantic content. Based on these insights, we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation through selective inference and annealed decoding. Being orthogonal to the model, dataset, decoding strategy, and self-consistency baseline, our DMP consistently improves the efficiency of multi-response generation and holds promise for extension to alignment and reasoning tasks. Extensive experiments show that our method achieves up to a 3x speedup without sacrificing AUROC performance.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Full-Frequency Temporal Patching and Structured Masking for Enhanced Audio Classification</title>
<link>https://arxiv.org/abs/2508.21243</link>
<guid>https://arxiv.org/abs/2508.21243</guid>
<content:encoded><![CDATA[
arXiv:2508.21243v1 Announce Type: cross 
Abstract: Transformers and State-Space Models (SSMs) have advanced audio classification by modeling spectrograms as sequences of patches. However, existing models such as the Audio Spectrogram Transformer (AST) and Audio Mamba (AuM) adopt square patching from computer vision, which disrupts continuous frequency patterns and produces an excessive number of patches, slowing training, and increasing computation. We propose Full-Frequency Temporal Patching (FFTP), a patching strategy that better matches the time-frequency asymmetry of spectrograms by spanning full frequency bands with localized temporal context, preserving harmonic structure, and significantly reducing patch count and computation. We also introduce SpecMask, a patch-aligned spectrogram augmentation that combines full-frequency and localized time-frequency masks under a fixed masking budget, enhancing temporal robustness while preserving spectral continuity. When applied on both AST and AuM, our patching method with SpecMask improves mAP by up to +6.76 on AudioSet-18k and accuracy by up to +8.46 on SpeechCommandsV2, while reducing computation by up to 83.26%, demonstrating both performance and efficiency gains.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCQA: Hybrid Classical-Quantum Agent for Generating Optimal Quantum Sensor Circuits</title>
<link>https://arxiv.org/abs/2508.21246</link>
<guid>https://arxiv.org/abs/2508.21246</guid>
<content:encoded><![CDATA[
arXiv:2508.21246v1 Announce Type: cross 
Abstract: This study proposes an HCQA for designing optimal Quantum Sensor Circuits (QSCs) to address complex quantum physics problems. The HCQA integrates computational intelligence techniques by leveraging a Deep Q-Network (DQN) for learning and policy optimization, enhanced by a quantum-based action selection mechanism based on the Q-values. A quantum circuit encodes the agent current state using Ry gates, and then creates a superposition of possible actions. Measurement of the circuit results in probabilistic action outcomes, allowing the agent to generate optimal QSCs by selecting sequences of gates that maximize the Quantum Fisher Information (QFI) while minimizing the number of gates. This computational intelligence-driven HCQA enables the automated generation of entangled quantum states, specifically the squeezed states, with high QFI sensitivity for quantum state estimation and control. Evaluation of the HCQA on a QSC that consists of two qubits and a sequence of Rx, Ry, and S gates demonstrates its efficiency in generating optimal QSCs with a QFI of 1. This work highlights the synergy between AI-driven learning and quantum computation, illustrating how intelligent agents can autonomously discover optimal quantum circuit designs for enhanced sensing and estimation tasks.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot KWS for Children's Speech using Layer-Wise Features from SSL Models</title>
<link>https://arxiv.org/abs/2508.21248</link>
<guid>https://arxiv.org/abs/2508.21248</guid>
<content:encoded><![CDATA[
arXiv:2508.21248v1 Announce Type: cross 
Abstract: Numerous methods have been proposed to enhance Keyword Spotting (KWS) in adult speech, but children's speech presents unique challenges for KWS systems due to its distinct acoustic and linguistic characteristics. This paper introduces a zero-shot KWS approach that leverages state-of-the-art self-supervised learning (SSL) models, including Wav2Vec2, HuBERT and Data2Vec. Features are extracted layer-wise from these SSL models and used to train a Kaldi-based DNN KWS system. The WSJCAM0 adult speech dataset was used for training, while the PFSTAR children's speech dataset was used for testing, demonstrating the zero-shot capability of our method. Our approach achieved state-of-the-art results across all keyword sets for children's speech. Notably, the Wav2Vec2 model, particularly layer 22, performed the best, delivering an ATWV score of 0.691, a MTWV score of 0.7003 and probability of false alarm and probability of miss of 0.0164 and 0.0547 respectively, for a set of 30 keywords. Furthermore, age-specific performance evaluation confirmed the system's effectiveness across different age groups of children. To assess the system's robustness against noise, additional experiments were conducted using the best-performing layer of the best-performing Wav2Vec2 model. The results demonstrated a significant improvement over traditional MFCC-based baseline, emphasizing the potential of SSL embeddings even in noisy conditions. To further generalize the KWS framework, the experiments were repeated for an additional CMU dataset. Overall the results highlight the significant contribution of SSL features in enhancing Zero-Shot KWS performance for children's speech, effectively addressing the challenges associated with the distinct characteristics of child speakers.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics</title>
<link>https://arxiv.org/abs/2508.21249</link>
<guid>https://arxiv.org/abs/2508.21249</guid>
<content:encoded><![CDATA[
arXiv:2508.21249v1 Announce Type: cross 
Abstract: The computational cost associated with high-fidelity CFD simulations remains a significant bottleneck in the automotive design and optimization cycle. While ML-based surrogate models have emerged as a promising alternative to accelerate aerodynamic predictions, the field is characterized by a diverse and rapidly evolving landscape of specialized neural network architectures, with no single model demonstrating universal superiority. This paper introduces a novel meta-learning framework that leverages this architectural diversity as a strength. We propose a Mixture of Experts (MoE) model that employs a dedicated gating network to dynamically and optimally combine the predictions from three heterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable multi-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph neural network; and FigConvNet, a factorized implicit global convolution network. The gating network learns a spatially-variant weighting strategy, assigning credibility to each expert based on its localized performance in predicting surface pressure and wall shear stress fields. To prevent model collapse and encourage balanced expert contributions, we integrate an entropy regularization term into the training loss function. The entire system is trained and validated on the DrivAerML dataset, a large-scale, public benchmark of high-fidelity CFD simulations for automotive aerodynamics. Quantitative results demonstrate that the MoE model achieves a significant reduction in L-2 prediction error, outperforming not only the ensemble average but also the most accurate individual expert model across all evaluated physical quantities. This work establishes the MoE framework as a powerful and effective strategy for creating more robust and accurate composite surrogate models by synergistically combining the complementary strengths of specialized architectures.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Machine Learning for Optimizing Entanglement Distribution in Quantum Sensor Circuits</title>
<link>https://arxiv.org/abs/2508.21252</link>
<guid>https://arxiv.org/abs/2508.21252</guid>
<content:encoded><![CDATA[
arXiv:2508.21252v1 Announce Type: cross 
Abstract: In the rapidly evolving field of quantum computing, optimizing quantum circuits for specific tasks is crucial for enhancing performance and efficiency. More recently, quantum sensing has become a distinct and rapidly growing branch of research within the area of quantum science and technology. The field is expected to provide new opportunities, especially regarding high sensitivity and precision. Entanglement is one of the key factors in achieving high sensitivity and measurement precision [3]. This paper presents a novel approach utilizing quantum machine learning techniques to optimize entanglement distribution in quantum sensor circuits. By leveraging reinforcement learning within a quantum environment, we aim to optimize the entanglement layout to maximize Quantum Fisher Information (QFI) and entanglement entropy, which are key indicators of a quantum system's sensitivity and coherence, while minimizing circuit depth and gate counts. Our implementation, based on Qiskit, integrates noise models and error mitigation strategies to simulate realistic quantum environments. The results demonstrate significant improvements in circuit performance and sensitivity, highlighting the potential of machine learning in quantum circuit optimization by measuring high QFI and entropy in the range of 0.84-1.0 with depth and gate count reduction by 20-86%.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Optimizing Large Qubit Array based Quantum Sensor Circuits</title>
<link>https://arxiv.org/abs/2508.21253</link>
<guid>https://arxiv.org/abs/2508.21253</guid>
<content:encoded><![CDATA[
arXiv:2508.21253v1 Announce Type: cross 
Abstract: As the number of qubits in a sensor increases, the complexity of designing and controlling the quantum circuits grows exponentially. Manually optimizing these circuits becomes infeasible. Optimizing entanglement distribution in large-scale quantum circuits is critical for enhancing the sensitivity and efficiency of quantum sensors [5], [6]. This paper presents an engineering integration of reinforcement learning with tensor-network-based simulation (MPS) for scalable circuit optimization for optimizing quantum sensor circuits with up to 60 qubits. To enable efficient simulation and scalability, we adopt tensor network methods, specifically the Matrix Product State (MPS) representation, instead of traditional state vector or density matrix approaches. Our reinforcement learning agent learns to restructure circuits to maximize Quantum Fisher Information (QFI) and entanglement entropy while reducing gate counts and circuit depth. Experimental results show consistent improvements, with QFI values approaching 1, entanglement entropy in the 0.8-1.0 range, and up to 90% reduction in depth and gate count. These results highlight the potential of combining quantum machine learning and tensor networks to optimize complex quantum circuits under realistic constraints.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>